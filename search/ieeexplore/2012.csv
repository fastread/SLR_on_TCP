"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication_Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,"Reference Count","Copyright Year","License","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Prioritization of Test Cases Using Software Agents and Fuzzy Logic","C. Malz; N. Jazdi; P. Gohner","NA; NA; NA","2012 IEEE Fifth International Conference on Software Testing, Verification and Validation","","2012","","","483","486","Limited test time and restricted number of test resources confront test managers with big challenges, especially in the system test. Consequently, the test manager has to prioritize test cases before each test cycle. There is much information available for determining a reasonable prioritization order in software projects. However, due to the complexity of current software systems and the high number of existing test cases, the abundance of information relevant for prioritization is not manageable for the test manager, even with high effort. In this paper we present a concept for an automated prioritization of test cases using software agents and fuzzy logic. Our prioritization system determines the prioritization order which increases the test effectiveness and the fault detection rate.","2159-4848","978-0-7695-4670-4978-1-4577-1906","10.1109/ICST.2012.131","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200143","test case prioritization;software agent;fuzzy logic","Software agents;Fault detection;Estimation;Fuzzy logic;Testing;Software systems","fuzzy logic;program testing;software agents;software fault tolerance","test case;software agent;fuzzy logic;test resource;system test;test manager;test cycle;prioritization order;software project;software system complexity;automated prioritization;prioritization system;test effectiveness;fault detection rate","","12","12","","","","","","IEEE","IEEE Conferences"
"Dynamic Prioritization in Regression Testing","N. Kaushik; M. Salehie; L. Tahvildari; S. Li; M. Moore","NA; NA; NA; NA; NA","2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops","","2011","","","135","138","Although used extensively in industry, regression testing is challenging from both a process management as well as a resource management perspective. In literature, proposed test case prioritization techniques assume a constant pool of test cases with non-changing coverage during the regression testing process, and therefore they work with a fixed, prioritized test suite. However, in practice, test cases and their coverage metrics may change during regression testing due to modifications of software artefacts (e.g. due to bug fixing). For example, modifying obsolete test cases or source code may change the coverage metrics during the process. This may lead to some changes in test case priorities. Dealing with manual tests cases, scheduling test case execution in shared environments and other constraints in practice may cause the same effect. In this paper, we highlight these challenges in industrial regression testing and propose a paradigm called Dynamic Prioritization, which uses in-process events and the most up-to-date test suite to re-order test cases.","","978-0-7695-4345-1978-1-4577-0019","10.1109/ICSTW.2011.68","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954402","regression testing;test case prioritization;software testing","Testing;Measurement;Couplings;Software;Complexity theory;Manuals;Minimization","program testing","regression testing;test case prioritization technique;test case coverage metrics;dynamic prioritization paradigm","","3","14","","","","","","IEEE","IEEE Conferences"
"Test case prioritization for regression testing based on fault dependency","M. I. Kayes","Quality Assurance Engineer, SoftwarePeople, Dhaka, Bangladesh","2011 3rd International Conference on Electronics Computer Technology","","2011","5","","48","52","Test case prioritization techniques involve scheduling test cases for regression testing in an order that increases their effectiveness at meeting some performance goal. This is inefficient to re execute all the test cases in regression testing following the software modifications. Using information obtained from previous test case execution, prioritization techniques order the test cases for regression testing so that most beneficial are executed first thus allows an improved effectiveness of testing. One performance goal, rate of dependency detected among faults, measures how quickly dependency among faults are detected within the regression testing process. An improved rate of fault dependency can provide faster feedback on software and let developers start debugging on the severe faults that cause other faults to appear later. This paper presents the new metric for assessing rate of fault dependency detection and an algorithm to prioritize test cases. Using the new metric the effectiveness of this prioritization is shown comparing it with non-prioritized test case. Analysis proves that prioritized test cases are more effective in detecting dependency among faults.","","978-1-4244-8679-3978-1-4244-8678","10.1109/ICECTECH.2011.5941954","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5941954","Software Engineering;Software Testing;Test Case Prioritization;Regression Testing;Average Percentage of Fault Dependency Detected (APFDD)","Measurement;Software;Software testing;Fault detection;Software algorithms;Debugging","computer aided software engineering;program testing;regression analysis;scheduling","test case prioritization;scheduling test;test case execution;prioritization techniques;regression testing process;fault dependency detection","","7","10","","","","","","IEEE","IEEE Conferences"
"Agent-Based Test Case Prioritization","C. Malz; P. Göhner","NA; NA","2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops","","2011","","","149","152","In this paper an Adaptive Test Management System (ATMS) based on software agents is presented which prioritizes test cases considering available information from test teams and from developments teams about the software system and the test cases. The goal of the ATMS is to increase the number of found faults in the available test time with the determined prioritization order.","","978-0-7695-4345-1978-1-4577-0019","10.1109/ICSTW.2011.81","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954404","test case prioritization;software agents;fuzzy logic","Asynchronous transfer mode;Software agents;Databases;Software systems;Computer architecture;Fuzzy logic","program testing;software agents","agent-based test case prioritization;adaptive test management system;software agents","","6","5","","","","","","IEEE","IEEE Conferences"
"A source-based risk analysis approach for software test optimization","A. Hosseingholizadeh","Department of Computer Science, Ryerson University, Toronto, Canada","2010 2nd International Conference on Computer Engineering and Technology","","2010","2","","V2-601","V2-604","In this paper we introduce our proposed technique for software component test prioritization and optimization which is based on a source-code based risk analysis. Software test is one of the most critical steps in the software development. Considering that the time and human resources of a software project are limited, software test should be scheduled and planned very carefully. In this paper we introduce a classification approach that provides the developers with a risk model of the application which is specifically designed to assist the testing process by identifying the most important components and their corresponding test effort estimation. We designed an analyser tool to apply our technique to a test software project and we presented the results in this paper.","","978-1-4244-6349-7978-1-4244-6347-3978-1-4244-6348","10.1109/ICCET.2010.5485639","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5485639","Software Engineering;Software Risk;Software Test;Metric;Code-Analysis","Risk analysis;Software testing;Programming;Automatic testing;Risk management;Software quality;Computer science;Humans;Application software;Software tools","pattern classification;program testing;risk analysis;software reliability","source code-based risk analysis approach;software test optimization;software component test prioritization;software development;software project;classification approach","","2","12","","","","","","IEEE","IEEE Conferences"
"Prioritizing Requirements-Based Regression Test Cases: A Goal-Driven Practice","M. Salehie; S. Li; L. Tahvildari; R. Dara; S. Li; M. Moore","NA; NA; NA; NA; NA; NA","2011 15th European Conference on Software Maintenance and Reengineering","","2011","","","329","332","Any changes for maintenance or evolution purposes may break existing working features, or may violate the requirements established in the previous software releases. Regression testing is essential to avoid these problems, but it may be ended up with executing many time-consuming test cases. This paper tries to address prioritizing requirements-based regression test cases. To this end, system-level testing is focused on two practical issues in industrial environments: i) addressing multiple goals regarding quality, cost and effort in a project, and ii) using non-code metrics due to the lack of detailed code metrics in some situations. This paper reports a goal-driven practice at Research In Motion (RIM) towards prioritizing requirements-based test cases regarding these issues. Goal-Question-Metric (GQM) is adopted in identifying metrics for prioritization. Two sample goals are discussed to demonstrate the approach: detecting bugs earlier and maintaining testing effort. We use two releases of a prototype Web-based email client to conduct a set of experiments based on the two mentioned goals. Finally, we discuss lessons learned from applying the goal-driven approach and experiments, and we propose few directions for future research.","1534-5351;1534-5351","978-1-61284-259-2978-0-7695-4343","10.1109/CSMR.2011.46","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5741354","Test case prioritization;Software regression testing;Requirements-based test cases;Goal-driven approaches","Testing;Measurement;Computer bugs;Humans;Complexity theory;Software;Electronic mail","program testing;regression analysis;software maintenance;software metrics","requirement prioritization;regression test cases;system-level testing maintenance;noncode metrics;Research In Motion;goal-question-metric;bug detection;Web-based email client","","5","7","","","","","","IEEE","IEEE Conferences"
"Requirement based test case prioritization","R. Kavitha; V. R. Kavitha; N. Suresh Kumar","Computer Science and Engineering, VCET, Madurai, India; Master of Computer Applications, VCET, Madurai, India; Electronics and Communication Engineering, VCET, Madurai, India","2010 INTERNATIONAL CONFERENCE ON COMMUNICATION CONTROL AND COMPUTING TECHNOLOGIES","","2010","","","826","829","Test case prioritization involves scheduling test cases in an order that increases the effectiveness in achieving some performance goals. One of the most important performance goals is the rate of fault detection. Test cases should run in an order that increases the possibility of fault detection and also that detects faults at the earliest in its testing life cycle. In this paper, an algorithm is proposed for system level test case prioritization (TCP) from software requirement specification to improve user satisfaction with quality software and also to improve the rate of severe fault detection. The proposed model prioritizes the system test cases based on the three factors: customer priority, changes in requirement, implementation complexity. The proposed prioritization technique is validated with two different sets of industrial projects and the results show that the proposed prioritization technique improves the rate of severe fault detection.","","978-1-4244-7770-8978-1-4244-7769-2978-1-4244-7768","10.1109/ICCCCT.2010.5670728","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5670728","Test case;test case Prioritization","Fault detection;Complexity theory;Software algorithms;Software;Software testing;Algorithm design and analysis","formal specification;program testing;software fault tolerance;software quality","requirement based test case prioritization;test case scheduling;fault detection;testing life cycle;system level test case prioritization;software requirement specification;user satisfaction;industrial projects","","4","15","","","","","","IEEE","IEEE Conferences"
"A Heuristic Model-Based Test Prioritization Method for Regression Testing","X. Han; H. Zeng; H. Gao","NA; NA; NA","2012 International Symposium on Computer, Consumer and Control","","2012","","","886","889","Due to the resource and time constraints for re-executing large test suites in regression testing, developers are interested in detecting faults in the system as early as possible. Test case prioritization seeks to order test cases in such a way that early fault detection is maximized. In this paper, we present a model-based heuristic method to prioritize test cases for regression testing, which takes into account two types of information collected during execution of the modified model on the test suite. The experiment shows that our algorithm has better effectiveness of early fault detection.","","978-1-4673-0767-3978-0-7695-4655","10.1109/IS3C.2012.226","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6228450","Test prioritization;regression testing;model-based test prioritization;early fault detection","Fault detection;Testing;Computational modeling;Minimization;Software maintenance;Software systems","fault diagnosis;program testing;software maintenance","heuristic model-based test prioritization method;regression testing;fault detection;large test suites;software maintenance","","1","9","","","","","","IEEE","IEEE Conferences"
"Study on Test Case Automated Generation Technology Based on Genetic Algorithm and Ant Colony Optimization Algorithm","D. Xu; Z. Li","NA; NA","2010 International Conference on Electrical and Control Engineering","","2010","","","5655","5658","The enhancement of software quality and reliability has become an important task in the field of software engineering. Software testing is an important step that guarantees software quality and reliability and also an energy-consuming and time-consuming task. Therefore, improving the automation ability of software testing is very important for ensuring software's quality and reducing development cost. This paper designs a new Net-Path-Model, introduces the Genetic Algorithm and Ant Colony Optimization test cases Generating theory with the Model. The paper also realizes the theory for Path-cover Test Cases Generating based on ProGrammar in VC++6.0.","","978-1-4244-6881-2978-1-4244-6880-5978-0-7695-4031","10.1109/iCECE.2010.1374","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5631585","Software Testing;Test case;Path-Cover;Genetic Algorithm;Ant Colony Optimization Algorithm","Ant colony optimization;Software quality;Gallium;Software reliability;Nickel;Software testing","C++ language;genetic algorithms;program testing;software quality;software reliability","test case automated generation technology;genetic algorithm;ant colony optimization algorithm;software quality;software reliability;software engineering;software testing;net-path-model;path-cover test cases;ProGrammar;VC++6.0","","","","","","","","","IEEE","IEEE Conferences"
"Critical component analyzer — A novel test prioritization framework for component based real time systems","M. R. Praba; D. J. Mala","Dept. of Computer Applications, KLN College of Information Technology, Madurai, Tamilnadu, India; Dept. of Computer Applications, Thiagarajar College of Engineering, Madurai. Tamilnadu, India","2011 Malaysian Conference in Software Engineering","","2011","","","281","286","Component based software development system is composed of many components and it uses the reusable components as the building blocks for constructing the complex software system. The major challenges in CBS are testing component dependency that is; it is a tricky task to test each and every component for each possible input data which will lead to exhaustive testing. To reduce the cost, the industries are following some stopping criteria and release the product to the customer side. These stopping criteria will at times lead to skipping up of some of the components from rigorous testing. This will lead to hazardous side effects such as loss in terms of revenue, human life and resources. This insight leads to the need to identify critical components which have the higher dependability measure in terms of functionality and receives higher priority in testing with rigorous test procedures. Hence, this paper proposes a novel method for identifying the critical components from the Software under Test (SUT) and prioritizes them for testing with at most care based on various dependency metrics and measures among the components with the help of Component Execution Sequence Graph (CESG).","","978-1-4577-1531-0978-1-4577-1530-3978-1-4577-1529","10.1109/MySEC.2011.6140684","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140684","Software Testing;Test prioritization;Component based Testing;Critical component;metrics and measures","Couplings;Software;Software testing;Software measurement;Programming","graph theory;object-oriented programming;program testing;real-time systems;software cost estimation;software metrics;software reusability","critical component analyzer;test prioritization framework;component based real time systems;component based software development system;reusable components;complex software system;testing component dependency;exhaustive testing;cost reduction;stopping criteria;hazardous side effects;rigorous test procedures;software under test;SUT;dependency metrics;component execution sequence graph;CESG","","","13","","","","","","IEEE","IEEE Conferences"
"Code coverage-based regression test selection and prioritization in WebKit","Á. Beszédes; T. Gergely; L. Schrettner; J. Jász; L. Langó; T. Gyimóthy","University of Szeged, Department of Software Engineering & HAS Research Group on AI, Szeged, Hungary; University of Szeged, Department of Software Engineering & HAS Research Group on AI, Szeged, Hungary; University of Szeged, Department of Software Engineering & HAS Research Group on AI, Szeged, Hungary; University of Szeged, Department of Software Engineering & HAS Research Group on AI, Szeged, Hungary; University of Szeged, Department of Software Engineering & HAS Research Group on AI, Szeged, Hungary; University of Szeged, Department of Software Engineering & HAS Research Group on AI, Szeged, Hungary","2012 28th IEEE International Conference on Software Maintenance (ICSM)","","2012","","","46","55","Automated regression testing is often crucial in order to maintain the quality of a continuously evolving software system. However, in many cases regression test suites tend to grow too large to be suitable for full re-execution at each change of the software. In this case selective retesting can be applied to reduce the testing cost while maintaining similar defect detection capability. One of the basic test selection methods is the one based on code coverage information, where only those tests are included that cover some parts of the changes. We experimentally applied this method to the open source web browser engine project WebKit to find out the technical difficulties and the expected benefits if this method is to be introduced into the actual build process. Although the principle is simple, we had to solve a number of technical issues, so we report how this method was adapted to be used in the official build environment. Second, we present results about the selection capabilities for a selected set of revisions of WebKit, which are promising. We also applied different test case prioritization strategies to further reduce the number of tests to execute. We explain these strategies and compare their usefulness in terms of defect detection and test suite reduction.","1063-6773","978-1-4673-2312-3978-1-4673-2313","10.1109/ICSM.2012.6405252","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405252","Regression testing;test case selection;code coverage;test prioritization;test quality;WebKit","Testing;Databases;Reliability;Conferences;Software maintenance;Communities;Instruments","Internet;program testing;public domain software;regression analysis","code coverage-based regression testing;regression test selection;regression test prioritization;WebKit;continuously evolving software system;software change;selective retesting;testing cost reduction;defect detection capability;code coverage information;open source web browser engine project;test suite reduction","","7","16","","","","","","IEEE","IEEE Conferences"
"Prioritizing interaction test suite for t-way testing","S. K. Said; R. R. Othman; K. Z. Zamli","School of Electrical and Electronic Engineering, Universiti Sains Malaysia Enginering Campus, 14300 Nibong Tebal, Penang, Malaysia; School of Electrical and Electronic Engineering, Universiti Sains Malaysia Enginering Campus, 14300 Nibong Tebal, Penang, Malaysia; School of Electrical and Electronic Engineering, Universiti Sains Malaysia Enginering Campus, 14300 Nibong Tebal, Penang, Malaysia","2011 Malaysian Conference in Software Engineering","","2011","","","292","297","In recent years, many new t-way interaction based strategies (where t indicates the interaction strength), particularly based on covering arrays, have been developed in the literature. In search of an optimal strategy that generates the most minimum number of tests, many of existing t-way strategies have not sufficiently dealt with test prioritization (i.e. in terms of maximizing new interaction coverage per test). Addressing this issue, this paper highlights a useful prioritization algorithm to reorganize the test cases in order to improve the rate of interaction coverage. This algorithm takes a pre-generated t-way test suite as input and automatically generates a priority ordered test suite as output. In order to demonstrate its applicability, this paper demonstrates the use of the algorithm to help prioritize the test suite generated by existing t-way strategy, MC-MIPOG.","","978-1-4577-1531-0978-1-4577-1530-3978-1-4577-1529","10.1109/MySEC.2011.6140686","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140686","Interaction Testing;Test Prioritization","Software testing;Algorithm design and analysis;Software;Software engineering;Generators;Arrays","program testing","interaction test suite;t-way testing;t-way interaction based strategy;optimal strategy;test prioritization;pregenerated t-way test suite;MC-MIPOG","","2","16","","","","","","IEEE","IEEE Conferences"
"Software Test Data Generation Automatically Based on Improved Adaptive Particle Swarm Optimizer","X. Zhu; X. Yang","NA; NA","2010 International Conference on Computational and Information Sciences","","2010","","","1300","1303","To advance efficiency of software test data generation automatically, based on traditional particle swarm optimizer (PSO) algorithm, we put forward an improved algorithm (APSO) in which inertia weight is adjusted according to the fitness value of particle. Experiment simulation result shows that APSO not only has better performance than immune genetic algorithm (IGA) but also better than PSO, and has broad application prospect in software test.","","978-1-4244-8814-8978-0-7695-4270","10.1109/ICCIS.2010.321","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5709521","particle swarm optimizer (PSO);adaptive;software test","Software;Software algorithms;Convergence;Algorithm design and analysis;Software testing;Optimization","particle swarm optimisation;program compilers;program testing","software test data generation;adaptive particle swarm optimizer;APSO;immune genetic algorithm","","2","7","","","","","","IEEE","IEEE Conferences"
"Test Case Prioritization Technique Based on Genetic Algorithm","W. Jun; Z. Yan; J. Chen","NA; NA; NA","2011 International Conference on Internet Computing and Information Services","","2011","","","173","175","With the rapid development of information technology, software testing, as a software quality assurance, is becoming more and more important. In the software life cycle, each time the code has changed need to be regression testing. The huge test case library makes running a full test case library being challenged. To this end, we designed a genetic algorithm-based test case prioritization algorithm and improved the genetic algorithm proposed software test case prioritization algorithm.","","978-1-4577-1561","10.1109/ICICIS.2011.50","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6063222","Software Testing;Test Case Prioritization;Genetic Algorithms;APBC","","genetic algorithms;program testing;quality assurance;regression analysis;software quality","information technology;software testing;software quality assurance;software life cycle;regression testing;test case library;genetic algorithm-based test case prioritization algorithm","","3","5","","","","","","IEEE","IEEE Conferences"
"Requirement-based test case generation and prioritization","Y. I. Salem; R. Hassan","College of Computing and Information Systems, Arab Academy for Science and Technology, Cairo, Egypt; College of Computing and Information Systems, Arab Academy for Science and Technology, Cairo, Egypt","2010 International Computer Engineering Conference (ICENCO)","","2010","","","152","157","Software release testing is a critical phase in the software development life cycle, as it validates the software against its requirements. Designing comprehensive release test cases that are driven by the software requirements remain the major success factor of the testing phase as far as the software customers are concerned. Further, availing sufficient traceability information to ensure complete coverage of requirements validation in the designed test case suite is significant to software quality assurance. In this paper, we propose a systematic mechanism to derive a set of release test cases from a set of requirements modeled with the Genetic Software Engineering (GSE) method. GSE models functional requirements with a semi-formal visual notation called Behavior Trees (BT). Our algorithm prioritizes the requirements modeled with BTs and derives a set of prioritized release test cases systematically. Additionally, our algorithm provides sufficient traceability information relating test cases to the requirements being tested. This allows for ensuring completeness of test case coverage. We also demonstrate our test case derivation mechanism through a case study.","","978-1-61284-185-4978-1-61284-184-7978-1-61284-183","10.1109/ICENCO.2010.5720443","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5720443","requirements-driven testig test case prioritization;test case generation","Unified modeling language;Semantics;Software;Systematics","program testing;project management;quality assurance;software quality;trees (mathematics)","software release testing;software development life cycle;software requirements;software customers;software quality assurance;genetic software engineering;GSE models;behavior trees;test case generation;test case prioritization","","1","15","","","","","","IEEE","IEEE Conferences"
"Cost Optimizations in Runtime Testing and Diagnosis of Systems of Systems","A. Gonzalez-Sanchez","NA","2011 Fourth IEEE International Conference on Software Testing, Verification and Validation","","2011","","","439","442","In practically all development processes tests are used to detect the presence of faults. This is not an exception for critical and high-availability systems. However, these systems cannot be taken offline or duplicated for testing in some cases. This makes runtime testing necessary. This paper presents work aimed at optimizing the three main sources of testing cost: preparation, execution and diagnosis. First, preparation cost is optimized by defining a metric of the runtime testability of the system, used to elaborate an implementation plan of preparative work for runtime testing. Second, the interrelated nature of test execution cost and diagnostic cost is highlighted and a new diagnostic test prioritization is introduced.","2159-4848","978-1-61284-174-8978-0-7695-4342","10.1109/ICST.2011.28","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770638","runtime testing;diagnostic prioritization","Software testing;Conferences","program diagnostics;program testing","cost optimizations;runtime testing;runtime diagnosis;systems of systems;testing cost;preparation;runtime testability;implementation plan;test execution cost;diagnostic cost;diagnostic test prioritization","","","19","","","","","","IEEE","IEEE Conferences"
"Design and Analysis of Cost-Cognizant Test Case Prioritization Using Genetic Algorithm with Test History","Y. Huang; C. Huang; J. Chang; T. Chen","NA; NA; NA; NA","2010 IEEE 34th Annual Computer Software and Applications Conference","","2010","","","413","418","During software development, regression testing is usually used to assure the quality of modified software. The techniques of test case prioritization schedule the test cases for regression testing in an order that attempts to increase the effectiveness in accordance with some performance goal. The most general goal is the rate of fault detection. It assumes all test case costs and fault severities are uniform. However, those factors usually vary. In order to produce a more satisfactory order, the cost-cognizant metric that incorporates varying test case costs and fault severities is proposed. In this paper, we propose a cost-cognizant test case prioritization technique based on the use of historical records and a genetic algorithm. We run a controlled experiment to evaluate the proposed technique's effectiveness. Experimental results indicate that our proposed technique frequently yields a higher Average Percentage of Faults Detected per Cost (APFDc). The results also show that our proposed technique is also useful in terms of APFDc when all test case costs and fault severities are uniform.","0730-3157;0730-3157;0730-3157","978-1-4244-7513-1978-1-4244-7512-4978-0-7695-4085","10.1109/COMPSAC.2010.66","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676289","regression testing;cost-cognizant test case prioritizaion;test cost;fault severity;rate of fault detection;Average Percentage of Faults Detected pre Cost(APFDc)","Testing;Biological cells;Fault detection;Measurement;Gallium;Search problems;Schedules","genetic algorithms;program testing;regression analysis;scheduling;software fault tolerance;software metrics;software quality","cost-cognizant test case prioritization technique;genetic algorithm;test history;software development;regression testing;modified software quality;test case prioritization schedule technique;fault detection;cost-cognizant metric;average percentage of faults detected per cost;APFDc","","6","12","","","","","","IEEE","IEEE Conferences"
"Critical components identification and verification for effective software test prioritization","D. J. Mala; M. R. Praba","Dept. of Computer Applications, Thiagarajar College of Engineering, Madurai; Dept. of MCA, KLN College of Information Technology, Madurai","2011 Third International Conference on Advanced Computing","","2011","","","181","186","Nowadays, software complexity increases as the number of components in a component based system (CBS) increases. As the complexity level increases, the testing and verification of components also increases. This in turn rose up the testing time and cost which thus made industries to skip off some of the components due to the hard timeline and resource limitations especially during maintenance. This leads to hazardous effects if some of these missed components are critical in term of their core functionality and dependability with other components. Hence, a regression testing which is usually performed during maintenance phase should be developed meticulously to identify and test these critical components rigorously before releasing the software on to the customer side. This paper proposed a novel regression testing method based on the criticality measure calculated by means of dependability metrics and internal complexity metrics. Also, this paper compares the performance of the proposed approach with existing approaches and concluded that the proposed framework outperforms them.","2377-6927","978-1-4673-0671-3978-1-4673-0670-6978-1-4673-0669","10.1109/ICoAC.2011.6165171","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165171","Software Testing;Test prioritization;Component based Testing;Critical component;dependency metrics and Component metrics","Measurement;Software;Complexity theory;Software testing;Programming;Couplings","object-oriented programming;program testing;program verification;regression analysis;software maintenance;software metrics;statistical testing","software test prioritization;critical component identification;critical component verification;software complexity;component based system;software maintenance;core functionality;regression testing;criticality measure;dependability metrics","","","16","","","","","","IEEE","IEEE Conferences"
"Improving Regression Testing Transparency and Efficiency with History-Based Prioritization -- An Industrial Case Study","E. Engström; P. Runeson; A. Ljung","NA; NA; NA","2011 Fourth IEEE International Conference on Software Testing, Verification and Validation","","2011","","","367","376","Background: History based regression testing was proposed as a basis for automating regression test selection, for the purpose of improving transparency and test efficiency, at the function test level in a large scale software development organization. Aim: The study aims at investigating the current manual regression testing process as well as adopting, implementing and evaluating the effect of the proposed method. Method: A case study was launched including: identification of important factors for prioritization and selection of test cases, implementation of the method, and a quantitative and qualitative evaluation. Results: 10 different factors, of which two are history-based, are identified as important for selection. Most of the information needed is available in the test management and error reporting systems while some is embedded in the process. Transparency is increased through a semi-automated method. Our quantitative evaluation indicates a possibility to improve efficiency, while the qualitative evaluation supports the general principles of history-based testing but suggests changes in implementation details.","2159-4848","978-1-61284-174-8978-0-7695-4342","10.1109/ICST.2011.27","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770626","regression testing;history-based prioritization;regression test selection;regression test prioritization;empirical evaluation;industrial case study;function testing","Testing;Software;History;Equations;Interviews;Context;Systematics","program testing;software houses","regression testing transparency;history-based prioritization;history based regression testing;regression test selection automation;large scale software development organization;manual regression testing process;test management systems;error reporting systems","","8","22","","","","","","IEEE","IEEE Conferences"
"Developing a Single Model and Test Prioritization Strategies for Event-Driven Software","R. C. Bryce; S. Sampath; A. M. Memon","Utah State University, Logan; University of Maryland, Baltimore; University of Maryland, College Park","IEEE Transactions on Software Engineering","","2011","37","1","48","64","Event-Driven Software (EDS) can change state based on incoming events; common examples are GUI and Web applications. These EDSs pose a challenge to testing because there are a large number of possible event sequences that users can invoke through a user interface. While valuable contributions have been made for testing these two subclasses of EDS, such efforts have been disjoint. This work provides the first single model that is generic enough to study GUI and Web applications together. In this paper, we use the model to define generic prioritization criteria that are applicable to both GUI and Web applications. Our ultimate goal is to evolve the model and use it to develop a unified theory of how all EDS should be tested. An empirical study reveals that the GUI and Web-based applications, when recast using the new model, show similar behavior. For example, a criterion that gives priority to all pairs of event interactions did well for GUI and Web applications; another criterion that gives priority to the smallest number of parameter value settings did poorly for both. These results reinforce our belief that these two subclasses of applications should be modeled and studied together.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5401169","Combinatorial interaction testing;covering arrays;event-driven software (EDS);t-way interaction coverage;test suite prioritization;user-session testing;Web application testing;GUI testing.","Software testing;Graphical user interfaces;Application software;Computer science;User interfaces;Protocols;Embedded software;Information systems;Educational institutions;Abstracts","graphical user interfaces;Internet;program testing;service-oriented architecture","event-driven software;test prioritization strategy;EDS;GUI testing;Web application testing;graphical user interface","","63","28","","","","","","IEEE","IEEE Journals & Magazines"
"Optimal software testing case research based on self-learning control algorithm","Lulu; Pan Shaobin; Huang Ying","Sch. of Comput. Sci. &amp; Eng., South China Univ. of Technol., Guangzhou, China; Sch. of Comput. Sci. &amp; Eng., South China Univ. of Technol., Guangzhou, China; Sch. of Comput. Sci. &amp; Eng., South China Univ. of Technol., Guangzhou, China","The 2nd International Conference on Software Engineering and Data Mining","","2010","","","106","110","This paper demonstrates an approach to optimizing software testing cases by rapidly fixing software deficiency with given software parameter uncertainty during a regressive testing process. Taking the software testing process into a time-varied system control problem, a state transform matrix model is presented. Because regressive testing is an iterative process, the two-dimensional variable-factor self-learning strategy is used to optimize the test case. The simulation results show that the learning control strategy is better than either random testing or the Markov testing strategy, and it can significantly reduce regressive test numbers and save test costs.","","978-89-88678-22-0978-1-4244-7324","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5542941","Software Testing;State Transforms Matrix;Self-Learning Control;Convergence","Software testing;Optimal control;Software algorithms;Automatic testing;Design optimization;Software systems;Uncertain systems;System testing;Computer science;Paper technology","adaptive control;iterative methods;learning systems;Markov processes;matrix algebra;program testing","optimal software testing;self-learning control algorithm;software deficiency;software parameter uncertainty;regressive testing process;time-varied system control problem;state transform matrix model;iterative process;two-dimensional variable-factor self-learning strategy;Markov testing strategy;regressive test numbers","","","15","","","","","","IEEE","IEEE Conferences"
"The Effects of Time Constraints on Test Case Prioritization: A Series of Controlled Experiments","H. Do; S. Mirarab; L. Tahvildari; G. Rothermel","North Dakota State University, Fargo; IBM, Vancouver; University of Waterloo, Waterloo; University of Nebraska—Lincoln, Lincoln","IEEE Transactions on Software Engineering","","2010","36","5","593","617","Regression testing is an expensive process used to validate modified software. Test case prioritization techniques improve the cost-effectiveness of regression testing by ordering test cases such that those that are more important are run earlier in the testing process. Many prioritization techniques have been proposed and evidence shows that they can be beneficial. It has been suggested, however, that the time constraints that can be imposed on regression testing by various software development processes can strongly affect the behavior of prioritization techniques. If this is correct, a better understanding of the effects of time constraints could lead to improved prioritization techniques and improved maintenance and testing processes. We therefore conducted a series of experiments to assess the effects of time constraints on the costs and benefits of prioritization techniques. Our first experiment manipulates time constraint levels and shows that time constraints do play a significant role in determining both the cost-effectiveness of prioritization and the relative cost-benefit trade-offs among techniques. Our second experiment replicates the first experiment, controlling for several threats to validity including numbers of faults present, and shows that the results generalize to this wider context. Our third experiment manipulates the number of faults present in programs to examine the effects of faultiness levels on prioritization and shows that faultiness level affects the relative cost-effectiveness of prioritization techniques. Taken together, these results have several implications for test engineers wishing to cost-effectively regression test their software systems. These include suggestions about when and when not to prioritize, what techniques to employ, and how differences in testing processes may relate to prioritization cost--effectiveness.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.58","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5482587","Regression testing;test case prioritization;cost-benefits;Bayesian networks;empirical studies.","Time factors;Software testing;Automatic testing;Maintenance engineering;Programming;System testing;Computer Society;Software systems;Bayesian methods;Software quality","belief networks;program testing;regression analysis;software fault tolerance","time constraints;test case prioritization techniques;regression testing;various software development processes;cost-benefit trade-offs;Bayesian networks","","65","62","","","","","","IEEE","IEEE Journals & Magazines"
"Constructing Prioritized Interaction Test Suite with Interaction Relationship","J. Yuan; Z. Jiang","NA; NA","2010 Second International Workshop on Education Technology and Computer Science","","2010","3","","181","184","Interaction testing has addressed some issues on how to select a small subset of test cases. In many systems where interaction testing is needed, the entire test suite is not executed because of time or budget constraints. It is important to prioritize the test cases in these situations. On the other hand, there are not always interactions among any factors in real systems. Moreover, some factors may need N-way (N&gt;2) testing since there is a closer relationship among them. We present a model for prioritized interaction testing with interaction relationship and propose a greedy algorithm for generating variable strength covering arrays with bias.","","978-1-4244-6389-3978-1-4244-6388","10.1109/ETCS.2010.239","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5459713","covering arrays;software interaction testing;test prioritization;greedy algorithm","System testing;Software testing;Computer science education;Greedy algorithms;Costs;Educational technology;Computer science;Laboratories;Embedded system;Embedded computing","greedy algorithms;program testing","prioritized interaction test suite;interaction relationship;interaction testing;budget constraints;greedy algorithm;variable strength;software testing","","","8","","","","","","IEEE","IEEE Conferences"
"Test Case Prioritization for Regression Testing Based on Function Call Path","Z. Zhang; Y. Mu; Y. Tian","NA; NA; NA","2012 Fourth International Conference on Computational and Information Sciences","","2012","","","1372","1375","Test case prioritization is an effective and practical technique of regression testing. It is helpful to increase the efficiency of regression testing by sorting and executing test cases according to their importance. Static paths on function call obtained by analyzing the source code, combined with the dynamic path after executing test cases, the correspondence is built between test cases and the static paths, identifying the changes which software developers modify program to correct defects, giving different priority to test case based on path coverage , test cases are selected in accordance with their priorities in regression testing. Firstly, the background and related concept of test case prioritization are introduced. And then, the relevant research work is outlined, a set of new prioritization algorithms are proposed; implementation and analysis of the algorithm are given finally.","","978-1-4673-2406-9978-0-7695-4789","10.1109/ICCIS.2012.312","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6301421","test case prioritization;function call;regression testing;algorithm","Testing;Algorithm design and analysis;Software;Software algorithms;Heuristic algorithms;Conferences;Sorting","program testing;regression analysis","test case prioritization;regression testing;function call path;static paths;source code;dynamic path;software developers","","","14","","","","","","IEEE","IEEE Conferences"
"A Static Approach to Prioritizing JUnit Test Cases","H. Mei; D. Hao; L. Zhang; L. Zhang; J. Zhou; G. Rothermel","Peking University, Beijing; Peking University, Beijing; Peking University, Beijing; Peking University, Beijing; Peking University, Beijing; University of Nebraska, Lincoln","IEEE Transactions on Software Engineering","","2012","38","6","1258","1275","Test case prioritization is used in regression testing to schedule the execution order of test cases so as to expose faults earlier in testing. Over the past few years, many test case prioritization techniques have been proposed in the literature. Most of these techniques require data on dynamic execution in the form of code coverage information for test cases. However, the collection of dynamic code coverage information on test cases has several associated drawbacks including cost increases and reduction in prioritization precision. In this paper, we propose an approach to prioritizing test cases in the absence of coverage information that operates on Java programs tested under the JUnit framework-an increasingly popular class of systems. Our approach, JUnit test case Prioritization Techniques operating in the Absence of coverage information (JUPTA), analyzes the static call graphs of JUnit test cases and the program under test to estimate the ability of each test case to achieve code coverage, and then schedules the order of these test cases based on those estimates. To evaluate the effectiveness of JUPTA, we conducted an empirical study on 19 versions of four Java programs ranging from 2K-80K lines of code, and compared several variants of JUPTA with three control techniques, and several other existing dynamic coverage-based test case prioritization techniques, assessing the abilities of the techniques to increase the rate of fault detection of test suites. Our results show that the test suites constructed by JUPTA are more effective than those in random and untreated test orders in terms of fault-detection effectiveness. Although the test suites constructed by dynamic coverage-based techniques retain fault-detection effectiveness advantages, the fault-detection effectiveness of the test suites constructed by JUPTA is close to that of the test suites constructed by those techniques, and the fault-detection effectiveness of the test suites constructed by some of JUPTA's variants is better than that of the test suites constructed by several of those techniques.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.106","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6363461","Software testing;regression testing;test case prioritization;JUnit;call graph","Software testing;Regression analysis;Scheduling","Java;program testing;regression analysis;software fault tolerance","static approach;regression testing;test case prioritization techniques;dynamic code coverage information;Java programs;JUnit test case prioritization techniques operating in the absence of coverage information;JUPTA;static call graphs;fault-detection effectiveness;dynamic coverage-based techniques","","47","44","","","","","","IEEE","IEEE Journals & Magazines"
"Automated Software Testing Using Metahurestic Technique Based on an Ant Colony Optimization","P. R. Srivastava; K. Baby","NA; NA","2010 International Symposium on Electronic System Design","","2010","","","235","240","Software testing is an important and valuable part of the software development life cycle. Due to time, cost and other circumstances, exhaustive testing is not feasible that's why there is a need to automate the testing process. Testing effectiveness can be achieved by the State Transition Testing (STT) which is commonly used in real time, embedded and web-based kinds of software systems. The tester's main job to test all the possible transitions in the system. This paper proposed an Ant Colony Optimization (ACO) technique for the automated and full coverage of all state-transitions in the system. Present paper approach generates test sequence in order to obtain the complete software coverage. This paper also discusses the comparison between two metaheuristic techniques (Genetic Algorithm and Ant Colony optimization) for transition based testing.","","978-1-4244-8979-4978-0-7695-4294","10.1109/ISED.2010.52","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5715182","Software Testing;Ant Colony Optimization (ACO);Genetic Algorithm (GA);State Transition Testing (STT);Test Data","Software;Gallium;Software testing;Genetic algorithms;Software algorithms;Artificial intelligence","automatic testing;life cycle costing;optimisation;program testing;software development management","automated software testing;metaheurestic technique;ant colony optimization;software development life cycle;state transition testing;ACO technique","","11","25","","","","","","IEEE","IEEE Conferences"
"A clustering approach to improving test case prioritization: An industrial case study","R. Carlson; H. Do; A. Denton","Microsoft, Fargo, ND, USA; Department of Computer Science, North Dakota State University, Fargo, USA; Department of Computer Science, North Dakota State University, Fargo, USA","2011 27th IEEE International Conference on Software Maintenance (ICSM)","","2011","","","382","391","Regression testing is an important activity for controlling the quality of a software product, but it accounts for a large proportion of the costs of software. We believe that an understanding of the underlying relationships in data about software systems, including data correlations and patterns, could provide information that would help improve regression testing techniques. We conjecture that if test cases have common properties, then test cases within the same group may have similar fault detection ability. As an initial approach to investigating the relationships in massive data in software repositories, in this paper, we consider a clustering approach to help improve test case prioritization. We implemented new prioritization techniques that incorporate a clustering approach and utilize code coverage, code complexity, and history data on real faults. To assess our approach, we have designed and conducted empirical studies using an industrial software product, Microsoft Dynamics Ax, which contains real faults. Our results show that test case prioritization that utilizes a clustering approach can improve the effectiveness of test case prioritization techniques.","1063-6773;1063-6773","978-1-4577-0664-6978-1-4577-0663-9978-1-4577-0662","10.1109/ICSM.2011.6080805","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080805","Regression testing;test case prioritization tecniques;clustering approach;industrial case study","Testing;Complexity theory;History;Fault detection;Measurement;Software systems","costing;pattern clustering;program testing;quality control;regression analysis;software fault tolerance;software quality;statistical testing","clustering approach;test case prioritization;industrial case study;regression testing;quality control;software cost;data correlation;fault detection;software repository;code coverage;code complexity;industrial software product;Microsoft Dynamics Ax","","22","26","","","","","","IEEE","IEEE Conferences"
"PSO based test coverage analysis for event driven software","A. Rauf; N. Ejaz; Q. Abbas; S. U. Rehman; A. A. Shahid","National University of Computer &amp; Emerging Sciences, Islamabad, Pakistan; National University of Computer &amp; Emerging Sciences, Islamabad, Pakistan; Department of Computer Science, Iqra university, Islamabad, Pakistan; National University of Computer &amp; Emerging Sciences, Islamabad, Pakistan; National University of Computer &amp; Emerging Sciences, Islamabad, Pakistan","The 2nd International Conference on Software Engineering and Data Mining","","2010","","","219","224","Graphical User Interface (GUI) includes graphical objects for certain functionalities and features that will determine the state of the GUI at any time. To ensure that software quality is par excellence, software development organizations have made a lot of efforts to test the software with care. However, the process of the examination of a GUI application requires, a huge effort, because of the complexity of these applications. As a result, these organizations have started to provide various automated testing techniques to achieve this goal. Flow graph of events (control flow graph) is an innovative technology being used in the automated GUI testing. The search for the ultimate guarantee for software quality through the introduction of automated software testing raises a more difficult question of ""quantity"" of tests necessary to ensure the best results. During automated software testing process, evolutionary algorithms can be used to endow with knowledge about the quality of automated test suite based on preset criterion. Usually this test criterion corresponds to a ""coverage function"" that measures how much of the automatically generated optimization parameters satisfies the given test criterion. In this paper, we have attempted to exploit the event driven nature of GUI. Based on this nature, we have presented a GUI testing and coverage analysis technique centered on Particle Swarm Optimization (PSO).","","978-89-88678-22-0978-1-4244-7324","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5542921","GUI Testing;particle swarm optimization;coverage criterion;event flow;test data","Software testing;Graphical user interfaces;Automatic testing;Software quality;Application software;Flow graphs;Programming;Automatic control;Evolutionary computation;Particle swarm optimization","evolutionary computation;flow graphs;graphical user interfaces;particle swarm optimisation;program testing;software quality","particle swarm optimisation;test coverage analysis;event driven software;graphical user interface;software quality;software development organizations;flow graph;automated GUI testing;automated software testing process;evolutionary algorithms","","","17","","","","","","IEEE","IEEE Conferences"
"Sequence-based techniques for black-box test case prioritization for composite service testing","A. Askarunisa; A. M. Abirami; K. A. J. Punitha; B. K. Selvakumar; R. Arunkumar","Department of Computer Science Engineering, Thiagarajar College of Engineering, Madurai, India; Department of Computer Science Engineering, Thiagarajar College of Engineering, Madurai, India; Department of Computer Science Engineering, Thiagarajar College of Engineering, Madurai, India; Department of Computer Science Engineering, Thiagarajar College of Engineering, Madurai, India; Department of Computer Science Engineering, Thiagarajar College of Engineering, Madurai, India","2010 IEEE International Conference on Computational Intelligence and Computing Research","","2010","","","1","4","Web service may evolve autonomously, making peer web services in the same service composition uncertain as to whether the evolved behaviors may still be compatible to its originally collaborative agreement. Although peer services may wish to conduct regression testing to verify the original collaboration, the source code of the former service can be inaccessible to them. Traditional code-based regression testing strategies are inapplicable to web services. In this paper, we formulate new test case prioritization strategies using sequences in XML messages to reorder regression test cases for composite web services, against the tag based techniques given in and reveal how the test cases use the interface specifications of the composite services. The results were evaluated experimentally and the results show that the new techniques can have a high probability of outperforming random ordering and the techniques given in.","","978-1-4244-5967-4978-1-4244-5965","10.1109/ICCIC.2010.5705784","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5705784","Black-box Regression Testing;Composite service Testing;Test case prioritization","Testing;Web services;XML;Fault detection;Meteorology;IEEE Computer Society Press;Software","groupware;identification technology;peer-to-peer computing;program testing;regression analysis;Web services;XML","peer Web service;collaborative agreement;regression testing;source code;test case prioritization strategies;XML message sequence;tag based techniques;interface specification;black-box test case prioritization;composite service testing","","2","11","","","","","","IEEE","IEEE Conferences"
"MOTCP: A tool for the prioritization of test cases based on a sorting genetic algorithm and Latent Semantic Indexing","M. M. Islam; A. Marchetto; A. Susi; F. B. Kessler; G. Scanniello","Trento, Italy; Trento, Italy; Trento, Italy; Trento, Italy; Dipartimento di Matematica e Informatica, Universit&#x00E0; della Basilicata, Potenza, Italy","2012 28th IEEE International Conference on Software Maintenance (ICSM)","","2012","","","654","657","Test prioritization techniques can be used to determine test case ordering and early discover faults in source code. Several of these techniques exploit a single objective function, e.g., code or requirements coverage. In this tool demo paper, we present MOTCP, a software tool that implements a multi-objective test prioritization technique based on the information related to the code and requirements coverage, as well as the execution cost of each test case. To establish users' and system requirements coverage, the MOTCP uses Latent Semantic Indexing to recover traceability links among application source code and requirements specifications. The test case ordering is then obtained by applying a non-dominated sorting genetic algorithm.","1063-6773","978-1-4673-2312-3978-1-4673-2313","10.1109/ICSM.2012.6405346","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405346","Genetic algorithms;multi-objective Optimization;test prioritization techniques;traceability","Conferences;Software maintenance;Genetic algorithms;Optimization;Sociology;Statistics","genetic algorithms;indexing;Pareto optimisation;program testing;software fault tolerance;software tools;sorting","MOTCP tool;software tool;sorting genetic algorithm;latent semantic indexing;test case ordering;early fault discovery;objective function;multiobjective test case prioritization technique;system requirements coverage;execution cost;traceability link recovery;application source code;software testing;multiobjective optimization","","3","15","","","","","","IEEE","IEEE Conferences"
"Web services regression test case prioritization","B. Athira; P. Samuel","Department of Computer Science, Cochin University of Science and Technology, India; Information Technology, School of Engineering, Cochin University of Science and Technology, India","2010 International Conference on Computer Information Systems and Industrial Management Applications (CISIM)","","2010","","","438","443","Web services and their underlying system grow over time and need to be retested whenever there is a change. This is essential for ensuring uncompromised quality. If we have modified only a small part of the system, it should be possible to reuse the existing test suite. Anyhow, for large modifications or for large systems, retesting the entire test suite will consume large amounts of time and computing resources. In this paper we propose a new method to prioritize test cases in web applications. Our test prioritization technique orders test cases in such a way that the most beneficial is executed first. Most of the existing test prioritization methods are based on the code of the system, but we propose a model-based test prioritization using activity diagram. Our technique identifies difference between original model and modified model. Using this information we plot activity paths for each test case and identify the most promising paths. The test case which covers these paths is considered as the most beneficial test cases. Our approach is effective in revealing the most promising regression test cases. We have applied our method on an online air ticket reservation system in which we could identify the most beneficial test cases from the existing ones.","","978-1-4244-7818-7978-1-4244-7817-0978-1-4244-7816","10.1109/CISIM.2010.5643499","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5643499","Web services;Test Prioritization;Activity Diagram;Activity paths","Unified modeling language;Biological system modeling;Business;Testing;Software;Computers;Information systems","model-based reasoning;program testing;regression analysis;software maintenance;Web services","Web service;test case prioritization;model based prioritization;regression test case;online air ticket reservation system","","5","18","","","","","","IEEE","IEEE Conferences"
"Making the Case for MORTO: Multi Objective Regression Test Optimization","M. Harman","NA","2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops","","2011","","","111","114","This paper argues that regression test optimization problems such as selection and prioritization require multi objective optimization in order to adequately cater for real world regression testing scenarios. The paper presents several examples of costs and values that could be incorporated into such a Multi Objective Regression Test Optimization (MORTO) approach.","","978-0-7695-4345-1978-1-4577-0019","10.1109/ICSTW.2011.60","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954399","regression testing;SBSE;testing;Search Based Software Engineering;test selection;test prioritization","Testing;Optimization;Minimization;Software engineering;USA Councils;IEEE Computer Society Press;Conferences","optimisation;program testing;regression analysis","multiobjective regression test optimization;test selection;test prioritization","","18","30","","","","","","IEEE","IEEE Conferences"
"Arranging software test cases through an optimization method","G. Y. Chen; J. Rogers","Chung Yuan Christian University, Industrial & Systems Engineering, Chung Li, Taiwan; University of Texas - Arlington, Industrial & Mfg. Systems Engineering, Arlington, TX - USA","PICMET 2010 TECHNOLOGY MANAGEMENT FOR GLOBAL ECONOMIC GROWTH","","2010","","","1","5","During the software testing process, the customers would be invited to review or inspect an ongoing software product. This phase is called the “in-plant” test, often known as an “alpha” test. Typically, this test phase lasts for a very short period of time in which the software test engineers or software quality engineers rush to execute a list of software test cases in the test suite with customers. Because of the time constraint, the test cases have to be arranged in terms of test case severities, estimated test time, and customers' demands. As important as the test case arrangement is, this process is mostly performed manually by the project managers and software test engineers together. As the software systems are getting more sophisticated and complex, a greater volume of test cases have to be generated, and the manual arrangement approach may not be the most efficient way to handle this. In this paper, we propose a framework for automating the process of test case arrangement and management through an optimization method. We believe that this framework will help software test engineers facing with the challenges of prioritizing test cases.","2159-5100","978-1-4244-8203-0978-1-890843-21","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5602131","","Software;Software testing;Measurement;Ant colony optimization;Programming;Manuals","program testing;software quality","software testing process;software product;software quality engineers;software test engineers;project managers;in-plant test;alpha test","","","10","","","","","","IEEE","IEEE Conferences"
"A Multi-objective Particle Swarm Optimization for Test Case Selection Based on Functional Requirements Coverage and Execution Effort","L. S. d. Souza; P. B. C. d. Miranda; R. B. C. Prudencio; F. d. A. Barros","NA; NA; NA; NA","2011 IEEE 23rd International Conference on Tools with Artificial Intelligence","","2011","","","245","252","Although software testing is a central task in the software lifecycle, it is sometimes neglected due to its high costs. Tools to automate the testing process minor its costs, however they generate large test suites with redundant Test Cases (TC). Automatic TC Selection aims to reduce a test suite based on some selection criterion. This process can be treated as an optimization problem, aiming to find a subset of TCs which optimizes one or more objective functions (i.e., selection criteria). The majority of search-based works focus on single-objective selection. In this light, we developed a mechanism for functional TC selection which considers two objectives simultaneously: maximize requirements' coverage while minimizing cost in terms of TC execution effort. This mechanism was implemented as a multi-objective optimization process based on Particle Swarm Optimization (PSO). We implemented two multi-objective versions of PSO (BMOPSO and BMOPSO-CDR). The experiments were performed on two real test suites, revealing very satisfactory results (attesting the feasibility of the proposed approach). We highlight that execution effort is an important aspect in the testing process, and it has not been used in a multi-objective way together with requirements coverage for functional TC selection.","2375-0197;1082-3409;1082-3409","978-1-4577-2068-0978-0-7695-4596","10.1109/ICTAI.2011.45","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6103335","Software testing;Test case selection;Multiobjective optimization;PSO;Particle Swarm Optimization","Measurement;Search problems;Optimization;Hypercubes;Software testing;Particle swarm optimization","particle swarm optimisation;program testing;software tools","multiobjective particle swarm optimization;functional requirements coverage;execution effort;software testing;software lifecycle;tools;testing process automation;automatic test case selection;search-based work;single-objective selection;functional test case selection;cost minimisation;multiobjective optimization process;BMOPSO-CDR","","15","28","","","","","","IEEE","IEEE Conferences"
"Efficient Software Verification: Statistical Testing Using Automated Search","S. Poulding; J. A. Clark","University of York, York; University of York, York","IEEE Transactions on Software Engineering","","2010","36","6","763","777","Statistical testing has been shown to be more efficient at detecting faults in software than other methods of dynamic testing such as random and structural testing. Test data are generated by sampling from a probability distribution chosen so that each element of the software's structure is exercised with a high probability. However, deriving a suitable distribution is difficult for all but the simplest of programs. This paper demonstrates that automated search is a practical method of finding near-optimal probability distributions for real-world programs, and that test sets generated from these distributions continue to show superior efficiency in detecting faults in the software.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5406530","Software/program verification;testing strategies;test coverage of code;optimization.","Statistical analysis;Software testing;Automatic testing;Probability distribution;Software engineering;Fault detection;Sampling methods;Software algorithms;Application software;Flow graphs","program testing;program verification;statistical distributions;statistical testing","software verification;statistical testing;automated search;software fault detection;dynamic testing;random testing;structural testing;test data;near-optimal probability distribution","","26","48","","","","","","IEEE","IEEE Journals & Magazines"
"Porantim-Opt: Optimizing the Combined Selection of Model-Based Testing Techniques","A. C. D. Neto; R. de Freitas Rodrigues; G. H. Travassos","NA; NA; NA","2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops","","2011","","","174","183","The combination of testing techniques is considered an effective strategy to evaluate a software product. However, the selection of which techniques to combine in a software project has been an interesting challenge in the Software Engineering field. This paper presents a proposal extending an approach developed to support the combined selection of model-based testing (MBT) techniques, named Porantim, applying Multiobjective Combinatorial Optimization strategy by determining the smallest dominating set in a bipartite and weighted graph. Thus, a local search strategy algorithm is proposed generating solutions aiming at maximizing the coverage of software project characteristics and skills required by the testing team to use the techniques and minimizing the eventual effort to construct models used for test cases generation. A preliminary evaluation analyzes this new approach when compared to the Porantim's original version, and the results indicate improvements in the MBT techniques selection.","","978-0-7695-4345-1978-1-4577-0019","10.1109/ICSTW.2011.33","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954407","software testing;model-based testing;testing techniques selection;search-based software engineering","Software;Testing;Analytical models;Software engineering;Optimization;Programming;Search problems","graph theory;optimisation;program testing;search problems","Porantim-Opt;model-based testing;software product;software project;software engineering;multiobjective combinatorial optimization;smallest dominating set;bipartite graph;weighted graph;local search strategy","","2","20","","","","","","IEEE","IEEE Conferences"
"Test effort optimization by prediction and ranking of fault-prone software modules","A. K. Pandey; N. K. Goyal","Reliability Engineering Centre, Indian Institute of Technology Kharagpur, W.B. -721302, India; Reliability Engineering Centre, Indian Institute of Technology Kharagpur, W.B. -721302, India","2010 2nd International Conference on Reliability, Safety and Hazard - Risk-Based Technologies and Physics-of-Failure Methods (ICRESH)","","2010","","","136","142","Identification of fault-prone or not fault-prone modules is very essential to improve the reliability and quality of a software system. Once modules are categorized as fault-prone or not fault-prone, test effort are allocated accordingly. Testing effort and efficiency are primary concern and can be optimized by prediction and ranking of fault-prone modules. This paper discusses a new model for prediction and ranking of fault-prone software modules for test effort optimization. Model utilizes the classification capability of data mining techniques and knowledge stored in software metrics to classify the software module as fault-prone or not fault-prone. A decision tree is constructed using ID3 algorithm for the existing project data. Rules are derived form the decision tree and integrated with fuzzy inference system to classify the modules as either fault-prone or not fault-prone for the target data. The model is also able to rank the fault-prone module on the basis of its degree of fault-proneness. The model accuracy are validated and compared with some other models by using the NASA projects data set of PROMOSE repository.","","978-1-4244-8343-3978-1-4244-8344","10.1109/ICRESH.2010.5779531","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5779531","software testing;fault-prone modules;software metrics;ID3 algorithm;fuzzy inference system (FIS)","","decision trees;inference mechanisms;optimisation;program testing;software fault tolerance;software metrics;software quality","test effort optimization;fault-prone software modules;software system reliability;software system quality;software metrics;ID3 algorithm;decision tree;fuzzy inference system","","7","29","","","","","","IEEE","IEEE Conferences"
"Optimal Software Testing Case Design Based on Self-Learning Control Algorithm","H. y. Lulu","NA","International Symposium on Parallel and Distributed Processing with Applications","","2010","","","478","482","This paper demonstrates an approach to optimizing software testing cases by rapidly fixing software deficiency with given software parameter uncertainty during a regressive testing process. Taking the software testing process into a time-varied system control problem, a state transform matrix model is presented. Because regressive testing is an iterative process, the two-dimensional variable-factor self-learning strategy is used to optimize the test case. The simulation results show that the learning control strategy is better than either random testing or the Markov testing strategy, and it can significantly reduce regressive test numbers and save test costs.","2158-9178;2158-9208","978-1-4244-8095-1978-0-7695-4190","10.1109/ISPA.2010.78","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5634369","Software Testing;State Transforms Matrix;Self-Learning Control;Convergence","Software testing;Software;Process control;Markov processes;Convergence;Software algorithms","iterative methods;learning systems;matrix algebra;optimisation;program testing;regression analysis;time-varying systems;transforms","optimal software testing case design;self-learning control algorithm;software deficiency;software parameter uncertainty;regressive testing process;time-varied system control problem;state transform matrix model;iterative process;2D variable-factor self-learning strategy;test case optimization","","","15","","","","","","IEEE","IEEE Conferences"
"A genetic algorithm based approach for prioritization of test case scenarios in static testing","S. Sabharwal; R. Sibal; C. Sharma","Department of computer Science and IT, Netaji Subhas Institute of Technology Delhi, India; Department of computer Science and IT, Netaji Subhas Institute of Technology Delhi, India; Department of computer Science and IT, Netaji Subhas Institute of Technology Delhi, India","2011 2nd International Conference on Computer and Communication Technology (ICCCT-2011)","","2011","","","304","309","White box testing is a test technique that takes into account program code, code structure and internal design flow. White box testing is primarily of two kinds-static and structural. Whereas static testing requires only the source code of the product, not the binaries or executables, in structural testing tests are actually run by the computer on built products. In this paper, we propose a technique for optimizing static testing efficiency by identifying the critical path clusters using genetic algorithm. The testing efficiency is optimized by applying the genetic algorithm on the test data. The test case scenarios are derived from the source code. The information flow metric is adopted in this work for calculating the information flow complexity associated with each node of the control flow graph generated from the source code. This research paper is an extension of our previous research paper [18].","","978-1-4577-1386-6978-1-4577-1385-9978-1-4577-1384","10.1109/ICCCT.2011.6075160","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6075160","software testing;genetic algorithm;Information flow metric;CFG","Testing;Biological cells;Genetic algorithms;Complexity theory;Computers;Measurement;Communications technology","flow graphs;genetic algorithms;program testing;software metrics","genetic algorithm;test case scenario prioritization;white box testing;program code;code structure;internal design flow;source code;structural testing;static testing efficiency optimisation;information flow metric;information flow complexity;control flow graph","","6","18","","","","","","IEEE","IEEE Conferences"
"A Framework to Support Research in and Encourage Industrial Adoption of Regression Testing Techniques","J. M. Kauffman; G. M. Kapfhammer","NA; NA","2012 IEEE Fifth International Conference on Software Testing, Verification and Validation","","2012","","","907","908","When software developers make changes to a program, it is possible that they will introduce faults into previously working parts of the code. As software grows, a regression test suite is run to ensure that the old functionality still works as expected. Yet, as the number of test cases increases, it becomes more expensive to execute the test suite. Reduction and prioritization techniques enable developers to manage large and unwieldy test suites. However, practitioners and researchers do not always use and study these methods due, in part, to a lack of availability. In response to this issue, this paper describes an already released open-source framework that supports both research and practice in regression testing. The sharing of this framework will enable the replication of empirical studies in regression testing and encourage faster industrial adoption of these useful, yet rarely used, techniques.","2159-4848","978-0-7695-4670-4978-1-4577-1906","10.1109/ICST.2012.194","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200107","open-source framework;regression testing","Testing;Open source software;Monitoring;Timing;Algorithm design and analysis;Java","fault diagnosis;program testing;public domain software;regression analysis","industrial adoption;regression testing techniques;software developers;regression test suite;old functionality;test cases;reduction techniques;prioritization techniques;open-source framework","","","8","","","","","","IEEE","IEEE Conferences"
"Prioritizing Tests for Software Fault Localization","A. Gonzalez-Sanchez; E. Piel; H. Gross; A. J. C. van Gemund","NA; NA; NA; NA","2010 10th International Conference on Quality Software","","2010","","","42","51","Test prioritization techniques select test cases that maximize the confidence on the correctness of the system when the resources for quality assurance (QA) are limited. In the event of a test failing, the fault at the root of the failure has to be localized, adding an extra debugging cost that has to be taken into account as well. However, test suites that are prioritized for failure detection can reduce the amount of useful information for fault localization. This deteriorates the quality of the diagnosis provided, making the subsequent debugging phase more expensive, and defeating the purpose of the test cost minimization. In this paper we introduce a new test case prioritization approach that maximizes the improvement of the diagnostic information per test. Our approach minimizes the loss of diagnostic quality in the prioritized test suite. When considering QA cost as the combination of testing cost and debugging cost, on the Siemens set, the results of our test case prioritization approach show up to a 53% reduction of the overall QA cost, compared with the next best technique.","2332-662X;1550-6002;1550-6002","978-1-4244-8078-4978-0-7695-4131","10.1109/QSIC.2010.28","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562943","testing;test prioritization;debugging;diagnosis","Software","fault location;program debugging;program testing;software quality","software fault localization;test prioritization technique;quality assurance;failure detection;diagnosis quality;test cost minimization;test case prioritization approach","","14","18","","","","","","IEEE","IEEE Conferences"
"A Two-Level Prioritization Approach for Regression Testing of Web Applications","D. Garg; A. Datta; T. French","NA; NA; NA","2012 19th Asia-Pacific Software Engineering Conference","","2012","2","","150","153","A test case prioritization technique reschedules test cases for regression testing in an order to achieve specific goals like early fault detection. We propose a new two level prioritization approach to prioritize test cases for web applications as a whole. Our approach automatically selects modified functionalities in a web application and executes test cases on the basis of the impact of modified functionalities. We suggest several new prioritization strategies for web applications and examine whether these prioritization strategies improve the rate of fault detection for web applications. We propose a new automated test suite prioritization model for web applications that selects test cases related to modified functionalities and reschedules them using our new prioritization strategies to detect faults early in test suite execution.","1530-1362;1530-1362;1530-1362","978-1-4673-4930-7978-0-7695-4922","10.1109/APSEC.2012.34","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6462796","Regression testing;Test case prioritization;Web applications;Two level prioritization","Unified modeling language;Software;Conferences;Software testing;Software engineering;Educational institutions","Internet;program testing;regression analysis","two-level prioritization;regression testing;Web applications;test case prioritization","","2","13","","","","","","IEEE","IEEE Conferences"
"Prioritizing State-Based Aspect Tests","D. Xu; J. Ding","NA; NA","2010 Third International Conference on Software Testing, Verification and Validation","","2010","","","265","274","In aspect-oriented programming, aspects are essentially incremental modifications to their base classes. Therefore aspect-oriented programs can be tested in an incremental fashion - we can first test the base classes and then test the base classes and aspects as a whole. This paper demonstrates that, in this incremental testing paradigm, we can prioritize aspect tests so as to report failure earlier. We explore test prioritization for testing aspect-oriented programs against their state models with transition coverage and round-trip coverage. Aspect tests are generated from woven state models obtained by composing aspect models into their base class models. We prioritize aspect tests by identifying the extent to which an aspect modifies its base classes. The modification is measured by the number of new and changed components in state transitions (start state, event, precondition, postcondition, end state). Transitions with more changes have higher priorities for test generation. We evaluate the impact of aspect test prioritization through mutation analysis of two AspectJ programs, where all aspects and their base classes can be modeled by finite state machines. We create aspect mutants of each AspectJ program according to a comprehensive AspectJ fault model. Then we test each mutant with the test suites generated without prioritization and with prioritization, respectively. Our experiment results show that prioritization of aspect tests has accelerated failure report.","2159-4848","978-1-4244-6436-4978-1-4244-6435-7978-0-7695-3990","10.1109/ICST.2010.14","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477076","software testing;aspect-oriented programming;model-based testing;finite state machine;test prioritization","Object oriented modeling;Automata;Software testing;Protection;Computer science;Genetic mutations;Life estimation;Quality assurance","aspect-oriented programming;fault tolerant computing;finite state machines;program testing","state based aspect test;aspect oriented programming;incremental testing paradigm;aspect test prioritization;mutation analysis;AspectJ program;finite state machine;AspectJ fault model","","5","33","","","","","","IEEE","IEEE Conferences"
"Optimal Allocation of Testing Effort Considering Software Architecture","L. Fiondella; S. S. Gokhale","Department of Computer Science and Engineering, University of Connecticut, Storrs, CT, USA; Department of Computer Science and Engineering, University of Connecticut, Storrs, CT, USA","IEEE Transactions on Reliability","","2012","61","2","580","589","The growing dependence of society on software systems places a high premium on their reliable operation. Moreover, the stringent reliability expectations imposed on these systems must be achieved despite their increasing size and complexity, and decreasing resources available for their development and maintenance. To mitigate these dual challenges, a systematic approach to guide the allocation of resources to the components of a software system is necessary. This paper presents an optimization framework which considers the contribution of each component to system reliability to determine the amount of effort to be allocated to each component, towards the ultimate objective of achieving the specified system reliability target with minimal effort. We assume that the contribution of a component to system reliability is governed by two factors: the system architecture, and the effort-reliability relationship of the component. This characterization is referred to as “architecture-based optimization” because it considers the system architecture explicitly in the effort allocation process. It is demonstrated that the architecture-based optimization framework outperforms other effort allocation strategies, including equal component weighting, and expert opinion.","0018-9529;1558-1721","","10.1109/TR.2012.2192016","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6179570","Architecture-based optimization;effort minimization;optimization;software architecture;software reliability","Software reliability;Optimization;Computer architecture;Testing;Resource management;Software","optimisation;program testing;resource allocation;software architecture;software maintenance;software reliability","optimal resource allocation;software architecture;software testing;software reliability;software system;architecture-based optimization;software development;software maintenance;system reliability;system architecture;effort-reliability relationship","","7","28","","","","","","IEEE","IEEE Journals & Magazines"
"An Improved Algorithm for Test Data Generation Based on Particle Swarm Optimization","W. Jianfeng; J. Shouda","NA; NA","2011 First International Conference on Instrumentation, Measurement, Computer, Communication and Control","","2011","","","404","407","The test case generation is one of key issues of combinatorial testing. In this paper, a new algorithm for test data generation based on Particle Swarm Optimization (PSO) is presented. Based on Particle Swarm Optimization, the optimization base and extended parameters are introduced. The number of the current output test data is adjusted dynamically according to the data generated before. The efficiency of the test data generation is improved effectively on the premise of ensuring the optimization of the data generated.","","978-0-7695-4519","10.1109/IMCCC.2011.108","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6154133","test case generation combinatorial testing;Particle Swarm Optimization;the optimization base;extended parameters","Optimization;Particle swarm optimization;Testing;Heuristic algorithms;Algorithm design and analysis;Software;Computers","particle swarm optimisation;program testing","test data generation;particle swarm optimization;combinatorial testing;optimization base","","","10","","","","","","IEEE","IEEE Conferences"
"Analytical survey on automated software test data evaluation","A. Mansoor","Shaheed Zulfikar Ali Bhutto Institute of Science and Technologies, Islamabad, Pakistan","4th International Conference on New Trends in Information Science and Service Science","","2010","","","580","585","Automated software test data optimization has become a major aspect in quality of any software. For quality different test cases has to be performed for testing. In order to evaluate every aspect of the software program the number of test cases has increased tremendously. In this paper author have tried to evaluate different proposed techniques for automated software test data optimization and emphasize is made to extract the critical factors which need to be present in any technique to make the technique optimized one. These factors are then evaluated on the basis of different papers and concluded some results which are beneficial to work for the creation of an optimized technique.","","978-89-88678-17-6978-1-4244-6982","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5488549","Test Case Generation (TCG);Test Case Reduction (TCR);Test Case Prioritization (TCP);Echelon (Prioritization System)","Automatic testing;Software testing;Software quality;Application software;System testing;Benchmark testing;Performance evaluation;Data mining","program testing","software test data optimization;software program;software test data evaluation","","","17","","","","","","IEEE","IEEE Conferences"
"A Multi-Objective Technique to Prioritize Test Cases Based on Latent Semantic Indexing","M. M. Islam; A. Marchetto; A. Susi; G. Scanniello","NA; NA; NA; NA","2012 16th European Conference on Software Maintenance and Reengineering","","2012","","","21","30","To early discover faults in source code, test case ordering has to be properly chosen. To this aim test prioritization techniques can be used. Several of these techniques leave out the execution cost of test cases and exploit a single objective function (e.g., code or requirements coverage). In this paper, we present a multi-objective test prioritization technique that determines sequences of test cases that maximize the number of discovered faults that are both technical and business critical. The technique uses the information related to the code and requirements coverage, as well as the execution cost of each test case. The approach also uses recovered trace ability links among source code and system requirements via the Latent Semantic Indexing technique. We evaluated our proposal against both a random prioritization technique and two single-objective prioritization techniques on two Java applications. The results indicate that our proposal outperforms the baseline techniques and that additional improvements are still possible.","1534-5351","978-0-7695-4666-7978-1-4673-0984","10.1109/CSMR.2012.13","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6178873","Regression Testing;Requirements;Testing;Test Case Prioritization;Traceability","Software;Testing;Large scale integration;Weight measurement;Indexing;Semantics;Business","Java;program testing;software fault tolerance","test case prioritization;latent semantic indexing;fault discovery;source code;single objective function;multiobjective test prioritization technique;traceability links;system requirements;random prioritization technique;single-objective prioritization techniques;Java applications","","6","34","","","","","","IEEE","IEEE Conferences"
"Metamorphic Testing of Stochastic Optimisation","S. Yoo","NA","2010 Third International Conference on Software Testing, Verification, and Validation Workshops","","2010","","","192","201","Testing stochastic optimisation algorithms presents an unique challenge because of two reasons. First, these algorithms are non-testable programs, i.e. if the test oracle was known, there wouldn't have been the need for those algorithms in the first place. Second, their performance can vary depending on the problem instances they are used to solve. This paper applies the statistical metamorphic testing approach to stochastic optimisation algorithms and investigates the impact that different problem instances have on testing optimisation algorithms. The paper presents an empirical evaluation of the approach using instances of Next Release Problem (NRP). The effectiveness of the testing method is evaluated using mutation testing. The result shows that, despite the challenges from the stochastic nature of the optimisation algorithm, metamorphic testing can be effective in testing them.","","978-1-4244-6774-7978-1-4244-6773-0978-0-7695-4050","10.1109/ICSTW.2010.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463648","metamorphic testing;stochastic optimisation;search-based software engineering","Stochastic processes;Software testing;Software engineering;Software algorithms;Machine learning algorithms;Genetic mutations;Educational institutions;Life testing;Software tools;Application software","program testing;software engineering;statistical analysis;stochastic programming","metamorphic testing;stochastic optimisation;test oracle;statistical metamorphic testing approach;next release problem;mutation testing","","8","34","","","","","","IEEE","IEEE Conferences"
"Input Domain Reduction through Irrelevant Variable Removal and Its Effect on Local, Global, and Hybrid Search-Based Structural Test Data Generation","P. McMinn; M. Harman; K. Lakhotia; Y. Hassoun; J. Wegener","University of Sheffield, Sheffield; University College London, London; University College London, London; King's College London, London; Berner & Mattner Systemtechnik GmbH, Berlin","IEEE Transactions on Software Engineering","","2012","38","2","453","477","Search-Based Test Data Generation reformulates testing goals as fitness functions so that test input generation can be automated by some chosen search-based optimization algorithm. The optimization algorithm searches the space of potential inputs, seeking those that are “fit for purpose,” guided by the fitness function. The search space of potential inputs can be very large, even for very small systems under test. Its size is, of course, a key determining factor affecting the performance of any search-based approach. However, despite the large volume of work on Search-Based Software Testing, the literature contains little that concerns the performance impact of search space reduction. This paper proposes a static dependence analysis derived from program slicing that can be used to support search space reduction. The paper presents both a theoretical and empirical analysis of the application of this approach to open source and industrial production code. The results provide evidence to support the claim that input domain reduction has a significant effect on the performance of local, global, and hybrid search, while a purely random search is unaffected.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5710949","Search-based software testing;evolutionary testing;automated test data generation;input domain reduction.","Input variables;Software testing;Optimization;Algorithm design and analysis;Search problems;Software algorithms","automatic test pattern generation;optimisation;program compilers;program slicing;program testing;public domain software;search problems","input domain reduction;irrelevant variable removal;hybrid search-based structural test data generation;fitness functions;test input generation;search-based optimization algorithm;key determining factor;search-based software testing;search space reduction;static dependence analysis;program slicing;open source approach;industrial production code","","26","52","","","","","","IEEE","IEEE Journals & Magazines"
"A Preliminary Study on Factors Affecting Software Testing Team Performance","T. Kanij; R. Merkel; J. Grundy","NA; NA; NA","2011 International Symposium on Empirical Software Engineering and Measurement","","2011","","","359","362","With the growth of the software testing industry, many in-house testing groups and outsourcing testing companies have been established. Underlying the success of these testing groups and companies are team(s) of testers. This research investigates the importance of different factors, diversity and experience on building a successful testing team. We collected the opinions of testing practitioners on these factors via a survey. The outcome strongly indicates the relative importance of different factors and that diversity is helpful for a testing team. The results also support the importance of suitable team experience.","1949-3789;1949-3770;1938-6451","978-1-4577-2203-5978-0-7695-4604","10.1109/ESEM.2011.48","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092588","testing team;rank;diversity;experience","Software measurement;Software engineering","program testing;software development management","software testing team performance;outsourcing testing companies;suitable team experience;software testing industry","","6","11","","","","","","IEEE","IEEE Conferences"
"Convergence of stochastic optimization procedure for adequacy criteria of testing","Y. M. Chabanyuk; D. V. Fedasyuk; M. M. Seniv; U. T. Khimka","NA; NA; NA; NA","Proceedings of International Conference on Modern Problem of Radio Engineering, Telecommunications and Computer Science","","2012","","","397","397","The stochastic optimization procedure with the markov switching for analysis of restoration systems reliability is designed.","","978-617-607-138-9978-1-4673-0283","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6192652","stochastic optimization procedure;software reliability;adequacy criteria of testing","Software testing;Software reliability;Switches;Software;Convergence","Markov processes;optimisation;program testing;stochastic processes","stochastic optimization procedure;adequacy criteria;Markov switching;restoration systems reliability;software testing","","","4","","","","","","IEEE","IEEE Conferences"
"Advanced models for software reliability prediction","Z. Bluvband; S. Porotsky; M. Talmor","A.L.D. Ltd., 52 Menachem Begin Road, Tel-Aviv, Israel 67137; ALD Software Ltd., 52 Manachem Begin Road, Tel-Aviv, Israel 67137; Reliability Center, RAFAEL Ltd., 14A Savion St., Quriat Yam, 29500, Israel","2011 Proceedings - Annual Reliability and Maintainability Symposium","","2011","","","1","5","This article describes the advanced parametric models for assessment and prediction of software reliability, based on statistics of bugs at the initial stage of testing. The parametric model approach, commonly associated with reliability issues, deals with the evaluation of the amount of bugs in the code. Computed parameter values inserted into the model allow to estimate: (a) number of bugs remaining in the product, and (b) time required to detect the remaining bugs. Many models are developed for similar purpose: Duane Reliability Growth Model, Goel Model, Weibull Model, Classical S-shaped Model, Ohba S-shaped Model, etc. Taking into account some detailed, but practical, aspects of the software testing process, a few Advanced Models were developed and usefully implemented by the authors. The proposed models are sensitive to the situations typical for the early stages of Software development. As a result, one deals with the essentially non-linear, multimodal goal function to define the optimal value as the estimation of the unknown control parameter. To support the optimization of such complex models, the Cross-Entropy Global Optimization Method is proposed. Some authentic numerical examples are considered to demonstrate the efficiency of the proposed models.","0149-144X;0149-144X","978-1-4244-8856-8978-1-4244-8857-5978-1-4244-8855","10.1109/RAMS.2011.5754487","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5754487","software reliability;analytical models;cross-entropy","Computer bugs;Software reliability;Testing;Optimization;Software;Maximum likelihood estimation","optimisation;program debugging;program testing;reliability;software reliability","software reliability prediction;duane reliability growth model;Goel Model;Weibull Model;s-shaped model;ohba s-shaped model;software testing process;software development;cross-entropy global optimization method","","2","4","","","","","","IEEE","IEEE Conferences"
"CRANE: Failure Prediction, Change Analysis and Test Prioritization in Practice -- Experiences from Windows","J. Czerwonka; R. Das; N. Nagappan; A. Tarvo; A. Teterev","NA; NA; NA; NA; NA","2011 Fourth IEEE International Conference on Software Testing, Verification and Validation","","2011","","","357","366","Building large software systems is difficult. Maintaining large systems is equally hard. Making post-release changes requires not only thorough understanding of the architecture of a software component about to be changed but also its dependencies and interactions with other components in the system. Testing such changes in reasonable time and at a reasonable cost is a difficult problem as infinitely many test cases can be executed for any modification. It is important to obtain a risk assessment of impact of such post-release change fixes. Further, testing of such changes is complicated by the fact that they are applicable to hundreds of millions of users, even the smallest mistakes can translate to a very costly failure and re-work. There has been significant amount of research in the software engineering community on failure prediction, change analysis and test prioritization. Unfortunately, there is little evidence on the use of these techniques in day-to-day software development in industry. In this paper, we present our experiences with CRANE: a failure prediction, change risk analysis and test prioritization system at Microsoft Corporation that leverages existing research for the development and maintenance of Windows Vista. We describe the design of CRANE, validation of its useful-ness and effectiveness in practice and our learnings to help enable other organizations to implement similar tools and practices in their environment.","2159-4848","978-1-61284-174-8978-0-7695-4342","10.1109/ICST.2011.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770625","Software Reliability;Failure Prediction","Testing;Maintenance engineering;Cranes;Operating systems;Software maintenance;Measurement","object-oriented programming;operating systems (computers);program diagnostics;program testing;risk management;software architecture;software maintenance;software performance evaluation","CRANE;failure prediction;change analysis;large software systems;software component architecture;risk assessment;post-release change fixes;software engineering community;day-to-day software development;change risk analysis;test prioritization system;Microsoft Corporation;Windows Vista;organizations","","19","24","","","","","","IEEE","IEEE Conferences"
"On the Integration of Test Adequacy, Test Case Prioritization, and Statistical Fault Localization","B. Jiang; W. K. Chan","NA; NA","2010 10th International Conference on Quality Software","","2010","","","377","384","Testing and debugging account for at least 30% of the project effort. Scientific advancements in individual activities or their integration may bring significant impacts to the practice of software development. Fault localization is the foremost debugging sub-activity. Any effective integration between testing and debugging should address how well testing and fault localization can be worked together productively. How likely does a testing technique provide test suites for effective fault localization? To what extent may such a test suite be prioritized so that the test cases having higher priority can be effectively used in a standalone manner to support fault localization? In this paper, we empirically study these two research questions in the context of test data adequacy, test case prioritization and statistical fault localization. Our preliminary postmortem analysis results on 16 test case prioritization techniques and four statistical fault localizations show that branch-adequate test suites on the Siemens suite are unlikely to support effective fault localization. On the other hand, if such a test suite is effective, around 60% of the test cases can be further prioritized to support effective fault localization, which indicates that the potential savings in terms of effort can be significant.","2332-662X;1550-6002;1550-6002","978-1-4244-8078-4978-0-7695-4131","10.1109/QSIC.2010.64","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562990","debugging;testing;continuous integration","Equations;Servers;Measurement;Testing;Debugging;Software;Inspection","continuous improvement;program debugging;program testing;software metrics;software quality;statistical analysis","test adequacy;test case prioritization;statistical fault localization;software debugging;software development;branch adequate test suite;Siemens suite;continuous integration","","4","23","","","","","","IEEE","IEEE Conferences"
"Test Case Generation and Prioritization from UML Models","A. Gantait","NA","2011 Second International Conference on Emerging Applications of Information Technology","","2011","","","345","350","This paper proposes a novel approach to generating test cases from UML 2.0 activity diagrams and prioritizing those test cases using model information encapsulated in the activity diagrams. The test cases generated according to our approach are suitable for system level testing of the application. For prioritization of test cases, we propose a method based on coverage of all transitions in the activity diagram and usage probability of a particular flow in the activity model. We also propose an approach for selecting test data based on analysis of the branch conditions of the decision nodes in the activity diagrams.","","978-1-4244-9683","10.1109/EAIT.2011.63","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5734956","UML;Model;Activity Diagram;Test case;Prioritization;Test data","Unified modeling language;Object oriented modeling;Testing;Software;Programming;Data models;Business","probability;program testing;Unified Modeling Language","test case generation;test case prioritization;UML 2.0 activity diagrams;UML models;usage probability","","3","17","","","","","","IEEE","IEEE Conferences"
"A random walk based algorithm for structural test case generation","J. Xuan; H. Jiang; Z. Ren; Y. Hu; Z. Luo","School of Mathematical Sciences Dalian University of Technology Dalian, China; School of Software Dalian University of Technology Dalian, China; School of Mathematical Sciences Dalian University of Technology Dalian, China; School of Software Dalian University of Technology Dalian, China; School of Mathematical Sciences Dalian University of Technology Dalian, China","The 2nd International Conference on Software Engineering and Data Mining","","2010","","","583","588","Structural testing is a significant and expensive process in software development. By converting test data generation into an optimization problem, search-based software testing is one of the key technologies of automated test case generation. Motivated by the success of random walk in solving the satisfiability problem (SAT), we proposed a random walk based algorithm (WalkTest) to solve structural test case generation problem. WalkTest provides a framework, which iteratively calls random walk operator to search the optimal solutions. In order to improve search efficiency, we sorted the test goals with the costs of solutions completely instead of traditional dependence analysis from control flow graph. Experimental results on the condition-decision coverage demonstrated that WalkTest achieves better performance than existing algorithms (random test and tabu search) in terms of running time and coverage rate.","","978-89-88678-22-0978-1-4244-7324","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5542856","automatic test generation;condition-decision coverage;random walk;structural testing","Software testing;Automatic testing;Iterative algorithms;Software algorithms;Reflective binary codes;Flow graphs;Programming;Space technology;Helium;Software prototyping","automatic test software;computability;flow graphs;program testing;search problems","random walk based algorithm;structural test case generation;software development process;optimization problem;search-based software testing;automated test case generation;satisfiability problem;WalkTest;dependence analysis;control flow graph","","1","25","","","","","","IEEE","IEEE Conferences"
"An optimized approach to generate object oriented software test case by Colored Petri Net","E. Mirzaeian; S. Ghaderi Mojaveri; H. Motameni; A. Farahi","Department of IT and Communication, Payam Noor University, Tehran, Iran; Department of IT and Communication, Payam Noor University, Tehran, Iran; Department of Computer Engineering, Islamic Azad University, Sari, Iran; Faculty Member of IT and Communication Department, Payam Noor University, Tehran, Iran","2010 2nd International Conference on Software Technology and Engineering","","2010","2","","V2-251","V2-255","In object-oriented software testing, a class is considered to be a basic unit of testing. Attributes of object-oriented software such as inheritance and polymorphism make behavior analysis and test significantly complicated because the state of the objects may cause faults that cannot be easily revealed with traditional testing techniques. In this paper, we propose a new technique for generating the test case by Colored Petri Nets (CPN), which is an extended version of Petri Nets and usually used to system modeling and simulation. Our method considers net-explosion problem and also our generated Net covers all Instances of Objects from Different Classes in the same hierarchy by introducing new algorithm to convert UML Statechart to CPN. A case study is presented to show the benefit of our approach and resulting Net is implemented in CPN-Tools.","","978-1-4244-8666-3978-1-4244-8667","10.1109/ICSTE.2010.5608812","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5608812","test cases;colored petri net;state space graph;object-oriented","Unified modeling language;Software;Testing;Color;Petri nets;Conferences;Object oriented modeling","graph colouring;inheritance;object-oriented programming;Petri nets;program testing;Unified Modeling Language","optimized approach;object oriented software test case generation;colored Petri net;inheritance;polymorphism;behavior analysis;system modeling;net-explosion problem;UML statechart","","3","13","","","","","","IEEE","IEEE Conferences"
"Test Case Selection Method for Emergency Changes","F. d. Farzat","NA","2nd International Symposium on Search Based Software Engineering","","2010","","","31","35","Software testing is an expensive task that significantly contributes to the total cost of a software development project. Among the many strategies available to test a software project, the creation of automated test cases that can be enacted after building a release or resolving a defect is increasingly used in the industry. However, certain defects found in the system operation may block major business operations. These critical defects are sometimes resolved directly in the production environment under such a restricted deadline that there is not enough time to run the complete set of automated test cases upon the patched version of the software. Declining to run the test case suite allows a quicker release of the software to production, but also allows other defects to be introduced into the system. This paper presents a heuristic approach to select test cases that might support emergency changes aiming to maximize the coverage and diversity of the testing activity under a strict time constraint and given the priority of the features that were changed.","","978-1-4244-8341","10.1109/SSBSE.2010.13","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635168","Software Testing;Genetic Algorithm;Time Constraint;Process development;Risk Management","Software;Testing;Optimization;History;Companies;Time factors;Industries","program testing;software engineering","test case selection method;emergency changes;software testing;software development project;test case suite","","1","11","","","","","","IEEE","IEEE Conferences"
"A Random search based effective algorithm for pairwise test data generation","S. Khatun; K. F. Rabbi; C. Y. Yaakub; M. F. J. Klaib","Faculty of Computer Systems & Software Engineering, University Malaysia Pahang, Malaysia; Faculty of Computer Systems & Software Engineering, University Malaysia Pahang, Malaysia; Faculty of Computer Systems & Software Engineering, University Malaysia Pahang, Malaysia; Faculty of Science and Information Technology, Jadara University, Jordan","International Conference on Electrical, Control and Computer Engineering 2011 (InECCE)","","2011","","","293","297","Testing is a very important task to build error free software. As the resources and time to market is limited for a software product, it is impossible to perform exhaustive test i.e., to test all combinations of input data. To reduce the number of test cases in an acceptable level, it is preferable to use higher interaction level (t way, where t ≥ 2). Pairwise (2-way or t = 2) interaction can find most of the software faults. This paper proposes an effective random search based pairwise test data generation algorithm named R2Way to optimize the number of test cases. Java program has been used to test the performance of the algorithm. The algorithm is able to support both uniform and non-uniform values effectively with performance better than the existing algorithms/tools in terms of number of generated test cases and time consumption.","","978-1-61284-230-1978-1-61284-229-5978-1-61284-228","10.1109/INECCE.2011.5953894","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5953894","Combinatorial interaction testing;Software testing;Pairwise testing;Test case generation","Software;Conferences;Algorithm design and analysis;Software algorithms;Equations;Software testing","Java;program testing;search problems","random search based effective algorithm;pairwise test data generation;pairwise interaction;software fault detection;R2Way algorithm;Java program;software testing","","1","17","","","","","","IEEE","IEEE Conferences"
"Generating Test Data for Structural Testing Based on Ant Colony Optimization","C. Mao; X. Yu; J. Chen; J. Chen","NA; NA; NA; NA","2012 12th International Conference on Quality Software","","2012","","","98","101","Software testing has been always viewed as an effective way to ensure software quality both in academic and industry. In fact, the quality of test data set plays a critical role in the success of software testing activity. According to the basic line of search-based software testing, we introduced ant colony optimization (ACO) to settle this problem and proposed a framework of ACO-based test data generation. In our algorithm TDG_ACO, the local transfer rule, global transfer rule and pheromone update rule are re-defined to handle the continuous input domain searching. Meanwhile, the most widely-used coverage criterion, i.e., branch coverage, is adopted to construct fitness function. In order to validate the feasibility and effectiveness of our method, five real-world programs are utilized to perform experimental analysis. The results show that our algorithm outperforms the existing simulated annealing and genetic algorithm in most cases.","2332-662X;1550-6002;1550-6002","978-1-4673-2857-9978-0-7695-4833","10.1109/QSIC.2012.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6319230","Test data generation;ant colony optimization;branch coverage;fitness function;meta-heuristic search","Software testing;Measurement;Educational institutions;Software algorithms;Genetic algorithms;Algorithm design and analysis","ant colony optimisation;data handling;program testing;software quality","test data generation;structural testing;ant colony optimization;software testing;software quality;ACO;local transfer rule;global transfer rule;branch coverage","","3","11","","","","","","IEEE","IEEE Conferences"
"Towards Impact Analysis of Test Goal Prioritization on the Efficient Execution of Automatically Generated Test Suites Based on State Machines","S. Weißleder","NA","2011 11th International Conference on Quality Software","","2011","","","150","155","Test prioritization aims at reducing test execution costs. There are several approaches to prioritize test cases based on collected data of previous test runs, e.g., in regression testing. In this paper, we present a new approach to test prioritization for efficient test execution that is focused on the artifacts used in model-based test generation from state machines. We propose heuristics for test goal prioritizations and evaluate them using two different test models. Our finding is that the prioritizations can have a positive impacton the test execution efficiency. This impact, however, is hard to predict for a concrete situation. Thus, the question for the general gain of test goal prioritizations is still open.","2332-662X;1550-6002;1550-6002","978-1-4577-0754-4978-0-7695-4468","10.1109/QSIC.2011.38","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004322","Model-Based Testing;Coverage Criteria;Test Goal Prioritization;Test Execution Efficiency","Testing;Unified modeling language;Fault detection;Redundancy;Generators;Search problems;Couplings","automatic test pattern generation;finite state machines;regression analysis;Unified Modeling Language","test goal prioritization;automatically generated test suite;state machine;test execution cost reduction;regression testing;model-based test generation","","","19","","","","","","IEEE","IEEE Conferences"
"Software Reliability and Testing Time Allocation: An Architecture-Based Approach","R. Pietrantuono; S. Russo; K. S. Trivedi","Federico II University of Naples, Naples; Federico II University of Naples, Naples; Duke University, Durham","IEEE Transactions on Software Engineering","","2010","36","3","323","337","With software systems increasingly being employed in critical contexts, assuring high reliability levels for large, complex systems can incur huge verification costs. Existing standards usually assign predefined risk levels to components in the design phase, to provide some guidelines for the verification. It is a rough-grained assignment that does not consider the costs and does not provide sufficient modeling basis to let engineers quantitatively optimize resources usage. Software reliability allocation models partially address such issues, but they usually make so many assumptions on the input parameters that their application is difficult in practice. In this paper, we try to reduce this gap, proposing a reliability and testing resources allocation model that is able to provide solutions at various levels of detail, depending upon the information the engineer has about the system. The model aims to quantitatively identify the most critical components of software architecture in order to best assign the testing resources to them. A tool for the solution of the model is also developed. The model is applied to an empirical case study, a program developed for the European Space Agency, to verify model's prediction abilities and evaluate the impact of the parameter estimation errors on the prediction accuracy.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.6","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5383374","Reliability;software architecture;software testing.","Software reliability;Software testing;Reliability engineering;Predictive models;Software systems;Guidelines;Cost function;Application software;System testing;Resource management","program testing;software architecture;software reliability","software reliability;testing time allocation;architecture-based approach;rough-grained assignment","","41","40","","","","","","IEEE","IEEE Journals & Magazines"
"A Theoretical and Empirical Study of Search-Based Testing: Local, Global, and Hybrid Search","M. Harman; P. McMinn","King's College London, London; University of Sheffield, Sheffield","IEEE Transactions on Software Engineering","","2010","36","2","226","247","Search-based optimization techniques have been applied to structural software test data generation since 1992, with a recent upsurge in interest and activity within this area. However, despite the large number of recent studies on the applicability of different search-based optimization approaches, there has been very little theoretical analysis of the types of testing problem for which these techniques are well suited. There are also few empirical studies that present results for larger programs. This paper presents a theoretical exploration of the most widely studied approach, the global search technique embodied by Genetic Algorithms. It also presents results from a large empirical study that compares the behavior of both global and local search-based optimization on real-world programs. The results of this study reveal that cases exist of test data generation problem that suit each algorithm, thereby suggesting that a hybrid global-local search (a Memetic Algorithm) may be appropriate. The paper presents a Memetic Algorithm along with further empirical results studying its performance.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.71","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5342440","Automated test data generation;search-based testing;search-based software engineering;Evolutionary Testing;Genetic Algorithms;Hill Climbing;schema theory;Royal Road;testing and debugging;testing tools;artificial intelligence;problem solving;control methods;and search;heuristic methods;algorithms;experimentation;measurement;performance;theory.","Software testing;Automatic testing;Genetic algorithms;Costs;Hybrid power systems;Software engineering;Automation;Stress;Debugging;Artificial intelligence","automatic test software;genetic algorithms;program testing;search problems","search based testing;search based optimization techniques;structural software test data generation;genetic algorithms;hybrid global-local search problem;memetic algorithm;real-world programs","","147","61","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic Test Data Generation for Software Path Testing Using Evolutionary Algorithms","G. I. Latiu; O. A. Cret; L. Vacariu","NA; NA; NA","2012 Third International Conference on Emerging Intelligent Data and Web Technologies","","2012","","","1","8","Software testing is a very expensive and time consuming process. Test methods which generate test data based on the program's internal structure are intensively used. This paper presents a comparison between three important Evolutionary Algorithms used for automatic test data generation, a technique that forces the execution of a desired path of the program called target path. Two new approaches, based on Particle Swarm Optimization and Simulated Annealing algorithms, used in conjunction with the approximation level and branch distance metrics, are compared with Genetic Algorithms for generating test data. The results obtained based on the proposed approaches suggest that evolutionary testing strategies are very well suited to generate test data which cover a target path inside a software program.","","978-1-4673-1986-7978-0-7695-4734","10.1109/EIDWT.2012.25","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6354714","software testing;evolutionary algorithms;path testing","Evolutionary computation;Software;Sociology;Statistics;Approximation methods;Software testing","approximation theory;evolutionary computation;particle swarm optimisation;program testing;simulated annealing;software metrics","automatic test data generation;software path testing;evolutionary algorithms;target path;particle swarm optimization;simulated annealing algorithms;approximation level;branch distance metrics;evolutionary testing strategies;software program","","6","30","","","","","","IEEE","IEEE Conferences"
"Novel composition test functions algorithm for numerical optimization","Fu-ming Peng","College of Computer and Software, Nanjing Institute of Industry Technology, China","2011 International Conference on Computer Science and Service System (CSSS)","","2011","","","3348","3352","Since coming out, novel composition test functions have received wide attention from evolutionary computation researchers and have now become the target functions for numerical optimization algorithms. However, its numerical optimization can be transformed into numerical optimization of one-dimensional functions, which significantly reduces optimization level of difficulty. A novel composition test functions algorithm for numerical optimization is proposed, which quotes a muti-population revolutionary algorithm for numerical optimization and uses it to optimize the one-dimensional functions. The experiments proved the algorithm for numerical optimization of novel composition test functions converges to the global optimal solutions.","","978-1-4244-9763-8978-1-4244-9762-1978-1-4244-9761","10.1109/CSSS.2011.5974523","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5974523","novel composition test functions;dimension reduction;numerical optimization;one-dimensional function;multi-dimensions function","Optimization;Computers;Software;Software algorithms;Particle swarm optimization;Information technology","evolutionary computation;optimisation","composition test functions algorithm;evolutionary computation researchers;numerical optimization algorithms;one-dimensional functions;mutipopulation evolutionary algorithm","","","","","","","","","IEEE","IEEE Conferences"
"The research of test-suite reduction technique","C. Donghua; Y. Wenjie","College of Computer &amp; Software Taiyuan University of Technology, Taiyuan, China; College of Computer &amp; Software, Taiyuan University of Technology, Taiyuan, China","2011 International Conference on Consumer Electronics, Communications and Networks (CECNet)","","2011","","","4552","4554","Ant colony algorithm is a bionic optimization algorithm, it can solve combinatorial problems effectively. For the problem of the test suite reduction, this algorithm could find the balance point between the speed and the accuracy of solution. Unlike other existing algorithms, this algorithm used test cost criteria, as well as the test coverage criteria. Finally, the paper presented the results, the results is given by the others classical algorithms compared with this algorithms. The results show that this algorithm can significantly reduce the size and the cost of the test-suite, and achieved higher effectiveness of test-suite minimization.","","978-1-61284-459-6978-1-61284-458-9978-1-61284-457","10.1109/CECNET.2011.5768284","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5768284","test suite;test suite reduction;test coverage;test execution cost","Software algorithms;Software;Heuristic algorithms;Algorithm design and analysis;Optimization;Minimization;Computers","minimisation;optimisation;program testing","test suite reduction technique;ant colony algorithm;bionic optimization algorithm;test cost criteria;test coverage criteria;test suite minimization","","2","7","","","","","","IEEE","IEEE Conferences"
"Applying Particle Swarm Optimization to Pairwise Testing","X. Chen; Q. Gu; J. Qi; D. Chen","NA; NA; NA; NA","2010 IEEE 34th Annual Computer Software and Applications Conference","","2010","","","107","116","Combinatorial testing (also called interaction testing) is an effective specification-based test input generation technique. By now most of research work in combinatorial testing aims to propose novel approaches trying to generate test suites with minimum size that still cover all the pairwise, triple, or n-way combinations of factors. Since the difficulty of solving this problem is demonstrated to be NP-hard, existing approaches have been designed to generate optimal or near optimal combinatorial test suites in polynomial time. In this paper, we try to apply particle swarm optimization (PSO), a kind of meta-heuristic search technique, to pairwise testing (i.e. a special case of combinatorial testing aiming to cover all the pairwise combinations). To systematically build pairwise test suites, we propose two different PSO based algorithms. One algorithm is based on one-test-at-a-time strategy and the other is based on IPO-like strategy. In these two different algorithms, we use PSO to complete the construction of a single test. To successfully apply PSO to cover more uncovered pairwise combinations in this construction process, we provide a detailed description on how to formulate the search space, define the fitness function and set some heuristic settings. To verify the effectiveness of our approach, we implement these algorithms and choose some typical inputs. In our empirical study, we analyze the impact factors of our approach and compare our approach to other well-known approaches. Final empirical results show the effectiveness and efficiency of our approach.","0730-3157;0730-3157;0730-3157","978-1-4244-7513-1978-1-4244-7512-4978-0-7695-4085","10.1109/COMPSAC.2010.17","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676343","software testing;pairwise testing;meta-heuristic search techniques;particle swarm optimization","Arrays;Particle swarm optimization;Software testing;Software;Optimization;Software engineering","particle swarm optimisation;program debugging;program testing;search problems","pairwise testing;specification based test input generation technique;combinatorial testing;NP-hard;particle swarm optimization;meta-heuristic search technique","","21","27","","","","","","IEEE","IEEE Conferences"
"Regression test cases prioritization using Failure Pursuit Sampling","C. Simons; E. C. Paraiso","Postgraduate Program in Informatics, Pontifical Catholic University of Parana, Curitiba, Brazil; Postgraduate Program in Informatics, Pontifical Catholic University of Parana, Curitiba, Brazil","2010 10th International Conference on Intelligent Systems Design and Applications","","2010","","","923","928","The necessity of lowering the execution of system tests' cost is a consensual point in the software development community. The present study presents an optimization of the regression tests' activity, by adapting a test cases prioritization technique called Failure Pursuit Sampling-previously used and validated for the prioritization of tests in general-improving its efficiency for the exclusive execution of regression test. For this purpose, the clustering and sampling phases of the original technique were modified, so that it becomes capable of receive information from tests made on the previous version of a program, and can use this information to drive de efficiency of the new developed technique, for tests made on a present version. The adapted technique was implemented and executed using the Schedule program, of the Siemens suit. By using Average of the Percentage of Faults Detected charts, the modified Failure Pursuit Sampling technique presented a high level of efficiency improvement.","2164-7143;2164-7151","978-1-4244-8136-1978-1-4244-8134-7978-1-4244-8135","10.1109/ISDA.2010.5687069","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5687069","regression tests;test case prioritization;failure pursuit sampling;clustering","Nearest neighbor searches;Schedules;Software;Fault detection;Optimization;Intelligent systems;Clustering algorithms","optimisation;program testing;regression analysis;sampling methods;software development management","regression test cases prioritization;failure pursuit sampling;system test cost;software development community;optimization;regression tests;Siemens suit","","1","26","","","","","","IEEE","IEEE Conferences"
"Test case prioritization incorporating ordered sequence of program elements","K. Wu; C. Fang; Z. Chen; Z. Zhao","State Key Laboratory for Novel Software Technology, Nanjing University, 210093, China; State Key Laboratory for Novel Software Technology, Nanjing University, 210093, China; State Key Laboratory for Novel Software Technology, Nanjing University, 210093, China; State Key Laboratory for Novel Software Technology, Nanjing University, 210093, China","2012 7th International Workshop on Automation of Software Test (AST)","","2012","","","124","130","Test suites often grow very large over many releases, such that it is impractical to re-execute all test cases within limited resources. Test case prioritization, which rearranges test cases, is a key technique to improve regression testing. Code coverage information has been widely used in test case prioritization. However, other important information, such as the ordered sequence of program elements measured by execution frequencies, was ignored by previous studies. It raises a risk to lose detections of difficult-to-find bugs. Therefore, this paper improves the similarity-based test case prioritization using the ordered sequence of program elements measured by execution counts. The empirical results show that our new technique can increase the rate of fault detection more significantly than the coverage-based ART technique. Moreover, our technique can detect bugs in loops more quickly and be more cost-benefits than the traditional ones.","","978-1-4673-1822-8978-1-4673-1821","10.1109/IWAST.2012.6228980","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6228980","Test case prioritization;similarity;ordered sequence;edit distance;farthest-first algorithm","Computer bugs;Subspace constraints","fault diagnosis;program testing;regression analysis","program elements;test suites;regression testing;code coverage information;similarity-based test case prioritization;ordered sequence;fault detection","","","26","","","","","","IEEE","IEEE Conferences"
"Reference-Based Testing Technique for Automated Test Generation","E. Kim; S. Ryoo","NA; NA","2010 International Conference on Computational Science and Its Applications","","2010","","","239","243","This paper introduces a technique to generate tests automatically for the legacy code by utilizing existing software assets. Many aspects of the legacy code needs to be tested, such as, classes, methods, interfaces, relations, dependencies, etc. The proposed technique automatically generates tests by examining software packages that use the system under test to be tested. It presents an automatic test method based on searching calls to the APIs being tested in available repositories, ranking the relevance of the mined calls, and using them to generate test cases. It makes three important contributions for the issue. First, it provides an automatic way to build a test skeleton. Second, it finds previously unknown bugs and obviously unused dead codes by achieving high test coverage. Finally, developers confidently make changes to the codes written by someone else.","","978-1-4244-6462-3978-1-4244-6461-6978-0-7695-3999","10.1109/ICCSA.2010.58","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5476639","automated testing;test generation;unit testing;legacy code","Automatic testing;Software testing;System testing;Application software;Software packages;Open source software;Cities and towns;Computer bugs;Java;Humans","automatic test pattern generation;program testing;software maintenance","heuristic reference-based testing technique;automated test generation;legacy code;software assets;software packages;API","","","25","","","","","","IEEE","IEEE Conferences"
"Incremental Test Generation for Software Product Lines","E. Uzuncaova; S. Khurshid; D. Batory","Microsoft, Redmond; University of Texas at Austin, Austin; University of Texas at Austin, Austin","IEEE Transactions on Software Engineering","","2010","36","3","309","322","Recent advances in mechanical techniques for systematic testing have increased our ability to automatically find subtle bugs, and hence, to deploy more dependable software. This paper builds on one such systematic technique, scope-bounded testing, to develop a novel specification-based approach for efficiently generating tests for products in a software product line. Given properties of features as first-order logic formulas in Alloy, our approach uses SAT-based analysis to automatically generate test inputs for each product in a product line. To ensure soundness of generation, we introduce an automatic technique for mapping a formula that specifies a feature into a transformation that defines incremental refinement of test suites. Our experimental results using different data structure product lines show that an incremental approach can provide an order of magnitude speedup over conventional techniques. We also present a further optimization using dedicated integer constraint solvers for feature properties that introduce integer constraints, and show how to use a combination of solvers in tandem for solving Alloy formulas.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.30","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5456077","Software/program verification;testing and debugging;software engineering.","Software testing;Automatic testing;System testing;Computer bugs;Logic testing;Data structures;Software quality;Automatic logic units;Acoustic testing;Constraint optimization","computability;data structures;program testing;program verification","incremental test generation;software product lines;mechanical techniques;systematic testing;scope-bounded testing;specification-based approach;first-order logic formulas;SAT-based analysis;data structure product lines;dedicated integer constraint solvers;Alloy formulas;program verification","","31","60","","","","","","IEEE","IEEE Journals & Magazines"
"Genetic Algorithms for Randomized Unit Testing","J. H. Andrews; T. Menzies; F. C. H. Li","University of Western Ontario, London, Ont., Canada; West Virginia University, Morgantown, WV, USA; University of Western Ontario, London, Ont., Canada","IEEE Transactions on Software Engineering","","2011","37","1","80","94","Randomized testing is an effective method for testing software units. The thoroughness of randomized unit testing varies widely according to the settings of certain parameters, such as the relative frequencies with which methods are called. In this paper, we describe Nighthawk, a system which uses a genetic algorithm (GA) to find parameters for randomized unit testing that optimize test coverage. Designing GAs is somewhat of a black art. We therefore use a feature subset selection (FSS) tool to assess the size and content of the representations within the GA. Using that tool, we can reduce the size of the representation substantially while still achieving most of the coverage found using the full representation. Our reduced GA achieves almost the same results as the full system, but in only 10 percent of the time. These results suggest that FSS could significantly optimize metaheuristic search-based software engineering tools.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.46","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5704237","Software testing;randomized testing;genetic algorithms;feature subset selection;search-based optimization;testing tools.","Testing;Biological cells;Gallium;Receivers;Software;Java;Optimization","feature extraction;genetic algorithms;program testing;randomised algorithms;search problems;software engineering","genetic algorithm;randomized unit testing;relative frequency;Nighthawk;optimized test coverage;feature subset selection tool;metaheuristic search;software engineering tool;software testing","","32","49","","","","","","IEEE","IEEE Journals & Magazines"
"Change Sensitivity Based Prioritization for Audit Testing of Webservice Compositions","C. D. Nguyen; A. Marchetto; P. Tonella","NA; NA; NA","2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops","","2011","","","357","365","Modern software systems have often the form of Web service compositions. They take advantage of the availability of a variety of external Web services to provide rich and complex functionalities, obtained as the integration of external services. However, Web services change at a fast pace and while syntactic changes are easily detected as interface incompatibilities, other more subtle changes are harder to detect and may give raise to faults. They occur when the interface is compatible with the composition, but the semantics of the service response has changed. This typically involves undocumented or implicit aspects of the service interface. Audit testing of services is the process by which the service integrator makes sure that the service composition continues to work properly with the new versions of the integrated services. Audit testing of services is conducted under strict (sometimes extreme) time and budget constraints. Hence, prioritizing the audit test cases so as to execute the most important ones first becomes of fundamental importance. We propose a test case prioritization method specifically tailored for audit testing of services. Our method is based on the idea that the most important test cases are those that have the highest sensitivity to changes injected into the service responses (mutations). In particular, we consider only changes that do not violate the explicit contract with the service (i.e., the WSDL), but may violate the implicit assumptions made by the service integrator.","","978-0-7695-4345-1978-1-4577-0019","10.1109/ICSTW.2011.50","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954434","Audit Testing;Change Sensitivity;Test Prioritization;Webservice Composition","Testing;Sensitivity;XML;Web services;Monitoring;Semantics;Syntactics","program testing;Web services","change sensitivity based prioritization;audit testing;Web service composition;software system;test case prioritization method;WSDL;service integrator;service interface","","4","24","","","","","","IEEE","IEEE Conferences"
"Search-Based Software Testing: Past, Present and Future","P. McMinn","NA","2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops","","2011","","","153","163","Search-Based Software Testing is the use of a meta-heuristic optimizing search technique, such as a Genetic Algorithm, to automate or partially automate a testing task, for example the automatic generation of test data. Key to the optimization process is a problem-specific fitness function. The role of the fitness function is to guide the search to good solutions from a potentially infinite search space, within a practical time limit. Work on Search-Based Software Testing dates back to 1976, with interest in the area beginning to gather pace in the 1990s. More recently there has been an explosion of the amount of work. This paper reviews past work and the current state of the art, and discusses potential future research areas and open problems that remain in the field.","","978-0-7695-4345-1978-1-4577-0019","10.1109/ICSTW.2011.100","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954405","Search-Based Software Testing;Search-Based Software Engineering","Software testing;Optimization;Search problems;Software;Genetic algorithms;Databases","genetic algorithms;program testing;search problems","search-based software testing;metaheuristic optimizing search technique;genetic algorithm;automatic test data generation;optimization process;problem-specific fitness function;infinite search space","","51","58","","","","","","IEEE","IEEE Conferences"
"A Software Regression Testing Strategy Based on Bayesian Network","Z. Fang; H. Sun","NA; NA","2010 International Conference on Computational Intelligence and Software Engineering","","2010","","","1","4","The purpose of regression testing is to assure quality of the changed system not only due to fixing bugs but also software changes by re-execution a suitable test cases set which determines the quality of such activity. Because of the software complexity and other reasons, there are many uncertainties on selecting regression test case set. In this paper, we present a solution by applying Bayesian Network. Appling the Bayesian Network, model the system requirements and test cases, and define the dependent relationship of requirements. Attain the test suite of regression testing optimized by this way. And the Bayesian Network can be changed by learning constantly in order to optimize the test suite again and again.","","978-1-4244-5391-7978-1-4244-5392","10.1109/CISE.2010.5676806","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676806","","Bayesian methods;Software;Uncertainty;Software testing;Software engineering;Maintenance engineering","Bayes methods;belief networks;program testing;regression analysis;software quality","software regression testing strategy;Bayesian network;quality assurance;software complexity;regression test case set;system requirements","","","8","","","","","","IEEE","IEEE Conferences"
"The Effectiveness of Regression Testing Techniques in Reducing the Occurrence of Residual Defects","P. Nagahawatte; H. Do","NA; NA","2010 Third International Conference on Software Testing, Verification and Validation","","2010","","","79","88","Regression testing is a necessary maintenance activity that can ensure high quality of the modified software system, and a great deal of research on regression testing has been performed. Most of the studies performed to date, however, have evaluated regression testing techniques under the limited context, such as a short-term assessment, which do not fully account for system evolution or industrial circumstances. One important issue associated with a system lifetime view that we have overlooked in past years is the effects of residual defects - defects that persist undetected - across several releases of a system. Depending on an organization's business goals and the type of system being built, residual defects might affect the level of success of the software products. In this paper, we conducted an empirical study to investigate whether regression testing techniques are effective in reducing the occurrence and persistence of residual defects across a system's lifetime, in particular, considering test case prioritization techniques. Our results show that heuristics can be effective in reducing both the occurrence of residual defects and their age. Our results also indicate that residual defects and their age have a strong impact on the cost-benefits of test case prioritization techniques.","2159-4848","978-1-4244-6436-4978-1-4244-6435-7978-0-7695-3990","10.1109/ICST.2010.57","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477096","Regression testing;test case prioritization;residual defects;empirical study","Testing","program testing;software maintenance;software quality","regression testing;residual defects;software maintenance;software quality;modified software system;short-term assessment;test case prioritization","","2","31","","","","","","IEEE","IEEE Conferences"
"An approach to automatic generating test data for multi-path coverage by genetic algorithm","J. h. Sun; S. j. Jiang","School of Computer Science and Technology, China University of Mining and Technology, Xuzhou, China; School of Computer Science and Technology, China University of Mining and Technology, Xuzhou, China","2010 Sixth International Conference on Natural Computation","","2010","3","","1533","1536","Software test is an important step during software development. Improving the automation of software testing can increase the robustness of software and decrease the cost of development. The key of improving the automation ability of testing is improving the automatic test data generation. The paper presents a new method to generate test data for multi-path coverage. The method uses a two-dimensional matrix to record the paths whether the test data passed, and improve the fitness function to suit the new method. In the end, it presents a case study to evaluate the efficient of proposes an efficiency of the method.","2157-9555;2157-9563","978-1-4244-5961-2978-1-4244-5958-2978-1-4244-5959","10.1109/ICNC.2010.5583778","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5583778","software test;genetic algorithm;path coverage","Biological cells;Encoding;Software testing;Software;Input variables;Optimization;Algorithm design and analysis","automatic test software;genetic algorithms;program testing;software cost estimation","multipath coverage;genetic algorithm;software development cost;software testing;automatic test data generation;fitness function","","1","9","","","","","","IEEE","IEEE Conferences"
"Software testing suite prioritization using multi-criteria fitness function","A. A. Ahmed; M. Shaheen; E. Kosba","Computer Engineering Department., Alexandria High Institute of Engineering and Technology (A.I.E.T), Alexandria, Egypt; College of Computing and Information Technology, Arab Academy for Science and Technology &amp; Maritime Transport., Alexandria, Egypt; College of Computing and Information Technology, Arab Academy for Science and Technology &amp; Maritime Transport., Alexandria, Egypt","2012 22nd International Conference on Computer Theory and Applications (ICCTA)","","2012","","","160","166","Regression testing is the process of validating modifications introduced in a system during software maintenance. It is an expensive, yet an important process. As the test suite size is very large, system retesting consumes large amount of time and computing resources. Unfortunately, there may be insufficient resources to allow for the re-execution of all test cases during regression testing. Testcase prioritization techniques aim to improve the effectiveness of regression testing, by ordering the testcases so that the most beneficial are executed first with higher priority. The objective of test case prioritization is to detect faults as early as possible. An approach for automating the test case prioritization process using genetic algorithm with Multi-Criteria Fitness function is presented. It uses multiple control flow coverage metrics. These metrics measure the degree of coverage of conditions, multiple conditions and statements that the test case covers. Theses metrics are weighted by the number of faults revealed and their severity. The proposed Multi-criteria technique showed superior results compared to similar work.","","978-1-4673-2824-1978-1-4673-2823-4978-1-4673-2822","10.1109/ICCTA.2012.6523563","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6523563","Genetic algorithm;Regression testing;Testcase prioritization","","genetic algorithms;program testing;regression analysis;software maintenance","software testing suite prioritization;multicriteria fitness function;regression testing;modification validation;software maintenance;computing resource;test case prioritization process;genetic algorithm;multiple control flow coverage metrics;software fault","","5","14","","","","","","IEEE","IEEE Conferences"
"Research of Optimization Algorithm for Path-Based Regression Testing Suit","B. Jiang; Y. Mu; Z. Zhang","NA; NA; NA","2010 Second International Workshop on Education Technology and Computer Science","","2010","2","","303","306","A test suit need be reused in regressing test, which may contain repetitive and redundant test cases. To decrease the cost of maintaining the test suite and executing test cases in regression testing, according to selective regression testing, we select the test cases that can test part of changes, and then do the reduction for these selected test cases. Based on the requirements of static path coverage testing, this article proposed an algorithm for test suite optimization. According to the algorithm, we can find a change point with the minimum set of using cases for regression testing, and give application examples. The results show that, the algorithms can significantly reduce the size and the cost of the test suite for regression testing, and achieve good cost effectiveness.","","978-1-4244-6389-3978-1-4244-6388","10.1109/ETCS.2010.365","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5460314","Regression testing;test suite optimization;Static path;the changed point","Software testing;System testing;Educational technology;Costs;Information science;Laboratories;Personnel;Computer science education;Computer science","optimisation;program testing;regression analysis","optimization algorithm;path-based regression testing suit;regression testing","","9","9","","","","","","IEEE","IEEE Conferences"
"Applications of Optimization to Logic Testing","G. Kaminski; P. Ammann","NA; NA","2010 Third International Conference on Software Testing, Verification, and Validation Workshops","","2010","","","331","336","A tradeoff exists in software logic testing between test set size and fault detection. Testers may want to minimize test set size subject to guaranteeing fault detection or they may want to maximize faults detection subject to a test set size. One way to guarantee fault detection is to use heuristics to produce tests that satisfy logic criteria. Some logic criteria have the property that they are satisfied by a test set if detection of certain faults is guaranteed by that test set. An empirical study is conducted to compare test set size and computation time for heuristics and optimization for various faults and criteria. The results show that optimization is a better choice for applications where each test has significant cost, because for a small difference in computation time, optimization reduces test set size. A second empirical study examined the percentage of faults detected in a best, random, and worst case, first for a test set size of one and then again for a test set size of ten. This study showed that if you have a limited number of tests from which to choose, the exact tests you choose have a large impact on fault detection.","","978-1-4244-6774-7978-1-4244-6773-0978-0-7695-4050","10.1109/ICSTW.2010.49","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463667","Software Logic Testing;Logic Criteria;MUMCUT;Disjunctive Normal Form","Logic testing;Fault detection;Software testing;Boolean functions;Application software;Computer science;Cost function;Terminology","fault tolerant computing;logic programming;optimisation;program testing","optimization application;software logic testing;fault detection;computation time reduction;test set size reduction","","1","5","","","","","","IEEE","IEEE Conferences"
"A software safety test approach based on FTA and Bayesian networks","X. He; X. Tao","Dept. of Quality Engineering, China Aero-Polytechnology Establishment, Aviation Industry Corporation of China, Beijing, China; Dept. of Aerospace Application, Academy of Opto-Electronics, Chinese Academy of Sciences, Beijing, China","2011 Prognostics and System Health Managment Confernece","","2011","","","1","5","As an important way to verify software safety, software safety test has caught more attentions in practice. However, it is still an open question that how engineers could make software safety test more efficient. Currently, FTA based method is one of the approaches in software safety test, but it can not utilize the finished software test results, and can not be determined the priorities of all the use cases. In order to solve these problems, this paper gives a quantitative approach of software safety test based on FTA and Bayesian networks. In the approach, top-level events of fault trees are identified from system hazards firstly. Then, fault trees are built using FTA and transferred into Bayesian networks. Finally, test cases of software safety test are determined by the Bayesian networks. Besides, the paper also shows an example using the approach, which could guide software engineers to make software safety test more efficient. The example shows that the approach could take advantage of Bayesian Theorem and FTA methodology together, and give reasonable priorities of use cases in software safety test.","2166-563X;2166-5656","978-1-4244-7950-4978-1-4244-7951-1978-1-4244-7949","10.1109/PHM.2011.5939497","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5939497","software safety test;FTA;Bayesian networks","Hazards;Bayesian methods;Software algorithms;Software","Bayes methods;belief networks;fault trees;program testing;program verification;software engineering","software safety test;FTA network;Bayesian network;top level event;fault trees;system hazard;software engineers","","1","15","","","","","","IEEE","IEEE Conferences"
"Test Case Prioritization Due to Database Changes in Web Applications","D. Garg; A. Datta","NA; NA","2012 IEEE Fifth International Conference on Software Testing, Verification and Validation","","2012","","","726","730","A regression test case prioritization (TCP) technique reorders test cases for regression testing to achieve early fault detection. Most TCP techniques have been developed for regression testing of source code in an application. Most web applications rely on a database server for serving client requests. Any changes in the database result in erroneous client interactions and may bring down the entire web application. However, most prioritization techniques are unsuitable for prioritizing test suites for early detection of changes in databases. There are very few proposals in the literature for prioritization of test cases that can detect faults in the database early. We propose a new automated TCP technique for web applications that automatically identifies the database changes, prioritizes test cases related to database changes and executes them in priority order to detect faults early.","2159-4848","978-0-7695-4670-4978-1-4577-1906","10.1109/ICST.2012.163","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200175","Regression testing;Test case prioritization;Database schema;Functional dependency graph","Databases;Testing;Software;Conferences;Unified modeling language;Editorials;Fault detection","database management systems;Internet;regression analysis;statistical testing","database change;regression test case prioritization technique;regression testing;fault detection;Web application;client request;client interaction","","2","23","","","","","","IEEE","IEEE Conferences"
"Generating Feasible Test Paths from an Executable Model Using a Multi-objective Approach","T. Yano; E. Martins; F. L. de Sousa","NA; NA; NA","2010 Third International Conference on Software Testing, Verification, and Validation Workshops","","2010","","","236","239","Search-based testing techniques using meta-heuristics, like evolutionary algorithms, has been largely used for test data generation, but most approaches were proposed for white-box testing. In this paper we present an evolutionary approach for test sequence generation from a behavior model, in particular, Extended Finite State Machine. An open problem is the production of infeasible paths, as these should be detected and discarded manually. To circumvent this problem, we use an executable model to obtain feasible paths dynamically. An evolutionary algorithm is used to search for solutions that cover a given test purpose, which is a transition of interest. The target transition is used as a criterion to get slicing information, in this way, helping to identify the parts of the model that affect the test purpose. We also present a multi-objective search: the test purpose coverage and the sequence size minimization, as longer sequences require more effort to be executed.","","978-1-4244-6774-7978-1-4244-6773-0978-0-7695-4050","10.1109/ICSTW.2010.52","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463651","model-based testing;feasible path;executable model;multi-objective optimization","Software testing;Evolutionary computation;Automata;Pareto optimization;Production;Computer languages;Reachability analysis;Information analysis;Prototypes;Instruments","evolutionary computation;finite state machines;program testing","generating feasible test paths;executable model;multiobjective approach;search based testing techniques;evolutionary algorithms;white box testing;sequence generation;finite state machine;slicing information;multiobjective search","","11","13","","","","","","IEEE","IEEE Conferences"
"G-RankTest: Regression testing of controller applications","L. Mariani; O. Riganelli; M. Santoro; M. Ali","University of Milano Bicocca, Viale Sarca, 336, Italy; University of Milano Bicocca, Viale Sarca, 336, Italy; University of Milano Bicocca, Viale Sarca, 336, Italy; VTT Technical Research Centre of Finland, Tekniikankatu 1, FI-33101, Tampere, Finland","2012 7th International Workshop on Automation of Software Test (AST)","","2012","","","131","137","Since controller applications must typically satisfy real-time constraints while manipulating real-world variables, their implementation often results in programs that run extremely fast and manipulate numerical inputs and outputs. These characteristics make them particularly suitable for test case generation. In fact a number of test cases can be easily created, due to the simplicity of numerical inputs, and executed, due to the speed of computations. In this paper we present G-RankTest, a technique for test case generation and prioritization. The key idea is that test case generation can run for long sessions (e.g., days) to accurately sample the behavior of a controller application and then the generated test cases can be prioritized according to different strategies, and used for regression testing every time the application is modified. In this work we investigate the feasibility of using the gradient of the output as a criterion for selecting the test cases that activate the most tricky behaviors, which we expect easier to break when a change occurs, and thus deserve priority in regression testing.","","978-1-4673-1822-8978-1-4673-1821","10.1109/IWAST.2012.6228981","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6228981","regression testing;test case prioritization;test case generation;test automation","Testing;Control systems;Embedded software;Input variables;Coordinate measuring machines;Real time systems","control engineering computing;gradient methods;program testing;regression analysis;statistical testing","G-RankTest;regression testing;controller applications;variable manipulation;numerical input manipulation;numerical output manipulation;test case generation;test case prioritization;output gradient;test case selection criterion","","1","15","","","","","","IEEE","IEEE Conferences"
"A reliability improvement predictive approach to software testing with Bayesian method","C. Bai; C. Jiang; K. Cai","National Key Laboratory of Science and Technology on Holistic Control, Department of Automatic Control, Beihang University, Beijing 100191, China; National Key Laboratory of Science and Technology on Holistic Control, Department of Automatic Control, Beihang University, Beijing 100191, China; National Key Laboratory of Science and Technology on Holistic Control, Department of Automatic Control, Beihang University, Beijing 100191, China","Proceedings of the 29th Chinese Control Conference","","2010","","","6031","6036","The capability of improving software reliability is one of the main objectives of software testing. However, the previous testing methods did not pay much attention to how to improve software testing strategy based on software reliability improvement. The relationship between software testing and software reliability is very complex and this is mainly due to the complexity of software products and development processes. The software testing strategy with improving reliability on line needs to possess the ability to predict reliability. Model predictive control provides a good framework to improve predictive effect on line. However, one of the main issues in model predictive control is how to estimate the concern parameter. In this case, Bayesian method is used to estimate the concern parameter: reliability. This proposed reliability improvement predictive approach to software testing with Bayesian method can optimize test allocation scheme on line. The case study shows that it is not definitely true for a software testing method that can find more defects than others can get higher reliability. And the case study also shows that the proposed approach can get better result in the sense of improving reliability than random testing.","1934-1768;2161-2927","978-7-8946-3104-6978-1-4244-6263-6978-7-8946-3104","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5572883","Software Testing;Software Reliability;Model Predictive Control;Bayesian Method","Software;Software reliability;Software testing;Bayesian methods;Predictive models","predictive control;program testing;software process improvement;software reliability","reliability improvement predictive approach;software testing strategy;Bayesian method;software reliability;software development process;model predictive control;parameter estimation;test allocation scheme;random testing","","3","18","","","","","","IEEE","IEEE Conferences"
"Prioritization of test case scenarios derived from activity diagram using genetic algorithm","S. Sabharwal; R. Sibal; C. Sharma","Department of computer Science and IT, Netaji Subhas Institute of Technology, Delhi, India; Department of computer Science and IT, Netaji Subhas Institute of Technology, Delhi, India; Department of computer Science and IT, Netaji Subhas Institute of Technology, Delhi, India","2010 International Conference on Computer and Communication Technology (ICCCT)","","2010","","","481","485","Software testing involves identifying the test cases which discovers the errors in the program. However, the exhaustive testing is rarely impossible and very time consuming. In this paper, the software testing efficiency is optimized by identifying the critical path clusters. The test case scenarios are derived from the activity diagram and the testing efficiency is optimized by applying the genetic algorithm on the test data. The information flow metric is adopted in this work for calculating the information flow complexity associated with each node of activity diagram.","","978-1-4244-9034-9978-1-4244-9033-2978-1-4244-9032","10.1109/ICCCT.2010.5640479","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5640479","software testing;genetic algorithm;activity diagram;CFG","Biological cells;Gallium;Unified modeling language;Software testing;Software;Evolutionary computation","genetic algorithms;program testing","software testing;program error;critical path cluster;test case scenario;activity diagram;genetic algorithm;information flow metric;information flow complexity","","4","17","","","","","","IEEE","IEEE Conferences"
"Modular based multiple test case prioritization","N. Prakash; T. R. Rangaswamy","Department of Information Technology, B.S.A Crescent Engineering College, Chennai, India; Dean Academic, B.S.A Crescent Engineering College, Chennai, India","2012 IEEE International Conference on Computational Intelligence and Computing Research","","2012","","","1","7","Cost and time effective reliable test case prioritization technique is the need for present software industries. The test case prioritization for the entire program consumes more time and the selection of test case for entire software is also affecting the test performance. In order to alleviate the above problem a new methodology using modular based test case prioritization is proposed for regression testing. In this method the program is divided into multiple modules. The test cases corresponding to each module is prioritized first. In the second stage, the individual modular based prioritized test suites are combined together and further prioritized for the whole program. This method is verified for fault coverage and compared with overall program test case prioritization method. The proposed method is assessed using three standard applications namely University Students Monitoring System, Hospital Management System, and Industrial Process Operation System. The empirical studies show that the proposed algorithm is significantly performed well. The superiority of the proposed method is also highlighted.","","978-1-4673-1344-5978-1-4673-1342-1978-1-4673-1343","10.1109/ICCIC.2012.6510205","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6510205","Modular program;regression testing;test case prioritization","","program testing;regression analysis;statistical testing","industrial process operation system;hospital management system;university students monitoring system;program test case prioritization method;fault coverage;regression testing;test performance;program test;software industry;modular based test case prioritization","","","17","","","","","","IEEE","IEEE Conferences"
"Towards Fully Automated Test Management for Large Complex Systems","S. Eldh; J. Brandt; M. Street; H. Hansson; S. Punnekkat","NA; NA; NA; NA; NA","2010 Third International Conference on Software Testing, Verification and Validation","","2010","","","412","420","Development of large and complex software intensive systems with continuous builds typically generates large volumes of information with complex patterns and relations. Systematic and automated approaches are needed for efficient handling of such large quantities of data in a comprehensible way. In this paper we present an approach and tool enabling autonomous behavior in an automated test management tool to gain efficiency in concurrent software development and test. By capturing the required quality criteria in the test specifications and automating the test execution, test management can potentially be performed to a great extent without manual intervention. This work contributes towards a more autonomous behavior within a distributed remote test strategy based on metrics for decision making in automated testing. These metrics optimize management of fault corrections and retest, giving consideration to the impact of the identified weaknesses, such as fault-prone areas in software.","2159-4848","978-1-4244-6436-4978-1-4244-6435-7978-0-7695-3990","10.1109/ICST.2010.58","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477058","test management system;large complex systems;automation;efficiency;industrial system","Automatic testing;System testing;Software testing;Software systems;Software development management;Programming;Quality management;Performance evaluation;Decision making;Fault diagnosis","data handling;large-scale systems;program testing;software engineering;software metrics","fully automated test management;large complex systems;complex software intensive systems;concurrent software development;quality criteria;test specifications;distributed remote test strategy;decision making;automated testing","","3","29","","","","","","IEEE","IEEE Conferences"
"Research on the Application Technology of Unit Testing Based on Priority","F. Gao; G. Yuan; C. Zheng; C. Liu","NA; NA; NA; NA","2011 Fourth International Joint Conference on Computational Sciences and Optimization","","2011","","","997","1001","A testing strategy was proposed to deal with the parallelism of unit testing based on the priority of function. The influence of parameters, global and local variables on the proper realization of the control flow of function was analyzed in this paper. The quantitive mathematical model of the weight influencing factor was built, and a directed acycline graph with weight was generated by combining the function's control flow chart. Further more, the tested function's critical path was given by the algorithm based on AOE-net. After a test case was done, the hypo-critical path was chosen until the test criterion was guaranteed. The experiment results showed that with a scheduled test time among modules, the software test efficiency was improved achieving the objective of software quality assurance(SQA).","","978-1-4244-9712-6978-0-7695-4335","10.1109/CSO.2011.222","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5957825","unit testing;AOE-net;test case;critical path;critical function","Flow graphs;Complexity theory;Software testing;Algorithm design and analysis;Software quality","directed graphs;program testing;software quality","unit testing strategy;function priority;weight influencing factor;directed acyclic graph;function control flow chart;tested function critical path;AOE-net;hypo-critical path;software test efficiency;software quality assurance","","","10","","","","","","IEEE","IEEE Conferences"
"Optimization of testing time in RUP: A group testing approach","M. Kundi; B. Ahmad; M. Inaythulaha; Jamal Abdul Nasir","Abdul Wali Khan University, Mardan, Pakistan; ICIT Gomal University, D.I.Khan, Pakistan; University of Engineering &amp;Technology, Peshawer, Pakistan; ICIT Gomal University, D.I.Khan, Pakistan","2012 International Conference on Computer & Information Science (ICCIS)","","2012","2","","848","851","Testing is the most important effort consuming the step of software development life cycle. Mostly the testing begins in the last stages of development and schedule slippage is adjusted with little testing. In Rational Unified Process (RUP), testing is performed throughout during development process. Test is one of the discipline of Rational Unified Process (RUP) among its nine disciplines where testing is performed in all four phases of RUP. In Construction phase the component are tested separately and then move to Transition phase, where beta testing take place. Testing an individual object is a time consuming approach of Rational Unified Process. The concept of group testing is introduced as a sub-discipline of test discipline. In proposed approach a group of similar components are tested together in group form to minimize the testing time. All though testing is involved from very beginning in RUP in order to develop error free component. But still a mechanism to test individual component takes a lot of time. The proposed model saves testing time when accurate developed components are tested in group form.","","978-1-4673-1938-6978-1-4673-1937-9978-1-4673-1936","10.1109/ICCISci.2012.6297144","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6297144","Software Development;Test;RUP Discipline;Testing Time;Use Case Diagram","Unified modeling language","program testing;software development management","testing time optimization;RUP;group testing approach;software development life cycle;schedule slippage;rational unified process;construction phase;transition phase;beta testing","","","8","","","","","","IEEE","IEEE Conferences"
"Value-Based Coverage Measurement in Requirements-Based Testing: Lessons Learned from an Approach Implemented in the TOSCA Testsuite","R. Ramler; T. Kopetzky; W. Platz","NA; NA; NA","2012 38th Euromicro Conference on Software Engineering and Advanced Applications","","2012","","","363","366","Testing is one of the most widely practiced quality assurance measures and also one of the most resource-intensive activities in software development. Still, however, most of the available methods, techniques and tools for software testing are value-neutral and do not realize the potential value contribution of testing. In this paper we present an approach for value-based coverage measurement that can be used to align the testing effort with the achievable value associated with requirements and functional units. It has been implemented as part of a commercial test tool and was successfully applied in real-world projects. The results demonstrated its ability to adequately capture the distribution of the business value and risks involved in different requirements. The paper concludes with sharing important lessons learned from developing value-based coverage measurement in the practical setting of commercial tool development and real-world test projects.","1089-6503;2376-9505","978-0-7695-4790-9978-1-4673-2451","10.1109/SEAA.2012.43","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6328176","Value-Based Software Testing;Test Case Prioritization;Coverage Measurement;Requirements Coverage","Software;Business;Software testing;Software engineering;Standards;Software measurement","program testing;software quality","value-based coverage measurement;requirements-based testing;TOSCA testsuite;quality assurance measures;software development;software testing;commercial test tool;business value;business risks;real-world test projects;commercial tool development","","1","10","","","","","","IEEE","IEEE Conferences"
"Prioritizing tests for fault localization through ambiguity group reduction","A. Gonzalez-Sanchez; R. Abreu; H. Gross; A. J. C. van Gemund","Delft University of Technology, Software Technology Department, Mekelweg 4, 2628 CD, The Netherlands; University of Porto, Departament of Informatics Engineering, Rua Dr. Roberto Frias, 4200-465, Portugal; Delft University of Technology, Software Technology Department, Mekelweg 4, 2628 CD, The Netherlands; Delft University of Technology, Software Technology Department, Mekelweg 4, 2628 CD, The Netherlands","2011 26th IEEE/ACM International Conference on Automated Software Engineering (ASE 2011)","","2011","","","83","92","In practically all development processes, regression tests are used to detect the presence of faults after a modification. If faults are detected, a fault localization algorithm can be used to reduce the manual inspection cost. However, while using test case prioritization to enhance the rate of fault detection of the test suite (e.g., statement coverage), the diagnostic information gain per test is not optimal, which results in needless inspection cost during diagnosis. We present RAPTOR, a test prioritization algorithm for fault localization, based on reducing the similarity between statement execution patterns as the testing progresses. Unlike previous diagnostic prioritization algorithms, RAPTOR does not require false negative information, and is much less complex. Experimental results from the Software Infrastructure Repository's benchmarks show that RAPTOR is the best technique under realistic conditions, with average cost reductions of 40% with respect to the next best technique, with negligible impact on fault detection capability.","1938-4300","978-1-4577-1639-3978-1-4577-1638","10.1109/ASE.2011.6100153","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6100153","","Inspection;Testing;Complexity theory;Software;Subspace constraints;Estimation;Bayesian methods","program testing;software reliability","ambiguity group reduction;regression tests;fault localization algorithm;RAPTOR;test prioritization algorithm;software infrastructure repository;software reliability","","14","46","","","","","","IEEE","IEEE Conferences"
"Optimal allocation of testing resource for modular software based on testing-effort dependent software reliability growth","N. Ahmad; M. G. M. Khan; Syed Faizul Islam","University Department of Statistics and Computer Applications, T. M. Bhagalpur University, 812007, India; School of Computing Information and Mathematical Sciences, The University of the South pacific, Suva, Fiji Island, Fiji; University Department of Statistics and Computer Applications, T. M. Bhagalpur University, 812007, India","2012 Third International Conference on Computing, Communication and Networking Technologies (ICCCNT'12)","","2012","","","1","7","Software reliability is a key factor in software development process. Testing phase of software begins with module testing whereby, modules are tested independently to remove substantial amount of faults within a specified testing resource. Therefore, the available resource must be allocated among the modules in such a way that number of faults is removed as much as possible from each of the module to achieve higher software reliability. In this paper two optimization problem are discussed for optimal allocation of testing resources for the modular software system. These optimization problems are formulated as nonlinear programming problems (NLPP), which are modeled by a software reliability growth model based on a non-homogeneous Poisson process which incorporated Log-logistic testing-effort function. LINGO program is used to solve the optimization problems. Finally, numerical examples are given to illustrate the procedure developed in this paper. It is shown that the optimal allocation of testing-resources among software modules can improve software reliability.","","","10.1109/ICCCNT.2012.6395885","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6395885","Software reliability growth model;resource allocation problem;nonlinear programming problem;inflection S-shaped models","Computational modeling;Numerical models;Reliability;Mathematical model;Resource management;Software;Equations","nonlinear programming;program testing;resource allocation;software reliability;stochastic processes","testing resource allocation;modular software;software reliability growth;software development process;software testing phase;optimization problem;modular software system;nonlinear programming problem;nonhomogeneous Poisson process;log-logistic testing-effort function;LINGO program","","1","26","","","","","","IEEE","IEEE Conferences"
"Research on Intelligent Test Paper Generation Based on Multi-Variable Asymptotic Optimization","J. Li; J. Li; J. Zhang; N. Ding","NA; NA; NA; NA","2010 2nd International Conference on E-business and Information System Security","","2010","","","1","5","The design of question database is an important part of digitized campus construction. In this article, the current problems which has been existed in present intelligent strategies of test paper generating was analyzed, a new model of intelligent test paper was proposed. A new strategy of intelligently generation of test paper was used in this thesis. In order to identify the questions, barcode was used in this test paper generation system. The parameters were selected in sequence based on its effect on the overall constraints in test paper. This strategy can optimize the search path, improve the efficiency and flexibility of intelligently generation of test paper. The software testing was made for different problems database, then the software testing result was analyzed briefly. Practical operations show that with this approach, the searching process was optimized, the cost of time was reduced and the rate of successfully generating test papers was improved.","2161-5942;2161-5977","978-1-4244-5895-0978-1-4244-5893","10.1109/EBISS.2010.5473501","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5473501","","Data engineering;Design engineering;System testing;Deductive databases;Software testing;Sections;Design optimization;Cost function;Optimization methods;Stability","bar codes;educational administrative data processing;optimisation;program testing","intelligent test paper generation;multi variable asymptotic optimization;barcode;software testing","","","11","","","","","","IEEE","IEEE Conferences"
"Increasing test coverage using human-based approach of fault injection testing","N. S. M. Yusop; W. F. Abbas; H. Haron; K. A. Kamaruddin","Faculty of Computer &amp; Mathematical Sciences, Universiti Teknologi Mara, Selangor, Malaysia; Faculty of Computer &amp; Mathematical Sciences, Universiti Teknologi Mara, Selangor, Malaysia; Faculty of Computer &amp; Mathematical Sciences, Universiti Teknologi Mara, Selangor, Malaysia; Faculty of Computer &amp; Mathematical Sciences, Universiti Teknologi Mara, Selangor, Malaysia","2011 Malaysian Conference in Software Engineering","","2011","","","287","291","Fault injection testing (FIT) approach validates system's fault tolerance mechanism by actively injecting software faults into the targeted areas in the system in order to accelerate its failure rate. This highly complements other testing approaches such as requirements and regression testing implemented during the same testing phase. During testing, it is impossible to run all possible test scenarios. It is especially difficult to predict how the user might use the system functionality correctly as per design. The human interaction through the system may be varies and will leads to the functionality loophole. It is therefore important to have strategic testing approach for evaluating the dependability of computer systems especially in human errors. This paper proposed on applying Knowledge-Based, Fault Prediction Model and Test Case Prioritization approaches that can be combined to increase the test coverage. The goal of this paper is to highlight the needs and advantages of the selected approaches in performing FIT as one of effective testing techniques in the ongoing quest for increased software quality.","","978-1-4577-1531-0978-1-4577-1530-3978-1-4577-1529","10.1109/MySEC.2011.6140685","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140685","Fault Injection Testing;Software Quality","Software engineering;Humans;Predictive models;Software testing;Software quality","program testing;regression analysis;software fault tolerance","test coverage;human-based approach;fault injection testing;fault tolerance;software faults;regression testing;functionality loophole;strategic testing approach;computer systems;knowledge-based fault prediction model;test case prioritization","","","23","","","","","","IEEE","IEEE Conferences"
"An Efficient Automated Test Data Generation Method","H. Cui; L. Chen; B. Zhu; H. Kuang","NA; NA; NA; NA","2010 International Conference on Measuring Technology and Mechatronics Automation","","2010","1","","453","456","How to shorten the time for test data generation is concerned about the main points of the current era. For this purpose, this paper brought forward an efficient automated test data generation method. In this paper first elaborated the basic particle swarm optimization algorithm and the improved particle swarm optimization algorithm, and as a search strategy, and then in this paper focused on the path of trajectory similarity calculation method. Experiments show that their combination of two methods can more efficiently generate test data.","2157-1473;2157-1481","978-1-4244-5739-7978-1-4244-5001","10.1109/ICMTMA.2010.556","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5459797","Test data;particle swarm optimization algorithm;improved PSO;predicate function;path similarity calculation method","Automatic testing;Particle swarm optimization;Software testing;Genetic algorithms;Software quality;Programming;Costs;Current measurement;Time measurement;Mechatronics","particle swarm optimisation;program testing;search problems","automated test data generation method;particle swarm optimization algorithm;search strategy;trajectory similarity calculation method path;software testing","","2","8","","","","","","IEEE","IEEE Conferences"
"Application of Artificial Bee Colony Algorithm to Software Testing","S. S. Dahiya; J. K. Chhabra; S. Kumar","NA; NA; NA","2010 21st Australian Software Engineering Conference","","2010","","","149","154","This paper presents an artificial bee colony based novel search technique for automatic generation of structural software tests. Test cases are symbolically generated by measuring fitness of individuals with the help of branch distance based objective function. Evaluation of the test generator was performed using ten real world programs. Some of these programs had large ranges for input variables. Results show that the new technique is a reasonable alternative for test data generation, but doesn't perform very well for large inputs and where constraints are having many equality constraints.","2377-5408;1530-0803","978-1-4244-6476-0978-1-4244-6475-3978-0-7695-4006","10.1109/ASWEC.2010.30","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5475063","Automatic test data generation;Artificial Bee Colony Algorithm;Swarm intelligence;Symbolic testing;Soft computing;Search Algorithm","Application software;Software algorithms;Software testing;Automatic testing;Performance evaluation;Particle swarm optimization;Automation;Genetic algorithms;Simulated annealing;Software engineering","optimisation;program testing","artificial bee colony algorithm;software testing;structural software tests automatic generation","","11","31","","","","","","IEEE","IEEE Conferences"
"A Simulation Study on Some Search Algorithms for Regression Test Case Prioritization","S. Li; N. Bian; Z. Chen; D. You; Y. He","NA; NA; NA; NA; NA","2010 10th International Conference on Quality Software","","2010","","","72","81","Test case prioritization is an approach aiming at increasing the rate of faults detection during the testing phase, by reordering test case execution. Many techniques for regression test case prioritization have been proposed. In this paper, we perform a simulation experiment to study five search algorithms for test case prioritization and compare the performance of these algorithms. The target of the study is to have an in-depth investigation and improve the generality of the comparison results. The simulation study provides two useful guidelines: (1) Two search algorithms, Additional Greedy Algorithm and 2-Optimal Greedy Algorithm, outperform the other three search algorithms in most cases. (2) The performance of the five search algorithms will be affected by the overlap of test cases with regard to test requirements.","2332-662X;1550-6002;1550-6002","978-1-4244-8078-4978-0-7695-4131","10.1109/QSIC.2010.15","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562946","regression testing;test case prioritization;simulation study;search algorithms","Greedy algorithms;Testing;Software algorithms;Software;Arrays;Algorithm design and analysis;Fault detection","greedy algorithms;program testing;regression analysis;search problems;software fault tolerance","search algorithms;faults detection;regression test case prioritization;additional greedy algorithm;optimal greedy algorithm","","7","16","","","","","","IEEE","IEEE Conferences"
"Search Based Combinatorial Testing","C. Nie; H. Wu; Y. Liang; H. Leung; F. Kuo; Z. Li","NA; NA; NA; NA; NA; NA","2012 19th Asia-Pacific Software Engineering Conference","","2012","1","","778","783","Search techniques can dramatically change our ability to solve a host of problems in applied science and engineering, many search techniques have been developed and applied successfully in many fields, including search based software engineering (SBSE). As a key problem of combinatorial testing, covering array generation has been widely studied and many search techniques have been applied which can be named as search based combinatorial testing (SBCT). SBCT is a branch of search based software testing (SBST) within SBSE. In this paper, to explore the applicability and effectiveness of SBCT, we design six variants from existing search algorithms: Genetic Algorithm, Particle Swarm Optimization and Ant Colony Algorithm by reversing and randomizing their mechanisms. We study their effectiveness in terms of generating a covering array and compare their performance. Experiments show that these search techniques can work well with distinct performance in covering array generation. We believe that these search techniques can be further improved by fine-tuning their configuration and used in broad ranges of area.","1530-1362;1530-1362;1530-1362","978-1-4673-4930-7978-0-7695-4922","10.1109/APSEC.2012.16","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6462745","Genetic Algorithm;Ant Colony Algorithm;Particle Swarm Optimization;Covering Array;Combinatorial Testing;Search Based Combinatorial Testing","Arrays;Testing;Algorithm design and analysis;Search problems;Software engineering;Software algorithms","ant colony optimisation;combinatorial mathematics;genetic algorithms;particle swarm optimisation;program testing;search problems;software engineering","applied science;search based software engineering;SBSE;array generation;search based combinatorial testing;SBCT;search based software testing;SBST;search algorithms;genetic algorithm;particle swarm optimization;ant colony algorithm","","2","27","","","","","","IEEE","IEEE Conferences"
"Efficient Reduction of Model-Based Generated Test Suites through Test Case Pair Prioritization","H. Cichos; T. S. Heinze","NA; NA","2010 Workshop on Model-Driven Engineering, Verification, and Validation","","2010","","","37","42","During the development and maintenance of software, test suites often reach a size that exceeds the costs allocated for test suite execution. In such a case, the test suite needs to be reduced. Many papers are dedicated to the problem of test suite reduction. Most of them consider the removal or merging of test cases. However, less attention has been paid to the identification of test case pairs, which are eminently suitable for merging. In this paper, we fill this gap by presenting a novel approach that helps identifying those test case pairs within a given set of systematically generated test cases which, when merged, have potential for high test suite reduction. As a result, test suites reduced by our approach are considerably smaller in size than those, whose pairs are selected randomly.","","978-0-7695-4384","10.1109/MoDeVVa.2010.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5772249","model-based testing;test suite reduction","Merging;Fault detection;Unified modeling language;Testing;Iron;Barium;Computer aided software engineering","program testing;software maintenance","model-based generated test suites;test case pair prioritization;software development;software maintenance;systematically generated test cases;high test suite reduction","","3","9","","","","","","IEEE","IEEE Conferences"
"EasyA: Easy and Effective Way to Generate Pairwise Test Data","K. F. Rabbi; S. Khatun; C. Y. Yaakub; M. F. J. Klaib","NA; NA; NA; NA","2011 Third International Conference on Computational Intelligence, Communication Systems and Networks","","2011","","","164","167","Testing is a very important task to build error free software. As the resources and time to market is limited for a software product, it is impossible to perform exhaustive test i.e., to test all combinations of input data. To reduce the number of test cases in an acceptable level, it is preferable to use higher interaction level (t way, where t = 2). Pairwise (2- way or t = 2) interaction can find most of the software faults. This paper proposes a matrix based calculation for pairwise test data generation algorithm named EasyA to optimize the number of test cases. Java program has been used to test the performance of the algorithm. The performance is better than the existing algorithms/tools in terms of number of generated test cases and time consumption.","","978-1-4577-0975-3978-0-7695-4482","10.1109/CICSyN.2011.44","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6005671","Combinatorial interaction testing;Software testing;Pairwise testing;Test case generation","Conferences;Algorithm design and analysis;Computer languages;Operating systems;Software testing","Java;matrix algebra;program testing;software fault tolerance","EasyA;error free software;software product;software faults;matrix based calculation;pairwise test data generation algorithm;Java program","","1","17","","","","","","IEEE","IEEE Conferences"
"Hybrid regression testing technique: A multi layered approach","V. Gupta; D. S. Chauhan","Research Scholar, Uttarakhand Technical University, Dehradun, Uttarakhand, India; Vice Chancellor, Uttarakhand Technical University, Dehradun, Uttarakhand, India","2011 Annual IEEE India Conference","","2011","","","1","5","Software needs to be delivered well in time and within budgets. One way of doing this is performing incremental delivery of the software with each increment being adding new features along with changes requests. This incremental delivery is supported with requirement prioritization and needs to be tested for checking the reliability and quality. Testing of this increment calls for testing of not only of old newly added functionality but also of existing features so as to make sure that old parts that works perfectly well do not malfunctions after new code is added. Thus a new hybrid technique is proposed in this paper that clusters the test cases and prioritizes the clusters on basis of priorities of requirements represented by the clusters and series of selections and prioritizations at levels of test cases reduces the number of test cases to manageable level and execution of these test cases guarantees the testing of highest priority requirements associated with statements that are often associated with failures or has highest number of parents or sibling's statements dependent on it and is likely to be influenced by changes or failures in this statement.","2325-940X;2325-9418","978-1-4577-1109-1978-1-4577-1110-7978-1-4577-1108","10.1109/INDCON.2011.6139363","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6139363","Regression testing;hybrid approach;parallelism;clusters","Testing;Software;Arrays;Visualization;Debugging;Software engineering;Information processing","program testing;program verification;regression analysis;software quality;software reliability","hybrid regression testing technique;multilayered approach;incremental delivery;requirement prioritization;software reliability checking;software quality checking;increment calls;test case execution","","6","11","","","","","","IEEE","IEEE Conferences"
"Swarm Intelligence-Based Test Data Generation for Structural Testing","C. Mao; X. Yu; J. Chen","NA; NA; NA","2012 IEEE/ACIS 11th International Conference on Computer and Information Science","","2012","","","623","628","Automated generation of test data has always been a challenging problem in the area of software testing. Recently, meta-heuristic search (MHS) techniques have been proven to be a powerful tool to solve this difficulty. In the paper, we introduce an up-to-date search technique, i.e. particle swarm optimization (PSO), to settle this difficulty. After the basic idea of PSO is addressed, the overall framework of PSO-based test data generation is discussed. Here, the inputs of program under test are encoded into particles. During the search process, PSO algorithm is used to generate test inputs with the highest possible coverage rate. Once a set of test inputs is produced, test driver will seed them into program to run and collect coverage information simultaneously. Then, the value of fitness function for branch coverage can be calculated based on such information, which can direct the algorithm optimization in next iteration. In order to validate our method, five real-world programs are used for experimental analysis. The results show that PSO-based method outperforms other algorithms such as GA both in the coverage effect of test data and the convergence speed.","","978-1-4673-1536","10.1109/ICIS.2012.103","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6211162","Test data generation;PSO;branch coverage;fitness function;convergence speed","Algorithm design and analysis;Convergence;Software testing;Particle swarm optimization;Search problems;Vectors","automatic test pattern generation;automatic test software;convergence;iterative methods;particle swarm optimisation;program testing;search problems","automated test data generation;software testing;metaheuristic search techniques;up-to-date search technique;particle swarm optimization;PSO-based test data generation;program testing;PSO algorithm;test driver;fitness function;convergence speed;structural testing","","3","20","","","","","","IEEE","IEEE Conferences"
"A Principled Evaluation of the Effect of Directed Mutation on Search-Based Statistical Testing","S. Poulding; J. A. Clark; H. Waeselynck","NA; NA; NA","2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops","","2011","","","184","193","Statistical testing generates test inputs by sampling from a probability distribution that is carefully chosen so that the inputs exercise all parts of the software being tested. Sets of such inputs have been shown to detect more faults than test sets generated using traditional random and structural testing techniques. Search-based statistical testing employs a metaheuristic search algorithm to automate the otherwise labour-intensive process of deriving the probability distribution. This paper proposes an enhancement to this search algorithm: information obtained during fitness evaluation is used to direct the mutation operator to those parts of the representation where changes may be most beneficial. A principled empirical evaluation demonstrates that this enhancement leads to a significant improvement in algorithm performance, and so increases both the cost-effectiveness and scalability of search-based statistical testing. As part of the empirical approach, we demonstrate the use of response surface methodology as an effective and objective method of tuning algorithm parameters, and suggest innovative refinements to this methodology.","","978-0-7695-4345-1978-1-4577-0019","10.1109/ICSTW.2011.36","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954408","Software/Program Verification;Testing Strategies;Test Coverage of Code;Optimization;Experimental Design","Probability distribution;Statistical analysis;Testing;Software;Software algorithms;Response surface methodology;Algorithm design and analysis","program testing;search problems;statistical distributions;statistical testing","directed mutation;search-based statistical testing;probability distribution;software testing;fault detection;random testing;structural testing;metaheuristic search algorithm","","5","25","","","","","","IEEE","IEEE Conferences"
"Goal-Oriented Test Case Selection and Prioritization for Product Line Feature Models","A. Ensan; E. Bagheri; M. Asadi; D. Gasevic; Y. Biletskiy","NA; NA; NA; NA; NA","2011 Eighth International Conference on Information Technology: New Generations","","2011","","","291","298","The software product line engineering paradigm is amongst the widely used means for capturing and handling the commonalities and variabilities of the many applications of a target domain. The large number of possible products and complex interactions between software product line features makes the effective testing of them a challenge. To conquer the time and space complexity involved with testing a product line, an intuitive approach is the reduction of the test space. In this paper, we propose an approach to reduce the product line test space. We introduce a goal-oriented approach for the selection of the most desirable features from the product line. Such an approach allows us to identify the features that are more important and need to be tested more comprehensively from the perspective of the domain stakeholders. The more important features and the configurations that contain them will be given priority over the less important configurations, hence providing a hybrid test case reduction and prioritization strategy for testing software product lines.","","978-1-61284-427-5978-0-7695-4367","10.1109/ITNG.2011.58","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5945249","Feature Models;Test Case Prioritization;Test Case Selection;Product Lines;Goal Models","Testing;Software;Cognition;Maintenance engineering;Marketing and sales;Complexity theory;Analytical models","computational complexity;product development;program testing;software reusability","goal-oriented test case selection;product line feature model prioritization;software product line engineering paradigm;time complexity;space complexity;software product line testing","","4","20","","","","","","IEEE","IEEE Conferences"
"An Analysis of Failure-Based Test Profiles for Random Testing","R. Merkel; F. Kuo; T. Y. Chen","NA; NA; NA","2011 IEEE 35th Annual Computer Software and Applications Conference","","2011","","","68","75","In random testing, the distribution of the generated test cases is known as the test profile. We consider the effects of different test profiles, taking advantage of probabilistic information about likely failure-revealing inputs, on the effectiveness of random testing for debugging. We examine a failure-proportional testing strategy, in which tests are randomly sampled with replacement, with probability proportional to a previously identified failure probability distribution, compared to a uniform testing strategy, in which tests are randomly sampled uniformly from the entire input domain. We show that neither strategy optimises failure-detection capabilities, and show an alternative strategy that does. We also consider selection without replacement, and examine the robustness of some strategies given a divergence between the estimated and actual failure probability distributions.","0730-3157;0730-3157;0730-3157","978-0-7695-4439-7978-1-4577-0544-1978-0-7695-4439","10.1109/COMPSAC.2011.88","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032326","test profiles;failure-based testing;random testing;adaptive random testing","Testing;Mathematical model;Software;Presses;Equations;Probability distribution;Analytical models","automatic test software;probability;program debugging;program diagnostics;program testing;random processes;sampling methods;software reliability","failure-based test profile analysis;random testing;test case generation;probabilistic information;debugging;failure-proportional testing strategy;random sampling;identified failure probability distribution;failure-detection capability","","1","12","","","","","","IEEE","IEEE Conferences"
"Using Methods & Measures from Network Analysis for GUI Testing","E. Elsaka; W. E. Moustafa; B. Nguyen; A. Memon","NA; NA; NA; NA","2010 Third International Conference on Software Testing, Verification, and Validation Workshops","","2010","","","240","246","Graphical user interfaces (GUIs) for today's applications are extremely large. Moreover, they provide many degrees of freedom to the end-user, thus allowing the user to perform a very large number of event sequences on the GUI. The large sizes and degrees of freedom create severe problems for GUI quality assurance, including GUI testing. In this paper, we leverage methods and measures from network analysis to analyze and study GUIs, with the goal of aiding GUI testing activities. We apply these methods and measures on the event-flow graph model of GUIs. Results of a case study show that ""network centrality measures"" are able to identify the most important events in the GUI as well as the most important sequences of events. These events and sequences are good candidates for test prioritization. In addition, the ""betweenness clustering"" method is able to partition the GUI into regions that can be tested separately.","","978-1-4244-6774-7978-1-4244-6773-0978-0-7695-4050","10.1109/ICSTW.2010.61","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463654","GUI testing;event-flow graphs;network analysis;test prioritization;software testing;network centrality;betweenness clustering","Graphical user interfaces;Delay;Application software;Java;Software testing;Humans;Automatic testing;Time measurement;Informatics;Smart phones","graphical user interfaces;program testing;software quality","network analysis;GUI testing;graphical user interfaces;event sequences;GUI quality assurance;event-flow graph model;network centrality measures;betweenness clustering method","","2","17","","","","","","IEEE","IEEE Conferences"
"A model based prioritization technique for component based software retesting using uml state chart diagram","S. Mohanty; A. A. Acharya; D. P. Mohapatra","School of Computer Engineering, KIIT University, Bhubaneswar, India; School of Computer Engineering, KIIT University, Bhubaneswar, India; Department of Computer Science, &amp; Engineering National Institute of Technology, Rourkela, India","2011 3rd International Conference on Electronics Computer Technology","","2011","2","","364","368","Regression testing is the process of testing a modified system using the old test suite. As the test suite size is large, system retesting consumes large amount of time and computing resources. This issue of retesting of software systems can be handled using a good test case prioritization technique. A prioritization technique schedules the test cases for execution so that the test cases with higher priority executed before lower priority. The objective of test case prioritization is to detect fault as early as possible so that the debuggers can begin their work earlier. In this paper we propose a new prioritization technique to prioritize the test cases to perform regression testing for Component Based Software System (CBSS). The components and the state changes for a component based software systems are being represented by UML state chart diagrams which are then converted into Component Interaction Graph (CIG) to describe the interrelation among components. Our prioritization algorithm takes this CIG as input along with the old test cases and generates a prioritized test suit taking into account total number of state changes and total number of database access, both direct and indirect, encountered due to each test case. Our algorithm is found to be very effective in maximizing the objective function and minimizing the cost of system retesting when applied to few JAVA projects.","","978-1-4244-8679-3978-1-4244-8678","10.1109/ICECTECH.2011.5941719","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5941719","regression testing;software components;state chart diagram;CBSS;CIG","Databases;Testing;Software systems;Fault detection;Unified modeling language;Computational modeling","object-oriented programming;program testing;regression analysis;software fault tolerance;Unified Modeling Language","model based prioritization technique;component based software retesting;UML state chart diagram;regression testing;test case prioritization technique;fault detection;component interaction graph;objective function;JAVA","","1","7","","","","","","IEEE","IEEE Conferences"
"Using Coverage Information to Guide Test Case Selection in Adaptive Random Testing","Z. Q. Zhou","NA","2010 IEEE 34th Annual Computer Software and Applications Conference Workshops","","2010","","","208","213","Random Testing (RT) is a fundamental software testing technique. Adaptive Random Testing (ART) improves the fault-detection capability of RT by employing the location information of previously executed test cases. Compared with RT, test cases generated in ART are more evenly spread across the input domain. ART has conventionally been applied to programs that have only numerical input types, because the distance between numerical inputs is readily measurable. The vast majority of computer programs, however, involve non-numerical inputs. To apply ART to these programs requires the development of effective new distance measures. Different from those measures that focus on the concrete values of program inputs, in this paper we propose a method to measure the distance using coverage information. The proposed method enables ART to be applied to all kinds of programs regardless of their input types. Empirical studies are further conducted for the branch coverage Manhattan distance measure using the replace and space programs. Experimental results show that, compared with RT, the proposed method significantly reduces the number of test cases required to detect the first failure. This method can be directly applied to prioritize regression test cases, and can also be incorporated into code-based and model-based test case generation tools.","","978-1-4244-8089-0978-0-7695-4105","10.1109/COMPSACW.2010.43","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5615787","software testing;adaptive random testing;adaptive random sequence;test case prioritization;distance measure","","complete computer programs;program testing;random sequences;software engineering","coverage information;test case selection;adaptive random testing;fault detection capability;location information;test case generation;ART;computer program;nonnumerical input;software testing technique;branch coverage Manhattan distance measure;space program;regression test case;code based test case generation;model based test case generation","","17","27","","","","","","IEEE","IEEE Conferences"
"Tuple density: a new metric for combinatorial test suites (NIER track)","B. Chen; J. Zhang","Chinese Academy of Sciences, Beijing, China; Chinese Academy of Sciences, Beijing, China","2011 33rd International Conference on Software Engineering (ICSE)","","2011","","","876","879","We propose tuple density to be a new metric for combinatorial test suites. It can be used to distinguish one test suite from another even if they have the same size and strength. Moreover, it is also illustrated how a given test suite can be optimized based on this metric. The initial experimental results are encouraging.","1558-1225;0270-5257","978-1-4503-0445-0978-1-4503-0445","10.1145/1985793.1985931","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032541","combinatorial testing;metrics;test suites;tuple density","Optimization;Testing;Measurement;Arrays;Software;Software engineering;Computer science","combinatorial mathematics;program testing;software metrics","tuple density;combinatorial test suites;NIER track;software metrics","","5","6","","","","","","IEEE","IEEE Conferences"
"Optimizing for the Number of Tests Generated in Search Based Test Data Generation with an Application to the Oracle Cost Problem","M. Harman; S. G. Kim; K. Lakhotia; P. McMinn; S. Yoo","NA; NA; NA; NA; NA","2010 Third International Conference on Software Testing, Verification, and Validation Workshops","","2010","","","182","191","Previous approaches to search based test data generation tend to focus on coverage, rather than oracle cost. While there may be an aspiration that systems should have models, checkable specifications and/or contract driven development, this sadly remains an aspiration; in many real cases, system behaviour must be checked by a human. This painstaking checking process forms a significant cost, the oracle cost, which previous work on automated test data generation tends to overlook. One simple way to reduce oracle cost consists of reducing the number of tests generated. In this paper we introduce three algorithms which do this without compromising coverage achieved. We present the results of an empirical study of the effectiveness of the three algorithms on five benchmark programs containing non trivial search spaces for branch coverage. The results indicate that it is, indeed, possible to make reductions in the number of test cases produced by search based testing, without loss of coverage.","","978-1-4244-6774-7978-1-4244-6773-0978-0-7695-4050","10.1109/ICSTW.2010.31","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463645","search based testing;oracle cost;test suite minimization","Cost function;Software testing;Automatic testing;System testing;Humans;Object oriented modeling;Benchmark testing;Software algorithms;Application software;Educational institutions","formal specification;program testing","search based test data generation;oracle cost problem;checkable specifications;contract driven development;nontrivial search spaces;branch coverage","","29","40","","","","","","IEEE","IEEE Conferences"
"Symbolic System Time in Distributed Systems Testing","O. S. Dustmann; R. Sasnauskas; K. Wehrle","NA; NA; NA","2012 IEEE Fifth International Conference on Software Testing, Verification and Validation","","2012","","","893","894","We propose an extension of symbolic execution of distributed systems to test software parts related to timing. Currently, the execution model is limited to symbolic input for individual nodes, not capturing the important class of timing errors resulting from varying network conditions. In this paper, we introduce symbolic system time in order to systematically find timing-related bugs in distributed systems. Instead of executing time events at a concrete time, we execute them at a set of times and analyse possible event interleaving son demand. We detail on the resulting problem space, discuss possible algorithmic optimisations, and highlight our future research directions.","2159-4848","978-0-7695-4670-4978-1-4577-1906","10.1109/ICST.2012.193","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200205","Testing;Symbolic Execution;Distributed Systems;Networks","Testing;Concrete;Wireless sensor networks;Computer bugs;Optimization;Engines;Conferences","distributed processing;program debugging;program testing","symbolic system time;distributed systems testing;symbolic execution;software part testing;timing errors;timing-related bugs;algorithmic optimisations","","","6","","","","","","IEEE","IEEE Conferences"
"Oracle-Centric Test Case Prioritization","M. Staats; P. Loyola; G. Rothermel","NA; NA; NA","2012 IEEE 23rd International Symposium on Software Reliability Engineering","","2012","","","311","320","Recent work in testing has demonstrated the benefits of considering test oracles in the testing process. Unfortunately, this work has focused primarily on developing techniques for generating test oracles, in particular techniques based on mutation testing. While effective for test case generation, existing research has not considered the impact of test oracles in the context of regression testing tasks. Of interest here is the problem of test case prioritization, in which a set of test cases are ordered to attempt to detect faults earlier and to improve the effectiveness of testing when the entire set cannot be executed. In this work, we propose a technique for prioritizing test cases that explicitly takes into account the impact of test oracles on the effectiveness of testing. Our technique operates by first capturing the flow of information from variable assignments to test oracles for each test case, and then prioritizing to ``cover'' variables using the shortest paths possible to a test oracle. As a result, we favor test orderings in which many variables impact the test oracle's result early in test execution. Our results demonstrate improvements in rate of fault detection relative to both random and structural coverage based prioritization techniques when applied to faulty versions of three synchronous reactive systems.","1071-9458;1071-9458;2332-6549","978-1-4673-4638-2978-0-7695-4888","10.1109/ISSRE.2012.13","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405379","Software testing;software metrics","Testing;Fault detection;Measurement;Instruments;Java;Aircraft;Educational institutions","program testing;regression analysis","Oracle-centric test case prioritization;generating test oracles;mutation testing;regression testing tasks;fault detection;information flow;structural coverage based prioritization techniques;synchronous reactive systems","","6","44","","","","","","IEEE","IEEE Conferences"
"Challenges in Audit Testing of Web Services","C. D. Nguyen; A. Marchetto; P. Tonella","NA; NA; NA","2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops","","2011","","","103","106","Audit testing of Web services is a form of regression testing that applies to the integration of external services in a service composition. When such services change, the application that integrates them needs to be regressiontested, to ensure that the existing functionalities are not broken by the service changes. This means the external services must be audited for preserved compatibility with the integration. In this paper, we analyze the main research challenges behind audit testing of services. Such challenges descend from the limited testability of external services and encompass traditional regression testing problems, such as test case selection and prioritization, for which existing solutions are nonetheless inadequate. Moreover, in the paper we discuss in detail one such challenge, i.e., the problem of defining an oracle by making the integrator's assumptions explicit.","","978-0-7695-4345-1978-1-4577-0019","10.1109/ICSTW.2011.104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954397","Webservice Composition;Audit Testing;Regression Testing","Testing;Minimization;Conferences;Service oriented architecture;Protocols;Business","program testing;Web services","audit testing;Web services;regression testing;service composition;test case selection;test case prioritization","","1","8","","","","","","IEEE","IEEE Conferences"
"Calculating Prioritized Interaction Test Sets with Constraints Using Binary Decision Diagrams","E. Salecker; R. Reicherdt; S. Glesner","NA; NA; NA","2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops","","2011","","","278","285","Combinatorial interaction testing has become an established technique to systematically determine test sets for highly-configurable software systems. The generation of minimal test sets that fullfill the demanded coverage criteria is an NP-complete problem. Constraint handling and integrated test case prioritization, features necessary for practical use, further complicate the problem. We present a novel algorithm that exploits our observation that the combinatorial interaction testing problem with constraints can be modelled as a single propositional logic formula. Our test set calculation algorithm uses binary decision diagrams as efficient data structure for this formula. The algorithm supports constraints and prioritization. Our evaluation results prove its cost effectiveness. For many benchmark problems the algorithm calculates the best results compared to other greedy approaches.","","978-0-7695-4345-1978-1-4577-0019","10.1109/ICSTW.2011.79","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954420","combinatorial interaction testing;constraints;priorities;binary decision diagrams","Data structures;Boolean functions;Calibration;Testing;Color;Keyboards;Transforms","binary decision diagrams;computational complexity;formal logic;program testing","interaction test set;binary decision diagram;combinatorial interaction testing;highly-configurable software system;NP-complete problem;constraint handling;integrated test case prioritization;propositional logic formula","","5","16","","","","","","IEEE","IEEE Conferences"
"Case based reasoning approach for adaptive test suite optimization","B. Narendra Kumar Rao; A. RamaMohan Reddy","Dept. of CSE, SVEC, Tirupati, Andhra Pradesh, India; Dept. of CSE, SVEC, Tirupati, Andhra Pradesh, India","2012 Third International Conference on Computing, Communication and Networking Technologies (ICCCNT'12)","","2012","","","1","5","Case-based reasoning is an approach to problem solving and learning that has got a lot of attention over the last few years. This paper provides an overview of the foundational issues related to case-based reasoning, describing some of the leading methodological approaches within the field, and exemplifying the current state through pointers to some systems. The framework influences the recent methodologies for knowledge level descriptions of intelligent systems. The methods for case retrieval reuse, solution testing, and learning are summarized, and realization is discussed with few example systems that represent different CBR approaches. Regression testing occurs during the maintenance stage of the software life cycle, however, it requires large amounts of test cases to assure the attainment of a certain degree of quality. So, test suite sizes may grow significantly. This paper focuses primarily on application of CBR to test suite optimization.","","","10.1109/ICCCNT.2012.6395870","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6395870","Retrieve;Reuse;Revise;Retain;Test suite reduction;Test case Prioritization;Test suite Optimization","Optimization;Maintenance engineering;Algorithm design and analysis;Manuals;Computers","case-based reasoning;learning (artificial intelligence);problem solving;program testing;regression analysis;software maintenance;statistical testing","adaptive test suite optimization;problem solving;case-based reasoning approach;learning;methodological approaches;knowledge level descriptions;intelligent systems;case retrieval reuse methods;solution testing;CBR approaches;regression testing;software life cycle","","","17","","","","","","IEEE","IEEE Conferences"
"Testing Deadline Misses for Real-Time Systems Using Constraint Optimization Techniques","S. Di Alesio; A. Gotlieb; S. Nejati; L. Briand","NA; NA; NA; NA","2012 IEEE Fifth International Conference on Software Testing, Verification and Validation","","2012","","","764","769","Safety-critical real-time applications are typically subject to stringent timing constraints which are dictated by the surrounding physical environments. Specifically, tasks in these applications need to finish their execution before given deadlines, otherwise the system is deemed unsafe. It is therefore important to test real-time systems for deadline misses. In this paper, we present a strategy for testing real-time applications that aim sat finding test scenarios in which deadline misses become more likely. We identify such test scenarios by searching the possible ways that a set of real-time tasks can be executed according to the scheduling policy of the operating system on which they are running. We formulate this search problem using a constraint optimization model that includes (1) a set of constraints capturing how a given set of tasks with real-time constraints are executed according to a particular scheduling policy, and (2) a cost function that estimates how likely the given tasks are to miss their deadlines. We implement our constraint optimization model in ILOG SOLVER, apply our model to several examples, and report on the performance results.","2159-4848","978-0-7695-4670-4978-1-4577-1906","10.1109/ICST.2012.171","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200183","stress testing;real-time software systems;constraint optimization","Real time systems;Analytical models;Testing;Stress;Constraint optimization;IP networks","operating systems (computers);optimisation;program testing;real-time systems;safety-critical software;scheduling","deadline misses testing;real-time systems;constraint optimization techniques;safety-critical real-time applications;real-time tasks;scheduling policy;operating system;ILOG solver","","5","15","","","","","","IEEE","IEEE Conferences"
"Regression testing in Software as a Service: An industrial case study","H. Srikanth; M. B. Cohen","IBM Lotus Division, Littleton, MA, USA; Dept. of Computer Science &amp; Engineering, University of Nebraska-Lincoln, USA","2011 27th IEEE International Conference on Software Maintenance (ICSM)","","2011","","","372","381","Many organizations are moving towards a business model of Software as a Service (SaaS), where customers select and pay for services dynamically via the web. In SaaS, service providers face the challenge of delivering and maintaining high quality software solutions which must continue to work under an enormous number of scenarios; customers can easily subscribe and unsubscribe from services at any point. To date, there has been little research on unique approaches for regression test methodologies for testing in a SaaS environment. In this paper, we present an industrial case study of a regression testing approach to improve test effectiveness and efficiency in SaaS. We model service level use cases from field failures as abstract events and then generate sequences of these for testing to provide a broad coverage of the possible use cases. In subsequent releases of the system we prioritize the tests to improve time to detection of faults in the modified system. We have applied our technique to two releases of a large industrial enterprise level SaaS application and demonstrate that using our approach (1) we could have uncovered escaped faults prior to the system release in both versions of the system; (2) using a priority order we could have improved the efficiency of testing in the first version; and (3) prioritization based on failure history from the first version increases the fault detection rate in the new version, suggesting a correlation between the important sequences in versions that can be leveraged for regression testing.","1063-6773;1063-6773","978-1-4577-0664-6978-1-4577-0663-9978-1-4577-0662","10.1109/ICSM.2011.6080804","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080804","Regression Testing;Prioritization;Software as a Service;Cloud Computing","Testing;Software;Companies;Fault detection;Arrays;Maintenance engineering","cloud computing;fault diagnosis;program testing;regression analysis;software fault tolerance;software maintenance","regression testing approach;software-as-a-service;SaaS;industrial case study;Web;sequence generation;fault detection;industrial enterprise level;failure history","","7","25","","","","","","IEEE","IEEE Conferences"
"Mutation-Driven Generation of Unit Tests and Oracles","G. Fraser; A. Zeller","Saarland University, Saarbrücken; Saarland University, Saarbrücken","IEEE Transactions on Software Engineering","","2012","38","2","278","292","To assess the quality of test suites, mutation analysis seeds artificial defects (mutations) into programs; a nondetected mutation indicates a weakness in the test suite. We present an automated approach to generate unit tests that detect these mutations for object-oriented classes. This has two advantages: First, the resulting test suite is optimized toward finding defects modeled by mutation operators rather than covering code. Second, the state change caused by mutations induces oracles that precisely detect the mutants. Evaluated on 10 open source libraries, our μtest prototype generates test suites that find significantly more seeded defects than the original manually written test suites.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.93","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6019060","Mutation analysis;test case generation;unit testing;test oracles;assertions;search-based testing.","Testing;Genetic algorithms;Biological cells;Software;Software algorithms;Generators;Libraries","automatic test pattern generation;object-oriented programming;optimisation;program testing","mutation driven generation;unit test;oracle;test suites quality;artificial defects;automated test case generation;object-oriented classes;optimization;mutation operators;open source libraries","","80","52","","","","","","IEEE","IEEE Journals & Magazines"
"Microprocessor Software-Based Self-Testing","M. Psarakis; D. Gizopoulos; E. Sanchez; M. Sonza Reorda","University of Piraeus, 18534; University of Piraeus, Piraeus; Politecnico di Torino, Torino; Politecnico di Torino, Torino","IEEE Design & Test of Computers","","2010","27","3","4","19","This article discusses the potential role of software-based self-testing in the microprocessor test and validation process, as well as its supplementary role in other classic functional- and structural-test methods. In addition, the article proposes a taxonomy for different SBST methodologies according to their test program development philosophy, and summarizes research approaches based on SBST techniques for optimizing other key aspects.","0740-7475;1558-1918","","10.1109/MDT.2010.5","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5396292","design and test;microprocessor test;software-based self-test;SBST","Microprocessors;Built-in self-test;Automatic testing;Software testing;Semiconductor device testing;Hardware;Frequency;Taxonomy;System testing;High performance computing","microprocessor chips;program testing","microprocessor software based self testing;SBST methodologies;test program development","","121","33","","","","","","IEEE","IEEE Journals & Magazines"
"Taking Advantage of Service Selection: A Study on the Testing of Location-Based Web Services Through Test Case Prioritization","K. Zhai; B. Jiang; W. K. Chan; T. H. Tse","NA; NA; NA; NA","2010 IEEE International Conference on Web Services","","2010","","","211","218","Dynamic service compositions pose new verification and validation challenges such as uncertainty in service membership. Moreover, applying an entire test suite to loosely coupled services one after another in the same composition can be too rigid and restrictive. In this paper, we investigate the impact of service selection on service-centric testing techniques. Specifically, we propose to incorporate service selection in executing a test suite and develop a suite of metrics and test case prioritization techniques for the testing of location-aware services. A case study shows that a test case prioritization technique that incorporates service selection can outperform their traditional counterpart - the impact of service selection is noticeable on software engineering techniques in general and on test case prioritization techniques in particular. Further-more, we find that points-of-interest-aware techniques can be significantly more effective than input-guided techniques in terms of the number of invocations required to expose the first failure of a service composition.","","978-1-4244-8146-0978-0-7695-4128","10.1109/ICWS.2010.98","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5552784","test case prioritization;location-based web service;service-centric testing;service selection","Testing;Web services;Measurement;Entropy;Global Positioning System;Cities and towns;Sorting","mobile computing;program testing;program verification;Web services","service selection;location-based Web services;test case prioritization;dynamic service composition;verification;validation;service membership;service-centric testing;location-aware services;software engineering","","14","24","","","","","","IEEE","IEEE Conferences"
"T-Way Test Data Generation Strategy Based on Particle Swarm Optimization","B. S. Ahmed; K. Z. Zamli","NA; NA","2010 Second International Conference on Computer Research and Development","","2010","","","93","97","Due to market demands, software has grown tremendously in size and functionalities over the years. As side effects of such growth, there tend to be more and more unwanted interaction between software and system parameters. These unwanted interactions can sometimes lead to nasty and difficult bugs to detect. In order to address these issues, t-way strategies (i.e. where t indicates interaction strength) are helpful to generate a set of test cases (i.e. to form a complete suite) that cover the required interaction strength as least once from a typically large space of possible test values. In this paper, we highlight a new t-way strategy based on Particle Swarm Optimization, called PSTG. Preliminary results demonstrated that PSTG compares well against other existing t-way strategies.","","978-0-7695-4043","10.1109/ICCRD.2010.56","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5489394","t-way Testing;Interaction Testing;Artificial Intelligence;Particle Swarm Optimization","Particle swarm optimization;Lab-on-a-chip;Software testing;Electronic equipment testing;Software engineering;Consumer electronics;Software systems;Computer bugs;Artificial intelligence;System testing","particle swarm optimisation;program testing","t-way test data generation strategy;Particle Swarm Optimization;PSTG;interaction testing","","4","19","","","","","","IEEE","IEEE Conferences"
"An integrated Automatic Test Generation and executing system","H. Chen; D. Chen; J. Ye; W. Cao; L. Gao","Beijing, HeroTec Test & Control Ltd., 1 Huayuan Rd., Haidian, 100191, China; Microsoft Corp., 1 Microsoft Way, Redmond, WA 98052, USA; Beijing, HeroTec Test & Control Ltd., 1 Huayuan Rd., Haidian, 100191, China; Beijing, HeroTec Test & Control Ltd., 1 Huayuan Rd., Haidian, 100191, China; Beijing, HeroTec Test & Control Ltd., 1 Huayuan Rd., Haidian, 100191, China","2011 IEEE AUTOTESTCON","","2011","","","383","390","This paper presents an integrated Automatic Test Generation (ATG) and Automatic Test Executing/Equipment (ATE) system for complex boards. We developed an ATG technique called Behavior-Based Automatic Test Generation technique (namely BBATG). BBATG uses the device behavior fault model and represents a circuit board as interconnection of devices. A behavior of a device is a set of functions with timing relations on its in/out pins. When used for a digital circuit board test generation, BBATG utilizes device behavior libraries to drive behavior error signals and sensitize paths along one or multiple vectors so that a heavy and complicated iterating process can be avoided for sequential circuit test deductions. We have developed a complete set of test executing software and test supporting hardware for the ATE which can use the BBATG generated test data directly to detect behavior faults and diagnose faults at the device level for complex circuit boards. In addition, we have proposed and implemented useful technique, especially Design For Testability (DFT) [1][2] application technique on the integrated system, so the test generating/executing for complex boards with VLSI can be further simplified and optimized.","1558-4550;1088-7725;1088-7725","978-1-4244-9363-0978-1-4244-9362-3978-1-4244-9361","10.1109/AUTEST.2011.6058726","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6058726","Behavior Fault Model;Detect Test;Diagnosis Test;Automatic Test Generation (ATG);Automatic Test Executing/Equipment (ATE);Design for Testability (DFT) application;JTAG Device (Device with JTAG circuit)","Circuit faults;Very large scale integration;Integrated circuit modeling;Printed circuits;Pins;Libraries;Wires","automatic test equipment;automatic test pattern generation;design for testability;electronic engineering computing;fault simulation;interconnections;printed circuit testing","integrated automatic test generation;executing system;automatic test executing/equipment system;ATE;complex board;behavior based automatic test generation technique;device behavior fault model;device interconnection;digital circuit board test generation;drive behavior error signal;test executing software;test supporting hardware;design for testability;VLSI","","2","16","","","","","","IEEE","IEEE Conferences"
"Software fault localization based on program slicing spectrum","W. Wen","School of Computer Science and Engineering, Southeast University, Nanjing, China","2012 34th International Conference on Software Engineering (ICSE)","","2012","","","1511","1514","During software development and maintenance stages, programmers have to frequently debug the software. One of the most difficult and complex tasks in the debugging activity is software fault localization. A commonly-used method to fix software fault is computing suspiciousness of program elements according to failed test executions and passed test executions. However, this technique does not give full consideration to dependences between program elements, thus its capacity for efficient fault localization is limited. Our research intends to introduce program slicing technique and statistical method which extracts dependencies between program elements and refines execution history, then builds program slicing spectra to rank suspicious elements by a statistical metric. We expect that our method will contribute directly to the improvement of the effectiveness and the accuracy of software fault localization and reduce the software development and maintenance effort and cost.","1558-1225;0270-5257;0270-5257","978-1-4673-1067-3978-1-4673-1066-6978-1-4673-1065","10.1109/ICSE.2012.6227049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227049","fault localization;program slicing spectrum;software debugging","Software;Debugging;Measurement;History;Statistical analysis;Generators;Software algorithms","program debugging;program slicing;program testing;software maintenance;statistical analysis","software fault localization;program slicing spectrum;software development;software maintenance;software debugging;failed test executions;passed test executions;program element dependency extraction;statistical method;execution history refinement;statistical metric","","9","20","","","","","","IEEE","IEEE Conferences"
"Dependency-Based Test Case Selection and Prioritization in Embedded Systems","P. Caliebe; T. Herpel; R. German","NA; NA; NA","2012 IEEE Fifth International Conference on Software Testing, Verification and Validation","","2012","","","731","735","Embedded systems in automotive engineering are getting more and more complex due to a higher rate of integration and shared usage of sensor signals. A common solution to testing these systems is deriving test cases from models, so called model-based testing. In practice, generated test suites are typically very huge and have to be reduced by methods of regression-test selection and prioritization. In our field of application, we additionally suffer from the lack of knowledge on system internals like the source code. Therefore, our approach is based on dependences between the components of embedded systems. The model we use is derived from the system architecture and system requirements. We are using graph algorithms for selecting and prioritizing test cases to run on a certain system version. First statistical evaluations and the current implementation already have shown promising reduction of test-cases for regression testing.","2159-4848","978-0-7695-4670-4978-1-4577-1906","10.1109/ICST.2012.164","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200176","Software Testing;Component-Based;Dependences;Structure-Based;Regression Testing;Verification;Validation","Testing;Unified modeling language;Automotive engineering;Embedded systems;Integrated circuit modeling","automotive engineering;embedded systems;graph theory;mechanical engineering computing;program testing;regression analysis","dependency-based test case selection;embedded systems;automotive engineering;test cases;model-based testing;regression-test selection;system architecture;system requirements;graph algorithms;regression testing","","10","18","","","","","","IEEE","IEEE Conferences"
"Automatic Test Data Generation for Path Testing Using Genetic Algorithms","W. Xibo; S. Na","NA; NA","2011 Third International Conference on Measuring Technology and Mechatronics Automation","","2011","1","","596","599","Software testing is the important means that guarantee software quality and reliability. Improving the automation ability of software testing is very important for ensuring software's quality and reducing development cost, and improving the automation ability of test cases generation is the key point for the entire process. This paper discusses the methods and techniques of genetic algorithm as the key algorithm to automatically generating the test data, and elaborates some specific problems need to solve in realization process: such as coding, the selection of fitness function and the improvement of hereditary operator, etc.","2157-1473;2157-1481","978-1-4244-9010","10.1109/ICMTMA.2011.152","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5720855","Genetic Algorithm;Path Testing;Automatic Test Data;Branch-Cover","Algorithm design and analysis;Instruments;Software testing;Software algorithms;Optimization;Data models","automatic test pattern generation;genetic algorithms;program testing","automatic test data generation;path testing;genetic algorithms;software testing","","2","7","","","","","","IEEE","IEEE Conferences"
"An Improved Metric for Test Case Prioritization","X. Zhang; B. Qu","NA; NA","2011 Eighth Web Information Systems and Applications Conference","","2011","","","125","130","Test case prioritization is an effective and practical technique of regression testing. To illustrate its effectiveness, many test metrics were proposed. In this paper, the physical meanings of these metrics were explained and their limitations were pointed out. Then, an improved metric and its extension for test case prioritization were proposed. The case study indicates that, compared with existing metrics, our new metric can provide much more precise illustration of the effectiveness of test case prioritization techniques.","","978-1-4577-1812","10.1109/WISA.2011.31","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6093578","Software testing;Regression testing;Test case prioritization;Metrics","Information systems","program testing;regression analysis;software metrics","test case prioritization;regression testing;test metrics","","1","9","","","","","","IEEE","IEEE Conferences"
"Analysis and optimization of software requirements prioritization techniques","M. Aasem; M. Ramzan; A. Jaffar","University Institute of Information Technology, Arid Agriculture University Rawalpindi, Pakistan; University Institute of Information Technology, Arid Agriculture University Rawalpindi, Pakistan; National University of Computer and Emerging Sciences Islamabad, Pakistan","2010 International Conference on Information and Emerging Technologies","","2010","","","1","6","Prioritizing requirements helps the project team to understand which requirements are most important and most urgent. Based on this finding a software engineer can decide what to develop/implement in the first release and what on the coming releases. Prioritization is also a useful activity for decision making in other phases of software engineering like development, testing, and implementation. There are a number of techniques available to prioritize the requirements with their associated strengths and limitations. In this paper we will examine state of the art techniques and analyze their applicability on software requirements domain. At the end we present a framework that will help the software engineer of how to perform prioritization process by combining existing techniques and approaches.","","978-1-4244-8003-6978-1-4244-8001-2978-1-4244-8002","10.1109/ICIET.2010.5625687","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5625687","Requirements Engineering;Prioritization Framework","Software;Decision making;Humans;Current measurement;Software engineering;Planning;Manuals","formal specification;formal verification;software development management;systems analysis","software requirement prioritization technique;project team;software engineering;decision making;software development;software testing;software implementation","","12","31","","","","","","IEEE","IEEE Conferences"
"Size-Constrained Regression Test Case Selection Using Multicriteria Optimization","S. Mirarab; S. Akhlaghi; L. Tahvildari","University of Texas at Austin, Austin; Shahed University, Tehran; University of Waterloo, Waterloo","IEEE Transactions on Software Engineering","","2012","38","4","936","956","To ensure that a modified software system has not regressed, one approach is to rerun existing test cases. However, this is a potentially costly task. To mitigate the costs, the testing effort can be optimized by executing only a selected subset of the test cases that are believed to have a better chance of revealing faults. This paper proposes a novel approach for selecting and ordering a predetermined number of test cases from an existing test suite. Our approach forms an Integer Linear Programming problem using two different coverage-based criteria, and uses constraint relaxation to find many close-to-optimal solution points. These points are then combined to obtain a final solution using a voting mechanism. The selected subset of test cases is then prioritized using a greedy algorithm that maximizes minimum coverage in an iterative manner. The proposed approach has been empirically evaluated and the results show significant improvements over existing approaches for some cases and comparable results for the rest. Moreover, our approach provides more consistency compared to existing approaches.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.56","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5928351","Software regression testing;test case selection;integer programming;Pareto optimality","Testing;Software;Time factors;Fault detection;Optimization;Estimation;IP networks","greedy algorithms;integer programming;linear programming;program testing;regression analysis","size constrained regression test case selection;multicriteria optimization;modified software system;integer linear programming problem;voting mechanism;greedy algorithm;iterative manner","","25","61","","","","","","IEEE","IEEE Journals & Magazines"
"Beyond plain video recording of GUI tests: Linking test case instructions with visual response documentation","R. Pham; H. Holzmann; K. Schneider; C. Brüggemann","Software Engineering Group, Leibniz Universität Hannover, Germany; Software Engineering Group, Leibniz Universität Hannover, Germany; Software Engineering Group, Leibniz Universität Hannover, Germany; Application Lifecycle Service Center, Capgemini Deutschland GmbH, Hanover, Germany","2012 7th International Workshop on Automation of Software Test (AST)","","2012","","","103","109","Information systems with sophisticated graphical user interfaces are still difficult to test and debug. As a detailed and reproducible report of test case execution is essential, we advocate the documentation of test case execution on several levels. We present an approach to video-based documentation of automated GUI testing that is linked to the test execution procedure. Viewing currently executed test case instructions alongside actual onscreen responses of the application under test facilitates understanding of the failure. This approach is tailored to the challenges of automated GUI testing and debugging with respect to technical and usability aspects. Screen recording is optimized for speed and memory consumption while all relevant details are captured. Additional browsing capabilities for easier debugging are introduced. Our concepts are evaluated by a working implementation, a series of performance measurements during a technical experiment, and industrial experience from 370 real-world test cases carried out in a large software company.","","978-1-4673-1822-8978-1-4673-1821","10.1109/IWAST.2012.6228977","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6228977","Automated Test;Graphical User Interface;Video;Code Tracing","Graphical user interfaces;Debugging;Testing;Memory management;Keyboards;Video recording;Documentation","graphical user interfaces;program debugging;program testing;video recording","plain video recording;test case instructions;visual response documentation;Information systems;graphical user interfaces;test case execution;video-based documentation;automated GUI testing;test execution procedure;automated GUI debugging;memory consumption;large software company","","1","22","","","","","","IEEE","IEEE Conferences"
"Scenario Driven Testing","K. Sivashanmugam; D. Lin; S. Palanisamy","NA; NA; NA","2011 Eighth International Conference on Information Technology: New Generations","","2011","","","299","303","Software testing has traditionally focused on evaluating the functionality of implemented modules against feature specifications. This approach assumes that customer requirements and usage scenarios are accurately translated into specifications and that individual modules implemented using the feature specifications would work seamlessly and coherently to solve business problems meant to be addressed by the software under test. To ensure software built would help customers solve their business problems as intended, test teams have to go beyond traditional feature driven testing approach and test software for quality and completeness with respect to targeted customer scenarios. For this, test teams have to adopt scenario driven test methodology which involves understanding the targeted customer scenarios and use them along with feature specifications for the intended software solution to translate them into test specifications, prioritization of test work items and use them throughout project for shared understanding of tradeoffs and making decisions. In this short paper, we describe scenario driven testing and share how it was applied to test a feature-set developed for a successful product line at Microsoft®.","","978-1-61284-427-5978-0-7695-4367","10.1109/ITNG.2011.59","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5945250","scenario driven testing;scenario driven engineering;variation modeling;customer oriented testing;test case generation","Testing;Software;Automation;Context;Planning;Business;Object oriented modeling","program testing;software quality","scenario driven testing;software testing;customer requirements;feature specifications;business problems;feature driven testing approach;software quality;Microsoft®","","3","6","","","","","","IEEE","IEEE Conferences"
"Automated software test optimisation framework - an artificial bee colony optimisation-based approach","D. J. Mala; V. Mohan; M. Kamalapriya","Department of Computer Applications, Thiagarajar College of Engineering, Madurai, Tamil Nadu, India; Department of Mathematics, Thiagarajar College of Engineering, Madurai, Tamil Nadu, India; Department of Computer Applications, Thiagarajar College of Engineering, Madurai, Tamil Nadu, India","IET Software","","2010","4","5","334","348","Software test suite optimisation is one of the most important problems in software engineering research. To achieve this optimisation, a novel approach based on artificial bee colony (ABC) optimisation is proposed here. The work applied in this approach is motivated by the intelligent behaviour of honey bees. Since the ABC system combines local search methods carried out by employed and onlooker bees with global search methods managed by scouts, the approach attains global or near-global optima. Here, the parallel behaviour of the three bees is used to reach the solution generation faster. The performance of the proposed approach is investigated based on coverage-based test adequacy criteria by comparing it with sequential ABC, random testing and genetic algorithm-based approaches. Based on the experimental results, it has been proved that the proposed parallel ABC approach outperforms the other approaches in test suite optimisation.","1751-8806;1751-8814","","10.1049/iet-sen.2009.0079","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5585608","","","genetic algorithms;program testing;search problems;software engineering","automated software test optimisation framework;artificial bee colony optimisation-based approach;software engineering research;honey bee intelligent behaviour;ABC system;local search methods;global search methods;coverage-based test adequacy criteria;random testing;genetic algorithm-based approaches","","13","","","","","","","IET","IET Journals & Magazines"
"Dealing with Test Automation Debt at Microsoft","J. Hartmann","NA","2011 IEEE 35th Annual Computer Software and Applications Conference Workshops","","2011","","","136","136","At Microsoft, substantial time and resources are expended in test case development, execution and verification. Thousands of new tests are added to existing test suites without any kind of review regarding their unique contribution to test suite effectiveness or impact on test suite efficiency. This talk describes how we leverage existing code coverage data, together with reduction and prioritization techniques, to help each test team analyze its test suite and guide them in improving their suite's effectiveness and efficiency. The analysis focuses on identifying and deprecating/prioritizing groups of tests cases, given specific tactical goals for example, increasing current test suite stability and reliability, better structuring of test suite migration efforts, reducing test suite execution time and testing with limited hardware resources.","","978-1-4577-0980-7978-0-7695-4459","10.1109/COMPSACW.2011.99","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032226","","Conferences;Software quality;Automation;Hardware;Reliability;Internet","data reduction;program testing;program verification;software reliability","test automation debt;Microsoft;test case development;test case execution;test case verification;test suite effectiveness;test suite efficiency;code coverage data;reduction techniques;prioritization techniques;test suite stability;test suite reliability;test suite migration efforts;test suite execution time;hardware resources","","","","","","","","","IEEE","IEEE Conferences"
"Automatic generation of software test data based on hybrid particle swarm genetic algorithm","Rui Ding; Xianbin Feng; Shuping Li; Hongbin Dong","Computer department, Mudanjiang Normal University, China; Computer department, Mudanjiang Normal University, China; Computer department, Mudanjiang Normal University, China; National Science Park, Harbin Engineering University, China","2012 IEEE Symposium on Electrical & Electronics Engineering (EEESYM)","","2012","","","670","673","A hybrid particle swarm genetic algorithm is purposed to apply in software testing using case automated generations. On the basis of classical genetic algorithm, the algorithm divided the population into “families”, influencing the convergence efficiency by crossover in family, keeping the diversity of the population by crossover between families; meanwhile, enhancing the speed of convergence by the PSO crossover (commixed the thought of PSO in genetic algorithm) According to the characteristics of software testing problems, we designed the corresponding fitness function and the encoding method. The results of data experiment were given to illustrate the effectiveness of the algorithm.","","978-1-4673-2365-9978-1-4673-2363-5978-1-4673-2364","10.1109/EEESym.2012.6258748","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6258748","Genetic Algorithm;Particle Swarm Optimization;Software Testing;Case Automatically Generates","Reliability;Genetics","encoding;genetic algorithms;particle swarm optimisation;program testing","software test data automatic generation;hybrid particle swarm genetic algorithm;convergence efficiency;population diversity;PSO crossover;fitness function;encoding method","","2","12","","","","","","IEEE","IEEE Conferences"
"Notice of Retraction<br>Weight optimization of CET-4 Test sections based on AHP method","Yueting Xu; Han Huang","School of English Education, Guangdong University of Foreign Studies, Guangzhou, China; School of Software Engineering, South China University of Technology, Guangzhou China","2010 International Conference on Optics, Photonics and Energy Engineering (OPEE)","","2010","2","","251","254","The reform of CET-4 (College English Test, Band 4) mainly has oriented in weight adjustment of the test sections. This paper introduces an AHP (Analytical Hierarchy Process) method which optimizes the weight of test sections in CET-4 in order to assess students' communicative competence. The AHP method begins with a questionnaire administered to English teachers from a university. The returned questionnaires are summarized and analyzed by mathematical approach and statistic software to generate the objective weights. The proposed weight is compared with the former weight in the discussion, and the conclusion is considered as a reference for making decisions in weight adjustment of CET-4. The case indicates that the results by AHP method reflect the college English teachers' professional agreement of assessment priority in college English education.;Notice of Retraction<br><br>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<br><br>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<br><br>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<br><br>","2158-7434;2158-7442","978-1-4244-5234-7978-1-4244-5236","10.1109/OPEE.2010.5507987","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5507987","Methodologies in examinations;weight optimization;AHP method;CET-4;collecge English education","Optimization methods;Testing;Educational institutions;Vocabulary;Writing;Education;Photonics;Power engineering and energy;Statistical analysis;Software engineering","computer aided instruction;educational courses;teaching","weight optimization;CET-4 test sections;AHP method;college english test;analytical hierarchy process;mathematical approach;statistic software;English education","","","11","","","","","","IEEE","IEEE Conferences"
"On Practical Adequate Test Suites for Integrated Test Case Prioritization and Fault Localization","B. Jiang; W. K. Chan; T. H. Tse","NA; NA; NA","2011 11th International Conference on Quality Software","","2011","","","21","30","An effective integration between testing and debugging should address how well testing and fault localization can work together productively. In this paper, we report an empirical study on the effectiveness of using adequate test suites for fault localization. We also investigate the integration of test case prioritization and statistical fault localization with a postmortem analysis approach. Our results on 16 test case prioritization techniques and four statistical fault localization techniques show that, although much advancement has been made in the last decade, test adequacy criteria are still insufficient in supporting effective fault localization. We also find that the use of branch-adequate test suites is more likely than statement-adequate test suites in the effective support of statistical fault localization.","2332-662X;1550-6002;1550-6002","978-1-4577-0754-4978-0-7695-4468","10.1109/QSIC.2011.37","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004308","Debugging;testing;continuous integration","Testing;Subspace constraints;Schedules;Debugging;Flexible printed circuits;Cities and towns;Measurement","program debugging;program testing;software fault tolerance;statistical analysis","integrated test case prioritization;debugging;program testing;postmortem analysis approach;statistical fault localization techniques;branch-adequate test suites;statement-adequate test suites","","7","26","","","","","","IEEE","IEEE Conferences"
"Test suite optimization using fuzzy logic","A. A. Haider; S. Rafiq; A. Nadeem","Center for Software Dependability, Mohammad Ali Jinnah University (MAJU), Islamabad, Pakistan; Center for Software Dependability, Mohammad Ali Jinnah University (MAJU), Islamabad, Pakistan; Center for Software Dependability, Mohammad Ali Jinnah University (MAJU), Islamabad, Pakistan","2012 International Conference on Emerging Technologies","","2012","","","1","6","Regression Test suite optimization is an effective technique to reduce time and cost of testing. Many researchers have used computational intelligence techniques to enhance the effectiveness of test suite. These approaches optimize test suite for a single objective. Introduction of fuzzy logic with genetic algorithm and swarm optimization may be used to optimize test suite for multi-objective selection criteria. Secondly, human intervention or expert judgment is required to opt for level of testing, technique used and quality aspect to be tested. Fuzzy logic has proved its worth in many other domains like communication, bio informatics, embedded applications, industrial and engineering control and network optimization. We propose an expert system that finds a trade off among the quality aspects, technique used and level of testing based on objective function defined by the tester, quite similar to human judgment using fuzzy logic based classification. Main focus of our approach is to find a test suite that is optimal for multi-objective regression testing. For proof of our concept, initially we focused on three quality aspects, i.e., performance, throughput and code coverage.","","978-1-4673-4451-7978-1-4673-4452-4978-1-4673-4450","10.1109/ICET.2012.6375440","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6375440","Regression testing;computational intelligence;fuzzy logic;test suite optimization","Testing;Optimization;Fuzzy logic;Throughput;Genetic algorithms;Subspace constraints;Minimization","expert systems;fuzzy logic;genetic algorithms;particle swarm optimisation;regression analysis","regression test suite optimization;computational intelligence;genetic algorithm;swarm optimization;multiobjective selection criteria;expert judgment;expert system;fuzzy logic based classification;multiobjective regression testing;code coverage","","7","24","","","","","","IEEE","IEEE Conferences"
"Adaptive Regression Testing Strategy: An Empirical Study","M. J. Arafeen; H. Do","NA; NA","2011 IEEE 22nd International Symposium on Software Reliability Engineering","","2011","","","130","139","When software systems evolve, different amounts and types of code modifications can be involved in different versions. These factors can affect the costs and benefits of regression testing techniques in different ways, and thus, there may be no single regression testing technique that is the most cost-effective technique to use on every version. To date, many regression testing techniques have been proposed, but no research has been done on the problem of helping practitioners systematically choose appropriate techniques on new versions as systems evolve. To address this problem, we propose adaptive regression testing (ART) strategies that attempt to identify the regression testing techniques that will be the most cost-effective for each regression testing session considering organization's situations and testing environment. To assess our approach, we conducted an experiment focusing on test case prioritization techniques. Our results show that prioritization techniques selected by our approach can be more cost-effective than those used by the control approaches.","2332-6549;1071-9458;1071-9458","978-1-4577-2060-4978-0-7695-4568","10.1109/ISSRE.2011.29","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6132961","Regression testing;adaptive regression testing strategy;test case prioritization;Analytical Hierarchy Process","Testing;Software;Mathematical model;Equations;Subspace constraints;Java;Fault detection","program testing;regression analysis;software process improvement","adaptive regression testing strategy;software systems;code modifications;cost-effective technique;ART strategy;test case prioritization technique","","5","34","","","","","","IEEE","IEEE Conferences"
"Test Case Prioritization for Web Service Regression Testing","L. Chen; Z. Wang; L. Xu; H. Lu; B. Xu","NA; NA; NA; NA; NA","2010 Fifth IEEE International Symposium on Service Oriented System Engineering","","2010","","","173","178","Regression testing is necessary to assure the quality of service-oriented business applications in their evolutions. However, because of the constraint of testing resource, entire test suite may not run as a result. Therefore, test case prioritization technique is required to increase the efficiency of Web service application regression testing. In this paper, we propose a dependence analysis based test case prioritization technique. First, we analyze the dependence relationship using control and data flow information in an orchestration language: WS-BPEL. Then we construct a weighted graph and do impact analysis to identify modification-affected elements. After that, we prioritize test cases according to covering more modification-affected elements with the highest weight. Finally we conduct a case study to illustrate the applicability of our method.","","978-1-4244-7326-7978-1-4244-7327-4978-0-7695-4081","10.1109/SOSE.2010.27","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5569910","Web service;regression testing;test case prioritization;impact analysis;dependence analysis","Testing;Web services;Complexity theory;Business;Flow graphs;Computational modeling;History","business data processing;quality assurance;quality of service;regression analysis;software architecture;Web services","Web service regression testing;quality of service;testing resource constraint;dependence analysis based test case prioritization technique;data flow information;orchestration language;WS-BPEL;weighted graph;modification-affected elements","","8","18","","","","","","IEEE","IEEE Conferences"
"GZoltar: an eclipse plug-in for testing and debugging","J. Campos; A. Riboira; A. Perez; R. Abreu","University of Porto, Portugal; University of Porto, Portugal; University of Porto, Portugal; University of Porto, Portugal","2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering","","2012","","","378","381","Testing and debugging is the most expensive, error-prone phase in the software development life cycle. Automated testing and diagnosis of software faults can drastically improve the efficiency of this phase, this way improving the overall quality of the software. In this paper we present a toolset for automatic testing and fault localization, dubbed GZoltar, which hosts techniques for (regression) test suite minimization and automatic fault diagnosis (namely, spectrum-based fault localization). The toolset provides the infrastructure to automatically instrument the source code of software programs to produce runtime data. Subsequently the data was analyzed to both minimize the test suite and return a ranked list of diagnosis candidates. The toolset is a plug-and-play plug-in for the Eclipse IDE to ease world-wide adoption.","","978-1-4503-1204-2978-1-4503-1204","10.1145/2351676.2351752","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6494960","Automatic Debugging;Automatic Testing;Eclipse plug-in;GZoltar;RZoltar","","fault diagnosis;program debugging;program testing;software quality;statistical testing","Eclipse IDE;plug-and-play plug-in;runtime data;software program source code;toolset;automatic software faults diagnosis;regression test suite minimization;software fault localization;software quality;automated software fault testing;software development life cycle;error-prone phase;debugging;Eclipse plug-in;GZoltar","","21","15","","","","","","IEEE","IEEE Conferences"
"Integrating Model-Based Testing with Evolutionary Functional Testing","F. Lindlar; A. Windisch; J. Wegener","NA; NA; NA","2010 Third International Conference on Software Testing, Verification, and Validation Workshops","","2010","","","163","172","Evolutionary Functional Testing (EFT) is a relatively recent approach to automating the testing process. The research presented in this paper aims at increasing the acceptability of EFT in industrial settings. An approach suitable for efficiently and effectively testing complex continuous control systems is introduced. The main focus is on generating realistic test stimuli, enabling interactivity between test driver and test object, and facilitating the process of designing a suitable fitness function. This is accomplished by integrating EFT with model-based testing methodologies resulting in an intuitive testing approach that enables even testers not familiar with search based testing to achieve good results with limited effort. A test environment optimized for deployment in the industrial domain is introduced. Features of the test environment include the capability of automatically generating realistic continuous test data sets, interacting with the system under test during test execution, and automatically executing and evaluating large numbers of tests. A thorough case study using an adaptive cruise control system from the automotive domain is performed to assess the approach. Results of this work indicate high usability, efficiency, and effectiveness of the proposed method for testing complex embedded systems.","","978-1-4244-6774-7978-1-4244-6773-0978-0-7695-4050","10.1109/ICSTW.2010.10","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463643","test data generation;functional testing;systematic test case design;industrial practise;continuous behavior testing","Automatic testing;System testing;Electrical equipment industry;Automatic control;Control systems;Process design;Adaptive systems;Programmable control;Adaptive control;Automotive engineering","embedded systems;evolutionary computation;program testing","integrating model based testing;evolutionary functional testing;EFT;industrial settings;continuous control systems;test driver;test object;fitness function;search based testing;industrial domain;adaptive cruise control system;automotive domain;embedded systems","","5","25","","","","","","IEEE","IEEE Conferences"
"Software requirements selection using Quantum-inspired Multi-objective Differential Evolution Algorithm","A. C. Kumari; K. Srinivas; M. P. Gupta","Department of Physics and Computer Science, Dayalbagh Educational Institute, Dayalbagh, Agra, India; Department of Electrical Engineering, Dayalbagh Educational Institute, Dayalbagh, Agra, India; Department of Management Studies, Indian Institute of Technology, Delhi, India","2012 CSI Sixth International Conference on Software Engineering (CONSEG)","","2012","","","1","8","This paper presents a Quantum-inspired Multi-objective Differential Evolution Algorithm (QMDEA) for the selection of software requirements, an issue in Requirements engineering phase of software development life cycle. Generally the software development process is iterative or incremental in nature, as request for new requirements keep coming from the customers from time to time for inclusion in the next release of the software. Due to the feasibility reasons it is not possible for a company to incorporate all the requirements in the software product. Consequently, it becomes a challenging task for the company to select a subset of the requirements to be included, by keeping the business goals in view. The problem is to identify a set of requirements to be included in the next release of the product, by minimizing the cost and maximizing the customer satisfaction. As minimizing the cost and maximizing the customer satisfaction are contradictory objectives, the problem is multi-objective and is also NP-hard in nature. Therefore it cannot be solved efficiently using traditional optimization techniques especially for the large problem instances. QMDEA combines the preeminent features of Differential Evolution and Quantum Computing. The features of QMDEA help in achieving quality Pareto-optimal front solutions with faster convergence. The performance of QMDEA is tested on six benchmark problems derived from the literature. The comparison of the obtained results indicates superior performance over the other methods reported in the literature.","","978-1-4673-2177-8978-1-4673-2174-7978-1-4673-2175-4978-1-4673-2176","10.1109/CONSEG.2012.6349487","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6349487","Multi-objective optimization;Search based software engineering;Software requirements selection;Multi-objective Next Release Problem;Quantum-inspired Multi-objective Differential Evolution","Software;Pareto optimization;Sociology;Software algorithms;Vectors","computational complexity;convergence;cost reduction;customer satisfaction;evolutionary computation;formal specification;formal verification;Pareto optimisation;quantum computing;software development management;software maintenance;systems analysis","software requirement selection;quantum-inspired multiobjective differential evolution algorithm;QMDEA;requirements engineering;software development life cycle;software development process;software product;business goals;product release;cost minimization;customer satisfaction maximization;multiobjective problem;NP-hard problem;optimization technique;quantum computing;Pareto-optimal front solution;convergence","","2","31","","","","","","IEEE","IEEE Conferences"
"Use of Clonal Selection Algorithm as Software Test Data Generation Technique","A. Pachauri; Gursaran","NA; NA","2012 Second International Conference on Advanced Computing & Communication Technologies","","2012","","","1","5","Clonal selection algorithm is an algorithm that belongs to the class of immune algorithm inspired form clonal selection principle of biological immune system. Initially clonal selection algorithm was designed for machine learning approach and was used in pattern recognition process of artificial intelligence. The other implementation of clonal selection algorithm is in the field of function optimization, which had gained a tremendous attention of the researchers. We too had used the clonal selection algorithm for the software test data generation technique for branch coverage with some modification according to the requirement.","2327-0632;2327-0659","978-1-4673-0471-9978-0-7695-4640","10.1109/ACCT.2012.118","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6168322","Software test data generation;Clonal Selection Algorithm;Path-prefix strategy","Software;Software algorithms;Cloning;Algorithm design and analysis;Machine learning algorithms;Approximation methods;Testing","biology computing;data handling;learning (artificial intelligence);pattern recognition;program testing","clonal selection algorithm;software test data generation technique;immune algorithm;biological immune system;machine learning approach;artificial intelligence;pattern recognition process","","2","15","","","","","","IEEE","IEEE Conferences"
"An Algorithm for Reducing Test Suite Based on Interface Parameters","L. Zhao; W. Luo","NA; NA","2010 International Conference on Computational Intelligence and Software Engineering","","2010","","","1","4","This paper presents an approach to eliminate a representative set of testing cases from a test suite but provide the same coverage as the entire test suite. This approach mainly is based on graph theory and maps the relationship between inputs and outputs of tested system into the bipartite graph. Using the properties of bipartite graph decompose the entire test suite into some small test suites so as to eliminate the redundant and obsolete test cases in the test suite. In this paper, we propose the definition, classification and properties related to the relationship between inputs and outputs and present an algorithm for the reduction and optimization of test suite. Finally, The reduction approach can reduce the size of test suite dramatically without reducing the coverage rate of test suite and enhance the efficiency of testing.","","978-1-4244-5391-7978-1-4244-5392","10.1109/CISE.2010.5676821","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676821","","Bipartite graph;Software;Software testing;Algorithm design and analysis;IEEE Press;Software algorithms","graph theory;optimisation;program testing","graph theory;bipartite graph;test suite optimization;software development testing","","1","16","","","","","","IEEE","IEEE Conferences"
"Prioritizing Unit Test Creation for Test-Driven Maintenance of Legacy Systems","E. Shihab; Z. M. Jiang; B. Adams; A. E. Hassan; R. Bowerman","NA; NA; NA; NA; NA","2010 10th International Conference on Quality Software","","2010","","","132","141","Test-Driven Development (TDD) is a software development practice that prescribes writing unit tests before writing implementation code. Recent studies have shown that TDD practices can significantly reduce the number of pre-release defects. However, most TDD research thus far has focused on new development. We investigate the adaptation of TDD-like practices for already implemented code, in particular legacy systems. We call this adaptation of TDD-like practices for already implemented code ``Test-Driven Maintenance'' (TDM). In this paper, we present an approach that assists software development and testing managers, who employ TDM, utilize the limited resources they have for testing legacy systems efficiently. The approach leverages the development history of the project to generate a prioritized list of functions that managers should focus their unit test writing resources on. The list is updated dynamically as the development of the legacy system progresses. To evaluate our approach, we conduct a case study on a large commercial legacy software system. Our findings suggest that heuristics based on the function size, modification frequency and bug fixing frequency should be used to prioritize the unit test writing of legacy systems.","2332-662X;1550-6002;1550-6002","978-1-4244-8078-4978-0-7695-4131","10.1109/QSIC.2010.74","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562952","","Computer bugs;Writing;Measurement;History;Time division multiplexing;Maintenance engineering;Data mining","program testing;software development management;software maintenance","test-driven maintenance;test-driven development;software development practice;unit test writing resources;legacy software system;modification frequency;bug fixing frequency","","3","45","","","","","","IEEE","IEEE Conferences"
"A Search-Based Approach to Functional Hardware-in-the-Loop Testing","F. Lindlar; A. Windisch","NA; NA","2nd International Symposium on Search Based Software Engineering","","2010","","","111","119","The potential of applying search-based testing principles to functional testing has been demonstrated in various cases. The focus was mainly on simulating the system under test using a model or compiled source code in order to evaluate test cases. However, in many cases only the final hardware unit is available for testing. This research presents an approach in which evolutionary functional testing is performed using an actual electronic control unit for test case evaluation. A test environment designed to be used for large-scale industrial systems is introduced. An extensive case study has been carried out to assess its capabilities. Results indicate that the approach proposed in this work is suitable for automated functional testing of embedded control systems within a Hardware-in the-Loop test environment.","","978-1-4244-8341","10.1109/SSBSE.2010.22","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635149","software testing;automatic testing;road vehicle electronics;optimization methods","Testing;Optimization;Vehicles;Driver circuits;Engines;Sections;Software","automatic testing;automotive electronics;control engineering computing;embedded systems;program testing","search-based approach;functional hardware-in-the-loop testing;search-based testing principles;system under test;compiled source code;evolutionary functional testing;electronic control unit;test case evaluation;large-scale industrial systems;automated functional testing;embedded control systems;hardware-in the-loop test environment","","6","16","","","","","","IEEE","IEEE Conferences"
"A tool for combination-based prioritization and reduction of user-session-based test suites","S. Sampath; R. C. Bryce; S. Jain; S. Manchester","Information Systems, UMBC, Baltimore, MD 21250, USA; Computer Science, Utah State University, Logan, 84341, USA; Information Systems, UMBC, Baltimore, MD 21250, USA; Computer Science, Utah State University, Logan, 84341, USA","2011 27th IEEE International Conference on Software Maintenance (ICSM)","","2011","","","574","577","Test suite prioritization and reduction are two approaches to managing large test suites. They play an important role in regression testing, where a large number of tests accumulate over time from previous versions of the system. Accumulation of tests is exacerbated in user-session-based testing of web applications, where field usage data is continually logged and converted into test cases. This paper presents a tool that allows testers to easily collect, prioritize, and reduce user-session-based test cases. Our tool provides four contributions: (1) guidance to users on how to configure their web server to log important usage information, (2) automated parsing of web logs into XML formatted test cases that can be used by test replay tools, (3) automated prioritization of test cases by length-based and combinatorial-based criteria, and (4) automated reduction of test cases by combinatorial coverage.","1063-6773;1063-6773","978-1-4577-0664-6978-1-4577-0663-9978-1-4577-0662","10.1109/ICSM.2011.6080833","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080833","","XML;Web servers;Educational institutions;Databases;Engines;Software testing","combinatorial mathematics;Internet;program testing;regression analysis;XML","combinatorial-based prioritization;user-session-based test suite;test suite prioritization;test suite reduction;regression testing;Web application;field usage data;user-session-based test cases;Web server;Web log;XML formatted test case;automated prioritization;length-based criteria;combinatorial-based criteria;combinatorial coverage","","6","11","","","","","","IEEE","IEEE Conferences"
"Using genetic algorithms for test case generation and selection optimization","I. Alsmadi","Yarmouk University","CCECE 2010","","2010","","","1","4","Genetic Algorithms (GAs) are adaptive search techniques that imitate the processes of evolution to solve optimization problems when traditional methods are considered too costly in terms of processing time and output effectiveness. In This research, we will use the concept of genetic algorithms to optimize the generation of test cases from the application user interfaces. This is accomplished through encoding the location of each control in the GUI graph to be uniquely represented and forming the GUI controls' graph. After generating a test case, the binary sequence of its controls is saved to be compared with future sequences. This is implemented to ensure that the algorithm will generate a unique test case or path through the GUI flow graph every time.","0840-7789;0840-7789","978-1-4244-5376-4978-1-4244-5377","10.1109/CCECE.2010.5575262","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5575262","Test case generation;genetic algorithms;GUI controls' graph;test automation","Graphical user interfaces;Biological cells;Color;Optimization;Testing;Software;Planning","genetic algorithms;graphical user interfaces;program testing;user interfaces","genetic algorithm;test case generation;selection optimization;adaptive search techniques;application user interfaces;GUI graph","","3","11","","","","","","IEEE","IEEE Conferences"
"A Diagnostic Point of View for the Optimization of Preparation Costs in Runtime Testing","A. G. Sanchez; É. Piel; H. Gross; A. J. C. van Gemund","NA; NA; NA; NA","2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops","","2011","","","654","660","Runtime testing is emerging as the solution for the validation and acceptance testing of service-oriented systems, where many services are external to the organization, and duplicating the system's components and their context is too complex, if possible at all. In order to perform runtime tests, an additional expense in the test preparation phase is required, both in software development and in hardware. Preparation cost prioritization methods have been based on runtime testability (i.e, coverage) and do not consider whether a good runtime testability is sufficient for a good runtime diagnosis quality in case faults are detected, and whether this diagnosis will be obtained efficiently (i.e., with a low number of test cases). In this paper we show (1) the direct relationship between testability and diagnosis quality, that (2) these two properties do not guarantee an efficient diagnosis, and (3) a measurement that ensures better prediction of efficiency.","","978-0-7695-4345-1978-1-4577-0019","10.1109/ICSTW.2011.21","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954478","runtime testing;diagnosis;preparation costs","Runtime;Testing;Correlation;Authentication;Databases;Sensitivity","costing;program testing;software development management","runtime testing;service-oriented systems;acceptance testing;preparation cost prioritization method;preparation cost optimization;runtime testability;software development","","","32","","","","","","IEEE","IEEE Conferences"
"CPTEST: A Framework for the Automatic Fault Detection, Localization and Correction of Constraint Programs","N. Lazaar","NA","2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops","","2011","","","320","321","Constraint programs, such as those written in high-level constraint modeling languages, e.g., OPL (Optimization Programming Language), are more and more used in business critical programs. As any other critical programs, they require to be thoroughly tested and corrected to prevent catastrophic loss of money. This paper is a demonstrations tool of CPTEST, a first testing tool for constraint programs. In particular, the paper presents the design of CPTEST and the implementation of our approaches enabling so automatic detection, localization and correction of faults in OPL programs.","","978-0-7695-4345-1978-1-4577-0019","10.1109/ICSTW.2011.20","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954428","constraint programming;testing tool;fault localization tool;bug fixing tool","Computational modeling;Software;Programming;Optimization;Software testing;Computer languages","constraint handling;fault diagnosis;optimisation;program testing","CPTEST;automatic fault detection;constraint programs;high-level constraint modeling languages;OPL;optimization programming language;catastrophic loss;business-critical program","","1","8","","","","","","IEEE","IEEE Conferences"
"Applying Particle Swarm Optimization to estimate software effort by multiple factors software project clustering","J. Lin; H. Tzeng","Dept. of Computer Science & Engineering, Tatung University, Taipei 10451, Taiwan; Dept. of Computer Science & Engineering, Tatung University, Taipei 10451, Taiwan","2010 International Computer Symposium (ICS2010)","","2010","","","1039","1044","In the IT industry, precisely evaluate the effort of each software development project to develop cost and development schedule management to the software company in the software are count for much. Since a project, majority of development teams will feel time isn't enough to use or the project valuation be false to make the software project failed. However the cost of the software project is almost a manpower cost, manpower cost and then become a direct proportion with development schedule, so precise effort the valuation more seem to be getting more important. Consequently, this research will use Pearson product-moment correlation coefficient and one-way analyze to select several factors then used K-Means clustering algorithm to software project clustering. After project clustering, we use Particle Swarm Optimization that take mean of MRE (MMRE) as a fitness value and N-1 test method to optimization of COCOMO parameters. Finally, take parameters that finsh the optimization to calculate the software project effort that is want to estimation. This research use 63 history software projects data of COCOMO to test. The experiment really expresses using base on project clustering with multiple factors can make more effective base on effort of the estimate software of COCOMO's three project mode.","","978-1-4244-7640-4978-1-4244-7639-8978-1-4244-7638","10.1109/COMPSYM.2010.5685538","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5685538","Particle Swarm Optimization;K-Means clustering algorithm;project clustering;software effort;correlation coefficient","Software;Mathematical model;Correlation;Estimation;Analysis of variance;Clustering algorithms;Equations","particle swarm optimisation;pattern clustering;software cost estimation;software development management","particle swarm optimization;software effort estimation;software project clustering;software development project;Pearson product-moment correlation coefficient;k-means clustering algorithm;constructive cost model;COCOMO model","","6","18","","","","","","IEEE","IEEE Conferences"
"Performance Testing and Optimization of J2EE-Based Web Applications","Q. Wu; Y. Wang","NA; NA","2010 Second International Workshop on Education Technology and Computer Science","","2010","2","","681","683","J2EE-based Web applications are becoming increasingly ubiquitous and with their increasing adoption, the performance is the attention focus and the most important factor of evaluating the system by users. In this paper, we present a systematic solution for performance testing and optimization of J2EE-based Web applications. The solution helps to identify and eliminate bottlenecks in the application design and ensures that systems are designed to meet their quality of service requirements. This paper firstly analyses the architecture of J2EE-based Web applications and performance testing principle, and then improves the JMeter testing framework for meeting the more concurrent users. Lastly, performance testing for J2EE-based Web applications is done; it finds performance bottlenecks and puts forward optimum measures, and compares the performance with the former one.","","978-1-4244-6389-3978-1-4244-6388","10.1109/ETCS.2010.583","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5458557","Web applications;performance;optimization;distributed;JMeter","Application software;System testing;Service oriented architecture;Performance analysis;Scalability;Educational institutions;Nonhomogeneous media;Business;System performance;Delay","Internet;Java;program testing;software performance evaluation","performance testing;J2EE-based Web applications optimization;systematic solution;quality of service requirements;JMeter testing framework;concurrent users","","5","8","","","","","","IEEE","IEEE Conferences"
"Research the performance testing and performance improvement strategy in web application","Kunhua Zhu; Junhui Fu; Yancui Li","School of Information Engineering, Henan Institute of Science and Technology, Xinxiang, 453003, China; School of Information Engineering, Henan Institute of Science and Technology, Xinxiang, 453003, China; School of Information Engineering, Henan Institute of Science and Technology, Xinxiang, 453003, China","2010 2nd International Conference on Education Technology and Computer","","2010","2","","V2-328","V2-332","With the web services used widely in all aspects of social life, the web application performance testing is gaining wide attention. In the paper, we firstly analyses and research the types, indicators and testing methods of the performance testing of the web, and then we put forward some testing process and methods to optimize the strategy.","2155-1812","978-1-4244-6370-1978-1-4244-6367-1978-1-4244-6369","10.1109/ICETC.2010.5529374","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5529374","web performance testing;test type;load;test method","System testing;Stress;Software testing;Educational technology;Application software;Computational modeling;System performance;Computer science education;Life testing;Web server","performance evaluation;program testing;security of data;software metrics;software reliability;Web services","performance testing;performance improvement strategy;web service;social life;test type;;test indicator","","4","7","","","","","","IEEE","IEEE Conferences"
"Stress Testing an AI Based Web Service: A Case Study","A. Chakravarty","NA","2010 Seventh International Conference on Information Technology: New Generations","","2010","","","1004","1008","The stress testing of AI-based systems differs from the approach taken for more traditional Web services, both in terms of the design of test cases and the metrics used to measure quality. The expected variability in responses of an AI-based system to the same request adds a level of complexity to stress testing, when compared to more standard systems where the system response is deterministic and any deviations may easily be characterized as product defects. Generating test cases for AI-based systems requires balancing breadth of test cases with depth of response quality: most AI-systems may not return a perfect answer. An example of a machine learning translation system is considered, and the approach used for stress testing it is presented, alongside comparisons with a more traditional approach. The challenges of shipping such a system to support a growing set of features and language pairs necessitate a mature prioritization of test cases. This approach has been successful in shipping a Web service that currently serves millions of users per day.","","978-1-4244-6271-1978-1-4244-6270-4978-0-7695-3984","10.1109/ITNG.2010.149","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5501500","Internet applications;measurement techniques;software engineering;software performance testing;software testing","Artificial intelligence;Web services;System testing;Software testing;Stress measurement;Application software;Software performance;Software measurement;Machine learning algorithms;Information technology","learning (artificial intelligence);program testing;Web services","stress testing;artificial intelligence;AI-based system;Web service;machine learning translation system","","1","10","","","","","","IEEE","IEEE Conferences"
"Regression Test Selection Techniques for Test-Driven Development","H. Cibulski; A. Yehudai","NA; NA","2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops","","2011","","","115","124","Test-Driven Development (TDD) is characterized by repeated execution of a test suite, enabling developers to change code with confidence. However, running an entire test suite after every small code change is not always cost effective. Therefore, regression test selection (RTS) techniques are important for TDD. Particularly challenging for TDD is the task of selecting a small subset of tests that are most likely to detect a regression fault in a given small and localized code change. We present cost-bounded RTS techniques based on both dynamic program analysis and natural-language analysis. We implemented our techniques in a tool called Test Rank, and evaluated its effectiveness on two open-source projects. We show that using these techniques, developers can accelerate their development cycle, while maintaining a high bug detection rate, whether actually following TDD, or in any methodology that combines testing during development.","","978-0-7695-4345-1978-1-4577-0019","10.1109/ICSTW.2011.28","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954400","Development Tools;Test coverage of code;Test execution","Testing;Natural languages;Java;Correlation;Frequency measurement;Context","natural language processing;program debugging;program diagnostics;program testing;software fault tolerance","test-driven development;code change;regression test selection;regression fault;cost-bounded RTS technique;dynamic program analysis;natural-language analysis;test rank tool;development cycle;bug detection rate","","6","35","","","","","","IEEE","IEEE Conferences"
"Optimal testing resource allocation for modular software considering imperfect debugging and change point using genetic algorithm","A. G. Aggarwal; G. Kaur; P. K. Kapur","Department of Operational Research, University of Delhi, 110007, India; Department of Operational Research, University of Delhi, 110007, India; Department of Operational Research, University of Delhi, 110007, India","2010 2nd International Conference on Reliability, Safety and Hazard - Risk-Based Technologies and Physics-of-Failure Methods (ICRESH)","","2010","","","535","541","Optimal decision making is of utmost importance for planning, controlling, and working of any industry. One of the fields where mathematical modeling and optimization have been vastly applied is software reliability, which is the most significant quality metric for commercial software. Such softwares are generally modular in structure. Testing provides a mathematical measure of software reliability and is a process of raising the confidence that the software is free of flaws. However, due to the complexity of software, the testing team may not be able to remove/correct the fault perfectly on observation/detection of a failure and the original fault may remain resulting in a phenomenon known as imperfect debugging, or get replaced by another fault causing error generation. Moreover, fault detection/correction rate may not be same throughout testing; it may change at any time moment called as change point. Also, testing cannot be done indefinitely due to the availability of limited testing resources. In this paper we formulate an optimization problem to allocate the resources among different modules such that the total fault removal is maximized while incorporating the effect of both types of imperfect debugging and change point. The relative importance of each module is obtained using Analytical Hierarchy Process. We have also considered the problem of determining minimum requirement of the testing resources so that a desired proportion of faults are removed from each module. The non linear optimization problems are solved using genetic algorithm. Numerical example has been discussed to illustrate the solution of the formulated optimal resource allocation problems.","","978-1-4244-8343-3978-1-4244-8344","10.1109/ICRESH.2010.5779607","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5779607","Software Reliability Growth model;Change Point;Genetic Algorithm","Resource management","decision making;failure analysis;fault diagnosis;genetic algorithms;nonlinear programming;program debugging;program testing;resource allocation;software metrics;software performance evaluation;software quality;software reliability","optimal testing resource allocation;modular software;imperfect debugging;genetic algorithm;optimal decision making;mathematical modeling;software reliability;quality metric;commercial software;mathematical measure;software complexity;testing team;failure observation;failure detection;fault causing error generation;fault correction rate;change point;fault removal;analytical hierarchy process;testing resources;nonlinear optimization problems;optimal resource allocation problems","","1","28","","","","","","IEEE","IEEE Conferences"
"Software reliability modelling and optimization for multi-release software development processes","Q. P. Hu; R. Peng; M. Xie; S. H. Ng; G. Levitin","Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Beijing, China; Department of Industrial and Systems Engineering, National University of Singapore, Singapore; Department of Industrial and Systems Engineering, National University of Singapore, Singapore; Department of Industrial and Systems Engineering, National University of Singapore, Singapore; Reliability &amp; Equipment Department, The Israel Electric Corporation Ltd, Haifa, Israel","2011 IEEE International Conference on Industrial Engineering and Engineering Management","","2011","","","1534","1538","During the lifespan of large software systems, iterative development procedure is commonly adopted with continuously incremental software versions released to the market. When to release each release plays an important role for balancing the competition in market and the risk of low-quality software. Traditionally, release-time issue is addressed with software reliability models for single version. How to model the reliability of multi-release software development process is our concern now. It is interesting to study the dynamics of software faults during this releasing procedure. Without the loss of generality, a specific iterative software development scenario is considered for our current study, where a software development team develops, tests and releases software version by version. The trend of the remaining number of faults over different versions is of great concern, and a modeling framework is proposed to study the expected number of remaining faults in each version. Cost model is then proposed, and cost oriented optimal release time analysis for n versions is studied. A numerical example is given for the illustrative purpose.","2157-362X;2157-3611;2157-3611","978-1-4577-0739-1978-1-4577-0740-7978-1-4577-0738","10.1109/IEEM.2011.6118174","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6118174","Software reliability;multiple releases;single release;release time","Software reliability;Testing;Programming;Cost function;Software systems","iterative methods;software cost estimation;software reliability","software reliability modelling;software reliability optimization;multirelease software development processes;software systems;iterative development procedure;incremental software versions;market competition balancing;low-quality software risk;release-time issue;software development team;cost oriented optimal release time analysis","","7","17","","","","","","IEEE","IEEE Conferences"
"µTIL: Mutation-based Statistical Test Inputs Generation for Automatic Fault Localization","M. Delahaye; L. C. Briand; A. Gotlieb; M. Petit","NA; NA; NA; NA","2012 IEEE Sixth International Conference on Software Security and Reliability","","2012","","","197","206","Automatic Fault Localization (AFL) is a process to locate faults automatically in software programs. Essentially, an AFL method takes as input a set of test cases including failed test cases, and ranks the statements of a program from the most likely to the least likely to contain a fault. As a result, the efficiency of an AFL method depends on the ""quality"" of the test cases used to rank statements. More specifically, in order to improve the accuracy of their ranking within test budget constraints, we have to ensure that program statements are executed by a reasonably large number of test cases which provide a coverage as uniform as possible of the input domain. This paper proposes μTIL, a new statistical test inputs generation method dedicated to AFL, based on constraint solving and mutation testing. Using mutants where the locations of injected faults are known, μTIL is able to significantly reduce the length of an AFL test suite while retaining its accuracy (i.e., the code size to examine before spotting the fault). In order to address the motivations stated above, the statistical generator objectives are two-fold: 1) each feasible path of the program is activated with the same probability, 2) the sub domain associated to each feasible path is uniformly covered. Using several widely used ranking techniques (i.e., Tarantula, Jaccard, Ochiai), we show on a small but realistic program that a proof-of-concept implementation of μTIL can generate test sets with significantly better fault localization accuracy than both random testing and adaptive random testing. We also show on the same program that using mutation testing enables a 75% length reduction of the AFL test suite without decrease in accuracy.","","978-1-4673-2067-2978-0-7695-4742","10.1109/SERE.2012.32","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6258309","Random testing;Automatic Fault Localization;Mutation analysis","Accuracy;Generators;Java;Software;Software testing;Debugging","program testing;software fault tolerance;statistical testing","μTIL;mutation-based statistical test inputs generation;automatic fault localization;software programs;test budget constraints;program statements;constraint solving;mutation testing;statistical generator;ranking techniques","","","35","","","","","","IEEE","IEEE Conferences"
"Literature survey of Ant Colony Optimization in software testing","B. Suri; S. Singhal","Comp Sc USIT, GGSIPU; IT JIMS, Rohini, GGSIPU","2012 CSI Sixth International Conference on Software Engineering (CONSEG)","","2012","","","1","7","Ant Colony Optimization (ACO) is a well known and rapidly evolving meta-heuristic technique. A large number of optimization problems have already taken advantage of the ACO technique while countless others are on their way. A copious amount of effort has also been put in by the researchers for applying ACO in solving various software testing problems. This paper presents a survey of twenty-one such studies, identified as relating to the use of ACO in diverse software testing concepts. To the best of our knowledge, no literature survey could be found published in the same context till date. Consequently, the twenty one studies have been rigorously analyzed to find some common parameters which can be grouped together or compared in order to provide a useful insight into the field.","","978-1-4673-2177-8978-1-4673-2174-7978-1-4673-2175-4978-1-4673-2176","10.1109/CONSEG.2012.6349501","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6349501","","Software testing;Unified modeling language;Optimization;Software;Flow graphs;Data models","ant colony optimisation;program testing","literature survey;ant colony optimization;meta-heuristic technique;ACO technique;software testing problems","","7","30","","","","","","IEEE","IEEE Conferences"
"Dynamic Fault Visualization Tool for Fault-based Testing and Prioritization","P. Daniel; K. Y. Sim","NA; NA","2012 International Conference on Advanced Computer Science Applications and Technologies (ACSAT)","","2012","","","301","306","Fault-based testing has been proven to be a cost effective testing technique for software logics and rules expressed in Boolean expressions. It can guarantee the elimination of common faults without exhaustive testing. However, average software testing practitioners may not have in-depth knowledge on Boolean algebra and complex logic derivations required to apply existing fault-based testing techniques. In this paper, a dynamic fault visualization tool has been proposed. This tool allows its user to visualize fault-based testing and prioritize test inputs with a simple greedy method. The performance evaluation of this tool has been done on Boolean expressions extracted from a real life aviation tool. The results show that it can achieve significant performance improvements compared to ordinary sequential order test execution and existing static technique. The proposed visualization tool could also identify possible faults to guide the debugging process.","","978-0-7695-4959-0978-1-4673-5832","10.1109/ACSAT.2012.55","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6516370","Software Testing;Fault-based Testing;Fault Visualization","","data visualisation;program debugging;program testing;software fault tolerance","dynamic fault visualization tool;fault-based testing;fault-based prioritization;software logic;software rule;Boolean expression;Boolean algebra;complex logic derivation;greedy method;aviation tool;sequential order test execution;static technique;debugging process","","1","14","","","","","","IEEE","IEEE Conferences"
"Point-of-Interest Aware Test Case Prioritization: Methods and Experiments","K. Zhai; W. K. Chan","NA; NA","2010 10th International Conference on Quality Software","","2010","","","449","456","Location based services personalize their behaviors based on location data. When data kept by a service have evolved or the code has been modified, regression testing can be employed to assure the quality of services. Frequent data update however may lead to frequent regression testing and any faulty implementation of a service may affect many service consumers. Proper test case prioritization helps reveal service problems efficiently. In this paper, we review a set of point-of-interest (POI) aware test case prioritization techniques and report an experiment on such techniques. The empirical results show that these POI-aware techniques are more effective than random ordering and input-guided test case prioritization in terms of APFD. Furthermore, their effectiveness is observed to be quite stable over different sizes of the test suite.","2332-662X;1550-6002;1550-6002","978-1-4244-8078-4978-0-7695-4131","10.1109/QSIC.2010.63","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5563000","test case prioritization;location-based service","Global Positioning System;Measurement;Entropy;Testing;Book reviews;Cities and towns;Software","mobile computing;program testing;regression analysis","point-of-interest aware test case prioritization;location based services;quality of services;frequent regression testing;faulty implementation;POI","","1","29","","","","","","IEEE","IEEE Conferences"
"Sophisticated Testing of Concurrent Programs","Z. Letko","NA","2nd International Symposium on Search Based Software Engineering","","2010","","","36","39","Search-based techniques were successfully applied to many different areas of testing but according to our knowledge there are no works that applies search-based techniques to testing of concurrent software, yet. This PhD paper describes plans and already achieved preliminary results with applying search-based techniques to testing of concurrent software. In particular, we plan to combine noise injection techniques for testing of concurrent software, various concurrency coverage measures, and several dynamic analyses with search-based optimization techniques.","","978-1-4244-8341","10.1109/SSBSE.2010.14","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635173","testing;concurrency;dynamic analysis;coverage analysis","Testing;Noise;Concurrent computing;Synchronization;Computer bugs;Software;Java","parallel programming;program testing;search problems","concurrent program testing;concurrent software testing;noise injection techniques;search-based optimization techniques","","","13","","","","","","IEEE","IEEE Conferences"
"Supporting Concern-Based Regression Testing and Prioritization in a Model-Driven Environment","R. S. S. Filho; C. J. Budnik; W. M. Hasling; M. McKenna; R. Subramanyan","NA; NA; NA; NA; NA","2010 IEEE 34th Annual Computer Software and Applications Conference Workshops","","2010","","","323","328","Traditional regression testing and prioritization approaches are bottom-up (or white-box). They rely on the analysis of the impact of changes in source code artifacts, identifying corresponding parts of software to retest. While effective in minimizing the amount of testing required to validate code changes, they do not leverage on specification-level design and requirements concerns that motivated these changes. Model-based testing approaches support a top-down (or black box) testing approach, where design and requirements models are used in support of test generation. They augment code-based approaches with the ability to test from a higher-level design and requirements perspective. In this paper, we present a model-based regression testing and prioritization approach that efficiently selects test cases for regression testing based on different concerns. It relies on traceability links between models, test cases and code artifacts, together with user-defined properties associated to model elements. In particular we describe how to support concern-based regression testing and prioritization using TDE/UML, an extensible model-based testing environment.","","978-1-4244-8089-0978-0-7695-4105","10.1109/COMPSACW.2010.63","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5615818","Model-driven testing;test development;regression testing;test prioritization","","program testing;regression analysis","supporting concern based regression testing;model driven environment;source code artifacts;specification level design;model based testing approaches;code based approaches;user defined properties;UML","","6","17","","","","","","IEEE","IEEE Conferences"
"A Reliability Verification Test Model Based on Hybrid Bayesian Prior Distribution","F. Gao; X. Zheng; C. Liu","NA; NA; NA","2012 Fifth International Joint Conference on Computational Sciences and Optimization","","2012","","","770","774","Using advantages of priori Bayesian method, a reliability verification test method based on Hybrid Bayesian Prior Distribution was brought forward. The prior distribution of unknown parameters can be obtained by using conjugate prior distribution method. Prior moment method and Maximum entropy method were used respectively to calculate two different groups of parameters, and then two different prior distributions can be obtained. Then confidence factors of the two prior distributions were determined by using the second category maximum likelihood method, and the final distribution can be got by integrating there two group of parameters according to their weight. Instance proved that the prior distribution obtained by this method is more accurate, and can fit better with the real distribution.","","978-1-4673-1365-0978-0-7695-4690","10.1109/CSO.2012.173","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6274837","Priori Bayesian model;reliability verification testing;Prior moment method;Maximum entropy method;Parameter integration","Entropy;Software reliability;Bayesian methods;Moment methods;Software;Testing","Bayes methods;maximum entropy methods;maximum likelihood estimation;program testing;program verification;software reliability;statistical distributions","reliability verification test model;hybrid Bayesian prior distribution;priori Bayesian method;reliability verification test method;unknown parameters;conjugate prior distribution method;prior moment method;maximum entropy method;confidence factors;maximum likelihood method","","1","10","","","","","","IEEE","IEEE Conferences"
"Evolutionary generation of test data for path coverage with faults detection","Y. Zhang; D. Gong; Y. Luo","School of Information and Electrical Engineering, China University of Mining and Technology, Xuzhou, Jiangsu, China; School of Information and Electrical Engineering, China University of Mining and Technology, Xuzhou, Jiangsu, China; School of Information and Electrical Engineering, China University of Mining and Technology, Xuzhou, Jiangsu, China","2011 Seventh International Conference on Natural Computation","","2011","4","","2086","2090","The aim of software testing is to find faults in the program under test. Previous methods of path-oriented test data generation can generate test data traversing target paths, but they may not guarantee to find faults in the program. We present a method of evolutionary generation of test data for path coverage with faults detection in this paper. First, we establish a mathematical model of the problem considered in this paper, in which the number of faults detected in the path traversed by test data, and the risk level of faults are optimization objectives, and the approach level of the traversed path from the target one is a constraint. Then, we generate test data using a multi-objective evolutionary optimization algorithm with constraints. Finally, we apply the proposed method in a benchmark program bubble sort and an industrial program totinfo, and compare it with the traditional method. The experimental results conform that our method can generate test data that not only traverse the target path but also detect faults in it. Our achievement provides a novel way to generate test data for path coverage with faults detection.","2157-9563;2157-9555;2157-9555","978-1-4244-9953-3978-1-4244-9950-2978-1-4244-9952","10.1109/ICNC.2011.6022397","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6022397","software testing;path coverage;faults detection;test data;multi-objective;evolutionary optimization","Fault detection;Optimization;Frequency division multiplexing;Software testing;Mathematical model;Instruments;Educational institutions","evolutionary computation;mathematical analysis;program testing;software fault tolerance","fault detection;software testing;program under test;path-oriented test data generation;data traversing target paths;mathematical model;risk level;multiobjective evolutionary optimization algorithm;totinfo program","","1","10","","","","","","IEEE","IEEE Conferences"
"On the role of diversity measures for multi-objective test case selection","A. De Lucia; M. Di Penta; R. Oliveto; A. Panichella","University of Salerno, via Ponte don Melillo, Fisciano (SA), 84084, Italy; University of Sannio, Palazzo ex Poste, Via Traiano, 82100 Benevento, Italy; University of Molise, Contrada Fonte Lappone, 86090 Pesche (IS), Italy; University of Salerno, via Ponte don Melillo, Fisciano (SA), 84084, Italy","2012 7th International Workshop on Automation of Software Test (AST)","","2012","","","145","151","Test case selection has been recently formulated as multi-objective optimization problem trying to satisfy conflicting goals, such as code coverage and computational cost. This paper introduces the concept of asymmetric distance preserving, useful to improve the diversity of non-dominated solutions produced by multi-objective Pareto efficient genetic algorithms, and proposes two techniques to achieve this objective. Results of an empirical study conducted over four programs from the SIR benchmark show how the proposed technique (i) obtains non-dominated solutions having a higher diversity than the previously proposed multi-objective Pareto genetic algorithms; and (ii) improves the convergence speed of the genetic algorithms.","","978-1-4673-1822-8978-1-4673-1821","10.1109/IWAST.2012.6228983","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6228983","Search-based Software Testing;Test Case Selection;Niched Genetic Algorithms;Empirical Studies","Genetic algorithms;Optimization;Minimization;Testing;Convergence;Measurement;Search problems","convergence;genetic algorithms;Pareto optimisation;program testing","diversity measure;multiobjective test case selection;multiobjective optimization problem;conflicting goal satisfaction;code coverage;computational cost;asymmetric distance preserving;multiobjective Pareto efficient genetic algorithm;SIR benchmark;nondominated solution;convergence speed;software testing","","2","20","","","","","","IEEE","IEEE Conferences"
"Graphical analysis of MC/DC using automated software testing","P. Mitra; S. Chatterjee; N. Ali","Department of IT, RCC Instt. of Information Technology, (West Bengal University of Technology), Kolkata-700015, India; Department of IT, RCC Instt. of Information Technology, (West Bengal University of Technology), Kolkata-700015, India; Department of IT, RCC Instt. of Information Technology, (West Bengal University of Technology), Kolkata-700015, India","2011 3rd International Conference on Electronics Computer Technology","","2011","3","","145","149","Testing has traditionally been one of the main techniques contributing to high software dependability and quality. Each stage in software development has a corresponding testing activity. Testing activity consumes about 50% of software development resources, so any technique aiming at reducing software-testing costs is likely to reduce software development costs. Proposed by NASA in 1994, the Modified Condition/Decision Coverage (MC/DC) criterion is a testing strategy required, among other practices, by the RTCA DO-178B. MC/DC is a white box testing criterion aiming at proving evidence that all clauses involved in a predicate can influence the predicate value in the required way. It subsumes other well-known coverage criteria such as statement and decision coverage. Our work involves a thorough study of the MC/DC criterion and our approach is organized in the following way. We automate the generation of number of test cases required to satisfy the MC/DC criterion. To this end we develop a generic algorithm that would for any given predicate, compute the minimum number of test cases that would cover the MC/DC criterion for this predicate.","","978-1-4244-8679-3978-1-4244-8678","10.1109/ICECTECH.2011.5941819","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5941819","testing;white box;modified condition;cfg;control flow graph;adjacency matrix;complexity","Flow graphs;Arrays;Complexity theory;Software;Software testing;Optimization","cost reduction;program testing;software development management","graphical analysis;automated software testing;software dependability;software development resource;software development cost reduction;NASA;Modified Condition/Decision Coverage criterion;MC/DC criterion;RTCA DO-178B;white box testing criterion;generic algorithm","","1","6","","","","","","IEEE","IEEE Conferences"
"On the Fault-Detection Capabilities of Adaptive Random Test Case Prioritization: Case Studies with Large Test Suites","Z. Q. Zhou; A. Sinaga; W. Susilo","NA; NA; NA","2012 45th Hawaii International Conference on System Sciences","","2012","","","5584","5593","An adaptive random (AR) testing strategy has recently been developed and examined by a growing body of research. More recently, this strategy has been applied to prioritizing regression test cases based on code coverage using the concepts of Jaccard Distance (JD) and Coverage Manhattan Distance (CMD). Code coverage, however, does not consider frequency, furthermore, comparison between JD and CMD has not yet been made. This research fills the gap by first investigating the fault-detection capabilities of using frequency information for AR test case prioritization, and then comparing JD and CMD. Experimental results show that ""coverage"" was more useful than ""frequency"" although the latter can sometimes complement the former, and that CMD was superior to JD. It is also found that, for certain faults, the conventional ""additional"" algorithm (widely accepted as one of the best algorithms for test case prioritization) could perform much worse than random testing on large test suites.","1530-1605;1530-1605","978-1-4577-1925-7978-0-7695-4525","10.1109/HICSS.2012.454","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6149572","","Subspace constraints;Frequency measurement;Software;Power capacitors;Software testing;Educational institutions","fault diagnosis;program testing","fault-detection capabilities;adaptive random test case prioritization;adaptive random testing strategy;code coverage;Jaccard distance concept;coverage Manhattan distance;frequency information;AR test case prioritization","","10","29","","","","","","IEEE","IEEE Conferences"
"Pragmatic prioritization of software quality assurance efforts","E. Shihab","Queen's University, Kingston, ON, Canada","2011 33rd International Conference on Software Engineering (ICSE)","","2011","","","1106","1109","A plethora of recent work leverages historical data to help practitioners better prioritize their software quality assurance efforts. However, the adoption of this prior work in practice remains low. In our work, we identify a set of challenges that need to be addressed to make previous work on quality assurance prioritization more pragmatic. We outline four guidelines that address these challenges to make prior work on software quality assurance more pragmatic: 1) Focused Granularity (i.e., small prioritization units), 2) Timely Feedback (i.e., results can be acted on in a timely fashion), 3) Estimate Effort (i.e., estimate the time it will take to complete tasks), and 4) Evaluate Generality (i.e., evaluate findings across multiple projects and multiple domains). We present two approaches, at the code and change level, that demonstrate how prior approaches can be more pragmatic.","1558-1225;0270-5257","978-1-4503-0445-0978-1-4503-0445","10.1145/1985793.1986007","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032601","change risk;software metrics;unit testing","Software;Pragmatics;Measurement;Testing;Guidelines;Data mining;History","computational linguistics;software metrics;software quality","software quality assurance;pragmatic prioritization","","","18","","","","","","IEEE","IEEE Conferences"
"Empirical Investigation of the Effects of Test Suite Properties on Similarity-Based Test Case Selection","H. Hemmati; A. Arcuri; L. Briand","NA; NA; NA","2011 Fourth IEEE International Conference on Software Testing, Verification and Validation","","2011","","","327","336","Our experience with applying model-based testing on industrial systems showed that the generated test suites are often too large and costly to execute given project deadlines and the limited resources for system testing on real platforms. In such industrial contexts, it is often the case that only a small subset of test cases can be run. In previous work, we proposed novel test case selection techniques that minimize the similarities among selected test cases and outperforms other selection alternatives. In this paper, our goal is to gain insights into why and under which conditions similarity-based selection techniques, and in particular our approach, can be expected to work. We investigate the properties of test suites with respect to similarities among fault revealing test cases. We thus identify the ideal situation in which a similarity-based selection works best, which is useful for devising more effective similarity functions. We also address the specific situation in which a test suite contains outliers, that is a small group of very different test cases, and show that it decreases the effectiveness of similarity-based selection. We then propose, and successfully evaluate based on two industrial systems, a solution based on rank scaling to alleviate this problem.","2159-4848","978-1-61284-174-8978-0-7695-4342","10.1109/ICST.2011.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770622","Test Case Selection;Similarity Measure;Distance Function;Adaptive Random Testing;Genetic Algorithms;Model Based Testing","Testing;Unified modeling language;Subspace constraints;Fault detection;Gallium;Context;Encoding","program testing","test suite properties;similarity-based test case selection technique;model-based testing;industrial systems;rank scaling","","14","19","","","","","","IEEE","IEEE Conferences"
"Activity Page Based Functional Test Automation for Android Application","L. Lu; Y. Hong; Y. Huang; K. Su; Y. Yan","NA; NA; NA; NA; NA","2012 Third World Congress on Software Engineering","","2012","","","37","40","Current status of mobile application testing is commonly used artificial test. In order to reduce the test costs and improve the availability and system stability of the android mobile applications, through an activity page based model to optimize the page crawling algorithm to generate a test case sequence, combined with some open source tools and frameworks, this paper proposed a low cost practical functional test automation for Android applications.","","978-1-4673-4546","10.1109/WCSE.2012.15","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6394920","Android test;functional test;test case;test automation","Software engineering","mobile computing;mobile handsets;operating system kernels;program testing","activity page based functional test automation;mobile application testing;artificial test;system stability;android mobile applications;page crawling algorithm;activity page based model;test case sequence;open source tools;open source framework","","6","9","","","","","","IEEE","IEEE Conferences"
"Automatic Generation of On-Line Test Programs through a Cooperation Scheme","L. M. Ciganda; M. Gaudesi; E. Lutton; E. Sanchez; G. Squillero; A. Tonda","NA; NA; NA; NA; NA; NA","2012 13th International Workshop on Microprocessor Test and Verification (MTV)","","2012","","","13","18","Test programs for Software-based Self-Test (SBST) can be exploited during the mission phase of microprocessor-based systems to periodically assess hardware integrity. However, several additional constraints must be imposed due to the coexistence of test programs with the mission application. This paper proposes a method for the generation of SBST on-line test programs for embedded RISC processors, systems where the impact of on-line constraints is significant. The proposed strategy exploits an evolutionary optimizer that is able to create a complete test set of programs relying on a new cooperative scheme. Experimental results showed high fault coverage values on two different modules of a MIPS-like processor core. These two case studies demonstrate the effectiveness of the technique and the low human effort required for its implementation.","1550-4093;2332-5674","978-0-7695-4877-7978-1-4673-4441","10.1109/MTV.2012.17","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6519728","SoC;pipelined processors;on-line testing;software-based self-test;Group Evolution","","automatic test software;embedded systems;multiprocessing systems;optimising compilers;program testing;reduced instruction set computing","automatic online test program generation;cooperation scheme;software-based self-test;SBST;microprocessor-based system mission phase;hardware integrity;embedded RISC processors;online constraints;optimizer;fault coverage values;MIPS-like processor core","","","15","","","","","","IEEE","IEEE Conferences"
"Impact of Software Optimization on Variable Lifetimes in a Microprocessor-Based System","S. Bergaoui; R. Leveugle","NA; NA","2011 Sixth IEEE International Symposium on Electronic Design, Test and Application","","2011","","","56","61","Assessing and improving the level of system robustness with respect to soft errors has become one of the main design challenges, especially when designing critical embedded systems. In systems using microprocessors (e.g. most of SoCs) the system dependability is strongly correlated with the variable lifetimes. In this paper, we discuss the impact of compilation optimizations on the lifetimes in both memory blocks and internal processor registers. Evaluations on many significant examples show that specific optimization options are required when the reliability is more important than execution time or code size.","","978-1-4244-9357-9978-0-7695-4306","10.1109/DELTA.2011.20","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5729540","Critical variables;microprocessor-based systems;Software Optimizations;Variable lifetimes","Registers;Optimization;Benchmark testing;Robustness;Software;Measurement;Resource management","optimising compilers;software engineering;system-on-chip","software optimization;microprocessor-based system;system-on-chips;microprocessor variable lifetimes;compilation optimization impact;internal processor registers;memory blocks","","1","11","","","","","","IEEE","IEEE Conferences"
"The limitations of genetic algorithms in software testing","S. H. Aljahdali; A. S. Ghiduk; M. El-Telbany","College of Computers and Information Sys, Taif University, Saudi Arabia; College of Computers and Information Sys, Taif University, Saudi Arabia; College of Computers and Information Sys, Taif University, Saudi Arabia","ACS/IEEE International Conference on Computer Systems and Applications - AICCSA 2010","","2010","","","1","7","Software test-data generation is the process of identifying a set of data, which satisfies a given testing criterion. For solving this difficult problem there were a lot of research works, which have been done in the past. The most commonly encountered are random test-data generation, symbolic test-data generation, dynamic test-data generation, and recently, test-data generation based on genetic algorithms. This paper gives a survey of the majority of software test-data generation techniques based on genetic algorithms. It compares and classifies the surveyed techniques according to the genetic algorithms features and parameters. Also, this paper shows and classifies the limitations of these techniques.","2161-5322;2161-5330","978-1-4244-7717-3978-1-4244-7716-6978-1-4244-7715","10.1109/AICCSA.2010.5586984","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5586984","genetic algorithms;software testing","Software testing;Biological cells;Software;Optimization;Classification algorithms;Genetics","genetic algorithms;program testing","software testing;genetic algorithm;software test data generation;random test data generation;symbolic test data generation;dynamic test data generation","","4","40","","","","","","IEEE","IEEE Conferences"
"An optimal combination test suite construction method","H. Chen; S. Wang; X. Pan","School of Computer Science and Technology, Xi'an University of Posts &amp; Telecommunications 710121, China; School of Computer Science and Technology, Xi'an University of Posts &amp; Telecommunications 710121, China; School of Computer Science and Technology, Xi'an University of Posts &amp; Telecommunications 710121, China","2012 8th International Conference on Natural Computation","","2012","","","649","653","Test data generation is a key problem of combination software testing. However, the complexity of test case generation problem for combinatorial testing is NP-complete. In this study, we propose a global optimization and generation method to construct combinatorial testing data. Firstly, an encoding mechanism is used to map the combinatorial testing problem domain to a binary coding space. Then, an improving ethnic group evolution algorithm (EGEA/H) is used to search the binary coding space in order to find the optimal code schema. After that, a decoding mechanism is used to read out the composition information of combinatorial testing data from the optimal code schema and construct optimal combinatorial testing suite according to it. The simulation results show this method is simple and effective, and it has the characteristics of less producing test data and time consumption.","2157-9563;2157-9555;2157-9555","978-1-4577-2133-5978-1-4577-2130-4978-1-4577-2132","10.1109/ICNC.2012.6234766","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6234766","combinatorial testing;test case generation;optimal combinatorial test suite;hierarchical ethnic group clustering;ethnic group evolution algorithm","Encoding;Optimization;Decoding;Software;Software testing;Clustering algorithms","combinatorial mathematics;evolutionary computation;optimisation;program testing","optimal combination test suite construction method;test data generation;combination software testing;test case generation problem;NP-complete;global optimization;combinatorial testing data;encoding mechanism;binary coding space;ethnic group evolution algorithm;EGEA/H;optimal code schema;decoding mechanism","","1","19","","","","","","IEEE","IEEE Conferences"
"Analysis of Software Testing Strategies Through Attained Failure Size","B. Zachariah","Chh. Shahu Institute of Business Education and Research, Kolhapur, India","IEEE Transactions on Reliability","","2012","61","2","569","579","This paper discusses efficacy issues in software testing strategies through attained failure size. Failure size is the probability of finding an input that causes a failure in the input domain. As testing progresses, failure size decreases due to debugging. The failure size at the termination of testing is called the attained failure size. Using this measure, we compare the efficacies of partition testing and random testing, derive conditions that lead to the superiority of partition testing, and obtain optimal time allocations in partition testing. The core findings are presented in a decision tree to assist testers in test management.","0018-9529;1558-1721","","10.1109/TR.2012.2194195","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6198319","Attained failure size;non linear programming;optimization;partition testing;random testing;testing strategies","Zirconium;Software;Software testing;Time measurement;Software measurement;Reliability","decision trees;failure analysis;program debugging;program testing","software testing;attained failure size;probability;debugging;decision tree;test management;partition testing;random testing","","3","22","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic generation of test data for path testing by adaptive genetic simulated annealing algorithm","Bo Zhang; Chen Wang","Department of Fire Engineering, The Chinese People's Armed Police Forced Academy, Langfang, Hebei Province, China; Department of Fire Engineering, The Chinese People's Armed Police Forced Academy, Langfang, Hebei Province, China","2011 IEEE International Conference on Computer Science and Automation Engineering","","2011","2","","38","42","Software testing has become an important stage of the software developing process in recent years, and it is crucial element of software quality assurance. Path testing has become one of the most important unit test methods, and it is a typical white box test. The generation of testing data is one of the key steps which have a great effect on the automation of software testing. GA is adaptive heuristic search algorithm premised on the evolutionary ideas of natural selection and genetic. Because it is a robust search method requiring little information to search effectively in a large or poorly-understood search space, it is widely used to search and optimize, and also can be used to generate test data. In this article we put the anneal mechanism of the Simulated Anneal Algorithm into the genetic algorithm to decide to accept the new individuals or not, and we import dynamic selections to adaptive select individuals which can be copied to next generation. Adaptive crossover probability, adaptive mutation probability and elitist preservation ensure that the best individuals can not be destroyed. The experiment results show that adaptive genetic simulated annealing algorithm is superior to genetic algorithm in effectiveness and efficiency.","","978-1-4244-8728-8978-1-4244-8727-1978-1-4244-8726","10.1109/CSAE.2011.5952418","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5952418","test data;path test;AGSAA;GA","","genetic algorithms;probability;program testing;search problems;simulated annealing;software quality","automatic test data generation;path testing;adaptive genetic simulated annealing algorithm;software testing;software developing process;software quality assurance;unit test method;white box test;adaptive heuristic search algorithm;robust search method;dynamic selection;adaptive crossover probability;adaptive mutation probability;elitist preservation","","2","14","","","","","","IEEE","IEEE Conferences"
"A Manifesto for Higher Order Mutation Testing","M. Harman; Y. Jia; W. B. Langdon","NA; NA; NA","2010 Third International Conference on Software Testing, Verification, and Validation Workshops","","2010","","","80","89","We argue that higher order mutants are potentially better able to simulate real faults and to reveal insights into bugs than the restricted class of first order mutants. The Mutation Testing community has previously shied away from Higher Order Mutation Testing believing it to be too expensive and therefore impractical. However, this paper argues that Search Based Software Engineering can provide a solution to this apparent problem, citing results from recent work on search based optimization techniques for constructing higher order mutants. We also present a research agenda for the development of Higher Order Mutation Testing.","","978-1-4244-6774-7978-1-4244-6773-0978-0-7695-4050","10.1109/ICSTW.2010.13","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463719","Higher Order Mutation Testing","Genetic mutations;Software testing;Software engineering;Educational institutions;Computer bugs;Fault detection;Helium","optimisation;program testing;search problems;software engineering","higher order mutation testing;real faults;search based software engineering;search based optimization technique","","23","62","","","","","","IEEE","IEEE Conferences"
"Automated GUI Test Coverage Analysis Using GA","A. Rauf; S. Anwar; M. A. Jaffer; A. A. Shahid","NA; NA; NA; NA","2010 Seventh International Conference on Information Technology: New Generations","","2010","","","1057","1062","A Graphical User Interface (GUI) is a graphical front-end to a software system. A GUI contains graphical objects with certain distinct values which can be used to determine the state of the GUI at any time. Software developing organizations always desire to test the software thoroughly to get maximum confidence about its quality. But this requires gigantic effort to test a GUI application due to the complexity involved in such applications. This problem has led to the automation of GUI testing and different techniques have been proposed for automated GUI Testing. Event-flow graph is a fresh technique being used in the field of automated GUI testing. Just as control-flow graph, another GUI model that represents all possible execution paths in a program, event-flow model, in the same way, represents all promising progressions of events that can be executed on the GUI. Another challenging question in software testing is, “How much testing is enough?” As development proceeds, there are fewer measures available that can be used to provide guidance on the quality of an automatic test suite. Genetic algorithm searches for the best possible test parameter combinations that are according to some predefined test criterion. Usually this test criterion corresponds to a “coverage function” that measures how much of the automatically generated optimization parameters satisfies the given test criterion. In this paper, we have attempted to exploit the event driven nature of GUI. Based on this nature, we have presented a GUI testing and coverage analysis technique centered on genetic algorithms.","","978-1-4244-6271-1978-1-4244-6270-4978-0-7695-3984","10.1109/ITNG.2010.95","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5501493","GUI Testing;Genetic Algorithm;Coverage Criterion;Coverage Analysis;Event Flow;Test Data Generation;Test Path;Automation Testing","Graphical user interfaces;Automatic testing;Software testing;Software quality;Application software;Genetic algorithms;System testing;Software systems;Automation;Automatic control","data analysis;genetic algorithms;graph theory;graphical user interfaces;program testing","GUI test coverage analysis;graphical user interface;genetic algorithms;event-flow graph;control-flow graph;software testing;test parameter combinations;coverage function","","12","32","","","","","","IEEE","IEEE Conferences"
"Novel fuzzy approach for ranking test vectors","G. Takacs","&#x00D3;buda University, Doctoral School of Applied Informatics, B&#x00E9;csi &#x00FA;t 96/b, 1034 Budapest, Hungary","2012 7th IEEE International Symposium on Applied Computational Intelligence and Informatics (SACI)","","2012","","","395","398","Software testing is essential part of the software development especially when the software has safety critical classification. In this paper authors introduce a method for select the right values and ranges for unit and functional testing. The key feature of this novel approach is that, test vectors can be ranked before selection in order to gain test time or quality. Fuzzy set theory is used for the ranking of the test sets. The introduced method presented on a example code, which helps the understanding of its basic principals. The authors take notes on the implementation of the test vector generation. Directions of further research are also discussed.","","978-1-4673-1013-0978-1-4673-1012","10.1109/SACI.2012.6250036","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6250036","","Support vector machine classification;Aircraft","fuzzy set theory;program testing","test vectors ranking;software testing;software development;unit testing;functional testing;fuzzy set theory;test sets","","","11","","","","","","IEEE","IEEE Conferences"
"Software and Communication of Fully Automatic Optimizing Cross-Cut saw","L. Xiaochun; D. Qingxin; Z. Honglin; Z. Shimin; X. Jiaxing; S. Guangbin","NA; NA; NA; NA; NA; NA","2010 International Conference on Electrical and Control Engineering","","2010","","","5603","5607","Software and communication is a core of fully automatic optimizing cross-cut saw (FAOCCS). As for the realization of software, principles and methods are presented for the realization of optimizing software and motion control system (MCS) software; and for the realization of communication, the method of UDP communication between IPC and MCS is presented. Then, the preparation of corresponding optimizing, MCS and communication software is illuminated. Finally, synthetic test on the optimizing software and MCS software as well as the UDP communication between IPC and MCS is conducted through some examples. The test results indicate that FAOCCS using above technologies offers superior performance compared with the same type of FAOCCS from foreign countries, in particular the optimizing software has better functions.","","978-1-4244-6881-2978-1-4244-6880-5978-0-7695-4031","10.1109/iCECE.2010.1361","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5631600","Optimizing software;Motion control system;Motion control system software;UDP Communication","Software;Petroleum;Integrated optics;Motion control;Software algorithms;IP networks;Visualization","control engineering computing;motion control;sawing machines","fully automatic optimizing cross-cut saw;FAOCCS;motion control system software;UDP communication;IPC;MCS;communication software","","","","","","","","","IEEE","IEEE Conferences"
"Approximating quality contracts for energy auto-tuning software","S. Götz; C. Wilke; S. Richly; U. Aßmann","Software Technology Group, Department of Computer Science, Technische Universität Dresden, D-01062 Dresden, Germany; Software Technology Group, Department of Computer Science, Technische Universität Dresden, D-01062 Dresden, Germany; Software Technology Group, Department of Computer Science, Technische Universität Dresden, D-01062 Dresden, Germany; Software Technology Group, Department of Computer Science, Technische Universität Dresden, D-01062 Dresden, Germany","2012 First International Workshop on Green and Sustainable Software (GREENS)","","2012","","","8","14","An emerging trend for future software systems is self-optimization, especially w.r.t. energy efficiency. Models of soft- and hardware components at runtime, expressing current and alternative system configurations, are exploited to improve service utility as well as to decrease energy consumption. In recent work we showed how quality contracts - expressing dependencies between software and hardware components - can be used for energy auto-tuning. Notably, the declared provisions and requirements of individual components depend on software containers (i.e., the servers, components are deployed on) and thus, cannot be declared completely at design time. In this paper we present a semi-automated contract creation process that combines manual created contract templates with benchmarking and mathematical approximations for nonfunctional properties depending on the components' runtime behavior as well as their service's input parameters. We identify individual process activities and show how the process can be applied to approximate the nonfunctional behavior of software components providing simple sorting functionality.","","978-1-4673-1832-7978-1-4673-1833","10.1109/GREENS.2012.6224264","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224264","Quality Contracts;Benchmarking;Energy Self-Optimization","Contracts;Software;Benchmark testing;Time factors;Hardware;Approximation methods;Runtime","contracts;energy consumption;optimisation;software engineering;sorting","quality contracts;energy auto-tuning software;future software systems;self-optimization;w.r.t. energy efficiency;service utility;energy consumption;software containers;semi-automated contract creation process;nonfunctional behavior;sorting functionality","","6","14","","","","","","IEEE","IEEE Conferences"
"Optimal Test Input Sequence Generation for Finite State Models and Pushdown Systems","A. Chander; D. Dhurjati; K. Sen; D. Yu","NA; NA; NA; NA","2011 Fourth IEEE International Conference on Software Testing, Verification and Validation","","2011","","","140","149","Finite state machines and pushdown systems are frequently used in model based testing. In such testing, the system under test is abstractly modeled as a finite state machine having a finite set of states and a labeled transition relation between the states. A pushdown system, additionally, has an unbounded stack. Test inputs are then generated by enumerating a set of sequences of transitions labels from the model. There has been a lot of research that focussed on generation of test input sequences satisfying various coverage criteria. In this paper, we consider the problem of generating a set of test input sequences that satisfy certain coverage criteria-cover all transition labels or cover all length-n transition label sequences at least once-while minimizing the sum of the length of the sequences in the set. We show that these optimal test input generation problems can be reduced to integer linear programming (ILP) problems. We also prove that our optimal test input generation problems are NP-Complete. We report our experimental results on a prototype implementation for finite states machines.","2159-4848","978-1-61284-174-8978-0-7695-4342","10.1109/ICST.2011.64","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770603","Software Testing;Model-based testing;Optimal test input generation","Testing;Graphical user interfaces;USA Councils;Integer linear programming;Polynomials;Optimization;Mobile communication","computational complexity;finite state machines;integer programming;linear programming;program testing;pushdown automata","optimal test input sequence generation;finite state models;pushdown systems;finite state machines;model based testing;system under test;transition relation;unbounded stack;transitions labels;test input sequences;coverage criteria-cover all transition labels;cover all length-n transition label sequences;optimal test input generation problems;integer linear programming problems;ILP problems;NP-complete;prototype implementation;finite states machines","","8","20","","","","","","IEEE","IEEE Conferences"
"Evolutionary generation of test data for many paths coverage","Wan-qiu Zhang; Dun-wei Gong; Xiang-juan Yao; Yan Zhang","School of Information and Electrical Engineering, China University of Mining and Technology, Xuzhou, 221116, China; School of Information and Electrical Engineering, China University of Mining and Technology, Xuzhou, 221116, China; School of Information and Electrical Engineering, China University of Mining and Technology, Xuzhou, 221116, China; School of Information and Electrical Engineering, China University of Mining and Technology, Xuzhou, 221116, China","2010 Chinese Control and Decision Conference","","2010","","","230","235","Generation of test data for path coverage is an important issue of software testing, but previous methods are only suitable for the case that a program only has a small number of paths. We focus on the problem of generating test data for many paths coverage in this paper, and present a method of evolutionary generation of test data for many paths coverage. First, target paths are divided into several groups based on their similarity, and each group forms a sub-optimization problem, which transforms a complicated optimization problem into several simpler sub-optimization problems; then a domain-based fitness is designed when genetic algorithms are employed to solve these problems; finally, these sub-optimization problems are simplified along with the process of generating test data, hence improving the efficiency of generating test data. Our method is applied in 2 benchmark programs, and compared with some previous methods. The experimental results show that our method has advantage in time-consumption and the number of uncovered target paths. Our achievement provides an efficient way for generating test data of complicated software.","1948-9439;1948-9447","978-1-4244-5181-4978-1-4244-5182","10.1109/CCDC.2010.5499081","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5499081","software testing;many paths coverage;test data;genetic algorithms;grouping","Software testing;Genetic algorithms;Design optimization;Algorithm design and analysis;Educational institutions;Computer science;Benchmark testing;Computer bugs;Software reliability;Costs","genetic algorithms;program debugging;program testing","software testing;test data generation;paths coverage;optimization;genetic algorithm","","1","16","","","","","","IEEE","IEEE Conferences"
"Search-based testing, the underlying engine of Future Internet testing","A. I. Baars; K. Lakhotia; T. E. J. Vos; J. Wegener","Centro de Métodos de Producción de Software (ProS), Universidad Politecnica de Valencia, Valencia, Spain; CREST, University College London, London, United Kingdom; Centro de Métodos de Producción de Software (ProS), Universidad Politecnica de Valencia, Valencia, Spain; Berner & Mattner, Berlin, Germany","2011 Federated Conference on Computer Science and Information Systems (FedCSIS)","","2011","","","917","923","The Future Internet will be a complex interconnection of services, applications, content and media, on which our society will become increasingly dependent. Time to market is crucial in Internet applications and hence release cycles grow ever shorter. This, coupled with the highly dynamic nature of the Future Internet will place new demands on software testing. Search-Based Testing is ideally placed to address these emerging challenges. Its techniques are highly flexible and robust to only partially observable systems. This paper presents an overview of Search-Based Testing and discusses some of the open challenges remaining to make search-based techniques applicable to the Future Internet.","","978-83-60810-39-2978-1-4577-0041-5978-83-60810-35","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6078178","evolutionary testing;search-based testing;research topics","Testing;Evolutionary computation;Search problems;Optimization;Internet;Genetic algorithms;Software","Internet;program testing;search engines","search-based testing;future Internet testing;time to market;Internet applications;software testing","","","62","","","","","","IEEE","IEEE Conferences"
"Regression Test Generation Approach Based on Tree-Structured Analysis","Z. Zhang; J. Huang; B. Zhang; J. Lin; X. Chen","NA; NA; NA; NA; NA","2010 International Conference on Computational Science and Its Applications","","2010","","","244","249","Regression test generation is an important process to make sure that changes of program have no unintended side-effects. To achieve full confidence, many projects have to re-run all the test cases for entire program, which makes it a time consuming and expensive activity. In this paper, a code based regression testing approach is proposed to generate selected test suites for unit testing. The framework contains five phases: program change detection phase, logical verification phase, branch pruning phase, test case prioritization phase and test suite generation phase. These five phases can achieve detection of program's modification, coding standard, test case pruning, test case prioritizing and inputs generation for regression test cases respectively. A prototype based on this framework is implemented using logical tree-structured analysis, and the preliminary experiment shows that proposed approach can provide efficient regression test suites.","","978-1-4244-6462-3978-1-4244-6461-6978-0-7695-3999","10.1109/ICCSA.2010.59","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5476640","regression test;automated test generation;white-box test;software verification","Regression tree analysis;Logic testing;Software testing;Automatic testing;Phase detection;Life testing;Software maintenance;Prototypes;Computer architecture;Tree data structures","program diagnostics;regression analysis","regression test generation approach;code based regression testing;program change detection phase;logical verification phase;branch pruning phase;test case prioritization phase;test suite generation phase;program modification detection;coding standard;test case pruning;logical tree-structured analysis","","","16","","","","","","IEEE","IEEE Conferences"
"Evolutionary Generation of Whole Test Suites","G. Fraser; A. Arcuri","NA; NA","2011 11th International Conference on Quality Software","","2011","","","31","40","Recent advances in software testing allow automatic derivation of tests that reach almost any desired point in the source code. There is, however, a fundamental problem with the general idea of targeting one distinct test coverage goal at a time: Coverage goals are neither independent of each other, nor is test generation for any particular coverage goal guaranteed to succeed. We present EVOSUITE, a search-based approach that optimizes whole test suites towards satisfying a coverage criterion, rather than generating distinct test cases directed towards distinct coverage goals. Evaluated on five open source libraries and an industrial case study, we show that EVOSUITE achieves up to 18 times the coverage of a traditional approach targeting single branches, with up to 44% smaller test suites.","2332-662X;1550-6002;1550-6002","978-1-4577-0754-4978-0-7695-4468","10.1109/QSIC.2011.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004309","Search based software engineering;length;branch coverage;genetic algorithm","Software;Genetic algorithms;Search problems;Genetic programming;Software testing;Optimization","program testing;public domain software;software libraries","evolutionary generation;test suite;software testing;source code;test coverage goal;EVOSUITE;search-based approach;coverage criterion;open source library","","39","31","","","","","","IEEE","IEEE Conferences"
"MOST: A Multi-objective Search-Based Testing from EFSM","T. Yano; E. Martins; F. L. de Sousa","NA; NA; NA","2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops","","2011","","","164","173","This paper introduces a multi-objective evolutionary approach to test case generation from extended finite state machines (EFSM), named MOST. Testing from an (E)FSM generally involves executing various transition paths, until a given coverage criterion (e.g. cover all transitions) is met. As traditional test generation methods from FSM only consider the control aspects, they can produce many infeasible paths when applied to EFSMs, due to conflicts in guard conditions along a path. In order to avoid the infeasible path generation, we propose an approach that obtains feasible paths dynamically, instead of performing static reachability analysis as usual for FSM-based methods. Previous works have treated EFSM test case generation as a mono-objective optimization problem. Our approach takes two objectives into account that are the coverage criterion and the solution length. In this way, it is not necessary to establish in advance the test case size as earlier approaches. MOST constructs a Pareto set approximation, i.e., a group of optimal solutions, which allows the test team to select the solutions that represent a good trade-off between both objectives. The paper shows empirical studies to illustrate the benefits of the approach and comparing the results with the ones obtained in a related work.","","978-0-7695-4345-1978-1-4577-0019","10.1109/ICSTW.2011.37","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954406","model-based testing;feasible path;search-based testing;EFSM","Testing;Analytical models;Unified modeling language;Adaptation models;Neodymium;Optimization;Asynchronous transfer mode","approximation theory;evolutionary computation;finite state machines;Pareto optimisation;program testing","multiobjective search-based testing;extended finite state machines;multiobjective evolutionary approach;test case generation;test transition path;test coverage criterion;reachability analysis;Pareto set approximation;monoobjective optimization problem","","7","22","","","","","","IEEE","IEEE Conferences"
"Applying Case-Based Reasoning to software requirements specifications quality analysis system","H. M. Jani","College of Information Technology, Universiti Tenaga Nasional, Km. 7, Jalan Kajang-Puchong, 43009 Kajang, Selangor Darul Ehsan, Malaysia","The 2nd International Conference on Software Engineering and Data Mining","","2010","","","140","144","Software Requirements Specifications (SRS) or software requirements are basically an organization's understanding of a customer's system requirements and dependencies at a given point in time. This research paper focuses only on the requirements specifications phase of the software development cycle (SDC). It further narrows it down to analyzing the quality of the prepared SRS to ensure that the quality is acceptable. It is a known fact that companies will pay less to fix problems that are found very early in any software development cycle. The Software Quality Assurance (SQA) audit technique is applied in this study to determine whether or not the required standards and procedures within the requirements specifications phase are being followed closely. The proposed online quality analysis system ensures that software requirements among others are complete, consistent, correct, modifiable, ranked, traceable, unambiguous, and understandable. The system interacts with the developer through a series of questions and answers session, and requests the developer to go through a checklist that corresponds to the list of desirable characteristics for SRS. The Case-Based Reasoning (CBR) technique is used to evaluate the requirements quality by referring to previously stored software requirements quality analysis cases (past experiences). CBR is an AI technique that reasons by remembering previously experienced cases.","","978-89-88678-22-0978-1-4244-7324","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5542935","case-based reasoning;software requirements specifications;quality analysis","Software quality;Programming;Software systems;Testing;Information analysis;Educational institutions;Information technology;Costs;Software standards;Artificial intelligence","case-based reasoning;formal specification;quality assurance;software quality","case-based reasoning technique;software requirements specifications;quality analysis system;customer system requirements;software development cycle;software quality assurance audit technique","","4","12","","","","","","IEEE","IEEE Conferences"
"Design Principles for Integration of Model-Driven Quality Assurance Tools","O. Crelier; R. S. S. Filho; W. M. Hasling; C. J. Budnik","NA; NA; NA; NA","2011 Fifth Brazilian Symposium on Software Components, Architectures and Reuse","","2011","","","100","109","The engineering of software systems is supported by tools in different phases of the software development. The integration of these tools is crucial to assure the trace ability of existing models and artifacts, and to support the automation of critical software development phases such as software testing and validation. In particular, the integration of novel software quality assurance tools into existing environments must be performed in a way that minimizes its impact on existing software process, while the benefits of the tool are leveraged. This guarantees the adoption of new methodologies with minimal interference in existing production workflow. In this paper we discuss our experience in integrating a model-driven software testing tool developed within SIEMENS with a widely-adopted model-driven design tool. In particular, we establish a set of design principles from the lessons learned in this integration. We conclude showing a design that prioritizes data integration over control and presentation that achieves a high degree of tool integration while minimizing the integration development effort.","","978-0-7695-4626-1978-1-4673-0208","10.1109/SBCARS.2011.11","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6114546","Tool Integration;Software Interoperability;Quality Assurance Tools;UML;TDE/UML;Model-based Testing","Unified modeling language;Protocols;Testing;Data models;Software tools;Software engineering","data integration;program diagnostics;program testing;software quality;software tools","model-driven quality assurance tools;software system engineering;software development;software traceability;critical software development;software validation;software quality assurance tools;model-driven software testing tool;SIEMENS;model-driven software design tool;data integration;integration development","","2","15","","","","","","IEEE","IEEE Conferences"
"MT-SBST: Self-test optimization in multithreaded multicore architectures","N. Foutris; M. Psarakis; D. Gizopoulos; A. Apostolakis; X. Vera; A. Gonzalez","University of Piraeus, Department of Informatics, Greece; University of Piraeus, Department of Informatics, Greece; University of Piraeus, Department of Informatics, Greece; University of Piraeus, Department of Informatics, Greece; Intel Barcelona Research Center, Intel Labs-UPC, Spain; Intel Barcelona Research Center, Intel Labs-UPC, Spain","2010 IEEE International Test Conference","","2010","","","1","10","Instruction-based or software-based self-testing (SBST) is a scalable functional testing paradigm that has gained increasing acceptance in testing of single-threaded uniprocessors. Recent computer architecture trends towards chip multiprocessing and multithreading have raised new challenges in the test process. In this paper, we present a novel self-test optimization strategy for multithreaded, multicore microprocessor architectures and apply it to both manufacturing testing (execution from on-chip cache memory) and post-silicon validation (execution from main memory) setups. The proposed self-test program execution optimization aims to: (a) take maximum advantage of the available execution parallelism provided by multiple threads and multiple cores, (b) preserve the high fault coverage that single-thread execution provides for the processor components, and (c) enhance the fault coverage of the thread-specific control logic of the multithreaded multiprocessor. The proposed multithreaded (MT) SBST methodology generates an efficient multithreaded version of the test program and schedules the resulting test threads into the hardware threads of the processor to reduce the overall test execution time and on the same time to increase the overall fault coverage. We demonstrate our methodology in the OpenSPARC T1 processor model which integrates eight CPU cores, each one supporting four hardware threads. MT-SBST methodology and scheduling algorithm significantly speeds up self-test time at both the core level (3.6 times) and the processor level (6.0 times) against single-threaded execution, while at the same time it improves the overall fault coverage. Compared with straightforward multithreaded execution, it reduces the self-test time at both the core level and the processor level by 33% and 20%, respectively. Overall, MT-SBST reaches more than 91% stuck-at fault coverage for the functional units and 88% for the entire chip multiprocessor, a total of more than 1.5M logic gates.","2378-2250;1089-3539;1089-3539","978-1-4244-7207-9978-1-4244-7206-2978-1-4244-7205","10.1109/TEST.2010.5699277","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5699277","","Instruction sets;Built-in self-test;Parallel processing;Manufacturing;Multicore processing;Logic gates","built-in self test;electronic engineering computing;microprocessor chips;multiprocessing systems;multi-threading","MT-SBST;self-test optimization;multithreaded multicore architecture;instruction-based testing;software-based self-testing;scalable functional testing;multithreaded microprocessor;multicore microprocessor;OpenSPARC T1 processor","","13","19","","","","","","IEEE","IEEE Conferences"
"ChEOPSJ: Change-Based Test Optimization","Q. D. Soetens; S. Demeyer","NA; NA","2012 16th European Conference on Software Maintenance and Reengineering","","2012","","","535","538","Software is an ever-changing entity and frequently executed unit tests are the primary means to increase the confidence that the changed system continues to work as expected. Executing an entire test suite however can take a significant amount of time - much longer than developers are willing to wait before tackling the next change action. We demonstrate how our tool prototype ChEOPSJ may alleviate this problem by adopting a change-centric approach. Our tool sits in the back of Eclipse and captures all changes made in the main editor while the developer is programming. The changes and the dependencies between them are analyzed to deduce which unit tests are relevant for a selected sequence of changes.","1534-5351","978-0-7695-4666-7978-1-4673-0984","10.1109/CSMR.2012.70","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6178938","","Europe;Software maintenance","object-oriented programming;software maintenance","ChEOPSJ tool prototype;change-based test optimization;change-centric approach;Eclipse;software evolution;software maintenance","","7","11","","","","","","IEEE","IEEE Conferences"
"Implementation of Common Genetic and Bacteriological Algorithms in Optimizing Testing Data in Mutation Testing","M. F. Rad; F. Akbari; A. J. Bakht","NA; NA; NA","2010 International Conference on Computational Intelligence and Software Engineering","","2010","","","1","6","In this article, at first mutation testing is dealt with. Due to the high costs of mutation testing, it is usually tried to improve testing data by the use of evolutionary algorithms. Genetic algorithms are one of the most commonly used evolutionary algorithms. At first these algorithms are simulated and implemented, and then they are compared with the new bacteriological algorithms, inspired by the growth of the bacteria in nature.","","978-1-4244-5391-7978-1-4244-5392","10.1109/CISE.2010.5676811","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676811","","Biological cells;Genetics;Arrays;Software;Evolutionary computation;Software testing","genetic algorithms;program testing;program verification","mutation testing;bacteriological algorithms;genetic algorithms;evolutionary algorithms","","4","17","","","","","","IEEE","IEEE Conferences"
"Verified Firewall Policy Transformations for Test Case Generation","A. D. Brucker; L. Brügger; P. Kearney; B. Wolff","NA; NA; NA; NA","2010 Third International Conference on Software Testing, Verification and Validation","","2010","","","345","354","We present an optimization technique for model-based generation of test cases for firewalls. Starting from a formal model for firewall policies in higher-order logic, we derive a collection of semantics-preserving policy transformation rules and an algorithm that optimizes the specification with respect of the number of test cases required for path coverage. The correctness of the rules and the algorithm is established by formal proofs in Isabelle/HOL. Finally, we use the normalized policies to generate test cases with the domain-specific firewall testing tool HOL-TestGen/FW. The resulting procedure is characterized by a gain in efficiency of two orders of magnitude. It can handle configurations with hundreds of rules such as frequently occur in practice. Our approach can be seen as an instance of a methodology to tame inherent state-space explosions in test case generation for security policies.","2159-4848","978-1-4244-6436-4978-1-4244-6435-7978-0-7695-3990","10.1109/ICST.2010.50","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477066","security testing;model-based testing","Software testing;Web and internet services;System testing;Information security;Logic testing;Design optimization;Explosions;Inspection;Error correction;IP networks","authorisation;computer networks;optimisation;security of data","firewall;test case generation;optimization technique;higher-order logic;policy transformation rules;state-space explosions;security policies","","8","23","","","","","","IEEE","IEEE Conferences"
"An evolutionary algorithm for regression test suite reduction","S. Nachiyappan; A. Vimaladevi; C. B. SelvaLakshmi","Velammal College Engineering and Technology,Department, Viraganoor, Madurai, India; Dept of Applied Science, Vivekananda Institute of Engg & Tech. for Women, Tiruchengode, India; Velammal College Engineering and Technology,Department, Viraganoor, Madurai, India","2010 International Conference on Communication and Computational Intelligence (INCOCCI)","","2010","","","503","508","As the software is modified and new test cases are added to the test-suite, the size of the test-suite grows and the cost of regression testing increases. In order to decrease the cost of regression testing, researchers have focused on the use of test-suite reduction techniques, which identify a subset of test cases that provides the same coverage of the software, according to some criterion, as the original test-suite. This paper investigates the use of an evolutionary approach, called genetic algorithms, for test-suite reduction. The proposed model builds the initial population based on test history, calculates the fitness value using coverage and run time of test case, and then selectively breeds the successive generations using genetic operations and allows only the fit tests to the reduced suite. This generational process is repeated until an optimized test-suite is found.","","978-81-8371-369","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5738781","Regression testing;coverage;genetic algorithm","Testing;Evolutionary computation;Minimization;Gallium;Fault detection;Software;Measurement","cost reduction;genetic algorithms;program testing;regression analysis;software maintenance","evolutionary algorithm;test suite reduction;regression testing;genetic algorithms;software maintenance","","5","11","","","","","","IEEE","IEEE Conferences"
"Symbolic search-based testing","A. Baars; M. Harman; Y. Hassoun; K. Lakhotia; P. McMinn; P. Tonella; T. Vos","Universidad Polit&#x00E9;cnica de Valencia, Spain; University College London, CREST Centre, U.K.; King's College London, U.K.; University College London, CREST Centre, U.K.; University of Sheffield, U.K.; Fondazione Bruno Kessler, Trento, Italy; Universidad Polit&#x00E9;cnica de Valencia, Spain","2011 26th IEEE/ACM International Conference on Automated Software Engineering (ASE 2011)","","2011","","","53","62","We present an algorithm for constructing fitness functions that improve the efficiency of search-based testing when trying to generate branch adequate test data. The algorithm combines symbolic information with dynamic analysis and has two key advantages: It does not require any change in the underlying test data generation technique and it avoids many problems traditionally associated with symbolic execution, in particular the presence of loops. We have evaluated the algorithm on industrial closed source and open source systems using both local and global search-based testing techniques, demonstrating that both are statistically significantly more efficient using our approach. The test for significance was done using a one-sided, paired Wilcoxon signed rank test. On average, the local search requires 23.41% and the global search 7.78% fewer fitness evaluations when using a symbolic execution based fitness function generated by the algorithm.","1938-4300","978-1-4577-1639-3978-1-4577-1638","10.1109/ASE.2011.6100119","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6100119","Search-Based Testing;Symbolic Execution;Fitness Functions","Approximation methods;Algorithm design and analysis;Software algorithms;Educational institutions;Approximation algorithms;Software testing","automatic test software;information retrieval;program diagnostics;program testing","symbolic search-based testing;fitness functions;branch adequate test data generation;symbolic information;dynamic analysis;symbolic execution;industrial closed source system;open source systems;global search-based testing technique;local search-based testing technique;one-sided paired Wilcoxon signed rank test","","24","30","","","","","","IEEE","IEEE Conferences"
"Study of Test Suite Reduction Based on Quantum Evolutionary Algorithm","Y. Zhang; J. Liu; X. Yang; Y. Cui; B. Zhang","NA; NA; NA; NA; NA","2010 International Conference on Artificial Intelligence and Computational Intelligence","","2010","2","","483","487","Test suite reduction aims at adequately testing all the test requirements with the least number of test cases. Through the coverage relationship between test requirements and test cases, test suite reduction problem can be converted into the standard optimization problem. Quantum evolutionary algorithm (QEA) is an intelligent algorithm based on quantum computation . In QEA, chromosomes are encoded with quantum bits as the basic information bits, and individual variation evolution is realized by quantum mutation based on quantum probability gates. So in QEA, the convergence speed and ability to search global best results are superior to the traditional evolutionary algorithms. Motivated by this, we propose a novel test suite reduction method using quantum evolutionary. Finally, experiments validate the technology with higher efficiency.","","978-1-4244-8432","10.1109/AICI.2010.221","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5655107","Test suite reduction;Quantum evolutionary algorithm;Quantum chromosome;Quantum rotating gates","Evolutionary computation;Biological cells;Logic gates;Quantum computing;Optimization;Testing;Software","evolutionary computation;probability;program testing;quantum gates","test suite reduction problem;quantum evolutionary algorithm;intelligent algorithm;standard optimization problem;quantum computation;quantum probability gates;quantum mutation;QEA;software testing","","","10","","","","","","IEEE","IEEE Conferences"
"Considering Signal Constraints in Search-Based Testing of Continuous Systems","B. Wilmes; A. Windisch","NA; NA","2010 Third International Conference on Software Testing, Verification, and Validation Workshops","","2010","","","202","211","This paper presents a technique to consider constraints on input signals of continuous systems when applying search-based testing. The signal constraints are described using a logic based on Signal Temporal Logic. We developed a distance-oriented evaluation technique for these constraints that provides an exact rating of the amount of constraint violation, thus allowing a ranking of the generated solutions in terms of constraint violation. An adaptive penalty function is then used to incorporate the evaluation results into the optimization. Finally, the overall method is shown to be capable of considering signal constraints appropriately when experimentally applied to search-based black-box testing of a MATLAB SIMULINK model of an automatic transmission controller.","","978-1-4244-6774-7978-1-4244-6773-0978-0-7695-4050","10.1109/ICSTW.2010.22","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463647","Search-Based Testing;Continuous Systems;Constraint Handling;Signal Constraints;Temporal Logic","System testing;Continuous time systems;Automatic testing;Software testing;Reproducibility of results;Evolutionary computation;Application software;Performance evaluation;Computer industry;Computer bugs","constraint handling;particle swarm optimisation;program testing","signal constraints consideration;continuous systems search based testing;signal constraints;temporal logic signal;distance oriented evaluation technique;constraint violation;black box testing;MATLAB SIMULINK model;automatic transmission controller","","3","16","","","","","","IEEE","IEEE Conferences"
"Black box test case prioritization techniques for semantic based composite web services using OWL-S","A. Askarunisa; K. A. J. Punitha; A. M. Abirami","Department of Computer Science, Thiagarajar College of Engineering, Madurai, India; Department of Computer Science, Thiagarajar College of Engineering, Madurai, India; Thiagarajar College of Engineering Madurai, Tamilnadu, India","2011 International Conference on Recent Trends in Information Technology (ICRTIT)","","2011","","","1215","1220","Web services are the basic building blocks for the business which is different from web applications. Testing of web services is difficult and increases the cost due to the unavailability of source code. Researchers have, web services are tested based on the syntactic structure using Web Service Description Language (WSDL) for atomic web services. This paper proposes an automated testing framework for composite web services based on semantics where the domain knowledge of the web services is described using protégé tool and the behaviour of the entire business operation flow for the composite web service is described by Ontology Web Language for services (OWL-S). Prioritization of test cases is performed based on various coverage criteria for composite web services. Series of experiments were conducted to assess the effectiveness of prioritization and empirical results shown that prioritization techniques perform well in detecting faults compared to traditional techniques.","","978-1-4577-0590-8978-1-4577-0588-5978-1-4577-0589","10.1109/ICRTIT.2011.5972354","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5972354","Average percent of Faults Detected;Composite web services;Harmonic Mean of TF;Harmonic Mean of Sequence Invocation;Ontology Web Language for services;protégé;Test Case Prioritization","Web services;Ontologies;Testing;Semantics;Fault detection;XML;Unified modeling language","automatic test software;knowledge representation languages;semantic Web;Web services","black box test case prioritization technique;semantic based composite Web services;OWL-S;source code;syntactic structure;Web service description language;atomic Web services;automated testing framework;domain knowledge;protégé tool;business operation flow;ontology web language for service","","7","15","","","","","","IEEE","IEEE Conferences"
"SBST for on-line detection of hard faults in multiprocessor applications under energy constraints","A. Merentitis; D. Margaris; N. Kranitis; A. Paschalis; D. Gizopoulos","Department of Informatics &amp; Telecommunications, University of Athens, Greece; Department of Informatics &amp; Telecommunications, University of Athens, Greece; Department of Informatics &amp; Telecommunications, University of Athens, Greece; Department of Informatics &amp; Telecommunications, University of Athens, Greece; Department of Informatics, University of Piraeus, Greece","2010 IEEE 16th International On-Line Testing Symposium","","2010","","","62","67","Software-Based Self-Test (SBST) has emerged as an effective method for on-line testing of processors integrated in non safety-critical systems. However, especially for multi-core processors, the notion of dependability encompasses not only high quality on-line tests with minimum performance overhead but also methods for preventing the generation of excessive power and heat that exacerbate silicon aging mechanisms and can cause long term reliability problems. In this paper, we initially extend the capabilities of a multiprocessor simulator in order to evaluate the overhead in the execution of the useful application load in terms of both performance and energy consumption. We utilize the derived power evaluation framework to assess the overhead of SBST implemented as a test thread in a multiprocessor environment. A range of typical processor configurations is considered. The application load consists of some representative SPEC benchmarks, and various scenarios for the execution of the test thread are studied (sporadic or continuous execution). Finally, we apply in a multiprocessor context an energy optimization methodology that was originally proposed to increase battery life for battery-powered devices. The methodology reduces significantly the energy and performance overhead without affecting the test coverage of the SBST routines.","1942-9398;1942-9401","978-1-4244-7723-4978-1-4244-7724-1978-1-4244-7722","10.1109/IOLTS.2010.5560233","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5560233","software-based self-testing;hard faults;multiprocessors;on-line test;low energy optimization","Program processors;Benchmark testing;Delay;Circuit faults;Energy consumption;Hardware","automatic test pattern generation;automatic test software;fault diagnosis;microprocessor chips;multi-threading","online hard fault detection;multiprocessor application;energy constraint;software-based self-test;processor testing;non safety-critical system;multicore processors;processor dependability;performance overhead;silicon aging;reliability problem;energy consumption;power evaluation framework;multiprocessor environment;processor configuration;sporadic execution;continuous execution;energy optimization","","","27","","","","","","IEEE","IEEE Conferences"
"Evaluating Complexity, Code Churn, and Developer Activity Metrics as Indicators of Software Vulnerabilities","Y. Shin; A. Meneely; L. Williams; J. A. Osborne","DePaul University, Chicago; North Carolina State University, Raleigh; North Carolina State University, Raleigh; North Carolina State University, Raleigh","IEEE Transactions on Software Engineering","","2011","37","6","772","787","Security inspection and testing require experts in security who think like an attacker. Security experts need to know code locations on which to focus their testing and inspection efforts. Since vulnerabilities are rare occurrences, locating vulnerable code locations can be a challenging task. We investigated whether software metrics obtained from source code and development history are discriminative and predictive of vulnerable code locations. If so, security experts can use this prediction to prioritize security inspection and testing efforts. The metrics we investigated fall into three categories: complexity, code churn, and developer activity metrics. We performed two empirical case studies on large, widely used open-source projects: the Mozilla Firefox web browser and the Red Hat Enterprise Linux kernel. The results indicate that 24 of the 28 metrics collected are discriminative of vulnerabilities for both projects. The models using all three types of metrics together predicted over 80 percent of the known vulnerable files with less than 25 percent false positives for both projects. Compared to a random selection of files for inspection and testing, these models would have reduced the number of files and the number of lines of code to inspect or test by over 71 and 28 percent, respectively, for both projects.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.81","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5560680","Fault prediction;software metrics;software security;vulnerability prediction.","Fault diagnosis;Software security;Complexity theory;Predictive models;Charge coupled devices","Linux;online front-ends;program testing;public domain software;software fault tolerance;software metrics","code churn;software vulnerabilities;developer activity metrics;security inspection;software metrics;source code;vulnerable code locations;open-source projects;Mozilla Firefox Web browser;Red Hat enterprise Linux kernel","","109","43","","","","","","IEEE","IEEE Journals & Magazines"
"Random DBPSO algorithm application in the Optimal Test-sequencing Problem of complicated electronic system","Xiaohong Qiu; Jun Liu; Xiaohui Qiu","School of Software, Jiangxi Agricultural University, Nanchang, 330045, China; Department of Automatic Control, Beijing Institute of Technology, 100081, China; Institute of Education, Jiangxi Agricultural University, Nanchang, 330045, China","2010 The 2nd International Conference on Computer and Automation Engineering (ICCAE)","","2010","1","","107","111","An algorithm of improved AO* based on discrete binary particle swarm optimization (DBPSO) with additional random item is proposed, which can solve the Optimal Test-sequencing Problem (OTP) in large-scale complicated electron system. DBPSO optimizes the test sets which can isolate the expanded node in AO* algorithm to decrease the number of node. The result of real operation show that this algorithm not only reduces the computational complexity, cuts down the test cost, shorts the test time; but also avoids the ¿computational explosion¿ when the test set is too large. Comparing with inertia weight, the particle's velocity is determined by previous velocity, own experience, public knowledge and random behavior defined by the additional random factor which helps to get the global optimization solution. Simulation results show that the method with the random factor is better than inertia weight and constriction factor.","","978-1-4244-5586-7978-1-4244-5569-0978-1-4244-5585","10.1109/ICCAE.2010.5451990","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5451990","Discrete binary particle swarm optimization;AO*;Test Sequence;Huffman coding;Design for testability;Random operator","Electronic equipment testing;System testing;Particle swarm optimization;Automatic testing;Large-scale systems;Explosions;Software algorithms;Software testing;Computational complexity;Dynamic programming","computational complexity;design for testability;particle swarm optimisation","random DBPSO algorithm;optimal test-sequencing problem;discrete binary particle swarm optimization;large-scale complicated electron system;computational complexity;global optimization solution;inertia weight factor;constriction factor;design for testability","","","11","","","","","","IEEE","IEEE Conferences"
"Toward Harnessing High-Level Language Virtual Machines for Further Speeding Up Weak Mutation Testing","V. H. S. Durelli; J. Offutt; M. E. Delamaro","NA; NA; NA","2012 IEEE Fifth International Conference on Software Testing, Verification and Validation","","2012","","","681","690","High-level language virtual machines (HLL VMs) are now widely used to implement high-level programming languages. To a certain extent, their widespread adoption is due to the software engineering benefits provided by these managed execution environments, for example, garbage collection (GC) and cross-platform portability. Although HLL VMs are widely used, most research has concentrated on high-end optimizations such as dynamic compilation and advanced GC techniques. Few efforts have focused on introducing features that automate or facilitate certain software engineering activities, including software testing. This paper suggests that HLL VMs provide a reasonable basis for building an integrated software testing environment. As a proof-of-concept, we have augmented a Java virtual machine (JVM) to support weak mutation analysis. Our mutation-aware HLL VM capitalizes on the relationship between a program execution and the underlying managed execution environment, thereby speeding up the execution of the program under test and its associated mutants. To provide some evidence of the performance of our implementation, we conducted an experiment to compare the efficiency of our VM-based implementation with a strong mutation testing tool (muJava). Experimental results show that the VM-based implementation achieves speedups of as much as 89% in some cases.","2159-4848","978-0-7695-4670-4978-1-4577-1906","10.1109/ICST.2012.158","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200170","Mutation Analysis;Java Virtual Machine;Software Testing;Maxine VM;mu Java","Arrays;Context;Testing;Instruments;Runtime;Java;Virtual machining","high level languages;program testing;virtual machines","weak mutation testing;high-level language virtual machines;high-level programming languages;software engineering;garbage collection;cross-platform portability;high-end optimizations;dynamic compilation;advanced GC techniques;integrated software testing environment;Java virtual machine;weak mutation analysis;mutation-aware HLL VM;program execution;managed execution environment;program under test","","2","25","","","","","","IEEE","IEEE Conferences"
"MDMeter A New Test Tool for Meta-data Throughput of Parallel File System","X. Yang; L. Zhu; Q. Li; W. Dai; N. Ma","NA; NA; NA; NA; NA","2010 Third International Joint Conference on Computational Science and Optimization","","2010","2","","427","431","As the existing performance evaluation benchmark software for parallel file system has too many test items, it can't evaluate the meta-data throughput accurately, and there are great difficulties to evaluate the meta-data throughput performance in practical application environment. To solve these problems, a new meta-data throughput evaluation tool, called as MDMeter, has been designed and accomplished in this paper. A new simulation model for file operation is designed according to the real file operation records which were captured by IOTrace tool. MDMeter can launch a variety of meta-data operations on parallel file system, and can provide a comprehensive test result report to users; it can evaluate the meta-data throughput performance accurately for different application environment. At the last, some tests are performed on Lustre parallel file system by MDMeter and traditional tools, the results show that MDMeter not only can give a simple and integrated performance test result to users, but also can evaluate the meta-data throughput for parallel file system more precisely than the existing test tools.","","978-1-4244-6813-3978-1-4244-6812","10.1109/CSO.2010.54","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5533100","performance evaluation;benchmark;meta-data throughput;parallel file system","System testing;Throughput;File systems;Performance evaluation;Sequential analysis;Concurrent computing;Software testing;Benchmark testing;Communication system software;Software performance","benchmark testing;meta data;parallel databases;program testing;software performance evaluation;software tools","meta data throughput;parallel file system;performance evaluation;benchmark software;MDMeter;file operation","","","10","","","","","","IEEE","IEEE Conferences"
"Research on automatic instrumentation for bytecode testing and debugging","C. Wang; X. Mao; Z. Dai; Y. Lei","School of Computer, NUDT, Changsha, China; School of Computer, NUDT, Changsha, China; School of Computer, NUDT, Changsha, China; School of Computer, NUDT, Changsha, China","2012 IEEE International Conference on Computer Science and Automation Engineering (CSAE)","","2012","1","","268","274","Because of inherent drawbacks of Software Engineering, no software is defect-free. If the defects are resulted from highly optimized compilers, or in software without source code, software maintainers may have to test and debug programs at binary level, considering the fact that there are not practical reverse engineering tools. We propose a dynamic and automatic instrumentation framework, DABITTD, to support bytecode programs testing and debugging. According to the user requirements, it can provide the program run-time information and alter program run-time behaviors as well. DABITTD works at bytecode level without needs to access source code and doesn't pollute the original class files. What is more, the whole process of instrumentation is performed fully automatically and dynamically. Meanwhile, in order to help maintainers fix defects, DABITTD can also directly edit class files on the disk statically.","","978-1-4673-0089-6978-1-4673-0088-9978-1-4673-0087","10.1109/CSAE.2012.6272595","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6272595","Dynamic Instrumentation;Automatic Instrumentation;Software Testing;Software Debugging","Instruments;Testing;Debugging;XML;Software;Java;Load modeling","automatic test equipment;optimising compilers;program debugging;program testing","automatic instrumentation;software engineering;software defects;optimized compilers;binary level;dynamic instrumentation framework;automatic instrumentation framework;DABITTD;bytecode program testing;bytecode program debugging;program run-time information;statical class file editing;disks","","1","27","","","","","","IEEE","IEEE Conferences"
"An Android-based distributed test data acquisition intelligent system for mobile network optimization","J. Bian; L. Liu; J. Kuang","Beijing Key Laboratory of Intelligent Telecommunications Software and Multimedia, Beijing University of Posts and Telecommunications, Beijing 100876, China; Beijing Key Laboratory of Intelligent Telecommunications Software and Multimedia, Beijing University of Posts and Telecommunications, Beijing 100876, China; Beijing Key Laboratory of Intelligent Telecommunications Software and Multimedia, Beijing University of Posts and Telecommunications, Beijing 100876, China","2012 IEEE 2nd International Conference on Cloud Computing and Intelligence Systems","","2012","01","","38","42","With the rapid development of mobile networks, mobile communication network optimization plays an increasingly important role, and the data collection before optimization is a very important process. This paper presents a distributed test data acquisition system, which is deployed in an Android-based cell phone that is conveniently carried. Our system uses a positioning method which combines GPS and LBS based on RSSI to get the location of mobile phones regularly, simultaneously the mobile communication networks status data are collected. Then the system uploads the data to certain servers. Every Android phone can be test data source, and these data would gather into a data cloud on the server side, which guarantees the quality of mobile network optimization. The optimization engineers can use these data to analyze the network overlay, received signal strength, co-channel and adjacent channel, cover extra, etc. Compared with the current DT and other data acquisition methods, our system is suitable for both indoor and outdoor use, provides huger data and costs less money.","2376-5933;2376-595X","978-1-4673-1857-0978-1-4673-1855","10.1109/CCIS.2012.6664363","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6664363","GSM network optimization;Data acquisition;Android;GPS;LBS","Smart phones;GSM;Servers;Optimization;Global Positioning System;Mobile communication;Data acquisition","cellular radio;data acquisition;Global Positioning System;mobile handsets","outdoor use;indoor use;cover extra;cochannel channel;adjacent channel;received signal strength;network overlay;optimization engineers;data cloud;mobile phones;RSSI;LBS;positioning method;GPS;Android-based cell phone;data collection;mobile communication network optimization;distributed test data acquisition intelligent system","","","10","","","","","","IEEE","IEEE Conferences"
"Optimizing Query Processing in Practical Software Database by Adapting","M. R. F. Derakhshi; H. Asil; A. Asil; E. Zafarani","NA; NA; NA; NA","2010 Third International Conference on Knowledge Discovery and Data Mining","","2010","","","375","378","Different methods have been presented because of the needs to query processing optimization in databases. The reason of these needs is to increase the amount of data and the queries sent to the Database Management System. Even though there are various methods presented to optimize queries, the problem is that in most of these methods, the execution plan is deleted after the query is conducted. This study tries to provide a method that uses optimized execution plan obtained from executing a query for the next executions or executing a similar query in order to reduce time needed for executing queries. The method presented in this study has been tested on real practical software databases and the results shows 11% improvement.","","978-1-4244-5398-6978-1-4244-5397","10.1109/WKDD.2010.121","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5432580","Relational Database;Agen;Queries Processing;Adaption","Query processing;Optimization methods;Spatial databases;Relational databases;Data mining;Computer industry;Engineering management;Data engineering;Database systems;Software testing","optimisation;query processing;relational databases","query processing optimization;practical software database;database management system;optimized execution plan","","","5","","","","","","IEEE","IEEE Conferences"
"An optimized design for the test case of web application","Hao Wu","Department of Computer Science, Zhuhai College of Jilin University, Guangdong, 519041, China","2012 IEEE Symposium on Electrical & Electronics Engineering (EEESYM)","","2012","","","148","151","In order to design an optimized test case to test web application, a method of equivalence partitioning is used in this paper. Based on the evaluation of equivalence classes for the input condition, the web application of the Accela GIS module is divided into seven partitions, and the twelve vital test cases are designed. The results of test show that the process of executing the test program can find errors, and in particular, the test cases have the high probability of finding as-yet-undiscovered errors. These test cases can basically cover all of the test points and be repeated less. The equivalence partitioning is effectively designed into test cases of the web application, which are feasible, strong and easy to be understood, to reduce the total number of test cases that must be developed.","","978-1-4673-2365-9978-1-4673-2363-5978-1-4673-2364","10.1109/EEESym.2012.6258611","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6258611","Test case;Software testing;Equivalence partitioning","Manuals;Servers;Bones;Mice;XML;Computer bugs;Concrete","geographic information systems;Internet;probability;program testing","Web application;optimized test case design;equivalence partitioning;equivalence classes;Accela GIS module;as-yet-undiscovered errors finding","","","3","","","","","","IEEE","IEEE Conferences"
"Risk-Based Testing of Safety-Critical Embedded Systems Driven by Fault Tree Analysis","J. Kloos; T. Hussain; R. Eschbach","NA; NA; NA","2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops","","2011","","","26","33","One important aspect of the quality assurance process of safety-critical embedded systems is verifying the appropriateness, correctness of the implementation and effectiveness of safety functions. Due to the rapid growth in complexity, manual verification activities are no longer feasible. This holds especially for testing. A popular method for testing such complex systems is model-based testing. Recent techniques for model-based testing do not sufficiently take into consideration the information derived from the safety analyses like Failure Mode and Effect Analysis and Fault Tree Analyses (FTA). In this paper, we describe an approach to use the results of FTA during the construction of test models, such that test cases can be derived, selected and prioritized according to the severity of the identified risks and the number of basic events that cause it. This approach is demonstrated on an example from the automation domain, namely a modular production system. We find that the method provides a significant increase in coverage of safety functions, compared to regular model based testing.","","978-0-7695-4345-1978-1-4577-0019","10.1109/ICSTW.2011.90","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954386","Safety;Model-based Testing;Fault-Tree Analysis;Risk-based Testing","Testing;Safety;Fault trees;Sensors;Manuals;Embedded systems","embedded systems;program testing;safety-critical software;software quality","risk-based testing;safety-critical embedded systems;fault tree analysis;quality assurance process;model-based testing;failure mode and effect analysis","","12","15","","","","","","IEEE","IEEE Conferences"
"Evaluation of development optimization of LTE base station OSS by product test automation","S. Iwami; Y. Kishikawa; K. Fujii; S. Namie; K. Takahashi; S. Akiyama","Network Development Department NTT DOCOMO, Inc., 3-5 Hikari-no-oka, Yokosuka-shi, Kanagawa, 239-8536, Japan; Network Development Department NTT DOCOMO, Inc., 3-5 Hikari-no-oka, Yokosuka-shi, Kanagawa, 239-8536, Japan; Network Development Department NTT DOCOMO, Inc., 3-5 Hikari-no-oka, Yokosuka-shi, Kanagawa, 239-8536, Japan; Network Development Department NTT DOCOMO, Inc., 3-5 Hikari-no-oka, Yokosuka-shi, Kanagawa, 239-8536, Japan; Network Development Department NTT DOCOMO, Inc., 3-5 Hikari-no-oka, Yokosuka-shi, Kanagawa, 239-8536, Japan; Management System Development Department, DOCOMO Technology, Inc., 3-5 Hikari-no-oka, Yokosuka-shi, Kanagawa, 239-8536, Japan","2011 13th Asia-Pacific Network Operations and Management Symposium","","2011","","","1","4","Containment of development cost associated with new services is important issue for telecom operators. Our OSS products have web front end. We require the regression testing for existing functions with new function addition development of OSS. Generally, the test items for regression testing do not change significantly at every development. In this paper, we propose the automated program test including the regression testing using web browser, and we show the cost efficiency of developing OSS.","","978-1-4577-1670-6978-1-4577-1668-3978-1-4577-1669","10.1109/APNOMS.2011.6076960","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6076960","LTE;Operation Support System;OSS;WEB Application;Regression Testing","Testing;Productivity;Automation;Software;Monitoring;Base stations;Maintenance engineering","decision support systems;Long Term Evolution;mobile computing;online front-ends;optimisation;regression analysis;Web services","optimization;LTE;base station;OSS development;product test automation;telecom operators;Web front end;regression testing;Web browser;operation support system","","","7","","","","","","IEEE","IEEE Conferences"
"Overview of TASE 2012 Talk on Search Based Software Engineering","M. Harman","NA","2012 Sixth International Symposium on Theoretical Aspects of Software Engineering","","2012","","","3","4","This is an overview of the keynote presentation on SBSE at the Sixth IEEE International Symposium on Theoretical Aspects of Software Engineering (TASE 2012), held on the 4th-6th July 2012 in Beijing, China.","","978-1-4673-2353","10.1109/TASE.2012.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6269620","SBSE;evolutionary computation;search based software engineering;AI techniques","Software engineering;Testing;Software;Optimization;Predictive models;Conferences;Industries","software engineering","search based software engineering","","","29","","","","","","IEEE","IEEE Conferences"
"Mutation-based diagnostic test generation for hardware design error diagnosis","S. Deng; K. Cheng; J. Bian; Z. Kong","Department of Computer Science and Technology, Tsinghua University, Beijing, 100084 China; Department of Electrical and Computer, Engineering, University of California, Santa Barbara, USA; Department of Computer Science and Technology, Tsinghua University, Beijing, 100084 China; Department of Computer Science and Technology, Tsinghua University, Beijing, 100084 China","2010 IEEE International Test Conference","","2010","","","1","1","We propose the use of mutation-based error injection to guide the generation of high-quality diagnostic test patterns. A software-based fault localization technique is employed to derive a ranked candidate list of suspect statements. Experimental results for a set of Verilog designs demonstrate that a finer diagnostic resolution can be achieved by patterns generated by the proposed method.","2378-2250;1089-3539;1089-3539","978-1-4244-7207-9978-1-4244-7206-2978-1-4244-7205","10.1109/TEST.2010.5699307","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5699307","","","fault diagnosis;hardware description languages;logic testing","mutation-based diagnostic test generation;hardware design error diagnosis;mutation-based error injection;software-based fault localization;Verilog design","","2","2","","","","","","IEEE","IEEE Conferences"
"An approach to improve accuracy of source-level TLMs of embedded software","Z. Wang; K. Lu; A. Herkersdorf","Technische Universit&#x00E4;t M&#x00FC;nchen, Arcisstra&#x00DF;e 21, 80290 M&#x00FC;nchen, Germany; Technische Universit&#x00E4;t M&#x00FC;nchen, Arcisstra&#x00DF;e 21, 80290 M&#x00FC;nchen, Germany; Technische Universit&#x00E4;t M&#x00FC;nchen, Arcisstra&#x00DF;e 21, 80290 M&#x00FC;nchen, Germany","2011 Design, Automation & Test in Europe","","2011","","","1","6","Virtual Prototypes (VPs) based on Transaction Level Models (TLMs) have become a de-facto standard for design space exploration and validation of complex software-centric multicore or multiprocessor systems. The most popular method to get timed software TLMs is to annotate timing information at the basic-block level granularity back into application source code, called source code instrumentation (SCI). The existing SCI approaches realize the back-annotation of timing information based on mapping between source code and binary code. However, optimizing compilation has a large impact on the code mapping and will lower the accuracy of the generated source-level TLMs. In this paper, we present an efficient approach to tackle this problem. We propose to use mapping between source-level and binary-level control flows as the basis for timing annotation instead of code mapping. Software TLMs generated by our approach allow for accurate evaluation of multiprocessor systems at a very high speed. This has been proven by our experiments with a set of benchmark programs and a case study.","1558-1101;1530-1591;1530-1591","978-3-9810801-8-6978-1-61284-208-0978-3-9810801-7","10.1109/DATE.2011.5763045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5763045","","Software;Binary codes;Instruments;Time domain analysis;Time varying systems;Delay","binary codes;formal specification;formal verification;multiprocessing systems;optimising compilers;software prototyping","source-level transaction level model;embedded software;virtual prototypes;design space exploration;software-centric multicore system validation;multiprocessor system validation;timed software transaction level model;timing information;basic-block level granularity;application source code;source code instrumentation;binary code;optimizing compilation;code mapping","","6","","","","","","","IEEE","IEEE Conferences"
"Injecting Memory Leaks to Accelerate Software Failures","J. Zhao; Y. Jin; K. S. Trivedi; R. Matias Jr.","NA; NA; NA; NA","2011 IEEE 22nd International Symposium on Software Reliability Engineering","","2011","","","260","269","A number of studies have reported the phenomenon of ""Software aging"", caused by resource exhaustion and characterized by progressive software performance degradation. We develop experiments that simulate an on-line bookstore application, following the standard configuration of TPC-W benchmark. We study the application failures caused by memory leaks, using the accelerated life tests method. In our experiments, the memory consumption rate is selected as the acceleration factor, and an IPL-lognormal model is used to estimate the time to failure at each acceleration level. Subsequently, the estimate of the time to failure distribution at normal condition is obtained. Our acceleration experimental results based on the IPL-lognormal model show that it can be used to greatly reduce the cost to obtain the time to failure at normal level, which can be used in scheduling software rejuvenation. Finally, we select the Weibull time to failure distribution at normal level, to be used in a semi-Markov process, to optimize the software rejuvenation trigger interval.","2332-6549;1071-9458;1071-9458","978-1-4577-2060-4978-0-7695-4568","10.1109/ISSRE.2011.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6132974","accelerated life tests;memory leaks;optimal software rejuvenation;semi-Markov process;software aging","Aging;Stress;Memory management;Acceleration;Java;Web servers","benchmark testing;life testing;Markov processes;software reliability;Weibull distribution","memory leaks;software failure;software aging;software performance degradation;online bookstore application;TPC-W benchmark;accelerated life tests method;IPL lognormal model;Weibull time;failure distribution;semi Markov process;software rejuvenation","","8","29","","","","","","IEEE","IEEE Conferences"
"SerDes Interoperability and Optimization","M. Kamm; H. Jun; L. Boluna","Cisco Systems, Inc., San Jose, CA , USA; Cisco Systems, Inc., San Jose; Cisco Systems, Inc., San Jose, CA, USA","IEEE Design & Test of Computers","","2012","29","5","47","53","As SerDes use in system-level applications increases, interoperability and overall system optimization become greater challenges. This work presents a solution to these problems utilizing the combination of embedded link transmit and receive tests via system diagnostics software layer, JTAG, and the upcoming IEEE P1687 standard.","0740-7475;1558-1918","","10.1109/MDT.2012.2201910","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6209507","SerDes;SPMT;ASIC;SerDes Characterization;BERT;BER;JTAG;P1687;PRBS;CDR;System Test","IP networks;Standards;Optimization;Software;Application specific integrated circuits;Bit error rate;Instruments","electronic engineering computing;embedded systems;open systems;optimisation;peripheral interfaces","SerDes interoperability;system-level applications;system optimization;embedded link transmit;system diagnostic software layer;JTAG;IEEE P1687 standard","","","12","","","","","","IEEE","IEEE Journals & Magazines"
"Using Non-redundant Mutation Operators and Test Suite Prioritization to Achieve Efficient and Scalable Mutation Analysis","R. Just; G. M. Kapfhammer; F. Schweiggert","NA; NA; NA","2012 IEEE 23rd International Symposium on Software Reliability Engineering","","2012","","","11","20","Mutation analysis is a powerful and unbiased technique to assess the quality of input values and test oracles. However, its application domain is still limited due to the fact that it is a time consuming and computationally expensive method, especially when used with large and complex software systems. Addressing these challenges, this paper makes several contributions to significantly improve the efficiency of mutation analysis. First, it investigates the decrease in generated mutants by applying a reduced, yet sufficient, set of mutants for replacing conditional (COR) and relational (ROR) operators. The analysis of ten real-world applications, with 400,000 lines of code and more than 550,000 generated mutants in total, reveals a reduction in the number of mutants created of up to 37% and more than 25% on average. Yet, since the isolated use of non-redundant mutation operators does not ensure that mutation analysis is efficient and scalable, this paper also presents and experimentally evaluates an optimized workflow that exploits the redundancies and runtime differences of test cases to reorder and split the corresponding test suite. Using the same ten open-source applications, an empirical study convincingly demonstrates that the combination of non-redundant operators and prioritization leveraging information about the runtime and mutation coverage of tests reduces the total cost of mutation analysis further by as much as 65%.","1071-9458;1071-9458;2332-6549","978-1-4673-4638-2978-0-7695-4888","10.1109/ISSRE.2012.31","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405400","","Runtime;Redundancy;Testing;Educational institutions;Connectors;Computer science;Software systems","program diagnostics;program testing","nonredundant operators;open-source applications;real-world application analysis;complex software systems;large software systems;computationally expensive method;test oracles;scalable mutation analysis;test suite prioritization;nonredundant mutation operators","","17","17","","","","","","IEEE","IEEE Conferences"
"Risk-Based Testing: A Case Study","E. Souza; C. Gusmão; J. Venâncio","NA; NA; NA","2010 Seventh International Conference on Information Technology: New Generations","","2010","","","1032","1037","This paper describes the application of risk-based testing for a software product evaluation in a real case study. Risk-based testing consists of a set of activities regarding risk factors identification related to software requirements. Once identified, the risks are prioritized according to their likelihood and impact and test cases are designed based on the strategies for treatment of the identified risk factors. Thus, test efforts are continuously adjusted according to risk monitoring. The paper also briefly reviews available risk-based approaches, describes the benefits that are likely to accrue from the growing body of work in this area and provides a set of problems, challenges and future work.","","978-1-4244-6271-1978-1-4244-6270-4978-0-7695-3984","10.1109/ITNG.2010.203","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5501497","Case Study;Risk-Based Testing;Risk Management;Software Testing;Testing Process","Software testing;System testing;Monitoring;Risk management;Software quality;Costs;Programming;Risk analysis;Automatic testing;Information technology","program testing;risk management","risk-based testing;software product evaluation;risk factor identification;software requirements","","5","12","","","","","","IEEE","IEEE Conferences"
"A Framework for Optimizing Effort in Testing of System of Systems","P. Bera; A. Pasala","NA; NA","2012 Third International Conference on Services in Emerging Markets","","2012","","","136","141","The concept of System-of-System (SoS) is increasingly used in building large and complex software systems which essentially provide a set of required functionalities in a box to the customers. Testing System-of-Systems includes testing of individual systems, validating the interactions among the constituent systems and verifying the compliance of the service level constraints. Testing complete interactions of constituent systems and checking the compliance with service level constraints as and when they are plugged into the SoS requires significant effort. Therefore, there is a need for testing methodologies to validate such complex systems in a comprehensive and optimal way. In this paper, we propose a test framework based on knowledge characterization of constituent systems that are used to reduce the overall effort in validating the system of systems. The knowledge is characterized in terms of the different test parameters of constituent systems. A theoretical case study has been carried out on a next generation Network File System along with its constituent file systems to illustrate the efficacy of our framework.","","978-0-7695-4937-8978-1-4673-5729","10.1109/ICSEM.2012.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6468190","System of Systems;composable systems;Software Testing;Optimization","File systems;Testing;Hospitals;Silicon;Information systems;Medical diagnostic imaging;Systems engineering and theory","conformance testing;file organisation;program testing;program verification","effort optimization framework;system-of-system testing;SoS testing;constituent software system interaction validation;service level constraint compliance verification;compliance checking;knowledge characterization;constituent system test parameters;network file system;constituent file systems","","","15","","","","","","IEEE","IEEE Conferences"
"On Capturing Effects of Modifications as Data Dependencies","H. Ural; H. Yenigün","NA; NA","2012 IEEE 36th Annual Computer Software and Applications Conference","","2012","","","350","351","Dependence analysis on an Extended Finite State Machine (EFSM) representation of the requirements of a system under test has been used in requirements-based regression testing for regression test suite (RTS) reduction (reducing the size of a given test suite by eliminating redundancies), for RTS prioritization (ordering test cases in a given test suite for early fault detection) or for RTS selection (selecting a subset of a test suite covering the identified dependencies). These particular uses of dependence analysis are based on definitions of various types of control and data dependencies (between transitions in an EFSM) caused by a given set of modifications on the requirements. This abstract considers the definitions of data dependencies, gives examples of incompleteness of existing definitions, and presents insights on completing these definitions.","0730-3157;0730-3157;0730-3157","978-1-4673-1990-4978-0-7695-4736","10.1109/COMPSAC.2012.50","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6340172","model-based testing;regression testing;data dependence","Testing;Analytical models;Data models;Educational institutions;Abstracts;Software maintenance;Conferences","data reduction;finite state machines;program testing;regression analysis","data dependency;extended finite state machine;requirements-based regression testing;regression test suite;RTS reduction;RTS prioritization;RTS selection;EFSM;system under test","","","2","","","","","","IEEE","IEEE Conferences"
"A novel chaos discrete particle swarm optimization algorithm for test suite reduction","Jia-ze Sun; Shu-yan Wang","School of Computer Science & Technology, Xi'an University of Post & Telecommunications, 710061, China; School of Computer Science & Technology, Xi'an University of Post & Telecommunications, 710061, China","The 2nd International Conference on Information Science and Engineering","","2010","","","1","4","Aiming at providing the more measure of coverage with the least number of test cases for software regression testing, this paper presents a novel chaos discrete particle swarm optimization algorithm(CDPSO) for test suite reduction, which combines discrete particle swarm optimization (DPSO) with a chaos searching strategy. In the algorithm, particle swarm is initialized by chaotic series, and the position of the particle is produced by stochastic algorithm. Moreover, it introduces chaos to DPSO, making every particle select a suitable search direction from PSO search mechanism and chaos search mechanism, to avoid PSO getting into local best and appearing premature convergence. Finally, the classic example is used to illustrate the performance of the proposed algorithm. The experimental results indicate that the CDPSO algorithm can achieve higher performance, faster speed than PSO, GE, H and GRE, and has nothing to do with the initial value‥","2160-1283;2160-1291","978-1-4244-7618-3978-1-4244-7616-9978-1-4244-7617","10.1109/ICISE.2010.5689888","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5689888","test suite reduction;discrete particle swarm optimization algorithm(DPSO);chaos searching strategy","Chaos;Particle swarm optimization;Software algorithms;Databases;Heuristic algorithms;Optimization;Testing","","","","1","12","","","","","","IEEE","IEEE Conferences"
"A Regression Testing Approach for Software Product Lines Architectures","P. A. d. M. S. Neto; I. d. C. Machado; Y. C. Cavalcanti; E. S. d. Almeida; V. C. Garcia; S. R. d. L. Meira","NA; NA; NA; NA; NA; NA","2010 Fourth Brazilian Symposium on Software Components, Architectures and Reuse","","2010","","","41","50","In the Software Product Lines (SPL) context, where products are derived from a common platform, the reference architecture can be considered the main asset. In order to maintain its correctness and reliability after modifications, a regression testing approach based on architecture specification and code was developed. It aims to reduce the testing effort, by reusing test cases, execution results, as well as, selecting and prioritizing an effective set of test cases. Taking advantage of SPL architectures similarities, this approach can be applied among product architectures and between the reference and product architecture. This study also presents an evaluation performed in order to calibrate and improve the proposed approach.","","978-1-4244-8707-3978-0-7695-4259","10.1109/SBCARS.2010.14","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5631684","Software Product Lines;Regression Testing;Reference Architecture.","Testing;Software;Computer architecture;Maintenance engineering;Context;Planning;Feature extraction","product development;regression analysis;software architecture;software reliability","regression testing approach;software product lines architectures;architecture specification","","4","31","","","","","","IEEE","IEEE Conferences"
"Crashworthiness design optimization of S-rail","Hequan Wu; Xiaoming Ou; Hongwei Hu","College of Automotive and Mechanical Engineering, Changsha University of Science and Technology, China; College of Automotive and Mechanical Engineering, Changsha University of Science and Technology, China; College of Automotive and Mechanical Engineering, Changsha University of Science and Technology, China","2011 Second International Conference on Mechanic Automation and Control Engineering","","2011","","","1123","1126","The passenger vehicle rail is a major energy-absorbing member in the vehicle structure. Many longitudinal rails of vehicle frame have S-shaped structures. Design of such components is crucial in the automotive industry to ensure the vehicle structural integrity and the occupant safety during the vehicle impacts. Optimum crashworthiness design of the vehicle rail structure is one of the crucial tasks in designing crashworthy vehicles. In this paper, an efficient design optimization methodology is presented and utilized to obtain optimum crashworthiness design of the S-rail structures. The methodology adopted in this research makes use of Design of Experiments based approximation methods, numerical optimization algorithms and structural analysis software. The design optimization approach has been devoted to structural impact applications. The nonlinear explicit Finite Element code PAM-CRASH was used to conduct the rail impact problems and generate the energy functions to be maximized. Several design parameters have been proposed and used to optimize the rail structure. The results indicated the promising capabilities of the developed methodology for design optimization of the energy absorbing rails in automotive industry.","","978-1-4244-9439-2978-1-4244-9436-1978-1-4244-9438","10.1109/MACE.2011.5987133","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5987133","finite element;optimization;energy-absorbing;RSM;rail","Approximation methods;Rails;Vehicles;Vehicle crash testing;Design optimization;Response surface methodology","automobile industry;design engineering;finite element analysis;impact testing;optimisation;rails;road safety;vehicle dynamics","crashworthiness design optimization;passenger vehicle rail;vehicle structure;longitudinal rail;vehicle frame;S-shaped structure;automotive industry;vehicle structural integrity;occupant safety;vehicle impact;vehicle rail structure;crashworthy vehicle;design optimization methodology;S-rail structure;design of experiment;approximation method;numerical optimization algorithm;structural analysis software;finite element code PAM-CRASH","","1","7","","","","","","IEEE","IEEE Conferences"
"Dealing with Constraints in Boolean Expression Testing","A. Gargantini","NA","2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops","","2011","","","322","327","When testing a Boolean expression, one should consider also the constraints among the variables contained in it. Constraints model interdependence among the conditions in the expressions. Only tests that satisfy the constraints. i.e. valid tests, are really useful and can be applied to test the expression. We present three ways to deal with such constraints: (1) ignoring them during test generation and removing invalid tests later, (2) including them in the expression as conjoint and again removing invalid tests later, and (3) considering them during the test generation process in order to generate only valid tests from the start. We introduce a general framework in which the three policies are implemented and compared over a set of Boolean expressions commonly used as benchmarks. Although the third policy requires a constraints solving technique for actual test generation, it presents several benefits: it generates smaller test suites and it may require less time for tests generation. Moreover, ignoring the constraints during test generation can reduce the fault detection capability of the tests.","","978-0-7695-4345-1978-1-4577-0019","10.1109/ICSTW.2011.94","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954429","logic testing;Boolean specification testing;test generation","Fault detection;Optimization;Monitoring;Computational modeling;Logic testing;Terminology","program testing;programming languages","Boolean expression testing;testing constraint;test generation process;test fault detection capability","","1","23","","","","","","IEEE","IEEE Conferences"
"Numerical Constraints for Combinatorial Interaction Testing","P. M. Kruse; J. Bauer; J. Wegener","NA; NA; NA","2012 IEEE Fifth International Conference on Software Testing, Verification and Validation","","2012","","","758","763","Constraints can be found in many specifications of a software system. The impact of constraints varies with the test problem, but their presence causes problems for many existing combinatorial interaction testing (CIT) tools. Of the numerous existing tools supporting CIT design only a few offer full constraints support. Of these few tools those with full published details are even rarer. In extension to existing Boolean constraints we propose numerical constraints. We discuss definition, usage and handling in this work and integrate results with the classification tree method.","2159-4848","978-0-7695-4670-4978-1-4577-1906","10.1109/ICST.2012.170","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200182","classification tree method;combinatorial interaction testing;constraints","Data structures;Boolean functions;Upper bound;Optimization;Mice;Software testing","Boolean algebra;formal specification;pattern classification;program testing;software tools;trees (mathematics)","software system specification;test problem;combinatorial interaction testing tools;CIT design;Boolean constraints;numerical constraints;classification tree method","","3","17","","","","","","IEEE","IEEE Conferences"
"Comparison of metahuristic test generation strategies based on interaction elements coverage criterion","B. S. Ahmed; K. Z. Zamli","School of Electrical and Electronic Engineering, Universiti Sains Malaysia, 14300 Nibong Tebal, Penang, Malaysia; School of Electrical and Electronic Engineering, Universiti Sains Malaysia, 14300 Nibong Tebal, Penang, Malaysia","2011 IEEE Symposium on Industrial Electronics and Applications","","2011","","","550","554","Interaction testing represents an important technique, among those broader testing techniques involved in the software test data generation process. Within this technique, test cases are selected using a combination of test input parameters. We normally want that all combinations of parameters' values (called interaction elements) occur in the test suite at least once. Metaheuristics search algorithms have been used for constructing an interaction test suite by constructing test cases that can cover all the interaction elements. This paper introduces the interaction elements coverage as criterion to compare different metaheuristic interaction test suites generation strategies. In doing so, this paper gives an extensive review for different metaheuristic test generation strategies. The comparison results shows that by using the particle swarm optimization more interaction elements can be covered with fewer test cases and iterations.","","978-1-4577-1417-7978-1-4577-1418-4978-1-4577-1416","10.1109/ISIEA.2011.6108773","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6108773","interaction testing;metahueristics;combinatorial strategies;particle swarm optimization","Software;Particle swarm optimization;Systematics;Software testing;Conferences","particle swarm optimisation;program testing;search problems","metahuristic test generation strategies;interaction elements coverage criterion;software test data generation process;metaheuristics search algorithms;particle swarm optimization","","1","22","","","","","","IEEE","IEEE Conferences"
"Improvements in estimating software reliability from growth test data","D. Dwyer; P. D'Onofrio","BAE Systems, NCA012238, 95 Canal Street., Nashua, NH 03064; BAE Systems, NCA01119B, 95 Canal Street., Nashua, NH 03064","2011 Proceedings - Annual Reliability and Maintainability Symposium","","2011","","","1","5","John Musa's first book on Software Reliability Engineering advises the analyst to use the Musa Basic Law or the Musa-Okumoto Logarithmic Law to estimate failure intensity, depending on which provides the best fit to the data. He refered to the papers by Tractenberg and Downs as providing a foundation for these models. I propose using Musa's basic model in combination with an approach Duane and Codier used to estimate failure intensity (aka instantaneous failure rate) for software. Musa noted in his last book that the Basic law is optimistic in estimating residual errors and that the logarithmic law is pessimistic because it implies infinite errors. There is currently a problem in deciding which law to use for software, Basic or Logarithmic, other than which has a higher correlation coefficient. I recommend that Musa's basic law be used but that the line be drawn using a method that reflects the cumulative nature of the statistics. This approach is based on both Downs' paper and that presented by E. O. Codier at the 1968 Annual Symposium on Reliability which described how to draw the line for Duane growth plots. Codier argued that when reliability growth data is plotted: (1) “The latter points, having more information content, must be given more weight than earlier points” (2) “The normal curve fitting procedure of drawing the line through the `center of gravity' of all the points should not be used.” and (3) “Unless the data is exceptionally noisy, the best procedure is to start the line on the last data point and seek the region of highest density of points to the left of it.” With regard to Musa basic plots, the region of highest density would be to the right of the last point, not to the left of it. It should be noted here that the IEEE Recommended Practice on Software Reliability for the application of Duane states that “Least squares estimates for a and b of the straight line on log-log paper can be derived using recommended practice linear regression estimates”. But Codier's recommendations (above) have been shown to result in a more accurate measure of MTBF for hardware. This would be just as true for the application of Duane growth plo ts for software as for hardware. This paper shows even greater improvements when Codier's methods are applied to Musa's Basic model for Software. The early paper by Thomas Downs referred to the curved lines that are fitted to operational profile data as “convex”, not logarithmic, because there is no firm basis for calling the distribution logarithmic. There are examples in physics of exponential decay, as in the half life of a radioactive element, that follow a logarithmic curve, but no such mechanism exists to justify fitting a log function to software test data. The recommended method avoids defining the optimistic and pessimistic extremes of the curves as linear, logarithmic or any other specific shape. It simply draws a line that follows the changing slope of the points naturally as originally proposed by Codier. This paper develops a methodology for calculating failure intensity from the slope of the resulting line and from the cumulative failure rate at the final data point. It avoids the optimism of the Basic law and the pessimism of the Logarithmic law as well as the decision of which to use.","0149-144X;0149-144X","978-1-4244-8856-8978-1-4244-8857-5978-1-4244-8855","10.1109/RAMS.2011.5754434","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5754434","Duane;Musa Basic Model;Musa-Okumoto Logarithmic Law;reliability growth;software reliability","Software reliability;Testing;Gravity;Reliability engineering;Software systems","curve fitting;least squares approximations;program testing;regression analysis;software fault tolerance;software process improvement","software reliability estimation;growth test data;Musa basic law;Musa-Okumoto logarithmic law;failure intensity estimation;residual error estimation;cumulative statistics;curve fitting;least squares estimation;linear regression estimate;MTBF;Duane growth plots;distribution logarithmic;Codier method","","2","9","","","","","","IEEE","IEEE Conferences"
"A statistical approach to TPS transport optimization","J. L. Orlet; G. L. Murdock","The Boeing Company, Ground Support Systems, St. Louis, MO, USA; The Boeing Company, Ground Support Systems, St. Louis, MO, USA","2011 IEEE AUTOTESTCON","","2011","","","66","69","This paper discusses the statistical challenges of TPS Transport. A TPS is not considered fieldable until all tests are passing. Based on the number of tests in a TPS and the inherent complexity of the transport process, the probability exists that less than 100% of all tests will pass the first time they are tested after undergoing the transport process. Optimizing what types of tests are transported significantly improves the probability that the next TPS will be successfully transported. This paper illustrates that prioritizing which functions to focus on can greatly improve the probability of success while reducing the overall characterization effort.","1558-4550;1088-7725;1088-7725","978-1-4244-9363-0978-1-4244-9362-3978-1-4244-9361","10.1109/AUTEST.2011.6058752","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6058752","TPS;Transport;Optimization","Hardware;Testing;Runtime;Power supplies;Schedules;Emulation;Voltage measurement","automatic test equipment;automatic testing;optimisation;probability;software maintenance;statistical analysis","statistical approach;TPS transport optimization;transport process;probability;legacy system","","","3","","","","","","IEEE","IEEE Conferences"
"TestFul: An Evolutionary Test Approach for Java","L. Baresi; P. L. Lanzi; M. Miraz","NA; NA; NA","2010 Third International Conference on Software Testing, Verification and Validation","","2010","","","185","194","This paper presents TestFul, an evolutionary testing approach for Java classes that works both at class and method level. TestFul exploits a multi-objective evolutionary algorithm to identify the “best” tests. The paper introduces the main elements of TestFul. It also compares TestFul against well-known search-based solutions using a set of classes taken from literature, known software libraries, and independent testing benchmarks. The comparison considers statement and branch coverage, size of generated tests, and generation time. On considered classes, TestFul generates better tests than other search-based solutions, and achieves higher structural coverages with tests small enough to be usable.","2159-4848","978-1-4244-6436-4978-1-4244-6435-7978-0-7695-3990","10.1109/ICST.2010.54","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477084","Test Generation;Evolutionary Algorithms;Stateful Systems;Object-Oriented Paradigm;Multi-Objective Optimization","Java;Software testing;System testing;Evolutionary computation;Flow graphs;Benchmark testing;Software libraries;Proposals;Object oriented modeling;Simulated annealing","evolutionary computation;Java;program testing","TestFul;Java;evolutionary testing;multiobjective evolutionary algorithm;search-based solution","","24","31","","","","","","IEEE","IEEE Conferences"
"Preemptive Regression Test Scheduling Strategies: A New Testing Approach to Thriving on the Volatile Service Environments","L. Mei; K. Zhai; B. Jiang; W. K. Chan; T. H. Tse","NA; NA; NA; NA; NA","2012 IEEE 36th Annual Computer Software and Applications Conference","","2012","","","72","81","A workflow-based web service may use ultra-late binding to invoke external web services to concretize its implementation at run time. Nonetheless, such external services or the availability of recently used external services may evolve without prior notification, dynamically triggering the workflow-based service to bind to new replacement external services to continue the current execution. Any integration mismatch may cause a failure. In this paper, we propose Preemptive Regression Testing (PRT), a novel testing approach that addresses this adaptive issue. Whenever such a late-change on the service under regression test is detected, PRT preempts the currently executed regression test suite, searches for additional test cases as fixes, runs these fixes, and then resumes the execution of the regression test suite from the preemption point.","0730-3157;0730-3157;0730-3157","978-1-4673-1990-4978-0-7695-4736","10.1109/COMPSAC.2012.17","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6340125","adaptive service composition;adaptive regression test¬ing;preemptive regression testing;test case prioritization","Testing;Educational institutions;Availability;Books;Service oriented architecture","program testing;regression analysis;Web services","preemptive regression test scheduling strategies;volatile service environments;dynamically triggered workflow-based external Web service;ultralate binding;run time;integration mismatch;PRT;test cases;preemption point;adaptive service composition","","3","26","","","","","","IEEE","IEEE Conferences"
"An optimized method for generating cases of metamorphic testing","Leilei Chen; Lizhi Cai; Jiang Liu; Zhenyu Liu; Shiyan Wei; Pan Liu","School of Information Science &amp; Engineering East China University of Science and Technology, Shanghai, China; School of Information Science &amp; Engineering East China University of Science and Technology, Shanghai, China; School of Information Science &amp; Engineering East China University of Science and Technology, Shanghai, China; Shanghai Key Laboratory of Computer Software Evaluating &amp; Testing, China; School of Information Science &amp; Engineering East China University of Science and Technology, Shanghai, China; College of Computer Engineering and Science, Shanghai Business School, China","2012 6th International Conference on New Trends in Information Science, Service Science and Data Mining (ISSDM2012)","","2012","","","439","443","The information contained in the successful test case has been fully tapped by metamorphic testing which can effectively solve the oracle problem of software testing. One of the key factors affecting the results of the metamorphic testing is the generation of test cases. In this paper, we propose a criterion called ECCEM (Equivalence-Class Coverage for Every Metamorphic Relation), which covers the test cases based on equivalence classes, the criterion can availably generate fewer test case sets with high detection rate. This paper also proposes a new measure of test cases - the Test Case Rate of utilization (TCR), which can comprehensively assess the generated test suite.","","978-89-94364-20-9978-1-4673-0876","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6528673","metamorphic testing;ECCEM criterion;measurement criterion;test case rate of utilization","","equivalence classes;program testing","optimized method;metamorphic testing;oracle problem;software testing;ECCEM criterion;equivalence-class coverage for every metamorphic relation;test case rate of utilization;TCR","","","9","","","","","","IEEE","IEEE Conferences"
"4UMS Cassava harvester type of simulation analysis and structural optimization","Yuan Zhang; Zhende Cui; H. Huang; Ming Li","Agri-Machinery Research Institute of Chinese Academy of Tropical Agricultural Sciences, Zhanjiang, China; Agri-Machinery Research Institute of Chinese Academy of Tropical Agricultural Sciences, Zhanjiang, China; Agri-Machinery Research Institute of Chinese Academy of Tropical Agricultural Sciences, Zhanjiang, China; Agri-Machinery Research Institute of Chinese Academy of Tropical Agricultural Sciences, Zhanjiang, China","2011 3rd International Conference on Computer Research and Development","","2011","4","","45","48","The first few batches of cassava harvester embedded depth is not deep enough or too deep, the rate of pick up is not efficient and not efficient cassava into the soil of time differences, and for different levels of the above limitations, this paper through simulation analysis and trajectory analysis of cassava harvester to identify the performance of the structural parameters of the above. Using ADAMS software, kinematics simulation of the prototype test and optimize the main technical parameters of the next generation prototype, the results show that the virtual test and practical test, the basic trend of convergence, the relative error of 9.7% ~ 16.8%, the simulation analysis of the Cassava harvester of structural design and optimization of the basic compliance testing requirements.","","978-1-61284-840-2978-1-61284-839-6978-1-61284-838","10.1109/ICCRD.2011.5763850","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5763850","Simulation;Optimization;Design;Harveste","Soil;Analytical models;Prototypes;Agriculture;Computational modeling;Simulation;Optimization","agricultural machinery;crops;design engineering;machine testing;machinery production industries;mechanical engineering computing;optimisation","4UMS cassava harvester;structural optimization;trajectory analysis;ADAMS software;kinematics simulation;virtual testing;structural design;compliance testing requirements;pick up rate","","1","8","","","","","","IEEE","IEEE Conferences"
"A Precise Dynamic Flaw-Detection Method for Embedded Software","F. Liu; J. Liu; S. Li; M. Hou","NA; NA; NA; NA","2010 International Conference on Computational Intelligence and Software Engineering","","2010","","","1","4","With the continuous development of embedded technology, embedded software is in great demand, which makes how to dynamically find the code that affect the efficiency of embedded software and optimize the embedded software performance with it critical for embedded software development. This paper begins with analysis into the shortcomings of current flaw-detection methods and related tools available; and proposes an accurate, real-time online dynamic flaw-detection method, which can test the hardware-platform-related and instruction-inefficiency-causing flaw event; for example, the pipeline flush caused by branch prediction failure. The method is then development into implemented tools. Finally, with the use of the tool on the unit code, it demonstrates flaw event tests such as data cache missing to verify the correctness of the proposed method.","","978-1-4244-5391-7978-1-4244-5392","10.1109/CISE.2010.5676822","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676822","","Registers;Embedded software;Phasor measurement units;Kernel;Real time systems;Linux","cache storage;embedded systems;flaw detection;program compilers;program testing;security of data;software performance evaluation","precise dynamic flaw-detection method;continuous development;embedded technology;embedded software performance;embedded software development;real-time online dynamic flaw-detection method;hardware-platform-related flaw event;instruction-inefficiency-causing flaw event;pipeline flush;branch prediction failure;unit code;flaw event tests;data cache missing","","","8","","","","","","IEEE","IEEE Conferences"
"Challenges with Software Verification and Validation Activities in the Space Industry","R. Feldt; R. Torkar; E. Ahmad; B. Raza","NA; NA; NA; NA","2010 Third International Conference on Software Testing, Verification and Validation","","2010","","","225","234","Developing software for high-dependable space applications and systems is a formidable task. With new political and market pressures on the space industry to deliver more software at a lower cost, optimization of their methods and standards need to be investigated. The industry has to follow standards that strictly set quality goals and prescribes engineering processes and methods to fulfill them. The overall goal of this study is to evaluate if current use of the standards from the European Cooperation for Space Standardization (ECSS) is cost efficient and if there are ways to make the process leaner while still maintaining quality and to analyze if their verification and validation (V&amp;V) activities can be optimized. This paper presents results from two industrial case studies of companies in the European space industry that are following ECSS standards in various V&amp;V activities. The case studies reported here focus on how ECSS standards are used by the companies, how that affects their processes and, in the end, how their V&amp;V activities can be further optimized.","2159-4848","978-1-4244-6436-4978-1-4244-6435-7978-0-7695-3990","10.1109/ICST.2010.37","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477080","case study;European Cooperation for Space Standardization;verification and validation","Aerospace industry;Computer industry;Space technology;Standards development;Application software;Cost function;Standardization;Software quality;Software testing;Optimization methods","aerospace industry;program verification;safety-critical software;software standards","software verification;software validation;high-dependable space applications;European Cooperation for Space Standardization;European space industry;ECSS standards","","3","27","","","","","","IEEE","IEEE Conferences"
"Search Based Software Engineering: A Review from the Brazilian Symposium on Software Engineering","S. R. Vergilio; T. E. Colanzi; A. T. R. Pozo; W. K. G. Assuncao","NA; NA; NA; NA","2011 25th Brazilian Symposium on Software Engineering","","2011","","","50","55","Search Based Software Engineering (SBSE) is the field of software engineering research and practice that applies search based techniques to solve different Software Engineering problems. SBSE contributes to reduce efforts and costs associated to software development since the solutions found by search techniques satisfy constraints that are usually in conflict and, in general, are very difficult to be obtained for software engineers. The field is emerging in Brazil, the number of published works and research groups have significantly increased in the last three years. The goal of this paper is to divulge SBSE, serving as a reference to this novel field and contributing to its consolidation in Brazil. The paper introduces SBSE concerns and provides a review of works produced by the Brazilian community, related to the Brazilian Symposium on Software Engineering (SBES), discussing trends, challenges, and open research problems for this emergent area.","","978-1-4577-2187","10.1109/SBES.2011.13","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065145","search based algorithms;software engineering","Software engineering;Software;Optimization;Communities;Software testing;Programming","search problems;software engineering","search based software engineering;Brazilian symposium;software development;open research problems","","1","48","","","","","","IEEE","IEEE Conferences"
"Testing a High Performance, Random Access Priority Queue: A Case Study","J. D. McCaffrey; A. Bonar","NA; NA","2011 Eighth International Conference on Information Technology: New Generations","","2011","","","280","285","This paper presents a case study of the functional verification of a custom implementation of a random access priority queue which was optimized for performance. Although data structures have been used for decades few studies have examined the effectiveness of different testing strategies applied to complex data structures. In this study, four different testing approaches were used to test a priority queue. The results showed that a state transition testing approach (13 faults discovered) was clearly superior with regards to the number of faults found than the alternatives of a manual testing approach (3 faults discovered), a unit testing approach (4 faults discovered), and a classical test harness approach (6 faults discovered). Because the state transition testing approach used was in essence a modified form of random input testing, the results of this study suggest that the notion that random input testing is typically less effective than other forms of testing may be an overly broad generalization.","","978-1-61284-427-5978-0-7695-4367","10.1109/ITNG.2011.56","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5945247","Priority queue;software testing;state transition;test harness;unit testing","Testing;Arrays;Indexes;Manuals;Queueing analysis;Algorithm design and analysis","data structures;formal verification;program testing;queueing theory","random access priority queue;functional verification;data structure;state transition testing approach;manual testing approach;unit testing approach;classical test harness approach","","","10","","","","","","IEEE","IEEE Conferences"
"The Research of Path-Oriented Test Data Generation Based on a Mixed Ant Colony System Algorithm and Genetic Algorithm","M. Yi","NA","2012 8th International Conference on Wireless Communications, Networking and Mobile Computing","","2012","","","1","4","It is very practical significance to seek an effective path-oriented test data automatic generation method. The genetic algorithm, ant colony algorithm is commonly used to generate test data, and the both can improve the efficiency of test data generation. But, for both algorithms, there was a little limitation to target path in path testing for being prone to local optimal solution. Some researchers have combined the genetic algorithm and ant colony algorithm to generate the test data path, in which the result was better. At the same time, they found hybrid ant colony algorithm was still subject to the limitation of global search ability of ant colony algorithm. The Ant colony system algorithm is improved based on the ant colony algorithm. It is proved that it is more suitable for global search. In the present study, we propose to combine the ant colony system algorithm and genetic algorithm (ACSGA) to generate path-oriented software testing data. Classical triangle discrimination problem in path-oriented software testing is chose as a simulation experiment to verify ACSGA. The results show that the generation efficiency of target path has been improved apparently.","2161-9654;2161-9646;2161-9646","978-1-61284-683-5978-1-61284-684-2978-1-61284-682","10.1109/WiCOM.2012.6478716","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6478716","","Genetic algorithms;Algorithm design and analysis;Software algorithms;Software testing;Software;Heuristic algorithms","ant colony optimisation;genetic algorithms;program testing;search problems","mixed ant colony system algorithm;genetic algorithm;automatic test data generation method;test data generation efficiency improvement;local optimal solution;hybrid ant colony algorithm;global search ability;path-oriented software testing data generation;classical triangle discrimination problem;ACSGA verification","","","16","","","","","","IEEE","IEEE Conferences"
"Design and Routine Test Optimization of Modern Protection Systems With Reliability and Economic Constraints","A. H. Etemadi; M. Fotuhi-Firuzabad","Electrical and Computer Engineering Department, University of Toronto, Toronto, Canada; Center of Excellence in Power System Control and Management, Department of Electrical Engineering, Sharif University of Technology, Tehran, Iran","IEEE Transactions on Power Delivery","","2012","27","1","271","278","This paper approaches the topic of protection system reliability from an economic point of view by 1) designing an optimal modern protective relay based on a prescribed level of reliability subject to economic constraints and 2) determining optimal routine test intervals by balancing the cost of routine tests and losses due to relay failure. This paper proposes a method that enables the designer to optimally select hardware and software components of a digital protective relay to gain the highest possible overall reliability with a restricted budget. This design can be viewed as a reliability and redundancy allocation problem for which a new easy-to-implement algorithm is proposed. In contrast to the existing literature that views the problem of routine test interval optimization solely from a reliability viewpoint, this paper proposes an optimization procedure for determining optimal test intervals. The optimization objective function consists of three terms: cost of routine tests, losses due to relay unresponsiveness, and losses due to relay maloperation. The value of the objective function is calculated based on a new, simple, and comprehensive Markov model. Illustrative numerical examples clarify the application of the proposed methods and demonstrate their effectiveness in achieving an optimum for both cases.","0885-8977;1937-4208","","10.1109/TPWRD.2011.2170859","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6078449","Design;optimization;protection systems;reliability;routine tests","Software reliability;Power system reliability;Relays;Optimization;Hardware;Software","maintenance engineering;Markov processes;power system economics;power system reliability;redundancy;relay protection;testing","routine test optimization;modern protection system design;protection system reliability;economic constraints;optimal modern protective relay;optimal routine test;relay failure;digital protective relay;redundancy allocation problem;optimization procedure;relay maloperation;Markov model","","10","22","","","","","","IEEE","IEEE Journals & Magazines"
"Optimizing software integration by considering integration test complexity and test effort","M. Steindl; J. Mottok","University of Applied Sciences Regensburg, Laboratory for Safe and Secure Systems (LaS3), Seybothstr. 2, D-93053 Regensburg, Germany; University of Applied Sciences Regensburg, Laboratory for Safe and Secure Systems (LaS3), Seybothstr. 2, D-93053 Regensburg, Germany","Proceedings of the 10th International Workshop on Intelligent Solutions in Embedded Systems","","2012","","","63","68","Software integration testing is often a bottleneck in the development process. Selecting the next component to integrate often depends heavily on integrators expertise or solely on the integration schedule. This may lead to an increasing number of stubs and makes testing more difficult. In this work we present a novel metric to calculate the test complexity of a certain integration order and provide an approach for optimizing it with respect to the integration test complexity and the integration test effort using simulated annealing.","","978-3-902463-09-8978-1-4673-2464","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6273606","","Complexity theory;Testing;Software;Simulated annealing;Measurement;Couplings","program testing;simulated annealing","integration test complexity;software integration testing;development process;integration schedule;integration order;integration test effort;simulated annealing","","","26","","","","","","IEEE","IEEE Conferences"
"Modified particle swarm optimization and its application in multimodal function optimization","Na Li; Song Zhu","State Key Lab of Software Engineering, Wuhan University, China; Center of Computing &amp; Experimenting, South-central University for Nationalities, Wuhan, China","Proceedings 2011 International Conference on Transportation, Mechanical, and Electrical Engineering (TMEE)","","2011","","","375","378","In multimodal optimization, the basic particle swarm optimization is easy to duplicate and miss points of the optimal value. To solve this problem, a modified particle swarm optimization algorithm, called BNPSO, is proposed. This modified particle swarm optimization algorithm is based on the niche particle swarm optimization (NPSO) algorithm, and implemented a Bernoulli trial. It is proved theoretically that the algorithm BNPSO is much more effective than the algorithm NPSO for multimodal function optimization problems. However, the time complexity of the new scheme is increased. Testing of the algorithm indicate that the algorithm BNPSO has better perform in stability and convergence.","","978-1-4577-1701-7978-1-4577-1700-0978-1-4577-1699","10.1109/TMEE.2011.6199221","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6199221","multimodal function optimization;particle swarm optimization;niche technology;Bernoulli trial","Particle swarm optimization;Algorithm design and analysis;Optimization;Vectors;Signal processing algorithms;Mathematical model;Genetic algorithms","particle swarm optimisation","modified particle swarm optimization;multimodal function optimization application;optimal value;niche particle swarm optimization;NPSO;Bernoulli trial","","","10","","","","","","IEEE","IEEE Conferences"
"Structure optimization of door-beam base on strength of side doors","Lu Fang; Wang Deng-feng","State Key Laboratory of Automotive Simulation and Control, Jilin University, Changchun, China; State Key Laboratory of Automotive Simulation and Control, Jilin University, Changchun, China","Proceedings 2011 International Conference on Transportation, Mechanical, and Electrical Engineering (TMEE)","","2011","","","766","769","Side doors are important parts of a vehicle, whose strength has big influence for the structure of the car and hence the safety of the passengers. Chinese National Regulation (GB15743-1995) has clear requirements for car doors and their testing processes. In this paper, we analyze the characteristic structure of the car door and study specifically on the structure of door beam and its influence for door strength. A finite element model of vehicle for door impact is created. We analyze the result by LS-DYNA software and validate the efficiency of this model through door impact test. The result shows that the initial crush resistance is less than the value required by national regulation. Through optimization of the cross structure of the door beam, the strength of side door is improved, and the result satisfies the requirement of the regulation.","","978-1-4577-1701-7978-1-4577-1700-0978-1-4577-1699","10.1109/TMEE.2011.6199315","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6199315","Passenger car;door;performance;door-beam;structure optimization","Resistance;Optimization;Immune system;Finite element methods;Load modeling;Analytical models;Force","automotive components;beams (structures);doors;finite element analysis;impact testing;optimisation;road safety","structure optimization;door beam;side door strength;passenger safety;Chinese National Regulation;GB15743-1995;car door testing process;finite element model;LS-DYNA software;door impact test;initial crush resistance","","","10","","","","","","IEEE","IEEE Conferences"
"PACOGEN: Automatic Generation of Pairwise Test Configurations from Feature Models","A. Hervieu; B. Baudry; A. Gotlieb","NA; NA; NA","2011 IEEE 22nd International Symposium on Software Reliability Engineering","","2011","","","120","129","Feature models are commonly used to specify variability in software product lines. Several tools support feature models for variability management at different steps in the development process. However, tool support for test configuration generation is currently limited. This test generation task consists in systematically selecting a set of configurations that represent a relevant sample of the variability space and that can be used to test the product line. In this paper we propose \pw tool to analyze feature models and automatically generate a set of configurations that cover all pair wise interactions between features. \pw tool relies on constraint programming to generate configurations that satisfy all constraints imposed by the feature model and to minimize the set of the tests configurations. This work also proposes an extensive experiment, based on the state-of-the art SPLOT feature models repository, showing that \pw tool scales over variability spaces with millions of configurations and covers pair wise with less configurations than other available tools.","2332-6549;1071-9458;1071-9458","978-1-4577-2060-4978-0-7695-4568","10.1109/ISSRE.2011.31","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6132960","","Testing;Minimization;Programming;Optimization;Arrays;Data models","automatic test pattern generation;formal specification;program testing;software development management;software tools","automatic test configuration generation;software product lines;formal specification;software tools;software variability management;software development;Pacogen;SPLOT feature models","","16","22","","","","","","IEEE","IEEE Conferences"
"Automated generation of test cases from output domain of an embedded system using Genetic algorithms","C. P. Vudatha; S. Nalliboena; S. K. Jammalamadaka; B. K. K. Duvvuri; L. S. S. Reddy","Dept. of Information Technology, KL University; Dept. of Information Technology, KL University; Dept. of Information Technology, KL University; Department of Freshmen Engineering, KL University; LBR College of Engineering, Mylavaram","2011 3rd International Conference on Electronics Computer Technology","","2011","5","","216","220","A primary issue in black-box testing is how to generate adequate test cases from input domain of the system under test on the basis of user's requirement specification. However, for some types of systems including embedded systems, developing test cases from output domain is more suitable than developing from input domain, especially, when the output domain is smaller. This approach ensures better reliability of the system under test. In this paper, the authors present a new approach to automate the generation of test cases from output domain of a pilot project “Temperature Monitoring and Controlling of Nuclear Reactor System” (TMCNRS) which is an embedded system developed using modified Cleanroom Software Engineering methodology. An Automated Test Case Generator (ATCG) that uses Genetic algorithms (GAs) extensively and generates test cases from output domain is proposed. The ATCG generates test cases which are useful to conduct pseudo - exhaustive testing to detect single, double and several multimode faults in the system. The generator considers most of the combinations of outputs, and finds the corresponding inputs while optimizing the number of test cases generated. In order to investigate the effectiveness of this approach, test cases were generated by ATCG and the tests were conducted on the target embedded system at a minimum cost and time. Experimental results show that this approach is very promising.","","978-1-4244-8679-3978-1-4244-8678","10.1109/ICECTECH.2011.5941989","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5941989","Automated Test case generation;Output domain testing;Genetic algorithms;embedded systems;Cleanroom software engineering;combinatorial testing;pseudo-exhaustive testing","Testing;Temperature sensors;Genetic algorithms;Embedded systems;Relays;Temperature control","automatic test pattern generation;embedded systems;formal specification;genetic algorithms;program testing","automated test case generation;genetic algorithm;black-box testing;user requirement specification;cleanroom software engineering methodology;'Temperature Monitoring and Controlling of Nuclear Reactor System"";pseudoexhaustive testing;embedded system;multimode fault","","1","8","","","","","","IEEE","IEEE Conferences"
"Fast evolutionary solution finding for optimization using opposite gradient movement","T. Saenphon; C. Lursinsap","Advanced Virtual and Intelligent (AVIC) Center, Department of Mathematics, Faculty of Science, Chulalongkorn University, Bangkok, Thailand; Advanced Virtual and Intelligent (AVIC) Center, Department of Mathematics, Faculty of Science, Chulalongkorn University, Bangkok, Thailand","2011 Seventh International Conference on Natural Computation","","2011","3","","1498","1501","In this paper, a hybrid algorithm of gradient movement is proposed. On a surface of continuous function, every random point has a gradient value of the function that minimize and convergence to zero when it is a neighborhood with the optimum solution. Each iteration calculates the gradient of function at every point and chooses a minimum gradient point with a shortest distance from the optimum solution to find a new closer candidate to be an optimum point. The comparative experiments were made between CA_PSO, PSO, CACO, and SGA. Results show the proposed algorithm with gradient movement techniques outperforms other.","2157-9563;2157-9555;2157-9555","978-1-4244-9953-3978-1-4244-9950-2978-1-4244-9952","10.1109/ICNC.2011.6022343","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6022343","Swarm intelligence;Ant colony optimization (ACO);Particle swarm optimization (PSO);Gradient;Continuous Function Optimization","Optimization;Algorithm design and analysis;Testing;Particle swarm optimization;Ant colony optimization;Heuristic algorithms;Software algorithms","convergence;evolutionary computation;gradient methods;particle swarm optimisation","evolutionary solution;gradient movement;random point;convergence;iteration;minimum gradient point;shortest distance;ant colony optimization;swarm intelligence;particle swarm optimization;continuous function optimization","","","5","","","","","","IEEE","IEEE Conferences"
"Research on optimization of software development project process based on theory of project management","Q. Wang; Z. Song","Network Information Center, Tianjin University of Finance and Economics, China, 300222; School of Management, Tianjin University of Technology, China, 300384","2010 3rd International Conference on Advanced Computer Theory and Engineering(ICACTE)","","2010","1","","V1-426","V1-430","Project management is a comprehensive activity which involves knowledge, skills, tools and techniques in project activities to meet project demands. In software development project, management on project demands throughout the project cycle is the critical factor for success of the project. It is the most important issue for implementation of project management to reasonably define the customer demand and the definite scope of project. Therefore, in the project scope definition phase, it is conducive to improving the process of software development projects and achieving project objectives to accurately define and understand the true demand of customer by increasing key sub-module identification for product, pre-design, test and needs redefinition sub-process modules. In this way, the enterprises could also strengthen the capacity of management innovation and master core competencies in order to adapt themselves to the changing social environment and commercial environment.","2154-7505;2154-7491;2154-7491","978-1-4244-6542-2978-1-4244-6539-2978-1-4244-6541","10.1109/ICACTE.2010.5578983","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5578983","Project Management;Software Development Process;Project Management Scope","Software;Analytical models;Software reliability;Resource management;Encoding;Educational institutions;Humans","innovation management;project management;software development management","software development project;project management theory;project scope definition phase;product sub-process module;pre-design sub-process module;test sub-process module;needs redefinition sub-process module;management innovation","","1","11","","","","","","IEEE","IEEE Conferences"
"Prediction of compression bound and optimization of compression architecture for linear decompression-based schemes","J. Li; Y. Huang; D. Xiang","School of Software, Tsinghua University, Beijing, China; Mentor Graphics Corporation, 300 Nickerson Rd., Marlboro, MA 01752, USA; School of Software, Tsinghua University, Beijing, China","29th VLSI Test Symposium","","2011","","","297","302","On-chip linear decompression-based schemes have been widely adopted by industrial circuits nowadays to effectively reduce the ever increasing test data volume and test time. Though they can easily achieve relatively high compression ratio, there is a bound of effective compression ratio for these compression schemes. Prior work tried to address this problem by trying different compression architectures to identify this compression bound. However, they can not predict this compression bound efficiently. In this paper, we will first analyze the correlation between the effective compression ratio and the compression architecture, thus to predict that compression bound efficiently. In addition, this paper will also propose how to design the compression architecture for target effective compression ratio with one-pass calculation, which was usually done by a time-consuming try-and-error process as well in the current DFT flow. Experimental results show the accuracy of the prediction and the effectiveness of the compression architecture design.","2375-1053;1093-0167;1093-0167","978-1-61284-656-9978-1-61284-657-6978-1-61284-655","10.1109/VTS.2011.5783737","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5783737","test compression;linear decompression-based;test compression optimization;compression bound prediction","Computer architecture;Circuit faults;Input variables;Correlation;Accuracy;System-on-a-chip;Matrix converters","circuit optimisation;design for testability;integrated circuit design;integrated circuit testing","on-chip linear decompression-based schemes;compression bound;optimization;compression architecture design;compression ratio;one-pass calculation;time-consuming try-and-error process;DFT flow","","4","10","","","","","","IEEE","IEEE Conferences"
"Test Coverage of Data-Centric Dynamic Compositions in Service-Based Systems","W. Hummer; O. Raz; O. Shehory; P. Leitner; S. Dustdar","NA; NA; NA; NA; NA","2011 Fourth IEEE International Conference on Software Testing, Verification and Validation","","2011","","","40","49","This paper addresses the problem of integration testing of data-centric dynamic compositions in service-based systems. These compositions define abstract services, which are replaced by invocations to concrete candidate services at runtime. Testing all possible runtime instances of a composition is often unfeasible. We regard data dependencies between services as potential points of failure, and introduce the k-node data flow test coverage metric. Limiting the level of desired coverage helps to significantly reduce the search space of service combinations. We formulate the problem of generating a minimum set of test cases as a combinatorial optimization problem. Based on the formalization we present a mapping of the problem to the data model of FoCuS, a coverage analysis tool developed at IBM. FoCuS can efficiently compute near-optimal solutions, which we then use to automatically generate and execute test instances of the composition. We evaluate our prototype implementation using an illustrative scenario to show the end-to-end practicability of the approach.","2159-4848","978-1-61284-174-8978-0-7695-4342","10.1109/ICST.2011.55","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770593","","Testing;Concrete;Cities and towns;Web services;Runtime;Measurement;Optimization","combinatorial mathematics;optimisation;program testing;search problems;service-oriented architecture;software metrics;systems analysis","data-centric dynamic composition;service-based system;integration testing;abstract service;data dependency;k-node data flow test coverage metric;search space reduction;combinatorial optimization problem;data model;FoCuS;coverage analysis tool","","6","23","","","","","","IEEE","IEEE Conferences"
"Optimizing the automatic test generation by SAT and SMT solving for Boolean expressions","P. Arcaini; A. Gargantini; E. Riccobene","Dip. di Tecnologie dell'Informazione - Universit&#x00E0; degli Studi di Milano, Italy; Dip. di Ing. dell'Informazione e Metodi Matematici - Universit&#x00E0; di Bergamo, Italy; Dip. di Tecnologie dell'Informazione - Universit&#x00E0; degli Studi di Milano, Italy","2011 26th IEEE/ACM International Conference on Automated Software Engineering (ASE 2011)","","2011","","","388","391","Recent advances in propositional satisfiability (SAT) and Satisfiability Modulo Theories (SMT) solvers are increasingly rendering SAT and SMT-based automatic test generation an attractive alternative to traditional algorithmic test generation methods. The use of SAT/SMT solvers is particularly appealing when testing Boolean expressions: These tools are able to deal with constraints over the models, generate compact test suites, and they support fault-based test generation methods. However, these solvers normally require more time and greater amount of memory than classical test generation algorithms, limiting their applicability. In this paper we propose several ways to optimize the process of test generation and we compare several SAT/SMT solvers and propositional transformation rules. These optimizations promise to make SAT/SMT-based techniques as efficient as standard methods for testing purposes, especially when dealing with Boolean expressions, as proved by our experiments.","1938-4300","978-1-4577-1639-3978-1-4577-1638","10.1109/ASE.2011.6100079","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6100079","","Optimization;Context;Limiting;Software testing;Context modeling;Libraries","Boolean functions;computability;optimisation;program testing","automatic test generation;Boolean expressions;optimization;prepositional satisfiability;satisfiability modulo theories solvers;compact test suites;propositional transformation rules","","3","15","","","","","","IEEE","IEEE Conferences"
"Minimum Pairwise Coverage Using Constraint Programming Techniques","A. Gotlieb; A. Hervieu; B. Baudry","NA; NA; NA","2012 IEEE Fifth International Conference on Software Testing, Verification and Validation","","2012","","","773","774","This paper presented the global constraint pairwise that can be used to enforce the presence of a given pair within a set of test cases or configurations. It also introduced several optimizations for implementing a method that computes the minimum set of test cases that covers pairwise. In addition, the method, seen as a constraint optimization problem, provides a way to compromise between time and efficiency by allowing anytime interruption or time-contract execution. Our approach has been implemented and evaluated on several instances of a test configurations generation problems [5] where input variables are boolean only. We envision to address other instances of these problem where the variables take their values in larger finite domains.","2159-4848","978-0-7695-4670-4978-1-4577-1906","10.1109/ICST.2012.174","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200186","Pairwise testing;constraint programming","Software testing;Conferences","constraint handling;optimisation","minimum pairwise coverage;constraint programming techniques;global constraint pairwise;test cases;test configurations;time-contract execution;finite domains","","2","6","","","","","","IEEE","IEEE Conferences"
"A Reactivity-based Framework of Automated Performance Testing for Web Applications","T. Gao; Y. Ge; G. Wu; J. Ni","NA; NA; NA; NA","2010 Ninth International Symposium on Distributed Computing and Applications to Business, Engineering and Science","","2010","","","593","597","To improve the reliability and feasibility of web applications, performance testing is very important for satisfying users. For reducing the cost and improve the efficiency of performance testing, we propose a new reactivity-based performance testing framework in this paper. We also provide a complete approach to generate test cases automatically from original web logs. First our approach retrieves user patterns through logs at the server side. Then, metrics derived from users' perspective are applied and usage pattern from client side are gained. At last test case can be generated automatically by solving an optimization problem through an evolutionary algorithm.","","978-1-4244-7540-7978-1-4244-7539-1978-0-7695-4110","10.1109/DCABES.2010.127","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5571539","performance testing;testing framework;automated test case generation;web applications","Testing;Unified modeling language;Measurement;Time factors;Servers;Software;Load modeling","automatic test pattern generation;client-server systems;evolutionary computation;Internet;performance evaluation","Web application;automated reactivity-based performance testing framework;Web logs;user pattern retrieval;test case generation;evolutionary algorithm;optimization problem","","1","12","","","","","","IEEE","IEEE Conferences"
"RankFuzz: Fuzz Testing Based on Comprehensive Evaluation","C. Li; Q. Wei; Q. Wang","NA; NA; NA","2012 Fourth International Conference on Multimedia Information Networking and Security","","2012","","","939","942","It has been proven successful that fuzz testing can successfully find security vulnerabilities in programs. However, traditional black box fuzz testing tools, which randomly mutate the input, are blind and ineffective. The white box fuzzing technology, known as the symbolic execution, is still facing the problem of low efficiency and path explosion. We present a new automated fuzzing technique based on comprehensive evaluation and a tool, Rank Fuzz, that implements this technique. By running dynamic taint analysis, we divide the input into several fields and make a rank to each of them according to the comprehensive evaluation results, in the hope that the potential vulnerability can be quickly found. We use several existing vulnerabilities to assess the reasonability of our evaluation system, finding that Rank Fuzz can effectively locate the bytes triggering the vulnerabilities and all of their ranks are on the top 30% of total fields. We also test two off-the-shelf applications within 8 hours and find 3 new vulnerabilities.","2162-8998","978-1-4673-3093-0978-0-7695-4852","10.1109/MINES.2012.161","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405840","fuzzing;dynamic taint analysis;comprehensive evaluation","Testing;Software;Security;Computer bugs;Complexity theory;Libraries;Semantics","automatic testing;fuzzy set theory;program testing;security of data;software reliability;system monitoring","RankFuzz;fuzz testing;automated fuzzing technique;comprehensive evaluation;dynamic taint analysis;security vulnerability;automatic software security testing technology","","","17","","","","","","IEEE","IEEE Conferences"
"The Generating Algorithm of Protocol Consistency Test Sequences Between Combined Control Flow and Data Flow","Y. Chen","NA","2010 International Conference on Machine Vision and Human-machine Interface","","2010","","","480","482","To take account of both date flow and control flow in protocol consistency test, based on EFSM, this article puts forward that in data flow test, the method of the data dependence relation between variables is reused to generate test sequence, which improves path coverage rate; it also puts forward that in control flow test, the method of sub-sequence shortest covering-path is created to generate control flow test sequence, and the path stack algorithm is applied to merge test sequence to optimize test sets.","","978-1-4244-6596-5978-1-4244-6595","10.1109/MVHI.2010.200","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5532636","protocol consistency test;EFSM;data flow;control flow;test sequence","Protocols;Software testing;Machine vision;Man machine systems;Meteorology;Optimization methods;Automata;Costs;Data analysis;Flow graphs","data flow analysis;finite state machines;program testing;protocols;sequences","protocol consistency test;combined control flow;data flow;EFSM;test sequences;sub-sequence shortest covering-path;path stack algorithm","","","4","","","","","","IEEE","IEEE Conferences"
"Dynamic testing for the seed tape tensions of the rice seed tape twisting machine","C. Hongguang; R. Wentao; Z. Fuling","College of Engineering, Shenyang Agricultural University, Shenyang, P. R. China; College of Engineering, Shenyang Agricultural University, Shenyang, P. R. China; College of Engineering, Shenyang Agricultural University, Shenyang, P. R. China","Proceedings of 2011 International Conference on Electronic & Mechanical Engineering and Information Technology","","2011","4","","1802","1805","In order to test the seed tape tensions of the rice seed tape twisting machine during the process, a dynamic testing system for the seed tape tensions was designed composed by the tensions sensor, data acquisition controller and PC, etc. The dynamic performance test of the machine was done and the seed tape tensions in the different speed was test by using the virtual instrument technology. The time domain analysis of the seed tape tensions was done by applied analysis software. The results show that, the seed tape tensions in the moving could be test by using tensions sensor and the virtual instrument technology. It has the important meaning for the seed tape tensions of dynamic measurement. It provided the basis for the dynamic performance testing and parameters optimization of the rice seed tape twisting machine.","","978-1-61284-088-8978-1-61284-087-1978-1-61284-086","10.1109/EMEIT.2011.6023454","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6023454","the rice seed tape;tensions testing of seed tape;virtual instrument technology;A/D conversion;dynamic testing","Testing;Calibration;Instruments;Mathematical model;Resistance;Force;Materials","agricultural machinery;crops;dynamic testing;optimisation;virtual instrumentation","rice seed tape tensions;rice seed tape twisting machine;virtual instrument technology;time domain analysis;dynamic measurement;dynamic performance testing;optimization","","","8","","","","","","IEEE","IEEE Conferences"
"Using Organizational Evolutionary Particle Swarm Techniques to Generate Test Cases for Combinatorial Testing","X. Pan; H. Chen","NA; NA","2011 Seventh International Conference on Computational Intelligence and Security","","2011","","","1580","1583","Based on the analysis of the characteristics of combinatorial testing, an organizational evolutionary particle swarm algorithm (OEPST) to generate test cases for combinatorial testing is proposed. This algorithm is used to select the test cases of local optimal coverage in current environment based on these test cases, and then a test suite satisfying the pair-wise coverage criterion is built. The empirical results show that this approach can effectively reduce the number of test case.","","978-1-4577-2008-6978-0-7695-4584","10.1109/CIS.2011.354","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6128395","organizational evolutionary;particle swarm;test cases;pairwise coverage","Organizations;Particle swarm optimization;Testing;Software algorithms;Algorithm design and analysis;Lead;Software","combinatorial mathematics;particle swarm optimisation","combinatorial testing;organizational evolutionary particle swarm algorithm;pair-wise coverage criterion","","1","8","","","","","","IEEE","IEEE Conferences"
"Dynamic adaptive Search Based Software Engineering","M. Harman; E. Burke; J. A. Clark; X. Yao","CREST Centre, University College London, Gower Street, London, WC1E 6BT, UK; University of Stirling, Stirling, FK9 4LA Scotland, UK; Department of Computer Science, University of York, Deramore Lane, York, YO105GH, UK; School of Computer Science The University of Birmingham Edgbaston, Birmingham B15 2TT, UK","Proceedings of the 2012 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement","","2012","","","1","8","Search Based Software Engineering (SBSE) has proved to be a very effective way of optimising software engineering problems. Nevertheless, its full potential as a means of dynamic adaptivity remains under explored. This paper sets out the agenda for Dynamic Adaptive SBSE, in which the optimisation is embedded into deployed software to create self-optimising adaptive systems. Dynamic Adaptive SBSE will move the research agenda forward to encompass both software development processes and the software products they produce, addressing the long-standing, and as yet largely unsolved, grand challenge of self-adaptive systems.","1949-3770;1938-6451;1949-3789","978-1-4503-1056-7978-1-4503-1056-7978-1-4503-1056","10.1145/2372251.2372253","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6475391","SBSE;Search Based Optimization;Self-Adaptive Systems;Autonomic Computing","Software engineering;Software;Optimization;Algorithm design and analysis;Testing;Search problems;Software algorithms","adaptive systems;search problems;software engineering","dynamic adaptive search based software engineering;dynamic adaptive SBSE;self-optimising adaptive systems;software development process;software products","","12","73","","","","","","IEEE","IEEE Conferences"
"EFindBugs: Effective Error Ranking for FindBugs","H. Shen; J. Fang; J. Zhao","NA; NA; NA","2011 Fourth IEEE International Conference on Software Testing, Verification and Validation","","2011","","","299","308","Static analysis tools have been widely used to detect potential defects without executing programs. It helps programmers raise the awareness about subtle correctness issues in the early stage. However, static defect detection tools face the high false positive rate problem. Therefore, programmers have to spend a considerable amount of time on screening out real bugs from a large number of reported warnings, which is time-consuming and inefficient. To alleviate the above problem during the report inspection process, we present EFindBugs to employ an effective two-stage error ranking strategy that suppresses the false positives and ranks the true error reports on top, so that real bugs existing in the programs could be more easily found and fixed by the programmers. In the first stage, EFindBugs initializes the ranking by assigning predefined defect likelihood for each bug pattern and sorting the error reports by the defect likelihood in descending order. In the second stage, EFindbugs optimizes the initial ranking self-adaptively through the feedback from users. This optimization process is executed automatically and based on the correlations among error reports with the same bug pattern. Our experiment on three widely-used Java projects (AspectJ, Tomcat, and Axis) shows that our ranking strategy outperforms the original ranking in Find Bugs in terms of precision, recall and F1-score.","2159-4848","978-1-61284-174-8978-0-7695-4342","10.1109/ICST.2011.51","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770619","Static Analysis Tool;Error Ranking;FindBugs","Detectors;Computer bugs;Java;Correlation;Optimization;Sorting;Software","inspection;Java;program debugging;program diagnostics;software tools","EFindBugs;error ranking;static analysis tool;static defect detection tool;report inspection process;error sorting;optimization process;Java project;AspectJ;Tomcat;Axis","","7","20","","","","","","IEEE","IEEE Conferences"
"Test Case Reduction Based on Program Invariant and Genetic Algorithm","N. Pan; F. Zeng; Y. Huang","NA; NA; NA","2010 6th International Conference on Wireless Communications Networking and Mobile Computing (WiCOM)","","2010","","","1","5","Test case reduction is the focus of the field of software testing. In this paper, we integrate random testing methods, invariant technology and genetic algorithms for test case reduction. Program invariant here refers to the properties at some points of program, it can reveal the extent of program data coverage and other valuable information. We evaluate the parent program invariants to filter corresponding set of test cases, and keep the better ones on the offspring. Generation by generation, we can get the final reduced set of test cases. This method is easy to automate. Experimental results and analysis show that compared to similar methods, at the same coverage conditions, the method proposed in this paper have higher efficiency and can get better set of test cases.","2161-9646;2161-9654","978-1-4244-3708-5978-1-4244-3709","10.1109/WICOM.2010.5601284","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5601284","","Algorithm design and analysis;Optimization;Software testing;Software;Heuristic algorithms;Software engineering","genetic algorithms;program testing","test case reduction;program invariant;software testing;random testing methods;invariant technology;genetic algorithms","","4","10","","","","","","IEEE","IEEE Conferences"
"Test Generation for X-machines with Non-terminal States and Priorities of Operations","K. Bogdanov","NA","2011 Fourth IEEE International Conference on Software Testing, Verification and Validation","","2011","","","130","139","Testing methods aiming to demonstrate that an implementation behaves the same as a specification X-machine (extended finite-state machine) usually assume that (1) all states are terminal states and (2) there are no priorities associated with operations on transitions. The considered model for the machine is such that outputs for transitions leading to non-terminal states will be buffered and contents of buffers will only be made observable when terminal states are entered. The X-machine testing method has been extended in this work to handle such an extension of X-machines (EFSM).Priorities of operations determine the order in which guards of transitions are evaluated. This makes it possible to reduce the size of a test suite. For instance, if testing has shown that a transition with a specific guard g has been implemented from some state, then no lower-priority transition with a guard implied by g may ever be executed from that state. It is hence not necessary to test for the presence of such a lower-priority transition.","2159-4848","978-1-61284-174-8978-0-7695-4342","10.1109/ICST.2011.59","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770602","Test Generation;Finite-State Machines;Extended Finite-State Machines;FSM;EFSM;X-machines","Testing;Schedules;Automata;Context;Unified modeling language;Data models","finite state machines;program testing","test generation;X-machines;nonterminal states;operation priority;extended finite-state machine","","","22","","","","","","IEEE","IEEE Conferences"
"A Novel Mask-Coding Representation for Set Cover Problems with Applications in Test Suite Minimisation","S. Yoo","NA","2nd International Symposium on Search Based Software Engineering","","2010","","","19","28","Multi-Objective Set Cover problem forms the basis of many optimisation problems in software testing because the concept of code coverage is based on the set theory. This paper presents Mask-Coding, a novel representation of solutions for set cover optimisation problems that explores the problem space rather than the solution space. The new representation is empirically evaluated with set cover problems formulated from real code coverage data. The results show that Mask-Coding representation can improve both the convergence and diversity of the Pareto-efficient solution set of the multi-objective set cover optimisation.","","978-1-4244-8341","10.1109/SSBSE.2010.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635169","set-cover representation;search-based software engineering;test suite minimisation","Optimization;Space exploration;Search problems;Convergence;Greedy algorithms;Redundancy;Software testing","minimisation;program testing;set theory","mask-coding representation;test suite minimisation;multiobjective set cover problem;optimisation problems;software testing;code coverage;set theory;Pareto-efficient solution set","","2","24","","","","","","IEEE","IEEE Conferences"
"An empirical study of optimizations in YOGI","A. V. Nori; S. K. Rajamani","Microsoft Research India; Microsoft Research India","2010 ACM/IEEE 32nd International Conference on Software Engineering","","2010","1","","355","364","Though verification tools are finding industrial use, the utility of engineering optimizations that make them scalable and usable is not widely known. Despite the fact that several optimizations are part of folklore in the communities that develop these tools, no rigorous evaluation of these optimizations has been done before. We describe and evaluate several engineering optimizations implemented in the Yogi property checking tool, including techniques to pick an initial abstraction, heuristics to pick predicates for refinement, optimizations for interprocedural analysis, and optimizations for testing. We believe that our empirical evaluation gives the verification community useful information about which optimizations they could implement in their tools, and what gains they can realistically expect from these optimizations.","1558-1225;0270-5257","978-1-60558-719","10.1145/1806799.1806852","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062103","abstraction refinement;directed testing;software model checking;testing","Optimization;Testing;Runtime;Wavelength division multiplexing;Algorithm design and analysis;Communities;Aggregates","optimisation;program verification;software engineering","YOGI optimisation;verification tools;engineering optimizations;Yogi property checking tool;interprocedural analysis","","2","13","","","","","","IEEE","IEEE Conferences"
"Cross-layer analysis, testing and verification of automotive control software","M. Broy; S. Chakraborty; S. Ramesh; M. Satpathy; S. Resmerita; W. Pree","Dip Goswami, Tech. Univ. Munich, Munich, Germany; Dip Goswami, Tech. Univ. Munich, Munich, Germany; Gen. Motors R&amp;D, India Sci. Labs., India; Gen. Motors R&amp;D, India Sci. Labs., India; Univ. of Salzburg, Salzburg, Austria; Univ. of Salzburg, Salzburg, Austria","2011 Proceedings of the Ninth ACM International Conference on Embedded Software (EMSOFT)","","2011","","","263","272","Automotive architectures today consist of up to 100 electronic control units (ECUs) that communicate via one or more FlexRay and CAN buses. Multiple control applications - like cruise control, brake control, etc. - are specified as Simulink/Stateflow models, from which code is generated and mapped onto the different ECUs. In addition, scheduling policies and parameters, both for the ECUs and the buses, need to be specified. Code generation/optimization from the Simulink/Stateflow models, task partitioning and mapping decisions, as well as the parameters chosen for the schedulers - all of these impact the execution times and timing behaviour of the control tasks and control messages. These in turn affect control performance, such as stability and steady-/transient-state behaviour. This paper discusses different aspects of this multi-layered design flow and the associated research challenges. The emphasis is on model-based code generation, analysis, testing and verification of control software for automotive architectures, as well as on architecture or platform configuration to ensure that the required control performance requirements are satisfied.","","978-1-4503-0714-7978-1-4503-0714-7978-1-4503-0712","10.1145/2038642.2038683","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064534","Automotive Control Systems;Model-based code generation;Model-based testing;verification","Optimization;Software;Indium phosphide;Visualization","peripheral interfaces;program compilers;program testing;program verification;traffic control","automotive control software cross-layer analysis;automotive control software testing;automotive control software verification;automotive architecture;electronic control unit;FlexRay;CAN bus;Simulink model;Stateflow model;scheduling policy;code optimization;mapping decision;message control;transient-state behaviour;multilayered design flow;model-based code generation","","4","58","","","","","","IEEE","IEEE Conferences"
"Critical analysis of HPL performance under different process distribution patterns","A. Rajan; B. K. Joshi; A. Rawat","NA; Military College of Telecommunication Engineering Mhow; Computer Division Raja Ramanna Centre for Advanced Technology Indore","2012 CSI Sixth International Conference on Software Engineering (CONSEG)","","2012","","","1","4","High Performance Linpack (HPL) benchmark is a standard tool to measure the performance of High Performance Computing Clusters (HPCC). There are several parameters which can be optimized to maximize performance delivered by HPCC. HPL provides many such parameters for evaluation of performance. P &amp; Q are two such values which determine the distribution pattern of processes for executing HPL. In this paper HPL runs with different values of P and Q have been carried out for three different values of N while keeping the block size (NB) constant. Value of N represents the problem size of task being executed. Results obtained are included in tables and are also depicted in form of graph and are analytically discussed. These results also provide the basis for deeper analytical approaches for HPCC performance evaluations. This critical analysis also reveals that maximum performance is delivered if the task is evenly distributed over all the available cores and nodes in the HPCC.","","978-1-4673-2177-8978-1-4673-2174-7978-1-4673-2175-4978-1-4673-2176","10.1109/CONSEG.2012.6349499","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6349499","HPL;Process Distribution;Execution Estimation;Software Optimization","Computers;Niobium;Benchmark testing;Standards;Educational institutions;High performance computing;Performance evaluation","benchmark testing;software metrics;software performance evaluation;software tools","critical analysis;HPL performance;high performance linpack benchmark;process distribution patterns;performance measurement;high performance computing clusters;HPCC performance evaluations;block size constant;software tool","","1","8","","","","","","IEEE","IEEE Conferences"
"SMT-C: A Semantic Mutation Testing Tools for C","H. Dan; R. M. Hierons","NA; NA","2012 IEEE Fifth International Conference on Software Testing, Verification and Validation","","2012","","","654","663","Semantic Mutation Testing (SMT) is a technique that aims to capture errors caused by possible misunderstandings of the semantics of a description language. It is intended to target a class of errors which is different from those captured by traditional Mutation Testing (MT). This paper describes our experiences in the development of an SMT tool for the C programming language: SMT-C. In addition to implementing the essential requirements of SMT (generating semantic mutants and running SMT analysis) we also aimed to achieve the following goals: weak MT/SMT for C, good portability between different configurations, seamless integration into test routines of programming with C and an easy to use front-end.","2159-4848","978-0-7695-4670-4978-1-4577-1906","10.1109/ICST.2012.155","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200167","Semantic mutation testing;Mutation operator;Unit test;Weak mutation testing;Eclipse plugin","Testing;Semantics;Software;Computer languages;Grammar;Indexes;Optimization","C language;program testing","SMT-C;semantic mutation testing tools;description language;C programming language;semantic mutants;SMT analysis","","5","35","","","","","","IEEE","IEEE Conferences"
"Automated generation of test cases from output domain and critical regions of embedded systems using genetic algorithms","C. P. Vudatha; S. Nalliboena; S. K. Jammalamadaka; B. K. K. Duvvuri; L. S. S. Reddy","Dept. of Information Technology, KL University; Dept. of Information Technology, KL University; Dept. of Information Technology, KL University; Department of Freshmen Engineering, KL University; College of Engineering, Mylavaram","2011 2nd National Conference on Emerging Trends and Applications in Computer Science","","2011","","","1","6","A primary issue in black-box testing is how to generate adequate test cases from input domain of the system under test on the basis of user's requirement specification. However, for some types of systems including embedded systems, developing test cases from output domain is more suitable than developing from input domain, especially, when the output domain is smaller. Exhaustive testing of the embedded systems in the critical regions is important as the embedded systems must be basically fail safe systems. The Critical regions of the input space of the embedded systems can be pre-identified and supplied as seeds. In this paper, the authors presents an Automated Test Case Generator (ATCG) that uses Genetic algorithms (GAs) to automate the generation of test cases from output domain and the criticality regions of an embedded System. The approach is applied to a pilot project `Temperature monitoring and controlling of Nuclear Reactor System' (TMCNRS) which is an embedded system developed using modified Cleanroom Software Engineering methodology. The ATCG generates test cases which are useful to conduct pseudo-exhaustive testing to detect single, double and several multimode faults in the system. The generator considers most of the combinations of outputs, and finds the corresponding inputs while optimizing the number of test cases generated.","","978-1-4244-9581-8978-1-4244-9578-8978-1-4244-9580","10.1109/NCETACS.2011.5751411","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5751411","Automated Test case generation;Output domain testing;Genetic algorithms;embedded systems;Cleanroom software engineering;combinatorial testing;pseudo-exhaustive testing","Testing;Temperature sensors;Genetic algorithms;Embedded systems;Relays;Temperature control","computerised monitoring;control engineering computing;embedded systems;fission reactors;genetic algorithms;nuclear engineering computing;program testing;software engineering;temperature control","test case automated generation;embedded systems;genetic algorithms;black-box testing;output domain;critical regions;user requirement specification;input domain;temperature monitoring;temperature control;nuclear reactor system;modified cleanroom software engineering methodology","","1","8","","","","","","IEEE","IEEE Conferences"
"We're Finding Most of the Bugs, but What are We Missing?","E. J. Weyuker; R. M. Bell; T. J. Ostrand","NA; NA; NA","2010 Third International Conference on Software Testing, Verification and Validation","","2010","","","313","322","We compare two types of model that have been used to predict software fault-proneness in the next release of a software system. Classification models make a binary prediction that a software entity such as a file or module is likely to be either faulty or not faulty in the next release. Ranking models order the entities according to their predicted number of faults. They are generally used to establish a priority for more intensive testing of the entities that occur early in the ranking. We investigate ways of assessing both classification models and ranking models, and the extent to which metrics appropriate for one type of model are also appropriate for the other. Previous work has shown that ranking models are capable of identifying relatively small sets of files that contain 75-95% of the faults detected in the next release of large legacy systems. In our studies of the rankings produced by these models, the faults not contained in the predicted most fault prone files are nearly always distributed across many of the remaining files; i.e., a single file that is in the lower portion of the ranking virtually never contains a large number of faults.","2159-4848","978-1-4244-6436-4978-1-4244-6435-7978-0-7695-3990","10.1109/ICST.2010.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477073","fault prediction;classification model;ranking model;fault-percentile-average","Computer bugs;Predictive models;Fault diagnosis;Software testing;Software systems;Fault detection;Diseases;Computer industry;History;Production systems","program debugging;software fault tolerance","software fault-proneness;classification models;software system;software entity;ranking models;legacy systems;bugs","","16","17","","","","","","IEEE","IEEE Conferences"
"Managing Testing Complexity in Dynamically Adaptive Systems: A Model-Driven Approach","K. Welsh; P. Sawyer","NA; NA","2010 Third International Conference on Software Testing, Verification, and Validation Workshops","","2010","","","290","298","Autonomous systems are increasingly conceived as a means to allow operation in changeable or poorly understood environments. However, granting a system autonomy over its operation removes the ability of the developer to be completely sure of the system's behaviour under all operating contexts. This combination of environmental and behavioural uncertainty makes the achievement of assurance through testing very problematic. This paper focuses on a class of system, called an m-DAS, that uses run-time models to drive run-time adaptations in changing environmental conditions. We propose a testing approach which is itself model-driven, using model analysis to significantly reduce the set of test cases needed to test for emergent behaviour. Limited testing resources may therefore be prioritised for the most likely scenarios in which emergent behaviour may be observed.","","978-1-4244-6774-7978-1-4244-6773-0978-0-7695-4050","10.1109/ICSTW.2010.57","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463659","component;model-driven;model-directed;testing;autonomous;dynamically adaptive systems","System testing;Adaptive systems;Runtime environment;Software testing;Environmental management;Switches;Middleware;Logic;Conference management;Uncertainty","program testing;software engineering","complexity testing;dynamically adaptive systems;model-driven approach;autonomous systems;environmental uncertainty;behavioural uncertainty;m-DAS system;run-time models","","2","25","","","","","","IEEE","IEEE Conferences"
"Hardware and Software Stack for an SDR-Based RFID Test Platform","M. Dusek; V. Derbek; A. Povalac; J. Sebesta; R. Marsalek","NA; NA; NA; NA; NA","2012 Fourth International EURASIP Workshop on RFID Technology","","2012","","","41","45","This paper describes a software defined radio system for measurements of minimum activation power and backscatter power of UHF RFID tags. We propose a platform for performance testing of tags, which is based on a general purpose universal software radio peripheral (USRP) and optimized for operation in the frequency band from 700 MHz to 1100 MHz at the output power of 2 W ERP. The presented HW and SW stack extends the standard library of USRP with RFID specific functions, an interface to LabVIEW, and by a top level LabVIEW code for automated testing.","","978-0-7695-4813-5978-1-4673-2602","10.1109/RFID.2012.15","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6353846","RFID;WBX;USRP;LabVIEW","Radiofrequency identification;Power measurement;Software;Antennas;Hardware;Power generation;Accuracy","radiofrequency identification;software radio;telecommunication computing;virtual instrumentation","SDR-based rfid test platform;software stack;software defined radio system;minimum activation power measurement;backscatter power measurements;UHF RFID tags;general purpose universal software radio peripheral;USRP;LabVIEW code;top level LabVIEW code;automated testing;hardware stack;frequency 700 MHz to 1100 MHz;power 2 W","","1","10","","","","","","IEEE","IEEE Conferences"
"Dichotomous search in ABC and its application in parameter estimation of software reliability growth models","T. K. Sharma; M. Pant; A. Abraham","Department of Paper Technology, Indian Institute of Technology, Roorkee, India; Department of Paper Technology, Indian Institute of Technology, Roorkee, India; Machine Intelligence Research Labs (MIR Labs), WA, USA","2011 Third World Congress on Nature and Biologically Inspired Computing","","2011","","","207","212","ABC (Artificial Bee Colony) is one of the most recent nature inspired algorithm (NIA) based on swarming metaphor. Proposed by Karaboga in 2005, ABC has proven to be a robust and efficient algorithm for solving global optimization problems over continuous space. In this paper, we propose a modified version of the ABC to improve its performance, in terms of converging to individual optimal point and to compensate the limited amount of search moves of original ABC. In modified version called Dichotomous ABC (DABC), the idea is to move dichotomously in both directions to generate a new trial point. The performance of the proposed algorithm is analyzed on five standard benchmark problems and also we explored the applicability of the proposed algorithm to estimate the parameters of software reliability growth models (SRGM). The proposed algorithm presents significant advantages in handling variety of modeling problems such as the exponential model, power model and Delayed S Shaped model.","","978-1-4577-1124-4978-1-4577-1122-0978-1-4577-1123","10.1109/NaBIC.2011.6089460","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6089460","Artificial Bee Colony;Bidirectional optimization;Software Engineering;Software Reliability","Mathematical model;Software algorithms;Computational modeling;Optimization;Software reliability;Algorithm design and analysis;Benchmark testing","optimisation;parameter estimation;search problems;software reliability","dichotomous search;artificial bee colony;parameter estimation;software reliability growth models;nature inspired algorithm;swarming metaphor;global optimization problems;continuous space;exponential model;power model;delayed S shaped model","","3","35","","","","","","IEEE","IEEE Conferences"
"TripleT: Improving Test Responsiveness for High Performance Embedded Systems","R. Mitsching; C. Weise; F. Fiedler; S. Kowalewski; H. Bohnenkamp","NA; NA; NA; NA; NA","2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops","","2011","","","67","74","Timed testing, i.e. a method of testing where timing plays a crucial role in the test verdict of the test cases, is an important quality assurance strategy in the development of embedded systems. Tool support is essential for timed testing, as timed test cases cannot be executed manually with the required precision. In case studies, we found that the existing timed testing tools do not fulfill all requirements needed for real-world-timed testing: they lack both features and responsiveness. Therefore, we have started the implementation of a timed testing tool called TripleT. TripleT is based on the notion of timed ioco as conformance relation, and uses as basis algorithms already present in TorX. However, we have improved in the responsiveness and execution time of our test cases - two major points when it comes to timeliness in timed testing. This has been achieved by using look-ahead and parallelizing the test case execution of TripleT. In the paper we will report on our approach and the results achieved.","","978-0-7695-4345-1978-1-4577-0019","10.1109/ICSTW.2011.86","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954392","Timed Testing;TripleT;Embedded Systems","Testing;Clocks;Automata;Timing;Optimization;Embedded systems","embedded systems;program testing;software quality","test responsiveness;high performance embedded system;quality assurance strategy;timed testing;TripleT tool;timed ioco notion","","1","17","","","","","","IEEE","IEEE Conferences"
"Preliminary Test and Evaluation of Nondestructive Examination for ITER First Wall Development in Korea","S. Kim; E. H. Lee; J. Yoon; H. Jung; D. W. Lee; B. Kim","Korea Atomic Energy Research Institute, Daejeon, Korea; Korea Atomic Energy Research Institute, Daejeon, Korea; Korea Atomic Energy Research Institute, Daejeon, Korea; Korea Atomic Energy Research Institute, Daejeon, Korea; Korea Atomic Energy Research Institute, Daejeon, Korea; National Fusion Research Institute (NFRI), Daejeon, Korea","IEEE Transactions on Plasma Science","","2012","40","9","2285","2289","ITER first wall (FW) includes a beryllium armour joined to a Cu heat sink with a stainless steel back plate. These FW panels are one of the critical components in the ITER tokamak with a maximum surface heat flux of 5 MW/m<sup>2</sup>. Therefore, a qualification test needs to be performed with the goal to qualify the joining technologies required for the ITER FW. Various mock-ups were fabricated to develop the manufacturing procedure of FW components. For the nondestructive examination of the fabricated mock-ups, an ultrasonic test (UT) was performed with optimized probes. The UT was performed by using a three-axis digital ultrasonic C-scan system and software. The system comprised an ultrasonic pulser and receiver, model Panametrics 5800PR; a personal computer having an internal analog/digital converter board and four-axis motion control board; and a three-axis scanning tank. Two types of transducers were used for this experiment. One was Panametrics V312-SU, having a center frequency of 10 MHz (nominal) and a piezoelectric element diameter of 0.25 in with a flat protective layer for the Be/Cu. The other was Panametrics V309-SU with a center frequency of 5 MHz and an element diameter of 0.5 in for the Cu/SS interface. Winspect software controlled all aspects of data acquisition, motion control, data archiving, and image display. Based on the acceptance criteria, the average amplitude of the interface signals, which have about 50% of the reference echo amplitude, was recorded and analyzed on each beryllium tile. An image-analysis software analyzed the statistics of amplitude distribution and calculated the unacceptable area. Each mock-up that passed these UTs was concluded to qualify the joining technologies required for an ITER FW by using a high-heat flux test facility. As a result of these qualification tests based on the acceptance criteria of an ITER FW, the fabrication technologies will be utilized to develop the FW of plasma-facing components.","0093-3813;1939-9375","","10.1109/TPS.2012.2206614","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6253269","ITER;mock-up;nondestructive examination;qualification test;semiprototype;ultrasonic test (UT)","Acoustics;Beryllium;Heating;Fabrication;Software;Cooling","analogue-digital conversion;armour;beryllium;copper;data acquisition;heat sinks;image processing;information retrieval systems;joining processes;motion control;piezoelectric transducers;plasma toroidal confinement;plasma-wall interactions;protective coatings;stainless steel;Tokamak devices;ultrasonic materials testing","nondestructive testing;ITER first wall;beryllium armour;heat sink;stainless steel back plate;FW panels;ITER tokamak;surface heat flux;joining technology;manufacturing procedure;ultrasonic test;three-axis digital ultrasonic C-scan system;ultrasonic pulser;ultrasonic receiver;Panametrics 5800PR;internal analog-digital converter;four-axis motion control board;three-axis scanning tank;transducers;piezoelectric element;Winspect software;data acquisition;data archiving;image display;acceptance criteria;interface signals;reference echo amplitude;image-analysis software;amplitude distribution;high-heat flux test facility;plasma-facing components;frequency 10 MHz","","3","8","","","","","","IEEE","IEEE Journals & Magazines"
"Adopting Six Sigma approach in predicting functional defects for system testing","M. D. M. Suffian; S. Ibrahim","Faculty of Computer Science and Information System, Universiti Teknologi Malaysia, Kuala Lumpur, Malaysia; Advanced Informatics School, Universiti Teknologi Malaysia, Kuala Lumpur, Malaysia","2011 Malaysian Conference in Software Engineering","","2011","","","240","244","This research focuses on constructing a mathematical model to predict functional defects in system testing by applying Six Sigma approach. The motivation behind this effort is to achieve zero known post release defects of the software delivered to end-user. Besides serving as the indicator of optimizing testing process, predicting functional defects at the start of testing allows testing team to put comprehensive test coverage, find as many defects as possible and determine when to stop testing so that all known defects are contained within testing phase. Design for Six Sigma (DfSS) is chosen as the methodology as it emphasizes on customers' requirement and systematic techniques to build the model. Historical data becomes the crucial elements in this study. Metrics related to potential predictors and their relationships for the model are identified, which focuses on metrics in phases prior to testing phase. Repeatability and capability of testers' consistency in finding defects are analyzed. Type of data required are also identified and collected. The metrics of selected predictors which incorporate testing and development metrics are measured against total functional defects using multiple regression analysis. The best and most significant mathematical model generated by the regression analysis is selected as the proposed prediction model for functional defects in system testing phase. Validation of the model is then conducted to prove the goodness for implementation. Recommendation and future research work are provided at the end of this study.","","978-1-4577-1531-0978-1-4577-1530-3978-1-4577-1529","10.1109/MySEC.2011.6140677","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140677","defect prediction model;six sigma;regression analysis;functional test defect prediction","Mathematical model;Software;Predictive models;Equations;System testing;Measurement","design for quality;program testing;regression analysis;six sigma (quality);software quality","six sigma approach;mathematical model;software defects;testing process;comprehensive test coverage;design for six sigma;DfSS;customers requirement;systematic techniques;testing metrics;development metrics;total functional defects;multiple regression analysis;system testing phase","","","8","","","","","","IEEE","IEEE Conferences"
"Development and Application of Optimizing Software of Tubing and Production Casing Sizes for Gas and Flowing Wells","L. Li; J. Xiong","NA; NA","2011 International Conference on Computational and Information Sciences","","2011","","","671","674","The optimizing software of tubing and production casing sizes for gas and flowing wells is developed aimed at helping the production engineers to diagnose, analyze and design of tubing and production casing sizes for gas and flowing wells. This paper presents the development background, structure, technical contents of the software and examples for application. By testing, the software has a strong practicality and accuracy. It can deal with various challenges in optimizing the tubing and production casing sizes and boost work efficiency.","","978-1-4577-1540-2978-0-7695-4501","10.1109/ICCIS.2011.131","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6086287","software development;C#;optimizing;tubing and production casing sizes;nodal analysis;application","Production;Software;Optimization;Reservoirs;Computer languages;Educational institutions;Particle separators","","","","","11","","","","","","IEEE","IEEE Conferences"
"Search-based Prediction of Fault-slip-through in Large Software Projects","W. Afzal; R. Torkar; R. Feldt; G. Wikstrand","NA; NA; NA; NA","2nd International Symposium on Search Based Software Engineering","","2010","","","79","88","A large percentage of the cost of rework can be avoided by finding more faults earlier in a software testing process. Therefore, determination of which software testing phases to focus improvements work on, has considerable industrial interest. This paper evaluates the use of five different techniques, namely particle swarm optimization based artificial neural networks (PSO-ANN), artificial immune recognition systems (AIRS), gene expression programming (GEP), genetic programming (GP) and multiple regression (MR), for predicting the number of faults slipping through unit, function, integration and system testing phases. The objective is to quantify improvement potential in different testing phases by striving towards finding the right faults in the right phase. We have conducted an empirical study of two large projects from a telecommunication company developing mobile platforms and wireless semiconductors. The results are compared using simple residuals, goodness of fit and absolute relative error measures. They indicate that the four search-based techniques (PSO-ANN, AIRS, GEP, GP) perform better than multiple regression for predicting the fault-slip-through for each of the four testing phases. At the unit and function testing phases, AIRS and PSO-ANN performed better while GP performed better at integration and system testing phases. The study concludes that a variety of search-based techniques are applicable for predicting the improvement potential in different testing phases with GP showing more consistent performance across two of the four test phases.","","978-1-4244-8341","10.1109/SSBSE.2010.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635180","","Artificial neural networks;Companies;Programming;Software testing;Software quality;Predictive models","artificial immune systems;fault tolerant computing;genetic algorithms;neural nets;particle swarm optimisation;program testing;regression analysis","search-based prediction;fault-slip-through;software project;software testing process;particle swarm optimization;artificial neural network;PSO-ANN;artificial immune recognition system;AIRS;gene expression programming;GEP;genetic programming;GP;multiple regression;MR","","10","35","","","","","","IEEE","IEEE Conferences"
"Search-Based Stress Testing of Wireless Network Protocol Stacks","M. Woehrle","NA","2012 IEEE Fifth International Conference on Software Testing, Verification and Validation","","2012","","","794","803","The operation of wireless network protocol stacks is heavily dependent on the actual deployment of the system and especially on the corresponding network topology, e.g. due to channel contention. The nature of wireless communication does not allow for a-priori determination of network topology, network-defining metrics such as neighbor density and routing span may drastically differ for various deployments. Therefore, it is a difficult problem to foresee and consider the large number of possible topologies that a system may run on during protocol stack development. We propose to use an automated approach for searching topologies for which a protocol stack exhibits particularly poor quantitative performance. We formulate stress testing of protocol stacks on specific topologies as a multi-objective optimization problem and use an evolutionary algorithm for finding a set of small topologies that particularly stress the protocol stack of a wireless network. For searching the topology space, we present novel problem-specific variation operators and show their improvements on search performance in case studies. We showcase our results on stress testing using two protocol stacks for wireless sensor networks.","2159-4848","978-0-7695-4670-4978-1-4577-1906","10.1109/ICST.2012.178","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200190","Testing;Software Testing;Wireless Networks;Wireless Sensor Networks","Topology;Network topology;Protocols;Quality of service;Testing;Stress;Wireless sensor networks","evolutionary computation;protocols;search problems;telecommunication equipment testing;telecommunication network routing;telecommunication network topology;wireless sensor networks","search-based stress testing;wireless network protocol stack;network topology;channel contention;neighbor density metric;routing span metric;protocol stack development;multiobjective optimization problem;evolutionary algorithm;topology space;problem-specific variation operator;search performance;wireless sensor network","","3","40","","","","","","IEEE","IEEE Conferences"
"An ant colony metaheuristic approach for optimal reliability assessment of software systems incorporating redundancy","M. S. Rao; D. S. Roy; P. R. Parro; D. K. Mohanta","School of Computing, SASTRA University, Thanjavur, Tamilnadu, India; Department of Computer Science and Engineering, National Institute of Science and Technology, Berhampur, India; Department of Computer Science and Engineering, National Institute of Science and Technology, Berhampur, India; Department of Electrical and Electronics Engineering Birla Institute of Technology, Mesra, India","2012 World Congress on Information and Communication Technologies","","2012","","","577","582","With the all pervasive presence of computers to all aspects of life, software reliability assessment is assuming a position of utmost importance. Moreover many commercial and governmental software systems require high mission reliability requiring both hardware and the software to be very reliable. Software reliability is acknowledged to perk up with the amount of testing efforts invested, which in turn reduces the cost of software development and in turn system cost. The scale of redundancy employed affects reliability favorably, while increasing the cost of software design and development. This paper employs an ant colony meta-heuristic optimization method to solve the redundancy allocation problem (RAP) for software systems. Herein, an ant colony optimization algorithm for the software RAP (SRAP) is devised and tested on a computer relay software that is employed for fault handling in power system transmission lines and the results presented validates the efficacy of the approach.","","978-1-4673-4805-8978-1-4673-4806-5978-1-4673-4804","10.1109/WICT.2012.6409143","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6409143","","Decision support systems;Communications technology;Computers;Relays;Software","ant colony optimisation;power transmission faults;power transmission lines;program testing;redundancy;software reliability;ubiquitous computing","ant colony metaheuristic approach;optimal reliability assessment;governmental software systems;commercial software systems;hardware reliability;software reliability;software development;system cost;software testing;software design;redundancy allocation problem;software RAP;ant colony optimization algorithm;SRAP;computer relay software;fault handling;power system transmission lines","","","21","","","","","","IEEE","IEEE Conferences"
"A Formal Framework for Mutation Testing","F. Belli; M. Beyazit","NA; NA","2010 Fourth International Conference on Secure Software Integration and Reliability Improvement","","2010","","","121","130","Model-based approaches, especially based on directed graphs (DG), are becoming popular for mutation testing as they enable definition of simple, nevertheless powerful, mutation operators and effective coverage criteria. However, these models easily become intractable if the system under consideration is too complex or large. Moreover, existing DG-based algorithms for test generation and optimization are rare and rather in an initial stage. Finally, DG models fail to represent languages beyond type-3 (regular). This paper proposes a grammar-based mutation testing framework, together with effective mutation operators, coverage concepts and algorithms for test sequence generation. The objective is to establish a formal framework for model-based mutation testing which enables complementary or alternative use of regular grammars, depending on the preferences of the test engineer. A case study validates the approach and analyzes its characteristic issues.","","978-1-4244-7434-9978-1-4244-7435-6978-0-7695-4086","10.1109/SSIRI.2010.23","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5502849","(model-based) mutation testing;test coverage;test generation;directed graph;formal/regular grammar;manipulation operator","Genetic mutations;Software testing;Power system modeling;Roentgenium;System testing;Computer science;Mathematics;Mathematical model;Automata;Robustness","context-free grammars;directed graphs;program testing","formal framework;mutation testing;directed graphs;DG-based algorithms;grammar-based mutation testing framework;mutation operators;model-based mutation testing;regular grammars","","4","40","","","","","","IEEE","IEEE Conferences"
"A preliminary investigation towards test suite optimization approach for enhanced State-Sensitivity Partitioning","M. A. Sapaat; S. Baharom","Faculty Of Computer Science and Information Technology, University Putra Malaysia, Malaysia; Department of Information System, Faculty Of Computer Science and Information Technology, University Putra Malaysia, Malaysia","2011 2nd International Conference on Instrumentation, Communications, Information Technology, and Biomedical Engineering","","2011","","","40","45","Testing is crucial in software development. Continuous researches being done to discover effective approaches in testing that capable to detect faults despite of reducing cost. Previous work in State-Sensitivity Partitioning (SSP) technique, which based on all-transition coverage criterion, has been introduced to avoid exhaustively testing the entire data states of a module by partitioning it based on state's sensitivity towards events, conditions and action. The test data for that particular module testing is in form of event sequences (or test sequence) and sets of test sequences in test cases will perform SSP test suite. The problem occurs in SSP test suite is data state redundancy that leads towards suite growth. This paper aims to discuss an initial step of our ongoing research in enhancing prior SSP test suite. Our work will try to find out the best way in removing redundant data state in order to minimize the suite size but yet capable to detect faults introduced by five selective mutation operators effectively as the original suite.","","978-1-4577-1166-4978-1-4577-1167-1978-1-4577-1165","10.1109/ICICI-BME.2011.6108592","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6108592","data state redundancy;test sequences;State Sensitivity Partitioning (SSP);mutation operators;testing","Testing;Optimization;Redundancy;Fault detection;Information technology;Instruments;Biomedical engineering","program testing;software engineering","state-sensitivity partitioning;software development;event sequences;SSP test suite","","","22","","","","","","IEEE","IEEE Conferences"
"An Empirical Study on the Relation between Dependency Neighborhoods and Failures","T. Zimmerman; N. Nagappan; K. Herzig; R. Premraj; L. Williams","NA; NA; NA; NA; NA","2011 Fourth IEEE International Conference on Software Testing, Verification and Validation","","2011","","","347","356","Changing source code in large software systems is complex and requires a good understanding of dependencies between software components. Modification to components with little regard to dependencies may have an adverse impact on the quality of the latter, i.e., increase their risk to fail. We conduct an empirical study to understand the relationship between the quality of components and the characteristics of their dependencies such as their frequency of change, their complexity, number of past failures and the like. Our study has been conducted on two large software systems: Microsoft VISTA and ECLIPSE. Our results show that components that have outgoing dependencies to components with higher object-oriented complexity tend to have fewer field failures for VISTA, but the opposite relation holds for ECLIPSE. Likewise, other notable observations have been made through our study that (a) confirm that certain characteristics of components increase the risk of their dependencies to fail and (b) some of the characteristics are project specific while some were also found to be common. We expect that such results can be leveraged for use to provide new directions for research in defect prediction, test prioritization and related research fields that utilize code dependencies in their empirical analysis. Additionally, these results provide insights to engineers on the potential reliability impacts of new component dependencies based upon the characteristics of the component.","2159-4848","978-1-61284-174-8978-0-7695-4342","10.1109/ICST.2011.39","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770624","software quality;defects;dependency;empirical software engineering","Measurement;Complexity theory;Software systems;Servers;Couplings;Java","object-oriented programming;program testing;software quality;systems analysis","dependency neighborhood;source code changing;software component;component quality;Microsoft VISTA;ECLIPSE;object-oriented complexity;field failure;defect prediction;test prioritization;code dependency;empirical analysis","","11","26","","","","","","IEEE","IEEE Conferences"
"Selecting an appropriate framework for value-based requirements prioritization","N. Kukreja; B. Boehm; S. S. Payyavula; S. Padmanabhuni","Center for Systems and Software Engineering (CSSE), University of Southern California, Los Angeles, USA; Center for Systems and Software Engineering (CSSE), University of Southern California, Los Angeles, USA; Infosys Labs, Infosys Technologies, Bangalore, India; Infosys Labs, Infosys Technologies, Bangalore, India","2012 20th IEEE International Requirements Engineering Conference (RE)","","2012","","","303","308","There are usually more requirements than feasible in a given schedule. Thus, it's imperative to be able to choose the most valuable ones for implementation to ensure the delivery of a high value software system. There are myriad requirements prioritization frameworks and selecting the most appropriate one is a decision problem in its own right. In this paper we present our approach in selecting the most appropriate value based requirements prioritization framework as per the requirements of our stakeholders. Based on our analysis a single framework was selected, validated by requirements engineers and project managers and deployed for company-wide use by a major IT player in India.","2332-6441;1090-705X;1090-750X","978-1-4673-2785-5978-1-4673-2783-1978-1-4673-2784","10.1109/RE.2012.6345819","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6345819","requirements prioritization;decision making;framework;value-based software engineering","Testing;Planning;Companies;Surface acoustic waves;Vectors;Software engineering","decision making;formal verification;project management;value engineering","value-based requirement prioritization;high value software system;requirement engineers;project managers;IT player;India;decision problem","","6","39","","","","","","IEEE","IEEE Conferences"
"Optimizing of operation parameters in EDI process by orthogonal test method","Jian Zhao; Jianyou Wang","College of Environmental Science and Engineering, Nankai University, Tianjin, 300071, China; College of Environmental Science and Engineering, Nankai University, Tianjin, 300071, China","2011 Second International Conference on Mechanic Automation and Control Engineering","","2011","","","2350","2353","In recent years, electrode ionization technology has been widely used for ultrapure water production. The effects of major operational parameters such as feed hardness, dilute flux and stack voltage on dilute product resistivity and scaling extend were investigated through an orthogonal experiment(L<sub>9</sub>(3)<sup>4</sup>). The factorial results of Design-Expert software revealed that the interaction of dilute flux and stack voltage influenced the dilute product resistivity most significantly, while the single factor of stack voltage was the main influencing factor on the scaling index (SI), which was proposed to quantify the scaling extend. It was found that dilute product resistivity maximization (16.38 MΩ·cm) and scaling index minimization can be achieved when feed hardness, dilute flux and stack voltage were 3.9 mg/L (as CaCO<sub>3</sub>), 21 L/h and 14V, respectively, which can be used for designing and operating a large scale process.","","978-1-4244-9439-2978-1-4244-9436-1978-1-4244-9438","10.1109/MACE.2011.5987452","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5987452","electrodeionization;operation parameters;orthogonal test;Design-Expert software","Conductivity;Feeds;Mathematical model;Indexes;Ions;Software;Silicon","ion exchange;minimisation;purification;water treatment","operation parameters;EDI process;orthogonal test method;electrode ionization technology;ultrapure water production;Design-Expert software;dilute flux;stack voltage;dilute product resistivity maximization;scaling index minimization","","","4","","","","","","IEEE","IEEE Conferences"
"Performance prediction of component based software systems using interface automata","J. Karimpour; A. Isazadeh; H. Izadkhah","Department of Computer Science, Faculty of mathematical sciences, University of Tabriz, Iran; Department of Computer Science, Faculty of mathematical sciences, University of Tabriz, Iran; Department of Computer Science, Faculty of mathematical sciences, University of Tabriz, Iran","2011 CSI International Symposium on Computer Science and Software Engineering (CSSE)","","2011","","","69","76","Most techniques used to assess the qualitative characteristics of software are done in testing phase of software development. Assessment of performance in the early software development process is particularly important for risk management. In this paper, we present a method based on interface automata and use queuing theory to predict software components-based performance. The main difference between our proposed method and other methods is that we have used a lightweight formal language named interface automata to describe the behavior of software components; and use the optimistic composition of different software components on the sequence diagram. We develop interface automata so that the performance data related to interaction of software components can be considered in order to predict software system performance by using queuing theory.","","978-1-61284-207-3978-1-61284-206-6978-1-61284-205","10.1109/CSICSSE.2011.5963994","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5963994","Interface Automata;Software architecture;queuing theory","Automata;Unified modeling language;Computer architecture;Software systems;Queueing analysis;Software architecture","automata theory;queueing theory;software architecture;software performance evaluation","performance prediction;software systems;interface automata;software development;risk management;software components based performance;optimistic composition;queuing theory","","","14","","","","","","IEEE","IEEE Conferences"
"PSTG: A T-Way Strategy Adopting Particle Swarm Optimization","B. S. Ahmed; K. Z. Zamli","NA; NA","2010 Fourth Asia International Conference on Mathematical/Analytical Modelling and Computer Simulation","","2010","","","1","5","As an activity to ensure quality and conformance, testing is one of the most important activities in any software or hardware product development cycle. Often, the challenge in testing is that the system may support a wide range of configurations. Ideally, it is desirable to test all of these configurations exhaustively. However, exhaustive testing is practically impossible due to time and resource limitations. To address this issue, there is a need for a sampling strategy that can select a subset of inputs as test data from an inherently large search space. Recent findings demonstrate that t-way interaction testing strategies based on artificial intelligence (i.e. where t indicates interaction strength) have been successful to obtain a near optimal solution resulting into smaller test set to be considered. Motivated by such findings, we have developed a new test generation strategy, called Particle Swarm Test Generator (PSTG). In this paper, we discuss the design of PSTG and demonstrate our preliminary test size reduction results against other competing t-way strategies including IPOG, WHITCH, Jenny, TConfig, and TVG.","2376-1164;2376-1172","978-1-4244-7197-3978-1-4244-7196-6978-0-7695-4062","10.1109/AMS.2010.14","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5489308","Particle Swarm Optimization;t-way testing","Particle swarm optimization;Artificial intelligence;Software engineering;Hardware;System testing;Sampling methods;Convergence;Asia;Analytical models;Mathematical model","artificial intelligence;particle swarm optimisation;program testing","particle swarm optimization;t-way interaction testing strategy;artificial intelligence;test generation strategy;particle swarm test generator;PSTG","","13","20","","","","","","IEEE","IEEE Conferences"
"Assessing the precision of FindBugs by mining Java projects developed at a university","A. Vetro'; M. Torchiano; M. Morisio","Politecnico di Torino, Italy; Politecnico di Torino, Italy; Politecnico di Torino, Italy","2010 7th IEEE Working Conference on Mining Software Repositories (MSR 2010)","","2010","","","110","113","Software repositories are analyzed to extract useful information on software characteristics. One of them is external quality. A technique used to increase software quality is automatic static analysis, by means of bug finding tools. These tools promise to speed up the verification of source code; anyway, there are still many problems, especially the high number of false positives, that hinder their large adoption in software development industry. We studied the capability of a popular bug-finding tool, FindBugs, for defect prediction purposes, analyzing the issues revealed on a repository of university Java projects. Particularly, we focused on the percentage of them that indicates actual defects with respect to their category and priority, and we ranked them. We found that a very limited set of issues have high precision and therefore have a positive impact on code external quality.","2160-1852;2160-1860","978-1-4244-6803-4978-1-4244-6802","10.1109/MSR.2010.5463283","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463283","Software Quality;Automatic Static Code Analysis;Defect prediction;Bug Finding Tools","Java;Testing;Software quality;Information analysis;Software maintenance;Computer bugs;Data mining;Programming;Computer industry;Software tools","data mining;Java;program debugging;program diagnostics;program verification;software quality","FindBugs precision assessment;Java project;software repository;software characteristics;software quality;automatic static analysis;bug finding tools;source code verification;software development industry;defect prediction;code external quality;data mining","","5","16","","","","","","IEEE","IEEE Conferences"
"Iterated local search vs. hyper-heuristics: Towards general-purpose search algorithms","E. Burke; T. Curtois; M. Hyde; G. Kendall; G. Ochoa; S. Petrovic; J. A. Vázquez-Rodríguez; M. Gendreau","Automated Scheduling, optimisAtion and Planning (ASAP) Research Group, School of Computer Science, University of Nottingham, UK; Automated Scheduling, optimisAtion and Planning (ASAP) Research Group, School of Computer Science, University of Nottingham, UK; Automated Scheduling, optimisAtion and Planning (ASAP) Research Group, School of Computer Science, University of Nottingham, UK; Automated Scheduling, optimisAtion and Planning (ASAP) Research Group, School of Computer Science, University of Nottingham, UK; Automated Scheduling, optimisAtion and Planning (ASAP) Research Group, School of Computer Science, University of Nottingham, UK; Automated Scheduling, optimisAtion and Planning (ASAP) Research Group, School of Computer Science, University of Nottingham, UK; Automated Scheduling, optimisAtion and Planning (ASAP) Research Group, School of Computer Science, University of Nottingham, UK; Interuniversity Research Centre on Enterprise Networks, Logistics and Transportation (CIRRELT) Montreal, Canada","IEEE Congress on Evolutionary Computation","","2010","","","1","8","An important challenge within hyper-heuristic research is to design search methodologies that work well, not only across different instances of the same problem, but also across different problem domains. This article conducts an empirical study involving three different domains in combinatorial optimisation: bin packing, permutation flow shop and personnel scheduling. Using a common software interface (HyFlex), the same algorithms (high-level strategies or hyper-heuristics) can be readily run on all of them. The study is intended as a proof of concept of the proposed interface and domain modules, as a benchmark for testing the generalisation abilities of heuristic search algorithms. Several algorithms and variants from the literature were implemented and tested. From them, the implementation of iterated local search produced the best overall performance. Interestingly, this is one of the most conceptually simple competing algorithms, its advantage as a robust algorithm is probably due to two factors: (i) the simple yet powerful exploration/exploitation balance achieved by systematically combining a perturbation followed by local search; and (ii) its parameter-less nature. We believe that the challenge is still open for the design of robust algorithms that can learn and adapt to the available low-level heuristics, and thus select and apply them accordingly.","1089-778X;1941-0026","978-1-4244-6911-6978-1-4244-6909-3978-1-4244-6910","10.1109/CEC.2010.5586064","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5586064","","Personnel;Heuristic algorithms;Job shop scheduling;Algorithm design and analysis;Software algorithms;Software;Optimization","bin packing;combinatorial mathematics;flow shop scheduling;heuristic programming;optimisation;personnel;search problems","general-purpose search algorithm;hyper-heuristic research;combinatorial optimisation;bin packing;permutation flow shop;personnel scheduling;HyFlex software interface;interface module;domain module;heuristic search algorithm;iterated local search;exploration-exploitation balance;low-level heuristics","","13","42","","","","","","IEEE","IEEE Conferences"
"The way to an open-source software for automated optimization and learning — OpenOpal","R. Dornberger; T. Hanne; L. Frey","University of Applied Sciences Northwestern Switzerland, School of Business, Institute for Information Systems, Peter Merian-Str. 86, CH-4002 Basel, Switzerland; University of Applied Sciences Northwestern Switzerland, School of Business, Institute for Information Systems, Riggenbachstr. 16, CH-4600 Olten, Switzerland; University of Applied Sciences Northwestern Switzerland, School of Business, Institute for Information Systems, Riggenbachstr. 16, CH-4600 Olten, Switzerland","IEEE Congress on Evolutionary Computation","","2010","","","1","8","An optimization framework combines various methods, strategies, and programming interfaces on a robust software platform. Its development requires knowledge from application areas, and about optimization methods, as well as from software engineering. Different persons provide diverse know-how about modeling and simulating engineering and/or business problems, about search and optimization methods, and about new software trends to implement them into software. This paper describes the approach how an optimization framework based on evolutionary algorithms and other methods is developed in subsequent projects with application engineers and software developers cooperatively working together guaranteeing a sophisticated knowledge transfer. Therefore, particular knowledge management aspects are emphasized. As result, the optimization platform OpenOpal and the ideas behind its software architecture, supporting the know-how transfer, are presented. In order to continuously improve this optimization framework it is transferred into an open-source software initiative. The objective is to broaden the user group by increasing the number of knowledge contributors both from academia - integrating and testing newly developed optimization methods - and from various engineering areas - providing real-world problems to be solved.","1089-778X;1941-0026","978-1-4244-6911-6978-1-4244-6909-3978-1-4244-6910","10.1109/CEC.2010.5586123","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5586123","","Knowledge engineering;Optimization methods;Software tools;Companies","evolutionary computation;learning (artificial intelligence);mathematics computing;public domain software;software architecture","open-source software;OpenOpal software;optimization framework;software engineering;learning;evolutionary algorithms;software architecture;knowledge transfer","","7","21","","","","","","IEEE","IEEE Conferences"
"Automated GUI Test Case Generation","L. Lu; Y. Huang","NA; NA","2012 International Conference on Computer Science and Service System","","2012","","","582","585","GUI (Graphical User Interface) testing is complex and inefficient in nowadays software testing. This paper proposed a framework for automated GUI test case generation. The framework includes a reverse engineering of executive GUI to create GUI information and event-flow model. According to these information and model, an ant colony algorithm is applied to generate test cases. Our study shows that by identifying valid event interactions, the generated test cases are capable of finding deep faults without infeasible problems, the configuration of different parameters for ant colony and the good design of our framework provide flexible interfaces to control test suites.","","978-0-7695-4719-0978-1-4673-0721","10.1109/CSSS.2012.151","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6394389","test case generation;ant colony optimization;event-flow;GUI test","Graphical user interfaces;Testing;Software;Automation;Context;Computer science;Educational institutions","ant colony optimisation;graphical user interfaces;program testing;reverse engineering","automated GUI test case generation;graphical user interface;software testing;reverse engineering;executive GUI;GUI information;event-flow model;ant colony algorithm;valid event interaction identification;deep fault finding","","1","11","","","","","","IEEE","IEEE Conferences"
"Which Crashes Should I Fix First?: Predicting Top Crashes at an Early Stage to Prioritize Debugging Efforts","D. Kim; X. Wang; S. Kim; A. Zeller; S. C. Cheung; S. Park","Sogang University, Seoul; The Hong Kong University of Science and Technology, Hong Kong; The Hong Kong University of Science and Technology, Hong Kong; Saarland University, Saarbrücken; The Hong Kong University of Science and Technology, Hong Kong; Sogang University, Seoul","IEEE Transactions on Software Engineering","","2011","37","3","430","447","Many popular software systems automatically report failures back to the vendors, allowing developers to focus on the most pressing problems. However, it takes a certain period of time to assess which failures occur most frequently. In an empirical investigation of the Firefox and Thunderbird crash report databases, we found that only 10 to 20 crashes account for the large majority of crash reports; predicting these “top crashes” thus could dramatically increase software quality. By training a machine learner on the features of top crashes of past releases, we can effectively predict the top crashes well before a new release. This allows for quick resolution of the most important crashes, leading to improved user experience and better allocation of maintenance efforts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.20","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5711013","Top crash;machine learning;crash reports;social network analysis;data mining.","Fires;Feature extraction;Software;Testing;Computer bugs;Training","program debugging;software maintenance;software quality;system recovery","debugging;software systems;software failures;Firefox crash report databases;Thunderbird crash report databases;software quality;software maintenance","","35","59","","","","","","IEEE","IEEE Journals & Magazines"
"Prioritizing Mutation Operators Based on Importance Sampling","M. Sridharan; A. S. Namin","NA; NA","2010 IEEE 21st International Symposium on Software Reliability Engineering","","2010","","","378","387","Mutation testing is a fault-based testing technique for measuring the adequacy of a test suite. Test suites are assigned scores based on their ability to expose synthetic faults (i.e., mutants) generated by a range of well-defined mathematical operators. The test suites can then be augmented to expose the mutants that remain undetected and are not semantically equivalent to the original code. However, the mutation score can be increased superfluously by mutants that are easy to expose. In addition, it is infeasible to examine all the mutants generated by a large set of mutation operators. Existing approaches have therefore focused on determining the sufficient set of mutation operators and the set of equivalent mutants. Instead, this paper proposes a novel Bayesian approach that prioritizes operators whose mutants are likely to remain unexposed by the existing test suites. Probabilistic sampling methods are adapted to iteratively examine a subset of the available mutants and direct focus towards the more informative operators. Experimental results show that the proposed approach identifies more than 90% of the important operators by examining ? 20% of the available mutants, and causes a 6% increase in the importance measure of the selected mutants.","1071-9458;1071-9458;2332-6549","978-1-4244-9056-1978-0-7695-4255","10.1109/ISSRE.2010.16","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635074","Mutation Testing;Testing Effectiveness;Importance Sampling;Bayesian Reasoning","Testing;Monte Carlo methods;Entropy;Probability distribution;Equations;Probabilistic logic;Stochastic processes","Bayes methods;importance sampling;program testing;software fault tolerance","mutation operators;importance sampling;mutation testing;fault-based testing technique;Bayesian approach;probabilistic sampling method","","11","22","","","","","","IEEE","IEEE Conferences"
"Prioritization of Issues and Requirements by Cumulative Voting: A Compositional Data Analysis Framework","P. Chatzipetrou; L. Angelis; P. Rovegard; C. Wohlin","NA; NA; NA; NA","2010 36th EUROMICRO Conference on Software Engineering and Advanced Applications","","2010","","","361","370","Cumulative Voting (CV), also known as Hundred-Point Method, is a simple and straightforward technique, used in various prioritization studies in software engineering. Multiple stakeholders (users, developers, consultants, marketing representatives or customers) are asked to prioritize issues concerning requirements, process improvements or change management in a ratio scale. The data obtained from such studies contain useful information regarding correlations of issues and trends of the respondents towards them. However, the multivariate and constrained nature of data requires particular statistical analysis. In this paper we propose a statistical framework; the multivariate Compositional Data Analysis (CoDA) for analyzing data obtained from CV prioritization studies. Certain methodologies for studying the correlation structure of variables are applied to a dataset concerning impact analysis issues prioritized by software professionals under different perspectives. These involve filling of zeros, transformation using the geometric mean, principle component analysis on the transformed variables and graphical representation by biplots and ternary plots.","2376-9505;1089-6503","978-1-4244-7901","10.1109/SEAA.2010.35","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5598119","Prioritization;Compositional Data Analysis;Cumulative Voting;Hundred-Point Method;Hundred-Dollar test","Correlation;Statistical analysis;Software;Data analysis;Context;Covariance matrix;Software engineering","data analysis;principal component analysis;software engineering","cumulative voting;multivariate compositional data analysis;hundred-point method;software engineering;statistical analysis;CoDA;correlation structure;geometric mean;principle component analysis;graphical representation","","10","36","","","","","","IEEE","IEEE Conferences"
"Usability Testing Methodology: Effectiveness of Heuristic Evaluation in E-Government Website Development","A. Sivaji; A. Abdullah; A. G. Downe","NA; NA; NA","2011 Fifth Asia Modelling Symposium","","2011","","","68","72","Software development organizations consist of marketing, project management, development, design, and quality assurance team. It is important for the various teams within the organization to understand the benefits and limitation of incorporating various usability testing methods within the software development life cycle. Some of the reasons for poor usability include effort prioritization conflicts from project management, development and design team. The role of the usability engineer is to get involved as the heuristic evaluator and facilitate the development and design efforts are based on usability principles and at the same time adhering to the project time line. Two of the common usability inspection methods consist of user experience testing and expert review or more commonly known as Heuristic Evaluation (HE). This paper focuses on understanding the effectiveness of HE as a methodology for defect detection. The results show the effectiveness of the HE as a usability testing methodology in capturing defects and prioritizing development and design efforts. The results also reinforce the need for integrating traditional heuristics with modified heuristics customized to the domain or field of the project being tested such as E-Government.","2376-1164;2376-1172","978-1-4577-0193","10.1109/AMS.2011.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5961243","Usability Testing Methodology;Heuristice Evaluation;Defect Detection;Iterative Testing","Usability;Helium;Testing;Electronic government;Navigation;Inspection","government data processing;program testing;software development management;Web sites","usability testing methodology;e-government Website development;software development life cycle;effort prioritization conflicts;user experience testing;expert review;heuristic evaluation;defect detection","","12","13","","","","","","IEEE","IEEE Conferences"
"Real world oriented test functions","Fu-ming Peng","College of Computer and Software, Nanjing Institute of Industry Technology, China","2011 International Conference on Computer Science and Service System (CSSS)","","2011","","","1499","1505","The global optimal solutions of presently widely-used test functions are known or controllable, which leaves room for algorithm falsification. Moreover, the most algorithmic results to optimize test functions from papers are one-way conversation without peers' recognition, whose authenticity depends entirely on the authors' personal integrity. In this paper, in order to change the situation, two groups of real world oriented test functions and the resultative optimal solutions are given. The chief characteristics of these functions are its global optimal solution unknown to all people forever, which blocks off the loopholes of algorithmic cheating from the source. The comparison of the quality of the two algorithmic models depends on which can find out a better optimal solution coordinate. Therefore, a real world oriented test function can detect the actual level of algorithm.","","978-1-4244-9763-8978-1-4244-9762-1978-1-4244-9761","10.1109/CSSS.2011.5975041","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5975041","the global optimal solution;real world;test function;the optimal solution coordinate","Software algorithms;Three dimensional displays;Software;Industries;Computers;Educational institutions","optimisation;principal component analysis;security of data","global optimal solution;real world oriented test functions;algorithmic cheating;authenticity","","","","","","","","","IEEE","IEEE Conferences"
"An optimization method for neural network based on GA and TS algorithm","Pengyi Gao; Chuanbo Chen; Sheng Qin; Yingsong Hu","School of Computer Science and Engineering, Huazhong University of Science and Technology, Wuhan 430074, China; School of Software Engineering, Huazhong University of Science and Technology, Wuhan 430074, China; School of Informatics, University of Edinburgh, EH8 9AB, UK; School of Computer Science and Engineering, Huazhong University of Science and Technology, Wuhan 430074, China","2010 The 2nd International Conference on Computer and Automation Engineering (ICCAE)","","2010","1","","159","162","Although many global optimization search algorithms may be used to train feedforward neural networks, these algorithms have some weaknesses such as dependence of initial solution. This paper proposes a novel hybrid global optimization method for classification problem, called GTA, which combines the advantages of Genetic algorithm and Tabu search. The training process in proposed method is divided into two phase. First, a promising initial solution is searched by GA algorithm, and next the best solution is selected by tabu search. In this work, the optimization method and test are discussed. Results obtained by testing Diabetes Data Set have shown that the approach performs better than other optimization algorithm.","","978-1-4244-5586-7978-1-4244-5569-0978-1-4244-5585","10.1109/ICCAE.2010.5451978","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5451978","Neural networks;tabu search;genetic algorithms;global optimization","Optimization methods;Neural networks;Backpropagation algorithms;Genetic algorithms;Software algorithms;Feedforward neural networks;Testing;Simulated annealing;Computer science;Software engineering","genetic algorithms;neural nets;pattern classification;search problems","genetic algorithm;tabu search;neural network;hybrid global optimization;classification problem;GTA","","1","13","","","","","","IEEE","IEEE Conferences"
"Finding the Optimal Balance between Over and Under Approximation of Models Inferred from Execution Logs","P. Tonella; A. Marchetto; C. D. Nguyen; Y. Jia; K. Lakhotia; M. Harman","NA; NA; NA; NA; NA; NA","2012 IEEE Fifth International Conference on Software Testing, Verification and Validation","","2012","","","21","30","Models inferred from execution traces (logs) may admit more behaviours than those possible in the real system (over-approximation) or may exclude behaviours that can indeed occur in the real system (under-approximation). Both problems negatively affect model based testing. In fact, over-approximation results in infeasible test cases, i.e., test cases that cannot be activated by any input data. Under-approximation results in missing test cases, i.e., system behaviours that are not represented in the model are also never tested. In this paper we balance over- and under-approximation of inferred models by resorting to multi-objective optimization achieved by means of two search-based algorithms: A multi-objective Genetic Algorithm (GA) and the NSGA-II. We report the results on two open-source web applications and compare the multi-objective optimization to the state-of-the-art KLFA tool. We show that it is possible to identify regions in the Pareto front that contain models which violate fewer application constraints and have a higher bug detection ratio. The Pareto fronts generated by the multi-objective GA contain a region where models violate on average 2% of an application's constraints, compared to 2.8% for NSGA-II and 28.3% for the KLFA models. Similarly, it is possible to identify a region on the Pareto front where the multi-objective GA inferred models have an average bug detection ratio of 110 : 3 and the NSGA-II inferred models have an average bug detection ratio of 101 : 6. This compares to a bug detection ratio of 310928 : 13 for the KLFA tool.","2159-4848","978-0-7695-4670-4978-1-4577-1906","10.1109/ICST.2012.82","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200093","Model inference;Model-based testing;Search-based software engineering","Computational modeling;Genetic algorithms;Inference algorithms;Optimization;Approximation methods;Measurement;Testing","formal specification;genetic algorithms;Internet;program testing;public domain software","optimal balance;model over approximation;model under approximation;models Inferred from Execution Logs;execution traces;over-approximation;under-approximation;model based testing;system behaviours;multiobjective optimization;search-based algorithms;multiobjective genetic algorithm;NSGA-II;open-source Web applications;KLFA tool;bug detection ratio;Pareto fronts;multiobjective GA inferred models","","3","23","","","","","","IEEE","IEEE Conferences"
"Diffusion of Software Features: An Exploratory Study","F. Thung; D. Lo; L. Jiang","NA; NA; NA","2012 19th Asia-Pacific Software Engineering Conference","","2012","1","","368","373","New features are frequently proposed in many software libraries. These features include new methods, classes, packages, etc. These features are utilized in many open source and commercial software systems. Some of these features are adopted very quickly, while others take a long time to be adopted. Each feature takes much resource to develop, test, and document. Library developers and managers need to decide what feature to prioritize and what to develop next. As a first step to aid these stakeholders, we perform an exploratory study on the diffusion or rate of adoption of features in Java Development Kit (JDK) library. Our empirical study proposes such questions as how many new features are adopted by client applications, how long it takes for a new feature to spread to various software products, what features are diffused quickly, and what features are diffused widely. We perform an exploratory study with new features in Java Development Kit (JDK, from version 1.3 to 1.6) and provide empirical findings to answer the above research questions.","1530-1362;1530-1362;1530-1362","978-1-4673-4930-7978-0-7695-4922","10.1109/APSEC.2012.139","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6462682","Diffusion;Exploratory Study;Empirical Software Engineering","Feature extraction;Java;Software;XML;Software libraries;Communities","Java;public domain software;software libraries","software feature diffusion;software libraries;open source software systems;commercial software systems;library developers;library managers;Java development kit library;JDK;software products","","","26","","","","","","IEEE","IEEE Conferences"
"On optimizing test cost for Wafer-to-Wafer 3D-stacked ICs","M. Taouil; S. Hamdioui","Computer Engineering Laboratory, Delft University of Technology, Faculty of EE, Mathematics and CS, Mekelweg 4, 2628 CD Delft, The Netherlands; Computer Engineering Laboratory, Delft University of Technology, Faculty of EE, Mathematics and CS, Mekelweg 4, 2628 CD Delft, The Netherlands","7th International Conference on Design & Technology of Integrated Systems in Nanoscale Era","","2012","","","1","6","The increasing demand for more sophisticated ICs with more functionality mostly was realized by downscaling and increasing the number of transistors. A technology that promises further increase of transistor density (in addition with heterogeneous integration, better performance and less power dissipation at a smaller footprint) is the three-dimensional stacked ICs (3D-SICs). Several stacking approaches are under development to manufacture such 3D-SICs. Wafer-to-Wafer (W2W) stacking seems the most favorable approach when high manufacturing throughput, thinned wafers and small die handling is required. However, efficient and optimal test approaches to satisfy the required quality are still subject to research. Each manufactured 3D-SIC undergoes a test and therefore optimizing test cost will have a large overall impact. This paper discusses test cost optimization for W2W 3D-SICs. It first introduces a framework covering different test flows for 3D W2W ICs. Test flows that include pre-bond tests can benefit from wafer matching; in wafer matching a software algorithm is used to increase the compound yield by stacking wafers with similar fault distributions. Subsequently, the paper proposes a cost model to evaluate and estimate the impact of test flows on the overall 3D-SIC cost. Our simulation results show that test flows with pre-bond testing in general significantly reduce the overall cost. These test flows benefit mostly from the yield increase due to wafer matching.","","978-1-4673-1928-7978-1-4673-1926-3978-1-4673-1927","10.1109/DTIS.2012.6232983","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6232983","W2W;3D W2W test flows;pre-bond testing;wafer matching","Stacking;Packaging;Manufacturing;Testing;Compounds;Integrated circuit modeling;Semiconductor device modeling","integrated circuit manufacture;integrated circuit testing;integrated circuit yield;stacking;three-dimensional integrated circuits;transistors","test cost optimization;wafer-to-wafer stacking;three-dimensional integrated circuits;3D IC;transistors;wafer matching;software algorithm;compound yield;pre-bond testing","","","21","","","","","","IEEE","IEEE Conferences"
"A Framework for the Automatic Correction of Constraint Programs","N. Lazaar; A. Gotlieb; Y. Lebbah","NA; NA; NA","2011 Fourth IEEE International Conference on Software Testing, Verification and Validation","","2011","","","319","326","Constraint programs, such as those written in high-level constraint modelling languages, e.g., OPL (Optimization Programming Language), COMET, ZINC or ESSENCE, are more and more used in business-critical programs. As any other critical programs, they require to be thoroughly tested and corrected to prevent catastrophic loss of money. This paper presents a framework for the automatic correction of constraint programs that takes into account the specificity of the software development process of these programs as well as their typical faults. We implemented this framework in our testing platform CPTEST for OPL programs. Using mutation testing, our experimental results show that well-known constraint programs written in OPL can be automatically corrected using our framework.","2159-4848","978-1-61284-174-8978-0-7695-4342","10.1109/ICST.2011.41","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770621","","Computational modeling;Programming;Software testing;Software;Optimization;Space exploration","constraint handling;program testing;software engineering","constraint program correction;high-level constraint modelling language;Optimization Programming Language;COMET;ZINC;ESSENCE;business-critical program;software development process;CPTEST;mutation testing","","3","19","","","","","","IEEE","IEEE Conferences"
"Fault localization with intersection of control-flow based execution traces","Ye Gang; Li Xianjun; Li Zhongwen; Yin Jie","State Key Laboratory of Software Development Environment, Beihang University, Beijing, China; State Key Laboratory of Software Development Environment, Beihang University, Beijing, China; State Key Laboratory of Software Development Environment, Beihang University, Beijing, China; State Key Laboratory of Software Development Environment, Beihang University, Beijing, China","2011 3rd International Conference on Computer Research and Development","","2011","1","","430","434","Because debugging is notoriously time-consuming and laborious, fault localization becomes a major activity in software testing researches. This paper proposes a novel fault localization approach which utilizes intersection of control-flow based execution traces. It reports all program points, which are executed in every failed test case, as suspect of bugs and ranks them based on the frequency they appear in passed test cases. We develop a prototype tool named JFAULT which locates faults by reporting sorted suspicious program points. Programmers could examine them one by one until the fault was found. We conduct detailed experiments to compare our approach with previously proposed technique. The experimental results show that our approach has the potential to be effective in localizing faults.","","978-1-61284-840-2978-1-61284-839-6978-1-61284-838","10.1109/ICCRD.2011.5764051","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5764051","fault localization;control-flow;intersection;execution traces","Software;Instruments;Java;Software engineering;Computer bugs;Computer architecture;Testing","fault location;program debugging;program testing;software fault tolerance;software prototyping","fault localization;control flow intersection;execution traces;debugging;software testing;prototype tool;JFAULT","","","18","","","","","","IEEE","IEEE Conferences"
"A service oriented optimization platform for TE process","X. Li","School of Mathematics and Computer Science, JiangHan University, Wuhan 430056, P.R. China","2010 2nd International Conference on Future Computer and Communication","","2010","1","","V1-263","V1-267","For implementing the online control and optimization of TE process, an integration optimization platform is proposed based on web service technology. Professional software such as G2, GAMS, Matlab, are wrapped as web services based on multi-agent technology. Based on the web services in the platform, it can control and optimize the online simulation procedure of TE process. Finally the practicability and validity of the optimization platform are verified through the application in the TE process simulation and optimization.","","978-1-4244-5824-0978-1-4244-5821-9978-1-4244-5823","10.1109/ICFCC.2010.5497790","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5497790","TE process;information integration;online optimization;web service","Tellurium;Web services;Mathematical model;Web and internet services;Service oriented architecture;Collaboration;Application software;Protocols;Publishing;Mathematics","multi-agent systems;optimisation;process control;production engineering computing;production testing;Web services","service oriented optimization platform;TE process;Web service technology;multi-agent technology;test engineering","","","13","","","","","","IEEE","IEEE Conferences"
"Automated oracle creation support, or: How I learned to stop worrying about fault propagation and love mutation testing","M. Staats; G. Gay; M. P. E. Heimdahl","Division of Web Science &amp; Technology, Korea Advanced Institute of Science &amp; Technology; Department of Computer Science and Engineering, University of Minnesota; Department of Computer Science and Engineering, University of Minnesota","2012 34th International Conference on Software Engineering (ICSE)","","2012","","","870","880","In testing, the test oracle is the artifact that determines whether an application under test executes correctly. The choice of test oracle can significantly impact the effectiveness of the testing process. However, despite the prevalence of tools that support the selection of test inputs, little work exists for supporting oracle creation. In this work, we propose a method of supporting test oracle creation. This method automatically selects the oracle data - the set of variables monitored during testing - for expected value test oracles. This approach is based on the use of mutation analysis to rank variables in terms of fault-finding effectiveness, thus automating the selection of the oracle data. Experiments over four industrial examples demonstrate that our method may be a cost-effective approach for producing small, effective oracle data, with fault finding improvements over current industrial best practice of up to 145.8% observed.","1558-1225;0270-5257;0270-5257","978-1-4673-1067-3978-1-4673-1066-6978-1-4673-1065","10.1109/ICSE.2012.6227132","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227132","testing;test oracles;oracle data;oracle selection;verification","Testing;Training;Monitoring;Aerospace electronics;Greedy algorithms;Software systems","fault diagnosis;program testing","automated oracle creation support;fault propagation;love mutation testing;testing process;oracle data selection;mutation analysis;rank variables;fault-finding effectiveness","","19","32","","","","","","IEEE","IEEE Conferences"
"The results of software complex OPTAN use for modeling and optimization of standard engineering processes of printed circuit boards manufacturing","O. Bazylyk; P. Taradaha; O. Nadobko; L. Chyrun; T. Shestakevych","Lviv Polytechnic National University, S. Bandery Str., 12, 79013, UKRAINE; NA; NA; NA; NA","Proceedings of International Conference on Modern Problem of Radio Engineering, Telecommunications and Computer Science","","2012","","","107","108","The results of software OPTAN testing on the examples of modeling and optimization of technological processes of printed circuit boards manufacturing using various techniques are presented. The software efficiency is confirmed, and ways of the technological processes research improvement are suggested.","","978-617-607-138-9978-1-4673-0283","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6192417","software complex OPTAN;technological processes modeling;modeling and optimization of technological processes of printed circuit boards manufacturing","","printed circuit manufacture;production engineering computing","standard engineering processes;printed circuit board manufacturing;OPTAN software testing;technological process;software efficiency","","","4","","","","","","IEEE","IEEE Conferences"
"Security Requirements Analysis, Specification, Prioritization and Policy Development in Cyber-Physical Systems","K. K. Fletcher; X. Liu","NA; NA","2011 Fifth International Conference on Secure Software Integration and Reliability Improvement - Companion","","2011","","","106","113","In recent past, the security of cyber-physical systems (CPSs) has been the subject of major concern. One of the reasons is that, CPSs are often applied to mission-critical processes. Also, the automation CPSs bring in managing physical processes, and the detail of information available to them for carrying out their tasks, make securing them a prime importance. Securing CPSs is a difficult task as systems are interconnected. In order to achieve a continuous secured CPS environment, there is the need for an integrated methodology to analyze, specify and prioritize security requirements and also to develop policies to meet them. First, CPS assets are represented using high-order object models. Second, swim lane diagrams are extended to include malactivities and prevention or mitigation options to decompose use cases. We analyze security threats pertaining to the hardware components, software components and the hardware-software interaction. Security requirements are then specified, and an analytical prioritization approach, based on relative priority analysis is employed to prioritize them. Finally, security policies are then developed to meet the requirements. To demonstrate its effectiveness and evaluate its application, the proposed methodology is applied in a structured approach to a test bed - Ayushman, a Pervasive Health Monitoring System (PHMS).","","978-1-4577-0781-0978-0-7695-4454","10.1109/SSIRI-C.2011.25","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004511","cyber-physical systems (CPS);CPS security requirements;high order object oriented modeling technique;CPS security requirements prioritization;hardware-software interaction","Security;Object oriented modeling;Analytical models;Unified modeling language;Monitoring;Software;Object recognition","formal specification;health care;medical computing;object-oriented programming;security of data;systems analysis;ubiquitous computing","security requirements analysis;security requirement specification;security requirement prioritization;policy development;cyber-physical system;mission-critical process;swim lane diagram;malactivity;prevention options;mitigation options;security threat analysis;hardware components;software components;hardware-software interaction;relative priority analysis;Ayushman;Pervasive Health Monitoring System","","5","15","","","","","","IEEE","IEEE Conferences"
"Software Module Clustering as a Multi-Objective Search Problem","K. Praditwong; M. Harman; X. Yao","The University of Birmingham, Birmingham; University College London, London; The University of Birmingham, Birmingham","IEEE Transactions on Software Engineering","","2011","37","2","264","282","Software module clustering is the problem of automatically organizing software units into modules to improve program structure. There has been a great deal of recent interest in search-based formulations of this problem in which module boundaries are identified by automated search, guided by a fitness function that captures the twin objectives of high cohesion and low coupling in a single-objective fitness function. This paper introduces two novel multi-objective formulations of the software module clustering problem, in which several different objectives (including cohesion and coupling) are represented separately. In order to evaluate the effectiveness of the multi-objective approach, a set of experiments was performed on 17 real-world module clustering problems. The results of this empirical study provide strong evidence to support the claim that the multi-objective approach produces significantly better solutions than the existing single-objective approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5406532","SBSE;module clustering;multi-objective optimization;evolutionary computation.","Search problems;Computer science;Performance evaluation;Software engineering;Clustering algorithms;Computational intelligence;Testing;Educational institutions;Computer applications;Application software","optimisation;pattern clustering;search problems;software engineering","software module clustering;multi-objective search problem;program structure","","134","32","","","","","","IEEE","IEEE Journals & Magazines"
"Efficiently Running Test Suites Using Abstract Undo Operations","S. A. Khalek; S. Khurshid","NA; NA","2011 IEEE 22nd International Symposium on Software Reliability Engineering","","2011","","","110","119","The last decade has seen many advances in test input generation, specifically using systematic approaches that can enumerate many tests. While such approaches have enhanced our ability to find bugs in programs, running large numbers of tests remains a time consuming and expensive task, especially for tests that execute operations on external resources, such as a file system or a network. This paper presents a novel technique for optimizing execution of suites of tests, where several tests in a suite may contain common initial execution -- a property often exhibited by systematically generated suites, e.g., those for bounded exhaustive testing. Our insight is that we can cluster execution of such tests by defining abstract-level undo operations, which allow a common execution segment to be performed once, and its result to be shared across the tests, which then perform the rest of their operations. We present our framework for clustered execution of test suites, and evaluate it using three case-studies, which show our technique enables significant performance speed-up over traditional test execution.","2332-6549;1071-9458;1071-9458","978-1-4577-2060-4978-0-7695-4568","10.1109/ISSRE.2011.20","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6132959","test clustering;check-pointing;unit test suites;undo operations","Databases;Testing;Semantics;Receivers;Instruments;Clustering algorithms;Context","checkpointing;optimisation;pattern clustering;program debugging;program testing;program verification","test input generation;suite execution optimization;systematically generated suites;bounded exhaustive testing;execution clustering;abstract level undo operation;execution segment","","2","21","","","","","","IEEE","IEEE Conferences"
"A modular software framework supporting simulation-driven optimization techniques","M. Enriquez","MITRE Center for Advanced Aviation System Development (CAASD), McLean, VA","2011 IEEE/AIAA 30th Digital Avionics Systems Conference","","2011","","","7E3-1","7E3-10","Simulation-Driven Optimization (SDO) problems (also referred to as ""optimal control"" or ""4D optimization"") are optimization problems with a simulation constraint. In the past decade, SDO techniques have been established as a promising tool for aviation analysis. Given a parameter-dependent simulation model, SDO techniques can automatically determine the optimal parameters that yield the desired simulation behavior. SDO techniques have been applied to aviation problems such as flight trajectory optimization, air traffic flow design and safety analysis of auto-land systems. The algorithmic solution of an SDO problem requires communication between simulation code (e.g., the numerical solution of the equations of motion) and optimization code (e.g., the Newton method). Typically, multiple simulations must be performed to form the numerical derivative of the cost function we seek to minimize or maximize, which must then be passed to some optimization software. This paper introduces the ""Time-Stepping for Optimization"" software framework or TSOpt to aid solution of SDO problems. TSOpt orchestrates communication and data exchange between the optimization code and the simulation code. TSOpt also offers support for implementation variants of the adjoint state method, a numerically efficient way to form derivatives for SDO problems. Further, TSOpt is equipped with tests that help ensure the correct numerical solution of SDO problems. Besides a concrete C++ software package, TSOpt framework offers a software paradigm that can be used to solve SDO problems on any platform, and in any language. I demonstrate this claim by solving a exploring the effect of low-fidelity wind data for a trajectory-based optimization problem in MATLAB.","2155-7209;2155-7195;2155-7195","978-1-61284-798-6978-1-61284-797-9978-1-61284-796","10.1109/DASC.2011.6096134","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6096134","","Mathematical model;Numerical models;Optimization;Unified modeling language;Computational modeling;Atmospheric modeling;Differential equations","aerospace computing;aircraft landing guidance;digital simulation;electronic data interchange;optimising compilers;parameter estimation;software architecture","modular software framework;simulation-driven optimization technique;SDO problem;aviation analysis;parameter-dependent simulation model;optimal parameter determination;autoland systems;simulation code;optimization code;time-stepping for optimization software framework;MATLAB;trajectory-based optimization problem;low-fidelity wind data;data exchange;TSOpt","","","6","","","","","","IEEE","IEEE Conferences"
"Incremental attribute based particle swarm optimization","W. Bai; S. Cheng; E. M. Tadjouddine; S. Guan","Department of Computer Science, University of Liverpool, UK; Department of Electrical Engineering and Electronics, University of Liverpool, UK; Department of Computer Science and Software Engineering, Xi'an Jiaotong-Liverpool University, Suzhou, China; Department of Computer Science and Software Engineering, Xi'an Jiaotong-Liverpool University, Suzhou, China","2012 8th International Conference on Natural Computation","","2012","","","669","674","An incremental-attribute based particle swarm optimization (IAPSO) which utilizes incremental learning strategy in function optimization is presented in this paper. Traditionally, particle swarm optimization (PSO) searches all the dimensions at the same time. Decomposition strategy is utilized in IAPSO to decompose the whole search space (D-dimension) into D numbers of one-dimensional space. In this approach, incremental learning strategy optimizes the function by searching the D-dimensional space one by one. Experimental results show that IAPSO gets more accurate and stable results than standard PSO in multimodal problems. IAPSO could avoid the “local optima”, i.e., it has better “exploration” ability than standard PSO.","2157-9563;2157-9555;2157-9555","978-1-4577-2133-5978-1-4577-2130-4978-1-4577-2132","10.1109/ICNC.2012.6234699","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6234699","Particle swarm optimization;Incremental learning;Multimodal function optimization","Optimization;Particle swarm optimization;Convergence;Benchmark testing;Educational institutions;Equations;Computer science","learning (artificial intelligence);particle swarm optimisation;search problems","incremental attribute-based particle swarm optimization;IAPSO;incremental learning strategy;function optimization;decomposition strategy;search space decomposition;one-dimensional space;D-dimensional space search;multimodal problems;exploration ability","","6","15","","","","","","IEEE","IEEE Conferences"
"Automatic path test data generation based on GA-PSO","Sheng Zhang; Ying Zhang; Hong Zhou; Qingquan He","School of Information Engineering, Nanchang Hangkong University, Jiangxi Province 330063, China; School of Information Engineering, Nanchang Hangkong University, Jiangxi Province 330063, China; School of Information Engineering, Nanchang Hangkong University, Jiangxi Province 330063, China; School of Information Engineering, Nanchang Hangkong University, Jiangxi Province 330063, China","2010 IEEE International Conference on Intelligent Computing and Intelligent Systems","","2010","1","","142","146","Automatic test data generation is a key issue to achieve test automation. The path test data generation is a hot point in the research field of software test investigation. The previous approaches of generating test data are mostly based on Genetic Algorithms (GA) and its improved algorithm. These approaches have tow shortcomings: one is too complex to use and difficult to set parameters. The other is weak local search and slow convergence. We propose a hybrid algorithm (GA-PSO) which combines Genetic Algorithm and Particle Swarm Optimization (PSO) in this paper. The new algorithm is proved effective by a representative test of the “triangle type of discrimination”. The experiment shows that the new algorithm has higher performance when the value of Φ is 20%.","","978-1-4244-6585-9978-1-4244-6582-8978-1-4244-6584","10.1109/ICICISYS.2010.5658735","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5658735","Particle Swarm Optimization;Genetic Algorithm;test data generation;GA-PSO Algorithm","Gallium;Security","automatic testing;genetic algorithms;particle swarm optimisation;program testing","automatic path test data generation;GA-PSO;test automation;genetic algorithms;particle swarm optimization;software testing","","7","8","","","","","","IEEE","IEEE Conferences"
"Software requirements selection using Quantum-inspired Elitist Multi-objective Evolutionary algorithm","A. Charan Kumari; K. Srinivas; M. P. Gupta","Department of Physics and Computer Science, Dayalbagh Educational Institute Agra, India; Department of Electrical Engineering, Dayalbagh Educational Institute Agra, India; Department of Management Studies, Indian Institute of Technology, Delhi, India","IEEE-International Conference On Advances In Engineering, Science And Management (ICAESM -2012)","","2012","","","782","787","This paper proposes a Quantum-inspired Elitist Multi-objective Evolutionary algorithm (QEMEA) for software requirements selection, a problem in search based software engineering. Most often software product developments are iterative and incremental in their nature, due to the changes in the customer requirements from time to time. It is a challenging task to select the requirements from a large number of candidates, for the accomplishment of the business goals. The problem is to identify a set of requirements to be included in the next release of the product, by minimizing the cost and maximizing the customer satisfaction. Since minimizing the total cost and maximizing the customer satisfaction are contradictory objectives, the problem has a multi-objective nature. This problem is NP-hard in its nature and so it cannot be solved efficiently by using traditional optimization techniques for large problem instances. QEMEA combines the best features of Evolutionary Algorithms and Quantum Computing. It employs the concepts of Quantum computing such as superposition and interference. The features of QEMEA help in achieving quality pareto-optimal front solutions and faster convergence while using a small population size. The performance of QEMEA is tested on six benchmark problems derived from the literature. The obtained results indicates consistent and superior performance of the algorithm.","","978-81-909042-2-3978-1-4673-0213-5INAVLID IS","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6215945","Search based software engineering;Software requirements selection;Multi-objective Next Release Problem;Quantum-inspired Multi-objective Evolutionary Algorithm","Benchmark testing","computational complexity;evolutionary computation;formal specification;optimisation;Pareto optimisation;quantum computing;software development management","software requirements selection;quantum-inspired elitist multiobjective evolutionary algorithm;QEMEA;search based software engineering;software product developments;cost minimization;customer satisfaction maximization;NP-hard problem;optimization techniques;quantum computing;Pareto-optimal front solutions","","1","9","","","","","","IEEE","IEEE Conferences"
"Recommender systems for manual testing","B. Miranda; E. Aranha; J. Lyoda","Centro de Inform&#x00E1;tica Universidade Federal de Pernambuco Recife-PE, Brazil CEP 50740-540; Departamento de Inform&#x00E1;tica e Matem&#x00E1;tica Aplicada Universidade Federal do Rio Grande do Norte Natal-RN, Brazil CEP 59078-970; Centro de Inform&#x00E1;tica Universidade Federal de Pernambuco Recife-PE, Brazil CEP 50740-540","Proceedings of the 2012 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement","","2012","","","201","210","BACKGROUND: Software testing can be an arduous and expensive activity. A typical activity to maximise testing productivity is to allocate test cases according to the testers' profile. However, optimising the allocation of manual test cases is not a trivial task: in big companies, test managers are responsible for allocating hundreds of test cases among several testers. OBJECTIVE: In this paper we propose and evaluate 2 assignment algorithms for test case allocation and 3 tester profiles based on recommender systems. Each assignment algorithm can be combined with 3 tester profiles, which results in six possible allocation systems. METHOD: We run a controlled experiment that uses 100 test suites, each one with at least 50 test cases, from a real industrial setting in order to compare our allocation systems to the manager's allocation in terms of precision, recall and unassignment (percentage of test cases the algorithm could not allocate). RESULTS: In our experiment, the statistical analysis shows that one of the systems outperforms the others with respect to the precision and recall metrics. For unassignment, three of our six allocation systems achieved zero (best value) for the unassignment rate. CONCLUSION: The results of our experiment suggest that, in similar environments, test managers can use our allocation systems to reduce the amount of time spent in the test case allocation task. In the real industrial setting in which our work was developed, managers spend from 16 to 30 working days a year on test case allocation. Our algorithms can help them do it faster and better.","1949-3770;1938-6451;1949-3789","978-1-4503-1056-7978-1-4503-1056-7978-1-4503-1056","10.1145/2372251.2372289","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6475418","Recommender systems;Test allocation;Manual testing","Resource management;Recommender systems;Testing;Measurement;Context;Approximation algorithms;Manuals","program testing;recommender systems;statistical analysis","recommender systems;manual testing;software testing;testing productivity maximization;test case allocation;assignment algorithms;tester profiles;statistical analysis;precision metrics;recall metrics;unassignment rate","","5","19","","","","","","IEEE","IEEE Conferences"
"CPN-a hybrid model for software cost estimation","C. V. M. K. Hari; Tegjyot Singh Sethi; B. S. S. Kaushal; A. Sharma","Department of Information Technology, GITAM Institute of Technology, GITAM University, Visakhapatnam, India; Department of Computer Science Engineering, GITAM Institute of Technology, GITAM University, Visakhapatnam, India; Department of Computer Science Engineering, GITAM Institute of Technology, GITAM University, Visakhapatnam, India; Department of Computer Science Engineering, GITAM Institute of Technology, GITAM University, Visakhapatnam, India","2011 IEEE Recent Advances in Intelligent Computational Systems","","2011","","","902","906","One of the challenges faced by the managers in the software industry today is the ability to accurately define the requirements of the software projects early in the software development phase. The cost-benefit analysis forms the basis of the planning and decision making throughout the software development lifecycle. As such there is a need for efficient software cost estimation techniques for making any endeavor viable. Software cost estimation is the process of prognosticating the amount of effort required to build a software project. In this paper we have proposed a Particle Swarm Optimization (PSO) technique which operates on data sets clustered using the K-means clustering algorithm. PSO is employed to generate parameters of the COCOMO model for each cluster of data values. The clusters and effort parameters are then trained to a Neural Network by using Back propagation technique, for classification of data. Here we have tested the model on the COCOMO 81 dataset and also compared the obtained values with standard COCOMO model. By making use of the experience from Neural Networks and the efficient tuning of parameters by PSO operating on clusters, the proposed model is able to generate better results and it can be applied efficiently to larger data sets.","","978-1-4244-9477-4978-1-4244-9478-1978-1-4244-9475-0978-1-4244-9476","10.1109/RAICS.2011.6069439","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6069439","Particle Swarm Optimization (PSO);K-Means algorithm;Software Cost estimation;COnstructive COst MOdel(COCOMO);Back propagation algorithm;CPN:Clustering-PSO-Neural Networks","Software;Clustering algorithms;Estimation;Training;Software algorithms;Mathematical model;Data models","backpropagation;cost-benefit analysis;neural nets;particle swarm optimisation;pattern classification;pattern clustering;Petri nets;software cost estimation","CPN hybrid model;software cost estimation;software project requirement;software development phase;cost-benefit analysis;particle swarm optimization;K-means clustering algorithm;neural network;backpropagation technique;data classification;COCOMO model;colored Petri nets","","","14","","","","","","IEEE","IEEE Conferences"
"Evolutionary Improvement of Programs","D. R. White; A. Arcuri; J. A. Clark","Department of Computer Science, University of York, York, U.K.; Simula Research Laboratory, Lysaker, Norway; Department of Computer Science, University of York, York, U.K.","IEEE Transactions on Evolutionary Computation","","2011","15","4","515","538","Most applications of genetic programming (GP) involve the creation of an entirely new function, program or expression to solve a specific problem. In this paper, we propose a new approach that applies GP to improve existing software by optimizing its non-functional properties such as execution time, memory usage, or power consumption. In general, satisfying non-functional requirements is a difficult task and often achieved in part by optimizing compilers. However, modern compilers are in general not always able to produce semantically equivalent alternatives that optimize non-functional properties, even if such alternatives are known to exist: this is usually due to the limited local nature of such optimizations. In this paper, we discuss how best to combine and extend the existing evolutionary methods of GP, multiobjective optimization, and coevolution in order to improve existing software. Given as input the implementation of a function, we attempt to evolve a semantically equivalent version, in this case optimized to reduce execution time subject to a given probability distribution of inputs. We demonstrate that our framework is able to produce non-obvious optimizations that compilers are not yet able to generate on eight example functions. We employ a coevolved population of test cases to encourage the preservation of the function's semantics. We exploit the original program both through seeding of the population in order to focus the search, and as an oracle for testing purposes. As well as discussing the issues that arise when attempting to improve software, we employ rigorous experimental method to provide interesting and practical insights to suggest how to address these issues.","1089-778X;1089-778X;1941-0026","","10.1109/TEVC.2010.2083669","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5688317","Coevolution;embedded systems;execution time;genetic programming;multiobjective optimization;non-functional criteria;search based software engineering","Optimization;Semantics;Evolutionary computation;Sorting;Genetic programming;Program processors","genetic algorithms;optimising compilers;probability;program testing;software engineering","genetic programming;evolutionary improvement;GP;optimizing compilers;probability distribution;software testing;multiobjective optimization;coevolution;software engineering","","43","65","","","","","","IEEE","IEEE Journals & Magazines"
"Parameter tuning of evolutionary algorithm by Meta-EAs for WCET analysis","I. Ashraf; G. M. Hassan; K. M. Yahya; S. A. A. Shah; S. Ullah; A. Manzoor; M. Murad","TU Delft, The Netherlands; University of Engineering &amp; Technology, Peshawar, Pakistan; University of Engineering &amp; Technology, Peshawar, Pakistan; University of Engineering &amp; Technology, Peshawar, Pakistan; University of Engineering &amp; Technology, Peshawar, Pakistan; University of Engineering &amp; Technology, Peshawar, Pakistan; University of Engineering &amp; Technology, Peshawar, Pakistan","2010 6th International Conference on Emerging Technologies (ICET)","","2010","","","7","10","Testing is the most important Quality Assurance (QA) measure which consumes a significant portion of budget, time and effort in the development process. For real time systems, temporal testing is as crucial as functional testing. An important activity in dynamic testing is the test case design. Evolutionary testing has shown promising results for the automation of test case design process at a reasonable computational cost. The disadvantage of evolutionary testing is that its time consuming and it depends on the parameter settings. Evolutionary algorithms can be used to find the optimal parameter settings of another evolutionary algorithm. In this research paper, a Meta level Evolutionary Algorithm (Meta-EA) is utilized to tune the parameters of evolutionary algorithm for Worst Case Execution Time (WCET) analysis. A number of experiments have been conducted for analysis using X32 (32-bit soft processor core implemented on FPGA) as the target hardware. Famous sorting algorithms have been used as programs under test for these experiments. Results have shown an average improvement of 25% in finding WCET by an evolutionary algorithm with tuned parameters compared to evolutionary algorithm with standard parameters. Furthermore, performance gap was found to be increasing with increase in test input size.","","978-1-4244-8058-6978-1-4244-8057-9978-1-4244-8056","10.1109/ICET.2010.5638389","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5638389","Evolutionary Testing;Random Testing;Parameter Tuning;Meta EA;Worst Case Execution Time Analysis","Testing;Evolutionary computation;Real time systems;Timing;Software;Tuning;Optimization","evolutionary computation;program testing;quality assurance;real-time systems;sorting","parameter tuning;meta-EA;WCET analysis;quality assurance measure;QA measure;real time systems;temporal testing;functional testing;dynamic testing;evolutionary testing;test case design process;optimal parameter settings;meta level evolutionary algorithm;worst case execution time analysis;32-bit soft processor core;FPGA;target hardware;sorting algorithms;programs under test","","","14","","","","","","IEEE","IEEE Conferences"
"Interaction test data generation using Harmony Search Algorithm","A. A. Alsewari; K. Z. Zamli","School of Electrical and Electronic Engineering, Universiti Sains Malaysia, Nibong Tebal, Pulau Pinang, Malaysia; School of Electrical and Electronic Engineering, Universiti Sains Malaysia, Nibong Tebal, Pulau Pinang, Malaysia","2011 IEEE Symposium on Industrial Electronics and Applications","","2011","","","559","564","This paper describes the adoption of Harmony Search Algorithm based strategy, called HSS, for generating interaction test data. In a nutshell, HSS generates a set of test data (as a complete test suite) that covers the t-way interaction at least once in a greedy manner (i.e. here, t indicates the interaction strength). The main feature of HSS is the fact that it is the first t-way strategy that is based on the Harmony Search Algorithm. Preliminary results demonstrate that HSS gives comparable results with other existing t-way strategies.","","978-1-4577-1417-7978-1-4577-1418-4978-1-4577-1416","10.1109/ISIEA.2011.6108775","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6108775","Harmony Search Algorithm;Software Testing;T-Way Testing;Combinatorial Testing","Testing;Optimization;Particle swarm optimization;Genetic algorithms;Educational institutions;Software algorithms","automatic testing","interaction test data generation;harmony search algorithm;t-way interaction","","7","29","","","","","","IEEE","IEEE Conferences"
"An improved template-based method for mining association rules from defect repositories","N. Li; Z. Li; X. Li","School of Computer Science and Technology, Northwestern Polytechnical University, Xi'an, China; School of Computer Science and Technology, Northwestern Polytechnical University, Xi'an, China; School of Computer Science and Technology, Northwestern Polytechnical University, Xi'an, China","2010 5th International Conference on Computer Science & Education","","2010","","","1796","1799","Frequent pattern mining from defect repositories has started playing an important role in software defect detection and analysis. In this paper, we present a new improved template-based method to generate association rules from defect repositories. We aim to provide manager and tester with more efficient method of generating rules. The improved method optimizes the generation using rule templates firstly, then reduces unnecessary rule generation by the theorems described in Section III-A. The advantages of the improved method are validated by an experiment based on 1860 defect reports. The experimental results show the improved template-based method can significantly reduce the total number of the rules, and shorten the run time. The pruned association rule set makes it easier to find the interesting rules.","","978-1-4244-6005-2978-1-4244-6002-1978-1-4244-6004","10.1109/ICCSE.2010.5593798","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5593798","Association rules;Template;Software defect;Black-box testing","Association rules;Software;Itemsets;Testing;Software algorithms","data mining;program testing;software development management","template-based method;association rules mining;defect repositories;pattern mining;software defect detection;rule generation","","1","8","","","","","","IEEE","IEEE Conferences"
"Using SQL Hotspots in a Prioritization Heuristic for Detecting All Types of Web Application Vulnerabilities","B. Smith; L. Williams","NA; NA","2011 Fourth IEEE International Conference on Software Testing, Verification and Validation","","2011","","","220","229","Development organizations often do not have time to perform security fortification on every file in a product before release. One way of prioritizing security efforts is to use metrics to identify core business logic that could contain vulnerabilities, such as database interaction code. Database code is a source of SQL injection vulnerabilities, but importantly may be home to unrelated vulnerabilities. The goal of this research is to improve the prioritization of security fortification efforts by investigating the ability of SQL hotspots to be used as the basis for a heuristic for prediction of all vulnerability types. We performed empirical case studies of 15 releases of two open source PHP web applications: Word Press, a blogging application, and WikkaWiki, a wiki management engine. Using statistical analysis, we show that the more SQL hotspots a file contains per line of code, the higher the probability that file will contain any type of vulnerability.","2159-4848","978-1-61284-174-8978-0-7695-4342","10.1109/ICST.2011.15","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770611","sql;hotspots;sql injection;prioritization;empirical;wordpress;wikkawiki","Security;Predictive models;Measurement;Mathematical model;Complexity theory;Software;Databases","Internet;probability;search engines;security of data;software metrics;SQL;statistical analysis;Web sites","SQL hotspots;prioritization heuristic;Web application vulnerability;development organizations;core business logic;database interaction code;database code;SQL injection;security fortification efforts;open source PHP Web applications;Word Press;blogging application;WikkaWiki;wiki management engine;statistical analysis;probability","","11","17","","","","","","IEEE","IEEE Conferences"
"Optimal Linearity Testing of Sigma-Delta Based Incremental ADCs Using Restricted Code Measurements","S. Kook; A. Gomes; L. Jin; D. Wheelright; A. Chatterjee","NA; NA; NA; NA; NA","2011 IEEE 17th International Mixed-Signals, Sensors and Systems Test Workshop","","2011","","","72","77","Linearity testing of high-precision (beyond 20-bit resolution) Analog-to-Digital converters (ADCs) is extremely expensive due to the large number of codes (&gt;;16 million for a 24-bit converter) that need to be tested and the associated low data rates making traditional histogram based testing infeasible. Industry often performs linearity test for such high-precision data converters with significantly reduced numbers of code measurements during production test. Given a specified allowed number of code measurements, the problem is to determine the requisite code points that result in the highest failure coverage. In this paper, a methodology and tools for analyzing the ""goodness"" of a particular choice of test code points versus another is described. A least squares based polynomial fitting approach using measurements made at selected test code points is used to characterize the transfer function of the ADC for INL (Integral Nonlinearity) error. In addition, the characteristics of devices that may escape from the proposed approach (test escapes) are revealed for the specified test via an optimization based search technique. Software simulations are performed to study and validate the proposed methodology.","","978-0-7695-4479-3978-1-4577-1144","10.1109/IMS3TW.2011.32","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6132741","","Polynomials;Histograms;Voltage measurement;Testing;Fitting;Operational amplifiers;Optimization","integrated circuit testing;least squares approximations;optimisation;polynomials;sigma-delta modulation;transfer functions","sigma-delta based incremental ADC;optimal linearity testing;restricted code measurements;analog-to-digital converters;histogram based testing;high-precision data converters;production test;failure coverage;test code points;least squares based polynomial fitting;transfer function;integral nonlinearity error;optimization;search technique","","","13","","","","","","IEEE","IEEE Conferences"
"Using Weighted Attributes to Improve Cluster Test Selection","Y. Wang; Z. Chen; Y. Feng; B. Luo; Y. Yang","NA; NA; NA; NA; NA","2012 IEEE Sixth International Conference on Software Security and Reliability","","2012","","","138","146","Cluster Test Selection (CTS) is widely-used in observation-based testing and regression testing. CTS selects a small subset of tests to fulfill the original testing task by clustering execution profiles. In observation-based testing, CTS saves human efforts for result inspection by reducing the number of tests and finding failures as many as possible. This paper proposes a novel strategy, namely WAS (Weighted Attribute based Strategy), to improve CTS. WAS is inspired by the idea of fault localization, which ranks the program entities to find possible faulty entities. The ranking of entity is considered as a weight of attribute in WAS. And then it helps build up a more suitable distance space for CTS. As a result, a more accurate clustering is obtained to improve CTS. We conducted an experiment on three open-source programs: flex, grep and gzip. The experimental results show that WAS can outperform all existing CTS techniques in observation-based testing.","","978-1-4673-2067-2978-0-7695-4742","10.1109/SERE.2012.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6258303","Iteratively clustering;Attribute weight;Fault localization;Test selection;Jaccard","Conferences;Software;Security;Software reliability","program testing;public domain software;regression analysis","cluster test selection;CTS;observation-based testing;regression testing;clustering execution profiles;inspection;failure finding;weighted attribute based strategy;fault localization;open-source programs;flex;grep;gzip","","2","22","","","","","","IEEE","IEEE Conferences"
"Applying a Modified Particle Swarm Optimizer to Discrete Truss Topology Optimization","B. Yang; Q. Zhang","NA; NA","2010 International Conference on Computational Intelligence and Software Engineering","","2010","","","1","4","Particle Swarm Optimization (PSO) is a new paradigm of Swarm Intelligence which proposes that the co-operation of individuals promotes the evolution of the swarm. Recently, a modified Particle Swarm Optimizer (MLPSO) has been succeeded in solving truss topological optimization problems with continuous design variable and competitive results were obtained. Since most of structural problems involve discrete design variables, an effect strategy is involved in MLPSO in order to operate on discrete variables. The performance of the proposed algorithm is then tested by two discrete truss topology optimization problems which belong to the most challenging fields of optimization. The proposed optimization algorithm exhibited competitive performance compared with the best results so far obtained due to it improved global searching ability. Final part is conclusion and outlook.","","978-1-4244-5391-7978-1-4244-5392","10.1109/CISE.2010.5677255","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5677255","","Optimization;Particle swarm optimization;Algorithm design and analysis;Topology;Signal processing algorithms;Convergence","design engineering;particle swarm optimisation;structural engineering;supports","modified particle swarm optimizer;discrete truss topology optimization;MLPSO;structural design","","2","11","","","","","","IEEE","IEEE Conferences"
"A novel particle swarm optimization algorithm","W. Dongyun; Z. Ping; L. Luowei; W. Kai","Department of Electronic and Information Zhongyuan Institute of Technology Zhengzhou, China; Department of Electronic and Information Zhongyuan Institute of Technology Zhengzhou, China; Department of Electronic and Information Zhongyuan Institute of Technology Zhengzhou, China; Department of Electronic and Information Zhongyuan Institute of Technology Zhengzhou, China","2010 IEEE International Conference on Software Engineering and Service Sciences","","2010","","","408","411","A novel particle swarm optimization (NPSO) algorithm with dynamically changing inertia weight based on fltness and iterations was presented for improving the performance of the Particle Swarm Optimization algorithm. The new algorithm was tested with three benchmark functions. The experimental results show that the swarm can escape from local optimum, and it also can speed up the convergence of particles to improve the performance.","2327-0586;2327-0594","978-1-4244-6055-7978-1-4244-6054-0978-1-4244-6053","10.1109/ICSESS.2010.5552354","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5552354","Particle Swarm Optimization;inertia weight;extreme value;convergence speed;premature","Particle swarm optimization;Heuristic algorithms;Equations;Convergence;IEEE services;Optimization;Conferences","convergence;particle swarm optimisation","particle swarm optimization algorithm;inertia weight;particle convergence","","1","12","","","","","","IEEE","IEEE Conferences"
"Modeling the Diagnostic Efficiency of Regression Test Suites","A. Gonzalez-Sanchez; H. Gross; A. J. C. van Gemund","NA; NA; NA","2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops","","2011","","","634","643","Diagnostic performance, measured in terms of the manual effort developers have to spend after faults are detected, is not the only important quality of a diagnosis. Efficiency, i.e., the number of tests and the rate of convergence to the final diagnosis is a very important quality of a diagnosis as well. In this paper we present an analytical model and a simulation model to predict the diagnostic efficiency of test suites when prioritized with the information gain algorithm. We show that, besides the size of the system itself, an optimal coverage density and uniform coverage distribution are needed to achieve an efficient diagnosis. Our models allow us to decide whether using IG with our current test suite will provide a good diagnostic efficiency, and enable us to define criteria for the generation or improvement of test suites.","","978-0-7695-4345-1978-1-4577-0019","10.1109/ICSTW.2011.22","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954476","test prioritization;diagnosis;diagnostic effectiveness","Predictive models;Analytical models;Fault diagnosis;Entropy;Testing;Mathematical model;Bayesian methods","program testing;regression analysis","regression test suites;diagnostic efficiency;information gain algorithm;optimal coverage density;uniform coverage distribution","","3","35","","","","","","IEEE","IEEE Conferences"
"Preliminary test and evaluation of non-destructive examination for ITER First Wall development in Korea","Suk-Kwon Kim; Eo Hwak Lee; Jae-Sung Yoon; Hyun-Kyu Jung; Dong Won Lee; Byoung-Yoon Kim","Korea Atomic Energy Research Institute, Daejeon, Republic of Korea; Korea Atomic Energy Research Institute, Daejeon, Republic of Korea; Korea Atomic Energy Research Institute, Daejeon, Republic of Korea; Korea Atomic Energy Research Institute, Daejeon, Republic of Korea; Korea Atomic Energy Research Institute, Daejeon, Republic of Korea; ITER Korea, National Fusion Research Institute, Daejeon, Republic of Korea","2011 IEEE/NPSS 24th Symposium on Fusion Engineering","","2011","","","1","4","ITER First Wall (FW) includes beryllium armour joined to a Cu heat sink with a stainless steel back plate. These first wall panels are one of the critical components in the ITER tokamak with a maximum surface heat flux of 5 MW/m2. So, a qualification test needs to be performed with the goal to qualify the joining technologies required for the ITER first wall. Various mockups were fabricated to develop the manufacturing procedure of first wall components. For the non-destructive examination (NDE) of the fabricated mockups, an ultrasonic test (UT) was performed with optimized probes. The UT test was performed by using a three-axis digital ultrasonic C-scan system and software. The system is comprised of an ultrasonic pulser/receiver, model Panametrics 5800PR, a personal computer having an internal analog/digital converter board and four axis motion control board, and a three-axis scanning tank. Two type transducers were used for this experiment. One was Panametrics V312-SU, having a center frequency of 10 MHz (nominal), a piezoelectric element diameter of 0.25 inch with a flat protective layer for the Be/Cu. The other was Panametrics V309-SU with a center frequency of 5 MHz and an element diameter of 0.5 inch for the Cu/SS interface. Winspect software controlled all aspects of data acquisition, motion control, data archiving, and image display. Based on the acceptance criteria, average amplitude of the interface signals, which have about 50% of the reference echo amplitude, was recorded and analyzed on each beryllium tile. Image analysis software analyzed the statistics of amplitude distribution and calculated the unacceptable area. Each mockup that passed these UT tests was concluded to qualify the joining technologies required for an ITER first wall by using high heat flux test facility. As a result of these qualification tests based on the acceptance criteria of an ITER first wall, the fabrication technologies will be utilized to develop the first wall of plasma facing components.","2155-9953;1078-8891;1078-8891;1078-8891","978-1-4577-0668-4978-1-4577-0669-1978-1-4673-0103-9978-1-4577-0667","10.1109/SOFE.2011.6052288","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6052288","ITER;semi-prototype;mockup;NDE;Ultrasonic test;Qualification test","Image resolution;Acoustics;Copper;Heating;Prototypes;Receivers;Oscilloscopes","analogue-digital conversion;beryllium;copper;data acquisition;fusion reactor design;heat sinks;joining processes;motion control;nondestructive testing;plasma toroidal confinement;plasma transport processes;plasma-wall interactions;stainless steel;Tokamak devices;transducers","nondestructive method;ITER first wall development;heat sink;stainless steel back plate;ITER tokamak critical component;surface heat flux;joining technology;ultrasonic test;mockup test;three-axis digital ultrasonic C-scan system;ultrasonic pulser;ultrasonic receiver;Panametrics 5800PR model;analog-digital converter board;four axis motion control board;three-axis scanning tank;Panametrics V312-SU;piezoelectric element diameter;Winspect software;data acquisition;data archiving;image display analysis;image analysis software;amplitude distribution statistics;high heat flux test facility;plasma facing components;size 0.25 inch;frequency 10 MHz;frequency 5 MHz;size 0.5 inch","","","8","","","","","","IEEE","IEEE Conferences"
"Can clone detection support test comprehension?","B. Hauptmann; M. Junker; S. Eder; E. Juergens; R. Vaas","Technische Universita&#x00A8;t Mu&#x00A8;nchen, Garching b. Mu&#x00A8;nchen, Germany; Technische Universita&#x00A8;t Mu&#x00A8;nchen, Garching b. Mu&#x00A8;nchen, Germany; Technische Universita&#x00A8;t Mu&#x00A8;nchen, Garching b. Mu&#x00A8;nchen, Germany; CQSE GmbH, Mu&#x00A8;nchen, Germany; Munich Re Group, Mu&#x00A8;nchen, Germany","2012 20th IEEE International Conference on Program Comprehension (ICPC)","","2012","","","209","218","Tests are central artifacts of software systems. Therefore, understanding tests is essential for activities such as maintenance, test automation, and efficient execution. Redundancies in tests may significantly decrease their understandability. Clone detection is a technique to find similar parts in software artifacts. We suggest using this technique to gain a better understanding of tests and to provide guidance for testing activities. We show the capabilities as well as the limits of this approach by conducting a case study analyzing more than 4000 tests of seven industrial software systems.","1092-8138","978-1-4673-1216-5978-1-4673-1213-4978-1-4673-1215","10.1109/ICPC.2012.6240490","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6240490","Program Comprehension;Software Testing;Software Maintenance;Clone Detection","Cloning;Manuals;Testing;Automation;Inspection;Optimization;Maintenance engineering","program testing","clone detection;test comprehension;software system central artifacts;industrial software systems","","4","32","","","","","","IEEE","IEEE Conferences"
"Optimization of design and application of micro-grid energy management system data acquisition system","L. Zhou; M. Ding; R. Bi","Photovoltaic System Research Center of MOE, Hefei University of Technology, China; Photovoltaic System Research Center of MOE, Hefei University of Technology, China; Photovoltaic System Research Center of MOE, Hefei University of Technology, China","The 2nd International Symposium on Power Electronics for Distributed Generation Systems","","2010","","","765","768","In a Micro-grid research platform by studying distributed generation with distributed power, the reliability and data real-time of Data Acquisition System, directly affect the validity and accuracy of Control System Operation. In the process of design Data acquisition system, we put forward the optimization communication model by the combination between completion port and the thread pool, the model of the communication pipeline maintenance with the hierarchical management of data collection objects' contexts. Firstly, Data Acquisition System established the network hardware platform which ensures real-time data and scalability system. Secondly, the internal structure of DAS is designed by three aspects of the structure model, data model and information model, describing the optimization structure. Lastly, based on the above software system design ideas, DAS was realized in the physical platform, reached the performance testing by professional testing software and union-test experiment with each functional module of platform.","2329-5759;2329-5767","978-1-4244-5670-3978-1-4244-5669-7978-1-4244-5671","10.1109/PEDG.2010.5545845","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5545845","Micro-grid;Data Acquisition System;IOCP;Two-step Contexts","Data acquisition;Software;Real time systems;Hardware;Data models;Context","data acquisition;distributed power generation;energy management systems;power grids;program testing","micro-grid energy management system;data acquisition system;distributed generation;optimization communication model;completion port;thread pool;software system design;performance testing;professional testing software;union-test experiment","","","10","","","","","","IEEE","IEEE Conferences"
"Low Energy Online Self-Test of Embedded Processors in Dependable WSN Nodes","A. Merentitis; N. Kranitis; A. Paschalis; D. Gizopoulos","University of Athens, Athens; University of Athens, Athens; University of Athens, Athens; University of Athens, Athens","IEEE Transactions on Dependable and Secure Computing","","2012","9","1","86","100","Wireless Sensor Network (WSN) nodes are often deployed in harsh environments where the possibility of permanent and especially intermittent faults due to environmental hazards is significantly increased, while silicon aging effects are also exacerbated. Thus, online and in-field testing is necessary to guarantee correctness of operation. At the same time, online testing of processors integrated in WSN nodes has the requirement of minimum energy consumption, because these devices operate on battery, cannot be connected to any external power supply, and the battery duration determines the lifetime of the system. Software-Based Self-Test (SBST) has emerged as an effective strategy for online testing of processors integrated in nonsafety critical applications. However, the notion of dependability includes not only reliability but also availability. Thus, in order to encase both aspects we present a methodology for the optimization of SBST routines from the energy perspective. The refined methodology presented in this paper is able to be effectively applied in the case that the SBST routines are not initially available and need to be downloaded to the WSN nodes, as well as the case that the SBST routines are available in a flash memory. The methodology is extended to maximize the energy gains for WSN architectures offering clock gating or Dynamic Frequency Scaling features. Simulation results show that energy savings at processor level are up to 36.5 percent, which depending on the characteristics of the WSN system, can translate in several weeks of increased lifetime, especially if the routines need to be downloaded to the WSN node.","1545-5971;1941-0018;2160-9209","","10.1109/TDSC.2011.17","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5714702","Dependability;low energy;online testing;processor test;test generation;Wireless Sensor Network nodes.","Wireless sensor networks;Program processors;Encoding;Optimization;Built-in self-test;Testing;Online services","embedded systems;energy consumption;microprocessor chips;program testing;telecommunication computing;wireless sensor networks","low energy online self test;embedded processors;dependable WSN nodes;wireless sensor network;environmental hazards;energy consumption;external power supply;battery duration;software based self test;SBST;flash memory;clock gating;dynamic frequency scaling features","","10","39","","","","","","IEEE","IEEE Journals & Magazines"
"UHF RFID Portal Performance Optimization & Evaluation Method","H. Tao; T. Li; D. Wang","NA; NA; NA","2010 Second International Conference on Computer Engineering and Applications","","2010","2","","354","357","UHF RFID portal, usually composed by two or three antennas which connect to one reader. However, RFID system designers often feel puzzle about the antenna deployment since they have no idea of how high or which angle the antenna should be or how many antennas should be deployed. An antenna deployment optimization and evaluation method was proposed in this paper, which can guide the designer to optimize RFID portal system deployment, and it can also help them evaluate their portal performance. The biggest advantage of the method is it's effect and easy to use.","","978-1-4244-6080-9978-1-4244-6079","10.1109/ICCEA.2010.220","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5445670","RFID;Portal;optimization;evaluation","Radiofrequency identification;Portals;Testing;Design optimization;Antenna arrays;Logistics;Transmitting antennas;Computer applications;Application software;Computer science","optimisation;portals;radiofrequency identification;UHF antennas","UHF RFID portal performance optimization;RFID system designers;antenna deployment optimization method;antenna deployment evaluation method;radiofrequency identification","","","15","","","","","","IEEE","IEEE Conferences"
"An Effective Approach to Identifying Optimal Software Reliability Allocation with Consideration of Multiple Constraints","G. Kim; J. Park; J. Baik","NA; NA; NA","2012 IEEE/ACIS 11th International Conference on Computer and Information Science","","2012","","","541","546","After a software reliability goal is set for safety-critical or mission-critical software system development, it is necessary to allocate software reliability goals for individual subsystems or components so that they meet the system reliability goal. This reliability allocation also helps to allocate test resources such as cost and schedule effectively. In order to achieve these advantages, it needs to identify the optimal software reliability allocation for multiple objectives, which maximize software reliability and minimize cost and schedule. However, it is difficult to identify an optimal software reliability allocation because reliability, cost, and schedule conflict with each other. Currently, there are several software reliability allocation techniques, but schedule planning and effective resource use are unavailable because they do not support multi-objective optimization. This paper proposes an effective software reliability allocation technique that supports multi-objective optimization. The proposed approach uses a multi-objective genetic algorithm (NSGA-II) for the multi-objective optimization. This approach assists schedule planning as well as effective resources allocation. A case study is presented to show the strength of the proposed approach. Our approach helps project manager to identify well-balanced strategies with consideration of reliability, cost, and schedule in the software reliability allocation phase.","","978-1-4673-1536","10.1109/ICIS.2012.29","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6211150","Software Reliability Allocation;Software Reliability;Optimization;Genetic Algorithm","Software reliability;Schedules;Resource management;Biological cells;Mathematical model;Optimization","genetic algorithms;safety-critical software","optimal software reliability allocation identification;mission-critical software system development;safety-critical software system development;multiobjective genetic algorithm","","","20","","","","","","IEEE","IEEE Conferences"
"Cloud Enabling Data Centers for Optimized Development and Test Operations","S. R. Pillai","NA","2012 IEEE International Conference on Cloud Computing in Emerging Markets (CCEM)","","2012","","","1","5","The demand from IT for the development organizations today are required to reduce capital expense and maximize existing investment. At the same time deliver high quality IT services for large development and testing teams that are geographically dispersed, with better visibility and control. As such, cloud computing with its widely-touted benefits made a convincing case for adoption. The imperative is to consolidate, virtualize and cloud enable the Data Centers in order to reduce escalating costs and free up resources to support growth and innovation.","","978-1-4673-4422-7978-1-4673-4421-0978-1-4673-4420","10.1109/CCEM.2012.6354602","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6354602","","Servers;Cloud computing;Testing;Automation;Organizations;Hardware","cloud computing;computer centres;groupware;organisational aspects;program testing;software quality;virtualisation","data centers;organization development;capital expense reduction;investment maximization;IT service quality;geographically dispersed testing teams;geographically dispersed development teams;cloud computing;virtualization;escalating cost reduction;free resources","","","3","","","","","","IEEE","IEEE Conferences"
"The Workflow Management System Model of Spacecraft Test Based on Web Service","X. Du; J. Song; Y. Zhao; X. Li","NA; NA; NA; NA","2010 International Conference on Intelligent Computing and Cognitive Informatics","","2010","","","26","28","The long time and cumbersome process of spacecraft test have hampered the development of China's space industry. In order to improve the launch success rate, shorten the test time, optimize the test process, a new type of implementing project of the test and launch flow was proposed based on the theory of system engineering. In addition, this paper introduced the idea of workflow into the area of spacecraft test. A new workflow management system of spacecraft test based on Web service was built. It solved the complexity and deficiencies of system integrating ability in current workflow system. And it simplified the flow pattern, reduces the modeling complexity, supports visualizing modeling and enhances the system integrating. Particularly, the framework can propose a series of feasible solutions and give us higher-level of efficiency in improving the quality of spacecraft test.","","978-1-4244-6641-2978-1-4244-6640","10.1109/ICICCI.2010.22","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5566044","workflow management system;spacecraft test;web service;modeling","Space vehicles;Engines;Process control;Databases;Workflow management software;Web services;Solid modeling","aerospace computing;aircraft testing;space vehicles;systems engineering;Web services;workflow management software","workflow management system model;spacecraft test;Web service;China space industry;system engineering;system integrating ability","","","19","","","","","","IEEE","IEEE Conferences"
"RealProct: Reliable Protocol Conformance Testing with Real Nodes for Wireless Sensor Networks","J. Xiong; E. C. -. Ngai; Y. Zhou; M. R. Lyu","NA; NA; NA; NA","2011IEEE 10th International Conference on Trust, Security and Privacy in Computing and Communications","","2011","","","572","581","Despite the various applications of wireless sensor network (WSN), experiences from real WSN deployments show that protocol implementations in sensor nodes are susceptible to software failures, which may cause network failures or even breakdown. Pre-deployment protocol conformance testing is essential to ensure reliable communications for WSNs. Unfortunately, existing solutions with simulators cannot test the exact hardware and implementation environment as real sensors, whereas testbeds are expensive and limited to small scale networks and topologies. In this paper, we present RealProct, a novel and reliable framework for testing protocol implementations against their specifications in WSNs. RealProct utilizes real sensors for protocol conformance testing to ensure that the results are close to the real deployment. Using different techniques from those in simulations and real deployments, RealProct virtualizes a large network with any topology and generate non-deterministic events using only a small number of sensors to provide flexibility and to reduce the cost. The framework is carefully designed to support efficient testing in resource-limited sensors. Moreover, test execution and verdict are optimized to minimize the number of runs, while guaranteeing satisfactory false posi tive and false negative rates. We implement RealProct and test it with the IIP TCP/IP protocol stack and a routing protocol developed for WSNs in Contiki-2.4. The results demonstrate the effectiveness of RealProct by detecting several new bugs and all previously discovered bugs in various versions of the μIP TCP/IP protocol stack.","2324-898X;2324-9013","978-1-4577-2135","10.1109/TrustCom.2011.74","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6120867","Protocol conformance testing;wireless sensor networks","Protocols;Testing;Wireless sensor networks;Topology;Computer bugs;Software;Network topology","complex networks;conformance testing;optimisation;routing protocols;sensor placement;software radio;telecommunication network reliability;telecommunication network topology;transport protocols;wireless sensor networks","wireless sensor networks;reliability;WSN deployments;software failures;protocol conformance testing;small-scale networks;network topology;optimization;RealProct;TCP/IP protocol;routing protocol;μIP protocol stack","","2","27","","","","","","IEEE","IEEE Conferences"
"A memory binary particle swarm optimization","Zhen Ji; Tao Tian; Shan He; Zexuan Zhu","Shenzhen City Key Laboratory of Embedded System Design, College of Computer Science and Software Engineering, Shenzhen University, China, 518060; Shenzhen City Key Laboratory of Embedded System Design, College of Computer Science and Software Engineering, Shenzhen University, China, 518060; School of Computer Science, University of Birmingham, Edgbaston, B15 2TT, UK; Shenzhen City Key Laboratory of Embedded System Design, College of Computer Science and Software Engineering, Shenzhen University, China, 518060","2012 IEEE Congress on Evolutionary Computation","","2012","","","1","5","This paper proposes a memory binary particle swarm optimization algorithm (MBPSO) based on a new updating strategy. Unlike the traditional binary PSO, which updates the binary bits of a particle ignoring their previous status, MBPSO memorizes the bit status and updates them according to a new defined velocity. As such, precious historical information could be retained to guide the search. The velocity vector of MBPSO is designed as a probability for deciding whether the particle bits change or not. The proposed algorithm is tested on four discrete benchmark functions. The experimental results reported over 100 runs show that MBPSO is capable of obtaining encouraging performance in discrete optimization problems.","1089-778X;1941-0026","978-1-4673-1509-8978-1-4673-1510-4978-1-4673-1508","10.1109/CEC.2012.6256150","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6256150","Binary Particle Swarm Optimization;PSO;Discrete PSO","Particle swarm optimization;Optimization;Benchmark testing;Algorithm design and analysis;Educational institutions;Cities and towns","particle swarm optimisation;probability","memory binary particle swarm optimization;MBPSO;binary PSO;historical information;probability;discrete benchmark functions;discrete optimization problems","","1","14","","","","","","IEEE","IEEE Conferences"
"Research on Fatigue Life of the Automobile Differential Case","X. Yu; X. Chen; P. Huang; G. Jie","NA; NA; NA; NA","2010 International Conference on Measuring Technology and Mechatronics Automation","","2010","3","","74","78","Basing on the fatigue breaking of the differential case of the drive axle with the material of nodular graphite cast iron, the fatigue analysis is made. According to the criterion of `Q/T 543-1999 Drive Axle Bench Test Standard', with the fatigue software FEMFAT 4.7D, the research on the cumulative damage and endurance limit safety factors in differential case is studied in terms of the Miner modified fatigue theory. The reason of the failure of the differential case can be sure that the stress concentration is at the region of the differential shaft hole in the differential case, and the safety factors are beyond the safe yield limit of the material QT450-10. Same is the comparison of the experimental results to simulation results on the failure sample, which is sure that the fatigue analysis method is reasonable and effective. It is helpful to optimize the structure parameter in the differential case design through the optimization algorithm for the drive axle in the automobile.","2157-1473;2157-1481","978-1-4244-5739-7978-1-4244-5001","10.1109/ICMTMA.2010.64","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5460281","Differential case Miner modified fatigue theory Cumulative damage Endurance limit safety factor Simulation analysis","Fatigue;Automobiles;Axles;Software safety;Design optimization;Automotive materials;Cast iron;Software testing;Software standards;Stress","automotive components;automotive engineering;axles;benchmark testing;cast iron;failure analysis;fatigue;optimisation","fatigue life;automobile differential case;drive axle;QT 543-1999 drive axle bench test standard;damage;endurance limit safety factors;failure analysis;optimization;nodular graphite cast iron","","1","7","","","","","","IEEE","IEEE Conferences"
"Research on the static function call path generating automatically","Y. Zheng; Y. Mu; Z. Zhang","Open Computer System Laboratory Beijing Information Science & Technology University Beijing, China; Open Computer System Laboratory Beijing Information Science & Technology University Beijing, China; Open Computer System Laboratory Beijing Information Science & Technology Universit Beijing, China","2010 2nd IEEE International Conference on Information Management and Engineering","","2010","","","405","409","In the regression testing of analyzing the impact of changes, it will give a great help to software testers to choose and optimize test cases for function call path coverage testing, while knowing the global static function call path. In this article, according to the structure of C program, a new method of generating the global static function call path and getting the function call graph as well was presented through analyzing control flow and data flow. We empirically show that this algorithm has high accuracy and efficiency and can help to improve coverage rate of function call path coverage testing and software stability.","","978-1-4244-5263-7978-1-4244-5265","10.1109/ICIME.2010.5478114","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5478114","function call path;function call path coverage testing;call graph;control flow;dataflow","Software testing;System testing;Tree graphs;Automatic testing;Flow graphs;Logic testing;Laboratories;Information science;Control systems;Switches","data flow graphs;functional analysis;program testing;regression analysis","static function call path coverage testing;regression testing;software testers;C program;function call graph;control flow;data flow;software stability","","8","13","","","","","","IEEE","IEEE Conferences"
"Finding software fault relevant subgraphs a new graph mining approach for software debugging","Z. Mousavian; M. Vahidi-Asl; S. Parsa","Faculty of Computer engineering, Iran University of Science and Technology, Tehran, Iran; Faculty of Computer engineering, Iran University of Science and Technology, Tehran, Iran; Faculty of Computer engineering, Iran University of Science and Technology, Tehran, Iran","2011 24th Canadian Conference on Electrical and Computer Engineering(CCECE)","","2011","","","000908","000911","In this paper, a new approach for analyzing program behavioral graphs to detect fault suspicious subgraphs is presented. The existing graph mining approaches for bug localization merely detect discriminative subgraphs between failing and passing runs, which are not applicable when the context of a failure is not appeared in a discriminative pattern. In our proposed method, the suspicious transitions are identified by contrasting nearest neighbor failing and passing dynamic behavioral graphs. The technique takes advantage of null hypothesis testing and a new formula for ranking edges is presented. To construct the most bug relevant subgraph, the high ranked edges are applied and presented to the debugger. The experimental results on Siemens test suite and Space program reveal effectiveness of the proposed method on weighted dynamic graphs for locating bugs in comparison with other methods.","0840-7789;0840-7789","978-1-4244-9789-8978-1-4244-9788-1978-1-4244-9787","10.1109/CCECE.2011.6030590","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6030590","Graph Mining;Weighted Graphs;Software Fault Localization;Null Hypothesis Testing","Data mining;Computer bugs;Software debugging;Testing;Debugging;Context","data mining;graph theory;program debugging;program testing","program behavioral graph;fault suspicious subgraph detection;suspicious transitions;nearest neighbor failing graphs;passing dynamic behavioral graphs;bug relevant subgraph;Siemens test suite;Space program;program debugger;weighted dynamic graphs;software debugging","","","17","","","","","","IEEE","IEEE Conferences"
"Multiobjective optimization with an asymptotically uniform coverage of Pareto front","J. Míchal; J. Dobeš; D. Černý","Department of Mixed Signal Design Silicon & Software Systems Klicperova 12, 150 00 Praha 5, Czech Republic; Department of Radio Engineering Czech Technical University in Prague Technická 2, 166 27 Praha 6, Czech Republic; Department of Radio Engineering Czech Technical University in Prague Technická 2, 166 27 Praha 6, Czech Republic","Proceedings of 2010 IEEE International Symposium on Circuits and Systems","","2010","","","2912","2915","This paper suggests an enhancement of an existing method for the multiobjective optimization known as GAM (goal attainment method). In our proposal, the GAM algorithm is combined with a mechanism that automatically provides a set of parameters (weights, coordinates of the reference point) for which the method generates noninferior solutions uniformly spread over a suitably selected part of the Pareto front. The resulting set of solutions is then presented in a suitable graphic form so that the solution representing the most satisfactory tradeoff can be easily chosen. The whole algorithm was implemented as a program and tested on several RF design examples (video, low-noise, and power amplifiers), whose optimization results are also presented. For a comparison, the first design example was also solved by another method known as WMM (weighted metrics method).","0271-4302;2158-1525","978-1-4244-5308-5978-1-4244-5309","10.1109/ISCAS.2010.5538044","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5538044","","Pareto optimization;Optimization methods;Proposals;Graphics;Testing;Radio frequency;Algorithm design and analysis;Low-noise amplifiers;Radiofrequency amplifiers;Design optimization","network synthesis;Pareto optimisation","multiobjective optimization;Pareto front;goal attainment method;GAM algorithm;RF design;weighted metric method;circuit design","","2","5","","","","","","IEEE","IEEE Conferences"
"Optimization design of ZL50 loader gearbox based on genetic algorithm","L. Chang; L. Xu; R. Ge","Faculty of Transportation Engineering, HuaiYin Institute of Technology, Huai'an, China; School of Automobile and Traffic Engineering, Jiangsu University, Zhenjiang, China; School of Automobile and Traffic Engineering, Jiangsu University, Zhenjiang, China","2010 International Conference on Computer, Mechatronics, Control and Electronic Engineering","","2010","3","","451","454","This paper analyzed the method of calculating work resistance and consumed power in hydraulic system in a work cycle of loader, established mathematical model of work resistance, and selected the CYC_TEST_10sec as a work cycle. Based on the simulation software of power performance and fuel economy of loader, we designed the interface mode between simulation software and optimization algorithm. To take the gear ratio of loader as optimization goal and make the cumulative fuel consumption as objective function, we optimized the gear ratio of ZL50 loader with genetic algorithm and calculated its power performance and fuel economy before and after optimization respectively. The results show that the optimization design proposal can improve power performance and fuel economy of ZL50 loader.","2159-6026;2159-6034","978-1-4244-7958-0978-1-4244-7957-3978-1-4244-7955","10.1109/CMCE.2010.5610281","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5610281","loader;gearbox;gear ratio;genetic algorithm;optimization","Optimization;Biological system modeling;Load modeling;Gears;Acceleration;Software","design engineering;fuel economy;gears;genetic algorithms;hydraulic systems;mechanical engineering computing;power consumption","optimization design;ZL50 loader gearbox;genetic algorithm;work resistance;power consumption;hydraulic system;work cycle;CYC_TEST_10sec;simulation software;fuel economy;power performance;mathematical model","","","6","","","","","","IEEE","IEEE Conferences"
"Graph-Based Optimization Algorithm and Software on Kidney Exchanges","Y. Chen; Y. Li; J. D. Kalbfleisch; Y. Zhou; A. Leichtman; P. X. -. Song","Department of Biostatistics, University of Michigan, Ann Arbor, USA; Department of Biostatistics, University of Michigan, Ann Arbor, USA; Department of Biostatistics, University of Michigan, Ann Arbor, USA; Department of Biostatistics, University of Michigan, Ann Arbor, USA; Department of Internal Medicine , University of Michigan, Ann Arbor, USA; Department of Biostatistics, University of Michigan, Ann Arbor, USA","IEEE Transactions on Biomedical Engineering","","2012","59","7","1985","1991","Kidney transplantation is typically the most effective treatment for patients with end-stage renal disease. However, the supply of kidneys is far short of the fast-growing demand. Kidney paired donation (KPD) programs provide an innovative approach for increasing the number of available kidneys. In a KPD program, willing but incompatible donor-candidate pairs may exchange donor organs to achieve mutual benefit. Recently, research on exchanges initiated by altruistic donors (ADs) has attracted great attention because the resultant organ exchange mechanisms offer advantages that increase the effectiveness of KPD programs. Currently, most KPD programs focus on rule-based strategies of prioritizing kidney donation. In this paper, we consider and compare two graph-based organ allocation algorithms to optimize an outcome-based strategy defined by the overall expected utility of kidney exchanges in a KPD program with both incompatible pairs and ADs. We develop an interactive software-based decision support system to model, monitor, and visualize a conceptual KPD program, which aims to assist clinicians in the evaluation of different kidney allocation strategies. Using this system, we demonstrate empirically that an outcome-based strategy for kidney exchanges leads to improvement in both the quantity and quality of kidney transplantation through comprehensive simulation experiments.","0018-9294;1558-2531","","10.1109/TBME.2012.2195663","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6188515","Kidney exchanges;optimal matches;software","Kidney;Resource management;Computational modeling;Optimal matching;Blood;Uncertainty;Graphical user interfaces","decision support systems;diseases;graph theory;interactive systems;kidney;knowledge based systems;medical computing;optimisation;patient monitoring;patient treatment","graph-based optimization algorithm;kidney exchanges;kidney transplantation;patient treatment;end-stage renal disease;kidney paired donation programs;altruistic donors;organ exchange mechanisms;rule-based strategy;graph-based organ allocation algorithms;outcome-based strategy;interactive software-based decision support system;conceptual KPD program","Algorithms;Databases, Factual;Histocompatibility Testing;Humans;Kidney Transplantation;Living Donors;Medical Informatics Applications;Software;User-Computer Interface","11","19","","","","","","IEEE","IEEE Journals & Magazines"
"Application of data mining technology on particle swarm optimization and support vector regression in shareprice prediction","L. Shu-ping; Z. Yong; X. Chun-yan","Department of computer science Mudanjiang Teachers College, Mudanjiang 157012, China; Key Lab of Electronic Engineering, Heilongjiang University, Harbin 150080, China; Department of computer science Mudanjiang Teachers College, Mudanjiang 157012, China","2010 2nd IEEE International Conference on Information Management and Engineering","","2010","","","27","30","Precise forecasting for shareprice is very important to investment and financing. Support vector regression, called as SVR, is a novel learning algorithm based on statistical learning theory, which has greater generalization ability than traditional neural networks. In order to select the appropriate parameters of SVR, particle swarm optimization is introduced to choose the user-determined parameters of SVR here. Therefore, data mining technology on particle swarm optimization and support vector regression is presented to shareprice prediction. Closingprice of 23 trading days of routon electronic is applied to testify the feasibility of the proposed method in the shareprice forecasting. The experiment results demonstrate that the proposed algorithm is better than the traditional shareprice forecasting algorithm.","","978-1-4244-5263-7978-1-4244-5265","10.1109/ICIME.2010.5477761","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477761","data mining;particle swarm optimization;support vector regression;shareprice prediction","Data mining;Particle swarm optimization;Investments;Statistical learning;Neural networks;Electronic equipment testing;Kernel;Birds;Application software;Computer science","data mining;financial data processing;forecasting theory;generalisation (artificial intelligence);investment;particle swarm optimisation;regression analysis;share prices;support vector machines","data mining technology;particle swarm optimization;support vector regression;shareprice prediction;shareprice forecasting;investment;statistical learning theory;generalization ability;routon electronic","","","5","","","","","","IEEE","IEEE Conferences"
"The Human Competitiveness of Search Based Software Engineering","J. T. de Souza; C. L. Maia; F. G. de Freitas; D. P. Coutinho","NA; NA; NA; NA","2nd International Symposium on Search Based Software Engineering","","2010","","","143","152","This paper reports a comprehensive experimental study regarding the human competitiveness of search based software engineering (SBSE). The experiments were performed over four well-known SBSE problem formulations: next release problem, multi-objective next release problem, workgroup formation problem and the multi-objective test case selection problem. For each of these problems, two instances, with increasing sizes, were synthetically generated and solved by both metaheuristics and human subjects. A total of 63 professional software engineers participated in the experiment by solving some or all problem instances, producing together 128 responses. The comparison analysis strongly suggests that the results generated by search based software engineering can be said to be human competitive.","","978-1-4244-8341","10.1109/SSBSE.2010.25","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635158","human competitiveness;search based software engineering;SBSE;human subjects","Humans;Software engineering;Search problems;Simulated annealing;Software;Gallium;Software algorithms","human factors;optimisation;software engineering","human competitiveness;search based software engineering;SBSE problem formulation;multiobjective next release problem;workgroup formation problem;multiobjective test case selection;metaheuristics","","15","53","","","","","","IEEE","IEEE Conferences"
"An Optimization Strategy for Evolutionary Testing Based on Cataclysm","M. Wang; B. Li; Z. Wang; X. Xie","NA; NA; NA; NA","2010 IEEE 34th Annual Computer Software and Applications Conference Workshops","","2010","","","359","364","Evolutionary Testing (ET) is an effective test case generation technique which uses some meta-heuristic search algorithm, especially genetic algorithm, to generate test cases automatically. However, the prematurity of the population may decrease the performance of ET. To solve this problem, this paper presents a novel optimization strategy based on cataclysm. It monitors the diversity of population during the evolution process of ET. Once the prematurity is detected, it will use the operator, cataclysm, to recover the diversity of the population. The experimental results show that the proposed strategy can improve the performance of ET evidently.","","978-1-4244-8089-0978-0-7695-4105","10.1109/COMPSACW.2010.69","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5614562","Evolutionary Testing;premature;cataclysm;diversity measure","Testing;Thigh;Optimization;Gallium;Monitoring;Genetic algorithms;Genetics","genetic algorithms;program testing;search problems","optimization strategy;evolutionary testing;cataclysm;metaheuristic search algorithm;genetic algorithm","","1","12","","","","","","IEEE","IEEE Conferences"
"Feedback loop mechanisms based particle swarm optimization with neighborhood topology","J. Zhang; S. Chen; D. Levy; Y. Lu","School of Electrical and Information Engineering, University of Sydney, Australia; Information Engineering Laboratory, CSIRO ICT Centre, Australia; School of Electrical and Information Engineering, University of Sydney, Australia; School of Software Engineering, Huazhong University of Science and Technology, China","2011 IEEE Congress of Evolutionary Computation (CEC)","","2011","","","1864","1871","Particle swarm optimization (PSO) is an optimization approach and has been widely used for a verity of optimization problem in both research and industrial domains. Due to the potential of PSO, several variants of the original PSO algorithms have been developed to improve PSO's efficiency and robustness. This paper proposes another variant of particle swarm optimization algorithm, called N-PωSO. This N-PωSO algorithm is based on classical feedback control theory and topological neighborhood, which offers better search efficiency and convergence stability. As a result, our N-PuωSO method features faster searching from the proportional term without steady-state error. And empirical results show that our N-PωSO algorithm is able to achieve high performance for both unimodal and multimodal optimization problems.","1941-0026;1089-778X","978-1-4244-7835-4978-1-4244-7834-7978-1-4244-7833","10.1109/CEC.2011.5949842","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5949842","Particle swarm optimization;evolutionary computing;proportional-integral-derivative (PID) controller;global optimization;control theory","Optimization;Convergence;Particle swarm optimization;Mathematical model;Benchmark testing;Acceleration;Equations","feedback;particle swarm optimisation","feedback loop mechanisms;neighborhood topology;particle swarm optimization algorithm;N-PωSO algorithm;classical feedback control theory;unimodal optimization problems;multimodal optimization problems","","3","22","","","","","","IEEE","IEEE Conferences"
"Benchmark-Based Aggregation of Metrics to Ratings","T. L. Alves; J. P. Correia; J. Visser","NA; NA; NA","2011 Joint Conference of the 21st International Workshop on Software Measurement and the 6th International Conference on Software Process and Product Measurement","","2011","","","20","29","Software metrics have been proposed as instruments, not only to guide individual developers in their coding tasks, but also to obtain high-level quality indicators for entire software systems. Such system-level indicators are intended to enable meaningful comparisons among systems or to serve as triggers for a deeper analysis.Common methods for aggregation range from simple mathematical operations (e.g. addition and central tendency) to more complex methodologies such as distribution fitting, wealth inequality metrics (e.g. Gini coefficient and Theil Index) and custom formulae.However, these methodologies provide little guidance for interpreting the aggregated results or to trace back to individual measurements.To resolve such limitations, a two-stage rating approach has been proposed where (i) measurement values are compared to thresholds to summarize them into risk profiles, and (ii) risk profiles are mapped to ratings.In this paper, we extend our approach for deriving metric thresholds from benchmark data into a methodology for benchmark-based calibration of two-stage aggregation of metrics into ratings.We explain the core algorithm of the methodology and we demonstrate its application to various metrics of the SIG quality model, using a benchmark of 100 software systems.We present an evaluation of the sensitivity of the algorithm to the underlying data.","","978-1-4577-1930","10.1109/IWSM-MENSURA.2011.15","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113040","Metrics aggregation;Benchmark;Software quality;Complexity","Benchmark testing;Indexes;Partitioning algorithms;Calibration;Software;Optimization","software metrics;software quality","benchmark-based aggregation;software metrics;coding tasks;high-level quality indicators;software systems;system-level indicators;mathematical operations;distribution fitting;wealth inequality metrics;custom formulae;two-stage rating approach;risk profiles;SIG quality model","","17","19","","","","","","IEEE","IEEE Conferences"
"Notice of Retraction<br>Study on optimization of ore pillar structure parameters of mining environment based on 3DMINE and MIDAS/GTS","Huanyu Deng; Keping Zhou; Qingfa Chen; Lili Wang","College of resources and security engineering; central south university, Changsha,china 410083; College of resources and security engineering; central south university, Changsha,china 410083; College of resources and security engineering; central south university, Changsha,china 410083; College of resources and security engineering; central south university, Changsha,china 410083","2010 2nd International Conference on Computer Engineering and Technology","","2010","1","","V1-177","V1-181","Accurate three-dimensional geological and numerical computing model were established by the mining engineering software 3DMINE and finite element analysis code MIDAS/GTS, and the excavation process was simulated completely according to the pre-designed mining sequence of panel stope. The results showed that the stability of stope structure with the size increasing of ore pillar of reconstructed mining environment had some rules as follow: 1) The maximum value of tensile stress and vertical displacement of roof structure decreased gradually, and stress conditions had been improved. 2) Most part of the four size ore pillars's von mises equivalent stress value was less than the ore-rock uniaxial compressive strength. 3) When the size changed within a certain range, it was not too much significance to reduce the tensile stress area and vertical displacement of roof by increasing its size. Thus, as for ore pillar, from the perspective of mechanics, there is a reasonable size range in which stability of the re-constructed mining environment is in an ideal state.;Notice of Retraction<br><br>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<br><br>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<br><br>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<br><br>","","978-1-4244-6349-7978-1-4244-6347-3978-1-4244-6348","10.1109/ICCET.2010.5486223","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5486223","Optimization;Numerical computing;Ore pillar;MIDAS/GTS;Mining environment reconstruction","Ores;Tensile stress;Stability;Numerical simulation;Geology;Finite element methods;Compressive stress;Application software;Filling;Testing","compressive strength;finite element analysis;mathematics computing;mining industry;optimisation;structural engineering computing;tensile strength","ore pillar structure parameter optimization;3DMINE software;3D geological and numerical computing model;mining engineering software;finite element analysis code MIDAS/GTS;excavation process;stope structure stability;mining environment reconstruction;tensile stress;roof structure vertical displacement;ore-rock uniaxial compressive strength","","","10","","","","","","IEEE","IEEE Conferences"
"Development of the “sandwich” guardrail based on crash simulation and test","Z. Bai; Y. Liu; G. Zhang","State Key Laboratory of Advanced Design and Manufacturing for Vehicle Body, Hunan University, Changsha, China; State Key Laboratory of Advanced Design and Manufacturing for Vehicle Body, Hunan University, Changsha, China; State Key Laboratory of Advanced Design and Manufacturing for Vehicle Body, Hunan University, Changsha, China","2011 International Conference on Electric Information and Control Engineering","","2011","","","4938","4941","Synthetic plastic materials were used in the new guardrail considering the shortcoming of the current highway guardrail such as high cost, easy to corrosion and so on. In addition the ""sandwich"" structure was used to enhance the crashworthiness of the new guardrail. A finite element simulation crash model of highway buses and semi-rigid guardrail was established in Hypermesh and Ls-dyna software. After a number of simulation experiments and structural optimization, the new highway semi-rigid guardrail was proven to meet the regulatory requirements in the real vehicle crash test.","","978-1-4244-8039-5978-1-4244-8036-4978-1-4244-8038","10.1109/ICEICE.2011.5777161","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5777161","FE model of bus-guardrail;synthetic plastic;composite sandwich structure;guardrail protection","Vehicle crash testing;Vehicles;Road transportation;Manufacturing;Laboratories;Analytical models;Safety","finite element analysis;impact testing;mechanical engineering computing;plastics;rails;road vehicles;vehicle dynamics","sandwich guardrail;crash simulation;crash test;synthetic plastic materials;highway guardrail;finite element simulation crash model;highway buses;semirigid guardrail;Hypermesh software;Ls-dyna software;structural optimization;vehicle crash test","","","","","","","","","IEEE","IEEE Conferences"
"Toward optimized code generation through model-based optimization","A. Charfi; C. Mraidha; S. Gérard; F. Terrier; P. Boulet","CEA LIST, Laboratory of model driven engineering for embedded systems, Point Courrier 94, F-91191, Gif sur Yvette, France; CEA LIST, Laboratory of model driven engineering for embedded systems, Point Courrier 94, F-91191, Gif sur Yvette, France; CEA LIST, Laboratory of model driven engineering for embedded systems, Point Courrier 94, F-91191, Gif sur Yvette, France; CEA LIST, Laboratory of model driven engineering for embedded systems, Point Courrier 94, F-91191, Gif sur Yvette, France; Université Lille 1, Sciences et Technologies, cité scientifique, 59655, Villeneuve d'Ascq Cedex, France","2010 Design, Automation & Test in Europe Conference & Exhibition (DATE 2010)","","2010","","","1313","1316","Model-Based Development (MBD) provides an additional level of abstraction, the model, which lets engineers focus on the business aspect of the developed system. MBD permits automatic treatments of these models with dedicated tools like synthesis of system's application by automatic code generation. Real-Time and Embedded Systems (RTES) are often constrained by their environment and/or the resources they own in terms of memory, energy consumption with respect to performance requirements. Hence, an important problem to deal with in RTES development is linked to the optimization of their software part. Although automatic code generation and the use of optimizing compilers bring some answers to application optimization issue, we will show in this paper that optimization results may be enhanced by adding a new level of optimizations in the modeling process. Our arguments are illustrated with examples of the Unified Modeling Language (UML) state machines diagrams which are widely used for control aspect modeling of RTES. The well-known Gnu Compiler Collection (GCC) is used for this study. The paper concludes on a proposal of two step optimization approach that allows reusing as they are, existing compiler optimizations.","1558-1101;1530-1591;1530-1591","978-3-9810801-6-2978-1-4244-7054-9978-3-9810801-6","10.1109/DATE.2010.5457010","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5457010","","Optimizing compilers;Unified modeling language;Embedded system;Real time systems;Energy consumption;Application software;Buildings;Laboratories;Model driven engineering;Proposals","optimising compilers;real-time systems;Unified Modeling Language","optimized code generation;model-based optimization;model-based development;business aspect;automatic treatments;automatic code generation;real-time systems;embedded systems;optimizing compilers;modeling process;Unified Modeling Language;state machines diagrams;control aspect modeling;Gnu compiler collection;compiler optimizations","","4","19","","","","","","IEEE","IEEE Conferences"
"The design of test case based on combinatorial and orthogonal experiment","Pu Yunming","School of computer engineering, Jimei university, xiamen, china","The 2nd International Conference on Information Science and Engineering","","2010","","","2440","2443","The combinatorial testing technology and the orthogonal experiment method were studied, optimizing of the design of test case, decreasing the workload of test case. The method of test case was designed with AETG algorithm. A load testing solution was suggested with orthogonal theory and Range analysis method. The load testing factor of system response time was analyzed, including network bandwidth, processor speed, concurrency users etc, the optimization performance criteria of system and testing combinatorial factors were selected.","2160-1283;2160-1291","978-1-4244-7618-3978-1-4244-7616-9978-1-4244-7617","10.1109/ICISE.2010.5690660","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5690660","Combinatorial testing;Load testing;Performance criteria;Test case;Orthogonal experiment","Software;Software testing;Time factors;Heuristic algorithms;Concurrent computing;Optimization","","","","","9","","","","","","IEEE","IEEE Conferences"
"Ant Colony Optimization based fixed order controller design and system simulations international conference on control, automation and systems (ICCAS 2011)","F. Mahar; S. A. Ali; A. Hussain; Z. Bhutto","Department of Electronics Engineering, Iqra University, Karachi, Pakistan; Department of Electronics Engineering, Iqra University, Karachi, Pakistan; Department of Electrical Engineering & CS, Hanyan University, Asnan, South Korea; Department of Software Engineering, Mehran UET, Jamshoro, Pakistan","2011 11th International Conference on Control, Automation and Systems","","2011","","","1369","1372","Thus far, researchers test several heuristic in order to produce the optimal solutions of the cost function and controller parameters. In this paper, an implementation of the Ant Colony Optimization (ACO) algorithm for the optimization of cost function and controller parameters has been discussed. To implement the ACO algorithm with a problem requires defining; cost function and the constraint. All simulation has been performed using a software program developed in the MATLAB environment. The simulation results show that ACO algorithm can be used to counterbalance the effect and improve the performance of any control system. Moreover, the proposed scheme overcomes the weaknesses of conventional fixed gain controller and improvement is accomplished in terms of settling time, oscillations and overshoot.","2093-7121","978-89-93215-03-8978-1-4577-0835","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6106139","Controller;optimization;parameters and cost function","Cost function;Control systems;Algorithm design and analysis;Ant colony optimization;Simulation;Robustness","ant colony optimisation;control system synthesis;controllers;H∞control;robust control","ant colony optimization;fixed order controller design;cost function;controller parameter;fixed gain controller","","","8","","","","","","IEEE","IEEE Conferences"
"A System Analysis Study Comparing Reverse Engineered Combinatorial Testing to Expert Judgment","A. M. Cunningham Jr.; J. Hagar; R. J. Holman","NA; NA; NA","2012 IEEE Fifth International Conference on Software Testing, Verification and Validation","","2012","","","630","635","The Lockheed Martin F-16 ventral fin redesign-combinatorial test study effort was to demonstrate how Combinatorial Testing (CT) could have been applied to system hardware design flaw analysis. The historic analysis was able to determine a set of combinations, which isolated the problem and tested a solution. However, the original effort was expensive, time consuming, and required highly specialized knowledge from the expert to be effective. The new study was an effort to understand if combinatorial test could be applied to similar situations using the original data but conducted by a less senior person without an expert's knowledgebase. The situation and CT approaches are detailed in this paper. In the study, a series of iterations created combinatorial test cases which could have ""replicated"" the original highly optimized and successful test program, without the expert.","2159-4848","978-0-7695-4670-4978-1-4577-1906","10.1109/ICST.2012.151","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200163","Case-Study;Combinatorial Test;Design problem","Testing;Aircraft;Aerodynamics;Industries;NIST;Analytical models;Fatigue","aerospace components;aerospace computing;failure analysis;program testing;reverse engineering","reverse engineered combinatorial testing;expert judgment;system analysis study;Lockheed Martin F-16 ventral fin redesign combinatorial test study;system hardware design flaw analysis;CT approaches;test program","","3","7","","","","","","IEEE","IEEE Conferences"
"Influences of different excitation parameters upon PEC testing for deep-layered defect detection with rectangular sensor","X. Hu; F. Luo","College of Mechatronics and Automation, National University of Defense Technology, Changsha, China; College of Mechatronics and Automation, National University of Defense Technology, Changsha, China","2010 International Conference on Computer, Mechatronics, Control and Electronic Engineering","","2010","3","","579","582","In pulsed eddy current testing, repetitive excitation signals with different parameters: duty-cycle, frequency and amplitude have different response representations. This work studies the influences of different excitation parameters on pulsed eddy current testing for deep-layered defects detection of stratified samples with rectangular sensor. The sensor had been proved to be superior in quantification and classification of defects in multi-layered structures compared with traditional circular ones. Experimental results show necessities to optimize the parameters of pulsed excitation signal, and advantages of obtaining better performances to enhance the POD of PEC testing.","2159-6026;2159-6034","978-1-4244-7958-0978-1-4244-7957-3978-1-4244-7955","10.1109/CMCE.2010.5610253","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5610253","pulsed eddy current testing;rectangular sensor;Influences of different excitation parameters;defect detection;multi-layer structur","Testing;Software","aerospace materials;crack detection;eddy current testing;inspection;multilayers;probability","excitation parameters;PEC testing;deep layered defect detection;rectangular sensor;pulsed eddy current testing;repetitive pulsed excitation signals;duty cycle;defect quantification;defect classification;multilayered structures","","","9","","","","","","IEEE","IEEE Conferences"
"Processing Parameter Optimization for Injection Moulding Products in Agricultural Equipment Based on Orthogonal Experiments and Analysis","Yanwei; Huyong","NA; NA","2011 International Conference on Computer Distributed Control and Intelligent Environmental Monitoring","","2011","","","560","564","There are many quality-related factors in the manufacture of injection molding products of agriculture equipment. It is a trouble to find out the key factors and to optimize the process plan with these key factors. This study introduces a practice way: to design the factors test based on orthogonal experiment, and to analog test with software Moldflow MPI 6.0, then to handles the data with range and variance analysis theory, finally turn out the optimization process plan.","","978-1-61284-278-3978-0-7695-4350","10.1109/CDCIEM.2011.396","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5747880","injection molding;orthogonal experiment;processing parameter optimization","Injection molding;Plastics;Temperature;Optimization;Design automation;Software;Emulation","agricultural machinery;CAD/CAM;injection moulding;machinery production industries;process planning;production engineering computing;statistical analysis","processing parameter optimization;injection moulding products;agricultural equipment;quality-related factors;process plan;orthogonal experiment;analog test;Moldflow MPI 6.0 software;variance analysis theory;range analysis theory;CAD/CAE","","","","","","","","","IEEE","IEEE Conferences"
"Testing is an Event-Centric Activity","F. Belli; M. Beyazit; A. Memon","NA; NA; NA","2012 IEEE Sixth International Conference on Software Security and Reliability Companion","","2012","","","198","206","Recent advances in techniques for testing graphical user interfaces (GUIs) enabled to develop workflow models and successfully employ them to generate large numbers of test cases by defining new test adequacy criteria and optimizing test suites for increasing the test efficiency. The key to the success of these event-focused techniques, especially event flow graphs and event sequence graphs, is that they primarily focus on the input space, and model the workflow in simple terms. If necessary, they can also be augmented to model more complex systems and processes to adapt to the needs of test engineers. We now posit that we can extend these techniques to also domains other than GUIs to create a general event-driven paradigm for testing.","","978-1-4673-2670-4978-0-7695-4743","10.1109/SERE-C.2012.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6258472","modeling;testing;event;state;event-centric;directed graph","Testing;Unified modeling language;Semantics;Mathematical model;Syntactics;Automata;Graphical user interfaces","graph theory;graphical user interfaces;testing","event-centric activity testing;graphical user interfaces testing;GUI testing;workflow models;test adequacy criteria;test suites optimization;test efficiency;event-focused techniques;event flow graphs;event sequence graphs;complex systems;test engineers;event-driven paradigm","","5","27","","","","","","IEEE","IEEE Conferences"
"Injection Mold Design Based on Plastic Advisor Analysis Software in Pro/E","Z. Chun-ying; W. Li-tao","NA; NA","2011 International Conference on Multimedia and Signal Processing","","2011","1","","205","208","Plastic Advisor analysis method is used to optimize the injection position and configure the process parameters rationally so that the weld mark of bigger wall thickness of PP-R plumbing can be minimized. The evaluation and optimization method with Plastic Advisor analysis method in design scheme of injection mold runner provides a reliable technical method for improving the quality of injection mold.","","978-1-61284-314-8978-1-61284-314","10.1109/CMSP.2011.49","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5957409","njection mold;Plastic Advisor;Weld mark","Welding;Plastics;Logic gates;Optimization;Software;Testing;Analytical models","design;injection moulding;optimisation;production engineering computing","injection mold design;plastic advisor analysis software;Pro/E;injection position;weld mark;wall thickness;PP-R plumbing;optimization method","","","5","","","","","","IEEE","IEEE Conferences"
"Input-input relationship constraints in T-way testing","R. R. Othman; K. Z. Zamli","School of Computer and Communication, Universiti Malaysia Perlis (UniMAP), PO Box 77, d/a Pejabat Pos Besar, 01007 Kangar, Malaysia; School of Electrical Engineering, Universiti Sains Malaysia, Engineering Campus, Nibong Tebal, 14300 Penang, Malaysia","2011 IEEE Symposium on Industrial Electronics and Applications","","2011","","","527","531","T-way testing is designed to detect faults due to interaction. In order to be effective, all t combinations of input parameters must be tested. While many t-way strategies can be used to generate the t-way test data (e.g. IPOG, AETG, GT-Way, Jenny, TVG and MIPOG), most do not ensure that all t combinations of input parameters can be practically tested. Addressing this issue, this paper highlights a new type of constraints that might prevent some t-way parameter interactions from being tested (and hence compromising the effectiveness of t-way testing), termed input-input relationship constraints. Apart from ensuring all t combinations are properly tested, input-input relationship constraints can further optimize the generated test data since all impossible combinations are completely ignored. In addition, this paper also introduces a new strategy that supports input-input relationship constraints and demonstrates the correctness of the strategy as well as the effectiveness of test data with input-input relationship.","","978-1-4577-1417-7978-1-4577-1418-4978-1-4577-1416","10.1109/ISIEA.2011.6108767","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6108767","interaction testing;t-way testing;input-input relationship;combinatorial testing","Software testing;Educational institutions;Software;Generators;Hardware","automatic testing","input-input relationship constraints;t-way testing;generated test data","","1","20","","","","","","IEEE","IEEE Conferences"
"Measuring Code Quality to Improve Specification Mining","C. Le Goues; W. Weimer","University of Virginia, Charlottesville; University of Virginia, Charlottesville","IEEE Transactions on Software Engineering","","2012","38","1","175","190","Formal specifications can help with program testing, optimization, refactoring, documentation, and, most importantly, debugging and repair. However, they are difficult to write manually, and automatic mining techniques suffer from 90-99 percent false positive rates. To address this problem, we propose to augment a temporal-property miner by incorporating code quality metrics. We measure code quality by extracting additional information from the software engineering process and using information from code that is more likely to be correct, as well as code that is less likely to be correct. When used as a preprocessing step for an existing specification miner, our technique identifies which input is most indicative of correct program behavior, which allows off-the-shelf techniques to learn the same number of specifications using only 45 percent of their original input. As a novel inference technique, our approach has few false positives in practice (63 percent when balancing precision and recall, 3 percent when focused on precision), while still finding useful specifications (e.g., those that find many bugs) on over 1.5 million lines of code.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.5","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5680914","Specification mining;machine learning;software engineering;code metrics;program understanding.","Software measurement;Refactoring;Data mining;Maintenance engineering;Cloning;Optimization","data mining;formal specification;program debugging;program testing;software quality","specification mining;formal specifications;program testing;optimization;refactoring;documentation;debugging;repair;automatic mining techniques;temporal-property miner;code quality metrics;software engineering process","","11","61","","","","","","IEEE","IEEE Journals & Magazines"
"An inner convex approximation algorithm for BMI optimization and applications in control","Q. Tran Dinh; W. Michiels; S. Gros; M. Diehl","Department of Electrical Engineering (ESAT/SCD) and Optimization in Engineering Center (OPTEC), Katholieke Universiteit Leuven, Belgium; Department of Computer Science and Optimization in Engineering Center (OPTEC), KU Leuven, Belgium; Department of Electrical Engineering (ESAT/SCD) and Optimization in Engineering Center (OPTEC), Katholieke Universiteit Leuven, Belgium; Department of Electrical Engineering (ESAT/SCD) and Optimization in Engineering Center (OPTEC), Katholieke Universiteit Leuven, Belgium","2012 IEEE 51st IEEE Conference on Decision and Control (CDC)","","2012","","","3576","3581","In this work, we propose a new local optimization method to solve a class of nonconvex semidefinite programming (SDP) problems. The basic idea is to approximate the feasible set of the nonconvex SDP problem by inner positive semidefinite convex approximations via a parameterization technique. This leads to an iterative procedure to search a local optimum of the nonconvex problem. The convergence of the algorithm is analyzed under mild assumptions. Applications to optimization problems with bilinear matrix inequality (BMI) constraints in static output feedback control are benchmarked and numerical tests are implemented based on the data from the COMPL<sub>e</sub>ib library.","0191-2216;0743-1546;0743-1546","978-1-4673-2066-5978-1-4673-2065-8978-1-4673-2063-4978-1-4673-2064","10.1109/CDC.2012.6427102","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6427102","","Optimization;Approximation methods;Approximation algorithms;Programming;Symmetric matrices;Output feedback;Software algorithms","approximation theory;concave programming;control system synthesis;convergence;feedback;iterative methods;linear matrix inequalities","inner convex approximation algorithm;BMI optimization;local optimization method;nonconvex semidefinite programming problems;nonconvex SDP problem;inner positive semidefinite convex approximations;parameterization technique;iterative procedure;bilinear matrix inequality constraints;static output feedback control design;numerical tests;COMPLeib library","","2","21","","","","","","IEEE","IEEE Conferences"
"Fabrication and testing of convex conic aspheric surface","B. Wang; M. Li; R. Lian; Q. Zhu","Changchun University of Science and Technology, Changchun, China; Changchun University of Science and Technology, Changchun, China; Changchun University of Science and Technology, Changchun, China; Changchun Keyu precision optical, Technology Development Company, Changchun, China","2012 International Conference on Optoelectronics and Microelectronics","","2012","","","448","450","Based on the third-order aberration theory and paraxial formula, the null test theory is derived. And after using the ZEMAX software to optimize the optical path, a more simple and convenience testing technology is discussed with a diameter of 60 mm convex paraboloid lens. Compared with traditional testing method, it doesn't need expensive testing equipment. The practice shows that this method not only promotes work efficiency but also reduces the cost of test, at the same time, the PV of parabolic surface of the lens reaches λ/5 after fulfillment.","","978-1-4673-2639-1978-1-4673-2638-4978-1-4673-2637","10.1109/ICoOM.2012.6316312","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6316312","geometrical optics;null testing;convex aspheric surface;hird-order aberration theory","Testing;Optical reflection;Integrated optics;Surface treatment;Optical imaging;Accuracy;Optical sensors","aberrations;aspherical optics;lenses;optical fabrication;optical testing","convex conic aspheric surface;third-order aberration theory;paraxial formula;null test theory;ZEMAX software;optical path;convex paraboloid lens;parabolic surface;size 60 mm","","","6","","","","","","IEEE","IEEE Conferences"
"Optimization Algorithm for Temperature Field Distribution of Rat Brain Tissue Undergoing Radiofrequency Ablation Treatment","L. Qian; G. Hu; Z. Qian; W. Li; J. Xie","NA; NA; NA; NA; NA","2011 First International Workshop on Complexity and Data Mining","","2011","","","108","111","This paper established an optimized algorithm for temperature field distribution of rat brain tissue undergoing radio frequency ablation (RFA) treatment. Firstly, adopting the Pennes bio-heat transfer equation, obtained the temperature distribution of rat brain tissue with different RFA temperature and duration by using the finite element analysis software FEMLAB (COMSOL). Then, obtained the optimized algorithm of temperature field distribution of biological tissue undergoing RFA treatment by extracting and analyzing the temperatures at different positions using MATLAB6.5 and the fitting software 1stOpt. Finally, model validation was carried out with FEMLAB simulations. Results show that the optimized algorithm of temperature field distribution established in this paper is feasible. According to the optimized algorithm, it is convenient, fast, accurate and real-time to obtain the instantaneous temperature at any position of the rat with any RFA temperature and duration.","","978-1-4577-2007-9978-0-7695-4585","10.1109/IWCDM.2011.32","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6128443","optimized algorithm;temperature field distribution;radiofrequency ablation;finite element;pennes equation","Temperature distribution;Mathematical model;Equations;Biological system modeling;Biological tissues;Solid modeling;Testing","biothermics;brain;electromagnetic waves;finite element analysis;medical computing;optimisation;radiofrequency heating;temperature distribution","optimization algorithm;temperature field distribution;radiofrequency ablation treatment;rat brain tissue;Pennes bioheat transfer equation;finite element analysis software;FEMLAB;COMSOL;MATLAB6.5;fitting software;1stOpt software","","","16","","","","","","IEEE","IEEE Conferences"
"Refactoring as Testability Transformation","M. Harman","NA","2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops","","2011","","","414","421","This paper briefly reviews the theory of Testability Transformation and outlines its implications for and relationship to refactoring for testing. The paper introduces testability refactorings, a subclass of Testability Transformations and discusses possible examples of testability refactorings. Several approaches to testability refactoring are also introduced. These include the novel concept of test-carrying code and the use of pareto optimization for balancing the competing needs of machine and human in search based testability refactoring.","","978-0-7695-4345-1978-1-4577-0019","10.1109/ICSTW.2011.38","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954441","refactoring;testing;testability;testability transformation;transformation","Testing;Humans;Semantics;Measurement;Programming;Search problems;Documentation","Pareto optimisation;program testing;search problems;software maintenance","testability transformation;search based testability refactoring;test-carrying code;Pareto optimization","","9","83","","","","","","IEEE","IEEE Conferences"
"Parameters optimization for the thread of crank-slider-CST type low rear protective device of truck: Parameters optimization for thread of crank-slider-CST protective device","S. Wei; Z. Lei; M. Lei; Y. Li","Engineering Research Center of Catastrophic Prophylaxis, and Treatment of Road and Traffic Safety (Changsha, University of Science and Technology), Ministry of Education, Changsha, P.R. China; Key Laboratory of Highway Engineering (Changsha, University of Science and Technology), Ministry of Education, Changsha, P.R. China; School of Civil Engineering and Architecture, Changsha University of Science and Technology, Changsha, P.R. China; Hunan Province Communications Planning, Survey &amp; Design Institute, Changsha, P.R. China","2011 International Conference on Consumer Electronics, Communications and Networks (CECNet)","","2011","","","527","530","To make the crank slider protective devices to meet the requirements of regulation, optimization was carried out to the absorbing thread of the crank-slider-CST protective device. Based on the finite element model in VPG and simulation in LS-DYNA, with the orthogonal optimization design, obtained the parameters of the absorbing thread. The crash speed in these simulations is 50km/h, which is higher than the regulatory speed. The simulation showed that all indexes could meet the regulation very well, and the effectiveness of the device was demonstrated.","","978-1-61284-459-6978-1-61284-458-9978-1-61284-457","10.1109/CECNET.2011.5768948","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5768948","Crank-slider;Collision;Crash compatibility;CST;Orthogonal design","Optimization;Indexes;Numerical models;Safety;Vehicle crash testing;Absorption;Software","finite element analysis;optimisation;road safety;road vehicles","parameters optimization;crank-slider-CST type low rear protective device;truck;crank-slider-CST protective device;absorbing thread;finite element model;VPG;LS-DYNA;orthogonal optimization design;crash speed","","1","9","","","","","","IEEE","IEEE Conferences"
"Dynamic memoization for energy efficiency in financial applications","G. Agosta; M. Bessi; E. Capra; C. Francalanci","Dipartimento di Elettronica e Informazione, Politecnico di Milano, Italia; Dipartimento di Elettronica e Informazione, Politecnico di Milano, Italia; Dipartimento di Elettronica e Informazione, Politecnico di Milano, Italia; Dipartimento di Elettronica e Informazione, Politecnico di Milano, Italia","2011 International Green Computing Conference and Workshops","","2011","","","1","8","Software applications directly impact on IT energy consumptions as they indirectly guide hardware operations. Optimizing algorithms has a direct beneficial impact on energy efficiency, but it requires domain knowledge and an accurate analysis of the code, which may be infeasible and too costly to perform for large code bases. In this paper we present an approach based on dynamic memoization to increase software energy efficiency. This implies to identify a subset of pure functions that can be tabulated and to automatically store the results corresponding to the most frequent invocations. We implemented a prototype software system to apply memoization and tested it on a set of financial functions. Empirical results show average energy savings of 74% and time performance savings of 79%.","","978-1-4577-1221-0978-1-4577-1222-7978-1-4577-1220","10.1109/IGCC.2011.6008559","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6008559","green software;memoization;computation intensive application;pure function","Benchmark testing;Java;Software;Hardware;Energy consumption;Instruments;Servers","energy conservation;energy consumption;financial data processing;information technology;optimising compilers;power aware computing;program testing;software prototyping","dynamic memoization;financial application;IT energy consumption;software optimisation;software energy efficiency;prototype software system;financial function set;software testing;energy savings","","","13","","","","","","IEEE","IEEE Conferences"
"Performance analysis of comercial simulation-based optimization packages: OptQuest and Witness Optimizer","H. Eskandari; E. Mahmoodi; H. Fallah; C. D. Geiger","Tarbiat Modares University, Advanced Simulation Lab, Dept. of Industrial Engineering, Tehran, 14117, IRAN; Tarbiat Modares University, Advanced Simulation Lab, Dept. of Industrial Engineering, Tehran, 14117, IRAN; Tarbiat Modares University, Advanced Simulation Lab, Dept. of Industrial Engineering, Tehran, 14117, IRAN; University of Central Florida, Dept. of Industrial Engineering and Management Systems, Orlando, 32816, USA","Proceedings of the 2011 Winter Simulation Conference (WSC)","","2011","","","2358","2368","The objective of this study is to evaluate and compare two commercial simulation-based optimization packages, OptQuest and Witness Optimizer, to determine their relative performance based on the quality of obtained solutions in a reasonable computational effort. Two well-known benchmark problems, the pull manufacturing system and the inventory system, are used to evaluate and compare the performance of OptQuest and Witness Optimizer. Significant validation efforts are made to ensure that simulation models developed in Arena and Witness are identical. The experimental results indicate that both optimization packages have good performance on the given problems. Both packages found near-global optimal (or satisfactory) solutions in an acceptable computation time.","1558-4305;0891-7736;0891-7736;0891-7736","978-1-4577-2109-0978-1-4577-2108-3978-1-4577-2106-9978-1-4577-2107","10.1109/WSC.2011.6147946","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6147946","","Optimization;Manufacturing systems;Algorithm design and analysis;Computational modeling;Software;Software algorithms;Search problems","benchmark testing;inventory management;manufacturing systems;optimisation","performance analysis;commercial simulation-based optimization package;OptQuest optimizer;Witness optimizer;benchmark problem;pull manufacturing system;inventory system;simulation model;near-global optimal solution;acceptable computation time","","7","12","","","","","","IEEE","IEEE Conferences"
"Code Hot Spot: A tool for extraction and analysis of code change history","W. Snipes; B. Robinson; E. Murphy-Hill","Industrial Software Systems, ABB Corporate Research, Raleigh, NC USA; Industrial Software Systems, ABB Corporate Research, Raleigh, NC USA; North Carolina State University, Department of Computer Science, Raleigh, USA","2011 27th IEEE International Conference on Software Maintenance (ICSM)","","2011","","","392","401","Commercial software development teams have limited time available to focus on improvements to their software. These teams need a way to quickly identify areas of the source code that would benefit from improvement, as well as quantifiable data to defend the selected improvements to management. Past research has shown that mining configuration management systems for change information can be useful in determining faulty areas of the code. We present a tool named Code Hot Spot, which mines change records out of Microsoft's TFS configuration management system and creates a report of hot spots. Hot spots are contiguous areas of the code that have higher values of metrics that are indicators of faulty code. We present a study where we use this tool to study projects at ABB to determine areas that need improvement. The resulting data have been used to prioritize areas for additional code reviews and unit testing, as well as identifying change prone areas in need of refactoring.","1063-6773;1063-6773","978-1-4577-0664-6978-1-4577-0663-9978-1-4577-0662","10.1109/ICSM.2011.6080806","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080806","Decision Support;Metrics;Refactoring;Verification;Quality;Configuration Management","Measurement;History;Complexity theory;Data mining;Correlation;Software;Couplings","data flow analysis;software maintenance;software management;software metrics;software tools","code change history extraction tool;code change history analysis tool;commercial software development teams;source code;software management;configuration management system mining;change information;Code Hot Spot;Microsoft TFS configuration management system;metrics;faulty code;ABB","","","18","","","","","","IEEE","IEEE Conferences"
"Diversity maximization speedup for fault localization","L. Gong; D. Lo; L. Jiang; H. Zhang","Tsinghua University, China; Singapore Management University, Singapore; Singapore Management University, Singapore; Tsinghua University, China","2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering","","2012","","","30","39","Fault localization is useful for reducing debugging effort. However, many fault localization techniques require non-trivial number of test cases with oracles, which can determine whether a program behaves correctly for every test input. Test oracle creation is expensive because it can take much manual labeling effort. Given a number of test cases to be executed, it is challenging to minimize the number of test cases requiring manual labeling and in the meantime achieve good fault localization accuracy. To address this challenge, this paper presents a novel test case selection strategy based on Diversity Maximization Speedup (DMS). DMS orders a set of unlabeled test cases in a way that maximizes the effectiveness of a fault localization technique. Developers are only expected to label a much smaller number of test cases along this ordering to achieve good fault localization results. Our experiments with more than 250 bugs from the Software-artifact Infrastructure Repository show (1) that DMS can help existing fault localization techniques to achieve comparable accuracy with on average 67% fewer labeled test cases than previously best test case prioritization techniques, and (2) that given a labeling budget (i.e., a fixed number of labeled test cases), DMS can help existing fault localization techniques reduce their debugging cost (in terms of the amount of code needed to be inspected to locate faults). We conduct hypothesis test and show that the saving of the debugging cost we achieve for the real C programs are statistically significant.","","978-1-4503-1204-2978-1-4503-1204","10.1145/2351676.2351682","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6494903","Fault Localization;Test Case Prioritization","","C language;program debugging;program testing","C programs;test case prioritization techniques;software-artifact infrastructure repository;DMS;test case selection strategy;manual labeling;test oracle creation;fault localization techniques;debugging effort reduction;diversity maximization speedup","","7","32","","","","","","IEEE","IEEE Conferences"
"Research of orthogonal genetic algorithms in probability simulation","S. Li; Y. Zhang; N. Gai; J. Yu","Department of Information Engineering, Environment Management College of China, EMCC, Qinhuangdao 066004, China; Department of Information Engineering, Environment Management College of China, EMCC, Qinhuangdao 066004, China; Department of Information Engineering, Environment Management College of China, EMCC, Qinhuangdao 066004, China; Department of Information Engineering, Environment Management College of China, EMCC, Qinhuangdao 066004, China","2011 IEEE 3rd International Conference on Communication Software and Networks","","2011","","","707","710","In order to solve the complex optimization problems of uncertain phenomena effectively, this paper gives probability simulation optimization method based on orthogonal genetic algorithm. This method integrates simulation technology, orthogonal genetic algorithm with statistical testing perfectly. So it can more effectively solve complex optimization problems. In theory, by adopting probability method the study in complex optimization problem with uncertain phenomenon has strong rationality and operability. This method has better optimal performance than the traditional methods.","","978-1-61284-486-2978-1-61284-485-5978-1-61284-484","10.1109/ICCSN.2011.6014366","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014366","orthogonal genetic algorithm;computer simulation;simulation optimization","Software algorithms;Software","genetic algorithms;probability;statistical testing;uncertain systems","orthogonal genetic algorithms;complex optimization problems;uncertain phenomena;probability simulation optimization method;simulation technology;statistical testing;probability method;uncertain phenomenon","","","11","","","","","","IEEE","IEEE Conferences"
"Chaotic-NSGA-II: An effective algorithm to solve multi-objective optimization problems","Danqing Guo; Junping Wang; Jun Huang; Renmin Han; Maoqiang Song","School of Software Engineering, Beijing Univ. of Posts &amp; Telecom., China; School of Software Engineering, Beijing Univ. of Posts &amp; Telecom., China; Network School of Information Center Software Engineering, Beijing Univ. of Posts &amp; Telecom., China; Network School of Information Center Software Engineering, Beijing Univ. of Posts &amp; Telecom., China; Network School of Information Center Software Engineering, Beijing Univ. of Posts &amp; Telecom., China","2010 International Conference on Intelligent Computing and Integrated Systems","","2010","","","20","23","This paper presents a new approach to handle multi-objective optimization problems (MOP) by incorporating logistic mapping function into the process of NSGA-II. NSGA-II is a well-known evolutionary algorithm for optimization, it is famous for its small computational complexity and simpleness, its ability to maintain a good spread of solutions makes it converge better in the obtained non-dominated front than PAES and SPEA. But it may lack of diversity, so we introduce chaos into this NSGA-II, aiming to add chaos to the solutions generated by the genetic process. Chaos optimization algorithm (COA) was proposed that can solve complex function optimization and has a high efficiency of calculation. Through the comparison of Chaotic-NSGA-II and NSGA-II on six test problems, we can see that this algorithm has a good performance on searching the global optimization.","","978-1-4244-6837-9978-1-4244-6834-8978-1-4244-6836","10.1109/ICISS.2010.5654998","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5654998","component;NSGA-II;chaotic;logistic map;multiple objective evolutionary algorithm;Pareto dominance","Educational institutions;Optimization","chaos;genetic algorithms;search problems","chaotic-NSGA-II;multiobjective optimization problem;logistic mapping function;evolutionary algorithm;computational complexity;nondominated front;genetic process;chaos optimization algorithm;complex function optimization","","2","7","","","","","","IEEE","IEEE Conferences"
"Differential clonal selection algorithm for solving constrained optimization problems","Y. Yang; H. Fang","College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China","Proceedings of 2011 International Conference on Electronic & Mechanical Engineering and Information Technology","","2011","9","","4894","4898","In this paper, a differential clonal selection algorithm (DCSA) combined with an adaptive penalty function method is proposed for solving constrained optimization problems. In order to improve the diversity of the solution, a minority part of the antibodies located in sparse region are selected to do proportional cloning according to their minimum neighbor distance values. Comparison is made to four state-of-the-art algorithms in solving eleven well-known standard test problems. Simulation results show that DCSA performs better or similarly than the four approaches.","","978-1-61284-088-8978-1-61284-087-1978-1-61284-086","10.1109/EMEIT.2011.6024060","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6024060","immune algorithm;clonal selection scheme;differential evolution;constrained optimization","Optimization;Cloning;Evolutionary computation;Immune system;Benchmark testing;Evolution (biology);Educational institutions","artificial immune systems","differential clonal selection algorithm;constrained optimization problems;adaptive penalty function method;minimum neighbor distance values","","","14","","","","","","IEEE","IEEE Conferences"
"Optimization design and parametric drawing of Amorphous Alloy Transformer","W. Xin; Z. Jinlong; H. Dexu; G. Xianjin","Shandong Electric Power Research Institute, Electric Power Robotic Technology Laboratory of State, Grid Corporation of China, Jinan, China; Shandong Electric Power Research Institute, Electric Power Robotic Technology Laboratory of State, Grid Corporation of China, Jinan, China; Shandong Electric Power Research Institute, Electric Power Robotic Technology Laboratory of State, Grid Corporation of China, Jinan, China; Shandong Electric Power Research Institute, Electric Power Robotic Technology Laboratory of State, Grid Corporation of China, Jinan, China","2012 2nd International Conference on Consumer Electronics, Communications and Networks (CECNet)","","2012","","","422","425","Amorphous Alloy Transformer CAD which includes the electromagnetic optimization design and parametric drawing, is researched and worked out in order to solve problems of high design cost and long design cycle. While this software system has an instantiate test, users speak highly of the software because it could reduce the designing cost of 5% ~ 10%, economize expenses of millions and save time from 10 days to 10 minutes.","","978-1-4577-1415-3978-1-4577-1414-6978-1-4577-1413","10.1109/CECNet.2012.6202169","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6202169","amorphous alloy transformer;the electromagnetic optimization design;the parametric drawing","Power transformers;Optimization;Metals;Transformer cores;Software;Design automation","CAD;optimisation;power transformers","parametric drawing;amorphous alloy transformer;CAD;electromagnetic optimization;high design cost;long design cycle","","2","9","","","","","","IEEE","IEEE Conferences"
"Code optimization for enhancing SystemC simulation time","H. Alemzadeh; S. Aminzadeh; R. Saberi; Z. Navabi","CAD Research Laboratory, Department of Electrical and Computer Engineering, School of Engineering, University of Tehran, Iran; CAD Research Laboratory, Department of Electrical and Computer Engineering, School of Engineering, University of Tehran, Iran; CAD Research Laboratory, Department of Electrical and Computer Engineering, School of Engineering, University of Tehran, Iran; CAD Research Laboratory, Department of Electrical and Computer Engineering, School of Engineering, University of Tehran, Iran","2010 East-West Design & Test Symposium (EWDTS)","","2010","","","431","434","The main contribution of this paper is suggesting a number of techniques to enhance SystemC simulation time. Simulation speed is very important, especially at the early stages of the system design. On the one hand, these techniques guide SystemC developers, and on the other, they can be used in automatic code translators. The experimental results show a significant improvement in the simulation time of SystemC codes.","","978-1-4244-9556-6978-1-4244-9555-9978-1-4244-9554","10.1109/EWDTS.2010.5742036","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5742036","","Optimization;Hardware design languages;Converters;Hardware;Switches;Solid modeling;Software","C language;hardware description languages;optimisation","code optimization;SystemC simulation time enhancement;system design;automatic code translator","","2","10","","","","","","IEEE","IEEE Conferences"
"Repulsive Particle Swarm Optimization based on new diversity","Guochao Niu; Baodi Chen; J. Zeng","Software Engineering Institute, Xidian University, Xi'an 710071, China; Software Engineering Institute, Xidian University, Xi'an 710071, China; Complex System and Computational Intelligence Laboratory, Taiyuan University of Science and Technology, 030024, China","2010 Chinese Control and Decision Conference","","2010","","","815","819","To avoid the problem of premature convergence, a new diversity-guided Particle Swarm Optimizer (PSO), namely MARPSO is proposed, which is a modification of attractive and repulsive PSO (ARPSO), suggested by Riget and Vesterstorm. A novel measure of population diversity function is presented and a new concept of the particle's best flight direction is introduced. The simulation test results of four classic functions show that: compared with Standard PSO (BPSO) and ARPSO, MARPSO can effectively increase the diversity of swarm, while maintain a higher convergence speed.","1948-9439;1948-9447","978-1-4244-5181-4978-1-4244-5182","10.1109/CCDC.2010.5498113","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5498113","Particle Swarm Optimization;Population Diversity;Global Search;Particle's Best Flight Direction","Particle swarm optimization;Convergence;Stochastic processes;Computational intelligence;Laboratories;Testing;Software engineering;Particle measurements;Genetic algorithms;Simulated annealing","particle swarm optimisation","repulsive particle swarm optimization;diversity-guided particle swarm optimizer;MARPSO;repulsive PSO;population diversity function;particle best flight direction","","","11","","","","","","IEEE","IEEE Conferences"
"Community optimization: Function optimization by a simulated web community","C. B. Veenhuis","Berlin University of Technology, Germany","2012 12th International Conference on Intelligent Systems Design and Applications (ISDA)","","2012","","","508","514","In recent years a number of web-technology supported communities of humans have been developed. Such a web community is able to let emerge a collective intelligence with a higher performance in solving problems than the single members of the community. Based on the successes of collective intelligence systems like Wikipedia, the web encyclopedia, the question arises, whether such a collaborative web community could also be capable of function optimization. This paper introduces an optimization algorithm called Community Optimization (CO), which optimizes a function by simulating a collaborative web community, which edits or improves an article-base, or, more general, a knowledge-base. In order to realize this, CO implements a behavioral model derived from the human behavior that can be observed within certain types of web communities (e.g., Wikipedia or open source communities). The introduced CO method is applied to four well-known benchmark problems. CO significantly outperformed the Fully Informed Particle Swarm Optimization as well as two Differential Evolution approaches in all four cases especially in higher dimensions.","2164-7143;2164-7143;2164-7151","978-1-4673-5119-5978-1-4673-5117-1978-1-4673-5118","10.1109/ISDA.2012.6416590","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6416590","Community Optimization;Collective Intelligence;Web Community;Behavioral Model;Knowledge Base","Communities;Knowledge based systems;Humans;Optimization;Benchmark testing;Encyclopedias;Birds","behavioural sciences;learning (artificial intelligence);optimisation;public domain software;Web sites","community optimization algorithm;Web-technology supported human community members;collective intelligence systems;Wikipedia;Web encyclopedia;function optimization;CO;collaborative Web community simulation;article-base editing;article-base improvement;knowledge-base editing;knowledge-base improvement;human behavioral model;open source communities;benchmark problems","","","11","","","","","","IEEE","IEEE Conferences"
"Development of a non-deterministic input-output based relationship test data set minimization strategy","O. H. Yeh; K. Z. Zamli","School of Electrical and Electronic Engineering, Universiti Sains Malaysia, Engineering Campus, 14300 Nibong Tebal, Penang, Malaysia; School of Electrical and Electronic Engineering, Universiti Sains Malaysia, Engineering Campus, 14300 Nibong Tebal, Penang, Malaysia","2011 IEEE Symposium on Computers & Informatics","","2011","","","800","805","Numerous efficient interaction testing strategies have been proposed in the past literatures to generate optimized test cases for software system under test (SUT). Meanwhile, most of the reported tools enumerate their test cases by covering all t-interactions of parameters involved, which are also known as uniform strength interaction testing. Later on, variable strength interaction testing has been proposed to allow certain subsets to cover higher t-interactions. Nevertheless, this consideration is still insufficient to generate test cases based on actual interactions. In fact, recent researches claimed that neither uniform nor variable strength interaction testing is capable to exactly cover actual factors interaction. Recently, interaction testing based on input-output relationship is reported to accommodate the actual interactions for SUT. This approach claimed that the interaction testing should focus on those input combinations that affect a program output, rather than considering all possible input combinations. The problem of generating minimal test cases is considered NP-complete as no single strategy in input-output interaction testing can generate the most efficient test cases. Therefore, a non-deterministic input-output based relationship test data sets minimization strategy, TIOR, has been proposed in this paper. The benchmarking inputs that reported in the literatures have been executed on TIOR in order to demonstrate its effectiveness and TIOR generally gives competitive results against other strategies.","","978-1-61284-691-0978-1-61284-689-7978-1-61284-690","10.1109/ISCI.2011.5959020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5959020","","Testing;Arrays;Software;Software engineering;Conferences;Software algorithms","minimisation;program testing","nondeterministic input-output interaction;interaction testing strategies;software system under test;uniform strength interaction testing;variable strength interaction testing;program output;NP-complete problem;TIOR;test data set minimization strategy","","1","24","","","","","","IEEE","IEEE Conferences"
"Photodiode sensor array design for photovoltaic system inter-row spacing optimization-calculating module performance during in-situ testing / simulated shading","P. M. Jansson; U. K. W. Schwabe","Department of Electrical and Computer Engineering, Center for Sustainable Design - South Jersey Technology Park, Rowan University College of Engineering, Glassboro, New Jersey, USA; Department of Electrical and Computer Engineering, Center for Sustainable Design - South Jersey Technology Park, Rowan University College of Engineering, Glassboro, New Jersey, USA","2010 IEEE Sensors Applications Symposium (SAS)","","2010","","","235","240","Data currently published in the literature provides estimates of photovoltaic (PV) module performance under essentially two standard test conditions. However, real design optimization by electrical and PV engineers working to achieve maximum production for systems they are responsible for requires additional knowledge as to how these modules can be expected to perform under all expected field conditions and in the presence of real obstructions; often including necessary inter-row shading. This work provides, for the first time, detailed field calculations of module performance for two types of PV modules illustrating the detrimental production impact of inter-row shading. The research indicates that using photodiode sensors to provide critical solar data from dispersed locations on the modules could aid in the development of an optimization procedure for minimizing the negative aspects of this common phenomenon. Extension of this work to cover the majority of the many module types will enable engineers to adequately address inter-row shading when they attempt to maximize the size of a PV system for a given roof or ground mounted project. It is clear from these research results that the impacts of inter-row shading on module output and energy production performance are more significant than originally anticipated by current PV design software models and must be considered in future design programs. The research provides a PV module shading test protocol, proposed sensor array layout design and results of the analysis of inter-row module shading (up to one cell row) and cell row coverage (20% through 100% in 20% increments) showing how important and worthwhile this sensor system can be to forwarding the design efforts of engineers working with high voltage photovoltaic power modules.","","978-1-4244-4988-0978-1-4244-4989","10.1109/SAS.2010.5439398","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5439398","photodiode sensors;photovoltaic system design criteria;photovoltaic system optimization;module inter-row shading","Sensor arrays;Photodiodes;Sensor systems;Photovoltaic systems;Design optimization;System testing;Solar power generation;Design engineering;Production;Power engineering and energy","electrical engineering;photodiodes;photovoltaic power systems;power engineering computing;sensor arrays;testing","photodiode sensor array design;photovoltaic system inter-row spacing optimization;simulated shading;in-situ testing;PV engineers;electrical engineers;energy production;current PV design software models;high voltage photovoltaic power modules","","1","9","","","","","","IEEE","IEEE Conferences"
"Parallelization of an ultrasound reconstruction algorithm for non destructive testing on multicore CPU and GPU","A. Pédron; L. Lacassagne; F. Bimbard; S. Le Berre","CEA, LIST, F-91191 Gif-sur-Yvette, France; CEA, LIST, F-91191 Gif-sur-Yvette, France; Institut d'Electronique Fondamentale, UMR 8622, Université Paris-Sud 11, F-91405 Orsay, France; CEA, LIST, F-91191 Gif-sur-Yvette, France","Proceedings of the 2011 Conference on Design & Architectures for Signal & Image Processing (DASIP)","","2011","","","1","8","The CIVA software platform developed by CEA-LIST offers various simulation and data processing modules dedicated to non-destructive testing (NDT). In particular, ultrasonic imaging and reconstruction tools are proposed, in the purpose of localizing echoes and identifying and sizing the detected defects. Because of the complexity of data processed, computation time is now a limitation for the optimal use of available information. In this article, we present performance results on parallelization of one computationally heavy algorithm on general purpose processors (GPP) and graphic processing units (GPU). GPU implementation makes an intensive use of atomic intrinsics. Compared to initial GPP implementation, optimized GPP implementation runs up to ×116 faster and GPU implementation up to ×631. This shows that, even with irregular workloads, combining software optimization and hardware improvements, GPU give high performance.","","978-1-4577-0621-9978-1-4577-0620-2978-1-4577-0619","10.1109/DASIP.2011.6136904","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6136904","non-destructive testing;ultrasonic reconstruction;parallelization;general purpose processors;graphic processing units","Graphics processing unit;Instruction sets;Image reconstruction;Software algorithms;Kernel;Testing;Optimization","graphics processing units;image reconstruction;materials science computing;ultrasonic imaging;ultrasonic materials testing","ultrasound reconstruction algorithm parallelization;nondestructive testing;multicore CPU;multicore GPU;CIVA software platform;data processing modules;ultrasonic imaging;ultrasonic reconstruction;computationally heavy algorithm;general purpose processors;graphic processing units;atomic intrinsics;software optimization;hardware improvements","","","13","","","","","","IEEE","IEEE Conferences"
"Adding Optimization to the Decompilable Code Editor","S. Ribic; A. Salihbegovic","NA; NA","2010 Fifth International Conference on Software Engineering Advances","","2010","","","88","93","The authors of this paper recently researched the possibility of developing programming language implementation, that is neither compiler, nor interpreter. The concept is based on keeping the complete program in native machine code, but the specialized editor can 'on the fly' decompile the machine code and display it as high level language. The displayed code can be edited and saved again as pure machine code. This paper reviews the possibility of optimizing generated code, while still retaining the possibility of decompilation. We found many important code sequences which can be replaced with shorter ones while keeping the code in decompilable executable format.","","978-1-4244-7788-3978-0-7695-4144","10.1109/ICSEA.2010.21","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5614983","compilation;decompilation;programming languages","Artificial neural networks;Optimization;Program processors;Arrays;Testing;High level languages","high level languages;optimising compilers","decompilable code editor;optimization;programming language;native machine code;high level language;code generation;code sequences","","","13","","","","","","IEEE","IEEE Conferences"
"GeTeX: A Tool for Testing Real-Time Embedded Systems Using CAN Applications","M. S. AbouTrab; S. Counsell; R. M. Hierons","NA; NA; NA","2011 18th IEEE International Conference and Workshops on Engineering of Computer-Based Systems","","2011","","","61","70","Real-Time Embedded Systems (RTES) have an increasing role in controlling the IT that we use on a day-to-day basis. The behaviour of an RTES is not based solely on the interactions it might have with its surrounding environment, but also on the timing requirements it induces. As a result, ensuring that an RTES behaves correctly is non-trivial, especially after adding time as a new dimension to the complexity of the testing process. In previous research, we introduced a 'priority-based' approach which tested the logical and timing behaviour of an RTES modeled formally as UPPAAL Timed Automata (UTA). The 'priority-based' approach was based on producing sets of timed test traces by achieving timing constraints coverage according to three sets of priorities, namely boundary, out-boundary and in-boundary. In this paper, we introduce a new testing tool 'GeTeX' that deploys the ""priority-based"" testing approach. GeTeX is a complete testing tool which generates timed test-cases from UTA models and executes them on the System Under Test (SUT) to identify faults. In its current version, GeTeX supports Control Area Network (CAN) applications.","","978-0-7695-4379-6978-1-4577-0065","10.1109/ECBS.2011.25","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5934805","Real-time embedded systems;real-time model-based testing;testing tool;CAN","Testing;Clocks;Timing;Automata;Real time systems;Cost accounting;Semantics","automata theory;controller area networks;embedded systems;program testing;software tools","GeTeX;real-time embedded system testing;CAN;priority-based approach;UPPAAL timed automata;timing constraints;out-boundary priority;in-boundary priority;priority-based testing;testing tool;system under test;control area network;boundary priority","","1","30","","","","","","IEEE","IEEE Conferences"
"SVM parameters optimization based on artificial bee colony algorithm and its application in handwriting verification","Y. Ming; A. Yue-qiao","School of Computer Science and Software, Hebei University of Technology, Tianjin, China; School of Computer Science and Software, Hebei University of Technology, Tianjin, China","2011 International Conference on Electrical and Control Engineering","","2011","","","5026","5029","In order to overcome the defect of falling into local optimal solution which all the common SVM parameters optimization methods had in different degree, a new SVM parameters optimization method based on artificial bee colony algorithm was proposed and applied to handwriting verification. Penalty factor C and kernel function parameter of SVM were taken as the optimization object, and classification accuracy of SVM was used as fitness value. Then the artificial bee colony algorithm was adopted in this work to achieve the global optimal solution of parameter C and . The proposed method was tested on four UCI standard datasets and compared with genetic algorithm and other conventional optimization algorithms. It was indicated from the result that the proposed method overcame the local optimal solution problem and acquire higher classification accuracy. The cost time of searching optimized parameters of small number classification problem was also reduced. Then the proposed method was applied to handwriting verification. Mean and variance of high frequency wavelet coefficient matrixes of handwriting images were taken as the classification feature. At last, the proposed method was used in handwriting verification and high classification precision was acquired.","","978-1-4244-8165-1978-1-4244-8162-0978-1-4244-8164","10.1109/ICECENG.2011.6057135","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6057135","artificial bee colony algorithm;support vector machine;parameter optimization;wavelet transform;handwriting verification","Support vector machines;Educational institutions;Tin;Optimization;Genetic algorithms;Classification algorithms;Algorithm design and analysis","feature extraction;genetic algorithms;handwriting recognition;image classification;matrix algebra;support vector machines;wavelet transforms","SVM parameter optimization;support vector machines;artificial bee colony algorithm;handwriting verification;penalty factor;fitness value;genetic algorithm;classification feature;wavelet coefficient matrix","","","","","","","","","IEEE","IEEE Conferences"
"Cost and power optimized electrical drive train system design for an electric three-wheel vehicle based on field test data acquisition and offline simulations","I. Saegesser; A. Vezzini; B. Galliker","Laboratory for Industrial Electronics, Bern UAS, CH-2501 Biel/Bienne, Switzerland; Laboratory for Industrial Electronics, Bern UAS, CH-2501 Biel/Bienne, Switzerland; S.A.M. Group AGCH-4702 Oensingen, Switzerland","2011 IEEE Vehicle Power and Propulsion Conference","","2011","","","1","5","One of the first pure electric vehicles with Lithium Ion Battery running on the European market is the SAM EVII developed by S A.M. Group AG. It is a light weight three wheel vehicle with automotive comfort, sold to customers since November 2009. To gather information about its daily usage and therefore to be able to design the drive train of the follow-up model, SAM 3, a field test has been started. In already sold customer cars used in different daily conditions, CAN data loggers have been recording data for several months. They log around 50 values in periodic time steps of 110 ms. In order to analyze this flood of information, software has been developed. The data is compared with simulations results based on static and dynamic requirements for the future model. Together the analysis allows a cost and power optimized choice of the electric drive train parameters. In this paper, the way to get and analyze the data is presented, as well as a small part of the first results.","1938-8756","978-1-61284-247-9978-1-61284-248-6978-1-61284-246","10.1109/VPPC.2011.6042984","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6042984","Electric Vehicle;Electric Motor optimization;EV Field Tests;Sizing of Drive Train","Batteries;Torque;Hybrid electric vehicles;Vehicle dynamics;Inverters;Data acquisition","data acquisition;electric drives;electric vehicles;secondary cells","electrical drive train system design;electric three wheel vehicle;field test data acquisition;offline simulations;electric vehicles;lithium ion battery","","1","1","","","","","","IEEE","IEEE Conferences"
"The Coal Mining Safety Equipment's Intelligent Monitoring and Management of Being based on the Web-environment","S. Zhao; L. Chen","NA; NA","2010 10th IEEE International Conference on Computer and Information Technology","","2010","","","2084","2088","The system utilizes the object-oriented analysis method and the broad-based B / S / D software architecture. Then researching and developing the coal mining security equipment's intelligent monitoring and management system of being based on the web-environment, the Web server software on this system using the tomcat6.0. Connecting the database by struts and ibatis dynamic web page technology and framework technology. Using the Myeclipse5.5.1 as a development tool; for the management process of different units, different functions, different users to design their features so that they can achieve their ""party-led, three-linkage"" of the new management, according to this mentioned above, the dynamic optimization to the security management of the prevention process is achieved and Safety equipment on the production site which can quickly feedback information, dynamic tracking and closed loop controlling. Therefore, it is effective to reduce the accident rate; and having important practical significances to the exploitation of coal mine safety production and decision-making management.","","978-1-4244-7548-3978-1-4244-7547-6978-0-7695-4108","10.1109/CIT.2010.354","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5578385","web- environment;device testing;intelligent management;monitoring","Safety;Monitoring;Databases;Testing;Mining equipment;Security","coal;computerised monitoring;decision making;mining;object-oriented methods;optimisation;safety systems;software architecture;software management;software tools;Web services","object-oriented analysis;software architecture;coal mining security equipment;intelligent monitoring;management system;Web server;web page technology;Myeclipse5.5.1 development tool;dynamic optimization;decision making;coal mining safety equipment","","1","5","","","","","","IEEE","IEEE Conferences"
"The design of vehicle twist beam rear axle based on structure topology optimization","Shi Peicheng; Xiao Ping; Gao Lixin; Si Kaizhong","School of Mechanical and Automotive Engineering, Anhui Polytechnic University, Wuhu, Nostate241000, China; School of Mechanical and Automotive Engineering, Anhui Polytechnic University, Wuhu, Nostate241000, China; Chery Automobile Co.,Ltd, Wuhu, Nostate241000, China; Chery Automobile Co.,Ltd, Wuhu, Nostate241000, China","2011 International Conference on Electric Information and Control Engineering","","2011","","","6252","6255","The structure optimization design of vehicle twist beam rear axle was conducted in this paper on the basis of topology optimization and HyperWorks software. The perfect material topology was found through optimizing defined designing space and the accuracy of optimization results was tested through K&amp;C performance analysis so that rear axles with lighter weight but better performance can be designed. In this way, the direct design of products can be realized. Therefore, this paper will have great significance and guidance on the tightweight design of other vehicle parts.","","978-1-4244-8039-5978-1-4244-8036-4978-1-4244-8038","10.1109/ICEICE.2011.5776978","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5776978","Twist beam rear axle;Vehicle lightweight;Structure topology optimization","Optimization;Topology;Axles;Vehicles;Employee welfare;Force;Finite element methods","automobiles;automotive components;axles;beams (structures);CAD;lightweight structures;mechanical engineering computing;optimisation;product design","vehicle twist beam rear axle design;structure topology optimization;HyperWorks software;material topology;K&C performance analysis;direct product design;vehicle parts","","","5","","","","","","IEEE","IEEE Conferences"
"Test-bench set-up for high-power PMSM test drive for validating novel control schemes","M. Oettmeier; M. Spichartz; V. Staudt; A. Steimel","Ruhr-University Bochum, D-44780 Bochum, Germany; Ruhr-University Bochum, D-44780 Bochum, Germany; Ruhr-University Bochum, D-44780 Bochum, Germany; Ruhr-University Bochum, D-44780 Bochum, Germany","2012 13th International Conference on Optimization of Electrical and Electronic Equipment (OPTIM)","","2012","","","509","514","Complex control procedures have to be developed for new drive control schemes to improve especially the dynamic behaviour of drives for safety-relevant features like slip-slide control or electronic stability programs. Today the development of the control algorithm is usually completely performed in software-based simulation tools, allowing a fast analysis of the system and of the control behaviour. The increasing calculation power of modern processor units allows by now the emulation of the drive chain and the converter in hardware-in-the-loop or power-hardware-in-the-loop test systems. Nevertheless high-power test benches are necessary to validate the proper operation of the control under `real' conditions and hereby proving the industrial suitability of the derived concept. This paper presents the set-up of a 140-kW PMSM laboratory test bench. The test bench with its real-time control system is presented in detail and first measurements of the controlled PMSM drive are presented.","1842-0133;1842-0133;1842-0133","978-1-4673-1653-8978-1-4673-1650-7978-1-4673-1652","10.1109/OPTIM.2012.6231803","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6231803","","Field programmable gate arrays;Switches;Real time systems;Voltage measurement;Inverters;Torque","drives;machine control;machine testing;permanent magnet machines;synchronous machines","test-bench set-up;high-power PMSM test drive;drive control schemes;slip-slide control;electronic stability programs;software-based simulation tools;processor units;power-hardware-in-the-loop test systems;PMSM laboratory test bench;real-time control system;power 140 kW","","6","20","","","","","","IEEE","IEEE Conferences"
"Log-based approach for performance requirements elicitation and prioritization","O. M. Mendizabal; M. Spier; R. Saad","Centro de Ciências Computacionais, Universidade Federal do Rio Grande - FURG, Rio Grande, Brazil; Expedia Worldwide Engineering, Expedia Inc., Bellevue, WA, USA; Performance Engineering & Testing Practice, Dell Inc, Porto Alegre, Brazil","2012 20th IEEE International Requirements Engineering Conference (RE)","","2012","","","297","302","Requirements engineering activities are a critical part of a project's lifecycle. Success of subsequent project phases is highly dependent on good requirements definition. However, eliciting and achieving consensus on priority between all stakeholders is a complex task. Considering software development of large scale global applications, the challenges increase by the need of managing discussions between groups of stakeholders with different roles and background. This paper presents a practical approach for requirements elicitation and prioritization based on realistic user behaviors observation. It uses basic statistic analysis and application usage information to automatically identify the most relevant requirements for majority of stakeholders. An industry case illustrates the feasibility and efficiency of our approach.","2332-6441;1090-705X;1090-750X","978-1-4673-2785-5978-1-4673-2783-1978-1-4673-2784","10.1109/RE.2012.6345818","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6345818","Performance Requirements;Requirements Elicitation;Requirements Prioritization;Case Study","Time factors;Servers;Companies;Measurement;Software;Tuning","formal verification;software performance evaluation;statistical analysis","log-based approach;performance requirements elicitation;requirements engineering activities;good requirements definition;software development;large scale global applications;requirements prioritization;realistic user behaviors observation;basic statistic analysis;application usage information","","1","13","","","","","","IEEE","IEEE Conferences"
"Ranking Attacks Based on Vulnerability Analysis","J. A. Wang; H. Wang; M. Guo; L. Zhou; J. Camargo","NA; NA; NA; NA; NA","2010 43rd Hawaii International Conference on System Sciences","","2010","","","1","10","Now that multiple-known attacks can affect one software product at the same time, it is necessary to rank and prioritize those attacks in order to establish a better defense. The purpose of this paper is to provide a set of security metrics to rank attacks based on vulnerability analysis. The vulnerability information is retrieved from a vulnerability management ontology, which integrates commonly used standards like CVE, CWE, CVSS, and CAPEC. Among the benefits of ranking attacks through the method proposed here are: a more effective mitigation or prevention of attack patterns against systems, a better foundation to test software products, and a better understanding of vulnerabilities and attacks.","1530-1605","978-1-4244-5510-2978-1-4244-5509","10.1109/HICSS.2010.313","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5428663","","Information security;Software tools;Software testing;Data security;Ontologies;Software systems;Programming;Computer industry;Software standards;Information retrieval","ontologies (artificial intelligence);program testing;security of data","ranking attacks;vulnerability analysis;software product;vulnerability management ontology","","7","19","","","","","","IEEE","IEEE Conferences"
"Three-phase induction motor design software","G. Scutaru; A. Negoita; R. M. Ionescu","&#x201C;Transilvania&#x201D; University of Brasov, Romania; &#x201C;Transilvania&#x201D; University of Brasov, Romania; &#x201C;Transilvania&#x201D; University of Brasov, Romania","2010 IEEE International Conference on Automation, Quality and Testing, Robotics (AQTR)","","2010","3","","1","4","The paper presents a program for optimized design or re-design of three-phase induction motors conceived in VISUAL BASIC language. The program structure is modular and can thus be adjusted to the most varied requirements of induction motor design. For its users, the program offers several facilities, i.e. a friendly interface, different ways of formulating the optimum problem, an updatable database including information on the characteristics of the materials used and the manufactured motors, the user's guide that comprises the detailed description of the program and explanations for each computation step.","","978-1-4244-6725-9978-1-4244-6724-2978-1-4244-6723","10.1109/AQTR.2010.5520758","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5520758","","Induction motors;Software design;Spatial databases;Computational geometry;Design optimization;Computer aided manufacturing;Magnetic circuits;Magnetic materials;Computer interfaces;Switches","CAD;computer aided engineering;design engineering;induction motors","three-phase induction motor design software;optimized design;VISUAL BASIC language;program structure","","3","6","","","","","","IEEE","IEEE Conferences"
"A fast adaptive MOEA and its application to dynamic reactive power optimization","Yi Zhu; A. Zhang","College of Software Engineering, Chengdu University of Information Technology, China; School of Electrical Engineering & Information, Sichuan University, Chengdu, China","2010 2nd International Conference on Computer Engineering and Technology","","2010","4","","V4-141","V4-143","Multi-objective Evolutionary Algorithm (MOEA) has been applied to calculate many optimization problems in electrical system. But it is very difficult to use MOEA to handle dynamic reactive power optimization due to its high computing complexity. An adaptive ε domination technology was applied to facilitate calculate dynamic reactive power optimization in this paper. The method includes 2 stages to complete the calculation: 1st one is to get initial solutions for the 1st hour condition of a day, the 2nd one is to handle the succedent 23 hours conditions as a whole. The approach has been verified by the numerical simulation on IEEE 30 bus test system.","","978-1-4244-6349-7978-1-4244-6347-3978-1-4244-6348","10.1109/ICCET.2010.5485678","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5485678","MOEA;dynamic reactive power optimization;ε domination;multi-objective","Reactive power;Nonlinear dynamical systems;Power system modeling;Voltage control;Capacitors;Dynamic programming;Dynamic voltage scaling;Application software;Educational institutions;Software engineering","evolutionary computation;reactive power","fast adaptive MOEA;dynamic reactive power optimization;multiobjective evolutionary algorithm;electrical system;computing complexity;adaptive ε domination technology;numerical simulation;IEEE 30 bus test system","","","8","","","","","","IEEE","IEEE Conferences"
"Optimizing Constraint Solving to Better Support Symbolic Execution","I. Erete; A. Orso","NA; NA","2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops","","2011","","","310","315","Constraint solving is an integral part of symbolic execution, as most symbolic execution techniques rely heavily on an underlying constraint solver. In fact, the performance of the constraint solver used by a symbolic execution technique can considerably affect its overall performance. Unfortunately, constraint solvers are mostly used in a black-box fashion within symbolic execution, without leveraging any of the contextual and domain information available. Because constraint solvers are optimized for specific kinds of constraints and heavily based on heuristics, this leaves on the table many opportunities for optimizing the solvers' performance. To address this problem, we propose a novel optimization strategy that uses domain and contextual information to optimize the performance of constraint solvers during symbolic execution. We also present a study in which we assess the effectiveness of our and other related strategies when used within dynamic symbolic execution performed on real software. Our results are encouraging, they show that optimizing constraints based on domain and contextual information can improve the efficiency and effectiveness of constraint solving and ultimately benefit symbolic execution.","","978-0-7695-4345-1978-1-4577-0019","10.1109/ICSTW.2011.98","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954425","symbolic execution;constraint solving","Concrete;Testing;Software;Context;Constraint optimization;Manuals","program testing","constraint solving optimisation;black-box fashion;dynamic symbolic execution","","10","8","","","","","","IEEE","IEEE Conferences"
"DfT optimization for pre-bond testing of 3D-SICs containing TSVs","J. Li; D. Xiang","School of Software, Tsinghua University, Beijing, China; School of Software, Tsinghua University, Beijing, China","2010 IEEE International Conference on Computer Design","","2010","","","474","479","This paper proposes to provide testability for the breaking point produced by the Through-Silicon-Vias (TSVs) of 3D-Stacked ICs (3D-SICs) during pre-bond testing with low Design-for-Testability (DfT) cost. Different from prior solutions which utilize two additional wrapper cells for the breaking point at each TSV, this paper proposes to provide the testability of two ends of the TSVs respectively by reusing the existing Primary-Inputs (PIs)/ Primary-Outputs (POs) and Pseudo-PIs/Pseudo- POs (PPIs/PPOs). To further reduce the hardware overhead and enhance the efficiency of the proposed method, this paper has also proposed the metrics and algorithm on deciding the selecting order of the TSVs and the PIs/PPIs (POs/PPOs) to be reused. The experimental results on larger ITC'99 benchmark circuits validate the effectiveness of the proposed method.","1063-6404;1063-6404","978-1-4244-8937-4978-1-4244-8936-7978-1-4244-8935","10.1109/ICCD.2010.5647651","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5647651","","Through-silicon vias;Testing;Logic gates;Observability;Controllability;Correlation;Optimization","design for testability;integrated circuit testing;matrix algebra;multiprocessor interconnection networks;silicon;three-dimensional integrated circuits","DfT optimization;prebond testing;3D-SIC;TSV;Through-Silicon-Vias;3D stacked IC;design-for-testability","","9","9","","","","","","IEEE","IEEE Conferences"
"ACGHSIM: A Simulation Tool for Accelerated Conjugate Hessian Gradient Optimization Algorithm","R. Mousavi; A. Mohammadbagheri; R. K. Moghaddam","NA; NA; NA","2011 Third International Conference on Computational Intelligence, Modelling & Simulation","","2011","","","181","186","In this article a graphical user interface (GUI), ""ACGHSIM"" is developed for Simulation the accelerated conjugate Hessian optimization algorithm (ACGHES). ACGHSIM is provided for educational purpose to improve the understanding the convergence and acceleration of ACGHES algorithm in solving large-scale unconstrained optimization problems. ACGHSIM contains a library for 73 different test functions in optimization. The user has option to select one of these functions and simulate the ACGHES algorithm to calculate the minimum. Additionally the users are able to apply themselves initial points or use the suggested initial points by this application. In order to demonstrate the User Friendly features of ACGHSIM, some examples are given.","2166-8523;2166-8531","978-1-4577-1797-0978-0-7695-4562","10.1109/CIMSim.2011.39","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6076353","Nonlinear Optimization;ACGHES Algorithm;MATLAB;GUI","Optimization;Graphical user interfaces;Approximation algorithms;Vectors;Software algorithms;MATLAB;Acceleration","conjugate gradient methods;graphical user interfaces;human computer interaction;optimisation;problem solving","simulation tool;accelerated conjugate Hessian gradient algorithm;graphical user interface;ACGHSIM;accelerated conjugate Hessian optimization algorithm;ACGHES;user friendly features;problem solving","","","6","","","","","","IEEE","IEEE Conferences"
"Improved Particle Swarm Optimization Algorithm","Y. Gao; S. Li","NA; NA","2010 International Conference on Computational Intelligence and Software Engineering","","2010","","","1","4","Particle Swarm Optimization (PSO) is a new random computational method for tackling optimization functions. However, it is easily trapped into the local optimum when solving the complexity and high-dimensional problems, which makes the performance of PSO greatly reduced. To overcome this shortcoming, the paper proposes an Improved Particle Swarm Optimization (IPSO), by adding the third particle of having a more room for progress to guide the current particles' velocity updating rule, Which can keep the diversity of the particles and reduce the probability of trapping into the local optimization .Besides, the program enhances and improves the stability and the convergence speed of the algorithm according to adjusting the particles which go beyond the default position space in each interiors. Five benchmark functions are tested, and the results indicate the effectiveness of the new program.","","978-1-4244-5391-7978-1-4244-5392","10.1109/CISE.2010.5677054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5677054","","Particle swarm optimization;Optimization;Heuristic algorithms;Convergence;Charge carrier processes;Stability criteria","particle swarm optimisation;stability","particle swarm optimization algorithm;stability;convergence speed;velocity updating rule","","1","9","","","","","","IEEE","IEEE Conferences"
"An improved crowding-based differential evolution for multimodal optimization","L. Chen; L. Ding","State Key Laboratory of Software Engineering, Wuhan University, Wuhan, China; State Key Laboratory of Software Engineering, Wuhan University, Wuhan, China","2011 International Conference on Electrical and Control Engineering","","2011","","","1973","1977","Traditional optimization technologies usually try to find a global optimum, however, many optimization problems are multimodal with many global or local optima. In real world, multiple optima are usually interested in and can give people multi-choices. Crowding-based differential evolution (CRDE) algorithm is a simple but very powerful for multimodal optimization. CRDE has good explorative ability to find the optima in search space. The main shortcoming of CRDE is the convergence speed is low. To welcome this, an improved CRDE with local search on the individuals nearest optima in the population is introduced. Local search uses Gaussian mutation whose mutation range decreases linearly with iteration. It makes refined search in the area around the optima and improves the exploitable ability. To identify the best individuals around the optima in the current population, the idea of specifying the seeds of species (i.e. the best individuals in niches) in species-based particle swam optimization (SPSO) is adapted. The introduced algorithm is tested on multimodal benchmark problems CRDE used and the test shows it outperforms CRDE in convergence speed greatly.","","978-1-4244-8165-1978-1-4244-8162-0978-1-4244-8164","10.1109/ICECENG.2011.6057739","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6057739","multimodal optimization;differential Evolution;local search;crowding scheme","Optimization;Accuracy;Benchmark testing;Convergence;Evolutionary computation;Genetic algorithms;Euclidean distance","convergence of numerical methods;demography;differential equations;Gaussian processes;iterative methods;particle swarm optimisation;search problems","improved crowding-based differential evolution;multimodal optimization;multiple optima;search space;Gaussian mutation;iteration method;species-based particle swam optimization;multimodal benchmark problem;CRDE","","2","10","","","","","","IEEE","IEEE Conferences"
"An optimization to automatic Fault Tree Analysis and Failure Mode and Effect Analysis approaches for processes","Danhua Wang; Jingui Pan","State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China","2010 International Conference On Computer Design and Applications","","2010","3","","V3-153","V3-157","There are two issues to be addressed in the automatic Fault Tree Analysis (FTA) and Failure Mode and Effect Analysis (FMEA) approaches: resource fault and channel fault are not considered when generating the artifact flow graph (AFG) from Little-JIL processes. The AFG is incomplete, thus the fault trees and FMEA reports automatically generated partially based on AFG is incomplete. In this paper, we put forward to an approach of introducing resource instances and channel instances into fault propagation in Little-JIL processes. Thus, it makes the FTA and FMEA for Little-JIL processes to be much more effective. That is to say, how resource instances and channel instances might affect the fault propagation in Little-JIL processes is taken into account while performing these two automatic safety analysis techniques.","","978-1-4244-7164-5978-1-4244-7164","10.1109/ICCDA.2010.5541008","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5541008","FTA;FMEA;resource;channel","Failure analysis;Fault trees;US Department of Transportation;Laboratories;Flow graphs;Design optimization;Safety;Performance analysis;Industrial relations;Testing","fault diagnosis;fault trees;flow graphs;optimisation;safety","optimization;automatic fault tree analysis;failure mode analysis;failure effect analysis;resource fault;channel fault;artifact flow graph;Little-JIL process;resource instance;channel instance;fault propagation;automatic safety analysis","","2","12","","","","","","IEEE","IEEE Conferences"
"Resource-Aware Component Scheduling Rejuvenation Model Using Lagrangian Relaxation","C. Pengfei; Q. Yong","NA; NA","2010 Seventh Web Information Systems and Applications Conference","","2010","","","155","159","As the time of system running increases, the performance of the system will decrease. This phenomenon is called software aging. Recently software rejuvenation as an efficient method to counteract software aging emerges. This method recovers the performance of the system through cleaning the internal state of system. Now component-oriented programming concept has been proposed and spreaded. And software system based on component emerges. These components with different function not only promote software reusability and self-management but also software reliability and availability. This paper demonstrates a software rejuvenation model based on resource-aware component optimized scheduling to realize software rejuvenation and avoid system crashing. In this paper we give the Lagrangian Relaxation component scheduling algorithm under single constrained condition and multi constrained condition. Then we test the algorithm and evaluate the model. Comparing system reboot, this rejuvenation model reduces MTTR (mean time to recover) very much. So the continuity of the service is promoted and the cost of downtime is decreased.","","978-1-4244-8440","10.1109/WISA.2010.13","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5581313","Software aging;Component;Software rejuvenation;CSP","Software;Aging;Markov processes;Switches;Analytical models;Computer crashes;Organizations","object-oriented programming;relaxation theory;scheduling;software maintenance;software reusability;system recovery","resource-aware component scheduling rejuvenation model;system running;software aging;internal state;component-oriented programming concept;software system;software reusability;self-management;software reliability;software availability;software rejuvenation model;resource-aware component optimized scheduling;system crashing;Lagrangian relaxation component scheduling algorithm;single constrained condition;multiconstrained condition;system reboot;MTTR;mean time to recover","","","15","","","","","","IEEE","IEEE Conferences"
"Design optimization for electrical performance of a LFBGA package using EM-field simulation","C. Hunat; C. Tubulo; C. K. Wang; R. Liang; N. Suthiwongsunthorn","United Test and Assembly Center Ltd 5 Serangoon North Ave 5 Singapore 554916; United Test and Assembly Center Ltd 5 Serangoon North Ave 5 Singapore 554916; United Test and Assembly Center Ltd 5 Serangoon North Ave 5 Singapore 554916; United Test and Assembly Center Ltd 5 Serangoon North Ave 5 Singapore 554916; United Test and Assembly Center Ltd 5 Serangoon North Ave 5 Singapore 554916","2012 IEEE 14th Electronics Packaging Technology Conference (EPTC)","","2012","","","770","773","In this paper we presented methods of optimizing a 17×17mm LFBGA package having improve its electrical performance using an electromagnetic-field software through simulation, particularly on controlled signals (single-end & differential pairs) and high-speed I/O signal traces.","","978-1-4673-4552-1978-1-4673-4553-8978-1-4673-4551","10.1109/EPTC.2012.6507188","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6507188","","Electronics packaging;Conferences","ball grid arrays;design engineering;electromagnetic fields","design optimization;electrical performance;LFBGA package;EM field simulation;electromagnetic field software","","","7","","","","","","IEEE","IEEE Conferences"
"A GP effort estimation model utilizing line of code and methodology for NASA software projects","F. S. Alaa; A. Al-Afeef","Computers and Systems Department Electronics Research Institute (ERI), Cairo, Egypt; Information Technology Department, Al-Balqa Applied University (BAU), Salt, Jordan","2010 10th International Conference on Intelligent Systems Design and Applications","","2010","","","290","295","There is still an urgent need of finding a mathematical model which can provide an accurate relationship between the software project effort/cost and the cost drivers. A powerful algorithm which can optimize such a relationship via developing a mathematical relationship between model variables is urgently needed. In this paper, we explore the use of GP to develop a software cost estimation model utilizing the effect of both the developed line of code and the used methodology during the development. An application of estimating the effort for some NASA software projects is introduced. The performance of the developed Genetic Programming (GP) based model was tested and compared to known models in the literature. The developed GP model was able to provide good estimation capabilities compared to other models.","2164-7143;2164-7151","978-1-4244-8136-1978-1-4244-8134-7978-1-4244-8135","10.1109/ISDA.2010.5687251","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5687251","Software Cost Estimation;Software Engineering;Genetic Programming;NASA Software","Mathematical model;Software;Computational modeling;Estimation;NASA;Genetic programming;Equations","genetic algorithms;project management;software cost estimation;software development management","GP effort estimation model;genetic programming;NASA software projects;software cost estimation model;line-of-code","","2","26","","","","","","IEEE","IEEE Conferences"
"Identifying Failure-Inducing Combinations in a Combinatorial Test Set","L. S. G. Ghandehari; Y. Lei; T. Xie; R. Kuhn; R. Kacker","NA; NA; NA; NA; NA","2012 IEEE Fifth International Conference on Software Testing, Verification and Validation","","2012","","","370","379","A t-way combinatorial test set is designed to detect failures that are triggered by combinations involving no more than t parameters. Assume that we have executed a t-way test set and some tests have failed. A natural question to ask is: What combinations have caused these failures? Identifying such combinations can facilitate the debugging effort, e.g., by reducing the scope of the code that needs to be inspected. In this paper, we present an approach to identifying failure-inducing combinations, i.e., combinations that have caused some tests to fail. Given a t-way test set, our approach first identifies and ranks a set of suspicious combinations, which are candidates that are likely to be failure-inducing combinations. Next, it generates a set of new tests, which can be executed to refine the ranking of suspicious combinations in the next iteration. This process can be repeated until a stopping condition is satisfied. We conducted an experiment in which our approach was applied to several benchmark programs. The experimental results show that our approach can effectively and efficiently identify failure-inducing combinations in these programs.","2159-4848","978-0-7695-4670-4978-1-4577-1906","10.1109/ICST.2012.117","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200129","Combinatorial Testing;Fault Localization;Debugging","Complexity theory;Benchmark testing;Debugging;Algorithm design and analysis;USA Councils;Educational institutions","combinatorial mathematics;program debugging;program testing","failure-inducing combination;t-way combinatorial test set;program debugging","","20","13","","","","","","IEEE","IEEE Conferences"
"DREX: Developer Recommendation with K-Nearest-Neighbor Search and Expertise Ranking","W. Wu; W. Zhang; Y. Yang; Q. Wang","NA; NA; NA; NA","2011 18th Asia-Pacific Software Engineering Conference","","2011","","","389","396","This paper proposes a new approach called DREX (Developer Recommendation with k-nearest-neighbor search and Expertise ranking) to developer recommendation for bug resolution based on K-Nearest-Neighbor search with bug similarity and expertise ranking with various metrics, including simple frequency and social network metrics. We collect Mozilla Fire fox open bug repository as the experimental data set and compare different ranking metrics on the performance of recommending capable developers for bugs. Our experimental results demonstrate that, when recommending 10 developers for each one of the 250 testing bugs, DREX has produced better performance than traditional methods with multi-labeled text categorization. The best performance obtained by two metrics as Out-Degree and Frequency, is with recall as 0.6 on average. Moreover, other social network metrics such as Degree and Page Rank have produced comparable performance on developer recommendation as Frequency when used for developer expertise ranking.","1530-1362;1530-1362;1530-1362","978-1-4577-2199-1978-0-7695-4609","10.1109/APSEC.2011.15","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6130646","Open Bug Repository;Developer Recommendation;KNN Search;Expertise Ranking","Computer bugs;Social network services;Measurement;Software;Fires;Vectors;Testing","program debugging;social networking (online)","DREX;developer recommendation;K-nearest-neighbor search;expertise ranking;bug resolution;bug similarity;social network metrics;Mozilla Fire fox;bug repository;Page Rank","","25","23","","","","","","IEEE","IEEE Conferences"
"A scalable autotest platform for embedded system","Di Liu; Qiao Su; Yan Xie","School of Computer Science&amp; Engineering, University of Electronic Science and Technology of China, Sichuan, China; School of Computer Science&amp; Engineering, University of Electronic Science and Technology of China, Sichuan, China; China Aerodynamics Research and Development Center High Speed Aerodynamics Institute, China","The 2010 International Conference on Apperceiving Computing and Intelligence Analysis Proceeding","","2010","","","398","401","The complex and diversity of embedded system make the developing and testing embedded system a hard work. Providing a high-availability high-scale universal testing platform can improve the development of embedded software. The autotest platform focuses on layered scalability and continuous integration. Loose coupling between the layers makes the system easy to extend at the local layer. Continuous improvements reduce the initial investment, optimize the platform implementation, and expand capacity and capabilities according to requirements. Users commit test requests front-end, and background centralized testing cluster executes this task and returns results to users.","","978-1-4244-8026-5978-1-4244-8025-8978-1-4244-8024","10.1109/ICACIA.2010.5709928","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5709928","Autotest;embedded system;TDD;black box testing","Servers;Testing;Monitoring;Hardware;Embedded systems;Schedules","automatic test software;embedded systems","scalable autotest platform;embedded system;high-availability high-scale universal testing platform;embedded software;continuous integration;background centralized testing cluster","","","7","","","","","","IEEE","IEEE Conferences"
"Speculative dynamic vectorization for HW/SW codesigned processors","R. Kumar; A. Martínez; A. González","Dept. of Computer Architecture, Universitat Politècnica de Catalunya, 08034 Barcelona, Spain; Intel Barcelona Research Center, Intel Labs, 08034, BSpain; Dept. of Computer Architecture, Universitat Politècnica de Catalunya, 08034 Barcelona, Spain","2012 21st International Conference on Parallel Architectures and Compilation Techniques (PACT)","","2012","","","459","460","Hardware/Software (HW/SW) co-designed processors have emerged as a promising solution to the power and complexity problems of modern microprocessors. These processors utilize dynamic optimizations to improve the performance. However, vectorization, one of the most potent optimizations, has not yet received the deserved attention. This paper presents a speculative dynamic vectorization algorithm to explore its potential.","","978-1-4503-1182-3978-1-5090-6609","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7842973","HW/SW Co-designed processor;Vectorization;Speculation","Heuristic algorithms;Registers;Program processors;Optimization;Software algorithms;Benchmark testing;Computer architecture","hardware-software codesign;microcomputers;optimisation","speculative dynamic vectorization;hardware/software codesigned processor;HW/SW codesigned processor;microprocessor;dynamic optimization","","","2","","","","","","IEEE","IEEE Conferences"
"Managing Software Quality Requirements","L. B. Phillips; A. Aurum; R. B. Svensson","NA; NA; NA","2012 38th Euromicro Conference on Software Engineering and Advanced Applications","","2012","","","349","356","This research study explores current quality requirements (QR) management practices in Australian organisations focusing on the elicitation, handling processes, challenges faced, quantification methods used and interdependency management. This research was conducted through six mini case studies, examining organizations that varied in size, structure, industry and function. A mixed methodology was utilised through an online survey for gathering quantitative data and semi-structured interviews for gathering explanatory qualitative data. The results found that five out of the six organisations studied did not have a formal and defined process for the handling of QRs. Large organisations treated QRs are part of their overall project specifications, while smaller organisations saw the management of QRs as more ad hoc. When prioritising QRs, Accuracy was considered the most important priority followed by Security and Reliability. The main challenges that organisations face in their management of QRs is defining and quantifying these requirements.","1089-6503;2376-9505","978-0-7695-4790-9978-1-4673-2451","10.1109/SEAA.2012.65","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6328174","Software quality requirements;non-functional requirements;industrial practice;prioritization","Companies;Software;Interviews;Accuracy;Testing;Industries","software quality;systems analysis","software quality requirements;QR management practices;Australian organisations;mixed methodology;semi-structured interviews;quantitative data","","3","27","","","","","","IEEE","IEEE Conferences"
"Operation parameters optimization of butadiene extraction distillation based on neural network","F. Chen; J. Zheng","Glorious Sun School of Business and Management, Donghua University, Shanghai, 200051, China; Glorious Sun School of Business and Management, Donghua University, Shanghai, 200051, China","2011 6th IEEE International Conference on Nano/Micro Engineered and Molecular Systems","","2011","","","1157","1163","In this paper, based on the material balance, we used Aspen plus software to make sensitivity analysis of separation performance of the key component and the operational parameters. We quantitatively analyze the influence between the component parameters and the process. At last, we used the neural network to forecast the solvent ratio on diffident C4 feed and calculated the optimization solvent. At the end of the paper, we used the actual production data to test the validity of the model. On the view of reducing the energy consumption and ensuring the product quality, the research result told us that solvent ratio could be reduced nearly one percentage point.","","978-1-61284-777-1978-1-61284-775-7978-1-61284-776","10.1109/NEMS.2011.6017562","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6017562","Neural Network;Butadiene Extractive Distillation;Optimization;Solvent ratio;Reflux ratio","Solvents;Poles and towers;Optimization;Feeds;Distillation equipment;Predictive models;Production","energy consumption;neural nets;optimisation;production engineering computing;raw materials;rubber;separation;solvents (industrial)","butadiene extraction distillation;neural network;material balance;operation parameter optimization;Aspen plus software;sensitivity analysis;separation performance;solvent ratio;production data;energy consumption;product quality","","","8","","","","","","IEEE","IEEE Conferences"
"Impact Analysis of Configuration Changes for Test Case Selection","X. Qu; M. Acharya; B. Robinson","NA; NA; NA","2011 IEEE 22nd International Symposium on Software Reliability Engineering","","2011","","","140","149","Testing configurable systems, which are becoming prevalent, is expensive due to the large number of configurations and test cases. Existing approaches reduce this expense by selecting or prioritizing configurations. However, these approaches redundantly run the full test suite for the selected configurations. To address this redundancy, we propose a test case selection approach by analyzing the impact of configuration changes with static program slicing. Given an existing test suite T used for testing a system S under a configuration C, our approach decides for each t in T if t has to be used for testing S under a different configuration C'. We have evaluated our approach on a large industrial system within ABB with promising results.","2332-6549;1071-9458;1071-9458","978-1-4577-2060-4978-0-7695-4568","10.1109/ISSRE.2011.9","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6132962","configuration testing;test case selection;program slicing;static impact analysis","Testing;Safety;Equations;Google;Data structures;Approximation methods;Switches","program slicing;program testing","impact analysis;configuration changes;test case selection;configurable system testing;static program slicing","","7","31","","","","","","IEEE","IEEE Conferences"
"Efficient Refinement Checking for Model-Based Mutation Testing","B. K. Aichernig; E. Jöbstl","NA; NA","2012 12th International Conference on Quality Software","","2012","","","21","30","In model-based mutation testing, a test model is mutated for test case generation. The resulting test cases are able to detect whether the faults in the mutated models have been implemented in the system under test. For this purpose, a conformance check between the original and the mutated model is required. We have developed an approach for conformance checking of action systems, which are well-suited to specify reactive and non-deterministic systems. We rely on constraint solving techniques. Both, the conformance relation and the transition relation are encoded as constraint satisfaction problems. Earlier results showed the potential of our constraint-based approach to outperform explicit conformance checking techniques, which often face state space explosion. In this work, we go one step further and show optimisations that really boost our performance. In our experiments, we could reduce our runtimes by 80%.","2332-662X;1550-6002;1550-6002","978-1-4673-2857-9978-0-7695-4833","10.1109/QSIC.2012.58","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6319222","action systems;conformance;refinement;model-based testing;mutation testing;constraint solving","Testing;Semantics;Object oriented modeling;Syntactics;Optimization;Runtime;Helium","formal verification;program testing","efficient refinement checking;model based mutation testing;conformance checking;nondeterministic systems;reactive systems;constraint solving techniques;constraint satisfaction problems","","6","30","","","","","","IEEE","IEEE Conferences"
"An Interval-Based Model for Detecting Software Defect Using Alias Analysis","H. Zhou; D. Jin; Y. Gong","NA; NA; NA","2012 19th Asia-Pacific Software Engineering Conference","","2012","2","","136","144","Alias analysis is a branch of static program analysis aiming at computing variables which are alias of each other. It is a basis research for many analyses and optimizations in software engineering and compiler construction. Precise modeling of alias analysis is fundamental for software analysis. This paper presents two practical approximation models for representing and computing alias: memory-sensitive model (MSM) and value-sensitive model (VSM). Based on defect-oriented detecting, we present a method to detect software defect using VSM and MSM, which realizes inter-procedure detecting by procedure summary. According to whether type of analysis object coming from defect is value-sensitive or memory-sensitive, we propose two detecting algorithms based on two alias models respectively. One is for memory leak (ML) based on MSM, and the other is for invalid arithmetic operation (IAO) based on VSM. We apply a defect testing system (DTS) to detect six C++ open source projects for proving our models effectiveness. Experimental results show that applying our technique to detect IAO and ML defect can improve detecting efficiency, at the same time reduce potential false positives and false negatives.","1530-1362;1530-1362;1530-1362","978-1-4673-4930-7978-0-7695-4922","10.1109/APSEC.2012.14","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6462792","static analysis;defect detection;alias analysis;interval computation;procedure summary","Analytical models;Computational modeling;Algorithm design and analysis;Software;Abstracts;Argon;Automata","approximation theory;C++ language;program compilers;program diagnostics;software engineering","interval-based model;software defect detection;alias analysis;static program analysis;software engineering optimization;compiler construction optimization;software analysis;approximation model;memory-sensitive model;value-sensitive model;defect-oriented detecting;interprocedure detection;invalid arithmetic operation;defect testing system;C++ open source project","","3","22","","","","","","IEEE","IEEE Conferences"
"Prioritising Refactoring Using Code Bad Smells","M. Zhang; N. Baddoo; P. Wernick; T. Hall","NA; NA; NA; NA","2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops","","2011","","","458","464","We investigated the relationship between six of Fowler et al.'s Code Bad Smells (Duplicated Code, Data Clumps, Switch Statements, Speculative Generality, Message Chains, and Middle Man) and software faults. In this paper we discuss how our results can be used by software developers to prioritise refactoring. In particular we suggest that source code containing Duplicated Code is likely to be associated with more faults than source code containing the other five Code Bad Smells. As a consequence, Duplicated Code should be prioritised for refactoring. Source code containing Message Chains seems to be associated with a high number of faults in some situations. Consequently it is another Code Bad Smell which should be prioritised for refactoring. Source code containing only one of the Data Clumps, Switch Statements, Speculative Generality, or Middle Man Bad Smell is not likely to be fault-prone. As a result these Code Bad Smells could be put into a lower refactoring priority.","","978-0-7695-4345-1978-1-4577-0019","10.1109/ICSTW.2011.69","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954447","Code Bad Smells;Refactoring;Fault","Software;Switches;Fault diagnosis;Encoding;Taxonomy;Systematics;Java","software engineering;system recovery","code bad smells;duplicated code;data clumps;switch statements;speculative generality;message chains;middle man;software faults;source code","","2","21","","","","","","IEEE","IEEE Conferences"
"Optimizing the Software Architecture for Extensibility in Hard Real-Time Distributed Systems","Q. Zhu; Y. Yang; M. Natale; E. Scholte; A. Sangiovanni-Vincentelli","Intel Corporation; EECS Department, University of California at Berkeley; Scuola Superiore S. Anna, Pisa; United Technologies Research Center; EECS Department, University of California at Berkeley","IEEE Transactions on Industrial Informatics","","2010","6","4","621","636","We consider a set of control tasks that must be executed on distributed platforms so that end-to-end latencies are within deadlines. We investigate how to allocate tasks to nodes, pack signals to messages, allocate messages to buses, and assign priorities to tasks and messages, so that the design is extensible and robust with respect to changes in task requirements. We adopt a notion of extensibility metric that measures how much the execution times of tasks can be increased without violating end-to-end deadlines. We optimize the task and message design with respect to this metric by adopting a mathematical programming front-end followed by postprocessing heuristics. The proposed algorithm as applied to industrial strength test cases shows its effectiveness in optimizing extensibility and a marked improvement in running time with respect to an approach based on randomized optimization.","1551-3203;1941-0050","","10.1109/TII.2010.2053938","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5535223","Design space exploration;distributed system;extensibility;platform-based design;real-time","Software architecture;Real time systems;Robustness;Design optimization;Control system synthesis;Automatic control;Supply chains;Production;Sensor systems;Control systems","distributed processing;optimisation;software architecture","software architecture;hard real-time distributed systems;end-to-end latency;extensibility metric notion;randomized optimization;mathematical programming front-end","","30","22","","","","","","IEEE","IEEE Journals & Magazines"
"Multi-objective test suite minimisation using Quantum-inspired Multi-objective Differential Evolution Algorithm","A. C. Kumari; K. Srinivas; M. P. Gupta","Department of Physics &amp; Computer Science, Dayalbagh Educational Institute, Dayalbagh, Agra, India; Department of Electrical Engineering, Dayalbagh Educational Institute, Dayalbagh, Agra, India; Department of Management Studies, Indian Institute of Technology, Delhi, India","2012 IEEE International Conference on Computational Intelligence and Computing Research","","2012","","","1","7","This paper presents the solution for multi-objective test suite minimisation problem using Quantum-inspired Multi-objective differential Evolution Algorithm. Multi-objective test suite minimisation problem is to select a set of test cases from the available test suite while optimizing the multi objectives like code coverage, cost and fault history. As test suite minimisation problem is an instance of minimal hitting set problem which is NP-complete; it cannot be solved efficiently using traditional optimization techniques especially for the large problem instances. This paper presents Quantum-inspired Multi-objective Differential Evolution Algorithm (QMDEA) for the solution of multi-objective test suite minimisation problem. QMDEA combines the preeminent features of Differential Evolution and Quantum Computing. The features of QMDEA help in achieving quality Pareto-optimal front solutions with faster convergence. The performance of QMDEA is tested on two real world applications and the results are compared against the state-of-the-art multi-objective evolutionary algorithm NSGA-II. The comparison of the obtained results indicates superior performance of QMDEA.","","978-1-4673-1344-5978-1-4673-1342-1978-1-4673-1343","10.1109/ICCIC.2012.6510272","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6510272","regression testing;software testing;test suite minimisation","","computational complexity;genetic algorithms;minimisation;Pareto optimisation;program testing","multiobjective test suite minimisation;quantum-inspired multiobjective differential evolution algorithm;code coverage;fault history;minimal hitting set problem;NP-complete problem;Pareto-optimal front solution;NSGA-II algorithm;nondominated sorting genetic algorithm","","1","14","","","","","","IEEE","IEEE Conferences"
"Development of operator seat vibration test device of earth-moving machinery and its control system","B. Yang; Y. Yang; Y. Zhang; X. Chen; H. Li; T. Shen","College of Mechanical Science and Engineering, Jilin University, Changchun 130022, China; College of Mechanical Science and Engineering, Jilin University, Changchun 130022, China; College of Mechanical Science and Engineering, Jilin University, Changchun 130022, China; College of Mechanical Science and Engineering, Jilin University, Changchun 130022, China; College of Mechanical Science and Engineering, Jilin University, Changchun 130022, China; College of Mechanical Science and Engineering, Jilin University, Changchun 130022, China","2012 IEEE International Conference on Mechatronics and Automation","","2012","","","1086","1090","Earth-moving machines were always used in the harshest environments, which can decrease operator comfort. The operator seat can reduce the effect of shock and vibration on the operator and provide him with stable support so that he can control the machine effectively. Based on the laboratory evaluation standard and test methods, a set of electro-hydraulic servo operator seat vibration test device and its control system were developed independently in this paper. The system configures WinTest10 all-digital hydraulic servo controller with software system. The software consists of two main parts: Winquick vibration control software and Ranvib random vibration signal processing software. Through the seat vibration test, the seat effective amplitude transmission rate (SEAT) factor and the transmissibility at resonance were obtained. The results provide the basis for the seat vibration-laboratory evaluation. The research indicates that the system is of great significance for the evaluation of operator seat vibration and seat optimization development.","2152-7431;2152-744X","978-1-4673-1278-3978-1-4673-1275-2978-1-4673-1277","10.1109/ICMA.2012.6283401","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6283401","vibration test;the transmissibility at resonance;operator seat;earth-moving machine","Vibrations;Standards;Resonant frequency;Damping;Software;Suspensions;Machinery","control engineering computing;earthmoving equipment;electrohydraulic control equipment;optimisation;seats;shock control;signal processing;vibration control","earth-moving machinery;control system;earth-moving machines;harshest environments;operator comfort;shock;laboratory evaluation standard;laboratory test methods;electro-hydraulic servo operator seat vibration test device;WinTest10 all-digital hydraulic servo controller;software system;Winquick vibration control software;Ranvib random vibration signal processing software;seat effective amplitude transmission rate factor;SEAT factor;seat vibration-laboratory evaluation;seat optimization development","","","6","","","","","","IEEE","IEEE Conferences"
"The Design of On-site PD Test Circuit for High-Voltage Reactor Based on the Series Resonant Theory","L. Ji; G. Mao; W. Chen; Z. Zhang; Y. Zhang","NA; NA; NA; NA; NA","2010 Asia-Pacific Power and Energy Engineering Conference","","2010","","","1","4","The on-site PD testing of high-voltage reactor is necessary to impose its rated voltage. Owing to the size and weight of the ordinary frequency test equipment, its transportation, installation and adjustment is very difficult. The series resonant test device can be used as an effective solution. In this paper, a PD test circuit of high-voltage reactor based on the series resonant theory is designed. First, the frequency-dependent impedance (30-300 Hz) of the high-voltage reactor is calculated by the finite element analysis software. By the comparing of the calculated results with the testing results, this method has been proved to be correct. According to the results, the appropriate parameters of resonant capacitor and compensation capacitor can be identified; then the sensitivity analysis model is established, and the sensitivity of the circuits is calculated by means of the network method to optimize the parameters of high-voltage filter and the detect impedance. Finally, examples are given to verify the method presented in the text, which provides an important basis for the circuit design and parameter determination of high-voltage reactor's PD test.","2157-4839;2157-4847","978-1-4244-4812-8978-1-4244-4813","10.1109/APPEEC.2010.5449360","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5449360","","Circuit testing;RLC circuits;Inductors;Resonance;Frequency;Impedance;Capacitors;Voltage;Test equipment;Transportation","finite element analysis;partial discharges;power engineering computing;reactors (electric);sensitivity analysis;test equipment","on-site PD test circuit;high-voltage reactor;series resonant theory;ordinary frequency test equipment;frequency-dependent impedance;finite element analysis software;resonant capacitor;compensation capacitor;sensitivity analysis model","","","10","","","","","","IEEE","IEEE Conferences"
"Automated testing of embedded automotive systems from requirement specification models","S. Siegl; K. Hielscher; R. German; C. Berger","Department of Computer Sciences, University Erlangen-Nuremberg, Germany; Department of Computer Sciences, University Erlangen-Nuremberg, Germany; Department of Computer Sciences, University Erlangen-Nuremberg, Germany; Automotive Safety Technologies GmbH, Ingolstadt, Germany","2011 12th Latin American Test Workshop (LATW)","","2011","","","1","6","Embedded software for modern automotive and avionic systems is increasingly complex. In early design phases, even when there is still uncertainty about the feasibility of the requirements, valuable information can be gained from models that describe the expected usage and the desired system reaction. The generation of test cases from these models indicates the feasibility of the intended solution and helps to identify scenarios for which the realization is hardly feasible or the intended system behavior is not properly defined. In this paper we present the formalization of requirements by models to simulate the expected field usage of a system. These so called usage models can be enriched by information about the desired system reaction. Thus, they are the basis for all subsequent testing activities: First, they can be used to verify the first implementation models and design decisions w.r.t. the fulfillment of requirements and second, test cases can be derived in a random or statistic manner. The generation can be controlled with operational profiles that describe different classes of field usage. We have applied our approach at a large German car manufacturer in the early development phase of active safety functionalities. Test cases were generated from the usage models to assess the implementation models in MATLAB/Simulink. The parametrization of the systems could be optimized and a faulty transition in the implementation models was revealed. These design and implementation faults had not been discovered with the established test method.","2373-0862","978-1-4577-1490-0978-1-4577-1489-4978-1-4577-1488","10.1109/LATW.2011.5985928","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5985928","","Safety;Testing;Timing;MATLAB;Belts;Clocks","automotive engineering;avionics;embedded systems;formal specification;program testing;software fault tolerance","automated testing;embedded automotive system;requirement specification model;embedded software;avionic system;early design phase;intended system;requirement formalization;German car manufacturer;safety functionality;test case;MATLAB-Simulink;faulty transition","","1","21","","","","","","IEEE","IEEE Conferences"
"Ramp Rate Constrained Unit Commitment by Improved Priority List and Enhanced Particle Swarm Optimization","W. Ge","NA","2010 International Conference on Computational Intelligence and Software Engineering","","2010","","","1","8","This paper proposes a new approach to solve ramp rate constrained unit commitment (RUC) problem by improving the method of particle swarm optimization, namely improved priority list and enhanced particle swarm optimization (IPL-EPSO). The IPL-EPSO proposed in this paper is a combination of improved priority list (IPL) and enhanced particle swarm optimization (EPSO), which decomposes UC problem into two sub-optimization problems and solves them respectively. The IPL is applied to solve unit scheduling problem, considering power balance constraint, system reverse constraint, start-up/shut-down ramp rate constraint, operation ramp rate constraint and minimum up/down-time constraint, and hence the EPSO is used to solve ramp rate constrained economic dispatch (RED) problem, in order to provide specific solutions satisfying power balance constraint and ramp rate constraint. Such cooperation fully presents not only the advantage of intelligence algorithm for addressing NP-hard problem, but also the guiding effects of human knowledge, reducing the complexity of computation. Problem formulation, representation, parameter testing and the final simulation results for 10, 20, 40 generator-scheduling problems are represented. Results clearly show that IPL-EPSO is very competent in solving the UC problem in comparison to other existing methods using PSO or IPL.","","978-1-4244-5391-7978-1-4244-5392","10.1109/CISE.2010.5677043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5677043","","Spinning;Generators;Indexes;Particle swarm optimization;Production;Economics;Hopfield neural networks","particle swarm optimisation;power generation dispatch;power generation economics;power generation scheduling","ramp rate constrained unit commitment;particle swarm optimization;IPL-EPSO;improved priority list;unit scheduling problem;ramp rate constrained economic dispatch;NP-hard problem","","5","","","","","","","IEEE","IEEE Conferences"
"Accurate source-level simulation of embedded software with respect to compiler optimizations","Z. Wang; J. Henkel","Karlsruhe Institute of Technology, Embedded Sytems, Germany; Karlsruhe Institute of Technology, Embedded Sytems, Germany","2012 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2012","","","382","387","Source code instrumentation is a widely used method to generate fast software simulation models by annotating timing information into application source code. Source-level simulation models can be easily integrated into SystemC based simulation environment for fast simulation of complex multiprocessor systems. The accurate back-annotation of the timing information relies on the mapping between source code and binary code. The compiler optimizations might make it hard to get accurate mapping information. This paper addresses the mapping problems caused by complex compiler optimizations, which are the main source of simulation errors. To obtain accurate mapping information, we propose a method called fine-grained flow mapping that establishes a mapping between sequences of control flow of source code and binary code. In case that the code structure of a program is heavily altered by compiler optimizations, we propose to replace the altered part of the source code with functionally-equivalent IR-level code which has an optimized structure, leading to Partly Optimized Source Code (POSC). Then the flow mapping can be established between the POSC and the binary code and the timing information is back-annotated to the POSC. Our experiments demonstrate the accuracy and speed of simulation models generated by our approach.","1558-1101;1530-1591;1530-1591","978-1-4577-2145-8978-3-9810801-8","10.1109/DATE.2012.6176501","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6176501","","Timing;Optimization;Binary codes;Accuracy;Instruments;Debugging;Transforms","C++ language;digital simulation;multiprocessing systems;program compilers","source-level simulation model;embedded software;compiler optimizations;source code instrumentation;software simulation models;timing information back-annotation;application source code;SystemC based simulation environment;complex multiprocessor system simulation;source code-binary code mapping;fine-grained flow mapping;source code-binary code control flow;IR-level code;partly optimized source code;POSC","","3","14","","","","","","IEEE","IEEE Conferences"
"An Interactive Programming Platform","Y. Ye; L. Hongxing","NA; NA","2011 First International Workshop on Complexity and Data Mining","","2011","","","18","23","The optimization softwares, on linear programming and nonlinear programming, are making much more difference in social, economic and engineering fields. Most current optimization softwares usually either employ classic local optimization techniques or adopt some global optimization techniques. In this paper, an interactive optimization software with a specially designed evolutionary algorithm, named an interactive optimization platform, is introduced. The key procedure of the platform design is analyzed, and the scheme of the platform is proposed, and finally the platform is implemented. The features of the designed platform include: (1) dynamically adjusting the penalty factor related to the extended objective function at interface, (2) using only two specially designed interactive evolution operators, (3) having a particular mechanism of parallel computing, (4) and a going-back mechanism. With several tests, it was indicated that the implemented platform is easy to use, and can achieve precise solutions stably, having outstanding advantages, and so, the proposed scheme is satisfactory.","","978-1-4577-2007-9978-0-7695-4585","10.1109/IWCDM.2011.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6128408","evolutionary algorithm;genetic algorithm;platform;optimization;mathematical programming","Optimization;Evolutionary computation;Software;Programming;Graphical user interfaces;Software algorithms;Testing","evolutionary computation;interactive programming;nonlinear programming;parallel processing","interactive programming platform;nonlinear programming;global optimization technique;interactive optimization software;evolutionary algorithm;penalty factor;interactive evolution operators;parallel computing;going-back mechanism","","","6","","","","","","IEEE","IEEE Conferences"
"Sequential DE enhanced by neighborhood search for Large Scale Global Optimization","H. Wang; Z. Wu; S. Rahnamayan; D. Jiang","State Key Laboratory of Software Engineering, Wuhan University, Wuhan, 430072 China; State Key Laboratory of Software Engineering, Wuhan University, Wuhan, 430072 China; Faculty of Engineering and Applied Science, University of Ontario Institute of Technology (UOIT), 2000 Simcoe Street North, Oshawa, ON L1H 7K4, Canada; Department of Computer Science, Shantou University, Shantou 515063 China","IEEE Congress on Evolutionary Computation","","2010","","","1","7","In this paper, the performance of a sequential Differential Evolution (DE) enhanced by neighborhood search (SDENS) is reported on the set of benchmark functions provided for the CEC2010 Special Session on Large Scale Global Optimization. The original DENS was proposed in our previous work, which differs from existing works which are utilizing the neighborhood search in DE, such as DE with neighborhood search (NSDE) and self-adaptive DE with neighborhood search (SaNSDE). In SDENS, we focus on searching the neighbors of individuals, while the latter two algorithms (NSDE and SaNSDE) work on the adaption of the control parameters F and CR. The proposed algorithm consists of two following main steps. First, for each individual, we create two trial individuals by local and global neighborhood search strategies. Second, we select the fittest one among the current individual and the two created trial individuals as a new current individual. Additionally, sequential DE (DE with one-array) is used as a parent algorithm to accelerate the convergence speed in large scale search spaces. The simulation results for twenty benchmark functions with dimensionality of one thousand are reported.","1089-778X;1941-0026","978-1-4244-6911-6978-1-4244-6909-3978-1-4244-6910","10.1109/CEC.2010.5586358","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5586358","Differential evolution;neighborhood search;local search;large scale global optimization;high dimensional","Optimization;Benchmark testing;Convergence;Chromium;Particle swarm optimization;Search problems;Topology","optimisation;query formulation;search problems","large scale global optimization;sequential differential evolution;CEC2010 special session;self-adaptive differential evolution;global neighborhood search strategies","","15","25","","","","","","IEEE","IEEE Conferences"
"A Multi-Objective Genetic Algorithm to Test Data Generation","G. H. L. Pinto; S. R. Vergilio","NA; NA","2010 22nd IEEE International Conference on Tools with Artificial Intelligence","","2010","1","","129","134","Evolutionary testing has successfully applied search based optimization algorithms to the test data generation problem. The existing works use different techniques and fitness functions. However, the used functions consider only one objective, which is, in general, related to the coverage of a testing criterion. But, in practice, there are many factors that can influence the generation of test data, such as memory consumption, execution time, revealed faults, and etc. Considering this fact, this work explores a multiobjective optimization approach for test data generation. A framework that implements a multi-objective genetic algorithm is described. Two different representations for the population are used, which allows the test of procedural and object-oriented code. Combinations of three objectives are experimentally evaluated: coverage of structural test criteria, ability to reveal faults, and execution time.","2375-0197;1082-3409","978-1-4244-8817","10.1109/ICTAI.2010.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5670025","","Context;Genetics;Testing;Java;Memory management;Software;Optimization","genetic algorithms;program testing","evolutionary testing;search based optimization;test data generation;fitness function;memory consumption;execution time;multiobjective optimization;multiobjective genetic algorithm;population representation;object-oriented code;structural test criteria","","14","32","","","","","","IEEE","IEEE Conferences"
"Interactive fault localization leveraging simple user feedback","L. Gong; D. Lo; L. Jiang; H. Zhang","School of Software, Tsinghua University, Beijing 100084, China, Tsinghua National Laboratory for Information Science and Technology (TNList); School of Information Systems, Singapore Management University, Singapore; School of Information Systems, Singapore Management University, Singapore; School of Software, Tsinghua University, Beijing 100084, China, Tsinghua National Laboratory for Information Science and Technology (TNList)","2012 28th IEEE International Conference on Software Maintenance (ICSM)","","2012","","","67","76","Many fault localization methods have been proposed in the literature. These methods take in a set of program execution profiles and output a list of suspicious program elements. The list of program elements ranked by their suspiciousness is then presented to developers for manual inspection. Currently, the suspicious elements are ranked in a batch process where developers' inspection efforts are rarely utilized for ranking. The inaccuracy and static nature of existing fault localization methods prompt us to incorporate user feedback to improve the accuracy of the existing methods. In this paper, we propose an interactive fault localization framework that leverages simple user feedback. Our framework only needs users to label the statements examined as faulty or clean, which does not require additional effort than conventional non-interactive methods. After users label suspicious program elements as faulty or clean, our framework incorporates such information and re-orders the rest of the suspicious program elements, aiming to expose truly faulty elements earlier. We have integrated our solution with three well-known fault localization methods: Ochiai, Tarantula, and Jaccard. The evaluation on five Unix programs and the Siemens test suite shows that our solution achieves significant improvements on fault localization accuracy.","1063-6773","978-1-4673-2312-3978-1-4673-2313","10.1109/ICSM.2012.6405255","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405255","","Inspection;Debugging;Mathematical model;Accuracy;Conferences;Software maintenance","program testing;software fault tolerance;user interfaces","interactive fault localization;simple user feedback;fault localization method;program execution profile;program element ranking;suspicious element;faulty statement;clean statement;Ochiai method;Tarantula method;Jaccard method;Unix program;Siemens test suite","","10","31","","","","","","IEEE","IEEE Conferences"
"Towards reliable web applications: ISO 19761","M. A. Talib; E. Mendes; A. Khelifi","College of Information Technology, Zayed University, UAE; College of Information Technology, Zayed University, UAE; Software Engineering Department, ALHOSN University, UAE","IECON 2012 - 38th Annual Conference on IEEE Industrial Electronics Society","","2012","","","3144","3148","This research adopts a new scenario-based black box testing methodology for testing web applications. It combines a black box testing strategy with the functions (scenarios) measured by the COSMIC-FFP measurement procedure (ISO/IEC 19761 standard) to produce an optimal set of test cases. This testing approach shows its applicability during all the development phases. Moreover, it can be applied during the early development phase once the specifications have been documented as well as after the development phase where we don't have the access to the code. This paper also considers the use of a functional complexity measure for assigning priorities to the generated test cases. Finally, those concepts have been applied on part of Online Banking System as a case study.","1553-572X;1553-572X","978-1-4673-2421-2978-1-4673-2419-9978-1-4673-2420","10.1109/IECON.2012.6389396","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6389396","Web applications testing;COSMIC-FFP;ISO 19761","ISO;Indexes;Ice;IEC standards","IEC standards;Internet;ISO standards;program testing;software metrics","Web application testing;scenario-based black box testing;COSMIC-FFP measurement procedure;ISO/IEC 19761 standard;functional complexity measure;online banking system","","1","12","","","","","","IEEE","IEEE Conferences"
"Optimal load economic distribution research and engineering realization","Huang yu; Han Pu; Zhao Yun; Li Yong-ling; Wang Dong-feng","Hebei Engineering Research Center of Simulation Optimized Control for Power Generation, North China Electric Power University, Baoding, 071003, China; Hebei Engineering Research Center of Simulation Optimized Control for Power Generation, North China Electric Power University, Baoding, 071003, China; Hebei Engineering Research Center of Simulation Optimized Control for Power Generation, North China Electric Power University, Baoding, 071003, China; Hebei Engineering Research Center of Simulation Optimized Control for Power Generation, North China Electric Power University, Baoding, 071003, China; Hebei Engineering Research Center of Simulation Optimized Control for Power Generation, North China Electric Power University, Baoding, 071003, China","2012 24th Chinese Control and Decision Conference (CCDC)","","2012","","","1990","1995","Nowadays, the difference between peak and lowest of load in electricity market is increasing. Central command requires the power plant to complete the load demand quickly. For this situation, we use particle swarm optimization to solve load economic distribution. We change the weight between speed and economy within the plant, taking into account the auxiliary running. Make the power plant keep up with the central command and keep the unit running at lower coal consumption. Tests show that: Our algorithm is stable and reliable. And we have developed thermal power plant load distribution software, which has been put into commercial operation.","1948-9439;1948-9447","978-1-4577-2074-1978-1-4577-2073-4978-1-4577-2072","10.1109/CCDC.2012.6244321","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6244321","Optimal Load distribution;Particle Swarm Optimization;Mathematical Model;Engineering Realization","Coal;Power generation;Mathematical model;Fitting;Equations;Curve fitting;Standards","electricity supply industry;load distribution;particle swarm optimisation;power engineering computing","optimal load economic distribution research;engineering realization;electricity market load;particle swarm optimization;thermal power plant load distribution software","","","10","","","","","","IEEE","IEEE Conferences"
"A UML Model Based White Box Reliability Prediction to Identify Unreliable Components","D. Hong; T. Gu; J. Baik","NA; NA; NA","2011 Fifth International Conference on Secure Software Integration and Reliability Improvement - Companion","","2011","","","152","159","In general, existing reliability analysis techniques have a limitation in that they normally require run-time information to assess a software system's reliability. So, reliability analysis has been delayed until system testing or operation where principal design decisions had been already made. Also, they normally focus on estimating the reliability of the whole system rather than finding out unreliable sources or components. In this paper, we propose a UML model based white box reliability prediction methodology. The proposed methodology provides the capability of reliability prediction in the early phase by utilizing design artifacts like UML model, and by analyzing the probability of failure in three different levels of the system. Hence, it makes possible to rank the unreliable parts of the system quantitatively not only at the component level but also at the user's system usage level and system's activity level. In order to validate the proposed method, we performed a case study with safety-critical embedded software and compare the results with traditional reliability models. The result of proposed methodology shows the better prediction accuracy as well as the more detailed analysis of the unreliable part of the system.","","978-1-4577-0781-0978-0-7695-4454","10.1109/SSIRI-C.2011.30","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004517","Software reliability;Architecture based reliability;software reliability prediction;White-box Reliability Model;Component based reliability;UML model","Software reliability;Unified modeling language;Software;Insulin;Probability;Predictive models","formal specification;object-oriented programming;probability;safety-critical software;systems analysis;Unified Modeling Language","UML model;white box reliability prediction;unreliable component identify;reliability analysis;run-time information;software system reliability assessment;system testing;design artifacts;failure probability;user system usage level;system activity level;safety-critical embedded software;system analysis","","4","16","","","","","","IEEE","IEEE Conferences"
"Reliability Analysis of Multi-Stress Accelerated Life Test Based on BP Neural Network","X. Wang; J. Shen","NA; NA","2012 Fifth International Conference on Business Intelligence and Financial Engineering","","2012","","","6","9","To construct multi-stress accelerated life model, the traditional method utilizes the observed value acquired in the process of accelerated life test to build likelihood equations of accelerated model, however, the excessive parameters of multi-stress accelerated model will lead to the difficulty in solving pluralism likelihood equations. Based on the predictability and convergence of genetic algorithm optimum BP neural network, the multi-stress accelerated life model of genetic neural network is built. Take the level and reliability of accelerated stress in the accelerated life test as neural network input, then draw the scatter diagram by software and use the nonlinear least square to fit raw data to obtain the regression equation. Consequently, generate large quantities of test data, which shall be input into the neural network and optimize the weight and threshold value by genetic algorithm. By this means, conquer the blindness in selecting the original weight and threshold value. Finally, input constant stress and set reliability into well-trained network, thus get the predicting curve of reliability. Simulation results show that the method above is efficient and practical.","","978-1-4673-2092-4978-0-7695-4750","10.1109/BIFE.2012.10","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6305068","Reliability;Accelerated Life Test;BP Neural Network;Genetic Algorithm","Stress;Life estimation;Mathematical model;Neural networks;Reliability;Genetic algorithms;Equations","backpropagation;convergence of numerical methods;genetic algorithms;least squares approximations;life testing;mechanical engineering computing;neural nets;nonlinear equations;product life cycle management;regression analysis;reliability;stress analysis","multistress accelerated life test;pluralism likelihood equations;genetic algorithm optimum BP neural network;convergence;predictability;scatter diagram;nonlinear least squares;regression equation;weight optimization;threshold value optimization;constant stress;reliability predicting curve","","","7","","","","","","IEEE","IEEE Conferences"
"A software update service with self-protection capabilities","M. Neukirchner; S. Stein; H. Schrom; R. Ernst","Institut für Datentechnik und Kommunikationsnetze, Technische Universität Braunschweig, Germany; Institut für Datentechnik und Kommunikationsnetze, Technische Universität Braunschweig, Germany; Institut für Datentechnik und Kommunikationsnetze, Technische Universität Braunschweig, Germany; Institut für Datentechnik und Kommunikationsnetze, Technische Universität Braunschweig, Germany","2010 Design, Automation & Test in Europe Conference & Exhibition (DATE 2010)","","2010","","","903","908","Integration of system components is a crucial challenge in the design of embedded real-time systems, as complex non-functional interdependencies may exist. We propose a software update service with self-protection capabilities against unverified system updates - thus solving the integration problem in-system. As modern embedded systems may evolve through software updates, component replacement or even self-optimization, possible system configurations are hard to predict. Thus the designer of system updates does not know the exact system configuration. This turns the proof of system feasibility into a critical challenge. This paper presents the architecture of a framework and associated protocols enabling updates in embedded systems while ensuring safe operation w.r.t. non-functional properties. The proposed process employs contract based principles at the interfaces towards applications to perform an in-system verification. Practical feasibility of our approach is demonstrated by an implementation of the update process, which is analyzed w.r.t. the memory consumption overhead and execution time.","1558-1101;1530-1591;1530-1591","978-3-9810801-6-2978-1-4244-7054-9978-3-9810801-6","10.1109/DATE.2010.5456925","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5456925","","Embedded system;Contracts;Testing;Embedded software;Real time systems;Protocols;Resource management;Automotive engineering;Computer architecture;Runtime","embedded systems;optimisation;software engineering","software update service;self protection capabilities;embedded real time systems;component replacement;self optimization;system feasibility proof","","5","21","","","","","","IEEE","IEEE Conferences"
"The ant colony Optimization Algorithm for web services composition on Preference Ontology","Yamei Xia; Chen Liu; Zhengqiu Yang; Jiapeng Xiu","Software Institute, Beijing University of Posts and Telecommunications, 100876, China; Software Institute, Beijing University of Posts and Telecommunications, 100876, China; Beijing Key Laboratory of Intelligent Telecommunications Software and Multimedia, Beijing University of Posts and Telecommunications, 100876, China; Beijing Key Laboratory of Intelligent Telecommunications Software and Multimedia, Beijing University of Posts and Telecommunications, 100876, China","2011 International Conference on Advanced Intelligence and Awareness Internet (AIAI 2011)","","2011","","","193","198","The Optimization Algorithm for Web Services Composition on Preference Ontology (OAWSCP) is put forward. OAWSCP, which makes some improvements on primary ACO (Ant Colony Optimization), builds simulation model based on services composition, and sets multiple pheromones and pheromone weights to denote the preference to different properties of a service. The algorithm can also simulate the instability of the flow of services composition, and react according to the flow change of the services composition. The algorithm can also detect if the optimizing is converging to local optimization findings, and in this case the algorithm can take measures to change its direction, and as a result reduce the probability of the algorithm to converge to local optimization findings. In order to verify the feasibility of the algorithm, the paper also builds simulation application system. The result of the performance test proves that the algorithm is more effective than primary ACO.","","978-1-84919-471","10.1049/cp.2011.1455","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6233224","Semantic Web;Services Composition;Ontology;Ant Colony Optimizatio","","ant colony optimisation;ontologies (artificial intelligence);probability;semantic Web;Web services","ant colony optimization algorithm;Web services composition;preference ontology;OAWSCP;ACO;local optimization findings;probability reduction;simulation application system;semantic Web","","","","","","","","","IET","IET Conferences"
"A Nonlinear Array Subscripts Dependence Test","Z. Jie; Z. Rongcai; H. Lin","NA; NA; NA","2012 IEEE 14th International Conference on High Performance Computing and Communication & 2012 IEEE 9th International Conference on Embedded Software and Systems","","2012","","","764","771","Linear dependence tests determine the dependences with linear array subscripts, but only give the passive results for those with nonlinear ones. That is to say, dependences exist as long as there are nonlinear cases, which may lead to pseudo-dependences. However, to maximize the parallelism of applications and improve the credibility of the optimizing compiler, it is necessary to develop a nonlinear dependence test to eliminate the pseudo-dependences. By analyzing the optimal solution of the quadratic subscripts with the indexes bounds constraints, a new nonlinear dependence test was proposed. We theoretically proved that the nonlinear dependences satisfying the quadratic programming model can be determined, and introduced a nonlinear dependence testing algorithm based on quadratic programming. Effectiveness of this algorithm was verified at the end.","","978-1-4673-2164-8978-0-7695-4749","10.1109/HPCC.2012.108","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6332246","dependence testing;nonlinear array subscripts;pseudo-dependences;quadratic programming;positive semi-definite matrices","Arrays;Quadratic programming;Mathematical model;Equations;Testing;Symmetric matrices;Indexes","parallel processing;program compilers;program testing;quadratic programming","nonlinear array subscripts dependence test;linear dependence tests;linear array subscripts;nonlinear cases;compiler optimization;pseudodependences elimination;optimal solution;quadratic subscripts;indexes bounds constraints;quadratic programming model;nonlinear dependence testing algorithm","","1","29","","","","","","IEEE","IEEE Conferences"
"Optimization of Thorax-Pelvic Airbag for Occupant Safety in Vehicle Side Impact","J. Sun; J. Yang","NA; NA","2012 Third International Conference on Digital Manufacturing & Automation","","2012","","","586","589","Based on the C-NCAP side impact test results and component tests of a type of car, two typical type of pelvic-thorax airbag sub-models was developed using MADYMO and LS-DYNA software. They are single-chamber airbag with one inflator and dual-chamber airbag with two inflators. Based on the analysis of parameters of the single-chamber airbag, the factors that influence the dual-chamber airbag are determined. The protection performance of those side airbags were investigated using orthogonal experiment design (OED) and comprehensive equilibrium methods. The models with better performance were found and factors that influence the two different side airbag were determined. The injury of the dummy decreased dramatically in side impact and met the C-NCAP requirement.","","978-1-4673-2217-1978-0-7695-4772","10.1109/ICDMA.2012.139","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6298586","side impact;thorax-pelvic airbag;C-NCAP;MADYMO","Vehicles;Atmospheric modeling;Injuries;Force;Acceleration;Deformable models;Bones","automotive components;design of experiments;impact (mechanical);impact testing;inflatable structures;optimisation;road safety;vehicle dynamics","thorax-pelvic airbag;occupant safety;vehicle side impact;C-NCAP side impact test;component tests;car;pelvic-thorax airbag submodel;MADYMO software;LS-DYNA software;single-chamber airbag;inflator;dual-chamber airbag;airbag protection performance;orthogonal experiment design;comprehensive equilibrium methods;OED","","","6","","","","","","IEEE","IEEE Conferences"
"Using Mutation to Automatically Suggest Fixes for Faulty Programs","V. Debroy; W. E. Wong","NA; NA","2010 Third International Conference on Software Testing, Verification and Validation","","2010","","","65","74","This paper proposes a strategy for automatically fixing faults in a program by combining the processes of mutation and fault localization. Statements that are ranked in order of their suspiciousness of containing faults can then be mutated in the same order to produce possible fixes for the faulty program. The proposed strategy is evaluated against the seven benchmark programs of the Siemens suite and the Ant program. Results indicate that the strategy is effective at automatically suggesting fixes for faults without any human intervention.","2159-4848","978-1-4244-6436-4978-1-4244-6435-7978-0-7695-3990","10.1109/ICST.2010.66","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477098","program debugging;mutation;fault localization;fault-fixing;software testing","Genetic mutations;Programming profession;Debugging;Software testing;Humans;Fault detection;Computer science;Benchmark testing;Explosives;Manuals","fault tolerant computing;program debugging;program testing","faulty program;mutation process;fault localization;Siemens suite;Ant program","","63","38","","","","","","IEEE","IEEE Conferences"
"Development of Impact Test System for Waterjet Descaling Nozzles with LabVIEW","Y. Liu; F. Ma; H. Xie; Y. Li","NA; NA; NA; NA","2010 International Conference on Web Information Systems and Mining","","2010","1","","3","7","High-pressure water jet technology is widely used in hot-rolling billet descaling process. Impact force and behavior of jets play a vital role on effect of water jet billet descaling. The parameters of nozzles and jet determine the characteristics of jetting. Some of these parameters, however, can only be obtained through experimentation. The mechanism of high-pressure water jet billet descaling was analyzed in this paper. The factors affecting billet descaling were also discussed. Furthermore, theoretical formula accounting for water jet impact force was derived from the discussion. An indoor test set-up investigating impact properties of water jet descaling nozzles was established. Virtual instrument Lab VIEW was combined with mathematics software MATLAB to develop a measuring system of the test set-up. The real-time test of descaling nozzles parameters can be achieved through the system. The measuring system included three modules: pressure-flow curve, three-dimensional platform movement track and jet impact force measurement. Various parameters of the nozzles and jet can be obtained by means of the modules. Experimental results were compared with theoretical ones and observations were made regarding the variation of parameters. Results showed that variation of the parameters yielded by experiments agreed well with theoretical prediction. Variation of each parameter was visually depicted. The impact force and behavior of the jet were accurately measured. It contributes to descaling nozzle design improvement and descaling system structure optimization.","","978-1-4244-8438","10.1109/WISM.2010.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5662250","high-pressure waterjet;descaling nozzle;test system;virtual instrument","Force;Billets;Testing;Skin;Iron;Surface treatment;Tracking","billets;hot rolling;impact (mechanical);impact testing;jets;mathematics computing;mechanical engineering computing;nozzles;production engineering computing;virtual instrumentation","impact test system;hot rolling billet descaling process;waterjet impact force;virtual instrument;Matlab;waterjet descaling nozzles;LabVIEW","","4","13","","","","","","IEEE","IEEE Conferences"
"Peer-to-Peer Driven Software Engineering Considering Security, Reliability, and Performance","K. Beckers; S. Faßbender","NA; NA","2012 Seventh International Conference on Availability, Reliability and Security","","2012","","","485","494","Internet-scale applications require scalability that peer-to-peer (P2P) architectures provide. Traditional software engineering processes start with requirements and move onto architectures, software design, implementation, and testing. Choosing a P2P architecture, however, has significant constraints on the requirements of a given software engineering process in terms of security, reliability and performance. In addition, requirements for P2P architectures have to be expressed in notions of network engineering, because these architectures rank from the application layer to the IP networklayer. Thus, engineering P2P systems is a cross-disciplinary task between software and network engineers. We explain the ramifications P2P applications have on requirements of a given software engineering problem. A structured method supports software engineers in understanding the constraints of different kinds of P2P architectures and protocols on requirements. In addition, we present patterns of how the requirements have to be expressed, so that they contain all required information for network engineers that implementthe P2P architectures.","","978-1-4673-2244-7978-0-7695-4775","10.1109/ARES.2012.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6329221","P2P;security;requirements engineering;software architecture;performance;reliability;network engineering","Peer to peer computing;Quality of service;Computer architecture;Protocols;Reliability;Security;Real-time systems","peer-to-peer computing;security of data;software architecture;software performance evaluation;software reliability","peer-to-peer driven software engineering;P2P architectures;software design;software engineering process;network engineering;software security;software reliability;software performance;protocols","","3","20","","","","","","IEEE","IEEE Conferences"
"Memetic Algorithm with PSO or BFO's chemotaxis mechanism as local search for function optimization","D. G. Jadhav; S. Devi; S. S. Pattnaik","National Institute of Technical Teachers Training & Research (NITTTR), Chandigarh, 160019 India; National Institute of Technical Teachers Training & Research (NITTTR), Chandigarh, 160019 India; National Institute of Technical Teachers Training & Research (NITTTR), Chandigarh, 160019 India","2012 International Conference on Recent Advances in Computing and Software Systems","","2012","","","42","47","Memetic Algorithm (MA) is a metaheuristic search method. In proposed memetic algorithm two memes are used in definite proportion out of total local calls and are applied with random selection. Genetic Algorithm due to its good exploration capability is used as main algorithm and Particle Swarm Optimization (PSO) as well as chemotaxis mechanism of Bacterial Foraging Optimization (BFO) are used as local searches. The memetic process is realized using global best fitness among particles of PSO and by imitating the nutrient information from the bacteria of the best fitness in BFO. The proposed variant of memetic algorithm is tested on the standard benchmark functions with unimodal and multimodal property. When the results are compared, the proposed memetic algorithm shows better performance than MA using PSO (pMA) and MA using BFO (bMA) both in terms of speed of convergence and quality of solutions.","","978-1-4673-0255-5978-1-4673-0252-4978-1-4673-0254","10.1109/RACSS.2012.6212695","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6212695","Hyper-heuristic;multi-meme;Memetic Algorithm (MA);Particle Swarm Optimization (PSO);Bacterial Foraging Optimization (BFO)","Memetics;Genetic algorithms;Convergence;Optimization;Microorganisms;Heuristic algorithms;Standards","cell motility;genetic algorithms;microorganisms;particle swarm optimisation;search problems","memetic algorithm;BFO chemotaxis mechanism;PSO chemotaxis mechanism;local search;function optimization;metaheuristic search method;random selection;genetic algorithm;particle swarm optimization;bacterial foraging optimization;memetic process;nutrient information;standard benchmark functions;multimodal property;unimodal property","","","17","","","","","","IEEE","IEEE Conferences"
"Standard test bench for optimization and characterization of combinational circuits","S. C. Tiwari; M. A. Khan; K. Singh; A. Sangal","N.S.I.T.(University of Delhi), Delhi, India; (C-DAC)(Ministry of Communications and IT, Govt. of India), Noida, UP, India 201 307; Deptt. of EE, Delhi Technological University; Coreel Technologies","2012 IEEE International Conference on Signal Processing, Computing and Control","","2012","","","1","5","Choice of a combinational circuit among large number of circuits having same functionality has been always a complex and time consuming task for digital designers. Different circuits (where they are initially proposed) were optimized using different techniques and objectives. Moreover there merits vary as per optimization methodology and technique variations. Hence every time when there is a requirement of particular functionality circuit, choosing best one amongst available circuits requires re-characterization. The paper presents a thorough investigation of existing optimization techniques while presenting their merits and demerits over each other. Based on same, the paper proposes a standard test bench for optimization and characterization of combinational circuits. Finally using the proposed methodology a combinational circuitry has been successfully characterized.","","978-1-4673-1318-6978-1-4673-1317-9978-1-4673-1316","10.1109/ISPCC.2012.6224346","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224346","Combinational circuit;low power;VLSI;optimization;Logic Effort","Delay;Logic gates;Optimization;Transistors;SPICE;Algorithm design and analysis;Application software","combinational circuits;optimisation","standard test bench;digital designer;optimization methodology;functionality circuit;combinational circuitry","","","19","","","","","","IEEE","IEEE Conferences"
"A Framework to Compare Alert Ranking Algorithms","S. Allier; N. Anquetil; A. Hora; S. Ducasse","NA; NA; NA; NA","2012 19th Working Conference on Reverse Engineering","","2012","","","277","285","To improve software quality, rule checkers statically check if a software contains violations of good programming practices. On a real sized system, the alerts (rule violations detected by the tool) may be numbered by the thousands. Unfortunately, these tools generate a high proportion of ""false alerts"", which in the context of a specific software, should not be fixed. Huge numbers of false alerts may render impossible the finding and correction of ""true alerts"" and dissuade developers from using these tools. In order to overcome this problem, the literature provides different ranking methods that aim at computing the probability of an alert being a ""true one"". In this paper, we propose a framework for comparing these ranking algorithms and identify the best approach to rank alerts. We have selected six algorithms described in literature. For comparison, we use a benchmark covering two programming languages (Java and Smalltalk) and three rule checkers (Find Bug, PMD, Small Lint). Results show that the best ranking methods are based on the history of past alerts and their location. We could not identify any significant advantage in using statistical tools such as linear regression or Bayesian networks or ad-hoc methods.","2375-5369;1095-1350","978-0-7695-4891-3978-1-4673-4536","10.1109/WCRE.2012.37","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6385123","","Java;Benchmark testing;Measurement;Training;History;Open source software","Java;program debugging;Smalltalk;software quality;software tools","alert ranking algorithm;software quality;rule violation detection;false alert generation;true alerts;probability;Java programming language;Smalltalk programming language;FindBug rule checker;PMD rule checker;SmallLint rule checker","","4","38","","","","","","IEEE","IEEE Conferences"
"Research on auto-composing test paper system based on improved genetic algorithm","Li Dongmei; Huang Xiantong; Yang Xinfeng; Jiao Xiaoxian","Department of Computer Science and Technology, Nanyang Institute of Technology, China; Department of Computer Science and Technology, Nanyang Institute of Technology, China; Department of Computer Science and Technology, Nanyang Institute of Technology, China; Information and Engineering Department, Henan School of Economics and Management, Nanyang, China","2011 IEEE 2nd International Conference on Software Engineering and Service Science","","2011","","","834","837","Comosing test paper is an important part of examination system. Based on the researches on coding policy, fitness faction, genetic operation and control parameter, an improved genetic algorithm is advanced for the auto-composing test paper system. It is an efficient way to overcome the premature convergence and the genetic drifting, and at the same time,to prevent the colony coming into the partial optimal solution considering the colony diversity by scaling the fitness faction and building the adaptive crossover probability and mutation probability. Experiment shows that the improved genetic algorithm cold compose test paper more efficiency.","2327-0586;2327-0594","978-1-4244-9698-3978-1-4244-9699-0978-1-4244-9697","10.1109/ICSESS.2011.5982470","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5982470","genetic algorithm;auto-composing test paper;fitness faction","Genetic algorithms;Genetics;Optimization;Convergence;Encoding;Search problems;Biological cells","genetic algorithms;intelligent tutoring systems","auto-composing test paper system;improved genetic algorithm;examination system;coding policy;fitness faction;genetic operation;control parameter;premature convergence;genetic drifting;colony diversity;adaptive crossover probability;mutation probability","","1","10","","","","","","IEEE","IEEE Conferences"
"Unit testing optimized functions in H.264","Hao Zhang; Yuetang Deng; Zhengye Liu; Yuan Zhao; Haiyan Zhan","School of Information Science and Engineering at Central South University, China; Tencent, Inc. ShenZhen, 518057 China; AT&amp;T Labs, San Ramon, CA 94583 USA; School of Information Science and Engineering at Central South University, China; School of Information Science and Engineering at Central South University, China","2011 International Conference on Multimedia Technology","","2011","","","3297","3300","Unit testing is a very useful but also very time consuming process to verify the correctness of individual functions or procedures. Although unit testing may have already been widely used by video codec engineers, to the best of our knowledge, designing efficient unit test suites for optimized video codec functions has not been systematically studied. Therefore, the authors propose to attack this problem by firstly summarizing various SIMD (Single Instruction, Multiple Data) related bugs based on past experiences. Then, random testing and manually designed test suites design are exploited to detect those bugs. Random test cases would be sampling uniformly from the admissible values and are shown to be effective detecting those bugs. The manually created test suites are specially designed for some type of errors and can guarantee the bug detection as well as locate those bugs.","","978-1-61284-774-0978-1-61284-771-9978-1-61284-773","10.1109/ICMT.2011.6001954","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6001954","video codecs;software testing","Computer bugs;Testing;Video coding;Assembly;Transforms;Quantization;Instruction sets","image sampling;parallel processing;program debugging;video coding","unit testing optimized function;video codec function;SIMD related bug;single instruction multiple data;random testing;sampling;bug detection;H.264 video coding standard","","","17","","","","","","IEEE","IEEE Conferences"
"CIMDS: Adapting Postprocessing Techniques of Associative Classification for Malware Detection","Y. Ye; T. Li; Q. Jiang; Y. Wang","Department of Computer Science, Xiamen University, Xiamen, China; School of Computer Science, Florida International University, Miami, USA; Software School, Xiamen University, Xiamen, China; Software School, Xiamen University, Xiamen, China","IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)","","2010","40","3","298","307","Malware is software designed to infiltrate or damage a computer system without the owner's informed consent (e.g., viruses, backdoors, spyware, trojans, and worms). Nowadays, numerous attacks made by the malware pose a major security threat to computer users. Unfortunately, along with the development of the malware writing techniques, the number of file samples that need to be analyzed, named ""gray list,"" on a daily basis is constantly increasing. In order to help our virus analysts, quickly and efficiently pick out the malicious executables from the ""gray list,"" an automatic and robust tool to analyze and classify the file samples is needed. In our previous work, we have developed an intelligent malware detection system (IMDS) by adopting associative classification method based on the analysis of application programming interface (API) execution calls. Despite its good performance in malware detection, IMDS still faces the following two challenges: (1) handling the large set of the generated rules to build the classifier; and (2) finding effective rules for classifying new file samples. In this paper, we first systematically evaluate the effects of the postprocessing techniques (e.g., rule pruning, rule ranking, and rule selection) of associative classification in malware detection, and then, propose an effective way, i.e., CIDCPF, to detect the malware from the ""gray list."" To the best of our knowledge, this is the first effort on using postprocessing techniques of associative classification in malware detection. CIDCPF adapts the postprocessing techniques as follows: first applying Chi-square testing and Insignificant rule pruning followed by using Database coverage based on the Chi-square measure rule ranking mechanism and Pessimistic error estimation, and finally performing prediction by selecting the best First rule. We have incorporated the CIDCPF method into our existing IMDS system, and we call the new system as CIMDS system. Case studies are performed on the large collection of file samples obtained from the Antivirus Laboratory at Kingsoft Corporation and promising experimental results demonstrate that the efficiency and ability of detecting malware from the ""gray list"" of our CIMDS system outperform popular antivirus software tools, such as McAfee VirusScan and Norton Antivirus, as well as previous data-mining-based detection systems, which employed Naive Bayes, support vector machine, and decision tree techniques. In particular, our CIMDS system can greatly reduce the number of generated rules, which makes it easy for our virus analysts to identify the useful ones.","1094-6977;1558-2442","","10.1109/TSMCC.2009.2037978","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5415575","Associative classification;malware detection;postprocessing;rule pruning;rule ranking;rule selection","Face detection;Software design;Computer viruses;Computer worms;Computer security;Writing;Robustness;Intelligent systems;Testing;Databases","associative processing;data mining;invasive software","intelligent malware detection system;malware writing techniques;computer virus;gray list;associative classification method;application programming interface;postprocessing techniques;Chi-square testing;insignificant rule pruning;rule ranking mechanism;Pessimistic error estimation;data mining;CIMDS system","","37","39","","","","","","IEEE","IEEE Journals & Magazines"
"Realtime diagnostic prognostic solution for life cycle management of thermomechanical system","B. Saxena; A. Kumar; A. Srivastava; A. Goel","University of Ottawa, 800 King Edward Street, Ottawa, ON, K1N 6N5; Tecsis Corporation, 210 Colonnade Road, Ottawa, ON, K2E 7L5; Tecsis Corporation, 210 Colonnade Road, Ottawa, ON, K2E 7L5; OMTEC Inc., Mississauga, ON, L4Z 3V3","2011 24th Canadian Conference on Electrical and Computer Engineering(CCECE)","","2011","","","000999","001003","Data based approach and methodology for real time diagnosis and prognosis solutions for thermomechanical systems is discussed. Coated turbine blade operation is emulated to sensor online temperature data as the real-time inputs for the software code developed. An algorithm is presented first and extended sampling based statistical hypothesis tests are used for anomaly detection tests. Paired t-test and rank sum hypotheses test are found to be appropriate for different data set combinations. Matlab statistical package is used for the software code. The algorithm and methodology work well with laboratory temperature data and seeded faults. Few limitations of the developed system code are identified.","0840-7789;0840-7789","978-1-4244-9789-8978-1-4244-9788-1978-1-4244-9787","10.1109/CCECE.2011.6030610","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6030610","Temperature;diagnosis;prognosis;anomaly;t-test;reference","Temperature sensors;Temperature measurement;Temperature distribution;Algorithm design and analysis;Turbines;Real time systems;Engines","blades;condition monitoring;fault diagnosis;mathematics computing;mechanical engineering computing;product life cycle management;sampling methods;software engineering;turbines","realtime diagnostic prognostic solution;thermomechanical system life cycle management;turbine blade operation;sensor online temperature data;software code development;databases;sampling;statistical hypothesis tests;anomaly detection tests;t-test hypotheses;rank sum hypotheses test;Matlab statistical package","","2","12","","","","","","IEEE","IEEE Conferences"
"Optimization of Software Codes for CPU Chip Reliability","J. Shi; Z. Lin; J. Wang","NA; NA; NA","2010 Fifth International Conference on Frontier of Computer Science and Technology","","2010","","","595","599","Through tests under Linux operating system, the fact that codes organized differently lead to different temperature of CPU chip was proved. The concept of optimizing codes for chip's reliability is put forwarded. Through tests and examples, a principle and path is proposed, and a few skills for optimizing codes are given.","2159-6301;2159-631X","978-1-4244-7779","10.1109/FCST.2010.82","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5575503","","Optimization;Reliability;Computers;Central Processing Unit;Instruction sets;Organizations;Sun","integrated circuit reliability;Linux;microprocessor chips","software code optimization;CPU chip reliability;Linux operating system;CPU chip temperature","","","13","","","","","","IEEE","IEEE Conferences"
"Research on health management of automatic test equipment","Wang Guohua; Wang Shuo","School of Instrumentation Science &amp; Opto-electronics Engineering, Beijing University of Aeronautics and Astronautics, China; School of Instrumentation Science &amp; Opto-electronics Engineering, Beijing University of Aeronautics and Astronautics, China","2011 Prognostics and System Health Managment Confernece","","2011","","","1","5","Automatic test equipment (ATE) is the facility to assure aircraft and weapon in well health state, thus, the health state of ATE itself is of vital significance. The connotation of health for ATE is the operation state with both correct function and satisfactory performance at the same time. To realize health management of ATE, the self-test and calibration method is introduced. High light is focused on ATE calibration interval prediction. To predict calibration intervals, a health state modeling and prognostic method was proposed based on Weibull distribution, which can provide an accurate prediction even with small samples. Moreover, cost was considered to optimize calibration intervals. An example of optimizing the calibration schedule of digital multimeter Agilent E1412A is given, which shows that this method is advanced and feasible. At last, an ATE health management implementation was introduced, which includes the traceability of ATE modules and the software architecture. Implementation shows that this method is feasible.","2166-563X;2166-5656","978-1-4244-7950-4978-1-4244-7951-1978-1-4244-7949","10.1109/PHM.2011.5939493","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5939493","automatic test equipment;health management;calibration interval;Weibull distribution","Extraterrestrial measurements;Calibration;Predictive models;Schedules;Software;Built-in self-test","aerospace computing;aircraft testing;automatic test equipment;calibration;maintenance engineering;Weibull distribution","automatic test equipment;aircraft;ATE health management;ATE calibration interval prediction;Weibull distribution;digital multimeter agilent E1412A;ATE modules;software architecture","","","10","","","","","","IEEE","IEEE Conferences"
"Multi-Objective Optimization of Double-Folded Yagi Antenna Using Genetic Algorithms","B. Yuan; X. Wang","NA; NA","2010 International Conference on Computational Intelligence and Software Engineering","","2010","","","1","4","The novel compact folded Yagi antenna, for its high gain and superiority in small size, is being interested in various electromagnetic application areas. But for its complex structure, as well as the strong coupling among the elements, making the optimization and design rather difficult. In this paper, the impact of multi-scale semi-solutions to the final performance of antenna is taken into account, a genetic algorithm applied to the multi-objective optimization of folded Yagi antenna is realized. To illustrate the feasibility and effectiveness of the algorithm, a three-element double-folded Yagi antenna is designed and fabricated, the measurements show good results in gain (7.40dBi) and perfect match in impedance (49.87-j0.49Ω), which fully verify the reliability of the algorithm.","","978-1-4244-5391-7978-1-4244-5392","10.1109/CISE.2010.5677016","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5677016","","Optimization;Antenna measurements;Antenna arrays;Impedance;Algorithm design and analysis","antenna testing;electromagnetic waves;genetic algorithms;Yagi antenna arrays","double-folded Yagi antenna;multiobjective optimization;genetic algorithms;compact folded Yagi antenna;electromagnetic application;multiscale semisolutions;perfect impedance match;reliability","","","6","","","","","","IEEE","IEEE Conferences"
"Optimization Design of Closed-loop Supply Chain Based on MILP Model","Z. Peilin; Y. Long; Y. Ling","NA; NA; NA","2011 International Conference on Internet Computing and Information Services","","2011","","","103","106","Along with the rapid development of social economy, resource crisis and environmental pollution problems are getting more and more serious, which impel the recycle and reuse of resource to obtain the unprecedented attention. It requires the manufacturing enterprises implement closed-loop supply chain management, realizing closed-loop flowing of materials in the supply chain while reducing resources consumption as well as waste emissions. The re-manufacturing closed-loop supply chain network optimization problem is discussed specially, and its general mixed integer programming mathematical model is put out to realize the optimal decision-making of multi-facility point location and the optimal flow distribution, and the mix integer programming method is chosen to solve the model. with the aid of the Lingo 8.0 software, the enterprise living example model is optimization solved, which proves the feasibility of the model, and provides the theory instruction for the enterprises when they construct and manage closed-loop supply chain.","","978-1-4577-1561","10.1109/ICICIS.2011.32","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6063204","Closed-loop supply chain;network optimization design;mixed integer programming;Lingo","Supply chains;Testing;Customer services;Recycling;Optimization;Biological system modeling","air pollution control;closed loop systems;decision making;integer programming;manufacturing industries;supply chain management","MILP model;social economy;resource crisis;environmental pollution problem;unprecedented attention;manufacturing enterprise;resource consumption;waste emission;remanufacturing closed loop supply chain network optimization problem;general mixed integer programming mathematical model;optimal decision making;multifacility point location;optimal flow distribution;integer programming method;Lingo8.0 software","","","6","","","","","","IEEE","IEEE Conferences"
"Software/hardware framework for generating parallel Gaussian random numbers based on the Monty Python method","Y. Li; P. Chow; J. Jiang; M. Zhang; S. Wei","School of Computer, National University of Defense Technology, Changsha, Hunan, China, 410073; Department of Electrical and Computer Engineering, University of Toronto, Ontario, Canada; School of Microelectronics, Shanghai Jiao Tong University, China; School of Computer, National University of Defense Technology, Changsha, Hunan, China, 410073; Institute of Microelectronics, Tsinghua University, Beijing, China","2012 International Conference on Field-Programmable Technology","","2012","","","190","197","We present a hardware architecture for efficient implementation of a Gaussian random number generator (GRNG), using the Monty Python method. To maximize the performance/complexity efficiency, an efficient word-length optimization model is proposed to find out both the optimal integer and fractional word-lengths for signals. Experimental results show that our optimized Fixed-Point design achieves a throughput of almost 1 sample-per-cycle and runs as fast as 375.9 MHz on a Xilinx XC6VLX240T FPGA device. This performance is 23.4-fold faster than a dedicated software version running on a 2.67-GHz Intel core i5 processor. It takes 1976 LUTs, 1785 Flip-Flops, 12 BRAMs and 35 DSPs, which is only about 1% of the device as well as a great reduction compared to its corresponding Floating-Point implementations. Furthermore, we develop a framework that is capable of partitioning the Gaussian distribution stream into an arbitrary number of parallel sub-streams. With support from software, this framework can obtain speedup roughly linearly with the number of parallel cores. The quality of the variables produced by our design are verified via the standard Gaussian statistical test suit, the chi-square (X<sup>2</sup>) test.","","978-1-4673-2845-6978-1-4673-2846-3978-1-4673-2844","10.1109/FPT.2012.6412133","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6412133","","Hardware;Optimization;Computer architecture;Algorithm design and analysis;Software;Generators;Polynomials","computational complexity;digital signal processing chips;field programmable gate arrays;fixed point arithmetic;flip-flops;floating point arithmetic;Gaussian distribution;parallel architectures;performance evaluation;random number generation;statistical testing","frequency 2.67 GHz;chi-square test;standard Gaussian statistical test suit;parallel substreams;Gaussian distribution;floating-point implementations;DSP;BRAM;flip-flop;LUT;Intel core i5 processor;Xilinx XC6VLX240T FPGA device;optimized fixed-point design;optimal fractional word-lengths;optimal integer word-lengths;word-length optimization model;performance efficiency maximization;complexity efficiency maximization;GRNG;hardware architecture;Monty Python method;parallel Gaussian random number generation;software-hardware framework","","3","27","","","","","","IEEE","IEEE Conferences"
"A Hybrid ACO algorithm for the Next Release Problem","H. Jiang; J. Zhang; J. Xuan; Z. Ren; Y. Hu","School of Software, Dalian University of Technology, Dalian 116621, China; School of Software, Dalian University of Technology, Dalian 116621, China; School of Mathematical Sciences, Dalian University of Technology, Dalian 116024, China; School of Mathematical Sciences, Dalian University of Technology, Dalian 116024, China; School of Software, Dalian University of Technology, Dalian 116621, China","The 2nd International Conference on Software Engineering and Data Mining","","2010","","","166","171","In this paper, we propose a Hybrid Ant Colony Optimization algorithm (HACO) for Next Release Problem (NRP). NRP, a NP-hard problem in requirement engineering, is to balance customer requests, resource constraints, and requirement dependencies by requirement selection. Inspired by the successes of Ant Colony Optimization algorithms (ACO) for solving NP-hard problems, we design our HACO to approximately solve NRP. Similar to traditional ACO algorithms, multiple artificial ants are employed to construct new solutions. During the solution construction phase, both pheromone trails and neighborhood information will be taken to determine the choices of every ant. In addition, a local search (first found hill climbing) is incorporated into HACO to improve the solution quality. Extensively wide experiments on typical NRP test instances show that HACO outperforms the existing algorithms (GRASP and simulated annealing) in terms of both solution quality and running time.","","978-89-88678-22-0978-1-4244-7324","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5542931","next release problem (NRP);ant colony optimization;local search;requirment engineering","Ant colony optimization;Simulated annealing;NP-hard problem;Testing;Software algorithms;Helium;Greedy algorithms;Genetic algorithms;Traveling salesman problems;Guidelines","computational complexity;simulated annealing;systems analysis","next release problem;hybrid ant colony optimization algorithm;NP hard problem;requirement engineering;customer requests;resource constraints;GRASP;simulated annealing;requirement selection","","2","23","","","","","","IEEE","IEEE Conferences"
"Dynamic Analysis for Diagnosing Integration Faults","L. Mariani; F. Pastore; M. Pezze","University of Milano Bicocca, Milan; University of Milano Bicocca, Milan; University of Milan Bicocca, Milano","IEEE Transactions on Software Engineering","","2011","37","4","486","508","Many software components are provided with incomplete specifications and little access to the source code. Reusing such gray-box components can result in integration faults that can be difficult to diagnose and locate. In this paper, we present Behavior Capture and Test (BCT), a technique that uses dynamic analysis to automatically identify the causes of failures and locate the related faults. BCT augments dynamic analysis techniques with model-based monitoring. In this way, BCT identifies a structured set of interactions and data values that are likely related to failures (failure causes), and indicates the components and the operations that are likely responsible for failures (fault locations). BCT advances scientific knowledge in several ways. It combines classic dynamic analysis with incremental finite state generation techniques to produce dynamic models that capture complementary aspects of component interactions. It uses an effective technique to filter false positives to reduce the effort of the analysis of the produced data. It defines a strategy to extract information about likely causes of failures by automatically ranking and relating the detected anomalies so that developers can focus their attention on the faults. The effectiveness of BCT depends on the quality of the dynamic models extracted from the program. BCT is particularly effective when the test cases sample the execution space well. In this paper, we present a set of case studies that illustrate the adequacy of BCT to analyze both regression testing failures and rare field failures. The results show that BCT automatically filters out most of the false alarms and provides useful information to understand the causes of failures in 69 percent of the case studies.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.93","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5611554","Dynamic Analysis;diagnosis;fault localization;false positive filters;regression failure analysis;field failure analysis.","Automata;Monitoring;Analytical models;Engines;Software;Testing;Inference algorithms","fault diagnosis;object-oriented programming;program testing;software fault tolerance","software components;Integration Faults;behavior capture and test technique;BCT;dynamic analysis;model-based monitoring;incremental finite state generation techniques","","32","71","","","","","","IEEE","IEEE Journals & Magazines"
"Using linear optimization and swarm intelligent heuristic to find a route for MANET","Y. Liang; G. Pond","Department of Mathematics and Computer Science, Royal Military College of Canada, Kingston, Canada; Defense Research and Development Canada, CORA, Kingston, Canada","2011 Seventh International Conference on Natural Computation","","2011","4","","2368","2372","The aim of this study is to explore potential routing protocol for software development of MANET (Mobile Ad-hoc Network) peer-to-peer system which can be used for battle field radio communications. MANET consists of a number of mobile devices which communicate over radio, and a MANET with cell phones (MANET-WCP) is the physical infrastructure of the underlined research. One of the biggest challenges of this type of network is to find a route between a source node to a destination node via intermediate nodes with mobility. In this paper, we present a routing protocol using swarm intelligent heuristic (SIH) for MANET-WCP. We refer to the protocol as the Swap Intelligent Heuristic Associate Based Routing, i.e., SIH-ABR. The protocol replaces the route selection mechanism with ant colony optimization heuristic. The main objectives of this approach are: 1). to test the adaptation of SIH-ABR; 2). to verify the efficiency of service; 3). to explore its scalability with a larger number of nodes in a land battle scenario.","2157-9563;2157-9555;2157-9555","978-1-4244-9953-3978-1-4244-9950-2978-1-4244-9952","10.1109/ICNC.2011.6022430","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6022430","routing protocol/algorithm;ad-hoc network;MANET;swarm intelligent heuristic","Routing protocols;Mobile computing;Mobile communication;Routing;Mobile ad hoc networks","mobile ad hoc networks;optimisation;peer-to-peer computing;routing protocols;software engineering","linear optimization;swarm intelligent heuristic;mobile ad hoc networks;MANET;routing protocol;software development;peer-to-peer system;battle field radio communications;mobile devices;cell phones;source node;destination node;intermediate nodes;swap intelligent heuristic associate based routing;ant colony optimization heuristic","","","14","","","","","","IEEE","IEEE Conferences"
"Autonomic Composite-service Architecture with MAWeS","E. P. Mancini; M. Rak; U. Villano","NA; NA; NA","2010 International Conference on Complex, Intelligent and Software Intensive Systems","","2010","","","1050","1056","The highly distributed nature and the load sensitivity of Service Oriented Architectures (SOA) make it very difficult to guarantee performance requirements under rapidly-changing load conditions. This paper deals with the development of service oriented autonomic systems that are capable to optimize themselves using a feed forward approach, by exploiting automatically generated performance predictions. The MAWeS (MetaPL/HeSSE Autonomic Web Services) framework allows the development of self-tuning applications that proactively optimize themselves by simulating the execution environment. After a discussion on the possible design choices for the development of autonomic web services applications, a soft real-time test application is presented and the performance results obtained in a composite-service execution scenario are commented.","","978-1-4244-5918-6978-1-4244-5917-9978-0-7695-3967","10.1109/CISIS.2010.166","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5447260","Autonomic systems;MAWeS;Web Services;Service Oriented Architecture","Service oriented architecture;Web services;Application software;Computer architecture;Biology computing;Competitive intelligence;Software performance;Software systems;Computer science;Testing","optimisation;software architecture;software fault tolerance;Web services;XML","autonomic composite-service architecture;MAWeS;service oriented architectures;rapidly-changing load conditions;feed forward approach;optimization;automatically generated performance predictions;MetaPL-HeSSE autonomic Web services","","","18","","","","","","IEEE","IEEE Conferences"
"Dynamic performance stubs to simulate the main memory behavior of applications","P. Trapp; M. Meyer; C. Facchi","University of Applied Sciences, Ingolstadt, Ingolstadt, Germany; University of Applied Sciences, Ingolstadt, Ingolstadt, Germany; University of Applied Sciences, Ingolstadt, Ingolstadt, Germany","2011 International Symposium on Performance Evaluation of Computer & Telecommunication Systems","","2011","","","127","134","Dynamic performance stubs provide a framework to simulate the performance behavior of software modules and functions. Hence, they can be used as an extension to software performance engineering methodologies. The methodology of dynamic performance stubs targets to gain oriented performance improvement. Other applications include the identification of ""hidden"" bottlenecks and the prioritization of optimization alternatives. Main memory stubs have been developed to extend the simulation possibilities of the dynamic performance stubs framework. They are able to simulate the heap and stack behavior of software modules or functions. This paper evaluates an algorithm to generate the simulation data file, which serves as input for the main memory stubs simulation algorithm. Moreover, it presents an automatic error correction algorithm to consider the results from the calibration functions to improve the simulation results. Additionally, a proof of concept is given to depict the results of the simulation data file generation and the automatic error correction algorithm. This paper shows that, it is possible to generate the simulation data file as well as to optimize the simulation data to compensate inaccuracies in order to create main memory stubs.","","978-1-4577-0139-9978-1-61782-309","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5984857","Memory Systems;Software Performance;Evaluation and Testing;Modeling;Performance Optimization;Bounds;Models;Case Studies","","optimisation;software performance evaluation;storage management","dynamic performance stubs;main memory behavior;performance behavior;software modules;software functions;software performance engineering;performance improvement;optimization;automatic error correction;calibration functions","","","13","","","","","","IEEE","IEEE Conferences"
"Evotec: Evolving the Best Testing Strategy for Contract-Equipped Programs","L. S. Silva; Y. Wei; B. Meyer; M. Oriol","NA; NA; NA; NA","2011 18th Asia-Pacific Software Engineering Conference","","2011","","","290","297","Automated random testing is efficient at detecting faults but it is certainly not an optimal testing strategy for every given program. For example, an automated random testing tool ignores that some routines have stronger preconditions, they use certain literal values, or they are more error-prone. Taking into account such characteristics may increase testing effectiveness. In this article, we present Evotec, an enhancement of random testing which relies on genetic algorithms to evolve a best testing strategy for contract-equipped programs. The resulting strategy is optimized for detecting more faults, satisfying more routine preconditions and establishing more object states on a given set of classes to test. Our experiment tested 92 classes over 1710 hours. It shows that Evotec detected 29% more faults than random+ and 18% more faults than the precondition-satisfaction strategy.","1530-1362;1530-1362;1530-1362","978-1-4577-2199-1978-0-7695-4609","10.1109/APSEC.2011.34","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6130699","Automated Software Testing;Genetic Algorithm;Static-Analysis","Testing;Biological cells;Genetic algorithms;Indexes;Contracts;Fault detection;Arrays","genetic algorithms;program testing;random processes","Evotec;automated random testing;contract-equipped program;genetic algorithm","","","33","","","","","","IEEE","IEEE Conferences"
"Using modern ANSI C development tools to increase productivity and ensure test and measurement application reliability","A. Kruger","National Instruments, Austin, TX","2010 IEEE AUTOTESTCON","","2010","","","1","3","Over the past few decades, the ANSI C language has become one of the most popular test and measurement programming languages due to its power and flexibility. ANSI C applications can be optimized for improved performance because of the developer's ability to have low level control through direct access to memory and hardware specific function calls. Although the ANSI C language provides low level control of application development resulting in the freedom to optimize source code for improved execution performance, it can also lead to programming mistakes. Some examples of the most common ANSI C programming mistakes are syntax errors, incorrect array indexing, memory leaks and inefficient function implementation, which can cause sluggish execution speeds on deployed test systems. Unlike high-level language developers, ANSI C developers are challenged with the added responsibility of ensuring that low level details are handled appropriately in order to ensure that applications are reliable and stable. Modern tools are becoming available to help ANSI C developers face these challenges and, often, integrate directly into existing development environments. This paper examines how and why ANSI C developers commonly make programming mistakes related to syntax, memory allocation and inefficiencies in application source code, and focuses on overcoming these challenges with modern development tools. Some of these modern ANSI C development tools include run-time debugging, resource tracking and run-time analysis and help to increase developer productivity and ensure the reliability of test systems.","1558-4550;1088-7725;1088-7725","978-1-4244-7961-0978-1-4244-7960-3978-1-4244-7959","10.1109/AUTEST.2010.5613623","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613623","","Debugging;Programming;Instruments;Productivity;Level control;Reliability;Hardware","ANSI standards;automatic test equipment;C language;program debugging;software reliability","modern ANSI C development tool;measurement application reliability;test application;ANSI C language;measurement programming language;direct access memory;hardware specific function call;source code optimization;syntax error;incorrect array indexing;memory leak;programming mistake;run time debugging;resource tracking;run time analysis","","","5","","","","","","IEEE","IEEE Conferences"
"Industrial experiences with automated regression testing of a legacy database application","E. Rogstad; L. Briand; R. Dalberg; M. Rynning; E. Arisholm","Simula Research Laboratory, Lysaker, University of Oslo, Dept. of Informatics, Norway; Simula Research Laboratory, Lysaker, University of Oslo, Dept. of Informatics, Norway; The Norwegian Tax Department, Oslo, Norway; The Norwegian Tax Department, Oslo, Norway; Testify AS, University of Oslo, Department of Informatics, Norway","2011 27th IEEE International Conference on Software Maintenance (ICSM)","","2011","","","362","371","This paper presents a practical approach and tool (DART) for functional black-box regression testing of complex legacy database applications. Such applications are important to many organizations, but are often difficult to change and consequently prone to regression faults during maintenance. They also tend to be built without particular considerations for testability and can be hard to control and observe. We have therefore devised a practical solution for functional regression testing that captures the changes in database state (due to data manipulations) during the execution of a system under test. The differences in changed database states between consecutive executions of the system under test, on different system versions, can help identify potential regression faults. In order to make the regression test approach scalable for large, complex database applications, classification tree models are used to prioritize test cases. The test case prioritization can be applied to reduce test execution costs and analysis effort. We report on how DART was applied and evaluated on business critical batch jobs in a legacy database application in an industrial setting, namely the Norwegian Tax Accounting System (SOFIE) at the Norwegian Tax Department (NTD). DART has shown promising fault detection capabilities and cost-effectiveness and has contributed to identify many critical regression faults for the past eight releases of SOFIE.","1063-6773;1063-6773","978-1-4577-0664-6978-1-4577-0663-9978-1-4577-0662","10.1109/ICSM.2011.6080803","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080803","regression testing;legacy database applications;industrial context","Databases;Testing;Mice;Keyboards;Unified modeling language","database management systems;organisational aspects;regression analysis;software maintenance;taxation","industrial experiences;automated regression testing;legacy database application;functional black-box regression testing;organizations;maintenance;regression faults;DART;Norwegian Tax Accounting System;SOFIE;Norwegian Tax Department;NTD;fault detection capabilities","","14","12","","","","","","IEEE","IEEE Conferences"
"OFDM PAPR reduction by convex optimization: A power amplifier point-of-view","C. Nader; P. Händel; N. Björsell","Center for RF Measurement Technology, University of Gävle, Gävle, SE-80176, Sweden; Center for RF Measurement Technology, University of Gävle, Gävle, SE-80176, Sweden; Center for RF Measurement Technology, University of Gävle, Gävle, SE-80176, Sweden","2010 IEEE International Microwave Workshop Series on RF Front-ends for Software Defined and Cognitive Radio Solutions (IMWS)","","2010","","","1","4","In this paper we evaluate the application of convex optimization for PAPR reduction on OFDM 802.11a signal type. A radio frequency power amplifier is measured and characterized while excited by both original and optimized OFDM signals. A state-of-art test setup was used for the purpose. Figure of merits such as power added efficiency, in-band errors, and out-of-band spectral emissions are investigated for their relevance and a study of the power distribution in the excitation signal is evaluated.","","978-1-4244-5753-3978-1-4244-5751-9978-1-4244-5753","10.1109/IMWS.2010.5440994","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5440994","Communication system performance;convex optimization;dirty radio;power amplifiers;power added efficiency;spectrum mask","OFDM;Peak to average power ratio;Power amplifiers;Radiofrequency amplifiers;Radio frequency;Power measurement;Frequency measurement;RF signals;Testing;Power distribution","OFDM modulation;power amplifiers;wireless LAN","OFDM PAPR reduction;convex optimization;OFDM 802.11a signal type;radio frequency power amplifier;OFDM signals;power added efficiency;in-band errors;out-of-band spectral emissions;power distribution;excitation signal","","","13","","","","","","IEEE","IEEE Conferences"
"An optimization approach for service deployment in service oriented clouds","T. Liu; T. Lu; Z. Liu","School of Computer Science, Fudan University, Shanghai, P.R. China; School of Computer Science, Fudan University, Shanghai, P.R. China; Shanghai Key Laboratory of Computer Software Evaluating and Testing, Shanghai, P.R. China","The 2010 14th International Conference on Computer Supported Cooperative Work in Design","","2010","","","390","395","In service oriented cloud, in order to meet the different users' service requirements, the cloud venders need to manage the service applications' deployment effectively first. During the deployment process, the deployment costs and the influence of service deployment to the cloud are very important issues to be paid attention to, which lacks in current research. In this paper, we design a novel optimization approach for service deployment to address above issues, which works in a service deployment framework. The approach can improve the deployment efficiency and reduce the deployment costs. Atom-services, the basic units of service application, are partitioned to different service families with its compatibility and installation policy. We present three algorithms to automatically standardize the installation expressions, simplify and optimize the installation expression series during the service deployment process respectively. An analysis about the service deployment result is given in the paper and the experiment data demonstrates the approach's efficiency.","","978-1-4244-6763-1978-1-4244-6763-1978-1-4244-6764","10.1109/CSCWD.2010.5471941","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5471941","cloud computing;service deployment;deployment optimization","Cloud computing;Application software;Cost function;Design optimization;Computer science;Collaborative work;Laboratories;Software testing;Conference management;Partitioning algorithms","","","","4","11","","","","","","IEEE","IEEE Conferences"
"Heuristic optimisation algorithm for Java dynamic compilation","Y. Liu; A. S. Fong","Guangdong University of Technology, University Mega Center, People¿s Republic of China; City University of Hong Kong, Hong Kong","IET Software","","2012","6","4","307","312","Dynamic compilation increases Java virtual machine (JVM) performance because running compiled codes is faster than interpreting Java bytecodes. However, inappropriate decision on dynamic compilation may degrade performance owing to compilation overhead. A good heuristic algorithm for dynamic compilation should achieve an appropriate balance between compilation overhead and performance gain in each method invocation sequence. A method-size and execution-time heuristic algorithm is proposed in the study. The key principle of the algorithm is that different method-sizes necessitate different compile thresholds for optimal performance. A parameter search mechanism using a genetic algorithm for dynamic compilation is proposed to find optimised multi-thresholds in the algorithm. This heuristic algorithm is evaluated in an openJDK Java Server JVM using SPEC JVM98 benchmark suite. The algorithm shows an overall advantage in performance speedup when testing benchmarks and gain speedup by 19.1% on average. The algorithm also increases the performance of original openJDK by 10.2% when extended to the whole benchmark suite.","1751-8806;1751-8814","","10.1049/iet-sen.2011.0144","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6322852","","","benchmark testing;genetic algorithms;Java;optimising compilers;software performance evaluation;virtual machines","heuristic optimisation algorithm;Java dynamic compilation;Java virtual machine performance;JVM performance;code compilation;performance degradation;compilation overhead;performance gain;method-size heuristic algorithm;execution-time heuristic algorithm;optimal performance thresholds;parameter search mechanism;genetic algorithm;optimised multithresholds;openJDK Java Server JVM;SPEC JVM98 benchmark suite;performance speedup;benchmark testing","","","","","","","","","IET","IET Journals & Magazines"
"Compression-aware capture power reduction for at-speed testing","J. Li; Q. Xu; D. Xiang","School of Software, Tsinghua University, Beijing, China 100084; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Shatin, Hong Kong; School of Software, Tsinghua University, Beijing, China 100084","16th Asia and South Pacific Design Automation Conference (ASP-DAC 2011)","","2011","","","806","811","Test compression has become a de facto technique in VLSI testing. Meanwhile, excessive capture power of at-speed testing has also become a serious concern. Therefore, it is important to co-optimize test power and compression ratio in at-speed testing. In this paper, a novel X-filling framework is proposed to reduce capture power of both LoC and LoS at-speed testing, which is applicable for different test compression schemes. The proposed technology has been validated by the experimental results on larger ITC'99 benchmark circuits.","2153-697X;2153-6961;2153-6961","978-1-4244-7516-2978-1-4244-7515-5978-1-4244-7514","10.1109/ASPDAC.2011.5722300","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5722300","","Testing;Entropy;Clocks;Safety;Switches;Logic gates","semiconductor device testing;VLSI","compression-aware capture power reduction;VLSI testing;X-fllling framework;LoC at-speed testing;LoS at-speed testing;test compression schemes","","","19","","","","","","IEEE","IEEE Conferences"
"Hardware/software co-design for fast-trainable speaker identification system based on SMO","J. Wang; J. Peng; J. Wang; P. Lin; T. Kuan","Department of Electrical Engineering, National Cheng Kung University, Tainan City, Taiwan; Department of Electrical Engineering, National Cheng Kung University, Tainan City, Taiwan; Department of Computer Science &amp; Information Engineering, National Central University, Jhongli City, Taiwan; Department of Electronics Engineering and Computer Science, Tung Fang Deign University, Kaohsiung Country, Taiwan; Department of Electrical Engineering, National Cheng Kung University, Tainan City, Taiwan","2011 IEEE International Conference on Systems, Man, and Cybernetics","","2011","","","1621","1625","Embedded speaker identification system is a popular research, but most of current systems can not provide fast training ability. Because of the low computational ability in the embedded environment, a large amount of waiting time usually makes the human-machine interface not friendly. This paper presents a hardware and software (HW/SW) co-design solution for fast-trainable speaker identification system. Fast training ability makes this embedded speaker identification system possess high flexibility and enhances the convenience to a wide range of real-world applications. The proposed system consists of a training phase and a multiclass identification phase. The sequential minimal optimization (SMO) training algorithm occupies the heaviest computational load and is realized as a dedicated VLSI module, i.e., the hardware component. The other processes such as speech preprocess, speech feature extraction, and SVM voting strategy are implemented by software. Moreover, a data-packed mechanism is presented to improve the bandwidth utilization. Compared with the embedded C code based on ARM processor, our system reduces 90% of the training time and achieves 89.9% identification rate with the NIST 2010 speaker recognition database. The proposed system was tested and found to be fully functional working on a Socle CDK prototype system with an AMBA based Xilinx FPGA and an ARM926EJ processor.","1062-922X;1062-922X","978-1-4577-0653-0978-1-4577-0652-3978-1-4577-0651","10.1109/ICSMC.2011.6083903","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6083903","Speaker Identification;Hardware/Software Co-design;Sequential Minimal Optimization (SMO)","Training;Support vector machines;Hardware;Feature extraction;Speech;Speaker recognition;Algorithm design and analysis","computational complexity;embedded systems;hardware-software codesign;optimisation;speaker recognition;support vector machines;VLSI","hardware-software codesign;fast-trainable speaker identification system;sequential minimal optimization;embedded system;human-machine interface;training phase;multiclass identification phase;sequential minimal optimization training algorithm;computational load;VLSI module;speech preprocess;speech feature extraction;SVM voting strategy;data-packed mechanism;embedded C code;ARM processor;NIST 2010 speaker recognition database;Socle CDK prototype;a AMBA based Xilinx FPGA;ARM926EJ processor","","10","13","","","","","","IEEE","IEEE Conferences"
"Verifying Code and Its Optimizations: An Experience Report","R. Metta","NA","2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops","","2011","","","578","583","We present our experience in formally verifying the correctness of a 200-line industrial C implementation of Cyclic Redundancy Check (CRC) and its optimizations. Our experience indicates that (a) both the specification and verification of even such small code can be hard to automate and needs intense manual effort, and (b) verifying certain optimizations to an existing code is relatively easier. It took us 10 days to specify and 5 days verify the correctness of this 200 line program. Our initial attempt to verify the correctness, using the model checker CBMC, did not scale up due to a huge loop unwinding (220 times). Since it is hard to scale model checking for such huge loops, we first verified each function's correctness independently using CBMC and then verified the correctness of the loop through manual application of induction using CBMC. This traditional modular and inductive approach helped CBMC verify the implementation within 25 seconds. Furthermore, the implementation was poorly coded to begin with and had to undergo several optimizations to meet its performance requirement. With our optimizations, the CRC implementation met its performance requirement. Due to the changes in code for the optimizations, we needed to reverify the complete implementation. To lessen the verification rework, we proved one optimization at a time, in each function, by proving that the behavior of the function is preserved by the optimization. It took us 7 days to code the optimizations and 1 day to verify them. This optimized correct implementation was accepted and shipped by our client along with the client's automotive software. In future, we would like to generalize this approach to verify behavior preserving changes, and we believe that some of these steps can be automated.","","978-0-7695-4345-1978-1-4577-0019","10.1109/ICSTW.2011.44","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954468","verification;code;scalability;evolution","Optimization;Polynomials;Computational modeling;Software;Automotive engineering;Manuals;Generators","cyclic redundancy check codes;formal specification;program verification","code verification;code optimization;formal verification;industrial C implementation;cyclic redundancy check;code specification;CBMC model checker;correctness verification;modular approach;inductive approach;automotive software","","2","10","","","","","","IEEE","IEEE Conferences"
"Assessing the performance of DG in distribution network","S. R. A. Rahim; I. Musirin; M. H. Sulaiman; M. H. Hussain; A. Azmi","School of Electrical System Engineering at Universiti Malaysia Perlis (UniMAP), Malaysia; The Faculty of Electrical Engineering, Universiti Teknologi MARA, 40450 Shah Alam, Malaysia; School of Electrical System Engineering, Universiti Malaysia Perlis, Malaysia; School of Electrical System Engineering at Universiti Malaysia Perlis (UniMAP), Malaysia; School of Electrical System Engineering at Universiti Malaysia Perlis (UniMAP), Malaysia","2012 IEEE International Power Engineering and Optimization Conference Melaka, Malaysia","","2012","","","436","441","Distributed Generation (DG) integrated into distribution networks continued to grow both in number and size. Governments' incentives and obligations for a sustainable energy ensure that DG is going to be an important element in the future distribution systems. To achieve the maximum benefits of DG, factors such as the number and the capacity of the units and the best location have to be considered. This problem is addressed in this study by the development of algorithm to optimize the output of the DG in order to assess the performance from its installation. The optimization technique that will be used is Hybrid Mutation-Evolutionary Programming (HM-EP) and Particle swarm Optimization (PSO). The proposed technique would be able to effectively improve the voltage profile and also reduction in distribution losses. The proposed technique will be tested on IEEE 69-bus Reliability Test Systems and will be developed using the MATLAB programming software.","","978-1-4673-0662-1978-1-4673-0660-7978-1-4673-0661","10.1109/PEOCO.2012.6230904","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6230904","Distributed Generation (DG);Hybrid Mutation-Evolutionary Programming (HM-EP);Particle Swarm Optimization (PSO)","Distributed power generation;Optimization;Standards;Conferences;Power engineering;Particle swarm optimization;Programming","distributed power generation;distribution networks;evolutionary computation;particle swarm optimisation","DG performance assessment;distribution network;distributed generation;government incentive;government obligation;sustainable energy;hybrid mutation-evolutionary programming;particle swarm optimization;HM-EP;PSO;voltage profile;distribution loss reduction;IEEE 69-bus reliability test systems;Matlab programming software","","4","19","","","","","","IEEE","IEEE Conferences"
"A method to summarize Disease Based Temporal state of human organ using laboratory test data and UMLS knowledge","S. Paul; R. Roy; S. Bissoyi; S. Bhaumik; M. A. Patil; V. S. Bhardwaj","Samsung India Software Operations, Bangalore, India; Samsung India Software Operations, Bangalore, India; Samsung India Software Operations, Bangalore, India; Samsung India Software Operations, Bangalore, India; Samsung India Software Operations, Bangalore, India; Samsung India Software Operations, Bangalore, India","2012 IEEE 12th International Conference on Bioinformatics & Bioengineering (BIBE)","","2012","","","13","18","A novel concept of Disease Based Temporal Score (DT-Score) is introduced to efficiently represent periodic laboratory test data. Through this score, temporal state of an organ can be represented by summarizing periodic laboratory test data. The score can be used to indicate early trend for chronic abnormalities and thus results in an effective wellness measure. Many of these chronic abnormalities have a late manifestation and are major contributors for healthcare cost and mortality. Resources of Unified Medical Language System (UMLS) are introduced for automatic generation of relational tree between laboratory test, disease and organs with relative rank. Doctor's annotations are used to create reference score and data mining techniques are employed in deriving a mathematical model for estimating the DT-Score. A novel human body based summarization is employed for an intuitive view of the DT-Score and resultant temporal state of the associated organ. The proposed method enables an efficient temporal summarization of high volume of laboratory data and eventually reduces the cognitive load on physician. This method has potential to impact larger population as this can be effectively built over low cost regular laboratory test.","","978-1-4673-4358-9978-1-4673-4357-2978-1-4673-4356","10.1109/BIBE.2012.6399699","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6399699","Disease Based Temporal Score;Periodic Laboratory Test;UMLS;Temporal State of Human Organ;Regression Mechanism","Laboratories;Diseases;Liver;Unified modeling language;Mathematical model;Vectors","biological organs;cognition;data mining;diseases;health care;medical disorders;medical information systems","UMLS knowledge;disease based temporal state;human organ;disease based temporal score;DT-Score;periodic laboratory test data;chronic abnormalities;healthcare cost;mortality;unified medical language system;automatic relational tree generation;doctor annotations;reference score;data mining techniques;mathematical model;human body based summarization;cognitive load","","","17","","","","","","IEEE","IEEE Conferences"
"Hierarchical control flow matching for source-level simulation of embedded software","K. Lu; D. Müller-Gritschneder; U. Schlichtmann","Institute for Electronic Design Automation, Technische Universität München, Germany; Institute for Electronic Design Automation, Technische Universität München, Germany; Institute for Electronic Design Automation, Technische Universität München, Germany","2012 International Symposium on System on Chip (SoC)","","2012","","","1","5","Source-level simulation (SLS) of embedded software annotates the source code based on the matching of the control flow graphs (CFG) between the source code and the cross-compiled binary code. However, existing SLS approaches still can not guarantee to find a matching for a CFG that is optimized by the compiler. Further, they rely on debug information, which may be unreliable. In this paper, the authors propose a hierarchical CFG matching approach to reduce the influence of compiler optimization and ambiguous debug information. This approach divides the CFGs of the source and binary code into nested regions. Then the matching of those two CFGs is performed for the regions in a top-down manner. In this way, heavy optimization or debug misinformation of certain basic blocks will not have global impact on the matching of other basic blocks. Moreover, optimized loops and branches are matched with respect to the optimization techniques used by the compiler.","","978-1-4673-2896-8978-1-4673-2895-1978-1-4673-2894","10.1109/ISSoC.2012.6376366","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6376366","","Optimization;Binary codes;Timing;Embedded software;Design automation;Benchmark testing","binary codes;flow graphs;optimisation;program compilers;program control structures;program debugging;source coding","hierarchical control flow matching;source-level simulation;embedded software;control flow graphs;cross-compiled binary code;SLS approaches;hierarchical CFG matching approach;compiler optimization;ambiguous debug information;binary code CFG;source code CFG;nested regions;loop optimization;branch optimization;optimization techniques","","4","15","","","","","","IEEE","IEEE Conferences"
"Mechanical design and analysis of innovative integrated circuit test socket","Z. Ehtiatkar; S. Basiri; N. Amin","Department of Electrical, Electronic &amp; System Engineering, Faculty of Engineering and Built Environment Universiti Kebangsaan Malaysia, UKM, Bangi 43600, Selangor, Malaysia; Department of Electrical, Electronic &amp; System Engineering, Faculty of Engineering and Built Environment Universiti Kebangsaan Malaysia, UKM, Bangi 43600, Selangor, Malaysia; Department of Electrical, Electronic &amp; System Engineering, Faculty of Engineering and Built Environment Universiti Kebangsaan Malaysia, UKM, Bangi 43600, Selangor, Malaysia","2011 IEEE Regional Symposium on Micro and Nano Electronics","","2011","","","65","69","Mechanical design and analysis of innovative test socket is done in this study by using ANSYS as Finite Element Analysis (FEA) software. It is designed to solve the problems brought to the IC leads by pogo pins in the IC testing process. Proposed test socket is formed as an array of holes filled with electrical conductive cells. Each cell consists of three different parts, including micro-contactors, polymer house and liquid metal. Liquid metal is used to absorb the pressure and prevent damaging to the IC leads. Mechanical and electrical analysis was done on the different models with 4, 8, 12, 16 and 20 micro-contactors. The 16 pin model showed the best results. This model was analyzed with different height and the results indicate that the 5mm height electrical conductive cell has the best characteristics. In optimization process, the 16 pin model with 5 mm height was optimized by changing the polymer house material and copper thickness. The enhancement was over 50 percents in some cases such as stress, strain and deformation. Stress and strain analysis shows that all of models have better results with many novelties to be commercially viable.","","978-1-61284-846-4978-1-61284-844-0978-1-61284-845","10.1109/RSM.2011.6088293","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6088293","Test Socket;Pogo Pin;Finite Element Analysis","Strain;Conductivity;Metals;Materials;Stress;Pins;Contactors","deformation;electronic engineering computing;finite element analysis;integrated circuit design;integrated circuit testing;optimisation","mechanical design;innovative integrated circuit test socket analysis;ANSYS;finite element analysis software;FEA software;IC testing process;electrical conductive cells;polymer house;liquid metal;microcontactors;electrical analysis;copper thickness;stress analysis;strain analysis;deformation;size 5 mm","","","7","","","","","","IEEE","IEEE Conferences"
"A fast and elitist grid selection evolutionary algorithm for Multi-objective Optimization: GSEA","Y. Qin; J. Ji; Y. Song; Yamin Wang; C. Liu","College of Computer Science and Technology, Beijing University of Technology, Beijing Municipal Key Laboratory of Multimedia and Intelligent Software Technology, 100124, China; College of Computer Science and Technology, Beijing University of Technology, Beijing Municipal Key Laboratory of Multimedia and Intelligent Software Technology, 100124, China; College of Computer Science and Technology, Beijing University of Technology, Beijing Municipal Key Laboratory of Multimedia and Intelligent Software Technology, 100124, China; College of Computer Science and Technology, Beijing University of Technology, Beijing Municipal Key Laboratory of Multimedia and Intelligent Software Technology, 100124, China; College of Computer Science and Technology, Beijing University of Technology, Beijing Municipal Key Laboratory of Multimedia and Intelligent Software Technology, 100124, China","2011 Seventh International Conference on Natural Computation","","2011","3","","1228","1232","Multi-objective optimization is an important and challenging topic in the field of industrial design and scientific research. Evolutionary algorithm is a population-based meta-heuristic technique to effectively solve Multi-objective Optimization Problem (MOP). In this paper, a novel EA is proposed, which applied the construction strategy of the elitist population based on spacial grid. In this strategy, firstly, a fast obtaining Pareto set approach with less computation cost is employed; then we filter Pareto set with the grid with the fixed side length to keep the diversity of solutions. Experimental results on test problems show that the GSEA we proposed improves time performance significantly, and is able to find solutions with good diversity and being nearer the true Pareto-optimal front compared to the known NSGA-II, SPEA2 and ε-MOEA.","2157-9563;2157-9555;2157-9555","978-1-4244-9953-3978-1-4244-9950-2978-1-4244-9952","10.1109/ICNC.2011.6022313","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6022313","","Evolutionary computation;Algorithm design and analysis;Optimization;Convergence;Approximation algorithms;Complexity theory;Measurement","evolutionary computation;Pareto optimisation","elitist grid selection evolutionary algorithm;multiobjective optimization problem;GSEA;population-based metaheuristic technique;elitist population;spacial grid;Pareto set;Pareto optimal front","","1","11","","","","","","IEEE","IEEE Conferences"
"A new safety certification method for high-risk flight testing subjects","D. Liu; H. Xu; L. Zhou; B. Pei","Department of Aircraft and Aeroengine, Air Force Engineering University, Xi'an, Shaanxi Province, China; Department of Aircraft and Aeroengine, Air Force Engineering University, Xi'an, Shaanxi Province, China; Department of Aircraft and Aeroengine, Air Force Engineering University, Xi'an, Shaanxi Province, China; Department of Aircraft and Aeroengine, Air Force Engineering University, Xi'an, Shaanxi Province, China","Proceedings of the 10th World Congress on Intelligent Control and Automation","","2012","","","3555","3560","Flight test subjects have high risk and difficult to evaluate quantificationally. A test pilot model based on distribution hypothesis verification is proposed to obtain the test pilot's manipulation parameter. Delay time is selected as an example to illustrate the method. Aircraft model and hydraulic system model is established. Failure model of hydraulic system's sensor and actuator malfunction, oil block and pump power were set up and simulated by Simulink and AMESim. To improve the precision of flight risk assessment, an improved extreme value risk evaluation model based on nonlinearly decreasing weight particle swarm optimization (NDW-PSO based EVT) is proposed. Dynamic simulation from component level to aircraft level was realized by a comprehensive virtual flight testing (VFT) framework based on AMESim, MATLAB/Simulink and Flightgear cross-connect. NO.43 risky subject about “hydraulic system malfunction” in Chinese GJB 626A-2006 was chosen as an example. Influence of hydraulic system failure to flight safety was analyzed and safety amelioration measures were proposed, which illustrate the former proposed method's validity.","","978-1-4673-1398-8978-1-4673-1397-1978-1-4673-1396","10.1109/WCICA.2012.6359063","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6359063","complex system modeling;virtual flight testing;safety certification;risk evaluation;particle swarm optimization","Mathematical model;Safety;Atmospheric modeling;Testing;Hydraulic systems;Aircraft;Software packages","aerospace safety;aerospace simulation;hydraulic systems;sensors","safety certification method;high risk flight testing subject;test pilot model;distribution hypothesis verification;test pilot manipulation parameter;delay time;aircraft model;hydraulic system model;failure model;hydraulic system sensor;actuator malfunction;oil block;pump power;Simulink;AMESim;flight risk assessment;risk evaluation model;nonlinearly decreasing weight particle swarm optimization;dynamic simulation;aircraft level;comprehensive virtual flight testing framework;flightgear cross connect;hydraulic system malfunction;hydraulic system failure;flight safety;safety amelioration measures","","","12","","","","","","IEEE","IEEE Conferences"
"A New Hybrid Path Relinking Algorithm for the Vehicle Routing Problem","X. Zhang; Q. Bai; L. Dong","NA; NA; NA","2010 International Conference on Intelligent Computation Technology and Automation","","2010","1","","234","237","This paper presents a new hybrid path relinking algorithm called ACO&amp;PR algorithm to solve the vehicle routing problem. Ant colony optimization (ACO) is a meta-heuristic approach, which simulates the decision-making processes of ant colonies as they forage for food and find the most efficient routes from their nests to food sources. Due to its constructive nature, we hybridize the solution construction mechanism of ACO with path relinking (PR), an evolutionary method, which introduces progressively attributes of the guiding solution into the initial solution to obtain the high quality solution as quickly as possible. The experimental results for benchmark VRP instances have shown that our proposed method is competitive to solve the vehicle routing problem compared with the best existing methods in terms of solution quality.","","978-1-4244-7280-2978-1-4244-7279","10.1109/ICICTA.2010.700","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5523338","Ant colony optimization;vehicle routing problem;path relinking","Routing;Ant colony optimization;Software algorithms;Paper technology;Simulated annealing;Computational modeling;Algorithm design and analysis;Intelligent vehicles;Automation;Educational institutions","benchmark testing;computational complexity;decision making;evolutionary computation;heuristic programming;optimisation;search problems","hybrid path relinking algorithm;vehicle routing;ACO&PR algorithm;ant colony optimization;metaheuristic approach;decision making process;food source;solution construction mechanism;evolutionary method;high quality solution;benchmark VRP instance","","","12","","","","","","IEEE","IEEE Conferences"
"Cloud test environment deployment based on the needs of individual users","Hong Peng; GenXing Yang; Lizhi Cai; Shidong Huang; Yun Hu","School of Information Science &amp; Engineering, East China University of Science and Technology, Shanghai, China; School of Information Science &amp; Engineering, East China University of Science and Technology, Shanghai, China; School of Information Science &amp; Engineering, East China University of Science and Technology, Shanghai, China; School of Information Science &amp; Engineering, East China University of Science and Technology, Shanghai, China; Shanghai Key Laboratory of Computer Software, Evaluating &amp; Testing, China","2012 6th International Conference on New Trends in Information Science, Service Science and Data Mining (ISSDM2012)","","2012","","","562","567","In the cloud testing environment, the virtual machine is a large scale of the testing resources. Users usually require its operation stability, and would not like to see the phenomenon of frequent migration of virtual machines during the test. Therefore, how to implement the optimized deployment of virtual machine effectively is a very important aspect of the cloud platform. This paper presents an optimized deployment scheme of virtual machines based on the individual testing requirements of users, which can select an optimum host to deploy the virtual machines for testing. This scheme can use resources scientifically and rationally, reduce the frequency of migration of virtual machines and obtain a better users' experience.","","978-89-94364-20-9978-1-4673-0876","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6528697","cloud computing;cloud testing;virtual machine deployment;virtual machine migration","","cloud computing;optimisation;virtual machines","cloud test environment deployment;virtual machine;operation stability;optimized deployment scheme;user experience;cloud platform","","","8","","","","","","IEEE","IEEE Conferences"
"A New Specification-based Test Data Generation Strategy for OSEK OS","L. Zhou; M. Yao; Y. Li; C. Zhang; L. Yao","NA; NA; NA; NA; NA","2010 10th IEEE International Conference on Computer and Information Technology","","2010","","","2311","2316","Specification-based test is used to test a software application to check if it conforms to its design specification. Writing adequate test suites is very challenging and labor-intensive. Producing Specification-based tests for Real-Time Operating System (RTOS) is more challenging than for generic applications since specifications of RTOS only provide resource management mechanisms rather than specific application functionality descriptions. In this paper, we investigate whether Genetic Algorithms (GAs) can generate qualified tests for RTOS, and we present an effective GA approach with an optimization of Kuhn-Munkres maximum weighted algorithm. Experiment results demonstrate the effectiveness of our approach. It is able to generate 100% state coverage tests within 200 generations.","","978-1-4244-7548-3978-1-4244-7547-6978-0-7695-4108","10.1109/CIT.2010.398","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5578319","Specification-based Test;Genetic Algorithms;Kuhn-Munkres Algorithms;OSEK OS","Testing;Optimization;Switches;Generators;Algorithm design and analysis;Bipartite graph;Real time systems","formal specification;genetic algorithms;operating systems (computers);program testing;real-time systems;resource allocation","specification-based test data generation strategy;OSEK OS;software application testing;design specification;real-time operating system;resource management mechanism;genetic algorithms;Kuhn-Munkres maximum weighted algorithm","","1","9","","","","","","IEEE","IEEE Conferences"
"Convergence dynamics of biochemical models to the global optimum","I. Mozga; E. Stalidzans","Biosystems Group, Department of Computer Systems, Latvia University of Agriculture, Liela iela 2, LV3001; Biosystems Group, Department of Computer Systems, Latvia University of Agriculture, Liela iela 2, LV3001","2011 E-Health and Bioengineering Conference (EHB)","","2011","","","1","4","Stochastic nature of convergence of steady state stochastic global optimization methods causes several seemingly attractive approaches to reduce the length of the optimization procedure. The properties of convergence dynamics of evolutionary programming (EP) and particle swarm (PS) are studied optimizing yeast glycolysis by COPASI software adjusting parameters of one, five, ten and fifteen reactions with five identical runs for each case. Results indicate the potential and risks of shortening the optimization time improving the possibilities of systematic search of adjustable parameter combinations. The choice of optimization method depending on the model size and the number of adjustable parameters should be based on number of tests on the convergence quality, speed and repeatability.","","978-606-544-078-4978-1-4577-0292","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6150357","bioprocess design;dynamic modelling;kinetic parameters;optimization;convergence dynamics","Optimization methods;Convergence;Biological system modeling;Stochastic processes;Steady-state;Particle swarm optimization","biochemistry;biology computing;convergence;evolutionary computation;microorganisms;particle swarm optimisation;reaction kinetics theory;stochastic processes","convergence dynamics;biochemical models;global optimum;stochastic convergence;steady state stochastic global optimization methods;optimization procedure;evolutionary programming;particle swarm optimization;yeast glycolysis;COPASI software;parameter combinations;convergence quality;convergence speed;convergence repeatability","","1","18","","","","","","IEEE","IEEE Conferences"
"Applying Lean Startup: An Experience Report -- Lean & Lean UX by a UX Veteran: Lessons Learned in Creating & Launching a Complex Consumer App","B. May","NA","2012 Agile Conference","","2012","","","141","147","This paper outlines lessons learned from a senior UX, product and software development consultant's experiences in conceiving, designing, developing and launching MiniDates.com, a complex consumer dating application. Key decisions and moments over our 15 month process are highlighted, including what we worked and what didn't, what we should have done that we didn't, and what we encountered that we couldn't have foreseen, in the belief that we can help other UX and product-focused startups and Agile teams be more effective. Key lessons include: 1. Beware fads and advice from others; 2. Optimize your team and the development process as much as possible; 3. Beware of seemingly innocent technological and architecture decisions; 4. Ensure UX planning, design and customer validation are up-front in your process; 5. TEST, TEST and TEST some more- at all levels of the business and at all stages, not just the software itself.","","978-1-4673-2622-3978-0-7695-4804","10.1109/Agile.2012.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6298105","Lean;Lean UX;Agile;Case Studies;Customer Development;prototyping;bootsrapping;startup;customer acquisition;entrepreneurship;testing;usability;research","Testing;Software;Prototypes;Best practices;Wires;Computer architecture","program testing;socio-economic effects;software engineering","UX product development consultant experiences;UX software development consultant experiences;complex consumer dating application;product-focused startups;agile team optimization;architecture decisions;technological decisions;UX planning;business levels;software testing;customer validation","","2","16","","","","","","IEEE","IEEE Conferences"
"Design and optimization for gating system of bearing base molding","Jian Shen; Chuanyang Wang","School of Mechanical and Electronic Engineering, Soochow University, Suzhou, China; School of Mechanical and Electronic Engineering, Soochow University, Suzhou, China","2011 Second International Conference on Mechanic Automation and Control Engineering","","2011","","","170","173","In this paper, the best gate location of the thin-walled part bearing base was chosen by using Moldflow software. Two types of gating systems, namely, point gate and disk-shaped gate were designed. By comparing with the analysis of filling, tempe rapture at flow front, volume shrinkage and weld lines of two different gating systems, the better gating system is determined. Type of gate is disk-shaped gate, which provides the basis for the entire mold design. Simulation method improves not only the efficiency of testing mold, but also quality of product.","","978-1-4244-9439-2978-1-4244-9436-1978-1-4244-9438","10.1109/MACE.2011.5986885","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5986885","Moldflow;Optimization;Gating system;Bearing base","Logic gates;Plastics;Welding;Filling;Software;Injection molding;Cavity resonators","design engineering;digital simulation;discs (structures);filling;injection moulding;machine bearings;production engineering computing;quality control;shrinkage;temperature;thin wall structures;welding","optimization;gating system;bearing base molding;gate location;thin-walled part bearing base;Moldflow software;point gate;disk-shaped gate;filling;flow front temperature;volume shrinkage;weld lines;mold design;simulation method;testing mold;product quality","","","6","","","","","","IEEE","IEEE Conferences"
"Measuring unmeasurable attributes of software quality using Pragmatic Quality Factor","J. H. Yahaya; A. Deraman","College of Arts and Sciences, Building of Information Technology, Universiti Utara Malaysia (UUM), 06010, Sintok, Kedah, Malaysia; Vice Chancellor Office, Universiti Malaysia Terengganu (UMT), 21030, Kuala Terengganu, Malaysia","2010 3rd International Conference on Computer Science and Information Technology","","2010","1","","197","202","Software quality is evolving beyond static measurement to a wider scope of quality definition. Previous studies have indicated the importance of human aspect in software quality. But the quality models have not included comprehensively this aspect together with the behavioural aspect of software quality. This research has proposed a Pragmatic Quality Factors (or PQF) as a software quality measurement and metrics that includes both aspects of quality. These aspects of quality are essential as to balance between technical and non-technical (human) facet. In addition, this model provides flexibility by giving priorities and weights to the quality attributes. The priority and weight are necessary to reflect business requirement in the real business environment. Therefore, it is more practical that suits with different users and purposes. It is implemented through collaborative perspective approach between users, developers and independent assessor. This model shows how the unmeasurable characteristics can be measured indirectly using measures and metrics approach. It has been tested involving assessment and certification exercises in real case studies in Malaysia.","","978-1-4244-5540-9978-1-4244-5537-9978-1-4244-5539","10.1109/ICCSIT.2010.5564077","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5564077","software quality;pragmatic quality factor;software measurement","Measurement;Accuracy;Usability;Art;Stability analysis","groupware;software metrics;software quality","unmeasurable attribute measuring;pragmatic quality factor;static measurement;software quality measurement;software metrics;business requirement;collaborative perspective approach;assessment exercises;certification exercises","","3","28","","","","","","IEEE","IEEE Conferences"
"Detailed analysis of compilation options for robust software-based embedded systems","A. Wecxsteen; S. Bergaoui; R. Leveugle","TIMA Laboratory (Grenoble INP, UJF, CNRS), 46 Avenue F&#x00E1;lix Viallet - 38031 Cedex - FRANCE; TIMA Laboratory (Grenoble INP, UJF, CNRS), 46 Avenue F&#x00E1;lix Viallet - 38031 Cedex - FRANCE; TIMA Laboratory (Grenoble INP, UJF, CNRS), 46 Avenue F&#x00E1;lix Viallet - 38031 Cedex - FRANCE","2012 13th Latin American Test Workshop (LATW)","","2012","","","1","6","Several criteria can be used to assess the criticality of registers or variables at compile time and to guide software optimization with respect to robustness constraints. On the basis of such criteria, we analyze in detail the impact of compilation optimizations on the system dependability. We show that optimizations enabled by default lead to criticality increase. However, selectively picking optimizations may increase the robustness of a system even if the consequences of a given optimization option may vary from a program to another.","2373-0862","978-1-4673-2355-0978-1-4673-2354","10.1109/LATW.2012.6261261","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6261261","compilation options;dependability;variable criticality;static analysis","Robustness;Benchmark testing;Optimization;Power capacitors;Transform coding;GSM;Decoding","","","","","7","","","","","","IEEE","IEEE Conferences"
"An Architectural Approach to Composing Reputation-Based Trustworthy Services","S. Phoomvuthisarn; Y. Liu; J. Han","NA; NA; NA","2010 21st Australian Software Engineering Conference","","2010","","","117","126","In SOA, Reputation-Based Trust (RBT) mechanism is applied to achieve trust management. RBT enables services to assess the trust level of other services based on the reputation accumulated from user recommendations. A key challenge to apply RBT is to prevent the strategic behavior of users when they provide recommendations - they might give unfair ratings to benefit themselves. In this paper, we propose a novel architectural approach to integrating auction mechanisms into the trust framework to prevent benefits from untruthful incentives. In this architecture we define an auction-based trust negotiation protocol and realize it in the trust framework. The contribution of our architecture is that it scales and produces accurate results to achieve protection against untruthful incentives, especially when a majority of ratings are unfair, without the potential increase in a computation overhead. An example on a travel agent scenario is devised to collect empirical evidence.","2377-5408;1530-0803","978-1-4244-6476-0978-1-4244-6475-3978-0-7695-4006","10.1109/ASWEC.2010.15","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5475064","Service Oriented Computing;Trust;Auction Mechanism;Software Architecture","Software testing;Automatic testing;Software algorithms;Application software;Performance evaluation;Particle swarm optimization;Automation;Genetic algorithms;Simulated annealing;Software engineering","data privacy;software architecture;user interfaces","reputation-based trustworthy services;service oriented architecture;user recommendations;auction mechanisms;auction-based trust negotiation protocol","","2","13","","","","","","IEEE","IEEE Conferences"
"Design and Stability Analyses of Floating Tidal Current Power Generation Test Platform","S. Wang; K. Yin; P. Yuan; H. Li; S. Li","NA; NA; NA; NA; NA","2010 Asia-Pacific Power and Energy Engineering Conference","","2010","","","1","4","Utilization of ocean tidal current, as a renewable energy, has attracted widespread attention of a number of research groups. In this paper, a floating tidal current power generation test platform to study flexible blades turbine, which is a novel ocean tidal current energy convert equipment, is designed. The modular design of the platform enables convenient operations in rotor replacement, parameter display and control, and automatic data collection. SESAM software is adopted to calculate the force and movement of the tidal power test platform under the resultant action of wind, wave, and tidal currents. The stability is analyzed to optimize the structure of the platform. Results show that the optimized model has good stability under the condition of a wind speed 20 m/s, wave height 3 m, and current velocity 2 m/s. The floating tidal current power generation test platform has also shown its availability under the above conditions when applying to real sea environment. The success in the performance of the sea trial prototype underscores the reliability and feasibility of our design.","2157-4839;2157-4847","978-1-4244-4812-8978-1-4244-4813","10.1109/APPEEC.2010.5448738","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5448738","","Stability analysis;Power generation;Testing;Oceans;Renewable energy resources;Blades;Turbines;Rotors;Displays;Automatic control","blades;power system stability;test equipment;tidal power stations;turbines","floating ocean tidal current power generation test platform;stability analyses;renewable energy;flexible blades turbine;ocean tidal current energy convert equipment;modular design;rotor replacement;parameter display;automatic data collection;SESAM software;parameter control;tidal power;sea environment","","","","","","","","","IEEE","IEEE Conferences"
"Design method of transparent water lens of model turbine taper pipe based on intelligent optimization algorithm","Y. Liu; Y. Zhao; D. Qin; H. Li","Harbin Institute of Large Electrical Machinery, Harbin Electric Machinery Company Limited, HEC, Harbin, China; Harbin Institute of Large Electrical Machinery, Harbin Electric Machinery Company Limited, HEC, Harbin, China; Harbin Institute of Large Electrical Machinery, Harbin Electric Machinery Company Limited, HEC, Harbin, China; Harbin Institute of Large Electrical Machinery, Harbin Electric Machinery Company Limited, HEC, Harbin, China","2011 International Conference on Electrical and Control Engineering","","2011","","","5822","5825","In order to solve the problems of observing cavitation during the testing process of model turbine, a design method of transparent water lens of model turbine taper pipe was proposed and the computer-aided design software was researched and developed. The dimension of water lens was obtained by the design method, with the least known quantity of model turbine, which is based on Fermat's principle and the improved genetic algorithm. At the same time, the field test condition was also considered. Design efficiency and accuracy were improved by the software which was developed by program languages of Visual Basic and Matlab. Working strength was reduced by the water lens which had the advantages of simple structure, low cost and practicality. It is suitable to be applied and popularized in field test.","","978-1-4244-8165-1978-1-4244-8162-0978-1-4244-8164","10.1109/ICECENG.2011.6057230","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6057230","Fermat's principle;Model turbine;Cavitation;Genetic algorithm","Lenses;Turbines;Mathematical model;Biological cells;Genetic algorithms;Software;Testing","CAD;cavitation;genetic algorithms;hydraulic systems;mechanical engineering computing;pipes;turbines;Visual BASIC","intelligent optimization algorithm;cavitation;testing process;model turbine;design method;transparent water lens;turbine taper pipe;computer-aided design software;Fermat principle;improved genetic algorithm;Visual Basic;Matlab;hydraulic machinery","","","7","","","","","","IEEE","IEEE Conferences"
"Optimal design of strap-down inertial navigation support under random loads","X. Hao; M. Li; H. Jia; M. Xuan","Changchun Institute of Optics, Fine Mechanics and Physics, the Chinese Academy of Science, Jilin 130033, China; Changchun Institute of Optics, Fine Mechanics and Physics, the Chinese Academy of Science, Jilin 130033, China; Changchun Institute of Optics, Fine Mechanics and Physics, the Chinese Academy of Science, Jilin 130033, China; Changchun Institute of Optics, Fine Mechanics and Physics, the Chinese Academy of Science, Jilin 130033, China","The 2010 IEEE International Conference on Information and Automation","","2010","","","1251","1255","In order to realize miniaturization and light weight of the strap-down inertial navigation system, and then to make sure that it works well under random loads, optimal design is applied to the strap-down inertial navigation support with the methods of topology optimization and size optimization. Firstly, based on the installation requirement of devices and connection requirement of the support and the carrier, the initial structure of the support is designed. Topology optimization with FEA software ANSYS is adopted on the initial structure to get the basic one. Then 5 critical sizes are chosen as design variables, and the support structure is optimized by means of size optimization to reach light weight with satisfying the requirement of dynamic stiffness. Finally, random vibration analysis is applied to the initial structure. In the mean time, random vibration test is carried out to qualify the analysis method. After the qualification, a random vibration analysis is applied to the optimized support structure to get the rms of displacement response and acceleration response of the support to validate whether the optimized structure is appropriate. The results indicate that the dynamic stiffness of the optimized support structure satisfies the design requirements, and its weight is lighter 49.38% than that of the initial one. This research can be a reference to the structure design of supports under random loads, and the result has been applied to the development and manufacture of a prototype aerocraft.","","978-1-4244-5704-5978-1-4244-5701-4978-1-4244-5702","10.1109/ICINFA.2010.5512119","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5512119","Strap-down inertial navigation system;Topology optimization;Size optimization;Random vibration analysis","Inertial navigation;Design optimization;Aerodynamics;Topology;Silicon compounds;Vibrations;Optical design;Acceleration;Frequency;Building materials","finite element analysis;inertial navigation;optimisation","strapdown inertial navigation support;random loads;topology optimization;size optimization;FEA software ANSYS;dynamic stiffness;random vibration analysis;prototype aerocraft","","3","7","","","","","","IEEE","IEEE Conferences"
"Fuzzy logic controller optimized by particle swarm optimization for DC motor speed control","R. Rahmani; M. S. Mahmodian; S. Mekhilef; A. A. Shojaei","Centre for Artificial Intelligence &amp; Robotics, Universiti Teknologi Malaysia, 54100 Kuala Lumpur, Malaysia; Department of Electrical Engineering, University of Malaya, 50603 Kuala Lumpur, Malaysia; Department of Electrical Engineering, University of Malaya, 50603 Kuala Lumpur, Malaysia; Centre for Artificial Intelligence &amp; Robotics, Universiti Teknologi Malaysia, 54100 Kuala Lumpur, Malaysia","2012 IEEE Student Conference on Research and Development (SCOReD)","","2012","","","109","113","In this paper, we presented an optimized fuzzy logic controller using particle swarm optimization for DC motor speed control. The controller model is simulated using MATLAB software and also experimentally tested on a laboratory DC motor. A comparison of the performance of different controllers such as PID controller, fuzzy logic controller and optimized fuzzy logic controller is presented as well. With reference to the results of digital simulations and experiment, the designed FLC-PSO speed controller obtains much better dynamic behavior compared to PID and the normal FLC designed. Moreover, it can acquire superior performance of the DC motor, and also perfect speed tracking with no overshoot. The optimized membership functions (MFs) are obviously proved to be able to provide a better performance and higher robustness in comparison with a regular fuzzy model, when the MFs were heuristically defined. Besides, experimental results verify the ability of proposed FLC under sudden change of the load torque which leads to speed variances.","","978-1-4673-5160-7978-1-4673-5158-4978-1-4673-5159","10.1109/SCOReD.2012.6518621","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6518621","fuzzy logic controller;particle swarm optimization;DC motor;optimization","","DC motors;fuzzy control;machine control;particle swarm optimisation;three-term control;velocity control","fuzzy logic controller;particle swarm optimization;DC motor speed control;Matlab software;laboratory DC motor;PID controller;digital simulations;FLC-PSO speed controller;dynamic behavior;membership functions;load torque;speed variances","","6","18","","","","","","IEEE","IEEE Conferences"
"Association rule mining algorithm study and improvement","L. Wang","Chongqing Industry Polytechnic College, China","2010 2nd International Conference on Software Technology and Engineering","","2010","2","","V2-362","V2-364","Through the in-depth study for the existing Apriori algorithm and its improved algorithms, this paper proposes an optimization program for the Apriori algorithm, and tests it. Experimental results show that this improved Apriori optimization association rule mining algorithm can reduce the time complexity of the original algorithm, especially for large databases, this improved algorithm has more evident effect for the association data mining optimization.","","978-1-4244-8666-3978-1-4244-8667","10.1109/ICSTE.2010.5608787","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5608787","data mining;association rule;Apriori algorithm","Algorithm design and analysis;Association rules;Databases;Software algorithms;Optimization;Software","data mining;optimisation;very large databases","association rule mining;Apriori algorithm;optimization program;large databases","","","8","","","","","","IEEE","IEEE Conferences"
"SEntre: A dynamic binary instrumentation infrastructure","F. Peng; X. Gao; N. Gu; J. Qiu","School of Computer Science and Technology, University of Science and Technology of China, Hefei China 230027; Institute of Computing Technology, Chinese Academy of Science, Beijing China 100080; School of Computer Science and Technology, University of Science and Technology of China, Hefei China 230027; Institute of Computing Technology, Chinese Academy of Science, Beijing China 100080","2011 International Conference on Electronics, Communications and Control (ICECC)","","2011","","","1016","1019","Dynamic binary instrumentation tools play a very important role in program analysis. The information gathered by these tools is quite useful in software development, testing, debugging, simulation and optimization. SEntre is such an instrumentation system with the features of efficiency, ease-to-use interface, transparency and comprehensiveness. To obtain good performance, we employ instrumentation while copying (IWC) mechanism. The interface provided by SEntre is very simple but powerful. User can develop different tools based on SEntre. To illustrate SEntre's versatility, we describe two tools, one is used to obtain memory access address and the other is an extremely fast simulator.","","978-1-4577-0321-8978-1-4577-0320-1978-1-4577-0319","10.1109/ICECC.2011.6066387","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6066387","instrumentation;program analysis;instrumentation model","Instruments;Runtime;Aerospace electronics;Context;Registers;Integrated circuit modeling;Educational institutions","program debugging;program diagnostics;program testing;software tools","SEntre;dynamic binary instrumentation infrastructure;dynamic binary instrumentation tool;program analysis;software development;software testing;software debugging;software simulation;software optimization;instrumentation while copying mechanism;memory access address","","","13","","","","","","IEEE","IEEE Conferences"
"Using Dependability Benchmarks to Support ISO/IEC SQuaRE","J. Friginal; D. d. Andres; J. Ruiz; R. Moraes","NA; NA; NA; NA","2011 IEEE 17th Pacific Rim International Symposium on Dependable Computing","","2011","","","28","37","The integration of Commercial-Off-The-Shelf (COTS) components in software has reduced time-to-market and production costs, but selecting the most suitable component, among those available, remains still a challenging task. This selection process, typically named benchmarking, requires evaluating the behaviour of eligible components in operation, and ranking them attending to quality characteristics. Most existing benchmarks only provide measures characterising the behaviour of software systems in absence of faults ignoring the hard impact that both accidental and malicious faults have on software quality. However, since using COTS to build a system may motivate the emergence of dependability issues due to the interaction between components, benchmarking the system in presence of faults is essential. The recent ISO/IEC 25045 standard copes with this lack by considering accidental faults when assessing the recoverability capabilities of software systems. This paper proposes a dependability benchmarking approach to determine the impact that faults (noted as disturbances in the standard) either accidental or malicious may have on the quality features exhibited by software components. As will be shown, the usefulness of the approach embraces all evaluator profiles (developers, acquirers and third-party evaluators) identified in the ISO/IEC 25000 ""SQuaRE"" standard. The feasibility of the proposal is finally illustrated through the benchmarking of three distinct software components, which implement the OLSR protocol specification, competing for integration in a wireless mesh network.","","978-1-4577-2005-5978-0-7695-4590","10.1109/PRDC.2011.13","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6133063","Dependability Benchmarking;Quality Evaluation;ISO/IEC SQuaRE","Benchmark testing;ISO standards;IEC standards;Software measurement;Software quality;Time measurement","benchmark testing;IEC standards;ISO standards;software quality;software reliability","ISO/IEC SQuaRE;commercial-off-the-shelf component;time-to-market;production cost;selection process;software quality system;COTS;ISO/IEC 25045 standard;accidental fault;recoverability capability;software system;dependability benchmarking;quality feature;software component;OLSR protocol specification;wireless mesh network","","3","25","","","","","","IEEE","IEEE Conferences"
"Using an FPGA-based fault injection technique to evaluate software robustness under SEEs: A case study","M. Portela-Garcia; A. Lindoso; L. Entrena; M. Garcia-Valderas; C. Lopez-Ongil; B. Pianta; L. B. Poehls; F. Vargas","Electronic Technology Department, Carlos III, University of Madrid (Spain); Electronic Technology Department, Carlos III, University of Madrid (Spain); Electronic Technology Department, Carlos III, University of Madrid (Spain); Electronic Technology Department, Carlos III, University of Madrid (Spain); Electronic Technology Department, Carlos III, University of Madrid (Spain); Electrical Engineering Dept., Catholic University - PUCRS (Brazil); Electrical Engineering Dept., Catholic University - PUCRS (Brazil); Electrical Engineering Dept., Catholic University - PUCRS (Brazil)","2011 12th Latin American Test Workshop (LATW)","","2011","","","1","6","Microprocessor-based system's robustness under Single Event Effects is a very current concern. A widely adopted solution to make robust a microprocessor-based system consists in modifying the software application by adding redundancy and fault detection capabilities. The efficiency of the selected software-based solution must be assessed. This evaluation process allows the designers to choose the more suitable robustness technique and check if the hardened system achieves the expected dependability levels. Several approaches with this purpose can be found in the literature, but their efficiency is limited in terms of the number of faults that can be injected, as well as the level of accuracy of the fault injection process. In this paper, we propose FPGA-based fault injection techniques to evaluate software robustness methods under Single Event Upset (SEU) as well as Single Event Transient (SET). Experimental results illustrate the benefits of using the proposed fault injection method, which is able to evaluate a high amount of faults of both types of events.","2373-0862","978-1-4577-1490-0978-1-4577-1489-4978-1-4577-1488","10.1109/LATW.2011.5985918","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5985918","Fault injection;SEU;SET;Embedded Software Design","Circuit faults;Pipelines;Robustness;Single event upset;Optimization;Software;Dictionaries","fault diagnosis;field programmable gate arrays;microprocessor chips;redundancy;software performance evaluation","FPGA-based fault injection technique;software robustness evaluation;SEE;microprocessor-based system robustness;redundancy;fault detection capability;dependability levels;single event upset;SET;single event transient;SEU","","3","10","","","","","","IEEE","IEEE Conferences"
"Minimization Algorithm of Unate Logic Functions","Q. Jianlin; G. Xiang; L. Fen; J. Dan; H. Peng","NA; NA; NA; NA; NA","2010 10th IEEE International Conference on Computer and Information Technology","","2010","","","2940","2945","Unate logic functions is an especially case of logic functions. For logic functions optimization, it costs more spaces and times, spending of memory is increase on two power by input variables. According by characteristic of unate logic functions, this paper introduces an algorithm of unate logic functions optimization and presents a method of algorithm realization. It analyzes influence factor of unate functions logic optimization efficiency. The software system overpasses the testing of right validate. The results of experimentation is proved to be correct and efficient. It is good for reduce spending of memory spaces and times. Optimizing of effect is reduced by increasing numbers of input variable in case of the same numbers of output variable, products and don't-care gene, and by increasing numbers of don't-care on case of the same numbers of input variable, output variable and products. The optimizing of effect is increased by increasing numbers of products on case of the same numbers of input variable, output variable and don't care gene.","","978-1-4244-7548-3978-1-4244-7547-6978-0-7695-4108","10.1109/CIT.2010.491","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5578498","Unate Logic Functions;Algorithm;Minimization;Complement Sets;Essential Prime Implicants","Logic functions;Optimization;Input variables;Redundancy;Testing;Software systems;Computers","Boolean algebra;formal logic;optimisation","minimization algorithm;logic functions optimization;unate logic functions;algorithm realization;software system;input variable numbers","","","11","","","","","","IEEE","IEEE Conferences"
"A Test Case Design Algorithm Based on Priority Techniques","H. Xian","NA","2011 Fifth International Conference on Management of e-Commerce and e-Government","","2011","","","57","62","Testing is an important step of building e-commerce system. In regression testing, it is the key issue that how to reuse the test suite efficiently. This paper presents a dynamic adjustment prioritization based on the design information of test suite which is a new exploration of regression test prioritization. It improves the shortcoming of the existing technologies which failed to use the design information of test cases effectively. It adjusts the priority of test case by collecting running information, gradually optimizes the test suite and makes the test suite adapted to the current test environment to obtain a better error detection results. Experiments show the error-detected efficiency of it has certain advantages than the existing algorithms.","","978-1-4577-1659","10.1109/ICMeCG.2011.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092632","test case;algorithm;priority techniques","Testing;Algorithm design and analysis;Heuristic algorithms;Complexity theory;History;Software algorithms;Correlation","electronic commerce;program testing","test case design algorithm;e-commerce system;regression testing","","","16","","","","","","IEEE","IEEE Conferences"
"Performance Improvement for Collection Operations Using Join Query Optimization","V. K. S. Nerella; S. K. Madria; T. Weigert","NA; NA; NA","2011 IEEE 35th Annual Computer Software and Applications Conference","","2011","","","668","673","Programming languages with explicit support for queries over collections allow programmers to express operations on collections more abstractly than relying on their realization in loops or through provided libraries. Join optimization techniques from the field of database technology support efficient realizations of such language constructs. We describe an algorithm that performs run-time query optimization and is effective for single runs of a program. The proposed approach relies on histograms built from the data at run time to estimate the selectivity of joins and predicates in order to construct query plans. Information from earlier executions of the same query during run time can be leveraged during the construction of the query plans, even when the data has changed between these executions. Experimental results demonstrate improvement over earlier approaches, such as JQL.","0730-3157;0730-3157;0730-3157","978-0-7695-4439-7978-1-4577-0544-1978-0-7695-4439","10.1109/COMPSAC.2011.93","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032415","collection operations;joins;query optimization","Histograms;Query processing;Optimization;Buildings;Benchmark testing;Frequency estimation","database management systems;query languages;query processing;software performance evaluation","collection operation performance improvement;programming language;explicit query support;join query optimization;database technology;language constructs;run-time query optimization;query plan construction","","2","16","","","","","","IEEE","IEEE Conferences"
"A software birthmark based on weighted k-gram","Xin Xie; Fenlin Liu; Bin Lu; Lin Chen","Zhengzhou Information Science and technology Institute, Henan, 450002, China; Zhengzhou Information Science and technology Institute, Henan, 450002, China; Zhengzhou Information Science and technology Institute, Henan, 450002, China; Zhengzhou Information Science and technology Institute, Henan, 450002, China","2010 IEEE International Conference on Intelligent Computing and Intelligent Systems","","2010","1","","400","405","A software birthmark is the invariable features of a program that can used to detect software theft. Software theft can be detected by a birthmark that can cover the whole behavior of a program. To improve the performance of k-grams of resisting semantics-preserving transformations, and also consider a birthmark should cover the whole behavior of a program. A static birthmark based on k-grams and their weights is proposed. Weight is computed by analyzing the rate of change of k-gram frequency between the original program and the transformed programs. Experimental results show that the performance of the proposed birthmark for Java applications is better than static k-gram birthmark against transformations performed by Smokescreen obfuscator, ZKM obfuscator and Jarg optimizer.","","978-1-4244-6585-9978-1-4244-6582-8978-1-4244-6584","10.1109/ICICISYS.2010.5658584","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5658584","software theft detection;software birthmark;semantics-preserving transformation;code obfuscation;k-gram","Libraries;Engines;Benchmark testing","Java;security of data;software engineering","software birthmark;weighted K-gram;software theft;semantics preserving transformation;static birthmark;k-gram frequency;transformed program;Java application;Smokescreen obfuscator;ZKM;Jarg optimizer","","6","25","","","","","","IEEE","IEEE Conferences"
"Apply particle swarm optimization to maximize the service reliability of grid computing system","S. Horng; F. Yang","Department of Computer Science &amp; Information, Engineering, Chaoyang University of Technology, Taichung, Taiwan, R.O.C.; Department of Biomedical Imaging and Radiological, Sciences, National Yang-Ming University, Taipei, Taiwan, R.O.C.","The International Conference on Information Networking 2011 (ICOIN2011)","","2011","","","235","240","In this paper, we propose an ordinal optimization (OO) based algorithm for solving the resource allocation optimization problem of grid computing system to maximize the service reliability. An approximate model is firstly proposed to estimate the service reliability of a resource allocation design within a tolerable computation time. Next, we employ the proposed algorithm to solve the resource allocation optimization problem. The OO based algorithm consists of two stages. A binary particle swarm optimization (BPSO) algorithm is employed in the first stage using the approximate model for fitness evaluation and selects a subset of good enough solutions. Then, we proceed with the goal softening searching procedure in the second stage using more refined approximate models to search for a good enough solution. We have demonstrated the test results by simulating on an 8-node and 11-link grid computing system including one resource-managing node. The good enough solution obtained by the proposed algorithm is promising in the aspects of solution quality and computational efficiency. In addition, the proposed algorithm spends only 2.35 minutes in a Pentium IV PC to obtain the good enough resource allocation design.","2332-5658;1550-445X;1976-7684","978-1-61284-663-7978-1-61284-661-3978-1-61284-662","10.1109/ICOIN.2011.5723185","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5723185","binary particle swarm optimization;ordinal optimization;grid computing system;service reliability;resource allocation","Approximation algorithms;Reliability;Resource management;Grid computing;Approximation methods;Algorithm design and analysis;Computational modeling","approximation theory;grid computing;particle swarm optimisation;resource allocation;software reliability","particle swarm optimization;service reliability;grid computing system;ordinal optimization;resource allocation optimization problem;approximate model;binary particle swarm optimization;goal softening searching procedure","","","15","","","","","","IEEE","IEEE Conferences"
"Improving virtualization performance and scalability with advanced hardware accelerations","Yaozu Dong; Xudong Zheng; Xiantao Zhang; Jinquan Dai; Jianhui Li; Xin Li; Gang Zhai; Haibing Guan","Intel China Software Center, Shanghai, China; Intel China Software Center, Shanghai, China; Intel China Software Center, Shanghai, China; Intel China Software Center, Shanghai, China; Intel China Software Center, Shanghai, China; Intel China Software Center, Shanghai, China; Intel China Software Center, Shanghai, China; Shanghai Key Laboratory of Scalable Computing and Systems, Shanghai Jiao Tong University, China","IEEE International Symposium on Workload Characterization (IISWC'10)","","2010","","","1","10","Many advanced hardware accelerations for virtualization, such as Pause Loop Exit (PLE), Extended Page Table (EPT), and Single Root I/O Virtualization (SR-IOV), have been introduced recently to improve the virtualization performance and scalability. In this paper, we share our experience with the performance and scalability issues of virtualization, especially those brought by the modern, multi-core and/or overcommitted systems. We then describe our work on the implementation and optimizations of the advanced hardware acceleration support in the latest version of Xen. Finally, we present performance evaluations and characterizations of these hardware accelerations, using both micro-benchmarks and a server consolidation benchmark (vConsolidate). The experimental results demonstrate an up to 77% improvement with these hardware accelerations, 49% of which is due to EPT and another 28% due to SR-IOV.","","978-1-4244-9296-1978-1-4244-9297","10.1109/IISWC.2010.5649499","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5649499","Virtualization;Virtual Machine;PLE;EPT;SR-IOV","Hardware;Acceleration;Driver circuits;Scalability;Optimization;Artificial intelligence;Iron","benchmark testing;multiprocessing systems;optimisation;virtual machines","advanced hardware acceleration;scalability;virtualization performance;multicore system;overcommitted system;optimization;Xen;microbenchmark;server consolidation benchmark;vConsolidate;virtual machine","","5","31","","","","","","IEEE","IEEE Conferences"
"Pin-count-aware online testing of digital microfluidic biochips","Y. Zhao; K. Chakrabarty","Department of Electrical and Computer Engineering, Duke University, Durham, NC 27708, USA; Department of Electrical and Computer Engineering, Duke University, Durham, NC 27708, USA","2010 28th VLSI Test Symposium (VTS)","","2010","","","111","116","On-line testing offers a promising method for detecting defects, fluidic abnormalities, and bioassay malfunctions in microfluidic biochips. To reduce product cost for disposable biochips, testing steps and functional fluidic operations must be implemented on pin-constrained designs. However, previous testing methods for pin-constrained designs do not optimize test schedules to reduce the number of control pins and test/assay completion time. We propose a pin-count-aware online testing method for pin-constrained designs to support the execution of both fault testing and the target bioassay protocol. The proposed method interleaves fault testing with the target bioassay protocol for online testing. It is aimed at significantly reducing the completion time for testing and for the bioassay, while keeping the number of control pins small. Two practical applications, namely a multiplexed bioassay and an interpolation-based mixing protocol, are used to evaluate the effectiveness of the proposed method.","2375-1053;1093-0167;1093-0167","978-1-4244-6650-4978-1-4244-6649-8978-1-4244-6648","10.1109/VTS.2010.5469602","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5469602","","Testing;Microfluidics;Pins;Protocols;Design automation;Design methodology;Biomedical electrodes;Drugs;Application software;Fault detection","fault diagnosis;lab-on-a-chip;microfluidics;testing;VLSI","pin-count-aware online testing;digital microfluidic biochip;defect detection;fluidic abnormality detection;bioassay malfunction detection;disposable biochip;fault testing;target bioassay protocol","","4","19","","","","","","IEEE","IEEE Conferences"
"Quality of service profiling","S. Misailovic; S. Sidiroglou; H. Hoffmann; M. Rinard","MIT CSAIL; MIT CSAIL; MIT CSAIL; MIT CSAIL","2010 ACM/IEEE 32nd International Conference on Software Engineering","","2010","1","","25","34","Many computations exhibit a trade off between execution time and quality of service. A video encoder, for example, can often encode frames more quickly if it is given the freedom to produce slightly lower quality video. A developer attempting to optimize such computations must navigate a complex trade-off space to find optimizations that appropriately balance quality of service and performance. We present a new quality of service profiler that is designed to help developers identify promising optimization opportunities in such computations. In contrast to standard profilers, which simply identify time-consuming parts of the computation, a quality of service profiler is designed to identify subcomputations that can be replaced with new (and potentially less accurate) subcomputations that deliver significantly increased performance in return for acceptably small quality of service losses. Our quality of service profiler uses loop perforation (which transforms loops to perform fewer iterations than the original loop) to obtain implementations that occupy different points in the performance/quality of service trade-off space. The rationale is that optimizable computations often contain loops that perform extra iterations, and that removing iterations, then observing the resulting effect on the quality of service, is an effective way to identify such optimizable subcomputations. Our experimental results from applying our implemented quality of service profiler to a challenging set of benchmark applications show that it can enable developers to identify promising optimization opportunities and deliver successful optimizations that substantially increase the performance with only small quality of service losses.","1558-1225;0270-5257","978-1-60558-719","10.1145/1806799.1806808","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062070","loop perforation;profiling;quality of service","Quality of service;Optimization;Benchmark testing;Distortion measurement;Manuals;Mathematical model","iterative methods;optimisation;quality of service;software metrics;software performance evaluation;software quality","service profiling quality;execution time;video encoder;quality of service profiler;loop perforation;benchmark applications;optimization opportunities","","30","26","","","","","","IEEE","IEEE Conferences"
"Ant colony optimization for solving combinatorial fuzzy Job Shop Scheduling Problems","P. Surekha; P. R. Mohanaraajan; S. Sumathi","PSG College of Technology, Coimbatore, India; PSG College of Technology, Coimbatore, India; Department of EEE, PSG College of Technology, Coimbatore, India","2010 International Conference on Communication and Computational Intelligence (INCOCCI)","","2010","","","295","300","In this paper, we present an ant colony optimization algorithm for solving the Job-shop Scheduling Problem (JSSP). Ant Colony Optimization (ACO) is a metaheuristic inspired by the foraging behavior of ants, which is also used to solve this combinatorial optimization problem. In JSSP ants move from one machine (nest) to another machine (food source) depending upon the job flow, thereby optimizing the sequence of jobs. The sequence of jobs is scheduled using Fuzzy logic and optimized using ACO. The makespan, completion time, makespan efficiency, algorithmic efficiency and the elapsed time for the ant colony algorithm are evaluated. Computational results of the optimization algorithm are evaluated by analyzing the two popular JSSP benchmark instances, FT10 and the ABZ10 problems and the simulation is carried out using the software, MATLAB.","","978-81-8371-369","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5738747","Job Shop Scheduling Problem;Fuzzy Logic;Ant Colony Optimization;Planning;Scheduling;Makespan","Job shop scheduling;Ant colony optimization;Processor scheduling;Optimization;Benchmark testing;Planning","combinatorial mathematics;fuzzy logic;fuzzy set theory;job shop scheduling;optimisation","ant colony optimization;combinatorial fuzzy job shop scheduling problem;combinatorial optimization problem;job sequences;fuzzy logic;ACO;JSSP benchmark instances;FT10 problems;ABZ10 problems","","","9","","","","","","IEEE","IEEE Conferences"
"Performance analysis of two-way cartridge calve based on AMESim and orthogonal test","Lei Tian; Jinjin Guo; Runsheng Yu","School of Mechanical Engineering, Tianjin University of Technology, China; School of Mechanical Engineering, Tianjin University of Technology, China; School of Mechanical Engineering, Tianjin University of Technology, China","2011 IEEE International Conference on Information and Automation","","2011","","","427","430","According to the two-way cartridge valve's small flow capacity in the actual application, our work is carried out to analysis the structure of the two-way cartridge valve by simulating the characteristic on AMESim software. Our team compares characteristics of the valve port with different spring stiffness, optimizes the structures of the valve by orthogonal test method. Simulation of hydraulic elements can replace some process of the physical test, and can help the designer more convenient to select and optimize design programs.","","978-1-4577-0270-9978-1-4577-0268-6978-1-4577-0269","10.1109/ICINFA.2011.5949030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5949030","Two-way Cartridge Valves;AMESim simulation;spring stiffness;orthogonal test method","Valves;Springs;Mathematical model;Integrated circuit modeling;Cavity resonators;Software;Simulation","hydraulic systems;mechanical engineering computing;springs (mechanical);valves","two-way cartridge valve small flow capacity;AMESim software;spring stiffness;orthogonal test method;hydraulic elements","","","9","","","","","","IEEE","IEEE Conferences"
"How to Recommend Appropriate Developers for Bug Fixing?","T. Zhang; B. Lee","NA; NA","2012 IEEE 36th Annual Computer Software and Applications Conference","","2012","","","170","175","Due to increased size and complexity of software, software maintenance has become a very difficult task for developers, especially on bug fixing. For famous open source systems, the number of daily submitted bug reports is very high. Unfortunately, as most of bug reports were not assigned to appropriate developers for fixing related bugs, these bug reports need to be reassigned. A larger number of reassignments cause the time of bug fixing to increase and the fixing probability to decrease. In order to resolve this problem, it is necessary to recommend appropriate developers for fixing bugs. In this paper, we propose a new developer recommendation method for assigning appropriate developers to fix bugs. This method, based on social network and experts' feedback, recommends some candidate developers. We also consider the fixing efficiency and the experience of these candidate developers to rank the candidate developers. We have validated our approach on the famous software bug repository and showed the feasibility and availability of the method by experiments.","0730-3157;0730-3157;0730-3157","978-1-4673-1990-4978-0-7695-4736","10.1109/COMPSAC.2012.27","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6340140","bug fixing;social network;fixing efficiency;feedback;developer recommendation;reassignment","Social network services;Computer bugs;Support vector machines;Feature extraction;Clustering algorithms;Testing;Software","probability;program debugging;public domain software;software maintenance","bug fixing;software size;software complexity;software maintenance;software developer;open source system;bug report;fixing probability;developer recommendation method;fixing efficiency;software bug repository","","","17","","","","","","IEEE","IEEE Conferences"
"Software and Hardware Co-designed Multi-level TLBs for Chip Multiprocessors","X. Zhang; M. Cong; G. Chen","NA; NA; NA","2011 IEEE 11th International Conference on Computer and Information Technology","","2011","","","609","614","Translation Look aside Buffers (TLBs) have a significant impact on system performance. Numerous prior studies focus on TLBs design for uniprocessors. As the advent of chip multiprocessors (CMPs), we need shift to TLBs for chip multiprocessors. This paper presents a software-implemented level-two TLB -- SL2-TLB which is a shared level-two TLB for multiprocessors. It not only reduces the cost of TLB refill handler for every processor core, but also reduces the redundant TLB misses' cost for CMPs effectively. Today, CMPs typically employ private per-core TLBs. SL2-TLB together with the hardware TLBs make up a software-hardware co-designed multilevel TLB system which brings great benefit to system performance while avoiding changing the hardware TLB. So it is a convenient and efficient method for CMPs' TLB performance improvement. The benefit brought by SL2-TLB to SPECCPU2000 is less than that to SPECCPU2006, about 5% and 7% separately. Therein to, the average performance improvement of SPECint 2006 reaches about 12.7%. That is because the overhead for TLB refill is small when the cache is large enough to avoid a miss as walking the page table of applications with small memory footprints. The further optimization for SL2-TLB is kept the SL2-TLB table stay in L2 cache forever by the cache locking scheme. SL2-TLB together with cache locking scheme improves the performances by over 13% for SPECint 2006. And an average performance improvement of over 7% is brought to the new emerging parallel benchmark suite-Princeton Application Repository for Shared-Memory Computers (PARSEC). And all the above evaluations are done on Godson-3 processors which is the latest generation of China's most powerful microprocessor family.","","978-1-4577-0383-6978-0-7695-4388","10.1109/CIT.2011.17","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6036833","Chip Multiprocessors (CMPs);Software-implemented shared level-two TLB;parallel benchmarks;redundant TLB misses;Godson-3 processors","Benchmark testing;Hardware;Computers;Prefetching;Computer architecture;System performance;Kernel","cache storage;circuit optimisation;hardware-software codesign;microprocessor chips;paged storage;performance evaluation;shared memory systems","chip multiprocessor;software-hardware codesign;multilevel TLB design;translation lookaside buffer;CMP;TLB refill handler;processor core;software-implemented level-two TLB;SPECint 2006 performance improvement;memory footprints;page table;SL2-TLB optimization;L2 cache;cache locking scheme;parallel benchmark suite;Princeton application repository for shared-memory computers;PARSEC;Godson-3 processor","","","23","","","","","","IEEE","IEEE Conferences"
"Tool for performance tuning and regression analyses of HPC systems and applications","S. Merchant; G. Prabhakar","High Performance Computing, IBM India; High Performance Computing, IBM India","2012 19th International Conference on High Performance Computing","","2012","","","1","6","Increasing sophistication in High Performance Computing (HPC) system architectures, software, and user environments has substantially increased its complexity. For example, tuning an application on a given platform to maximize performance requires playing with multitude of different optimization flags and environment variables. This is typically a highly repetitive and an ad hoc process of trying out different combinations of variable settings and manually comparing the results to find the optimal. Similar is the case for performance regression analyses of systems and applications, where one is interested in detecting performance regressions in the software version under test and analyzing the causes to fix issues. In both of these scenarios, the process involves creating a patchwork of scripts to deploy jobs, extract meaningful data from raw outputs, arrange this data in some reportable format to be able to analyze, and perform tweaks to enable subsequent iterations. When repeated over time, the ad-hoc process results in users re-writing similar set of scripts again and again for different applications, or architectures, or even new software builds on the same architecture resulting in significant wastage of productive man-hours. This paper presents JACE (Job Auto-creator and Executor), a tool that enables automation of creation and execution of complex functional and performance regression tests. JACE aims to address many pain points in performance engineering such as tedious scripting, parameter tuning, careful book-keeping, frequent debugging, assessing reliability of results, and comparative evaluation. The paper introduces the tool and describes its architecture and workflow. It also presents a sample walk-through of performance regression analyses using JACE.","","978-1-4673-2371-0978-1-4673-2372-7978-1-4673-2370","10.1109/HiPC.2012.6507528","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6507528","Performance Regression;Performance Analysis;Application Tuning;Application Optimization;High Performance Computing","","parallel processing;program debugging;program testing;regression analysis;software architecture;software performance evaluation","performance tuning;regression analyses;HPC system;HPC applications;high performance computing;system architecture;user environment;application tuning;optimization flag;environment variables;performance regression analysis;software version under test;script patchwork;job deployment;data extraction;data analysis;ad-hoc process;script rewriting;software build;software architecture;JACE tool;Job Auto-creator and Executor;performance engineering;debugging;result reliability assessment;comparative evaluation","","1","10","","","","","","IEEE","IEEE Conferences"
"The Optimization of Xen-Based Display Virtualization","F. Liu; G. Wu; R. Xie","NA; NA; NA","2010 International Conference on Computational Intelligence and Software Engineering","","2010","","","1","4","Virtualization technology can reuse the hardware platform to provide multiple isolated virtual system environments; however, this also causes unsatisfactory performance of the virtual guest system, especially the display in virtual guest system. In this paper, we study the virtual device model and improve the way of original virtual device implement based on Xen virtualization. A new display method is adopted which does improve the display environment of virtual guest system effectively.","","978-1-4244-5391-7978-1-4244-5392","10.1109/CISE.2010.5677201","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5677201","","Driver circuits;Hardware;Graphics;Performance evaluation;Streaming media;Linux;Testing","computer displays;optimisation;virtual machines;virtualisation","xen-based display virtualization;virtualization technology;optimization;virtual guest system","","","7","","","","","","IEEE","IEEE Conferences"
"Processing Parameter Optimization for Injection Molding Products Based on Orthogonal Experimenter and Analysis","Y. Wei","NA","2011 Second International Conference on Digital Manufacturing & Automation","","2011","","","871","873","There are many quality-related factors in the manufacture of injection molding products of agriculture equipment. It is a trouble to find out the key factors and to optimize the process plan with these key factors. This study introduces a practice way: to design the factors test based on orthogonal experiment, and to analog test with software Mold flow MPI 6.0, then to handles the data with range and variance analysis theory, finally turn out the optimization process plan.","","978-1-4577-0755-1978-0-7695-4455","10.1109/ICDMA.2011.215","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6052048","injection molding;orthogonal experiment;processing parameter optimization","Injection molding;Optimization;Temperature;Plastics;Plastic products;Software;Presses","agricultural machinery;CAD/CAM;injection moulding","parameter optimization;injection molding products;agriculture equipment;orthogonal experiment;Mold flow MPI 6.0","","","7","","","","","","IEEE","IEEE Conferences"
"The pipeline analysis of the complex algorithms adopted in the DSP based image processing","Li Shuhua; Tie Yong; Guo Gaizhi","College of Electronic and Information Engineering, Inner Mongolia University, Hohhot, China; College of Electronic and Information Engineering, Inner Mongolia University, Hohhot, China; College of Computer and Information Technology, Inner Mongolia Normal University, Hohhot, China","2010 2nd International Conference on Education Technology and Computer","","2010","3","","V3-13","V3-15","Algorithm optimization is an important part of DSP real-time system. In image processing, many core algorithms are the image traversal manipulation. Because there are a large number of multi-loop and generally loop is the best opportunity to improve the algorithm efficiency, those core algorithms efficiency can be improved greatly if the high effective software pipeline can be implemented. C6000 is a kind of chip with very long instruction word (VLIW) architecture, which can execute up to eight instructions in parallel. Adopting this parallelism is central to achieving peak performance. Instruction parallelism in C/C++ code is a kind of software pipelining to the loops. In software pipelining, multiple iterations of a loop are executed simultaneously in software. In this paper, based on the actual code used in the real project, an algorithm to analysis algorithm complexity and run effectively in DSP is presented, so a significant increase in speed of the real-time image processing is possible.","2155-1812","978-1-4244-6370-1978-1-4244-6367-1978-1-4244-6369","10.1109/ICETC.2010.5529610","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5529610","DSP algorithm optimization;image processing;software pipeline","Image analysis;Algorithm design and analysis;Digital signal processing;Image processing;Pipeline processing;Software algorithms;Testing;Educational institutions;VLIW;Parallel processing","computational complexity;digital signal processing chips;image processing;instruction sets;parallel architectures;pipeline processing;real-time systems","pipeline analysis;complex algorithm;DSP based image processing;real-time system;algorithm optimization;image traversal manipulation;software pipelining;very long instruction word architecture;C6000 chip;instruction parallelism;algorithm complexity analysis","","","5","","","","","","IEEE","IEEE Conferences"
"Notice of Retraction<br>Research on optimization design of heavy-duty truck frame based on the sensitivity","Han Quan-li; Tian Lin-hong; Qu Ling-jin","Department of Mechanical and Electronic Engineering, Henan Polytechnic Institute, Nan yang, China; Department of Mechanical and Electronic Engineering, Henan Polytechnic Institute, Nan yang, China; Department of Mechanical and Electronic Engineering, Henan Polytechnic Institute, Nan yang, China","2010 3rd International Conference on Computer Science and Information Technology","","2010","9","","745","748","Article will introduce the sensitivity into frame optimization analysis of the heavy-duty truck. Established the frame finite element model in a certain type of heavy truck, determined the boundary conditions and loads, and verified the electrical test. Select the frame rails and the beams as a sensitivity analysis of the object. According to the results of the analysis to select the design variables, with the maximum stress and displacement of the body as state variables, with the total mass of the body as the objective function, in order to achieve the optimization of the heavy truck.;Notice of Retraction<br><br>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<br><br>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<br><br>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<br><br>","","978-1-4244-5540-9978-1-4244-5537-9978-1-4244-5539","10.1109/ICCSIT.2010.5564508","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5564508","heavy-duty truck frame;finite elements analysis;sensitivity analysis;lightweight optimization","Sensitivity;Software","design engineering;finite element analysis;optimisation;road vehicles;sensitivity analysis","optimization design;heavy-duty truck frame;finite elements analysis;sensitivity analysis","","","7","","","","","","IEEE","IEEE Conferences"
"Optimization Design of Submersible Mixer Based on Simulation Study of Agitated Flow Field","X. Weixing; Y. Jianping","NA; NA","2011 Third International Conference on Measuring Technology and Mechatronics Automation","","2011","2","","807","810","The submersible mixer as major sewage treatment equipment is very important to sewage treatment plant. The numerical simulation of the internal 3-D incompressible turbulent flow agitated flow field was carried out. By changing some design parameters of mixer impeller, we can analyze the numerical simulation results, they showed that: the impeller produced vortex jet flow, the constant velocity lines advanced as ellipse, the velocities along the centerline are larger than others. For the same impeller diameter, the impeller whose hub-tip ratio is smaller has larger advancing speed in the agitated flow field, and the advancing speed is more evenly distributed, so the overall effect is better. The PEO/E software was applied to design the impeller. Using the results of the optimization design, we can deal with the actual engineering on the design, manufacture and testing of the submersible mixer. The operational results are good, and the benefits of economy and society are remarkable.","2157-1473;2157-1481","978-1-4244-9010","10.1109/ICMTMA.2011.485","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5721311","Optimization Design;Submersible Mixer;Agitated Flow Field;Simulation","Impellers;Mixers;Underwater vehicles;Mathematical model;Equations;Boundary conditions;Solid modeling","computational fluid dynamics;design engineering;flow simulation;impellers;jets;mixing;numerical analysis;optimisation;sewage treatment;solid modelling;turbulence;vortices","submersible mixer;sewage treatment equipment;numerical simulation;internal 3-D incompressible turbulent flow;agitated flow field;mixer impeller;vortex jet flow;constant velocity lines;hub-tip ratio;PEO/E software;optimization design","","","12","","","","","","IEEE","IEEE Conferences"
"A model checking strategy to test services in orchestrations","F. Corradini; F. De Angelis; G. De Angelis; A. Polini; A. Polzonetti","Department of Mathematics and Computer Science, University of Camerino, Via, Madonna delle Carceri 9, Camerino, 62032, Italy; Department of Mathematics and Computer Science, University of Camerino, Via, Madonna delle Carceri 9, Camerino, 62032, Italy; ISTI-CNR, Via G. Moruzzi 1, Pisa, 56124, Italy; Department of Mathematics and Computer Science, University of Camerino, Via, Madonna delle Carceri 9, Camerino, 62032, Italy; Department of Mathematics and Computer Science, University of Camerino, Via, Madonna delle Carceri 9, Camerino, 62032, Italy","2010 Future Network & Mobile Summit","","2010","","","1","8","Very late binding, run-time integration of software elements owned and managed by third parties, run-time changes. These are just some of the characteristics of the service oriented computing paradigm which strongly affect static and dynamic verification capabilities. In this domain verification and testing research communities have to face new issues and revise existing solutions; possibly profiting of the new opportunities that the new paradigm makes available. In this paper, focusing on service orchestrations, we propose a novel approach to automatic test case generation aiming in particular at checking the behaviour of services participating to a given orchestration. The approach exploits the availability of a runnable model and uses model checking techniques to derive test cases suitable to detect possible integration problems.","","978-1-905824-18-2978-1-905824-16","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5722450","Counter-example based Testing;Service Testing;Service Orchestrations","Testing;Optimized production technology;Protocols;Computational modeling;Software;System-on-a-chip;Organizations","formal verification","model checking strategy;test services orchestrations;run-time integration;software elements;domain verification;runnable model","","","7","","","","","","IEEE","IEEE Conferences"
"Automatic Metro Map Layout Using Multicriteria Optimization","J. Stott; P. Rodgers; J. C. Martinez-Ovando; S. G. Walker","University of Kent, Canterbury; University of Kent, Canterbury; University of Kent, Canterbury; University of Kent, Canterbury","IEEE Transactions on Visualization and Computer Graphics","","2011","17","1","101","114","This paper describes an automatic mechanism for drawing metro maps. We apply multicriteria optimization to find effective placement of stations with a good line layout and to label the map unambiguously. A number of metrics are defined, which are used in a weighted sum to find a fitness value for a layout of the map. A hill climbing optimizer is used to reduce the fitness value, and find improved map layouts. To avoid local minima, we apply clustering techniques to the map-the hill climber moves both stations and clusters when finding improved layouts. We show the method applied to a number of metro maps, and describe an empirical study that provides some quantitative evidence that automatically-drawn metro maps can help users to find routes more efficiently than either published maps or undistorted maps. Moreover, we have found that, in these cases, study subjects indicate a preference for automatically-drawn maps over the alternatives.","1077-2626","","10.1109/TVCG.2010.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5406516","Information visualization;diagram layout;graph drawing.","Optimization methods;Application software;System testing;Visualization;Geometry;Network topology;Navigation;Humans;Computer networks;Cancer","geographic information systems;optimisation;pattern clustering","automatic metro map layout;multicriteria optimization;automatic mechanism;metro map drawings;hill climbing optimizer;clustering techniques;published maps;undistorted maps","Algorithms;Computer Graphics;Computer Simulation;Computer-Aided Design;Humans;Information Storage and Retrieval;Maps as Topic;Pattern Recognition, Automated;Software;User-Computer Interface","29","29","","","","","","IEEE","IEEE Journals & Magazines"
"Notice of Retraction<br>Study on optimizing advance angle of injection of the armored vehicle diesel engine at different loads","Qingguo Luo; Hongbin Liu; Jie Zhang; Yong Gui","Department of Mechanical Engineering, AAFE, Beijing, China; Department of Mechanical Engineering, AAFE, Beijing, China; Military Agent Department, 318 Factory, Beijing, China; Department of Mechanical Engineering, AAFE, Beijing, China","2010 International Conference on Computer Application and System Modeling (ICCASM 2010)","","2010","1","","V1-294","V1-299","The GT-POWER software was used to build the working process model of the diesel engine; through the bench test of the diesel engine, the simulation results were contrasted with the test results to proof the accuracy. The advance angle of injection of diesel engine was optimized at different loads of 2200r/min 1500r/min, the major parameters regularity of the diesel engine was further studied and the optimum advance angle of injection was got. The foundation was provided gifts to carry out the electric control injection to improve the power and economy performance of the armored vehicle diesel engine.;Notice of Retraction<br><br>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<br><br>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<br><br>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<br><br>","2161-9069;2161-9077","978-1-4244-7237-6978-1-4244-7235","10.1109/ICCASM.2010.5619418","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5619418","diesel engine;GT-POWER;advance angle of injection;DOE","Engines;Software;Load modeling","diesel engines;mechanical engineering computing;military vehicles;software engineering","advance angle of injection;armored vehicle diesel engine;GT-POWER software;working process model;bench test;major parameters regularity;electric control injection","","","3","","","","","","IEEE","IEEE Conferences"
"Program flow graph oriented analysis of coverage and time performance","Z. Jiang; Y. Mu; Z. Zhang","Open Computer System Laboratory Beijing Information & Science Technology University Beijing, China; Open Computer System Laboratory Beijing Information & Science Technology University Beijing, China; Computer School Beijing Information Science & Technology University Beijing, China","2010 2nd IEEE International Conference on Information Management and Engineering","","2010","","","439","443","In order to track the dynamic information of the tested system, get the analysis of coverage and time performance, It is critical to optimize the test technique efficiency. With the help of instrumentation to identify program block, this paper proposed an algorithm IPFG for constructing program flow graph, which is based on analyzing instrumentation information derived from program block This approach comes up with a dynamic test framework based on program PGF instrumentation pretreatment. Experimental results show that, by using this framework to fulfill test instrumentation, coverage analysis and time performance analysis, the algorithm prove to be of high efficiency and accuracy.","","978-1-4244-5263-7978-1-4244-5265","10.1109/ICIME.2010.5478147","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5478147","IPGF algorithm;Instrumentation;Coverage;Time Performance","Flow graphs;Performance analysis;Instruments;Logic testing;Probes;Information analysis;Algorithm design and analysis;Software testing;Optimization methods;Laboratories","data flow graphs;dynamic testing;instrumentation;program testing","program flow graph;time performance;IPFG;instrumentation information;dynamic test;program PGF instrumentation;coverage analysis","","3","8","","","","","","IEEE","IEEE Conferences"
"Apply ant colony to event-flow model for graphical user interface test case generation","Y. Huang; L. Lu","South China University of Technology, People¿s Republic of China; South China University of Technology, People¿s Republic of China","IET Software","","2012","6","1","50","60","Aimed at the complex and inefficient testing issues of graphical user interface (GUI) software, this study describes an automatic framework for GUI test case generation. The framework includes a reverse engineering of executive GUI to create GUI information and event-flow model. According to these information and model, an ant colony algorithm is applied to generate test cases. The study shows that by identifying valid event interactions, the generated test cases are capable of finding deep faults without infeasible problems; the configuration of different parameters for ant colony and the good design of the proposed framework provide flexible interfaces to control test suites.","1751-8806;1751-8814","","10.1049/iet-sen.2011.0012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6151634","","","ant colony optimisation;graphical user interfaces;program testing;reverse engineering","ant colony algorithm;event-flow model;graphical user interface test case generation;graphical user interface software testing;reverse engineering;GUI information","","1","","","","","","","IET","IET Journals & Magazines"
"Design and implementation of dynamic sink node placement using Particle Swarm Optimization for life time maximization of WSN applications","R. Alageswaran; R. Usha; R. Gayathridevi; G. Kiruthika","K.L.N. College of Engineering, Madurai, Tamilnadu, India; ULTRA College of Engg.&amp; Tech for Women, Madurai, Tamilnadu, India; Smartek Consultancy Services, Chennai, Tamilnadu, India; TCS, Chennai, Tamilnadu, India","IEEE-International Conference On Advances In Engineering, Science And Management (ICAESM -2012)","","2012","","","552","555","Wireless Sensor Networks (WSN) are emerging because of the developments in wireless communication technology and miniaturization of the hardware. WSN consists of a large number of low-cost, low-power, multifunctional sensor nodes to monitor physical conditions, such as temperature, sound, vibration, pressure, motion, etc. As the energy available in the sensor nodes used in WSN is limited, the primary focus of WSN applications is to maximize the network life time by using the energy efficiently. Hence making a good use of energy is important in WSN application. There are techniques to utilize the energy in an efficient way. One such technique is to place the sink node in an optimal position. The widely used technique for finding optimal location for sink node is Particle Swarm Optimization (PSO). In this paper, an efficient method for finding optimized position of sink node using PSO is proposed. The initial constraints in finding optimal Base-Station (BS) locations in two-tiered wireless sensor networks proposed by B. Paul et al. using PSO are relaxed by placing Application Nodes (AN) dynamically based on Euclidean distance and probability of selection. This system is tested by establishing communication between the nodes and sink through the application nodes using query-driven model of WSN. The energy spent in transmission, reception and processing of query is analyzed and the results are presented.","","978-81-909042-2-3978-1-4673-0213-5INAVLID IS","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6215905","AN;BS;PSO;WSN","Wireless sensor networks;Mathematics;Temperature sensors;Particle swarm optimization;Robustness;Application software","particle swarm optimisation;wireless sensor networks","dynamic sink node placement;particle swarm optimization;life time maximization;WSN;wireless sensor network;wireless communication technology;low-cost sensor node;low-power sensor node;multifunctional sensor node;network life time;PSO;optimal base-station location;application nodes;Euclidean distance;query-driven model;query transmission;query reception;query processing","","","8","","","","","","IEEE","IEEE Conferences"
"A comparison of the four evolutionary programming implementations used as function optimizers","Shaojun Luan; Xuedong Gao; Bing Qi","School of Economics and Management of University of Science and Technology BeiJing, China; School of Economics and Management of University of Science and Technology BeiJing, China; Department of Computer Science and Software Engineering, 107 Dunstan Hall, Auburn University, AL 36849-5347, USA","2010 International Conference on E-Health Networking Digital Ecosystems and Technologies (EDT)","","2010","1","","421","425","In this artical, four evolutionary programming algorithms (Standard EP, Meta-EP, Continuous Standard EP and Continuous Meta-EP algorithms) are applied to test to maximizing a solution against a highly multimodal test objective function. We will use the same population size to compare the effectiveness of these four algorithms according to their success rate, average number of function evaluation and average best fitness. We experiments with different population size.","","978-1-4244-5517-1978-1-4244-5514-0978-1-4244-5516","10.1109/EDT.2010.5496545","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5496545","Evolutionary Programming;Optimization;maximize;fitness","Functional programming;Genetic programming;Genetic mutations;Software testing;Software algorithms;Software standards;Stochastic processes;Ecosystems;Computer network management;Conference management","demography;evolutionary computation","function optimizers;continuous standard evolutionary programming;continuous meta evolutionary programming;multimodal test objective function","","","4","","","","","","IEEE","IEEE Conferences"
"Design flow optimization for controller electronics in alternative electrical power systems using VHDL-AMS","S. Slawinski; L. Zacharias; M. Bodach; T. Hempel; B. Veit","Faculty of Electrical Engineering, University of Applied Sciences Zwickau, Zwickau, Germany; Faculty of Electrical Engineering, University of Applied Sciences Zwickau, Zwickau, Germany; Faculty of Electrical Engineering, University of Applied Sciences Zwickau, Zwickau, Germany; Faculty of Electrical Engineering, University of Applied Sciences Zwickau, Zwickau, Germany; Faculty of Electrical Engineering, University of Applied Sciences Zwickau, Zwickau, Germany","International Multi-Conference on Systems, Signals & Devices","","2012","","","1","3","This paper describes the development and use of a tool for model-driven, automated software development to control electronics design by using VHDL-AMS. As test applications serve alternative electrical power systems. The different parts of such a design process will be described. The paper focuses on the way of the code generator operation, which transfers the hardware description language VHDL-AMS into the corresponding C code for programming the control electronics.","","978-1-4673-1591-3978-1-4673-1590-6978-1-4673-1589","10.1109/SSD.2012.6197939","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6197939","Model Based Design;Software Development;VHDL-AMS;C/C++;Compiler;HIL;Alternative Electrical Power Systems;Super Capacitors;Controller Electronics","Hardware;Load modeling;Generators;Mathematical model;Programming;Adaptation models;Batteries","C language;hardware description languages;optimisation;power engineering computing;program compilers","VHDL-AMS;electrical power system;code generator operation;hardware description language;C code;design flow optimization;controller electronics;software development","","","18","","","","","","IEEE","IEEE Conferences"
"Research for SDL and TTCN-Based EMM Testing","X. Li; L. Xiao; X. Huang","NA; NA; NA","2011 7th International Conference on Wireless Communications, Networking and Mobile Computing","","2011","","","1","4","Protocol conformance testing is a very important step in implementation of protocols. The EMM protocol describes the function for EPS mobility management, so it is indispensable for the development of TD-LTE protocol stack. In the implementation and development phase of TD-LTE terminal tester, it must be used in order to tally with the protocol specification. In this article, a method based on Telelogic Tau 4.0 software is proposed. At the end of this paper, the testing method is verified by simulating and is optimized.","2161-9654;2161-9646;2161-9646","978-1-4244-6252-0978-1-4244-6250-6978-1-4244-6251","10.1109/wicom.2011.6040408","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6040408","","Testing;Protocols;Wireless communication;Software;Telecommunications;Educational institutions;Programming","Long Term Evolution;protocols","SDL;TTCN-based EMM testing;protocol conformance testing;EPS mobility management;TD-LTE protocol stack;Telelogic Tau 4.0 software","","","9","","","","","","IEEE","IEEE Conferences"
"Parametric optimization of a tracking system for the photovoltaic modules","M. A. Ionita; C. Alexandru","Department of Product Design, Mechatronics and Environment, Transilvania University of Brasov, Romania; Department of Product Design, Mechatronics and Environment, Transilvania University of Brasov, Romania","Proceedings of 2012 IEEE International Conference on Automation, Quality and Testing, Robotics","","2012","","","313","318","The paper presents the optimization of a tracking system for the photovoltaic modules. The study targets on the one hand the optimization of the mechanical device of the tracking mechanism (developed with the MBS ADAMS software), and on the other hand the optimization of the control system (conceived with DFC EASY5 software). The two major components of the mechatronic system, which are approached with the same technique of optimal parametric design, are integrated at the level of the virtual prototype. The optimization study leads to the minimization of the energy consumption necessary for the tracking (by minimizing the necessary motor forces) and respectively the maximization of the contribution of the incident solar radiation (by minimizing the tracking errors).","","978-1-4673-0704-8978-1-4673-0701-7978-1-4673-0703","10.1109/AQTR.2012.6237723","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6237723","tracking mechanism;virtual prototype;control system;optimization","Optimization;Tracking;Control systems;Solar radiation;Software;Photovoltaic systems;Joints","energy consumption;minimisation;photovoltaic power systems;power engineering computing;solar energy concentrators;virtual prototyping","parametric optimization;tracking system;photovoltaic module;mechanical device;control system optimization;mechatronic system;optimal parametric design;virtual prototype;minimization;energy consumption;solar radiation","","","10","","","","","","IEEE","IEEE Conferences"
"Feature Location Using Data Mining on Existing Test-Cases","C. Ziftci; I. Krüger","NA; NA","2012 19th Working Conference on Reverse Engineering","","2012","","","155","164","Feature location is the mapping of features onto static artifacts such as source code and configuration files. Recent effective feature location methods use dynamic-analysis and scenarios, i.e. executable use-cases that represent features in source code. Almost all of these techniques require that a distinct scenario is created for every feature, which puts a great burden on developers due to the inherent manual work that needs to be carried out. First, since scenarios are typically created and exercised manually, the act of locating features is not easily repeatable as software evolves. Second, there may be dependencies between features, which will be reflected on the collected execution traces during scenario execution. For existing feature location methods to perform well, developers typically need to mark the execution traces manually to specify which parts of it exactly represent each feature. In this work, we free developers of the manual process of creating scenarios by using association rule learning on a set of labeled test-cases, i.e. test-cases labeled with the features they exercise in the existing test-suite of the system, to locate features in source code. We also provide an evaluation of our method on three case studies and compare it with a well-known feature location technique that uses probabilistic ranking. Our method achieves results within 83-97% of the probabilistic ranking method on the case studies without any need to create scenarios as in the existing dynamic-analysis based feature location techniques.","2375-5369;1095-1350","978-0-7695-4891-3978-1-4673-4536","10.1109/WCRE.2012.25","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6385111","feature location;program features;data mining;program comprehension;software maintenance;reverse engineering","Association rules;Graphical user interfaces;Software;Reliability;Manuals;Servers","data mining;probability;software maintenance;system monitoring","feature location;data mining;test-cases;feature mapping;static artifact;source code;configuration file;dynamic analysis;software evolution;association rule learning;probabilistic ranking","","1","27","","","","","","IEEE","IEEE Conferences"
"Design and optimization of supporting structure for scanning mirror in aviation remote sensor","Li Yan-wei; Yang Hongbo; Yuan Guo-qin; Ding Ya-lin; Zhang Ji-chao; Cheng Zhi-feng","Changchun Institute of Optics, Fine Mechanics and Physics, Chinese Academy of Sciences, 130033, China; Changchun Institute of Optics, Fine Mechanics and Physics, Chinese Academy of Sciences, 130033, China; Changchun Institute of Optics, Fine Mechanics and Physics, Chinese Academy of Sciences, 130033, China; Changchun Institute of Optics, Fine Mechanics and Physics, Chinese Academy of Sciences, 130033, China; Changchun Institute of Optics, Fine Mechanics and Physics, Chinese Academy of Sciences, 130033, China; Changchun Institute of Optics, Fine Mechanics and Physics, Chinese Academy of Sciences, 130033, China","2011 International Conference on Electric Information and Control Engineering","","2011","","","3134","3138","An optical supporting structure is studied to improve its structure stiffness and reduce the rotate error to meet the requirement of aviation remote sensor. A formula to calculate fundamental frequency is proposed using Rayleigh-Ritz method, and a method of is analyzed. By means of OPTISTRUCT software, the maximum fundamental frequency is converted to object function, the deformation under the gravity and the mass are assigned to state variable, wall thicknesses of the structure are converted to design variables. The analysis and test results indicate that the fundamental frequency of designed optical supporting structure has been improved 37.2 Hz from 96.9 Hz, which guarantee the reliability of the sweep mechanism, and the rotate error is 4.89"", which is satisfied for the rotate accuracy in plunge angle. The method of optimization has a certain instructional significance for the design of supporting structures in aviation remote sensor.","","978-1-4244-8039-5978-1-4244-8036-4978-1-4244-8038","10.1109/ICEICE.2011.5778026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5778026","Scanning device;45° scanning mirror;Rayleigh-Ritz method;Finite element method;Optimum design","Mirrors;Optimization;Cameras;Remote sensing;Optical sensors;Precision engineering","aerospace instrumentation;mirrors;optical design techniques;optical scanners;optical sensors;optimisation;Rayleigh-Ritz methods;remote sensing","scanning mirror;aviation remote sensor;optical supporting structure optimization;structure stiffness;rotate error;Rayleigh-Ritz method;Optistruct software;maximum fundamental frequency;object function;state variable;design variables;sweep mechanism reliability;rotate accuracy;plunge angle;frequency 37.2 Hz","","1","","","","","","","IEEE","IEEE Conferences"
"Notice of Retraction<br>Parameters sensitivity analysis and optimization for steady-state steering characteristic of heavy-duty truck","Dengfeng Wang; Jinglai Sun; He Hao; Shanpo Wang; Yonggang Wu","State Key Laboratory of Automobile Dynamic, Simulation, Jilin University, Changchun 130022, China; State Key Laboratory of Automobile Dynamic, Simulation, Jilin University, Changchun 130022, China; State Key Laboratory of Automobile Dynamic, Simulation, Jilin University, Changchun 130022, China; China National Heavy Duty Truck Group Co. Ltd., Jinan, China; China National Heavy Duty Truck Group Co. Ltd., Jinan, China","2010 2nd International Conference on Computer Engineering and Technology","","2010","5","","V5-318","V5-322","A rigid-flexible coupling simulation model for heavy-duty truck was built using MSC.ADAMS software. Steady static circular performance for heavy-duty truck was analyzed based on National Standard of China GB/T 6323.6-1994. Validity of the model was verified by comparing the simulation result with experiment data for steady-state steering characteristic of the truck. On this basis, the main parameters sensitivity for the steady-state steering characteristic of the truck was calculated. Based on the result of the sensitivity analysis, it is possible to find out which parameters have obvious influence on the steady-state steering characteristic and optimize them with orthogonal optimization design method. The optimal combination was determined by analyzing the results of the optimization. The steady-state steering characteristic of the truck is improved, so the controllability and stability performance of the truck can be better.;Notice of Retraction<br><br>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<br><br>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<br><br>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<br><br>","","978-1-4244-6349-7978-1-4244-6347-3978-1-4244-6348","10.1109/ICCET.2010.5486040","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5486040","heavy-duty truck;controllability and stability;steady-state steering characteristic;sensitivity analysis;orthogonal optimization design","Sensitivity analysis;Steady-state;Vehicles;Design optimization;Stability;Controllability;Software testing;Virtual prototyping;Electronic mail;Automobiles","optimisation;road vehicles;sensitivity analysis;standards;steering systems;vehicle dynamics","parameter sensitivity analysis;steady-state steering characteristics;heavy-duty truck;rigid-flexible coupling simulation model;MSC.ADAMS software;national standard of China GB/T 6323.6- 1994;orthogonal optimization design method;stability;controllability","","","4","","","","","","IEEE","IEEE Conferences"
"Experimental Research on the Shelling Performance of Ginkgo Seeds under Compression Loading","Z. Li-hua; G. Wen; M. Rong-chao","NA; NA; NA","2011 International Conference on Computer Distributed Control and Intelligent Environmental Monitoring","","2011","","","751","757","It is significantly necessary to make a in-depth research on the mechanical performance of Ginkgo under Compression Loading for improving the design and manufacture of ginkgo processing machinery. In this paper, we, firstly, use LDS-W10A microcomputer controlled electronic materials testing machine to do the compression tests on the ginkgo in Southwest of China. Secondly, we carry out the multi-objective duplicate orthogonal test, taking ginkgo broken force, crushing energy consumption and the broken rate of almonds as target parameters, and the parts of putting press, loading rate and moisture content rate as factors respectively. At the same time, we obtain optimum Technology Parameters Constitution of Ginkgo-nut Shelling through the comprehensive evaluation of the Membership-Degree and Least Significant Range (LSR). The results show that when ginkgos are compressed with different moisture content rate along three different directions, there is apparent elastic deformation, but no obvious biological yield point. Tthe direction of ginkgo pressure, pressure rate, moisture content rate is significantly correlated with synthesis rate index (p&lt;;0.05). For the best quality of Ginkgo-shelling, the reasonable technology parameters constitution is as follow: Y direction compression loading, extrusion rate between 10 and 55mm/min, and moisture content rate of 23.72% (w.b.). Besides, the results provide a theoretical basis for improving the processing technology of ginkgo.","","978-1-61284-278-3978-0-7695-4350","10.1109/CDCIEM.2011.564","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5747925","Ginkgo Seed;Compression Test;Membership-Degree Method;Parameter Optimization","Moisture;Force;Energy consumption;Indexes;Loading;Optimization;Software","agricultural products;compressive strength;compressive testing;crushing;elastic deformation;energy consumption;extrusion;food products;microcomputers;moisture;optimisation","ginkgo seeds;compression loading;mechanical performance;ginkgo processing machinery;electronic material testing machine;LDS-W10A microcomputer;compression tests;China;multiobjective duplicate orthogonal test;ginkgo broken force;crushing energy consumption;almonds;putting press;loading rate;moisture content rate;ginkgo nut shelling performance;membership degree;least significant range;elastic deformation;ginkgo pressure rate;synthesis rate index;extrusion rate;least remarkable range method","","","19","","","","","","IEEE","IEEE Conferences"
"Myocardial iron measurement in thalassemia using cardiac magnetic resonance image processing software","E. Boonchieng; K. Duangchaemkarn","Department of Computer Sciences, Faculty of Sciences, Chiang Mai University, 50200, Thailand; School of Pharmaceutical Sciences, University of Phayao, Muang, 56000, Thailand","The 5th 2012 Biomedical Engineering International Conference","","2012","","","1","4","To assess the iron overload in human tissue especially in the heart of transfusion-dependent thalassemia patients using cardiac magnetic resonance imaging (CMRI) technique has been widely used to evaluate cardiac iron deposition because of its non-invasive procedure. The iron measure from this technique is represent as T2* (T2 Star) value. Cardiac T2* software was developed. The software was measuring T2* value from DICOM image with two algorithm processing components; regions of interest (ROI) identification, and T2* optimization. The software preliminary testing was performed using test cases DICOM image, three times repeating, from both healthy volunteer and thalassemia patient with cardiac iron overload, then compared T2* value with the reference tool (CMRtools with ThalassemiaTools plug-in). Mean T2* value analyzed from healthy volunteer CMRI was 22.36 (±0.94) and 37.10 (±1.62), respectively. While the mean T2* value analyzed from thalassemia patients CMRI was 15.52 (±0.85) and 18.67 (±1.15) in reference software and CMUT2Star software respectively. In conclusion, CMUT2Star produced a higher T2* value than the reference tool. Software has no any errors and no conflict with testing platform. The algorithm in T2* optimization component need to be adjusted in order to produce the accuracy and robust T2* value.","","978-1-4673-4892-8978-1-4673-4890-4978-1-4673-4891","10.1109/BMEiCon.2012.6465497","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6465497","Cardiac image processing;Cardiac T2*;Iron overload;Thalassemia","Software;Iron;Magnetic resonance imaging;Silicon;DICOM;Software algorithms","biomedical MRI;cardiology;iron;medical disorders;medical image processing","myocardial iron measurement;cardiac MRI processing software;magnetic resonance imaging;iron overload assessment;transfusion dependent thalassemia patients;CMRI technique;cardiac iron deposition evaluation;cardiac T2* software;T2* measurement;DICOM image;regions of interest identification;T2* optimization;cardiac iron overload","","1","7","","","","","","IEEE","IEEE Conferences"
"Semantic Mutation Analysis of Floating-Point Comparison","H. Dan; R. M. Hierons","NA; NA","2012 IEEE Fifth International Conference on Software Testing, Verification and Validation","","2012","","","290","299","Semantic Mutation Testing (SMT) is a technique that aims to capture errors caused by possible misunderstandings of the semantics of a description language. This paper focuses on the use of SMT to represent possible problems caused by the use of Floating Point Comparison (FPC) since this feature of programming languages can lead to subtle errors. We describe six FPC semantic mutation operators that have been implemented in a C SMT tool. These operators mutate a C program by introducing tolerances using three different algorithms. The paper reports on the results of experiments that explored the proposed mutation operators. It was found that random test suites were not good at killing the resultant mutants, suggesting also that random test suites are poor at revealing FPC problems. We therefore devised a new approach to generate test data to kill these mutants. The manually generated test suites produced using the new approach were more effective in killing FPC mutants. In addition, the random test suites and manual test suites killed different sets of FPC mutants. The impact of using three different algorithms in FPC mutation was also investigated with no dominates relationships being found between the three types of FPC operators. Finally, we ran the same experiments on a computer with a different configuration. We found that slightly different sets of mutants were killed on the two computers, indicating that portability problems can be introduced by FPC.","2159-4848","978-0-7695-4670-4978-1-4577-1906","10.1109/ICST.2012.109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200085","","Semantics;Libraries;Computers;Software;Optimization;Testing;Algorithm design and analysis","C language;program testing","semantic mutation analysis;floating-point comparison;semantic mutation testing;SMT;description language;programming languages;C program","","3","28","","","","","","IEEE","IEEE Conferences"
"QuickVina: Accelerating AutoDock Vina Using Gradient-Based Heuristics for Global Optimization","S. D. Handoko; X. Ouyang; C. T. T. Su; C. K. Kwoh; Y. S. Ong","Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore","IEEE/ACM Transactions on Computational Biology and Bioinformatics","","2012","9","5","1266","1272","Predicting binding between macromolecule and small molecule is a crucial phase in the field of rational drug design. AutoDock Vina, one of the most widely used docking software released in 2009, uses an empirical scoring function to evaluate the binding affinity between the molecules and employs the iterated local search global optimizer for global optimization, achieving a significantly improved speed and better accuracy of the binding mode prediction compared its predecessor, AutoDock 4. In this paper, we propose further improvement in the local search algorithm of Vina by heuristically preventing some intermediate points from undergoing local search. Our improved version of Vina-dubbed QVina-achieved a maximum acceleration of about 25 times with the average speed-up of 8.34 times compared to the original Vina when tested on a set of 231 protein-ligand complexes while maintaining the optimal scores mostly identical. Using our heuristics, larger number of different ligands can be quickly screened against a given receptor within the same time frame.","1545-5963;1557-9964;2374-0043","","10.1109/TCBB.2012.82","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6205733","Artificial intelligence;bioinformatics;global optimization;gradient methods.","Optimization;Databases;Proteins;Algorithm design and analysis;Bioinformatics;Computational biology;Drugs","bioinformatics;drugs;gradient methods;heuristic programming;macromolecules;molecular biophysics;optimisation;proteins","QuickVina;AutoDock Vina;gradient-based heuristics;global optimization;macromolecule;rational drug design;docking software;empirical scoring function;binding affinity;AutoDock 4;local search algorithm;acceleration;protein-ligand complexes;bioinformatics","Algorithms;Binding Sites;Drug Design;Ligands;Proteins;Proteins;Software;Thermodynamics","15","13","","","","","","IEEE","IEEE Journals & Magazines"
"Optimization for one gasoline engine exhaust system based on BOOST","W. Huang; Y. Zhou; Q. Chen; X. Peng","Automotive Institute, Tongji University, Shanghai, 201804 China; Automotive Institute, Tongji University, Shanghai, 201804 China; Automotive Institute, Tongji University, Shanghai, 201804 China; Automotive Institute, Tongji University, Shanghai, 201804 China","IEEE ICCA 2010","","2010","","","1086","1091","One-dimension unstable simulation was performed on a gasoline engine using AVL BOOST software. The results show that the 4-1 connection of the exhaust manifold caused the strongly disturbance between the nearby working cylinders, and the Volumetric efficiency was affected. Meanwhile, as the increasing maturity of the three way catalyst converter, the distance between the catalyst converter and the exhaust valve can be increased. Therefore, the connection of the exhaust system was changed from 4-1 to 4-2-1 and verified by the testing result. The redesigned exhaust system reduced disturbance between the nearby working cylinders, decreased the back pressure, and optimized the exhaust system.","1948-3457;1948-3449;1948-3449","978-1-4244-5195-1978-1-4244-5196","10.1109/ICCA.2010.5524197","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5524197","Gasoline engine;Exhaust manifold;Simulation;AVL Boost","Petroleum;Exhaust systems;Automotive engineering;Manifolds;Engine cylinders;Ice;Friction;Temperature;Equations;Software performance","digital simulation;engine cylinders;exhaust systems;internal combustion engines;mechanical engineering computing;optimisation;valves","gasoline engine exhaust system optimization;one-dimension unstable simulation;AVL BOOST software;volumetric efficiency;catalyst converter;exhaust valve;gasoline engine;exhaust manifold;engine cylinders","","","8","","","","","","IEEE","IEEE Conferences"
"The optimized data network security system based on 4G system for power grid system","X. Guo; H. Xue; S. Chen","Metrology and Testing Center, China Academy of Engineering Physics, Mianyang Sichuan 621900, China; Metrology and Testing Center, China Academy of Engineering Physics, Mianyang Sichuan 621900, China; Metrology and Testing Center, China Academy of Engineering Physics, Mianyang Sichuan 621900, China","2010 Global Mobile Congress","","2010","","","1","4","In the paper, an optimized data network security system based on 4G is designed. Abiding by the information system safety theory, the paper follows the methods of systems analysis and overall balance, and raises the principles systematically that by study and analyze the network venture of Power Supply Bureau in the process of informatization. And, according to the No.30 order of The State Economic and Trade Committee, the Power Supply Bureau data network is designed. The project it referred is based on the real requirement and technology practice when rebuilding Supply Bureau network, and bring out the guidance effect for generalizing the new unified operation system, planning the network flat and disposing the network safe policy.","","978-1-4244-9003-5978-1-4244-9001-1978-1-4244-9002","10.1109/GMC.2010.5634598","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5634598","Power Grid;Network Safety;4G;Data Network;Information Technology","Security;Servers;Safety;Power grids;Software radio;Virtual private networks;Reliability","4G mobile communication;computer network security;power grids","4G system;data network security system;power grid system;power supply bureau data network","","","6","","","","","","IEEE","IEEE Conferences"
"Distributed generation installation using particle swarm optimization","L. Y. Wong; S. R. A. Rahim; M. H. Sulaiman; O. Aliman","School of Electrical System Engineering, Universiti Malaysia Perlis, Malaysia; School of Electrical System Engineering, Universiti Malaysia Perlis, Malaysia; School of Electrical System Engineering, Universiti Malaysia Perlis, Malaysia; Faculty of Electrical and Electronic, Universiti Malaysia Pahang, Malaysia","2010 4th International Power Engineering and Optimization Conference (PEOCO)","","2010","","","159","163","This paper presents a particle swarm optimization approach for the placement of distributed generation (DG) in the distribution system. DG installation in the distribution system is very useful in reducing the line losses, as well as improving the voltage profiles. The proposed method combines particle swarm optimization and the Newton-Raphson load flow method to determine the location and size of the DG. The objective function to be minimized in this problem is the total power losses of the system. The proposed approach has been tested on IEEE 69-bus distribution test system and the program was simulated using MATLAB software. Test results show the effectiveness of the developed algorithm.","","978-1-4244-7128-7978-1-4244-7127-0978-1-4244-7126","10.1109/PEOCO.2010.5559168","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5559168","Distributed Generation;loss minimization;Particle Swarm Optimzation","Particle swarm optimization;Distributed power generation;Optimization;Load flow;Educational institutions;Power engineering","distribution networks;particle swarm optimisation;renewable energy sources","distributed generation installation;particle swarm optimization;distribution system;Newton-Raphson load flow method","","20","21","","","","","","IEEE","IEEE Conferences"
"Control-Flow-Driven Source Level Timing Annotation for Embedded Software Models on Transaction Level","D. Mueller-Gritschneder; K. Lu; U. Schlichtmann","NA; NA; NA","2011 14th Euromicro Conference on Digital System Design","","2011","","","600","607","Instrumented software models feature a combination of software functionality as well as timing information to model execution times on embedded processors. They aim to replace instruction set simulators in virtual prototypes (VP) of embedded systems to improve simulation efficiency. In this work, a novel control flow mapping algorithm is presented to automatically generate timing annotations for instrumented software models. The method is based on the analysis of loop and control dependency properties of basic code blocks in the binary and source code control flow. With these properties, the method can find suitable positions to annotate the timing delay statements of binary code basic blocks into the source code. It shows high accuracy even in the case that the binary code is optimized during compilation. The paper also presents the novel idea of adding timing control statements into the source code to improve timing accuracy. The error in runtime estimation was found to be below 6% for standard test programs. A case study for a VP shows a gain in simulation efficiency of three orders of magnitude compared to an ISS based model.","","978-1-4577-1048","10.1109/DSD.2011.82","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6037466","Embedded software;source code instrumentation;TLM","Binary codes;Delay;Instruments;Program processors;Optimization","binary codes;embedded systems;instruction sets;program compilers;program control structures;program testing;source coding;virtual prototyping","control-flow-driven source level timing annotation;embedded software models;transaction level;instrumented software models;software functionality;timing information;execution times;embedded processors;instruction set simulators;virtual prototypes;VP;embedded systems;simulation efficiency;novel control flow mapping algorithm;timing annotations;loop dependency property;control dependency property;code blocks;binary code control flow;source code control flow;timing delay statements;binary code basic blocks;compilation;adding timing control statements;timing accuracy;runtime estimation;standard test programs;ISS based model","","13","18","","","","","","IEEE","IEEE Conferences"
"The design and development of intelligent university program ranking system using JEE frameworks","M. Al-Kubati; S. A. Aljunid; N. E. Yusoff; M. Daud; M. A. A. M. Tajudin; W. Y. W. Hussin; Z. Ibrahim","Faculty of Computer and Mathematical Sciences (FSKM), UniversitiTeknologi MARA (UiTM), Selangor, Malaysia; FSKM, UiTM, Selangor, Malaysia; FSKM, UiTM, Selangor, Malaysia; FSKM, UiTM, Selangor, Malaysia; Student Intake Devision (BBP), UiTM, Selangor, Malaysia; FSKM, UiTM, Selangor, Malaysia; FSKM, UiTM, Selangor, Malaysia","2012 IEEE Conference on Open Systems","","2012","","","1","6","Web based systems have become the common medium of delivering software applications. Thus, developing web based systems has become a major concern for developers due to the large number of technologies, tools and techniques available today. Developers are often faced with the dilemma of choosing the best technology for their projects especially when developing a high quality system that must comply with certain standards. Enterprise developers often use development frameworks to simplify development job, and to insure that their code follows proven development methodologies. In this paper we describe our approach for developing an intelligent university program ranking system, using a JEE web application framework that has been developed utilizing industry standards and best practices. The development has been done using a well designed JEE web application framework and REpresentational State Transfer (REST) web services to minimize the development time and effort. The system which uses the powerful Adaptive Neuro-Fuzzy Inference System (ANFIS) algorithm for its AI engine is developed using the SEAM framework and RESTEasy web services framework. SEAM has been chosen due to its power and integration capabilities while RESTEasy framework is chosen for its seamless integration with JBoss SEAM. The developed system known as i-Selangkah is divided into two major components; a web based control panel to administer and modify the system as well as the programs fuzzy rules, and a web services interface to provide the existing Selangkah system with the ranking services. i-Selangkah has been successfully implemented, integrated and tested with the existing Selangkah system developed by UiTM. The tests showed that the system can effectively be used together with Selangkah to provide better program advising to these potential applicants with respect to their merit points, the specific and ever-changing program requirements and more significantly, the past intake trends.","","978-1-4673-1046-8978-1-4673-1044-4978-1-4673-1045","10.1109/ICOS.2012.6417618","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6417618","Artificial Intelligence;Web based applications;program ranking;advisory system;web based application frameworks;ANFIS;Java;JEE;Restful Web Services;SEAM;RestEasy","Web services;Educational institutions;Testing;Java;Artificial intelligence;Standards","educational administrative data processing;educational institutions;fuzzy neural nets;fuzzy reasoning;Java;user interfaces;Web services","intelligent university program ranking system;Web based systems;software applications;high quality system;enterprise developers;JEE Web application framework;industry standards;representational state transfer Web services;adaptive neurofuzzy inference system algorithm;ANFIS algorithm;AI engine;SEAM framework;RESTEasy Web service framework;integration capabilities;seamless integration;JBoss SEAM;i-Selangkah;Web based control panel;fuzzy rules;Web service interface;i-Selangkah system;ranking services;UiTM;program requirements","","","25","","","","","","IEEE","IEEE Conferences"
"Application-Specific Energy Optimization of General-Purpose Datapath Interconnect","B. Hidaji; S. Alipour; K. P. Subramaniyan; P. Larsson-Edefors","NA; NA; NA; NA","2011 IEEE Computer Society Annual Symposium on VLSI","","2011","","","301","306","A general-purpose data path is designed for efficient execution of diverse applications. An embedded processor, typically working with a limited application domain, does not necessarily utilize the fixed, general-purpose data path interconnect efficiently. If we consider the interconnect to be a flexible resource, the data path can be fine tuned to an application domain. The addition of an interconnect link between two data path units has the potential to reduce execution time, while the removal of an unused link can save area and power dissipation. Finding the most energy-efficient data path interconnect configuration for a software application domain is a time-consuming process, since it involves rescheduling of the targeted application(s) on different data path implementations. We present an automated optimization engine that is based on a genetic algorithm. This engine aids the designer in finding the most energy-efficient interconnect configuration of a simple processor data path. We show that an optimized data path interconnect can offer an energy saving of 38% with respect to a general-purpose data path reference, if the interconnect links are matched to the need of one application.","2159-3477;2159-3469;2159-3469","978-1-4577-0803-9978-0-7695-4447","10.1109/ISVLSI.2011.71","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5992523","Design Space Exploration;Application-Specific Energy Optimization;Genetic Algorithm;Optimization Engine;Fine-grained;Flexible;General-Purpose Datapath Interconnect","Biological cells;Energy dissipation;Optimization;Engines;Integrated circuit interconnections;Correlation;Benchmark testing","genetic algorithms;integrated circuit interconnections;microprocessor chips","application-specific energy optimization;general-purpose datapath interconnect;datapath units;power dissipation;energy-efficient datapath interconnect configuration;software application domain;automated optimization engine;genetic algorithm;general-purpose datapath reference;optimized datapath interconnect;interconnect links","","1","12","","","","","","IEEE","IEEE Conferences"
"An Optimization Method of Hidden Nodes for Neural Network","P. Gao; C. Chen; S. Qin","NA; NA; NA","2010 Second International Workshop on Education Technology and Computer Science","","2010","2","","53","56","The selection for the number of hidden nodes for a neural network is of critical importance. This paper proposes a novel algorithm to determine the number of hidden nodes of a neural network and optimize it. In the method, the number of hidden nodes H is first computed by empirical formulas, and the range of H is determined according to computed result. Then, the ""three points search"" is applied to search the best number of hidden nodes within the range. Finally, a GTA (Genetic algorithm and Tabu search Algorithm Approach) is developed to train the weights of neural network constructed with the best H. Test results obtained by using Iris data set has shown to be efficient, and better than those by the most commonly used optimization techniques.","","978-1-4244-6389-3978-1-4244-6388","10.1109/ETCS.2010.300","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5460017","Neural networks;architecture optimization;hidden node;three points search","Optimization methods;Neural networks;Artificial neural networks;Signal processing algorithms;Educational technology;Computer science;Computer architecture;Computer networks;Computer science education;Software engineering","genetic algorithms;neural nets;search problems","optimization method;neural network;hidden nodes;three points search;genetic algorithm;tabu search algorithm approach;iris data set","","5","20","","","","","","IEEE","IEEE Conferences"
"GRoundTram: An integrated framework for developing well-behaved bidirectional model transformations","S. Hidaka; Z. Hu; K. Inaba; H. Kato; K. Nakano","National Institute of Informatics, Japan; National Institute of Informatics, Japan; National Institute of Informatics, Japan; National Institute of Informatics, Japan; University of Electro-Communications, Japan","2011 26th IEEE/ACM International Conference on Automated Software Engineering (ASE 2011)","","2011","","","480","483","Bidirectional model transformation is useful for maintaining consistency between two models, and has many potential applications in software development including model synchronization, round-trip engineering, and software evolution. Despite these attractive uses, the lack of a practical tool support for systematic development of well-behaved and efficient bidirectional model transformation prevents it from being widely used. In this paper, we solve this problem by proposing an integrated framework called GRoundTram, which is carefully designed and implemented for compositional development of well-behaved and efficient bidirectional model transformations. GRoundTram is built upon a well-founded bidirectional framework, and is equipped with a user-friendly language for coding bidirectional model transformation, a new tool for validating both models and bidirectional model transformations, an optimization mechanism for improving efficiency, and a powerful debugging environment for testing bidirectional behavior. GRoundTram has been used by people of other groups and their results show its usefulness in practice.","1938-4300","978-1-4577-1639-3978-1-4577-1638","10.1109/ASE.2011.6100104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6100104","","Unified modeling language;Computational modeling;US Department of Transportation;Programming;Software engineering;Testing;Debugging","optimisation;software maintenance","GRoundTram;well behaved bidirectional model transformations;consistency maintainance;software development;model synchronization;round trip engineering;software evolution;user friendly language;bidirectional model transformation coding;optimization mechanism","","12","19","","","","","","IEEE","IEEE Conferences"
"System Monitoring with Metric-Correlation Models","M. Jiang; M. A. Munawar; T. Reidemeister; P. A. S. Ward","Shoshin Distributed Systems Group, Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON, Canada; Shoshin Distributed Systems Group, Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON, Canada; Shoshin Distributed Systems Group, Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON, Canada; Shoshin Distributed Systems Group, Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON, Canada","IEEE Transactions on Network and Service Management","","2011","8","4","348","360","Modern software systems expose management metrics to help track their health. Recently, it was demonstrated that correlations among these metrics allow errors to be detected and their causes localized. Prior research shows that linear models can capture many of these correlations. However, our research shows that several factors may prevent linear models from accurately describing correlations, even if the underlying relationship is linear. Common phenomena we have observed include relationships that evolve, relationships with missing variables, and heterogeneous residual variance of the correlated metrics. Usually these phenomena can be discovered by testing for heteroscedasticity of the underlying linear models. Such behaviour violates the assumptions of simple linear regression, which thus fail to describe system dynamics correctly. In this paper we address the above challenges by employing efficient variants of Ordinary Least Squares regression models. In addition, we automate the process of error detection by introducing the Wilcoxon Rank-Sum test after proper correlations modeling. We validate our models using a realistic Java-Enterprise-Edition application. Using fault-injection experiments we show that our improved models capture system behavior accurately.","1932-4537;2373-7379","","10.1109/TNSM.2011.120811.100033","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6102277","System monitoring;metric-correlation models;fault detection;recursive least squares;multi-variable correlations;heteroscedasticity","Correlation;Measurement;Monitoring;Computational modeling;Software systems;Adaptation models;Predictive models","error detection;least squares approximations;program testing;regression analysis;software fault tolerance;software metrics;system monitoring","system monitoring;metric-correlation model;software system management metrics;linear model;heterogeneous residual variance;heteroscedasticity testing;linear regression;system dynamics;ordinary least squares regression model;error detection;Wilcoxon rank-sum test;correlations modeling;Java-Enterprise-Edition application;fault injection experiment","","1","23","","","","","","IEEE","IEEE Journals & Magazines"
"Accelerated design and optimization of battery management systems using HIL simulation and Rapid Control Prototyping","R. Subramanian; P. Venhovens; B. P. Keane","MBtech North America Inc., 400 East Big Beaver, Troy, MI 48083; Clemson University, International Center for Automotive Research, 4 Research Drive, Greenville, SC 29607 USA; Inotec, Incorporated, P.O. Box 1587 Clemson, SC 29633 USA","2012 IEEE International Electric Vehicle Conference","","2012","","","1","5","This paper describes the use of Hardware-in-Loop (HIL) simulation and Rapid Control Prototyping (RCP) tools for the accelerated design and optimization of battery management systems (BMS) typically found in hybrid/electric vehicles. The BMS is an electronic system that manages a rechargeable battery pack. Its functions include monitoring the cell/pack voltage, current, temperature, state-of-charge, depth-of-discharge, and state-of-health. Besides reporting this data to a supervisory (powertrain) controller, the BMS protects the battery by preventing it from operating outside its safe operating range and balancing the individual cells. Programming, testing and validation of the BMS with real batteries is a time-consuming, expensive and potentially dangerous operation since physical batteries needs to be discharged and re-charged for every development iteration. With the help of virtual batteries models as part of a HIL simulation, the BMS algorithm can be developed, calibrated and validated in a very secure and time-efficient manner resulting in a significant product development time reduction.","","978-1-4673-1561-6978-1-4673-1562","10.1109/IEVC.2012.6183235","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6183235","Battry Management Systems;Hardware in-the-loop Simulation;Rapid Control Propotyping","Batteries;Vehicles;Computer architecture;Testing;Software;Threshold voltage;System-on-a-chip","battery management systems;battery powered vehicles;embedded systems;optimisation;power transmission (mechanical);secondary cells","HIL simulation;hardware-in-loop simulation;rapid control prototyping tools;battery management systems accelerated design;battery management systems optimization;hybrid-electric vehicles;electronic system;rechargeable battery pack;cell voltage;depth-of-discharge;state-of-health;supervisory controller;powertrain;BMS protection;physical batteries;virtual batteries models;BMS algorithm","","8","3","","","","","","IEEE","IEEE Conferences"
"The Confidence of Agile Assessment Methods in the Context of Software Process Improvement","C. Santana; C. Gusmao; D. Caetano; A. Vasconcelos","NA; NA; NA; NA","2011 Agile Conference","","2011","","","135","138","With the advent of CMMI in the late 1990s, software companies tried to use a kind of score to rank themselves in the market. Being a CMMI level five company for a long time represented the epitome of highly qualified company. This phenomenon is also emerging in the agile software development with the emergence of methods such as Nokia or comparative test agility. This paper presents an analysis of how these methods are placed in the context of agile software development and its contribution to the improvement of software process in the same context from a case study developed in real companies.","","978-1-61284-426-8978-0-7695-4370","10.1109/AGILE.2011.38","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6005495","Assessment Methods;Software Process Improvement;Agile Processes","Software;Companies;Programming;Context;Industries;Software engineering","Capability Maturity Model;software prototyping","agile assessment methods;software process improvement;CMMI;agile software development;capability maturity model integration","","2","18","","","","","","IEEE","IEEE Conferences"
"Direct optimization determination of auxiliary test signals for linear problems with model uncertainty","I. Andjelkovic; S. L. Campbell","Department of Mathematics, North Carolina State University, Raleigh, 27695-8205, USA; Department of Mathematics, North Carolina State University, Raleigh, 27695-8205, USA","2011 50th IEEE Conference on Decision and Control and European Control Conference","","2011","","","909","914","Recently there has been increased interest in active approaches for fault detection which use auxiliary test signals. Theory and algorithms have been presented in the literature for the design of fault detection signals for linear systems with model uncertainty. These approaches cannot solve many problems with constraints. This paper gives the first direct optimization formulation of the more general constrained problem. The use of a direct optimization formulation allows the solution of problems not possible by the original algorithms such as problems with input and state constraints. Computational examples are given both to illustrate the theory and to discuss computational issues.","0191-2216;0743-1546;0743-1546","978-1-61284-801-3978-1-61284-800-6978-1-4673-0457-3978-1-61284-799","10.1109/CDC.2011.6160186","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6160186","","Uncertainty;Noise;System-on-a-chip;Optimization;Software;Fault detection;Additives","fault diagnosis;linear systems;optimisation;signal processing;uncertain systems","direct optimization determination;auxiliary test signals;linear problems;model uncertainty;fault detection signal design;linear systems;input constraints;state constraints","","6","19","","","","","","IEEE","IEEE Conferences"
"Hardware description language based on message passing and implicit pipelining","D. Boulytchev; O. Medvedev","St.Petersburg State University, Russia; St.Petersburg State University, Russia","2010 East-West Design & Test Symposium (EWDTS)","","2010","","","438","441","We present a hardware description language (currently called “HaSCoL”) which is based on both reliable and unreliable message passing and implicit pipelining of message handlers. The language consists of a small core and a number of extensions, which cover many features of high level software languages as well as high level hardware description languages (HDLs). These extensions have simple projections into the core language and allow compact and concise description of complex algorithms. The core language in turn can be converted into efficient VHDL. We discuss place-and-route results for some benchmarks implemented both in HaSCoL and VHDL and suggest an optimization which should improve the results significantly and make them close to those for hand-coded VHDL.","","978-1-4244-9556-6978-1-4244-9555-9978-1-4244-9554","10.1109/EWDTS.2010.5742095","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5742095","","Hardware;Software;Registers;Field programmable gate arrays;Generators;Semantics;Message passing","hardware description languages;high level languages;message passing;optimisation;pipeline processing","hardware description language;message passing;implicit pipelining;high level software language;VHDL;HaSCoL;optimization","","2","10","","","","","","IEEE","IEEE Conferences"
"Research of ant colony algorithm and the application of 0–1 knapsack","L. He; Y. Huang","Software College, Xiamen University, Xiamen, 361005, China; Software College, Xiamen University, Xiamen, 361005, China","2011 6th International Conference on Computer Science & Education (ICCSE)","","2011","","","464","467","Among the different works inspired by ant colonies, the ant colony algorithm (ACA) is probably the most successful and popular one. ACA is a novel bio-inspired optimization algorithm, which simulates the foraging behavior of ants for solving various complex combinatorial optimization problems. In this paper, a well-structured definition of basic ACA, detailed implementation process and complexity analyses of basic ACA are presented. It is also devoted to the explanation of improvement strategies of ACA in discrete space optimization and in continuous space optimization. In order to handle the 0/1 knapsack problem, it revises the model of ant algorithm and use the computer to test the modified algorithm. Finally, outlines some ongoing and most promising research trends in ACA.","","978-1-4244-9718-8978-1-4244-9717-1978-1-4244-9716","10.1109/ICCSE.2011.6028680","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6028680","ant colony algorithms;knapsack problem;problem of TSP;pheromone","Optimization;Cities and towns;Partitioning algorithms;Computers;Mathematical model;Educational institutions;Traveling salesman problems","combinatorial mathematics;knapsack problems;optimisation","ant colony algorithm;ACA;bio-inspired optimization algorithm;combinatorial optimisation;discrete space optimization;continuous space optimization;0/1 knapsack problem","","1","20","","","","","","IEEE","IEEE Conferences"
"Research and application on RFID tags and antenna simulation deployment system","C. Han; D. Wang","Modern Logistics Information Technology and RFID labs, School of Software, Shanghai Jiao Tong University, Shanghai, China; Modern Logistics Information Technology and RFID labs, School of Software, Shanghai Jiao Tong University, Shanghai, China","2010 2nd IEEE International Conference on Information Management and Engineering","","2010","","","373","378","As RFID technology continues to evolve and the complexity of enterprise-class applications increased, the industry has become aware that, testing, analysis, and verify feasibility and reliability of speech is a must before putting RFID system into actual use, based on which we propose optimization strategies to meet the enterprise application standard or baseline, and reduce the maintenance cost of the post-deployment system. In this article, in exploring enterprise-level RFID application scenarios, exploring and designing different RFID applications based on the testing program, we focus on the computer simulation system of RFID test, take advantage of this system to test and to analysis of enterprise-class RFID scenarios, and provide adequate technical preparation and decision support.","","978-1-4244-5263-7978-1-4244-5265","10.1109/ICIME.2010.5477974","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477974","RFID;tag deployment;antenna deployment;samples testing;computer simulation","RFID tags;Radiofrequency identification;System testing;Application software;Computer simulation;Computational modeling;Logistics;Software testing;Radio frequency;Analytical models","antennas;radiofrequency identification","RFID tags;antenna simulation deployment system;radiofrequency identification","","","8","","","","","","IEEE","IEEE Conferences"
"CIASYS--Change Impact Analysis at System Level","G. Toth; C. Nagy; J. Jasz; A. Beszedes; L. J. Fulop","NA; NA; NA; NA; NA","2010 14th European Conference on Software Maintenance and Reengineering","","2010","","","198","201","The research field of change impact analysis plays an important role in software engineering theory and practice nowadays. Not only because it has many scientific challenges, but it has many industrial applications too (e.g., cost estimation, test optimization), and the current techniques are still not ready to fulfill the requirements of industry. Typically, the current solutions lack a whole-system view and give either precise results with high computation costs or less precise results with fast algorithms. For these reasons, they are not applicable to large industrial systems where both scalability and precision are very important. In this paper, we present a project whose main goal is to develop an innovative change impact analysis software-suit based on recent scientific results and modern technologies. The suite will use hybrid analysis techniques to benefit from all the advantages of static and dynamic analyses. In addition, it will be able to determine the dependencies at system level of software systems with heterogeneous architecture. The software is being developed by FrontEndART Ltd. while the theoretical and technological background is provided by the Department of Software Engineering at the University of Szeged. The project is funded by the Economic Development Operational Programme, New Hungary Development Plan.","1534-5351","978-0-7695-4321-5978-1-61284-369","10.1109/CSMR.2010.35","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5714436","change impact analysis;static analysis;dynamic analysis;hybrid analysis;system-level dependencies","Companies;Java;Software engineering;Testing;Software maintenance;Databases","innovation management;object-oriented programming;software architecture;software maintenance","CIASYS;change impact analysis at system level;software engineering;industrial system;innovative development;heterogeneous architecture;FrontEndART Ltd","","1","16","","","","","","IEEE","IEEE Conferences"
"Improving performance of ACO algorithms using crossover mechanism based on mean of pheromone tables","O. Gokalp; A. Ugur","Department of Software Engineering, Yasar University, Izmir, Turkey; Department of Computer Engineering, Ege University, Izmir, Turkey","2012 International Symposium on Innovations in Intelligent Systems and Applications","","2012","","","1","4","Ant Colony Optimization (ACO) Algorithms have been used to solve many optimization problems in various fields and several algorithms have been proposed based on ACO metaheuristic in the literature. This paper proposes a simple crossover mechanism based on mean of pheromone tables for ACO algorithms. Main purpose of the crossover operation is to produce solutions or individuals having greater performance than their parents by selecting useful parts. Original ACO Algorithms don't have crossover. Method that we developed employs more than one ant colonies and also solutions. Suitable low-cost average based operations are then applied to pheromone tables obtained after several iterations as crossover operator. Algorithm is tested on Traveling Salesman Problem using some benchmark problems from TSPLIB and results are presented. Our experiments and comparisons show that crossover mechanism improves the performance of ACO Algorithms.","","978-1-4673-1448-0978-1-4673-1446-6978-1-4673-1447","10.1109/INISTA.2012.6247022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6247022","ACO;Crossover;TSP;pheromone table;evolutionary algorithms","Optimization;Ant colony optimization;Traveling salesman problems;Evolutionary computation;Cities and towns;Search problems;Software algorithms","ant colony optimisation;travelling salesman problems","pheromone tables;crossover mechanism;ACO algorithm;ant colony optimization algorithm;ACO metaheuristic;crossover operation;traveling salesman problem;TSPLIB","","","14","","","","","","IEEE","IEEE Conferences"
"Mathematical efficiency calibration with uncertain source geometries using smart optimization","N. Menaa; A. Bosko; F. Bronson; R. Venkataraman; W. R. Russ; W. Mueller; V. Nizhnik; L. Mirolo","AREVA/CANBERRA Nuclear Measurements Business Unit, Saint Quentin-en-Yvelines 78182, France; AREVA/CANBERRA Nuclear Measurements Business Unit, Meriden, Connecticut, USA; AREVA/CANBERRA Nuclear Measurements Business Unit, Meriden, Connecticut, USA; AREVA/CANBERRA Nuclear Measurements Business Unit, Meriden, Connecticut, USA; AREVA/CANBERRA Nuclear Measurements Business Unit, Meriden, Connecticut, USA; AREVA/CANBERRA Nuclear Measurements Business Unit, Meriden, Connecticut, USA; International Atomic Energy Agency, Vienna, Austria; AREVA/CANBERRA Nuclear Measurements Business Unit, Saint Quentin-en-Yvelines 78182, France","2011 2nd International Conference on Advancements in Nuclear Instrumentation, Measurement Methods and their Applications","","2011","","","1","7","The In Situ Object Counting Software (ISOCS), a mathematical method developed by CANBERRA, is a well established technique for computing High Purity Germanium (HPGe) detector efficiencies for a wide variety of source shapes and sizes. In the ISOCS method, the user needs to input the geometry related parameters such as: the source dimensions, matrix composition and density, along with the source-to-detector distance. In many applications, the source dimensions, the matrix material and density may not be well known. Under such circumstances, the efficiencies may not be very accurate since the modeled source geometry may not be very representative of the measured geometry. CANBERRA developed an efficiency optimization software known as “Advanced ISOCS” that varies the not well known parameters within user specified intervals and determines the optimal efficiency shape and magnitude based on available benchmarks in the measured spectra. The benchmarks could be results from isotopic codes such as MGAU, MGA, IGA, or FRAM, activities from multi-line nuclides, and multiple counts of the same item taken in different geometries (from the side, bottom, top etc). The efficiency optimization is carried out using either a random search based on standard probability distributions, or using numerical techniques that carry out a more directed (referred to as “smart” in this paper) search. Measurements were carried out using representative source geometries and radionuclide distributions. The radionuclide activities were determined using the optimum efficiency and compared against the true activities. The “Advanced ISOCS” method has many applications among which are: Safeguards, Decommissioning and Decontamination, Non-Destructive Assay systems and Nuclear reactor outages maintenance.","","978-1-4577-0927-2978-1-4577-0925-8978-1-4577-0926","10.1109/ANIMMA.2011.6172913","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6172913","","Geometry;Benchmark testing;Detectors;Calibration;Software;Optimization methods","calibration;germanium radiation detectors;high energy physics instrumentation computing;optimisation;probability;radioactivity measurement;random processes","mathematical efficiency calibration;uncertain source geometries;smart optimization method;in situ object counting software;mathematical method;CANBERRA;high purity germanium detector efficiencies;geometry related parameters;matrix composition;matrix density;source-to- detector distance;matrix material;source geometry model;optimal efficiency shape;optimal efficiency magnitude;MGAU code;MGA code;IGA code;FRAM code;optimization efficiency;standard probability distributions;numerical techniques;radionuclide distributions;radionuclide activities;advanced ISOCS method;nondestructive assay systems;nuclear reactor outage maintenance;nuclear reactor decommissioning;nuclear reactor decontamination;safeguard analysis;random search method","","1","11","","","","","","IEEE","IEEE Conferences"
"Explicitly typed static single-assignment form","Baojian Hua; Bo Xu; Ying Gao","School of Software Engineering, &amp; Suzhou Institute for Advanced Study, University of Science and Technology of China, Hefei, China; School of Software Engineering, University of Science and Technology of China, Hefei,China; Intel China Research Center, Beijing,China","2010 2nd International Conference on Education Technology and Computer","","2010","1","","V1-43","V1-47","Static single-assignment form (SSA) is an important optimizing compiler intermediate representation, and is widely used in many real-world production compilers, such as GCC, LLVM, Jikes, and MLton, etc.. This paper presents the initial results of our project to apply the techniques of proof-carrying code and certifying compilation to practical highly-optimizing compilers. To be specific, this paper presents the design of an explicitly typed static single-assignment form called TSSA. This paper presents a formalization of TSSA by defining its abstract syntax, the operational semantics, and the static semantics. This paper leads to a general semantics foundation for SSA-based optimizing compilers.","2155-1812","978-1-4244-6370-1978-1-4244-6367-1978-1-4244-6369","10.1109/ICETC.2010.5529303","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5529303","static single-assignment form;type system","Optimizing compilers;Educational technology;Software engineering;Production;Formal verification;Safety;Testing;Computer science education;Programming;Mesons","optimising compilers;program diagnostics;programming language semantics;type theory","optimizing compiler intermediate representation;real-world production compilers;GCC;LLVM;Jikes;MLton;proof-carrying code;certifying compilation;highly-optimizing compilers;explicitly typed static single-assignment form;formalization;abstract syntax;operational semantics;static semantics;general semantics foundation;SSA-based optimizing compilers","","","9","","","","","","IEEE","IEEE Conferences"
"Optimizing application mapping algorithms for NoCs through a unified framework","C. Radu; L. Vinţan","Advanced Computer Architecture & Processing Systems Research Lab, “Lucian Blaga” University of Sibiu, Romania; Advanced Computer Architecture & Processing Systems Research Lab, “Lucian Blaga” University of Sibiu, Romania","9th RoEduNet IEEE International Conference","","2010","","","259","264","This paper presents a preliminary PhD research towards developing a framework to evaluate and optimize application mapping algorithms for Network-on-Chip architectures. Several such algorithms have been proposed for mapping the threads of a parallel application on a NoC architecture. However, the performance of those algorithms is evaluated only on some specific NoC designs. A unified approach for evaluating such algorithms allows a better comparison of their performance and can potentially lead to some optimizations. The proposed framework is intended to be flexible so that the algorithms can be tested on different NoC designs. To this end, a scalable and flexible Network-on-Chip simulator is proposed. Some preliminary results obtained with this simulator are presented, too. They show the flexibility of this simulator and that it is feasible for addressing the application mapping problem in a unified manner.","2247-5443;2068-1038;2068-1038","978-9-7373-9951-9978-1-4244-7335","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5541562","Network-on-Chip (NoC);parallel application NoC mapping;evaluation;optimization;software framework;simulation","Network-on-a-chip;Algorithm design and analysis;Application software;Routing;Tiles;Computer architecture;Testing;Network topology;Design optimization","network-on-chip;optimisation","application mapping algorithms;NoC;network-on-chip architectures;parallel application","","1","19","","","","","","IEEE","IEEE Conferences"
"Pareto efficient design for reconfigurable streaming applications on CPU/FPGAs","J. Zhu; I. Sander; A. Jantsch","Royal Institute of Technology, Stockholm, Sweden; Royal Institute of Technology, Stockholm, Sweden; Royal Institute of Technology, Stockholm, Sweden","2010 Design, Automation & Test in Europe Conference & Exhibition (DATE 2010)","","2010","","","1035","1040","We present a Pareto efficient design method for multi-dimensional optimization of run-time reconfigurable streaming applications on CPU/FPGA platforms, which automatically allocates applications with optimized buffer requirement and software/hardware implementation cost. At the same time, application performance is guaranteed with sustainable throughput during run-time reconfigurations. As the main contribution, we formulate the constraint based application allocation, scheduling, and reconfiguration analysis, and propose a design Pareto-point calculation flow. A public domain solver - Gecode is used in solutions finding. The capability of our method has been exemplified by two cases studies on applications from media and communication domains.","1558-1101;1530-1591;1530-1591","978-3-9810801-6-2978-1-4244-7054-9978-3-9810801-6","10.1109/DATE.2010.5456962","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5456962","","Field programmable gate arrays;Application software;Design optimization;Pareto optimization;Runtime;Design methodology;Hardware;Cost function;Throughput;Pareto analysis","field programmable gate arrays;media streaming;Pareto optimisation;reconfigurable architectures;resource allocation;scheduling","Pareto efficient design;multidimensional optimization;runtime reconfigurable streaming application;CPU-FPGA platform;software-hardware implementation cost;optimized buffer requirement;constraint based application allocation;scheduling;reconfiguration analysis;design Pareto-point calculation flow;public domain solver;Gecode","","","18","","","","","","IEEE","IEEE Conferences"
"Programming design of Digital-Down Converter based on software radio","Bai Fengming; Lv Xiaoli; Yan Fei","College of Electronic information Engineering, Changchun University of Science and Technology, No. 7089 Weixing Road, Jilin Province, China; College of Electronic information Engineering, Changchun University of Science and Technology, No. 7089 Weixing Road, Jilin Province, China; College of Electronic information Engineering, Changchun University of Science and Technology, No. 7089 Weixing Road, Jilin Province, China","2010 Second Pacific-Asia Conference on Circuits, Communications and System","","2010","1","","351","354","In this paper, three projects of the Digital-Down Converter software radio receivers are discussed. On this basis, efficient and optimized concrete implementation methods about Digital-Down Converter are put forward. It replaces the specialized Digital-Down Converter hardware integrated circuits, it strengthened flexibility and adaptability in IF system. To reduce calculation, in this paper, author adopts the project of integrating abstracting with filtrating. That multiphase-decompose the filter and memorize different phases. Tests show that this project is feasible and available.","","978-1-4244-7970-2978-1-4244-7969-6978-1-4244-7967","10.1109/PACCS.2010.5626883","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5626883","software radio;Digital-Down Converter module;C++ Builder;multiphase filtrating construction","Radio frequency","analogue-digital conversion;radio receivers;software radio","programming design;digital-down converter;software radio receiver","","2","9","","","","","","IEEE","IEEE Conferences"
"Optimizing Data-Flow Graphs with min/max, adding and relational operations","J. Pérez; P. Sánchez; V. Fernández","GIM (www.teisa.unican.es/gim), TEISA, University of Cantabria, Santander, Spain; GIM (www.teisa.unican.es/gim), TEISA, University of Cantabria, Santander, Spain; GIM (www.teisa.unican.es/gim), TEISA, University of Cantabria, Santander, Spain","2010 Design, Automation & Test in Europe Conference & Exhibition (DATE 2010)","","2010","","","1361","1364","During Electronic System-Level (ESL) design, High-Level Synthesis (HLS) tools normally translate the system description to a Control/Data Flow Graph. At this level, several transformations are performed as early as possible to reduce the number and complexity of the data operations. These preliminary transformations (for example, common sub-expression elimination, constant propagation, etc) are typically applied in algebraic expressions with arithmetic operators. This paper presents preliminary transformations that optimize Data-Flow Graphs with relational, maximum/minimum and arithmetic (addition/subtraction) operations. The proposed techniques produce a significant reduction in the number of operations. HLS tools and even software compilers and symbolic algebra packages are not able to generate similar results. The efficiency of the techniques has been evaluated with several modules of real telecommunications standards and their HW implementations show important area reductions and, sometimes, low impact on latency or critical path.","1558-1101;1530-1591;1530-1591","978-3-9810801-6-2978-1-4244-7054-9978-3-9810801-6","10.1109/DATE.2010.5457022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5457022","","High level synthesis;Flow graphs;Arithmetic;Signal processing algorithms;Equations;Control system synthesis;Costs;Digital signal processing;Optimizing compilers;Hardware","algebra;arithmetic;data flow graphs;high level synthesis;minimax techniques;program compilers;symbol manipulation","data-flow graphs;relational operations;electronic system-level design;high-level synthesis;subexpression elimination;constant propagation;arithmetic operators;algebraic expressions;symbolic algebra packages;software compilers","","","19","","","","","","IEEE","IEEE Conferences"
"Estimating Error-probability and its Application for Optimizing Roll-back Recovery with Checkpointing","D. Nikolov; U. Ingelsson; V. Singh; E. Larsson","NA; NA; NA; NA","2010 Fifth IEEE International Symposium on Electronic Design, Test & Applications","","2010","","","281","285","The probability for errors to occur in electronic systems is not known in advance, but depends on many factors including influence from the environment where the system operates. In this paper, it is demonstrated that inaccurate estimates of the error probability lead to loss of performance in a well known fault tolerance technique, Roll-back Recovery with checkpointing (RRC). To regain the lost performance, a method for estimating the error probability along with an adjustment technique are proposed. Using a simulator tool that has been developed to enable experimentation, the proposed method is evaluated and the results show that the proposed method provides useful estimates of the error probability leading to near-optimal performance of the RRC fault-tolerant technique.","","978-1-4244-6026-7978-1-4244-6025-0978-0-7695-3978","10.1109/DELTA.2010.25","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5438676","roll-back recovery with checkpointing;error-probability estimation;optimization","Checkpointing;Error probability;Fault tolerance;Performance loss;Fault detection;Error correction codes;Application software;Communication system control;Electronic equipment testing;Computer errors","checkpointing;error statistics;fault tolerance;integrated circuit testing;system-on-chip","roll-back recovery with checkpointing;error probability estimation;fault tolerance;system-on-chip","","5","15","","","","","","IEEE","IEEE Conferences"
"Optimization of threshold voltage nonvolatile flash memory using Taguchi method","A. Abdul Aziz; N. N. Nor Hissam","Faculty of Electrical Engineering, Universiti Teknologi MARA, 40450 Shah Alam, Selangor, Malaysia; Faculty of Electrical Engineering, Universiti Teknologi MARA, 40450 Shah Alam, Selangor, Malaysia","2012 International Conference on Computer, Information and Telecommunication Systems (CITS)","","2012","","","1","5","This paper presents the optimization of threshold voltage Flash memory by varying four control factors i.e. thickness of tunnel oxide, bottom-oxide, nitride, and top-oxide. The Flash memory structure was simulated by using Athena and Atlas module of Silvaco software. By implementation of Taguchi method, these control factors are varied each with four levels and applied in L16 orthogonal array. Signal-to-noise (S/N) ratio and Pareto ANOVA analysis are then used to determine the optimal thickness for each control factors and the main factors that have an effect upon the threshold voltage. Finally a confirmation test was conducted to verify the optimal result is as predicted. Based on S/N ratio analysis, the optimal parameter combination obtained was A4B2C3D1. From the analysis of Pareto ANOVA, bottom oxide thickness was found as the most important parameter that affecting the threshold voltage of Flash memory.","","978-1-4673-1550-0978-1-4673-1549-4978-1-4673-1548","10.1109/CITS.2012.6220368","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6220368","threshold voltage;taguchi method;Flash memory","Analysis of variance;Threshold voltage;Nonvolatile memory;Flash memory;Arrays;Optimization;Software","flash memories;random-access storage;statistical analysis;Taguchi methods","threshold voltage;nonvolatile flash memory;Taguchi method;tunnel oxide thickness;bottom oxide;top oxide;Athena and Atlas module;Silvaco software;L16 orthogonal array;signal-to-noise ratio;Pareto ANOVA analysis;confirmation test","","2","15","","","","","","IEEE","IEEE Conferences"
"Pinpointing the Subsystems Responsible for the Performance Deviations in a Load Test","H. Malik; B. Adams; A. E. Hassan","NA; NA; NA","2010 IEEE 21st International Symposium on Software Reliability Engineering","","2010","","","201","210","Large scale systems (LSS) contain multiple subsystems that interact across multiple nodes in sometimes unforeseen and complicated ways. As a result, pinpointing the subsystems that are the source of performance degradation for a load test in LSS can be frustrating, and might take several hours or even days. This is due to the large volume of performance counter data collected such as CPU utilization, Disk I/O, memory consumption and network traffic, the limited operational knowledge of analysts about all subsystems of an LSS and the unavailability of up-to-date documentation in a LSS. We have developed a methodology that automatically ranks the subsystems according to the deviation of their performance in a load test. Our methodology uses performance counter data of a load test to craft performance signatures for the LSS subsystems. Pair-wise correlations among the performance signatures of subsystems within a load test are compared with the corresponding correlations in a baseline test to pinpoint the subsystems responsible for the performance violations. Case studies on load test data obtained from a large telecom system and that of an open source benchmark application show that our approach provides an accuracy of 79% and do not require any instrumentation or domain knowledge to operate.","1071-9458;1071-9458;2332-6549","978-1-4244-9056-1978-0-7695-4255","10.1109/ISSRE.2010.43","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635038","Pinpointing;Performance Counters;Load Testing","Radiation detectors;Correlation;Testing;Servers;Generators;Principal component analysis;Databases","large-scale systems;performance evaluation;program testing","performance deviation;large scale system;load test;operational knowledge;user performance counter data;craft performance signature;pair wise correlation;open source benchmark;domain knowledge","","10","38","","","","","","IEEE","IEEE Conferences"
"Interval-based algorithms to extract fuzzy measures for Software Quality Assessment","X. Wang; A. F. G. Contreras; M. Ceberio; C. Del Hoyo; L. C. Gutierrez; S. Virane","Computer Science Department, The University of Texas at El Paso, El Paso, Texas 79968-0518; Computer Science Department, The University of Texas at El Paso, El Paso, Texas 79968-0518; Computer Science Department, The University of Texas at El Paso, El Paso, Texas 79968-0518; Computer Science Department, The University of Texas at El Paso, El Paso, Texas 79968-0518; Computer Science Department, The University of Texas at El Paso, El Paso, Texas 79968-0518; Computer Science Department, The University of Texas at El Paso, El Paso, Texas 79968-0518","2012 Annual Meeting of the North American Fuzzy Information Processing Society (NAFIPS)","","2012","","","1","6","In this paper, we consider the problem of automatically assessing sofware quality. We show that we can look at this problem, called Software Quality Assessment (SQA), as a multicriteria decision-making problem. Indeed, just like software is assessed along different criteria, Multi-Criteria Decision Making (MCDM) is about decisions that are based on several criteria that are usually conflicting and non-homogenously satisfied. Nonadditive (fuzzy) measures along with the Choquet integral can be used to model and aggregate the levels of satisfaction of these criteria by considering their relationships. However, in practice, fuzzy measures are difficult to identify. An automated process is necessary and possible when sample data is available. Several optimization approaches have been proposed to extract fuzzy measures from sample data; e.g., genetic algorithms, gradient descent algorithms, and the Bees algorithm, all local search techniques. In this article, we propose a hybrid approach, combining the Bees algorithm and an interval constraint solver, resulting in a focused search expected to be less prone to falling into local results. Our approach, when tested on SQA decision data, shows promise and compares well to previous approaches to SQA that were using machine learning techniques.","","978-1-4673-2338-3978-1-4673-2336-9978-1-4673-2337","10.1109/NAFIPS.2012.6291044","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6291044","","Object oriented modeling;Software quality;Optimization;Measurement;Predictive models;Data mining","decision making;fuzzy set theory;learning (artificial intelligence);optimisation;software quality","interval-based algorithms;fuzzy measures;software quality assessment;SQA;multicriteria decision making problem;MCDM;Choquet integral;optimization;machine learning","","2","26","","","","","","IEEE","IEEE Conferences"
"OFTEN Testing OpenFlow Networks","M. Kuzniar; M. Canini; D. Kostic","NA; NA; NA","2012 European Workshop on Software Defined Networking","","2012","","","54","60","Software-defined networking and OpenFlow in particular enable independent development of network devices and software that controls them. Such separation of concerns eases the introduction of new network functionality, however, it leads to distributed responsibility for bugs. Despite the common interface, separate development entails the need to test an integrated network before deployment. In this work-in-progress paper, we identify the challenges of creating an environment that simplifies and systematically conducts such tests. We discuss optimizations required for efficient and reliable OpenFlow switch black-box testing and present a possible approach to address other challenges. In our preliminary prototype, we combine systematic state-space exploration techniques with real switches execution to explore an integrated network behavior. Our initial results show that such methods help detect previously unrevealed inconsistencies in the network.","2379-0350;2379-0369","978-1-4673-4554-5978-0-7695-4870","10.1109/EWSDN.2012.21","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6385048","","Switches;Testing;Software;Reliability;Computer bugs;Process control","computer networks;data communication equipment;data flow analysis;program debugging;telecommunication computing;telecommunication switching","OFTEN testing;OpenFlow network;software defined networking;network device development;network functionality;OpenFlow switch black box testing;integrated network behavior","","15","16","","","","","","IEEE","IEEE Conferences"
"A power consumption optimization method for a hybrid renewable energy making system","D. I. Gota; C. Vigu; O. Capatana","IPA R&amp;D Institute for Automation, Cluj Subsidary; IPA R&amp;D Institute for Automation, Cluj Subsidary; IPA R&amp;D Institute for Automation, Cluj Subsidary","2010 IEEE International Conference on Automation, Quality and Testing, Robotics (AQTR)","","2010","3","","1","4","Renewable energy production is a field of great interest and is a modern and necessary topic of research. Scientists are developing hardware and software components to maximize the control and supervision rate of their systems placed in remote locations. This paper presents a power consumption optimization method for a system designed and developed in order to control and supervise a hybrid wind and solar renewable energy production system. This method reduces the total energy consumption and provides the system with some scenario action plans.","","978-1-4244-6725-9978-1-4244-6724-2978-1-4244-6723","10.1109/AQTR.2010.5520773","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5520773","","Energy consumption;Optimization methods;Renewable energy resources;Control systems;Battery charge measurement;Power measurement;Velocity measurement;Communication system control;Wind energy generation;Solar power generation","power consumption;solar power;wind power","power consumption optimization method;hybrid renewable energy making system;hybrid wind energy production system;solar renewable energy production system;total energy consumption","","","6","","","","","","IEEE","IEEE Conferences"
"Guiding Testing Activities by Predicting Defect-Prone Parts Using Product and Inspection Metrics","F. Elberzhager; S. Kremer; J. Münch; D. Assmann","NA; NA; NA; NA","2012 38th Euromicro Conference on Software Engineering and Advanced Applications","","2012","","","406","413","Product metrics, such as size or complexity, are often used to identify defect-prone parts or to focus quality assurance activities. In contrast, quality information that is available early, such as information provided by inspections, is usually not used. Currently, only little experience is documented in the literature on whether data from early defect detection activities can support the identification of defect prone parts later in the development process. This article compares selected product and inspection metrics commonly used to predict defect-prone parts. Based on initial experience from two case studies performed in different environments, the suitability of different metrics for predicting defect-prone parts is illustrated. These studies revealed that inspection defect data seems to be a suitable predictor, and a combination of certain inspection and product metrics led to the best prioritizations in our contexts.","1089-6503;2376-9505","978-0-7695-4790-9978-1-4673-2451","10.1109/SEAA.2012.30","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6328182","inspection metrics;product metrics;comparison;case study;focusing","Measurement;Inspection;Testing;Complexity theory;Context;Focusing;Quality assurance","inspection;program testing;quality assurance;software metrics;software quality","defect-prone parts prediction;product metrics;inspection metrics;defect-prone parts identification;quality assurance;quality information;early defect detection activities;inspection defect data","","4","8","","","","","","IEEE","IEEE Conferences"
"High-Accuracy Bias Control Software for Doherty RF Power Amplifiers","J. Zhang; T. Liu; Y. Ye; G. Xu; L. Li","NA; NA; NA; NA; NA","2011 7th International Conference on Wireless Communications, Networking and Mobile Computing","","2011","","","1","4","How to accurately control the gate bias voltage of a Doherty RF power amplifier is an important problem that has to be resolved for developing high efficiency RF power amplifiers. This paper proposes a digital bias solution to obtain adjustable accurate bias voltage, and discusses the implementation procedure of high-accuracy bias voltage control software for Doherty RF power amplifiers in detail. The software can not only precisely adjust the bias voltage of a PA to determine the optimal bias point of the PA, but also can be used to build an automatic optimization and debugging system for the power amplifier so as to reduce the fabrication costs of debugging and testing power amplifiers.","2161-9654;2161-9646;2161-9646","978-1-4244-6252-0978-1-4244-6250-6978-1-4244-6251","10.1109/wicom.2011.6040611","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6040611","","Software;Voltage measurement;Power amplifiers;Radio frequency;Universal Serial Bus;Graphical user interfaces;Voltage control","control engineering computing;electronic engineering computing;power amplifiers;radiofrequency amplifiers;voltage control","high-accuracy bias control software;Doherty RF power amplifiers;gate bias voltage control;digital bias solution;PA optimal bias point;automatic optimization;debugging system;power amplifier testing;fabrication cost reduction","","1","8","","","","","","IEEE","IEEE Conferences"
"The CST design method of matching both European and American crash regulations","Jianghua Fu; Zhengbao Lei; Muxi Lei; Xianhua Luo","School of Automotive and Mechanical Engineering, Changsha University of Science and Technology, China; Key Laboratory for Highway Engineering of Ministry of Education, Changsha University of Science and Technology, China; School of Civil Engineering and Architecture, Changsha University of Science and Technology, China; Engineering Research Center of the Road Disaster Prevent &amp; Mitigation and Transportation Safety, Ministry of Education, Changsha, China","2011 Second International Conference on Mechanic Automation and Control Engineering","","2011","","","778","781","To satisfy Collision safety requirements formuled by both FMVSS208 and ECE R94 for vehicles fixing CST, take metro as the research object, which mass is 0.8 tons, using universal principles of particle kinematics and dynamics, and obtain the CST-ideal energy absorption curve which can satisfy both FMVSS208 and ECE R94. To make sure that the designed CST can obtain ideal energy absorption curve, having optimized the practical structure and optimal parameters by orthogonal optimization, based on VPG and LS-DYNA software platform.","","978-1-4244-9439-2978-1-4244-9436-1978-1-4244-9438","10.1109/MACE.2011.5987043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5987043","Crash regulations;CST;Energy absorption curve;Orthogonal optimization","Acceleration;Absorption;Vehicles;Optimization;Vehicle dynamics;Vehicle crash testing;Safety","design engineering;impact testing;road safety;threading (machining);vehicle dynamics","CST design method;American crash regulations;European crash regulations;collision safety requirements;FMVSS208;ECE R94;particle kinematics;particle dynamics;CST-ideal energy absorption curve;LS-DYNA software platform;VPG software platform","","","6","","","","","","IEEE","IEEE Conferences"
"Impact on Reliability in the Control-Flow of Programs under Compiler Optimizations","R. Parizi; R. Ferreira; &#x00C1;. Freitas; L. Carro","NA; NA; NA; NA","2012 Brazilian Symposium on Computing System Engineering","","2012","","","43","48","This paper evaluates the impact on reliability in the control-flow of programs that compiler optimizations incur in terms of fault coverage for the Automatic Correction of Control-flow Errors technique. This technique was implemented in the LLVM framework, enabling the automated analysis of programs. In order to evaluate the efficiency of the technique of fault tolerance we performed a series of fault injection experiments using the MiBench benchmark suite as case study, measuring how individual and combined optimizations impact reliability.","2324-7894;2324-7886","978-0-7695-4929-3978-1-4673-5747","10.1109/SBESC.2012.17","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6473630","","Benchmark testing;Hardware;Reliability;Optimization;Program processors;Systems engineering and theory","error correction;optimising compilers;program control structures;program diagnostics;program testing;software fault tolerance","reliability;program control flow;compiler optimization;fault coverage;Automatic Correction of Control-flow Errors technique;LLVM framework;automated program analysis;fault tolerance;fault injection experiment;MiBench benchmark suite","","","10","","","","","","IEEE","IEEE Conferences"
"NCHC's Formosa V GPU cluster enters the TOP500 ranking","C. Hsieh; C. Y. Chou; T. Tsai; Y. Cheng; S. Kuo","National Center for High-Performance Computing, Hsin-chu, Taiwan; National Center for High-Performance Computing, Hsin-chu, Taiwan; National Center for High-Performance Computing, Hsin-chu, Taiwan; National Center for High-Performance Computing, Hsin-chu, Taiwan; National Center for High-Performance Computing, Hsin-chu, Taiwan","4th IEEE International Conference on Cloud Computing Technology and Science Proceedings","","2012","","","622","624","Top 500 list is a competition for supercomputer rank in the world twice a year. How to employ the high performance LINPACK (HPL) to obtain the maximal LINPACK performance (Rmax) plays a key role in this competition. In this paper we propose these strategies for NCHC (National Center for High-Performance Computing) Formosa V GPU cluster that it is a GPU-based heterogeneous cluster to stand 232<sup>nd</sup> place in the 39<sup>th</sup> 500 list which is announced on June 18<sup>th</sup>, 2012. We took three months to test this system and improve the hardware. We also cast a weighed function into HPL to make data decomposition more efficiency. Rmax is improved from 65 TFLOPS to near 90 TFLOPS!","","978-1-4673-4510-1978-1-4673-4511-8978-1-4673-4509","10.1109/CloudCom.2012.6427507","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6427507","GPU clustert;HPL;TOP 500 list","Graphics processing units;Random access memory;Supercomputers;Hardware;Computer architecture;Conferences;Cloud computing","graphics processing units;mainframes;parallel processing;software libraries","NCHC Formosa V GPU cluster;TOP500 ranking;top 500 list;supercomputer rank;high performance LINPACK;HPL;Rmax;National Center for High-Performance Computing Formosa V GPU cluster;GPU-based heterogeneous cluster;data decomposition","","","6","","","","","","IEEE","IEEE Conferences"
"Factorising the Multiple Fault Localization Problem: Adapting Single-Fault Localizer to Multi-fault Programs","C. Gong; Z. Zheng; Y. Zhang; Z. Zhang; Y. Xue","NA; NA; NA; NA; NA","2012 19th Asia-Pacific Software Engineering Conference","","2012","1","","729","732","Software failures are not rare and fault localizations always an important but laborious activity. Since there is no guarantee that no more than one fault exists in a faulty program, the approach to locate all the faults is necessary. Spectrum-based fault localization techniques collect dynamic program spectra as well as test results of program runs, and estimate the extent of program elements being related to fault(s). A popular solution into generate a ranked list of suspicious candidates, which are checked in order, stopping whenever a fault is found. Such single fault localizers locate one fault in one checking round, terminate, and wait to be triggered by the regression testing to validate the fixing of the located fault. In this paper, we study the manifestation of multiple faults in a program and propose an effective mechanism to indicate their presence. When a fault is reached during the checking round, we use it to interpret the failures observed, and update the indicator to judge whether there remain other faults in the program. Our indicator serves as a stopping criterion of checking the ranked list of suspicious candidates. Our work factories the multiple fault localization problem into developing single-fault localizers and adapting them to multi-fault programs. It both improves the fault localization efficiencies of single-fault localizers, and avoids the ineffective efforts of thoroughly abandoning the many single-fault localizers to develop multi-fault localizers.","1530-1362;1530-1362;1530-1362","978-1-4673-4930-7978-0-7695-4922","10.1109/APSEC.2012.22","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6462736","fault localization;program spectra;multi-fault","Testing;Software;Debugging;Accuracy;Educational institutions;Computer science;Cognition","software fault tolerance;software quality;software reliability","single-fault localization program;multifault localization program;spectrum-based fault localization technique;dynamic program spectra;program run;regression testing;stopping criterion","","1","17","","","","","","IEEE","IEEE Conferences"
"Comparison between two FPGA implementations of the Particle Swarm Optimization algorithm for high-performance embedded applications","D. M. Muñoz; C. H. Llanos; L. d. S. Coelho; M. Ayala-Rincón","Department of Mechanical Engineering, Automation and Control Group/GRACO, University of Brasilia, D.F, Brazil; Department of Mechanical Engineering, Automation and Control Group/GRACO, University of Brasilia, D.F, Brazil; Industrial and Systems Engineering, Graduate Program, LAS/PPGPES, Pontifical Catholic University of Parana, Curitiba, Brazil; Department of Computer Science and Mathematics, University of Brasilia, D.F, Brazil","2010 IEEE Fifth International Conference on Bio-Inspired Computing: Theories and Applications (BIC-TA)","","2010","","","1637","1645","Particle Swarm Optimization (PSO) algorithms have been proposed to solve engineering problems that require to find an optimal point of operation. There are several embedded applications which requires to solve online optimization problems with a high performance. However, the PSO suffers on large execution times, and this fact becomes evident when using Reduced Instruction Set Computer (RISC) microprocessors in which the operational frequencies are low in comparison with the high operational frequencies of traditional personal computers (PCs). This paper compares two hardware implementations of the parallel PSO algorithm using an efficient floating-point arithmetic which perform computations with large dynamic range and high precision. The full-parallel and the partially-parallel PSO architectures allow the parallel capabilities of the PSO to be exploited in order to decrease the running time. Three well-known benchmark test functions have been used to validate the hardware architectures and a comparison in terms of cost in logic area, quality of the solution and performance is reported. In addition, a comparison of the execution time between the hardware and two C-code software implementations, based on a Intel Core Duo at 1.6GHz and a embedded Microblaze microprocessor at 50MHz, are presented.","","978-1-4244-6440-1978-1-4244-6437-1978-1-4244-6439","10.1109/BICTA.2010.5645256","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5645256","global optimization;swarm intelligence;FPGAs;floating-point arithmetic","Mathematics;Hardware","field programmable gate arrays;mathematics computing;parallel algorithms;particle swarm optimisation;reduced instruction set computing","FPGA;parallel particle swarm optimization algorithm;high-performance embedded application;engineering problem;online optimization problem;reduced instruction set computer microprocessors;floating-point arithmetic;full-parallel particle swarm optimization architecture;partially-parallel particle swarm optimization architecture;benchmark test function;C-code software implementation;Intel Core Duo;embedded Microblaze microprocessor","","5","21","","","","","","IEEE","IEEE Conferences"
"Implementation of JVM tool interface on Dalvik virtual machine","Chien-Wei Chang; Chun-Yu Lin; Chung-Ta King; Yi-Fan Chung; Shau-Yin Tseng","Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; SoC Technology Center, Industrial Technology Research Institute, Hsinchu, Taiwan","Proceedings of 2010 International Symposium on VLSI Design, Automation and Test","","2010","","","143","146","Mobile devices such as cell phones, GPS guiding systems, and mp3 players, now become one of the most important consumer electronic products. Being an embedded system, mobile devices are highly integrated in software and hardware for robustness, high performance, and low cost. The problem is that this also makes it very difficult to understand the internal interactions of hardware as well as software modules in such devices and to identify performance bottlenecks and design faults. Profiling helps developers to understand the behaviors of a system, especially during the development of new platforms. Android is a new software platform intended for mobile devices. It is composed of Linux and a Java virtual machine called Dalvik. The ability to profile Android helps developers to familiarize with Android's features and optimize their applications. In this paper, we discuss the development of a profiling tool interface, JVM TI, on Android. With this tool interface, developers can profile their Java code running on Dalvik using JVM TI.","","978-1-4244-5269-9978-1-4244-5271","10.1109/VDAT.2010.5496711","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5496711","JVM TI;JAVA;Dalvik virtual machine;Google Android;Embeded systen;Profiling tool","Virtual machining;Software performance;Hardware;Java;Cellular phones;Global Positioning System;Consumer electronics;Embedded system;Embedded software;Robustness","embedded systems;Java;Linux;mobile handsets;software tools;virtual machines","JVM tool interface;Dalvik virtual machine;mobile devices;embedded system;Android;software platform;Linux;Java virtual machine;JVM TI","","3","13","","","","","","IEEE","IEEE Conferences"
"Isolating Suspiciousness from Spectrum-Based Fault Localization Techniques","X. Xie; T. Y. Chen; B. Xu","NA; NA; NA","2010 10th International Conference on Quality Software","","2010","","","385","392","Spectrum-based fault localization (SBFL) is one of the most promising fault localization approaches, which normally uses the failed and passed program spectrum to evaluate the risks for all program entities. However, it does not explicitly distinguish the different degree in definiteness between the information associated with the failed spectrum and the passed spectrum, which may result in an unreliable location of faults. Thus in this paper, we propose a refinement method to improve the accuracy of the predication by SBFL, through eliminating the indefinite information. Our method categorizes all statements into two groups according to their different suspiciousness, and then uses different evaluation schemes for these two groups. In this way, we can reduce the use of the unreliable information in the ranking list, and finally provide a more precise result. Experimental study shows that for some SBFL techniques, our method can significantly improve their performance in some situations, and in other cases, it can still remain the techniques' original performance.","2332-662X;1550-6002;1550-6002","978-1-4244-8078-4978-0-7695-4131","10.1109/QSIC.2010.45","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562991","program spectrum;fault localization;debugging;risk evaluation","Testing;Measurement;Indexes;Debugging;Software;Proposals;Software reliability","program debugging;program testing;software fault tolerance;software maintenance","spectrum based fault localization;program spectrum;risk evaluation","","9","28","","","","","","IEEE","IEEE Conferences"
"A new operational low cost GNSS Software receiver for Microsatellites","T. Grelier; L. Ries; P. Bataille; C. Perrot; G. Richard","Centre National d'Etudes Spatiales (CNES), Toulouse, France; Centre National d'Etudes Spatiales (CNES), Toulouse, France; Syrlinks, Bruz, France; Syrlinks, Bruz, France; Syrlinks, Bruz, France","2012 6th ESA Workshop on Satellite Navigation Technologies (Navitec 2012) & European Workshop on GNSS Signals and Signal Processing","","2012","","","1","5","This paper presents a new low cost GNSS Software (SW) receiver for Microsatellites. Syrlinks (Bruz, FRANCE) has been chosen by CNES for developing and manufacturing this equipment which should be tested on board the scientific microsatellite named MICROSCOPE that will be launched in 2015. The GNSS receiver under development is a highperformance equipment specially designed and optimized for the needs and constraints of small platforms for which small volume, low mass and low power consumption are important parameters. This equipment is based on COTS (Commercials Off The Shelf) in order to exploit the performance of the advanced technology developed for terrestrial applications and to reduce significantly the global cost of the equipment. The structure of this GNSS receiver is organized around a reconfigurable architecture with the use of one FPGA (Field Programmable Gate Array) associated with one DSP (Digital Signal Processor). The GNSS function is then split in two main parts according to the real time requirements of the processing and navigation operations. The hardware (HW) architecture has been optimized to be able to support a low power mode through a fractioned activity that has been implemented by introducing standby time frames during the mission. An orbital extrapolator is also implemented for propagating position and velocity without pseudo range measurements. This receiver is able to process GPS (L1) and GALILEO (E1) signals simultaneously in the first version of the software and will be able to evolve in a second step to a dual frequency mode, compatible with E5a/E1, or with E5b/E1. This GNSS receiver dedicated to low earth orbit (LEO) satellites will withstand the radiation environment therefore the hardware and software architecture has been defined to reduce the single event effects (SEE) and to maximize the service availability.","2325-5455;2325-5439;2325-5439","978-1-4673-2011-5978-1-4673-2010-8978-1-4673-2009","10.1109/NAVITEC.2012.6423051","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6423051","GPS;GALILEO;GNSS;Software receiver;orbit;satellites;space;CNES;Syrlinks","Receivers;Global Navigation Satellite Systems;Global Positioning System;Software;Radio frequency;Power demand;Hardware","aerospace engineering;artificial satellites;field programmable gate arrays;satellite navigation","GNSS software receiver;microsatellites;Syrlinks;Bruz;CNES;MICROSCOPE;COTS;commercials off the shelf;terrestrial applications;FPGA;field programmable gate array;DSP;digital signal processor;hardware architecture;orbital extrapolator;GPS;GALILEO;Low Earth Orbit satellites;LEO satellites;single event effects","","1","2","","","","","","IEEE","IEEE Conferences"
"Interval Type-2 Fuzzy Logic for Software Cost Estimation Using TSFC with Mean and Standard Deviation","C. V. M. K. Hari; P. R. P.V.G.D.; M. Jagadeesh; G. S. Ganesh","NA; NA; NA; NA","2010 International Conference on Advances in Recent Technologies in Communication and Computing","","2010","","","40","44","One of the biggest challenges in Software Engineering is accurately forecasting how much time and effort it will take either to develop a system. So far no model has proved to be successful at effectively and consistently predicting software development cost due to the lot of uncertainty factor of input size. In this paper we proposed an Interval Type 2 Fuzzy logic for software cost estimation. The inputs are fuzzified by using Takagi-Sugeno fuzzy controller of Universe Discourse with mean and standard deviation of size values affects the control performance. The software size can be regarded as a fuzzy set yielding the cost estimate also inform of a fuzzy set. The uncertainty is an inherit part in cost estimation. We reduce the uncertainty produced by the Type-1 functions by using Type-2 Fuzzy logic. We considered means FOU`s as a firing strength. The model deals effectively with imprecise and uncertain input and enhances the reliability of software cost estimation. The estimated effort is optimized using the developed model and tested on NASA software projects on the basis of three criterions for assessment of software cost estimation models. Comparison of the all models is done and it is found that the developed model provide better estimation.","","978-1-4244-8093-7978-0-7695-4201","10.1109/ARTCom.2010.40","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5656870","Software Cost Estimation;Interval Type-2;Fuzzy logic;Kloc;Takagi-Sugeno Fuzzy controller (TSFC)","Estimation;Software;Uncertainty;Fuzzy sets;Fuzzy logic;Biological system modeling;Computational modeling","fuzzy control;fuzzy logic;fuzzy set theory;software cost estimation","interval type-2 fuzzy logic;software cost estimation;TSFC;standard deviation;mean deviation;software engineering;Takagi-Sugeno fuzzy controller;universe discourse;fuzzy set;NASA software projects","","1","21","","","","","","IEEE","IEEE Conferences"
"Overview of Artificial Bee Colony (ABC) algorithm and its applications","F. S. Abu-Mouti; M. E. El-Hawary","Electrical and Computer Engineering Department, Dalhousie University, Halifax, NS, Canada; Electrical and Computer Engineering Department, Dalhousie University, Halifax, NS, Canada","2012 IEEE International Systems Conference SysCon 2012","","2012","","","1","6","Real-world optimization problems are very difficult and have high degrees of uncertainty. Conventional optimization algorithms have some limitations (i.e., local solution attainment and/or divergence) in solving such problems. On the other hand, meta-heuristic algorithms prove to be competent in outperforming deterministic algorithms, especially when the complexity of the problem increases. Practitioners have utilized those unconventional algorithms for the past few decades. This paper presents an overview of the literature employing the Artificial Bee Colony (ABC) algorithm in their solution approach. The ABC algorithm is a recently introduced population-based meta-heuristic optimization technique inspired by the intelligent foraging behavior of honeybee swarms. Key features of the ABC algorithm, as well as its performance characteristics, are also discussed.","","978-1-4673-0750-5978-1-4673-0748-2978-1-4673-0749","10.1109/SysCon.2012.6189539","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6189539","Meta-Heuristic Optimization Techniques;Artificial Bee Colony (ABC) Algorithm;Unconventional Optimization Methods","Algorithm design and analysis;Optimization;Signal processing algorithms;Benchmark testing;Clustering algorithms;Heuristic algorithms;Software algorithms","artificial intelligence;computational complexity;deterministic algorithms;optimisation","artificial bee colony algorithm;real-world optimization problem;optimization algorithm;metaheuristic algorithm;deterministic algorithm;problem complexity;unconventional algorithm;ABC algorithm;population-based metaheuristic optimization technique;intelligent foraging behavior;honeybee swarm","","14","49","","","","","","IEEE","IEEE Conferences"
"Heuristic search strategy of evolutionary programming","Zhi-Ming Han; Xian-Ping Liu; Miao Tang","Institute of Information and Spreading Engineering, Changchun University of Technology, 130012, China; School of Mechatronic Engineering, Changchun University of Technology, 130012, China; College of Software, Changchun University of Technology, 130012, China","2010 2nd International Asia Conference on Informatics in Control, Automation and Robotics (CAR 2010)","","2010","1","","241","243","Evolutionary programming (EP) has been successfully applied to many optimization problems in recent years. However, most experimental results of EP have been obtained under low-dimension condition and weren't perfect under high-dimension condition. A new evolutionary programming method which named Heuristic Search Strategy (HSS) was proposed for solving high-dimension optimization problem. It can grasp the information of distribution-status of population by control four parameters of population in the evolution process and adjust the mutation size of individual according to such information. HSS was test by using benchmark functions, the experimental results show that performance of HSS is better than other EP method obviously under high-dimension condition.","1948-3422;1948-3414;1948-3414","978-1-4244-5192-0978-1-4244-5194","10.1109/CAR.2010.5456858","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5456858","evolutionary programming;Heuristic Search Strategy;optimization","Genetic programming;Genetic mutations;Robotics and automation;Testing;Asia;Informatics;Automatic control;Robot control;Automatic programming;Robot programming","evolutionary computation;optimisation;search problems","evolutionary programming method;heuristic search strategy;high-dimension optimization problem;distribution-status information;evolution process","","","7","","","","","","IEEE","IEEE Conferences"
"User-driven automatic test-case generation for DTV/STB reliable functional verification","T. Tekcan; V. Zlokolica; V. Pekovic; N. Teslic; M. Gunduzalp","Vestel, Manisa, Turkey; University of Novi Sad, Serbia; RT-RK, Novi Sad, Serbia; University of Novi Sad, Serbia; Department of Electrical and Electronic Engineering, Ya¿ar University, ¿zmir, Turkey","IEEE Transactions on Consumer Electronics","","2012","58","2","587","595","In recent years Digital TV receivers (DTV) and Set- Top Boxes (STB) have been advancing in terms of new functionalities they offer to the end-users. This has mainly been triggered by the appearance of new multimedia formats and applications needed to be adequately handled by the DTV/STB. In such complex system within the environment consisting of wide range of non-interactive as well as interactive applications and services for television, internet, gamming, healthcare, surveillance, etc., efficient and reliable DTV functional verification becomes highly necessary and challenging task. Since efficient DTV testing is considered as one of the most important issues for the reduced time-to-market, its optimization is of crucial importance to the DTV manufacturing process. Having in mind the end-users as the main target of the DTV, optimal test generation process must also include estimated usage-profiles in addition to the pre-defined DTV designed specification. This is especially important considering new emerging multimedia technologies for DTV which are not yet fully standardized and evaluated, and which exact employment (from the point of end-users) is not known in advance. In this paper we propose a novel approach for user driven test-case generation that would enable adequate and efficient testing of the DTV/STB in different development stages before reaching the market, i.e., the end-users. The proposed user driven approach provides optimal test-case generation in terms of both test execution time and adequacy according to the real end-user DTV functional DTV operation and quality of experience (QoE). This is obtained by incorporating a novel design and selection scheme for test-cases generation based on the modeled end-users most required functional operations on the DTV/STB under test and its expectations of it.","0098-3063;1558-4127","","10.1109/TCE.2012.6227464","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227464","DTV;STB;functional verification;testing methodology;usage modeling;usage profile","Digital TV;Testing;Software;Multimedia communication;Reliability engineering","digital television;multimedia communication;telecommunication network reliability;television receivers","user-driven automatic test-case generation;DTV-STB reliable functional verification;digital TV receivers;set-top boxes;multimedia formats;complex system;television;internet;gamming;healthcare;surveillance;DTV functional verification;time-to-market;DTV manufacturing process;optimal test generation process;predefined DTV;usage-profiles;multimedia technologies;driven test-case generation;optimal test-case generation;real end-user DTV functional DTV operation;quality of experience;QoE;test-cases;modeled end-users","","5","38","","","","","","IEEE","IEEE Journals & Magazines"
"Reactive power load tracing via blended crossover continuous ant colony optimization","Z. Hamid; I. Musirin; M. M. Othman; M. N. A. Rahim","Faculty of Electrical Engineering, Universiti Teknologi MARA, Shah Alam, Selangor, Malaysia; Faculty of Electrical Engineering, Universiti Teknologi MARA, Shah Alam, Selangor, Malaysia; Faculty of Electrical Engineering, Universiti Teknologi MARA, Shah Alam, Selangor, Malaysia; Faculty of Electrical Engineering, Universiti Teknologi MARA, Shah Alam, Selangor, Malaysia","2011 Malaysian Conference in Software Engineering","","2011","","","393","398","In deregulated power system, the only way to provide fair and non-discriminatory transmission service pricing is by applying electricity tracing methods. The weakness of conventional methods like transaction based allocation has resulted to inaccurate transmission cost allocation as the physical power system constraints are not taken into account; whereas the proportional sharing principle (PSP) based power tracing techniques are very dependent on matrix inversion when obtaining the results. Hence, this paper presents an effective and new formulation technique for load tracing via a new hybrid algorithm; Blended Crossover Continuous Ant Colony Optimization (BX-CACO). This method is new, flexible and easy to be implemented as no complex mathematical derivations are needed and it is definitely free from PSP and matrix inversion. Validation on IEEE 14-bus test system has proven that BX-CACO reflects a great capability in being a sophisticated tool for fair losses and reactive power allocation with fast computation time.","","978-1-4577-1531-0978-1-4577-1530-3978-1-4577-1529","10.1109/MySEC.2011.6140704","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140704","BX-CACO;load tracing;matrix inversion;PSP","Reactive power;Resource management;Generators;Power transmission lines;Ant colony optimization;Optimization","electricity supply industry deregulation;IEEE standards;load management;matrix inversion;optimisation;power system economics;pricing;reactive power","reactive power load tracing;blended crossover continuous ant colony optimization;deregulated power system;nondiscriminatory transmission service pricing;electricity tracing methods;proportional sharing principle;matrix inversion;IEEE 14-bus test system;reactive power allocation;fair losses","","1","14","","","","","","IEEE","IEEE Conferences"
"A GNU-based packet radio for network management field testing","C. Serban; R. Chadha; C. J. Chiang; F. Ge; Y. Gottlieb; A. Sapello; K. Sinkar; K. Moeltner","Applied Communication Sciences, Piscataway, NJ 08854, USA; Applied Communication Sciences, Piscataway, NJ 08854, USA; Applied Communication Sciences, Piscataway, NJ 08854, USA; Applied Communication Sciences, Piscataway, NJ 08854, USA; Applied Communication Sciences, Piscataway, NJ 08854, USA; Applied Communication Sciences, Piscataway, NJ 08854, USA; Applied Communication Sciences, Piscataway, NJ 08854, USA; U.S. Army Communications Electronics Research, Development, and Engineering Center (CERDEC), Aberdeen Proving Ground, MD, 21005, USA","MILCOM 2012 - 2012 IEEE Military Communications Conference","","2012","","","1","6","Software Defined Radios advanced in the recent years to become the platform of choice for new military and emergency response applications, enabling such functionality as single hardware-multiple waveforms, agile spectrum reuse, and cross layer reconfiguration and optimization. Yet, the choice of platforms available for experimenting new adaptation strategies and re-configuration capabilities is severely limited due to proprietary and canned implementations and architectures. GNU Radio represents an open source radio platform operating on a general purpose processor (GPP,) capable of offering an unprecedented level of re-configurability using common and accessible programming languages and paradigms. As such, it represents an attractive and cost effective platform for experimenting with reconfigurable radio technologies. Yet, GNU Radio had been used so far mainly for experimenting with physical layer algorithms, making it less suitable for data packet radio and networking applications. This paper describes our effort to augment the GNU radio to transform it into a practical platform for communication in tactical ad-hoc networks, while maintaining its dynamic re-configurability capabilities. The results of this effort enabled us to create a deployable network capable of supporting the development and testing of TITAN NM, an integrated network management system with autonomous re-configuration capabilities, and its successful evaluation in C4ISR OTM 2012.","2155-7586;2155-7578;2155-7578","978-1-4673-1731-3978-1-4673-1729-0978-1-4673-1730","10.1109/MILCOM.2012.6415702","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6415702","GNU Radio;USRP;C4ISR;military environment;Network Management;Software Defined Radios","Decision support systems","ad hoc networks;military communication;optimisation;software radio;telecommunication network management","network management field testing;GNU-based packet radio;software defined radios;military applications;emergency response applications;single hardware-multiple waveforms;agile spectrum reuse;cross layer reconfiguration;optimization;open source radio platform;general purpose processor;common programming languages;accessible programming languages;reconfigurable radio technology;physical layer algorithms;data packet radio;tactical ad-hoc networks;TITAN NM;integrated network management system;autonomous re-configuration capability;C4ISR OTM 2012","","","10","","","","","","IEEE","IEEE Conferences"
"RDCurve: A Nonparametric Method to Evaluate the Stability of Ranking Procedures","X. Lu; A. Gamst; R. Xu","University of California, San Diego, CA; University of California, San Diego, CA; University of California, San Diego, CA","IEEE/ACM Transactions on Computational Biology and Bioinformatics","","2010","7","4","719","726","Great concerns have been raised about the reproducibility of gene signatures based on high-throughput techniques such as microarray. Studies analyzing similar samples often report poorly overlapping results, and the p-value usually lacks biological context. We propose a nonparametric ReDiscovery Curve (RDCurve) method, to estimate the frequency of rediscovery of gene signature identified. Given a ranking procedure and a data set with replicated measurements, the RDCurve bootstraps the data set and repeatedly applies the ranking procedure, selects a subset of k important genes, and estimates the probability of rediscovery of the selected subset of genes. We also propose a permutation scheme to estimate the confidence band under the Null hypothesis for the significance of the RDCurve. The method is nonparametric and model-independent. With the RDCurve, we can assess the signal-to-noise ratio of the data, compare the performance of ranking procedures in term of their expected rediscovery rates, and choose the number of genes to be reported.","1545-5963","","10.1109/TCBB.2008.138","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4731238","Biology and genetics, bootstrap, nonparametric statistics, gene ranking, reproducibility.","Stability;Testing;RNA;Statistical analysis;Statistical distributions;Reproducibility of results;Frequency estimation;Biomedical measurements;Statistics;Genetics","bioinformatics;genetics;probability;statistical analysis","RDCurve;nonparametric method;ranking procedure stability;gene signature;nonparametric ReDiscovery Curve;data set;permutation scheme;signal-to-noise ratio","Algorithms;Gene Expression Profiling;Oligonucleotide Array Sequence Analysis;Software","5","31","","","","","","IEEE","IEEE Journals & Magazines"
"Derating based hardware optimizations in soft error tolerant designs","V. Prasanth; V. Singh; R. Parekhji","Computer Design and Test Lab, Indian Institute of Science, Bangalore, India; Computer Design and Test Lab, Indian Institute of Science, Bangalore, India; Texas Instruments (India) Pvt. Ltd., Bangalore, India","2012 IEEE 30th VLSI Test Symposium (VTS)","","2012","","","282","287","Ensuring reliable operation over an extended period of time is one of the biggest challenges facing present day electronic systems. The increased vulnerability of the components to atmospheric particle strikes poses a big threat in attaining the reliability required for various mission critical applications. Various soft error mitigation methodologies exist to address this reliability challenge. A general solution to this problem is to arrive at a soft error mitigation methodology with an acceptable implementation overhead and error tolerance level. This implementation overhead can then be reduced by taking advantage of various derating effects like logical derating, electrical derating and timing window derating, and/or making use of application redundancy, e.g. redundancy in firmware/software executing on the so designed robust hardware. In this paper, we analyze the impact of various derating factors and show how they can be profitably employed to reduce the hardware overhead to implement a given level of soft error robustness. This analysis is performed on a set of benchmark circuits using the delayed capture methodology. Experimental results show up to 23% reduction in the hardware overhead when considering individual and combined derating factors.","2375-1053;1093-0167;1093-0167","978-1-4673-1074-1978-1-4673-1073-4978-1-4673-1072","10.1109/VTS.2012.6231067","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6231067","Soft error mitigation;hardware derating;delayed capture methodology","Abstracts;Microprogramming;Hardware;Clocks;Logic gates","circuit reliability;flip-flops;logic design;radiation hardening (electronics)","derating based hardware optimizations;soft error tolerant designs;electronic systems;atmospheric particle strikes;reliability;soft error mitigation methodology;error tolerance level;logical derating;electrical derating;timing window derating;firmware-software redundancy;hardware overhead;benchmark circuits;delayed capture methodology;flip-flop","","1","27","","","","","","IEEE","IEEE Conferences"
"Using Constraints to Verify Properties of Rule Programs","B. Berstel; M. Leconte","NA; NA","2010 Third International Conference on Software Testing, Verification, and Validation Workshops","","2010","","","349","354","Rule-based programming has been gaining interest in the industry for several years, through the growing use of Business Rules Management Systems. A demand for verification of semantic properties on rule programs has thus emerged. In this paper we present an approach to rule program verification, using constraints to model program executions and verification properties, and a Constraint-Based Programming Solver (CP Solver) to compute the answers to verification questions. We also study the use of constraint-based programming in rule program verification, and the consequences of this usage on the CP Solver compared to combinatorial optimization problems.","","978-1-4244-6774-7978-1-4244-6773-0978-0-7695-4050","10.1109/ICSTW.2010.42","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463668","rule-based programming;program analysis;program verification;constraint-based programming;constraint satisfiability","Engines;Companies;Computer languages;Software testing;Computer industry;Conference management;Engineering management;Constraint optimization;Production systems;Costs","combinatorial mathematics;constraint handling;formal verification;logic programming;optimisation","rule-based programming;business rules management systems;semantic property verification;rule program verification;constraint-based programming solver;program execution property;verification property;combinatorial optimization problems","","3","20","","","","","","IEEE","IEEE Conferences"
"Optimization design of C-post wire rope safety barrier","C. Chen; M. Lei; Z. Lei; X. Zhang; J. Guo; Y. Li","Engineering Research Center of the Road Disaster Prevent &amp; Mitigation and Transportation Safety, Ministry of Education, Changsha 410004, China; School of Civil Engineering and Architecture, Changsha University of Science and Technology, 410004, China; Key Laboratory of Highway Engineering (Changsha University of Science and Technology), Ministry of Education, 410004, China; Engineering Research Center of the Road Disaster Prevent &amp; Mitigation and Transportation Safety, Ministry of Education, Changsha 410004, China; Engineering Research Center of the Road Disaster Prevent &amp; Mitigation and Transportation Safety, Ministry of Education, Changsha 410004, China; Hunan Province Communications Planning, Survey &amp; Design Institute, Changsha 410008, China","2011 Second International Conference on Mechanic Automation and Control Engineering","","2011","","","2342","2345","The paper presents a new type of wire rope safety barrier, which has many posts with C-shaped cross section and 5 ropes, by way of designing the shape of the post, the diameter of the rope, the arrangement and the number of ropes etc., on the basis of the form of structure of the most advanced foreign existing wire rope safety barrier - BRIFEN. And in order to realize that the barrier is designed as A class protective flexible safety fence, a series of finite element models are established, taking the thickness of the steel plate of C-post. After simulation experiment, discussion and analysis, the relation of the maximum dynamic deformation and the thickness of C-post is gotten. The research shows that the flexural strength of C-post increases and the corresponding maximum dynamic deformation decreases with the thickness of C post increases, which providing an important reference for subsequent research and Vehicle Crash Test.","","978-1-4244-9439-2978-1-4244-9436-1978-1-4244-9438","10.1109/MACE.2011.5987450","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5987450","Traffic Engineering;Automobile Engineering;Wire Rope Safety Barrier;Finite Element Method;Post structure;Flexural strength","Wires;Safety;Vehicle dynamics;Road transportation;Finite element methods;Vehicles;Software","cables (mechanical);deformation;design engineering;finite element analysis;optimisation;plates (structures);road safety;ropes;steel","optimization design;C-post wire rope safety barrier;BRIFEN;protective flexible safety fence;finite element models;dynamic deformation;vehicle crash test","","1","10","","","","","","IEEE","IEEE Conferences"
"Smoothing Algorithm for Tetrahedral Meshes by Error-Based Quality Metric","S. Sun; H. Bao; M. Liu","NA; NA; NA","2010 International Conference on Computational Intelligence and Software Engineering","","2010","","","1","4","Smoothing or geometrical optimization is one of basic procedures to improve the quality of mesh. This paper first introduces an error-based mesh quality metric based on the concept of optimal Delaunay triangulations, and then examines the smoothing scheme which minimizes the interpolation error among all triangulations with the same number of vertices. Facing to its deficiency, a modified smoothing scheme and corresponding optimization model for tetrahedral mesh that avoid illegal elements are proposed. The optimization model is solved by integrating chaos search and BFGS (Broyden-Fletcher-Goldfarb-Shanno) algorithm efficiently. Quality improvement for tetrahedral mesh is realized through alternately applying the smoothing approach suggested and topological optimization technique. Testing results show that the proposed approach is effective to improve mesh quality and suitable for combining with topological technique.","","978-1-4244-5391-7978-1-4244-5392","10.1109/CISE.2010.5676713","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676713","","Smoothing methods;Optimization;Finite element methods;Algorithm design and analysis;Measurement;Approximation algorithms;Chaos","interpolation;mesh generation;optimisation","smoothing optimization;geometrical optimization;error-based mesh quality metric;Delaunay triangulation;smoothing scheme;chaos search;Broyden-Fletcher-Goldfarb-Shanno algorithm;topological optimization technique","","","18","","","","","","IEEE","IEEE Conferences"
"Runtime Optimization of Broadcast Communications Using Dynamic Network Topology Information from MPI","J. Godwin; C. Karlsson; Z. Chen","NA; NA; NA","2012 IEEE 14th International Conference on High Performance Computing and Communication & 2012 IEEE 9th International Conference on Embedded Software and Systems","","2012","","","287","294","Modern commodity compute clusters are often composed of many multi-core nodes, that are connected via a network to each other. On multi-core clusters, inter-node network communications are typically an order of magnitude slower than those between processes on the same node, which effectively creates a heterogeneous, tiered network topology. Presently, most MPI implementations assume a homogeneous network composition, which causes them to have less than optimal performance on multi-core clusters. In this paper, we treat a multi-core cluster as a heterogeneous cluster and optimize the performance of MPI broadcast communications by scheduling messages according to topology information. We experimentally demonstrate that previous heuristics for heterogeneous clusters such as Fastest Edge First (FEF) do not produce optimal results on multi-core clusters for broadcast communications. Our solution is to modify the Fastest Edge First heuristic by imposing an additional constraint, that permits only one core per node to participate in inter-node communications, creating a nested binomial tree structure. Using this constraint we are able to achieve performance gains of 20%-60% over the MPI broadcast implementation on homogeneous, multi-core clusters.","","978-1-4673-2164-8978-0-7695-4749","10.1109/HPCC.2012.46","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6332186","Message Passing Interface (MPI);Multicore;Cluster;Broadcast;Fastest Edges First","Network topology;Multicore processing;Clustering algorithms;Topology;Schedules;Benchmark testing;Timing","broadcast communication;computer communications software;message passing;multiprocessing systems;network topology;optimisation;processor scheduling;tree data structures","runtime optimization;dynamic network topology information;modern commodity compute clusters;multicore nodes;multicore clusters;inter-node network communications;heterogeneous network topology;tiered network topology;homogeneous network composition;heterogeneous cluster;performance optimization;MPI broadcast communications;message scheduling;FEF heuristic;fastest edge first heuristic;nested binomial tree structure","","","16","","","","","","IEEE","IEEE Conferences"
"VectorSTM: Software Transactional Memory without Atomic Instructions","L. Peng; L. Xie; X. Zhang; X. Xie","NA; NA; NA; NA","2010 Third International Joint Conference on Computational Science and Optimization","","2010","2","","278","282","Transactional Memory(TM) is a promising way to coordinate concurrent threads in multi-core processors. Software transactional memory (STM) can run on conventional processors without additional hardware support. In this paper we propose VectorSTM which reduces the cost of centralized concurrency control. VectorSTM employs distributed vector timestamps instead of a single global timestamp to track the progress of transactions. Conflict detecting and transaction committing are done by polling thread local bloom filter queues which are indexed by the vector timestamps. Without employing any atomic instructions, VectorSTM reduces synchronization cost on the global timestamp variable and provides more concurrency. VectorSTM provides privatization safety which is critical to software transactional memory safety and avoids live lock and starvation by effective contention manager. We evaluate VectorSTM with STAMP benchmarks and the results show that the design offer superior performance or stronger semantics than TL2 and RingSTM algorithm. On particular tests VectorSTM outperforms TL2 and RingSTM 27% and 41% respectively with 8 threads running.","","978-1-4244-6813-3978-1-4244-6812","10.1109/CSO.2010.145","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5533020","multi-core processors;software transactional memory;vector timestamp;atomic instructions","Software safety;Multicore processing;Concurrency control;Concurrent computing;Memory management","concurrency control;multiprocessing systems;multi-threading;transaction processing","VectorSTM;software transactional memory;atomic instruction;transactional memory;coordinate concurrent threads;multicore processor;centralized concurrency control;distributed vector timestamp;polling thread local bloom filter queues;synchronization cost;global timestamp variable;software transactional memory safety;contention manager;STAMP benchmark;RingSTM algorithm","","","15","","","","","","IEEE","IEEE Conferences"
"Principles and Tools for Collaborative Entity-Based Intelligence Analysis","E. A. Bier; S. K. Card; J. W. Bodnar","Palo Alto Research Center, Inc., Palo Alto; Palo Alto Research Center, Inc., Palo Alto; Science Applications International Corporation, McLean","IEEE Transactions on Visualization and Computer Graphics","","2010","16","2","178","191","Software tools that make it easier for analysts to collaborate as a natural part of their work will lead to better analysis that is informed by more perspectives. We are interested to know if software tools can be designed that support collaboration even as they allow analysts to find documents and organize information (including evidence, schemas, and hypotheses). We have modified the Entity Workspace system, described previously, to test such designs. We have evaluated the resulting design in both a laboratory study and a study where it is situated with an analysis team. In both cases, effects on collaboration appear to be positive. Key aspects of the design include an evidence notebook optimized for organizing entities (rather than text characters), information structures that can be collapsed and expanded, visualization of evidence that emphasizes events and documents (rather than emphasizing the entity graph), and a notification system that finds entities of mutual interest to multiple analysts. Long-term tests suggest that this approach can support both top-down and bottom-up styles of analysis.","1077-2626;1941-0506;2160-9306","","10.1109/TVCG.2009.104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5226632","User interfaces;graphical user interfaces (GUI);information search and retrieval;information filtering;information systems applications;miscellaneous;group and organization interfaces;collaborative computing;computer-supported cooperative work;Web-based interaction.","Collaborative tools;Collaborative work;Software tools;Collaborative software;Information analysis;Text analysis;System testing;Laboratories;Design optimization;Organizing","groupware;software tools;user interfaces","collaborative entity;intelligence analysis;software tools;entity workspace system;evidence notebook;information structure;evidence visualization;notification system;top-down analysis style;bottom-up analysis style","Algorithms;Artificial Intelligence;Computer Graphics;Computer Simulation;Cooperative Behavior;Information Storage and Retrieval;Models, Theoretical;Software;User-Computer Interface","11","18","","","","","","IEEE","IEEE Journals & Magazines"
"Energy optimal on-line Self-Test of microprocessors in WSN nodes","A. Merentitis; A. Paschalis; D. Gizopoulos; N. Kranitis","Dept. of Informatics &amp; Tel. Univ. of Athens, Greece; Dept. of Informatics &amp; Tel. Univ. of Athens, Greece; Dept. of Informatics, Univ. of Piraeus, Greece; Dept. of Informatics &amp; Tel. Univ. of Athens, Greece","2010 IEEE International Conference on Computer Design","","2010","","","376","383","Wireless Sensor Network (WSN) applications often need to be deployed in harsh environments, where the possibility of faults due to environmental hazards is significantly increased, while silicon aging and wearout effects are also exacerbated. For such applications, periodic on-line testing of the WSN nodes is an important step towards correctness of operation. However, on-line testing of processors integrated in WSN nodes has to address the additional challenge of minimum energy consumption, because these devices operate on battery, which usually cannot be replaced and in the absence of catastrophic failures determines the lifetime of the system. In this paper initially we derive analytically the optimal way for executing on-line periodic test with adjustable period, taking into account the degrading behavior of the system due to silicon aging effects but also the limited energy budget of WSN applications. The test is applied in the form of Software-Based Self-Test (SBST) routines, thus we proceed to the power optimized development of SBST routines targeting the transition delay fault model that is well suited for detecting timing violations due to silicon aging. Simulation results show that energy savings for the final SBST routine at processor level are up to 35.4% and the impact of test in the battery life of the system is negligible.","1063-6404;1063-6404","978-1-4244-8937-4978-1-4244-8936-7978-1-4244-8935","10.1109/ICCD.2010.5647693","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5647693","","Wireless sensor networks;Program processors;Delay;Testing;Circuit faults;Optimization;Weibull distribution","built-in self test;microprocessor chips;wireless sensor networks","energy optimal on-line self-test;microprocessor;WSN nodes;wireless sensor network;environmental hazard;silicon aging effect;wearout effect;software-based self-test;SBST","","","37","","","","","","IEEE","IEEE Conferences"
"Testing Method of Code Redundancy Simplification Based on Program Dependency Graph","Y. Fan; Z. Huanguo; Y. Fei; Y. Jian","NA; NA; NA; NA","2012 IEEE 11th International Conference on Trust, Security and Privacy in Computing and Communications","","2012","","","1895","1900","This paper presents a testing method of code redundancy simplification from the aspect of software static analysis. We study on the dependency relationships between the variables, branches and functions in source or intermediate code of the tested object by PDG (Program Dependence Graph). This method establishes an effective testing to discover and locate the redundant functional modules and the unreachable paths based on dependency relationship. Compared with the conventional code optimization which emphasizes the improvement of time efficiency, we compress the size of source code and object code, independent of the programming language the source program uses. Experimental results show that the source or object code size decreases by 1 to 3 percent approximately after our simplification, not only reducing the occupied space but also assuring the functional consistency.","2324-898X;2324-9013","978-1-4673-2172-3978-0-7695-4745","10.1109/TrustCom.2012.269","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6296219","Code Redundancy Simplification;Static Analysis;Program Dependency Graph;Unreachable Path","Redundancy;Optimization;Testing;Educational institutions;Software;Flow graphs;Algorithm design and analysis","graph theory;program compilers;program testing","code redundancy simplification;testing method;program dependency graph;software static analysis;intermediate code;source code;program dependence graph;PDG;programming language","","","24","","","","","","IEEE","IEEE Conferences"
"Improved artificial bee colony algorithm with mutual learning","Y. Liu; X. Ling; Y. Liang; G. Liu","School of Software, Dalian University of Technology, Dalian 116024, P. R. China; School of Software, Dalian University of Technology, Dalian 116024, P. R. China; Civil Aviation Flight University of China, Guanghan 618307, P. R. China; School of Software, Dalian University of Technology, Dalian 116024, P. R. China; School of Software, Dalian University of Technology, Dalian 116024, P. R. China","Journal of Systems Engineering and Electronics","","2012","23","2","265","275","The recently invented artificial bee colony (ABC) algorithm is an optimization algorithm based on swarm intelligence that has been used to solve many kinds of numerical function optimization problems. It performs well in most cases, however, there still exists an insufficiency in the ABC algorithm that ignores the fitness of related pairs of individuals in the mechanism of finding a neighboring food source. This paper presents an improved ABC algorithm with mutual learning (MutualABC) that adjusts the produced candidate food source with the higher fitness between two individuals selected by a mutual learning factor. The performance of the improved MutualABC algorithm is tested on a set of benchmark functions and compared with the basic ABC algorithm and some classical versions of improved ABC algorithms. The experimental results show that the MutualABC algorithm with appropriate parameters outperforms other ABC algorithms in most experiments.","1004-4132","","10.1109/JSEE.2012.00034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6190877","artificial bee colony (ABC) algorithm;numerical function optimization;swarm intelligence;mutual learning","Optimization;Benchmark testing;Particle swarm optimization;Educational institutions;Memory management","","","","7","","","","","","","BIAI","BIAI Journals & Magazines"
"Lightweight Data-Flow Analysis for Execution-Driven Constraint Solving","J. H. Siddiqui; D. Marinov; S. Khurshid","NA; NA; NA","2012 IEEE Fifth International Conference on Software Testing, Verification and Validation","","2012","","","91","100","Constraint-based testing is a methodology for finding bugs in code, which has been successfully used for testing real systems. A key element of the methodology is generation of test inputs from input constraints, i.e., properties of desired inputs, which is performed by solving the constraints. We present a novel approach to optimize input generation from imperative constraints, i.e., constraints written as predicates in an imperative language. A well known technique for solving such constraints is execution-driven monitoring, where the given predicate is executed on candidate inputs to filter and prune invalid inputs, and generate valid ones. Our insight is that a lightweight static data-flow analysis of the given imperative constraint can enable more efficient solving. This paper describes an approach that embodies our insight and evaluates it using a suite of well-studied subject constraints. The experimental results show our approach provides substantial speedup over previous work.","2159-4848","978-0-7695-4670-4978-1-4577-1906","10.1109/ICST.2012.89","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200100","Constraint-based testing;static data-flow analysis;multi-value comparisons","Algorithm design and analysis;Instruments;Testing;Binary search trees;Monitoring;Heuristic algorithms;Computer bugs","data flow analysis;program debugging;program testing","lightweight static data-flow analysis;execution-driven constraint solving;constraint-based testing;imperative constraint;imperative language;execution-driven monitoring;subject constraints","","","29","","","","","","IEEE","IEEE Conferences"
"GUI Software Fault Localization Using N-gram Analysis","Z. Yu; H. Hu; C. Bai; K. Cai; W. E. Wong","NA; NA; NA; NA; NA","2011 IEEE 13th International Symposium on High-Assurance Systems Engineering","","2011","","","325","332","Graphical User Interfaces (GUIs) have become an important and accepted way of interacting with today's software. Fault localization is considered to be one of the most expensive program debugging activities. This paper presents a fault localization technique designed for GUI software. Unlike traditional software, GUI test cases usually are event sequences and each individual event has a unique corresponding event handler. We apply data mining techniques to the event sequences and their output data in terms of failure detections collected in the testing phase to rank the fault proneness of the event handlers for fault localization. Our method applies N-gram analysis to rank the event handlers of GUI programs and data collected from case studies on four real life GUI programs demonstrate the effectiveness of the proposed technique.","1530-2059;1530-2059","978-1-4673-0107-7978-0-7695-4615","10.1109/HASE.2011.29","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113915","GUIs;fault localization;event sequences;event handler;N-gram analysis","Graphical user interfaces;Software;Software algorithms;Association rules;Itemsets;Testing","data mining;graphical user interfaces;software fault tolerance","GUI software fault localization;graphical user interface;N-gram analysis;event sequence;event handler;data mining technique","","5","12","","","","","","IEEE","IEEE Conferences"
"Automating test automation","S. Thummalapenta; S. Sinha; N. Singhania; S. Chandra","IBM Research - India; IBM Research - India; IBM Research - India; IBM T. J. Watson Research Center","2012 34th International Conference on Software Engineering (ICSE)","","2012","","","881","891","Mention “test case”, and it conjures up the image of a script or a program that exercises a system under test. In industrial practice, however, test cases often start out as steps described in natural language. These are essentially directions a human tester needs to follow to interact with an application, exercising a given scenario. Since tests need to be executed repeatedly, such manual tests then have to go through test automation to create scripts or programs out of them. Test automation can be expensive in programmer time. We describe a technique to automate test automation. The input to our technique is a sequence of steps written in natural language, and the output is a sequence of procedure calls with accompanying parameters that can drive the application without human intervention. The technique is based on looking at the natural language test steps as consisting of segments that describe actions on targets, except that there can be ambiguity in identifying segments, in identifying the action in a segment, as well as in the specification of the target of the action. The technique resolves this ambiguity by backtracking, until it can synthesize a successful sequence of calls. We present an evaluation of our technique on professionally created manual test cases for two open-source web applications as well as a proprietary enterprise application. Our technique could automate over 82% of the steps contained in these test cases with no human intervention, indicating that the technique can reduce the cost of test automation quite effectively.","1558-1225;0270-5257;0270-5257","978-1-4673-1067-3978-1-4673-1066-6978-1-4673-1065","10.1109/ICSE.2012.6227131","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227131","","Manuals;Automation;Optimization;Humans;Natural languages;Programming profession","natural languages;program testing","automating test automation;test case;natural language;programmer time;human intervention","","19","32","","","","","","IEEE","IEEE Conferences"
"Using Prioritized Disk Service to Expedite Program Execution","T. Yeh; L. Yang","NA; NA","2012 IEEE 14th International Conference on High Performance Computing and Communication & 2012 IEEE 9th International Conference on Embedded Software and Systems","","2012","","","1027","1032","Computer systems often host multiple programs in execution simultaneously. Among those running programs, some may be important and time-critical, which users would expect them to finish their execution as soon as possible. Generally speaking, the course of program execution includes CPU operation and hard disk operation (disk I/O). For the CPU operation, modern computer systems have the ability to adjust the CPU scheduling sequence according to program priority. However, most computer systems do not have effective ways to conduct disk I/O based on program priority. The Linux operating system has been widely used in many areas. It supports several disk schedulers. The Complete Fair Queuing (CFQ) and the Anticipatory Scheduling (AS) are among those most well-known. Currently, CFQ is the default disk scheduler in the Linux operating system. AS is the predecessor of CFQ. Unfortunately, CFQ only offers prioritized disk I/O to some extent through the tool ""ionice"", while AS does not provide any prioritized service at all. We propose and implement a new disk scheduler, namely Prioritized Anticipatory Scheduling (PAS), by adding schemes of supporting prioritized disk I/O into AS in the Linux kernel. Our experimental results show that PAS surpasses CFQ with ionice for the vast majority of all test cases. Compared with AS, PAS can improve the performance of programs with high disk I/O priority by up to 71.88%.","","978-1-4673-2164-8978-0-7695-4749","10.1109/HPCC.2012.150","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6332286","disk performance;disk scheduling","Linux;Kernel;Computers;Time factors;Hard disks;Processor scheduling","hard discs;input-output programs;Linux;multiprocessing systems;queueing theory;scheduling","prioritized disk service;multiple program execution course;CPU operation;hard disk operation;modern computer systems;CPU scheduling sequence;program priority-based disk I/O;Linux operating system;complete fair queuing;CFQ;anticipatory scheduling;AS;Linux operating system;CFQ predecessor;prioritized anticipatory scheduling;PAS;Linux kernel;high disk I-O priority","","","18","","","","","","IEEE","IEEE Conferences"
"Cyberphysical Systems: Workload Modeling and Design Optimization","R. Marculescu; P. Bogdan","Carnegie Mellon University; Carnegie Mellon University","IEEE Design & Test of Computers","","2011","28","4","78","87","Built to interact with the physical world, a cyberphysical system (CPS) must be efficient, reliable, and safe. To optimize such systems, a science of CPS design considering workload characteristics (e.g., self-similarity and nonstationarity) must be established. CPS modeling and design are greatly improved when statistical physics approaches - such as master equations, renormalization group theory, and fractional derivatives - are implemented in the optimization loop.","0740-7475;1558-1918","","10.1109/MDT.2010.142","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5661751","design and test;cyberphysical systems;stochastic processes;workloads;fractals;statistical physics;fractional calculus","Fractals;Mathematical model;Equations;Stochastic processes;Computational modeling;Cyberspace;Workflow management software","embedded systems;optimisation;programming;statistical analysis","cyberphysical system;CPS design;CPS workload modeling;statistical physics approach;master equation;renormalization group theory;fractional derivative;optimization loop;self-similarity characteristic;nonstationarity characteristic","","18","12","","","","","","IEEE","IEEE Journals & Magazines"
"Quality Playbook: Ensuring Release to Release Improvement","P. Rotella; S. Chulani; S. Pradhan","NA; NA; NA","2012 IEEE 23rd International Symposium on Software Reliability Engineering Workshops","","2012","","","53","53","Summary form only given. Before a major feature release is made available to customers, it is important to be able to anticipate if the release will be of lesser quality than its predecessor release. Our research group has developed models that use development and test times, resource levels, code added, and bugs found and fixed (or not fixed) to predict whether or not a new feature release will achieve a key quality goal - to be of better quality than its predecessor release. If the release quality prediction models, developed early in the development branches integration phase, indicate a likely upcoming quality problem in the field, another set of predictive models ('playbook' models) are then developed and used by our team to identify development or test practices that are in need of improvement. These playbook models are key components of what we call 'quality playbooks,' that are designed to address several objectives: . Identify 'levers' that positively influence feature release quality. Levers are in-process engineering metrics that are associated with specific development or test processes/practices and measure their adoption and effectiveness. . If possible, identify levers that can be invoked early in the lifecycle, to enable the development and test teams to improve deficient practices and remediate the current release under development. If it is not possible to identify early levers but possible to identify levers later in the lifecycle, we can only change deficient practices to improve the quality of future successor releases. . Determine the potential quality impact of changes suggested by the profile of significant levers. Low impact levers are likely not to be addressed by development teams. . Determine the resource and schedule investments needed to change and implement practices: Training, disruption, additional engineering time, etc. . Using impact and investment calculations identify which practices to change, either for the current release or just for subsequent releases. Develop a prioritization/ROI scheme to provide planning guidance to development and test teams. . Identify specific practice changes needed, or new practices to adopt. . Design and plan pilot programs to test the models, including the impact and investment components. Using this 'playbook' approach, our team has developed models for 31 major feature releases that are resident on 11 different hardware platforms. These models have identified six narrowly-defined classes of metrics that include both actionable levers and 'indicator' metrics that correlate well with release quality. (Indicator metrics do also correlate well, but are less specifically actionable.) The models for these six classes of metrics (and their associated practices) include strong levers and strong indicators for all releases and platforms thus far examined. Impact and investment results are also described in this paper, as are pilot programs that have tested the validity of the modeling and business calculation results. Two additional large-scale pilots of the 'playbook' approach are underway, and these are also described.","","978-1-4673-5048-8978-0-7695-4928","10.1109/ISSREW.2012.104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405417","","Predictive models;Investments;Abstracts;Computer bugs;Current measurement;Schedules","economic indicators;investment;prediction theory;quality management;resource allocation;scheduling","quality playbook;predecessor release improvement;resource levels;test times;bugs;key quality goal;quality prediction models;development branches integration phase;predictive models;development identification;test practices;in-process engineering metrics;test processes;test teams;future successor releases quality;potential quality impact;significant lever profile;schedule investments;resource investments;investment calculation identification;prioritization-ROI scheme;investment components;hardware platforms;actionable levers;indicator metrics;business calculation;modeling validity;large-scale pilots","","","","","","","","","IEEE","IEEE Conferences"
"Geometry optimization for fluid transport of a bio-inspired nano-fluidic system","Jae-Hwan Lee; R. M. Pidaparti","Department of Mechanical and Nuclear Engineering, Virginia Commonwealth University, Richmond, 23284, USA; Department of Mechanical and Nuclear Engineering, Virginia Commonwealth University, Richmond, 23284, USA","10th IEEE International Conference on Nanotechnology","","2010","","","443","446","A bio-inspired nano-fluidic system mimicking the nuclear pore complex (NPC) is investigated for fluidic transport by optimizing the geometry. In general, nuclear pore complex contains very distinct geometrical components to allow various macromolecules very effectively through the pore. In order to understand and design fluidic systems for drug delivery and other applications, this study explored the optimization of a central plug location of NPC for achieving the maximum velocity for fluidic transport. The approach involves conducting fluid simulations with ANSYS software and optimizing the results using EXCEL and Genetic Algorithm (GA) optimization including GANetXL and MATLAB software. Based on the results obtained, one configuration of the central plug location achieved maximum velocity through the modeled nano-fluidic system. Currently, the nano-fluidic system is further optimized for manufacturing and eventually, testing to validate the optimized design.","1944-9380;1944-9399;1944-9399","978-1-4244-7032-7978-1-4244-7033-4978-1-4244-7031","10.1109/NANO.2010.5697776","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5697776","","Optimization;Gallium;Plugs;Nanobioscience;Finite element methods;Mathematical model;Biological cells","biological fluid dynamics;bioMEMS;drug delivery systems;flow simulation;genetic algorithms;mathematics computing;medical computing;nanobiotechnology;nanofluidics","geometry optimization;fluid transport;bioinspired nanofluidic system;nuclear pore complex;macromolecules;drug delivery;central plug location;fluid simulations;ANSYS software;EXCEL;genetic algorithm;GANetXL;MATLAB software","","","4","","","","","","IEEE","IEEE Conferences"
"Designing a hardware in the loop wireless digital channel emulator for software defined radio","J. Matai; P. Meng; L. Wu; B. Weals; R. Kastner","Department of Computer Science and Engineering, University of California, San Diego, USA; Department of Computer Science and Engineering, University of California, San Diego, USA; Department of Computer Science and Engineering, University of California, San Diego, USA; Toyon Research Corporation, USA; Department of Computer Science and Engineering, University of California, San Diego, USA","2012 International Conference on Field-Programmable Technology","","2012","","","206","214","The testing, verification and evaluation of wireless systems is an important but challenging endeavor. The most realistic method to test a wireless system is a field deployment. Unfortunately, this is not only expensive but also time consuming. In this paper, we present the design and implementation of a digital wireless channel emulator, which connects directly to a number of radios, and mimics the wireless channels between them, across a range of scenarios, in real-time. We use high-level synthesis tools to design the emulator while performing design space exploration. We describe the optimizations and tradeoffs that were necessary to reach the target throughput and area requirements.","","978-1-4673-2845-6978-1-4673-2846-3978-1-4673-2844","10.1109/FPT.2012.6412135","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6412135","","Wireless communication;Optimization;Throughput;Doppler effect;Clocks;Delay;Field programmable gate arrays","high level synthesis;optimisation;software radio;wireless channels","hardware design;loop wireless digital channel emulator;software defined radio;wireless system testing;wireless system verification;wireless system evaluation;field deployment;digital wireless channel emulator;wireless channels;high-level synthesis tools;design space exploration;optimizations","","7","16","","","","","","IEEE","IEEE Conferences"
"Logistics distribution business process simulation and optimization based on Petri nets","Xinchun Wu; Li Chen; Lizhi Zhang","School of Logistics Engineering, WHUT, wuhan, 430063, China; School of Logistics Engineering, WHUT, wuhan, 430063, China; School of Logistics Engineering, WHUT, wuhan, 430063, China","2011 6th International Conference on Pervasive Computing and Applications","","2011","","","126","128","This paper studies a large supermarket chains, which using of logistics simulation software ExSpect to test the model performance and simulation experiment. It constructs logistics distribution business process model based on Petri net tools, analyzes the enterprise distribution business process, and then puts forward the improvement process optimization scheme according to the simulation data. Comparing the simulation optimization model with before, validated the proposed solution which based on Petri net to solve the logistics business process reengineering and optimization is effective and practical.","","978-1-4577-0208-2978-1-4577-0209-9978-1-4577-0207","10.1109/ICPCA.2011.6106491","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6106491","Petri net;Business Process reengineering;ExSpect simulation","Business;Analytical models;Data models;Uncertainty;Educational institutions","business process re-engineering;corporate modelling;logistics data processing;marketing data processing;optimisation;Petri nets","supermarket chains;logistic simulation software;ExSpect;model performance;logistic distribution business process simulation model;Petri net tools;enterprise distribution business process;improvement process optimization scheme;simulation optimization model;logistic business process reengineering","","","4","","","","","","IEEE","IEEE Conferences"
"Robust System Design","S. Mitra","NA","2010 23rd International Conference on VLSI Design","","2010","","","434","439","Robust system design ensures that future systems continue to meet user expectations despite rising levels of underlying disturbances. This paper discusses two essential aspects of robust system design: 1. Effective post-silicon validation, despite staggering complexity of future systems, using a new technique called Instruction Footprint Recording and Analysis (IFRA). 2. Cost-effective design of systems that overcome CMOS reliability challenges through built-in tolerance to errors in hardware during system operation. A combination of Built-In Soft Error Resilience (BISER) and circuit failure prediction, together with on-line self-test/diagnostics and software-orchestrated optimization across multiple abstraction layers, enable design of cost-effective resilient systems.","1063-9667;2380-6923","978-1-4244-5542-3978-1-4244-5541-6978-0-7695-3928","10.1109/VLSI.Design.2010.77","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5401207","Robust systems;Reliability;Validation;IFRA;BISER;Built-In Soft Error Resilience;Circuit Failure Prediction;On-line Self-Test;soft errors;aging;post-silicon validation","Robustness;Hardware;Computer bugs;Circuits;Costs;Computer crashes;System analysis and design;Logic design;Open source software;Failure analysis","automatic testing;built-in self test;CMOS integrated circuits;error correction;failure analysis;fault tolerance;integrated circuit design;integrated circuit reliability;integrated circuit testing","robust system design;user expectation;post-silicon validation;instruction footprint recording and analysis;CMOS reliability;built-in error tolerance;built-in soft error resilience;circuit failure prediction;self-diagnostics;software-orchestrated optimization;online self-test","","5","60","","","","","","IEEE","IEEE Conferences"
"Automated prediction of defect severity based on codifying design knowledge using ontologies","M. Iliev; B. Karasneh; M. R. V. Chaudron; E. Essenius","Leiden Institute of Advanced Computer Science, Leiden University, Leiden, The Netherlands; Leiden Institute of Advanced Computer Science, Leiden University, Leiden, The Netherlands; Leiden Institute of Advanced Computer Science, Leiden University, Leiden, The Netherlands; Technical Software Engineering, Logica Nederland B. V., Rotterdam, The Netherlands","2012 First International Workshop on Realizing AI Synergies in Software Engineering (RAISE)","","2012","","","7","11","Assessing severity of software defects is essential for prioritizing fixing activities as well as for assessing whether the quality level of a software system is good enough for release. In filling out defect reports, developers routinely fill out default values for the severity levels. The purpose of this research is to automate the prediction of defect severity. Our aim is to research how this severity prediction can be achieved through reasoning about the requirements and the design of a system using ontologies. In this paper we outline our approach based on an industrial case study.","","978-1-4673-1753-5978-1-4673-1752","10.1109/RAISE.2012.6227962","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227962","severity;defect;design;ontology;automatic classification","Ontologies;Testing;IEEE standards;Cognition;Software systems","inference mechanisms;ontologies (artificial intelligence);program compilers;software quality","automated defect severity prediction;design knowledge codification;ontologies;software defects severity;fixing activities;software system quality level;reasoning;industrial case study","","3","7","","","","","","IEEE","IEEE Conferences"
"A Profile-Based Method for Hardware/Software Co-design Applied in Evolutionary Robotics Using Reconfigurable Computing","M. A. Dias; D. O. Sales; F. S. Osorio","NA; NA; NA","2010 IEEE Electronics, Robotics and Automotive Mechanics Conference","","2010","","","463","468","Evolutionary algorithms are a very common technique on computational intelligence field. Some algorithms need a huge amount of memory, making them not so trivial to apply on embedded systems. In this work a profile-based approach is proposed and applied in an evolutionary algorithm with some characteristics that allow its use on embedded systems: the micro-GA. The main goal is to improve the execution time compared to other software algorithms obtaining also an acceptable design time. The results presents comparisons between implementations and discussions about soft-processor features influence on this type of algorithm.","","978-1-4244-8149","10.1109/CERMA.2010.59","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5692381","FPGA;Micro Genetic Algorithm;Evolutionary Computation;Hardware Optimization;HW/SW Co-Design","Hardware;Program processors;Robots;Field programmable gate arrays;Embedded systems;Benchmark testing","embedded systems;genetic algorithms;hardware-software codesign;robot programming;software engineering","profile based method;hardware software codesign;evolutionary robotic;reconfigurable computing;embedded system;profile based approach;software algorithm;microgenetic algorithm","","1","19","","","","","","IEEE","IEEE Conferences"
"CFDComm: An Optimized Library for Scalable Point-to-Point Communication for General CFD Applications","S. Haeri; J. S. Shrimpton","NA; NA","2012 IEEE 14th International Conference on High Performance Computing and Communication & 2012 IEEE 9th International Conference on Embedded Software and Systems","","2012","","","1001","1006","Domain decomposition is the most widely used technique to achieve parallelism in CFD applications. For complicated geometries usually graph partitioning programs are used to decompose the domain into smaller computational blocks such that the computation load is balanced and communication cost is minimized. In this paper an algorithm is provided and tested which avoids deadlocks in complicated communications patterns inherited from the graph decomposition process. The basic algorithm is implemented using FORTRAN 95 and MPI and then several optimization techniques are used to increase the scalability of the library which include addition of topologies, overlap of communication and computation to mask the message passing latency and non-blocking communication. The library is tested for up to 512 cores on the Iridis-3 cluster which incorporates 1008 compute nodes each composed of 2, 2.4 GHz 6-core Westmere processors. IO and inter-node communication is via a fast Infiniband network which is composed of groups of 32 nodes connected by DDR links to a 48 port QDR leaf-switch. The leaf switches then have 4 trunked QDR connections to 4 QDR 48-port core switches.","","978-1-4673-2164-8978-0-7695-4749","10.1109/HPCC.2012.146","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6332282","MPI;Point-to-point Communication;Domain Decomposition;Parallel Performance;Computational Fluid Dynamics","Program processors;Arrays;Libraries;Topology;Computational fluid dynamics;System recovery;Optimization","computational fluid dynamics;FORTRAN;message passing;multiprocessing systems;network theory (graphs);network topology;optimisation;software libraries","library optimization;scalable point-to-point communication;CFD;domain decomposition;graph partitioning program;graph decomposition process;FORTRAN 95;MPI;message passing latency;network topology;nonblocking communication;Iridis-3 cluster;Westmere processor;IO communication;internode communication;Infiniband network;DDR link;QDR leaf switch;frequency 2 GHz;frequency 2.4 GHz","","1","14","","","","","","IEEE","IEEE Conferences"
"Parallel algorithms for grounding grids corrosion diagnosis to improve computation speed","L. Jian; F. Yuan","School of Communication & Information Engineering Xi'an University of Science & Technology, Xi'an, China; School of Communication & Information Engineering Xi'an University of Science & Technology, Xi'an, China","2010 2nd IEEE International Conference on Information Management and Engineering","","2010","","","297","301","The existing grounding grid corrosion diagnosis algorithms have large amount of calculation, they take up lots of time and are inconvenient in the practical application. In this paper, a parallel algorithm based on Local Area Network (LAN) is put forward to solve above problems. The master sub-process sends the samples needed to be calculated to the slave sub-processes on the computers of the LAN. The slave sub-processes calculate the samples at the same time and send the results back to the master sub-process. The final result will be figured out by the master sub-process after all the results being gathered. The parallel algorithms of grounding grids testability evaluation, uncertain branches estimation and monitoring pole allocation optimization are discussed, respectively. An experimental grounding grid with sixty branches is used as an example to validate the proposed parallel algorithms. The test is based on a LAN with five computers. The results show that the proposed methods are feasible and the calculation time can be shortened remarkably.","","978-1-4244-5263-7978-1-4244-5265","10.1109/ICIME.2010.5478034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5478034","Grounding grids;corrosion diagnosis;testability evaluation;algorithm;genetic algorithm;uncertain branches;parallel algorithms","Parallel algorithms;Grounding;Corrosion;Concurrent computing;Grid computing;Local area networks;Master-slave;Testing;Application software;Computerized monitoring","corrosion testing;local area networks;optimisation;parallel algorithms;power aware computing;power grids","parallel algorithms;grounding grid corrosion diagnosis;computation speed;local area network;LAN;master sub-process;slave sub-processes;grounding grids testability evaluation;uncertain branches estimation;pole allocation optimization monitoring","","12","14","","","","","","IEEE","IEEE Conferences"
"Effective Software Fault Localization Using an RBF Neural Network","W. E. Wong; V. Debroy; R. Golden; X. Xu; B. Thuraisingham","Department of Computer Science, University of Texas, Dallas, USA; Department of Computer Science, University of Texas, Dallas, USA; School of Behavioral and Brain Sciences, University of Texas, Dallas, USA; Department of Computer Science, University of Texas, Dallas, USA; Department of Computer Science, University of Texas, Dallas, USA","IEEE Transactions on Reliability","","2012","61","1","149","169","We propose the application of a modified radial basis function neural network in the context of software fault localization, to assist programmers in locating bugs effectively. This neural network is trained to learn the relationship between the statement coverage information of a test case and its corresponding execution result, success or failure. The trained network is then given as input a set of virtual test cases, each covering a single statement. The output of the network, for each virtual test case, is considered to be the suspiciousness of the corresponding covered statement. A statement with a higher suspiciousness has a higher likelihood of containing a bug, and thus statements can be ranked in descending order of their suspiciousness. The ranking can then be examined one by one, starting from the top, until a bug is located. Case studies on 15 different programs were conducted, and the results clearly show that our proposed technique is more effective than several other popular, state of the art fault localization techniques. Further studies investigate the robustness of the proposed technique, and illustrate how it can easily be applied to programs with multiple bugs as well.","0018-9529;1558-1721","","10.1109/TR.2011.2172031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6058639","Fault location;radial basis function neural networks;software debugging","Neurons;Biological neural networks;Computer bugs;Radial basis function networks;Training;Software;Context","learning (artificial intelligence);program debugging;radial basis function networks;software fault tolerance","software fault localization;RBF neural network;radial basis function neural network;bugs location;statement coverage information;execution result;trained network;virtual test cases;fault localization techniques","","38","56","","","","","","IEEE","IEEE Journals & Magazines"
"Automated conformance evaluation of SystemC designs using timed automata","P. Herber; M. Pockrandt; S. Glesner","Berlin Institute of Technology (TU Berlin), Germany; Berlin Institute of Technology (TU Berlin), Germany; Berlin Institute of Technology (TU Berlin), Germany","2010 15th IEEE European Test Symposium","","2010","","","188","193","SystemC is widely used for modeling and simulation in hardware/software co-design. However, the co-verification techniques used for SystemC designs are mostly ad-hoc and non-systematic. A particularly severe drawback is that simulation results have to be evaluated manually. In previous work, we proposed to overcome this problem by conformance testing. We presented an algorithm that uses an abstract SystemC design to compute expected output traces, which are then compared with those of a refined design to evaluate its correctness. The main disadvantage of the algorithm is that it is very expensive because it computes the output traces offline and has to cope with non-deterministic systems. Furthermore, the designer has to compare the results manually with the outputs of a design under test. In this paper, we present an approach for efficient and fully-automatic conformance evaluation of SystemC designs. To achieve this, we first present optimizations of our previously proposed algorithm for the generation of conformance tests that drastically reduce computation time and memory consumption. The main idea is to exploit the specifics of the SystemC semantics to reduce the number of semantic states that have to be kept in memory during state-space exploration. Second, we present an approach to generate SystemC test benches from a set of expected output traces. These test benches allow fully-automatic test execution and conformance evaluation. Together with our previously presented model checking framework for abstract Sys-temC designs, we yield a fully-automatic HW/SW co-verification framework for SystemC that supports the whole design process. We demonstrate the performance and error detecting capability of our approach with experimental results.","1558-1780;1530-1877;1530-1877;1530-1877","978-1-4244-5835-6978-1-4244-5834-9978-1-4244-5832-5978-1-4244-5833","10.1109/ETSYM.2010.5512761","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5512761","","Automata;Algorithm design and analysis;System testing;Computational modeling;Embedded system;System-level design;Timing;Switches;Automatic testing;Hardware","conformance testing;hardware-software codesign","automated conformance evaluation;SystemC designs;timed automata;hardware/software co-design;state-space exploration","","2","20","","","","","","IEEE","IEEE Conferences"
"A binary Particle Swarm Optimization approach to fault diagnosis in parallel and distributed systems","R. Falcon; M. Almeida; A. Nayak","School of Information Technology and Engineering, University of Ottawa, 800 King Edward Ave., Ottawa ON, Canada; School of Information Technology and Engineering, University of Ottawa, 800 King Edward Ave., Ottawa ON, Canada; School of Information Technology and Engineering, University of Ottawa, 800 King Edward Ave., Ottawa ON, Canada","IEEE Congress on Evolutionary Computation","","2010","","","1","8","The efficient diagnosis of hardware and software faults in parallel and distributed systems remains a challenge in today's most prolific decentralized environments. System-level fault diagnosis is concerned with the identification of all faulty components among a set of hundreds (or even thousands) of interconnected units, usually by thoroughly examining a collection of test outcomes carried out by the nodes under a specific test model. This task has non-polynomial complexity and can be posed as a combinatorial optimization problem. Here, we apply a binary version of the Particle Swarm Optimization meta-heuristic approach to solve the system-level fault diagnosis problem (BPSO-FD) under the invalidation and comparison diagnosis models. Our method is computationally simpler than those already published in literature and, according to our empirical results, BPSO-FD quickly and reliably identifies the true ensemble of faulty units and scales well for large parallel and distributed systems.","1089-778X;1941-0026","978-1-4244-6911-6978-1-4244-6909-3978-1-4244-6910","10.1109/CEC.2010.5586002","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5586002","","Fault diagnosis;Optimization;Peer to peer computing;Mathematical model;Computational modeling;Particle swarm optimization;Complexity theory","combinatorial mathematics;computational complexity;distributed processing;fault diagnosis;particle swarm optimisation","binary particle swarm optimization approach;parallel systems;distributed systems;hardware faults;software faults;system-level fault diagnosis;nonpolynomial complexity;combinatorial optimization problem","","5","21","","","","","","IEEE","IEEE Conferences"
"Predicting microarchitectural power using Interval Based Hierarchical Support Vector Machine","M. Stockman; M. Awad","Electrical and Computer Engineering Department, American University of Beirut, Beirut, Lebanon; Electrical and Computer Engineering Department, American University of Beirut, Beirut, Lebanon","2010 International Conference on Energy Aware Computing","","2010","","","1","3","Microarchitectural design involves exploring an exponentially large design space in order to determine an optimal configuration for a number of hardware parameters. Determining a particular combination of these parameters which lead to low power consumption can be daunting. New configurations must be tested on software simulators using benchmark programs which typically take a considerable amount of time to run. In this paper we present Interval Based Hierarchical Support Vector Machine (IBH-SVM) for identifying optimal power aware combinations of microarchitectural parameters from this exponentially large design space. The advantage of this formulation is twofold in that it accurately and efficiently finds power aware configurations while considerably decreasing the number of software benchmark simulations needed to select the most appropriate configurations. The reduction in power is not only in terms of the savings on future applications run on these processors, but also on the testing time required during the design phase since suboptimal configurations are ignored early on.","2381-0947","978-1-4244-8275-7978-1-4244-8273-3978-1-4244-8274","10.1109/ICEAC.2010.5702317","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5702317","Hierarchical Support Vector Machine;Microarchitectural Design Space Exploration","Benchmark testing;Microarchitecture;Support vector machines;Predictive models;Computational modeling;Accuracy;Machine learning","benchmark testing;circuit simulation;computer architecture;digital simulation;optimisation;power aware computing;support vector machines","microarchitectural power;SVM;hierarchical support vector machine;benchmark program;power aware configuration;software benchmark simulation;power optimisation;power consumption","","1","8","","","","","","IEEE","IEEE Conferences"
"On the optimized generation of Software-Based Self-Test programs for VLIW processors","D. Sabena; M. S. Reorda; L. Sterpone","Dipartimento di Automatica e Informatica, Politecnico di Torino, Italy; Dipartimento di Automatica e Informatica, Politecnico di Torino, Italy; Dipartimento di Automatica e Informatica, Politecnico di Torino, Italy","2012 IEEE/IFIP 20th International Conference on VLSI and System-on-Chip (VLSI-SoC)","","2012","","","129","134","Software-Based Self-Test (SBST) approaches have shown to be an effective solution to detect permanent faults, both at the end of the production process, and during the operational phase. However, when Very Long Instruction Word (VLIW) processors are addressed these techniques require some optimization steps in order to properly exploit the parallelism intrinsic in these architectures. In this paper we present a new method that, starting from previously known algorithms, automatically generates an effective test program able to still reach high fault coverage on the VLIW processor under test, while reducing the test duration and the test code size. The method consists of three parametric phases and can deal with different VLIW processor models. The main goal of the proposed method is to automatically obtain a test program able to effectively reduce the test time and the required resources. Experimental results gathered on a case study show the effectiveness of the proposed approach.","","978-1-4673-2658","10.1109/VLSI-SoC.2012.7332089","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7332089","SBST;VLIW;Test Program Generation","VLIW;Program processors;Manuals;Libraries;Computer architecture;Schedules;Registers","","","","","14","","","","","","IEEE","IEEE Conferences"
"Model reduction techniques for the formal verification of hardware dependent software","W. Ecker; V. Esen; R. Findenig; T. Steininger; M. Velten","Infineon Technologies, Hagenberg; Infineon Technologies, Hagenberg; Infineon Technologies, Hagenberg; Infineon Technologies, Hagenberg; Infineon Technologies, Hagenberg","2010 IEEE International High Level Design Validation and Test Workshop (HLDVT)","","2010","","","148","153","Contemporary researches provide many solutions for formally verifying both hardware and software systems. In this paper, we describe the formal verification of assembly programs, which are part of the HW/SW interface in hybrid systems. We have developed several methods to model assembly programs in VHDL in order to verify their functionality. Our discussion will show that, by applying different reduction methods, we managed to formally verify the correctness of iterative algorithms with execution times higher than 6000 clock cycles.","1552-6674;1552-6674","978-1-4244-7806-4978-1-4244-7805-7978-1-4244-7804","10.1109/HLDVT.2010.5496647","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5496647","Formal Verification;Correctness of Assembly Programs;Control Flow Analysis;Cycle Accurate Modeling;Cycle Optimized Modeling","Reduced order systems;Formal verification;Hardware;Embedded system;Software systems;Costs;Assembly systems;Iterative algorithms;Clocks;Performance analysis","formal verification;hardware-software codesign;reduced order systems","model reduction techniques;formal verification;hardware dependent software;HW-SW interface;hybrid systems;iterative algorithms;VHDL","","3","11","","","","","","IEEE","IEEE Conferences"
"Evaluating Machine-Independent Metrics for State-Space Exploration","V. Jagannath; M. Kirn; Y. Lin; D. Marinov","NA; NA; NA; NA","2012 IEEE Fifth International Conference on Software Testing, Verification and Validation","","2012","","","320","329","Many recent advancements in testing concurrent programs can be described as novel optimization and heuristic techniques for exploring the tests of such programs. To empirically evaluate these techniques, researchers apply them on subject programs and capture a set of metrics that characterize the techniques' effectiveness. From a user's perspective, the most important metric is often the amount of real time required to find a error (if one exists), but using real time for comparison can be misleading because it is necessarily dependent on the machine configuration used for the experiments. On the other hand, using machine-independent metrics can be meaningless if they do not correlate highly with real time. As a result, it can be difficult to select metrics for valid comparisons among exploration techniques. This paper presents a study of the commonly used machine-independent metrics for two different exploration frameworks for Java (JPF and ReEx) by revisiting and extending a previous study (Parallel Randomized State-Space Search) and evaluating the correlation of the metrics with real time both on a single machine and on a high-performance cluster of machines. Our study provides new evidence for selecting metrics in future evaluations of exploration techniques by showing that several machine-independent metrics are a good substitute for real time, and that reporting real time results even from clusters can provide useful information.","2159-4848","978-0-7695-4670-4978-1-4577-1906","10.1109/ICST.2012.112","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200088","state-space;exploration;metrics;concurrency","Measurement;Real time systems;Schedules;Search problems;Correlation;System recovery;Computers","Java;program testing;software metrics","machine-independent metrics;state-space exploration;concurrent program testing;optimization techniques;heuristic techniques;machine configuration;exploration frameworks;Java;parallel randomized state-space search","","1","27","","","","","","IEEE","IEEE Conferences"
"Software effort estimation by tuning COOCMO model parameters using differential evolution","S. Aljahdali; A. F. Sheta","Dean of the College of Computers and Information Systems, Taif University, Saudi Arabia; Computer Science and Engineering, College of Computers and Information Systems, Taif University, Saudi Arabia","ACS/IEEE International Conference on Computer Systems and Applications - AICCSA 2010","","2010","","","1","6","Accurate estimation of software projects costs represents a challenge for many government organizations such as the Department of Defenses (DOD) and NASA. Statistical models considerably used to assist in such a computation. There is still an urgent need on finding a mathematical model which can provide an accurate relationship between the software project effort/cost and the cost drivers. A powerful algorithm which can optimize such a relationship via tuning mathematical model parameters is urgently needed. In two new model structures to estimate the effort required for software projects using Genetic Algorithms (GAs) were proposed as a modification to the famous Constructive Cost Model (COCOMO). In this paper, we follow up on our previous work and present Differential Evolution (DE) as an alternative technique to estimate the COCOMO model parameters. The performance of the developed models were tested on NASA software project dataset provided in. The developed COCOMO-DE model was able to provide good estimation capabilities.","2161-5322;2161-5330","978-1-4244-7717-3978-1-4244-7716-6978-1-4244-7715","10.1109/AICCSA.2010.5586985","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5586985","","Training;Tuning;Educational institutions","optimisation;software cost estimation","software effort estimation;COOCMO model parameter tuning;differential evolution;statistical model;mathematical model;constructive cost model;NASA software project dataset;optimization algorithm;genetic programming;software projects cost estimation","","9","33","","","","","","IEEE","IEEE Conferences"
"The research, test and verification of the HVDC centralized control system","Chunsheng Wang; Sen Liu; Lijun Huang; Hao Wu; K. Fan; Guohui Rao; Zhi He; Junfang Hao","Shenzhen Branch, XJ Electric Corp. Ltd., 518052, China; Guangzhou Branch, UHV Power Transmission Com., CSGP, 510405, China; XJ HVDC System Dept., Xuchang 461000, China; Guangzhou Branch, UHV Power Transmission Com., CSGP, 510405, China; Shenzhen Branch, XJ Electric Corp. Ltd., 518052, China; XJ HVDC System Dept., Xuchang 461000, China; Guangzhou Branch, UHV Power Transmission Com., CSGP, 510405, China; XJ HVDC System Dept., Xuchang 461000, China","CICED 2010 Proceedings","","2010","","","1","6","The necessity of the HVDC centralized control system is analyzed, and studies confirm its feasibility. Building a HVDC centralized control system to integrate all of the independent HVDC systems, is helpful to: (1) reducing the requirements of human resources and equipments, centralizing and strengthening the maintenance team, enhancing maintenance capabilities internally, (2) achieving the state of centralized monitoring-controlling and the high-level coordinate control with synthesized and optimized strategy. The designing principles of the centralized control centre are studied, and the key technologies need be verified are determined. To determining the HVDC centralized control system's feasibility and implementation procedure, it is necessary to build an appropriately simplified test system to verify all the involved software and hardware, the communication network, and the project organization and implementation procedure, carefully. The test tells, (1) The set of the HVDC centralized control system will not affect the safe operation of the original SCADA system in the converter station, but will get a backup system one more. (2) The HVDC centralized control system can receive all real-time data and SERs that are consistent with the original SCADA system without lost, (3) and can successfully execute all kinds of the HVDC control types such as deblocking the HVDC system and putting it into operation. (4) Based on the network communication middleware, the delay (&lt;;20ms) of the data transmission is in a range that can not be sensed. With a huge amount of data, the 2M-channel fully meet the real-time requirements of the transmission of data and commands (normally about 7.9KBps, in fault 20.9KBps) in the bandwidth test. (5) Benefitting from the distributed software architecture, the HVDC centralized control system can effectively withstand the stress test (18,000 data/second, CPU load rate &lt;;22%) caused by simulated faults happened in all existing and planned HVDC systems (10 converter stations) at the same time.","2161-7481;2161-749X","978-1-4577-0066","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5735924","HVDC Transmission;Centralized Control;SCADA;Converter Station;HVDC Control and Protection System","HVDC transmission;Monitoring;Variable speed drives;Humans;Local area networks;Optical switches","computer communications software;computer networks;formal verification;HVDC power convertors;HVDC power transmission;middleware;power engineering computing;power system control;power system reliability;power transmission control;program testing;SCADA systems","HVDC centralized control system;maintenance team;high level coordinate control;SCADA system;converter station;backup system;SER;network communication middleware","","","3","","","","","","IEEE","IEEE Conferences"
"Rule profiling for query optimizers and their implications","S. Chaudhuri; L. Giakoumakis; V. Narasayya; R. Ramamurthy","Microsoft Corp, One Microsoft Way Redmond WA, USA; Microsoft Corp, One Microsoft Way Redmond WA, USA; Microsoft Corp, One Microsoft Way Redmond WA, USA; Microsoft Corp, One Microsoft Way Redmond WA, USA","2010 IEEE 26th International Conference on Data Engineering (ICDE 2010)","","2010","","","1072","1080","Many modern optimizers use a transformation rule based framework. While there has been a lot of work on identifying new transformation rules, there has been little work focused on empirically evaluating the effectiveness of these transformation rules. In this paper we present the results of an empirical study of ""profiling"" transformation rules in Microsoft SQL Server using a diverse set of real world and benchmark query workloads. We also discuss the implications of these results for designing and testing query optimizers.","1063-6382;2375-026X","978-1-4244-5446-4978-1-4244-5445-7978-1-4244-5444","10.1109/ICDE.2010.5447870","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5447870","","Design optimization;Constraint optimization;Benchmark testing;Prototypes;Volcanoes;Software tools","optimisation;query processing;SQL","query optimizers rule profiling;transformation rule;microsoft SQL server","","","24","","","","","","IEEE","IEEE Conferences"
"Rate-Distortion Optimized Reference Picture Management for High Efficiency Video Coding","H. Li; B. Li; J. Xu","Department of Electronic Engineering and Information Science, University of Science and Technology of China, Hefei, Anhui, China; Department of Electronic Engineering and Information Science, University of Science and Technology of China, Hefei, Anhui, China; Microsoft Research Asia, Beijing, China","IEEE Transactions on Circuits and Systems for Video Technology","","2012","22","12","1844","1857","Motion compensation with multiple reference pictures has been widely used during the development of the emerging High Efficiency Video Coding (HEVC) standard, which greatly helps to improve the coding efficiency. Usually, a heuristic strategy is exploited to use the nearest reconstructed pictures as references. However, such a strategy may not be efficient on all occasions, especially when different content characteristics and coding settings are considered. In this paper, we investigate how to manage reference pictures so as to achieve better rate-distortion performance under the memory constraint of the decoded picture buffer at the decoder. We formulate the reference picture management as an optimization problem and approximate its optimal solution. Moreover, we explore how to adjust quality for each picture according to the reference structure to further improve coding efficiency. For some coding cases, where a complicated encoder optimization is unaffordable, we also develop fast algorithms to get the most benefit from reference picture selection. Among them, one strategy has been adopted by the HEVC software and common test conditions to generate the anchor. Experimental results show that the proposed full search algorithm and fast search algorithms achieve significant bitrate reduction.","1051-8215;1558-2205","","10.1109/TCSVT.2012.2223038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6324421","High Efficiency Video Coding (HEVC);rate-distortion (RD) optimization;reference picture management;reference picture selection","Encoding;Video coding;Optimization;Rate-distortion;Bit rate;Decoding","decoding;image reconstruction;optimisation;rate distortion theory;video coding","rate-distortion optimized reference picture management;high efficiency video coding;motion compensation;multiple reference pictures;HEVC standard;heuristic strategy;rate-distortion performance;decoded picture buffer;decoder;encoder optimization;reference picture selection;HEVC software","","20","32","","","","","","IEEE","IEEE Journals & Magazines"
"Research on Ant Colony algorithm for the QAP-based facility layout problems","H. Li; S. Sun; L. Ma","College of Management, Shanghai University of Science and Technology, Shanghai, P.R. China; College of Management, Shanghai University of Science and Technology, Shanghai, P.R. China; Information technology center, Jiujiang University, Jiujiang City, P.R. China","2011 6th IEEE Joint International Information Technology and Artificial Intelligence Conference","","2011","2","","130","134","In this paper, we propose a variant of the QAP-based facility layout problem: the exclusion facility layout problem (EFLP). Based on Ant Colony algorithm, we design the modified ant algorithm for solving the facility layout problem (FLP). In addition, we also design the ant algorithm for EFLP by increasing the operator of making solution feasible. Analyze the impact of the different parameters values on the results by a lot of data testing. Experimental results show that the algorithm has strong capability of global search and rapid speed of convergence.","","978-1-4244-8625-0978-1-4244-8622-9978-1-4244-8624","10.1109/ITAIC.2011.6030293","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6030293","quadratic assignment problem;ant colony algorithm;facility layout problem;feasible solutiond","Layout;Algorithm design and analysis;Optimization;Educational institutions;Logistics;Software algorithms;Resource management","facilities layout;optimisation;testing","ant colony algorithm;QAP-based facility layout problems;quadratic assignment problem;exclusion facility layout problem;EFLP;data testing","","","7","","","","","","IEEE","IEEE Conferences"
"Phase behavior in serial and parallel applications","A. Sembrant; D. Black-Schaffer; E. Hagersten","Uppsala University, Department of Information Technology, P.O. Box 337, SE-751 05, Sweden; Uppsala University, Department of Information Technology, P.O. Box 337, SE-751 05, Sweden; Uppsala University, Department of Information Technology, P.O. Box 337, SE-751 05, Sweden","2012 IEEE International Symposium on Workload Characterization (IISWC)","","2012","","","47","58","It is well known that most serial programs exhibit time varying behavior, for example, alternating between memory- and compute-bound phases. However, most research into program phase behavior has focused on the serial SPEC benchmark suite, with little investigations into large scale phase behavior in parallel applications. In this study we compare and examine the time-varying behavior of the SPEC2006 (serial) and the PARSEC 2.1 (parallel) benchmarks suites, and investigate the program phase behavior found in parallel applications with different parallelization models. To this end, we extend a general purpose runtime phase desection library to handle parallel applications. Our results reveal that serial applications have significantly more program phases (2.4x) with larger variation in CPI (1.5x) compared to parallel applications. While the number of phases are fewer in parallel applications, there still exists interesting phase behavior. In particular, we find that data-parallel applications have shorter phases with more threads. This makes phase-guided runtime optimizations (e.g., dynamic voltage frequency scaling) less attractive as the number of threads grows. Meaning it is much more difficult to exploit runtime optimizations in parallel applications.","","978-1-4673-4532-3978-1-4673-4531","10.1109/IISWC.2012.6402900","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6402900","","Benchmark testing;Runtime;Optimization;Vectors;Libraries;Measurement;Hardware","benchmark testing;optimisation;parallel processing;software libraries;software performance evaluation","parallel applications;serial applications;program phase behavior;serial SPEC benchmark suite;large scale phase behavior;time-varying behavior;SPEC2006 benchmark suite;PARSEC 2.1 benchmark suite;parallelization model;general purpose runtime phase desection library;CPI;data-parallel applications;phase-guided runtime optimization;cycle per instruction","","11","43","","","","","","IEEE","IEEE Conferences"
"A hybrid GA-based ant colony strategy for continuous correlated multiple response optimization problem","S. Kushwaha; I. Mukherjee","Cytel Statistical Software &amp; Services Pvt. Ltd, (A Subsidiary of Cytel Inc.), Pune, India; Shailesh J. Mehta School of Management, Indian Institute of Technology Bombay, Mumbai, India","2012 IEEE Colloquium on Humanities, Science and Engineering (CHUSER)","","2012","","","188","192","In this paper, suitability of an ant colony optimization (ACO) integrated with genetic algorithm-based local search for continuous multiple response optimization problem, commonly encountered in operations or production is validated. The overall work reported in this paper may be stratified into three parts. The first part is devoted to develop an ACO with diversification scheme for continuous search space using standard test functions. The second part discusses on how genetic algorithm (GA) is integrated with ACO, so as to improve the intensification of the search strategy. The final part of this work compares the performance of ACO-GA with simple ACO and real valued GA in multiple response optimization (MRO) problem. Multiple regression analysis and a `maximin' desirability function are used to reduce the dimensionality and solve an MRO problem. The overall results indicate suitability of ACO-GA strategy for both single and multiple response optimization problems.","","978-1-4673-4617-7978-1-4673-4615-3978-1-4673-4616","10.1109/CHUSER.2012.6504308","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6504308","Ant Colony Optimization;Desirability Function;Genetic Algorithm;Multiple Response Optimization","","ant colony optimisation;genetic algorithms;minimax techniques;regression analysis","hybrid GA-based ant colony strategy;genetic algorithm;continuous correlated multiple response optimization problem;search strategy;MRO problem;multiple regression analysis;maximin desirability function;ACO-GA strategy","","","16","","","","","","IEEE","IEEE Conferences"
"Modeling and Evaluation of Control Flow Vulnerability in the Embedded System","M. A. Rouf; S. Kim","NA; NA","2010 IEEE International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems","","2010","","","430","433","Faults in control flow-changing instructions are critical for correct execution because the faults could change the behavior of programs very differently from what they are expected to show. The conventional techniques to deal with control flow vulnerability typically add extra instructions to detect control flow-related faults, which increase both static and dynamic instructions, consequently, execution time and energy consumption. In contrast, we make our own control flow vulnerability model to evaluate the effects of different compiler optimizations. We find that different programs show very different degrees of control flow vulnerabilities and some compiler optimizations have high correlation to control flow vulnerability. The results observed in this work can be used to generate more resilient code against control flow-related faults.","2375-0227;1526-7539","978-1-4244-8181","10.1109/MASCOTS.2010.71","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5581492","control flow error;architectural vulnerbility factor;transient fault","Optimization;Benchmark testing;Mathematical model;Circuit faults;Registers;Program processors;Computational modeling","embedded systems;optimising compilers;program verification;software fault tolerance","control flow vulnerability;embedded system;control flow-changing instruction fault;correct execution;program behavior;control flow-related fault detection;compiler optimization","","3","12","","","","","","IEEE","IEEE Conferences"
"Implementation of DG for loss minimization and voltage profile in distribution system","S. R. A. Rahim; T. K. A. Rahman; I. Musirin; M. H. Hussain; M. H. Sulaiman; O. Aliman; Z. M. Isa","NA; NA; NA; NA; NA; NA; NA","2010 4th International Power Engineering and Optimization Conference (PEOCO)","","2010","","","490","494","This paper presents the effects of DG on the performance of an existing distribution network in terms of voltage stability, loss minimization and voltage profile. In this study, a new program was developed based on Artificial Immune System optimization technique in order to determine the optimal size of distributed generator. Various loading conditions were tested in order to evaluate the effectiveness of the proposed technique in determining the optimal size of the distributed generator. The suitable location of distributed generator is identified based on the results from voltage stability index. The proposed technique was tested on IEEE Reliability Test systems namely the IEEE 69-bus and the program was developed using the MATLAB programming software.","","978-1-4244-7128-7978-1-4244-7127-0978-1-4244-7126","10.1109/PEOCO.2010.5559251","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5559251","Distributed Generation (DG);Artificial Immune System (AIS);Voltage Stability Index","Distributed power generation;Generators;Stability criteria;Optimization;Indexes;Power system stability","artificial immune systems;distributed power generation;power distribution reliability","DG;loss minimization;power distribution system;distribution network;artificial immune system optimization technique;voltage stability index;IEEE reliability test systems","","6","16","","","","","","IEEE","IEEE Conferences"
"Decision ordering based property decomposition for functional test generation","M. Chen; P. Mishra","Software Engineering Institute, East China Normal University, China; CISE Department, University of Florida, USA","2011 Design, Automation & Test in Europe","","2011","","","1","6","SAT-based BMC is promising for directed test generation since it can locate the reason of an error within a small bound. However, due to the state space explosion problem, BMC cannot handle complex designs and properties. Although various optimization methods are proposed to address a single complex property, the test generation process cannot be fully automated. This paper presents an efficient automated approach that can scale down the falsification complexity using property decomposition and learning techniques. Our experimental results using both software and hardware benchmarks demonstrate that our approach can drastically reduce the overall test generation effort.","1558-1101;1530-1591;1530-1591","978-3-9810801-8-6978-1-61284-208-0978-3-9810801-7","10.1109/DATE.2011.5763037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5763037","","Algorithm design and analysis;Equations;Clocks;Clustering algorithms;Complexity theory;Delay;Arrays","automatic test pattern generation;computability;electronic engineering computing;formal verification","decision ordering based property decomposition;functional test generation;SAT-based BMC;state space explosion problem;falsification complexity;learning technique;satisfiability;bounded model checking","","1","","","","","","","IEEE","IEEE Conferences"
"Efficient Fault Detection and Diagnosis in Complex Software Systems with Information-Theoretic Monitoring","M. Jiang; M. A. Munawar; T. Reidemeister; P. A. S. Ward","University of Waterloo, Waterloo; University of Waterloo, Waterloo; University of Waterloo, Waterloo; University of Waterloo, Waterloo","IEEE Transactions on Dependable and Secure Computing","","2011","8","4","510","522","Management metrics of complex software systems exhibit stable correlations which can enable fault detection and diagnosis. Current approaches use specific analytic forms, typically linear, for modeling correlations. In practice, more complex nonlinear relationships exist between metrics. Moreover, most intermetric correlations form clusters rather than simple pairwise correlations. These clusters provide additional information and offer the possibility for optimization. In this paper, we address these issues by using Normalized Mutual Information (NMI) as a similarity measure to identify clusters of correlated metrics, without assuming any specific form for the metric relationships. We show how to apply the Wilcoxon Rank-Sum test on the entropy measures to detect errors in the system. We also present three diagnosis algorithms to locate faulty components: RatioScore, based on the Jaccard coefficient, SigScore, which incorporates knowledge of component dependencies, and BayesianScore, which uses Bayesian inference to assign a fault probability to each component. We evaluate our approach in the context of a complex enterprise application, and show that 1) stable, nonlinear correlations exist and can be captured with our approach; 2) we can detect a large fraction of faults with a low false positive rate (we detect up to 18 of the 22 faults we injected); and 3) we improve the diagnosis with our new diagnosis algorithms.","1545-5971;1941-0018;2160-9209","","10.1109/TDSC.2011.16","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5714701","Self-managing systems;fault detection;fault diagnosis;information theory;mutual information;autonomic systems.","Measurement;Entropy;Monitoring;Correlation;Random variables;Computational modeling;Uncertainty","entropy;fault location;fault tolerant computing;program testing;software metrics","fault detection;fault diagnosis;complex software system;information-theoretic monitoring;management metrics;intermetric correlation;normalized mutual information;similarity measure;Wilcoxon Rank-Sum test;entropy measure;faulty component;RatioScore;Jaccard coefficient;SigScore;omponent dependency;BayesianScore;Bayesian inference;fault probability;complex enterprise application;diagnosis algorithm","","13","20","","","","","","IEEE","IEEE Journals & Magazines"
"The research and realization of multi-threaded intelligent test paper generation based on genetic algorithm","Y. Shan","School of Computer Science and Software Engineering, Tianjin Polytechnic University, Tianjin, P. R. China","2010 International Conference on Computer and Information Application","","2010","","","461","464","In this paper, a new multi-threaded intelligent strategy of generating test paper based on genetic algorithm is proposed to replace the current test paper generation algorithms that have the disadvantages of lower power, longer time and difficulty in fulfilling the requirements. This algorithm is compared in a number of experiments with the series genetic algorithm and proved to be faster and better. It can be applied in the situations of practical test paper generation.","","978-1-4244-8598-7978-1-4244-8597","10.1109/ICCIA.2010.6141636","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6141636","genetic algorithm;multi-thread;mathematic model;multi-objective optimization","Genetic algorithms;Instruction sets;Computers;Databases;Parallel algorithms;Algebra;Artificial intelligence","computer aided instruction;genetic algorithms;multi-threading","multithreaded intelligent test paper generation;genetic algorithm","","1","7","","","","","","IEEE","IEEE Conferences"
"IGBT technology design and device optimization","A. Artamonov; V. Nelayev; I. Shelibak; A. Turtsevich","Belarusian State University of Informatics and Radioelectronics, Minsk, Belarus; Belarusian State University of Informatics and Radioelectronics, Minsk, Belarus; Belarusian State University of Informatics and Radioelectronics, Minsk, Belarus; Joint-Stock Company “Integral”, Minsk, Belarus","2011 9th East-West Design & Test Symposium (EWDTS)","","2011","","","233","236","Power semiconductor devices are important microelectronic components determined by the efficiency, size, and cost of electronic systems for energy application. Insulated Gate Bipolar Transistor (IGBT) is popular device from series of microelectronics elements base for power energetic applications. Exact design of the modern element base for microelectronics provides reliable operation of the system. The paper presents and discusses the results of IGBT manufacturing technology and device design. These results were obtained by means of Silvaco software package intended for technology/device simulation.","","978-1-4577-1958-5978-1-4577-1957-8978-1-4577-1956","10.1109/EWDTS.2011.6116415","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6116415","","Insulated gate bipolar transistors;Boron;Annealing;Manufacturing;Transistors;P-n junctions;Logic gates","insulated gate bipolar transistors;power bipolar transistors;semiconductor device manufacture","IGBT technology design;device optimization;power semiconductor devices;microelectronic components;electronic systems;insulated gate bipolar transistor;IGBT manufacturing technology;Silvaco software package","","","3","","","","","","IEEE","IEEE Conferences"
"The Snap Framework: A Web Toolkit for Haskell","G. Collins; D. Beardsley","Google Switzerland; Karamaan Group","IEEE Internet Computing","","2011","15","1","84","87","Haskell is an advanced functional programming language. The product of more than 20 years of research, it enables rapid development of robust, concise, and fast software. Haskell supports integration with other languages and has loads of built-in concurrency, parallelism primitives, and rich libraries. With its state-of-the-art testing tools and an active community, Haskell makes it easier to produce flexible, maintainable, high-quality software. The most popular Haskell implementation is the Glasgow Haskell Compiler (GHC), a highperformance optimizing native-code compiler. Here, we look at Snap, a Web-development framework for Haskell. Snap combines many other Web-development environments' best features: writing Web code in an expressive highlevel language, a rapid development cycle, fast performance for native code, and easy deployment in production.","1089-7801;1941-0131","","10.1109/MIC.2011.21","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676171","Haskell;Snap framework;HTTP;functional programming;Internet","Benchmark testing;Libraries;Programming;Indexes;Servers;Computer languages","functional languages;Internet;program compilers;program testing;software maintenance;software tools","Snap framework;Web toolkit;Haskell;advanced functional programming language;testing tools;Glasgow Haskell compiler;Web code;native-code compiler","","1","","","","","","","IEEE","IEEE Journals & Magazines"
"An Exploration Technique for the Ant Colony System Optimization Framework","Z. Lv; Y. Chen","NA; NA","2011 International Conference on Control, Automation and Systems Engineering (CASE)","","2011","","","1","4","Traveling salesman problem (TSP) is a well known combinatorial optimization problems. Ant Colony Algorithm (AC) is new Heuristic Optimization Algorithm. It is widely used in TSP. Ant Colony System (ACS) is the improvement of AC, but it is still not enough being perfect, WMASC follows this approach and tries to improve the performance of ACS algorithm by improving multiple ants and the worst ant which are allowed to updated global pheromone, adjusting the value of the parameter. The tests show that WMACS is better than ACS.","","978-1-4577-0860-2978-1-4577-0859-6978-1-4577-0858","10.1109/ICCASE.2011.5997576","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5997576","","Partitioning algorithms;Optimization;Heuristic algorithms;Convergence;Evolutionary computation;Cities and towns;Software algorithms","minimax techniques;travelling salesman problems","exploration technique;ant colony system optimization;traveling salesman problem;TSP;combinatorial optimization problem;heuristic optimization algorithm;ACS;WMASC;global pheromone;max-min ant system","","","9","","","","","","IEEE","IEEE Conferences"
"Impact Analysis in the Presence of Dependence Clusters Using Static Execute after in WebKit","L. Schrettner; J. Jász; T. Gergely; Á. Beszédes; T. Gyimóthy","NA; NA; NA; NA; NA","2012 IEEE 12th International Working Conference on Source Code Analysis and Manipulation","","2012","","","24","33","Impact analysis based on code dependence can be an integral part of software quality assurance by providing opportunities to identify those parts of the software system that are affected by a change. Because changes usually have far reaching effects in programs, effective and efficient impact analysis is vital, which has different applications including change propagation and regression testing. Static Execute After (SEA) is a relation on program elements (procedures) that is efficiently computable and accurate enough to be a candidate for use in impact analysis in practice. To assess the applicability of SEA in terms of capturing real defects, we present results on integrating it into the build system of Web Kit, a large, open source software system, and on related experiments. We show that a large number of real defects can be captured by impact sets computed by SEA, albeit many of them are large. We demonstrate that this is not an issue in applying it to regression test prioritization, but generally it can be an obstacle in the path to efficient use of impact analysis. We believe that the main reason for large impact sets is the formation of dependence clusters in code. As apparently dependence clusters cannot be easily avoided in the majority of cases, we focus on determining the effects these clusters have on impact analysis.","","978-0-7695-4783-1978-1-4673-2398","10.1109/SCAM.2012.22","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6392099","Change impact analysis;Source code analysis;Static Execute After;Regression testing;Dependence clusters","Testing;Algorithm design and analysis;Software systems;Software algorithms;Sea measurements;Approximation methods","Internet;software quality","impact analysis;dependence clusters;WebKit;code dependence;software quality assurance;software system;static execute after;SEA","","3","29","","","","","","IEEE","IEEE Conferences"
"Development of risk-based operation and maintenance management system for nuclear plants","M. Agarwal; P. V. Yarde","Reactor Group, Bhabha Atomic Research Centre, Mumbai 400085, India; Reactor Group, Bhabha Atomic Research Centre, Mumbai 400085, India","2010 2nd International Conference on Reliability, Safety and Hazard - Risk-Based Technologies and Physics-of-Failure Methods (ICRESH)","","2010","","","485","488","Traditional approach in Decision-Making in nuclear plants & regulatory bodies is based on engineering judgment, O&M experience & technical specifications. However, this approach is conservative and at times tends to be arbitrary in nature. This is because these are prescript by the designers or regulators. So, no quantified inputs go to decision making criteria for plant management. The uncertainties associated with data or model is also not quantified. The state of art in “PSA-An analytical approach” enables to represent performance parameters in form of reliability and safety. This approach provides probabilistic, systematic, flexible and rational framework for optimization of parameters in decision making also gives integrated plant performance view for safety performance. The outcome of this approach is used for several applications in decision making. This paper discusses the development of risk-based operation and maintenance management system for nuclear plants / industries. This system is developed for the ease and effective decision making by the O&M agencies, considering risk contributed by each component and system. This tool is having user friendly GUI for plant operators with almost a all important elements for risk-based management such as CDF calculation, risk profile graph, system unavailability and IE contribution to CDF, importance analysis, uncertainty analysis, login sessions, compare risk-based surveillance test interval to traditional surveillance test interval, and scope for technical specifications, this system also facilitates the shutdown maintenance planning and scheduling. For validation and verification PSA model of research reactors is developed and system is implemented. And most of the results are comparable with professional software available in market. This paper underlies goal and acceptance criteria for judgment and correctness of data and model.","","978-1-4244-8343-3978-1-4244-8344","10.1109/ICRESH.2010.5779598","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5779598","Probabilistic Safety Assessment;Core damage frequency;Risk management;importance analysis;uncertainty analysis;component database;surveillance test;maintenance scheduling","Accidents;Inductors;Sensitivity;Reliability engineering","decision making;fission reactor safety;fission research reactors;graphical user interfaces;nuclear engineering computing;nuclear power stations;optimisation;risk management;surveillance","nuclear plants;regulatory bodies;decision making criteria;plant management;PSA-an analytical approach;optimization;integrated plant performance;safety performance;risk-based operation;maintenance management system;GUI;plant operators;risk-based management;CDF calculation;risk profile graph;system unavailability;IE contribution;importance analysis;uncertainty analysis;login sessions;risk-based surveillance test interval;shutdown maintenance planning;research reactors;professional software;shutdown maintenance scheduling;PSA model;engineering judgment;O&M agencies","","","3","","","","","","IEEE","IEEE Conferences"
"Design of a real-time software-based GPS baseband receiver using GPU acceleration","Jyun-Cheng Wu; Lei Chen; Tzi-Dar Chiueh","Graduate Institute of Electronics Engineering, National Taiwan University, Taipei, Taiwan; Graduate Institute of Electronics Engineering, National Taiwan University, Taipei, Taiwan; Graduate Institute of Electronics Engineering, National Taiwan University, Taipei, Taiwan","Proceedings of Technical Program of 2012 VLSI Design, Automation and Test","","2012","","","1","4","In this study, we designed an energy-optimized real-time GPS software receiver that runs on desktop and server platforms. The proposed GPS software receiver moves most time-consuming operations to the GPU and achieves considerable increase in performance as well as substantial reduction in total energy consumption. The proposed receiver achieves a 7.5X speedup from the original CPU program. Furthermore, it reduces approximately 65% energy consumption when compared to a software receiver without GPU acceleration.","","978-1-4577-2081-9978-1-4577-2080-2978-1-4577-2079","10.1109/VLSI-DAT.2012.6212617","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6212617","GPS;software receiver;CUDA;parallel programming","Receivers;Graphics processing unit;Global Positioning System;Satellites;Tracking loops;Instruction sets","Global Positioning System;graphics processing units;parallel architectures;radio receivers;telecommunication computing","real-time software-based GPS baseband receiver;GPU acceleration;energy-optimized real-time GPS software receiver;desktop platforms;server platforms;substantial total energy consumption reduction;CPU program;CUDA;compute unified device architecture;general purpose parallel computing architecture","","","12","","","","","","IEEE","IEEE Conferences"
"Combining Static and Dynamic Analysis to Discover Software Vulnerabilities","R. Zhang; S. Huang; Z. Qi; H. Guan","NA; NA; NA; NA","2011 Fifth International Conference on Innovative Mobile and Internet Services in Ubiquitous Computing","","2011","","","175","181","Dynamic taint analysis has been proved to be very effective in solving security problems recently, especially in software vulnerability detection and malicious behavior prevention. Unfortunately, most of current researches in this field focus on the runtime protection, and are incapable to discover the potential threat in the software. This paper describes a novel approach to overcome the limitation of traditional dynamic taint analysis by integrating static analysis into the system and presents framework SDCF. The framework translates the binary into assembly code and tracks the data flow. Then with static method, the system can get the important information which can't be gained at runtime, such as unexecuted part of the code. When this information is acquired, they will be provided to the client tools. The practicability of the framework is validated by implementing and evaluating a tool built on SDCF. The result of the experiments shows that our system is able to detect latent software vulnerabilities efficiently.","","978-1-61284-733-7978-0-7695-4372","10.1109/IMIS.2011.59","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5976182","Taint Analysis;Software Vulnerability;Code Coverage;Data Flow Analysis","Software;Registers;Runtime;Performance analysis;Monitoring;Testing;Optimization","program diagnostics;security of data","static analysis;dynamic analysis;software vulnerabilities;dynamic taint analysis;security problems;software vulnerability detection;malicious behavior prevention;runtime protection;assembly code","","3","21","","","","","","IEEE","IEEE Conferences"
"The optimization techniques for time synchronization based on NTP","P. Ruiqing; H. Peng; Y. Wenxue; G. Min; Z. Bin","College of Electrical Engineering &amp; Information Science, Three Gorges Univ., Yichang Hubei, China; College of Electrical Engineering &amp; Information Science, Three Gorges Univ., Yichang Hubei, China; College of Electrical Engineering &amp; Information Science, Three Gorges Univ., Yichang Hubei, China; Information Technology Center, Three Gorges Univ., Yichang Hubei, China; Dept. of computer science, JiNing Univ., ShanDong, China","2010 2nd International Conference on Future Computer and Communication","","2010","2","","V2-296","V2-299","In this paper, a high-precision time synchronization technique is presented to solve the problems that the client clock is difficult to meet the accuracy of the time synchronization algorithm and the shortage of measuring network delay that produced when data packets of time synchronization are transmitted in the network. So a modified client clock is constructed to improve the accuracy of the clock. For this premise, the technique of Data Packet Selection is proposed, which is a method to determine whether to transfer by comparing the queue length of the router table with a specified value in the packet. By using this technique, the client clock precision can be raised to microsecond level, which can meet the time synchronization needs of NTP. Meanwhile, the optimized NTP time synchronization algorithm can also significantly reduce the errors that caused by the network delay. The emulation tests show this synchronization technique achieves the standard that the maximal offset is less than 10 milliseconds and the average of offset is less than 4 milliseconds as well as the average of standard deviation is less than 1 milliseconds.","","978-1-4244-5824-0978-1-4244-5821-9978-1-4244-5823","10.1109/ICFCC.2010.5497413","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5497413","data packet selection(DPS);time synchronization algorithm;clock accuracy;network time protocol(NTP)","Clocks;Synchronization;Delay effects;Hardware;Network servers;Oscillators;Registers;Operating systems;Application software;Helium","clocks;optimisation;protocols;synchronisation;telecommunication network routing","optimization techniques;high-precision time synchronization technique;network delay;data packets;data packet selection technique;modified client clock;router table;queue length;optimized NTP time synchronization algorithm;network time protocol","","2","6","","","","","","IEEE","IEEE Conferences"
"Hardware Particle Swarm Optimization with passive congregation for embedded applications","D. M. Muñoz; C. H. Llanos; L. d. S. Coelho; M. Ayala-Rincón","Department of Mechanical Engineering, Automation and Control Group/GRACO, University of Brasilia, Brazil; Department of Mechanical Engineering, Automation and Control Group/GRACO, University of Brasilia, Brazil; Industrial and Systems Engineering, Graduate Program, LAS/PPGPES, Pontifical Catholic University of Parana, Curitiba, Brazil; Department of Computer Science and Mathematics, University of Brasilia, Brazil","2011 VII Southern Conference on Programmable Logic (SPL)","","2011","","","173","178","Achieving high performance optimization algorithms for embedded applications can be very challenging, particularly when several requirements such as high accuracy computations, short elapsed time, area cost, low power consumption and portability must be accomplished. This paper proposes a hardware implementation of the Particle Swarm Optimization algorithm with passive congregation (HPPSOpc), which was developed using several floating-point arithmetic libraries. The passive congregation is a biological behavior which allows the swarm to preserve its integrity, balancing between global and local search. The HPPSOpc architecture was implemented on a Virtex5 FPGA device and validated using two multimodal benchmark problems. Synthesis, simulation and execution time results demonstrates that the passive congregation approach is a low cost solution for solving embedded optimization problems with a high performance.","","978-1-4244-8848-3978-1-4244-8847-6978-1-4244-8846","10.1109/SPL.2011.5782644","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5782644","","Hardware;Computer architecture;Software;Field programmable gate arrays;Benchmark testing;Optimization;Software algorithms","field programmable gate arrays;floating point arithmetic;particle swarm optimisation","hardware particle swarm optimization;passive congregation;embedded applications;floating-point arithmetic libraries;Virtex5 FPGA device","","3","19","","","","","","IEEE","IEEE Conferences"
"The optimized design of surveillance monitor system based on WLAN and 4G technologies","X. Guo; H. Xue; R. Bai","Metrology and Testing Center, China Academy of Engineering Physics, Mianyang Sichuan 621900, China; Metrology and Testing Center, China Academy of Engineering Physics, Mianyang Sichuan 621900, China; Metrology and Testing Center, China Academy of Engineering Physics, Mianyang Sichuan 621900, China","2010 Global Mobile Congress","","2010","","","1","3","The paper analyzed the update developing orientation of surveillance monitor system on WLAN. And a new surveillance monitor system based on 4G system is designed and exploited on the base of digital video frequency, image compression, wireless communication, together with remote control over a long-distance technology. And also, the application field, requirement of hardware and software, together with relative characteristics of system are given in details.","","978-1-4244-9003-5978-1-4244-9001-1978-1-4244-9002","10.1109/GMC.2010.5634600","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5634600","Surveillance Monitor System;WLAN;4G;Digital Video Frequency;Wireless Communication","Surveillance;Traffic control;Cameras;Wireless communication;Wireless LAN","4G mobile communication;video surveillance;wireless LAN","optimized design;surveillance monitor system;WLAN;4G technologies;digital video frequency;image compression;wireless communication","","","3","","","","","","IEEE","IEEE Conferences"
"Optimal fluctuations for satisfactory performance under parameter uncertainty","H. Kadim","LJMU, England, UK","2011 9th East-West Design & Test Symposium (EWDTS)","","2011","","","259","263","Maintaining constant performance in the presence of a set of changes in parameters and unwarranted events has become an essential aspect of present system designs. Knowing a predefined upper limit, for which a drop in performance is said to be satisfactory, enables autonomous systems to perform a control action to mitigate changes that violate such a predefined limit. This paper introduces an analytical model for optimisation of the maximum possible parameter fluctuations that permit robust operation.","","978-1-4577-1958-5978-1-4577-1957-8978-1-4577-1956","10.1109/EWDTS.2011.6116590","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6116590","","Fluctuations;Transfer functions;Mathematical model;Analytical models;Adaptive systems;Biological system modeling;Adaptation models","optimisation;software performance evaluation","optimal fluctuations;satisfactory performance;parameter uncertainty;system designs;autonomous systems;optimisation","","","10","","","","","","IEEE","IEEE Conferences"
"Profiling Software for Energy Consumption","S. Schubert; D. Kostic; W. Zwaenepoel; K. G. Shin","NA; NA; NA; NA","2012 IEEE International Conference on Green Computing and Communications","","2012","","","515","522","The amount of energy consumed by computer systems can be lowered through the use of more efficient algorithms and software. Unfortunately, software developers lack the tools to pinpoint energy-hungry sections in their code and therefore have to rely on their intuition when trying to optimize their code for energy consumption. We have developed eprof, a profiler that relates energy consumption to code locations, it attributes both the synchronously consumed energy in the CPU and the asynchronously consumed energy in peripheral devices like hard drives, network cards, etc. Eprof requires minimal changes to the kernel (tens of lines of code) and does not require special hardware to energy-profile software. Therefore eprof can be widely used to help developers make energy-aware decisions.","","978-1-4673-5146-1978-0-7695-4865","10.1109/GreenCom.2012.86","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6468359","Energy consumption by software;energy profiling","Energy consumption;Kernel;Hardware;Radiation detectors;Energy measurement;Benchmark testing","decision making;power aware computing;software engineering","software profiling;computer system energy consumption;software developers;energy-hungry sections;code locations;peripheral devices;hard drives;network cards;Eprof;energy-profile software;energy-aware decision making","","12","23","","","","","","IEEE","IEEE Conferences"
"An edge crossing minimization algorithm based on adjacency matrix transformation","Y. Zhang; Hao Chen; Deng-xin Hua; Ying-an Cui; Bao-wei Zhang","School of Computer Science &amp; Engineering, XAUT, Xi'an, China; School of Computer Science &amp; Engineering, XAUT, Xi'an, China; School of Computer Science &amp; Engineering, XAUT, Xi'an, China; School of Computer Science &amp; Engineering, XAUT, Xi'an, China; School of Computer Science &amp; Engineering, XAUT, Xi'an, China","2010 3rd International Conference on Advanced Computer Theory and Engineering(ICACTE)","","2010","1","","V1-672","V1-675","In software testing and maintenance, how to obtain useful information and relationships from large amounts of information quickly becomes an urgent problem which is required to be solved. Visualization technology is one of the effective tools. The tree structure of hierarchical graph can effectively display hierarchical structure of software testing and maintenance information, and which can help software maintainers to understand the software. Minimizing edge crossing is directly involved with the readability of the graph in drawing hierarchical graph. An optimization is given for minimizing edge crossing problem based on characteristics of adjacency matrix. The representation and transformation methods of hierarchical graph's adjacency matrix are given. Using the algorithm we can achieve the aims of optimization for edge crossing minimization during experimental results.","2154-7505;2154-7491;2154-7491","978-1-4244-6542-2978-1-4244-6539-2978-1-4244-6541","10.1109/ICACTE.2010.5578930","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5578930","Hierarchical Graph;Adjacency Matrix;Crossing Minimization","Pipelines;Minimization","data visualisation;matrix algebra;minimisation;program testing;software maintenance;trees (mathematics)","edge crossing minimization algorithm;adjacency matrix transformation;software testing;software maintenance;visualization technology;hierarchical graph structure;tree structure;graph representation method;graph transformation method","","1","8","","","","","","IEEE","IEEE Conferences"
"Study of Variations of Native Program Execution Times on Multi-Core Architectures","A. Mazouz; S. Touati; D. Barthou","NA; NA; NA","2010 International Conference on Complex, Intelligent and Software Intensive Systems","","2010","","","919","924","Program performance optimisations, feedback-directed iterative compilation and auto-tuning systems all assume a fixed estimation of execution time given a fixed input data for the program. However, in practice we observe non-negligible program performance variations on hardware platforms. While these variations are insignificant for sequential applications, we show that parallel native OpenMP programs have less performance stability. This article does not try to quantify nor to qualify the factors influencing the variations of program execution times, that we let for a future work. This article demonstrates three observations: 1) The performance variations of sequential applications is insignificant. 2) OpenMP program execution times on multi-core platforms show important variations. 3) The distribution of the execution times is not a Gaussian distribution in almost all cases. We finish by a discussion explaining why considering the minimal or the mean execution time within a sample of experiments is not the best estimation of program performance.","","978-1-4244-5918-6978-1-4244-5917-9978-0-7695-3967","10.1109/CISIS.2010.96","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5447480","Experimentation;Measurement;Statistical Performance Evaluation","Multicore processing;Application software;Optimization;Predictive models;Software measurement;Benchmark testing;Parallel programming;Competitive intelligence;Software systems","Gaussian distribution;message passing;multiprocessing systems;open systems;optimising compilers;parallel architectures;parallel programming","multicore architectures;program performance optimisations;feedback-directed iterative compilation;auto-tuning system;nonnegligible program performance variations;sequential applications;parallel native OpenMP program;Gaussian distribution","","7","7","","","","","","IEEE","IEEE Conferences"
"Development and application of online energy analysis system based on software integration and communications","Li yugang; Liu Huanbin; Li Jigeng; Zhou Yanming; Wu Bo","State Key Lab of Pulp and Paper Engineering, South China University of Technology, Guangzhou, China, 510640; State Key Lab of Pulp and Paper Engineering, South China University of Technology, Guangzhou, China, 510640; State Key Lab of Pulp and Paper Engineering, South China University of Technology, Guangzhou, China, 510640; State Key Lab of Pulp and Paper Engineering, South China University of Technology, Guangzhou, China, 510640; State Key Lab of Pulp and Paper Engineering, South China University of Technology, Guangzhou, China, 510640","2010 3rd International Conference on Computer Science and Information Technology","","2010","5","","250","254","An online energy analysis system for paper machine dryer section is developed. The data and information share is realized in different types of database. Supervisory and analysis software plays an important role in the system, such as iFIX of GE and eTMS. the techniques of gathering and analyzing the real time data from the paper machine dryer section and communication between different software are also introduced. The running result of the test shows that the system has a good effect for the paper machine optimizing operation, energy-saving, and consumption-reducing.","","978-1-4244-5540-9978-1-4244-5537-9978-1-4244-5539","10.1109/ICCSIT.2010.5563768","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5563768","online energy analysis system;real-time data acquisition and transmission;dryer section;database","Printers;Software design;Computers;Industries","database management systems;drying;energy conservation;paper making machines;power engineering computing;production engineering computing","online energy analysis system;software integration;software communications;paper machine dryer section;database;iFIX;eTMS;GE;paper machine optimizing operation","","","9","","","","","","IEEE","IEEE Conferences"
"Performance evaluation of adaptivity in software transactional memory","M. Payer; T. R. Gross","ETH Zurich, Switzerland; ETH Zurich, Switzerland","(IEEE ISPASS) IEEE International Symposium on Performance Analysis of Systems and Software","","2011","","","165","174","Transactional memory (TM) is an attractive platform for parallel programs, and several software transactional memory (STM) designs have been presented. We explore and analyze several optimization opportunities to adapt STM parameters to a running program. This paper uses adaptSTM, a flexible STM library with a non-adaptive baseline common to current fast STM libraries to evaluate different performance options. The baseline is extended by an online evaluation system that enables the measurement of key runtime parameters like read- and write-locations, or commit- and abort-rate. The performance data is used by a thread-local adaptation system to tune the STM configuration. The system adapts different important parameters like write-set hash-size, hash-function, and write strategy based on runtime statistics on a per-thread basis. We discuss different self-adapting parameters, especially their performance implications and the resulting trade-offs. Measurements show that local per-thread adaptation outperforms global system-wide adaptation. We position local adaptivity as an extension to existing systems. Using the STAMP benchmarks, we compare adaptSTM to two other STM libraries, TL2 and tinySTM. Comparing adaptSTM and the adaptation system to TL2 results in an average speedup of 43% for 8 threads and 137% for 16 threads. adaptSTM offers performance that is competitive with tinySTM for low-contention benchmarks; for high-contention benchmarks adaptSTM outperforms tinySTM. Thread-local adaptation alone increases performance on average by 4.3% for 16 threads, and up to 10% for individual benchmarks, compared to adaptSTM without active adaptation.","","978-1-61284-368-1978-1-61284-367","10.1109/ISPASS.2011.5762733","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5762733","","Benchmark testing;Adaptive systems;Optimization;Libraries;Runtime;Synchronization;Bioinformatics","concurrency control;multi-threading;software performance evaluation;transaction processing","performance evaluation;software transactional memory;STM library;thread-local adaptation system;write-set hash-size;hash-function;write strategy;STAMP benchmarks;daptSTM","","1","30","","","","","","IEEE","IEEE Conferences"
"Optimization of an isotropic etching process on silicon wafers","R. Dolah; H. Musa","RAZAK School of Engineering and Advanced Technology, Universiti Teknologi Malaysia International Campus, 54100 Kuala Lumpur, Malaysia; RAZAK School of Engineering and Advanced Technology, Universiti Teknologi Malaysia International Campus, 54100 Kuala Lumpur, Malaysia","2010 34th IEEE/CPMT International Electronic Manufacturing Technology Symposium (IEMT)","","2010","","","1","5","Etching process involves various chemical reactions and reflects significantly on silicon wafer quality. Etching parameters are evaluated in order to optimize the isotropic etching process. For optimization purpose, Design of Experiment (DOE) with full factorial design is employed. Etching factors namely the bubbling flow rate, wafer rotation, and etchant temperature are randomised with additional three centre points to observe any curvature. The responses studied are etching removal, total thickness variation (TTV) and wafer brightness. It is found that etchant temperature gives major impact on all three responses stated above. The etchant temperature is the main effect factor and significantly affects TTV. Additionally, the etchant temperature and bubbling flow rate provide interaction effects on both the etching removal and wafer brightness. A higher bubbling flow rate is required to ensure etching removal and brightness within specification. Besides studying these three responses, the wafer surface after etching is additionally analysed using ADE Infotool software which captures the etched profile and its thickness. The ADE result indicates that a higher temperature contributes to a more concave shape of etched wafer, thus resulting in higher TTV and sending the wafers out of specification. Finally, the optimum condition is tested on a final run. The removal uniformity is observed in removal distribution graphs. The etching performance is enhanced with the optimized value of bubbling flow rate, etchant temperature and wafer rotation to achieve the best removal distribution.","1089-8190;1089-8190","978-1-4244-8827-8978-1-4244-8825-4978-1-4244-8826","10.1109/IEMT.2010.5746677","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5746677","Design of Experiment;isotropic etching;silicon wafer","Temperature distribution;Brightness;Response surface methodology;Etching;Rotation measurement","design of experiments;etching;optimisation;wafer level packaging","optimization;isotropic etching;silicon wafers;chemical reactions;design of experiment;bubbling flow rate;wafer rotation;etchant temperature;etching removal;total thickness variation;wafer brightness;ADE Infotool software;removal distribution graphs","","","4","","","","","","IEEE","IEEE Conferences"
"The improvement of RTAI scheduler based on CPSS algorithm","L. Xueqiao; L. Shuang; C. Yuan","School of Computer and Communication Engineering, Zhengzhou University of Light Industry, Zheng Zhou, China; School of Computer and Communication Engineering, Zhengzhou University of Light Industry, Zheng Zhou, China; School of Computer and Communication Engineering, Zhengzhou University of Light Industry, Zheng Zhou, China","2011 IEEE 3rd International Conference on Communication Software and Networks","","2011","","","1","5","In real-time system, task scheduling policy is the key part of kernel design. How to design task scheduling to ensure that all the tasks will be completed before its deadline is an important problem in the research on real-time operating system. Because the scheduler of RTAI has a bad performance when the system is in heavy load or overload, this article adds a Comprehensive Priority Static Schedule (CPSS) algorithm to RTAI scheduler, and improves and optimizes the RTAI scheduler. Finally, a simulator of the scheduling algorithms and a test of the scheduler are presented. The experiment has proved that improved scheduler can increase the real time Linux system.","","978-1-61284-486-2978-1-61284-485-5978-1-61284-484","10.1109/ICCSN.2011.6014661","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014661","Real-time system;Task Scheduling;RTAI;Scheduler;CPSS algorithm","Scheduling;Artificial intelligence;Computational modeling;Processor scheduling;Concrete","Linux;operating system kernels;optimisation;real-time systems;scheduling;software engineering","CPSS algorithm;task scheduling;kernel design;real-time operating system;comprehensive priority static schedule algorithm;RTAI scheduler optimization;real-time Linux system","","","7","","","","","","IEEE","IEEE Conferences"
"Soft skills recommendation systems for IT jobs: A Bayesian network approach","A. A. Bakar; Choo-Yee Ting","Software Development Lab, MIMOS Berhad, 57000 Bukit Jalil, Malaysia; Faculty of Information Technology, Multimedia University, 63100 Cyberjaya, Malaysia","2011 3rd Conference on Data Mining and Optimization (DMO)","","2011","","","82","87","Today, soft skills are crucial factors to the success of a project. For a certain set of jobs, soft skills are often considered more crucial than the hard skills or technical skills, in order to perform the job effectively. However, it is not a trivial task to identify the appropriate soft skills for each job. In this light, this study proposed a solution to assist employers when preparing advertisement via identification of suitable soft skills together with its relevancy to that particular job title. Bayesian network is employed to solve this problem because it is suitable for reasoning and decision making under uncertainty. The proposed Bayesian Network is trained using a dataset collected via extracting information from advertisements and also through interview sessions with a few identified experts.","2155-6946;2155-6938","978-1-61284-212-7978-1-61284-211","10.1109/DMO.2011.5976509","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5976509","Bayesian Network;Data Mining;Soft Skills","Accuracy;Bayesian methods;Testing;Interviews;Qualifications;Software;Training","Bayes methods;belief networks;business data processing;human resource management;recommender systems","soft skill recommendation system;IT job;Bayesian network approach;hard skill;technical skill;employer assistance;advertisement via identification","","2","9","","","","","","IEEE","IEEE Conferences"
"Solving multiobjective flexible scheduling problem by improved DNA genetic algorithm","J. Li; S. Nie; F. Yang","School of Software Eng., South China University of Technology, Technical Institute of Industry &, Commerce, Guangzhou, China; School of Mechanical and Automotive, Eng., South China University of Technology, Guangzhou, China; School of Software Eng., South China University of Technology, Technical Institute of Industry &, Commerce, Guangzhou, China","2010 2nd International Asia Conference on Informatics in Control, Automation and Robotics (CAR 2010)","","2010","2","","458","461","Build mathematical models for multi-objective flexible scheduling problems, put forward a improved genetic algorithm based on DNA computation, combine it with Pareto non-dominated sorting method to work out multi-objective flexible scheduling optimization problems. In order to ensure the diversity of optimal solution sets, RNA four-digit-system encoder mode and genetic operator based on DNA computation were adopted, designed subs ection crossover and dynamic mutation operation. Through simul ation, test the designed algorithm performance; by comparing with conventional genetic algorithm test results, it proved the efficiency of the algorithm.","1948-3422;1948-3414;1948-3414","978-1-4244-5192-0978-1-4244-5194","10.1109/CAR.2010.5456596","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5456596","DNA computation;RNA computation;improved gene tic algorithm;pareto sorting;multi-objective scheduling","Genetic algorithms;Processor scheduling;DNA computing;Testing;Mathematical model;Sorting;Pareto optimization;RNA;Genetic mutations;Algorithm design and analysis","biocomputing;genetic algorithms;mathematical analysis;Pareto optimisation;scheduling","multiobjective flexible scheduling problem;DNA genetic algorithm;mathematical models;Pareto nondominated sorting method;optimal solution sets;RNA four digit system encoder mode","","","8","","","","","","IEEE","IEEE Conferences"
"Expressway OD Matrix Estimation Based on the Fuzzy Optimization Grey Prediction Model","J. Wang; G. Wei; F. Yang; Q. He","NA; NA; NA; NA","2010 2nd International Workshop on Intelligent Systems and Applications","","2010","","","1","5","PA mathematical prediction model of a large random, complex investigation fundamental data, which based on the traditional land-use patterns, population, income and other variables establish, has many deficiencies. Propose to establish a PA prediction model based on fuzzy optimization grey prediction theory. Adopt the algorithm which is a recursive estimation process time-series. It requires less priori data. And the process of the application software, data query, model calibration and testing of prediction accuracy was built. Based on the statistics of the toll station over the years, combine the result of model predictions to estimate the future for dynamic OD matrix. For a 10 OD pairs of the provincial freeway network, build the experimental platform which makes use of Trans CAD to establish in order to simulate and analyze. A large number of simulation results show that: compared with traditional models, both the relative error and absolute error of this method results are reduced.","","978-1-4244-5874-5978-1-4244-5872","10.1109/IWISA.2010.5473410","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5473410","","Predictive models;Mathematical model;Prediction theory;Recursive estimation;Application software;Calibration;Software testing;Accuracy;Statistics;Traffic control","grey systems;recursive estimation;traffic control","expressway OD matrix estimation;fuzzy optimization grey prediction model;PA prediction model;recursive estimation","","","8","","","","","","IEEE","IEEE Conferences"
"A Speculative HMMER Search Implementation on GPU","X. Li; W. Han; G. Liu; H. An; M. Xu; W. Zhou; Q. Li","NA; NA; NA; NA; NA; NA; NA","2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops & PhD Forum","","2012","","","735","741","Due to the exponentially growing bioinformatics databases and rapidly popular of GPU for general purpose computing, it is promising to employ GPU techniques to accelerate the sequence search process. Hmmsearch from HMMER bioinformatics software package is a wildly used software tool for sensitive profile HMM (Hidden Markov Model) searches of biological sequence databases. In this paper, we implement a speculative hmmsearch implementation on NVIDIA Fermi GPU and apply various optimizations to it. We test the enhancements in our GPU implementation in order to demonstrate the effectiveness of optimization strategies. Result shows that our speculative hmmsearch implementation achieves up to 6.5x speedup over previous fast single-threaded SSE implementation.","","978-1-4673-0974","10.1109/IPDPSW.2012.91","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6270713","HMMER 3.0;GPU;memory optimization;speculative","Graphics processing unit;Hidden Markov models;Optimization;Instruction sets;Databases;Kernel;Registers","bioinformatics;database management systems;graphics processing units;hidden Markov models;software packages;software tools","HMMER search implementation;bioinformatics databases;sequence search process;Hmmsearch;HMMER bioinformatics software package;software tool;sensitive profile HMM;hidden Markov model;biological sequence databases;NVIDIA Fermi GPU;optimization strategies;general purpose computing","","6","23","","","","","","IEEE","IEEE Conferences"
"Optimizing video processing algorithm with multidimensional DMA based on multimedia DSP","S. Pan; Q. Guan; S. Xu","School of Communication and Information Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Communication and Information Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Communication and Information Engineering, University of Electronic Science and Technology of China, Chengdu, China","International Conference on Computational Problem-Solving","","2010","","","362","365","In this paper, the DMA transfer tactics is used to optimize a packed/planar video format conversion program. The program method to employ the DMA transfer in the video processing algorithms will be present in this paper. The objective is to develop the efficiency of the DSP on loading the large image data, which are stored in the external memory. The synchronous problem of the DMA transfer data flow and the processing data flow need to be tackled. The software is executed on multimedia DSP TMS320DM6437 which is based on the TI's new Davinci multimedia technology. All the optimization has been fitted to the resource included in the DSP chip. The optimized packed/planar video format conversion program is tested on the DM6437EVM board with a 594 MHz system clock. The performance of the method is also compared with ordinary algorithm without DMA transfer. Experimental results show that the optimized method is stable, and the performance is better.","","978-981-08-6322-7978-1-4244-8654-0978-981-08-6322","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5696030","","Streaming media;Digital signal processing;Pixel;Software;Optimization;Loading;Instruments","digital signal processing chips;file organisation;video signal processing","video processing;multidimensional DMA;DMA transfer tactics;packed/planar video format conversion program;large image data;external memory;synchronous problem;DMA transfer data flow;multimedia DSP TMS320DM6437;Da Vinci multimedia technology;DSP chip;optimized packed video format conversion program;DM6437EVM board","","","11","","","","","","IEEE","IEEE Conferences"
"A New Efficient Data Structure for Storage and Retrieval of Multiple Biosequences","S. Steinbiss; S. Kurtz","University of Hamburg, Hamburg; University of Hamburg, Hamburg","IEEE/ACM Transactions on Computational Biology and Bioinformatics","","2012","9","2","345","357","Today's genome analysis applications require sequence representations allowing for fast access to their contents while also being memory-efficient enough to facilitate analyses of large-scale data. While a wide variety of sequence representations exist, lack of a generic implementation of efficient sequence storage has led to a plethora of poorly reusable or programming language- specific implementations. We present a novel, space-efficient data structure (GtEncseq) for storing multiple biological sequences of variable alphabet size, with customizable character transformations, wildcard support, and an assortment of internal representations optimized for different distributions of wildcards and sequence lengths. For the human genome (3.1 gigabases, including 237 million wildcard characters) our representation requires only 2 + 8 · 10<sup>-6</sup>bits per character. Implemented in C, our portable software implementation provides a variety of methods for random and sequential access to characters and substrings (including different reading directions) using an object-oriented interface. In addition, it includes access to metadata like sequence descriptions or character distributions. The library is extensible to be used from various scripting languages. GtEncseq is much more versatile than previous solutions, adding features that were previously unavailable. Benchmarks show that it is competitive with respect to space and time requirements.","1545-5963","","10.1109/TCBB.2011.146","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6081847","Data storage representations;biology and genetics;software engineering;reusable libraries.","Particle separators;Bioinformatics;Software;Genomics;Encoding;Libraries;Data structures","authoring languages;benchmark testing;bioinformatics;genetics;genomics;information retrieval;object-oriented programming;optimisation;programming languages;software engineering;spatial data structures","multiple biosequence storage;multiple biosequence retrieval;genome analysis applications;large-scale data;programming language;space-efficient data structure;internal representation optimization;wildcards;portable software implementation;reading directions;object-oriented interface;metadata like sequence descriptions;character distributions;scripting languages;benchmarks","Algorithms;Computational Biology;Databases, Genetic;Information Storage and Retrieval;Models, Genetic;Multigene Family;Sequence Analysis","6","42","","","","","","IEEE","IEEE Journals & Magazines"
"Identifying performance deviations in thread pools","M. D. Syer; B. Adams; A. E. Hassan","Software Analysis and Intelligence Lab (SAIL), School of Computing, Queen's University, Canada; Software Analysis and Intelligence Lab (SAIL), School of Computing, Queen's University, Canada; Software Analysis and Intelligence Lab (SAIL), School of Computing, Queen's University, Canada","2011 27th IEEE International Conference on Software Maintenance (ICSM)","","2011","","","83","92","Large-scale software systems handle increasingly larger workloads by implementing highly concurrent and distributed design patterns. The thread pool pattern uses pools of pre-existing and reusable threads to limit thread lifecycle over-head (thread creation and destruction) and resource thrashing (thread proliferation). However, these advantages are weighed against performance issues caused by concurrency risks, like synchronization errors or deadlock, and thread pool-specific risks, like poorly tuned pool size or thread leakage. Detecting these performance issues during load testing requires a thorough understanding of how thread pools behave, yet most performance analysts have limited knowledge of the system and are flooded with terabytes of data from load tests. We propose a methodology to identify threads with performance deviations in thread pools. Our methodology ranks threads based on the dissimilarity of their resource usage metrics. A case study on a large-scale industrial software system shows that our methodology can identify threads with performance deviations with an average precision of 100% and an average recall of 76.61%. Our methodology performs very well when ranking long-lived deviations, such as memory leaks, but more work is needed to rank short-lived deviations, such as CPU spikes.","1063-6773;1063-6773","978-1-4577-0664-6978-1-4577-0663-9978-1-4577-0662","10.1109/ICSM.2011.6080775","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080775","thread-pools;behaviour-based clustering;understanding ULS systems","Instruction sets;Covariance matrix;Queueing analysis;Synchronization;Educational institutions","electronic commerce;multi-threading;object-oriented methods;pattern clustering;resource allocation;synchronisation","identifying performance deviation;large-scale software system;distributed design pattern;thread pool pattern;thread lifecycle overhead;resource thrashing;concurrency risk;synchronization errors;thread pool-specific risk;thread leakage;performance deviation;large-scale industrial software system","","8","33","","","","","","IEEE","IEEE Conferences"
"Statistical modeling of solar cell using Taguchi method and TCAD tool","M. S. Bahrudin; S. F. Abdullah; I. Ahmad","Center for Micro and Nano Engineering (CeMNE), Department of Electronics and Communication Engineering, College of Engineering, Universiti Tenaga Nasional, Jalan IKRAM-UNITEN, 43000 Kajang, Selangor, Malaysia; Center for Micro and Nano Engineering (CeMNE), Department of Electronics and Communication Engineering, College of Engineering, Universiti Tenaga Nasional, Jalan IKRAM-UNITEN, 43000 Kajang, Selangor, Malaysia; Center for Micro and Nano Engineering (CeMNE), Department of Electronics and Communication Engineering, College of Engineering, Universiti Tenaga Nasional, Jalan IKRAM-UNITEN, 43000 Kajang, Selangor, Malaysia","2012 10th IEEE International Conference on Semiconductor Electronics (ICSE)","","2012","","","1","5","This paper focuses on optimizing silicon based solar cell fabrication using Taguchi Optimization Method (TOM). Optimization focused on 3 parameters namely doping concentration of boron, creating phosphorus PN-junction and energy used for ion-implantation with 2 noise factors, Diffuse time and diffuse temperature. The aim is to have a shallow junction in order to decrease the recombination process but higher fill factor (FF) for better efficiency. Fabricating are done in computer simulation environment by Silvaco TCAD software that also conducting an electrical testing for measurement. Each factor (product from the parameters through TOM) has 2 levels of best values taken from the previous researches. In this research, L<sub>8</sub>orthogonal array consists of 8 set of different combination of experiment has been done. Optimized values are analyzed by finding Signal to Noise Ratio (SNR) of each experiment and applied it on Larger the Better (LTB) for highest FF and Smaller the Better (STB) for shallowest junction depth. Result reveal that boron at concentration of 5.0×10<sup>15</sup>cm<sup>-3</sup>, phosphorus at concentration of 2.0×10<sup>16</sup>cm<sup>-3</sup>, and energy at 10 keV gave a result of 0.3 um ~ 0.5 um for junction depth and stable FF value of 0.8 at any noise factor contributing efficiency of 15% to 16%. As a conclusion, TOM has achieved predicting the best solution for optimizing silicon solar cell fabrication.","","978-1-4673-2396-3978-1-4673-2395-6978-1-4673-2394","10.1109/SMElec.2012.6417073","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6417073","Solar cell;Taguchi Optimization Method;PN-junction;ion-implantation and Silvaco TCAD software","Junctions;Photovoltaic cells;Signal to noise ratio;Silicon;Boron;Arrays;Optimization methods","boron;electronic engineering computing;ion implantation;ion recombination;optimisation;phosphorus;p-n junctions;semiconductor doping;solar cell arrays;statistical analysis;Taguchi methods;technology CAD (electronics)","statistical modeling;TCAD tool;Taguchi optimization method;TOM;doping concentration;phosphorus PN-junction;ion-implantation;noise factors;diffuse temperature;diffuse time;fill factor;ion recombination process;computer simulation environment;Silvaco TCAD software;electrical testing;L8 orthogonal array;signal-to-noise ratio;SNR;shallow junction depth;silicon solar cell fabrication;efficiency 15 percent to 16 percent;B;P","","","12","","","","","","IEEE","IEEE Conferences"
"Design Optimization of Waveguide Filters Using Continuum Design Sensitivity Analysis","N. Choi; D. Kim; G. Jeung; J. Park; J. Byun","<formula formulatype="inline"> <tex Notation="TeX">$^{1}$</tex></formula> The School of Electrical Engineering and Computer Science, Kyungpook National University, Buk-gu, Daegu, Korea; <formula formulatype="inline"><tex Notation="TeX">$^{1}$</tex></formula> School of Electrical Engineering and Computer Science, , Kyungpook National University, , Daegu , Korea; <formula formulatype="inline"> <tex Notation="TeX">$^{1}$</tex></formula> The School of Electrical Engineering and Computer Science, Kyungpook National University, Buk-gu, Daegu, Korea; <formula formulatype="inline"> <tex Notation="TeX">$^{1}$</tex></formula> The School of Electrical Engineering and Computer Science, Kyungpook National University, Buk-gu, Daegu, Korea; <formula formulatype="inline"> <tex Notation="TeX">$^{2}$</tex></formula> School of Electrical Engineering, , Soongsil University, , Seoul, Korea","IEEE Transactions on Magnetics","","2010","46","8","2771","2774","This paper presents a new methodology for design optimization of dielectric waveguide filters based on the continuum design sensitivity analysis in conjunction with standard electromagnetic analysis codes. To achieve this, first, an analytical sensitivity formula in the frequency domain is systematically derived by exploiting the augmented Lagrangian method, material derivative concept and adjoint variable method. Then unified program architecture integrating several engineering software packages into a design tool is proposed for optimum design of high-frequency devices. A 3-D dielectric resonator used in waveguide filters has been tested to prove the validity of the proposed method.","0018-9464;1941-0069","","10.1109/TMAG.2010.2044565","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5512939","Optimization;sensitivity analysis;waveguide filters","Design optimization;Filters;Sensitivity analysis;Electromagnetic waveguides;Code standards;Electromagnetic analysis;Frequency domain analysis;Lagrangian functions;Dielectric materials;Computer architecture","dielectric waveguides;sensitivity analysis;waveguide filters","design optimization;continuum design sensitivity analysis;dielectric waveguide filters;standard electromagnetic analysis codes;analytical sensitivity formula;frequency domain;augmented Lagrangian method;material derivative concept;adjoint variable method;high frequency devices;3D dielectric resonator","","4","10","","","","","","IEEE","IEEE Journals & Magazines"
"Evolutionary generation of test data for multiple paths coverage with faults detection","Y. Zhang; D. Gong","School of Information and Electrical Engineering, China University of Mining and Technology, Xuzhou, Jiangsu, 221116, China; Department of Computer Science and Technology, Mudanjiang Normal University, Heilongjiang, 157012, China","2010 IEEE Fifth International Conference on Bio-Inspired Computing: Theories and Applications (BIC-TA)","","2010","","","406","410","The aim of software testing is to find faults in the program under test. Generating test data which can reveal faults is the core issue. Although existing methods of path-oriented testing can generate test data which traverse target paths, they cannot guarantee that the data find the faults in the program. In this paper, we transform the problem into a multi-objective optimization problem with constrains and propose a method of evolutionary generation of test data for multiple paths coverage with faults detection. First, we establish the mathematical model of this problem and then a strategy based on multi-objective genetic algorithms is given. Finally we apply the proposed method in some programs under test and the experimental results validate that our method can find specified faults effectively. Compared with other methods of test data generation for multiple paths coverage, our method has greater advantage in faults detection and testing efficiency.","","978-1-4244-6440-1978-1-4244-6437-1978-1-4244-6439","10.1109/BICTA.2010.5645159","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5645159","","","genetic algorithms;program testing","test data evolutionary generation;multiple paths coverage;faults detection;software testing;path-oriented testing;multiobjective optimization problem;mathematical model;multiobjective genetic algorithms","","1","8","","","","","","IEEE","IEEE Conferences"
"Neuro-PID adaptive control scheme for blood pressure regulation","S. Enbiya; A. Hossain; F. Mahieddine","School of Computing, Informatics and Media, University of Bradford, Bradford, UK; School of Computing, Engineering and Information Sciences, Northumbria University; School of Engineering, Design and Technology, University of Bradford, Bradford, UK","2011 5th International Conference on Software, Knowledge Information, Industrial Management and Applications (SKIMA) Proceedings","","2011","","","1","4","Control of physiological states such as mean arterial pressure (MAP) has been successfully achieved using single drug by different control algorithms. Multi-drug delivery demonstrates a significantly challenging task as compared to control with a single-drug. Also the patient's sensitivity to the drugs varies from patient to patient. Therefore, the implementation of adaptive controller is very essential to improve the patient care in order to reduce the workload of healthcare staff and costs. This paper presents the design and implementation of a Proportional Integral Derivative controller (PID) using neural network based parameter tuning mechanism to regulate mean arterial pressure and cardiac output (CO) by administering vasoactive and inotropic drugs that are sodium nitroprusside (SNP) and dopamine (DPM) respectively. The parameters of PID controller were optimised offline using Simulink Response Optimization tool. The proposed Neuro-PID controller has been implemented, tested and verified to demonstrate its merits and capabilities as compared to the existing approaches to cover wide range of patients.","","978-1-4673-0248-7978-1-4673-0247-0978-1-4673-0246","10.1109/SKIMA.2011.6163199","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6163199","Mean Arterial Pressure (MAP);Cardiac Output (CO);Proportional-Integral-Derivative (PID) controller;SodiumNitroprusside (SNP);Dopamine (DPM);Simulink Response Optimisation and Neural Network","Drugs;Blood pressure;Adaptation models;Mathematical model;Sensitivity;Control systems;Adaptive control","adaptive control;blood pressure measurement;drug delivery systems;health care;neurocontrollers;optimisation;three-term control","neuro-PID adaptive control scheme;blood pressure regulation;mean arterial pressure;physiological states;MAP;Multi-drug delivery;adaptive controller;healthcare staff;proportional integral derivative controller;neural network based parameter tuning mechanism;cardiac output;inotropic drugs;vasoactive drugs;sodium nitroprusside;dopamine;simulink response optimization tool","","1","24","","","","","","IEEE","IEEE Conferences"
"An Intelligent Approach of Obtaining Feasible Machining Processes and Their Selection Priorities for Features Based on Neural Network","G. Hua; X. Fan","NA; NA","2010 International Conference on Computational Intelligence and Software Engineering","","2010","","","1","4","To obtain all feasible machining processes and their quantitative selection priorities, an intelligent making decision approach combining back-propagation neural network and backward planning is proposed. Uniform design method, which is adapted for the problem of multiple factors and multiple levels, is adopted to build representative sample sets for the neural network. The neural network is trained by an improved back-propagation algorithm which can adjust momentum factor and learning rate simultaneously, and tested by linear regression analysis. A case study has been conducted to demonstrate the effectiveness of the proposed approach.","","978-1-4244-5391-7978-1-4244-5392","10.1109/CISE.2010.5677004","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5677004","","Artificial neural networks;Planning;Boring;Training;Surface roughness;Materials","backpropagation;computer aided manufacturing;intelligent manufacturing systems;machining;neural nets;process planning;regression analysis","feasible machining process;intelligent making decision;backpropagation neural network;backward planning;momentum factor;learning rate;linear regression analysis;computer aided process planning","","","10","","","","","","IEEE","IEEE Conferences"
"On robust discrete berth allocation based on ant colony algorithm","B. Sun; J. Sun; F. Liu; P. Yang; M. Han; M. Feng","Tianjin Key Laboratory of Intelligence Computing and Novel Software Technology, Tianjin University of Technology, 300191, China; Tianjin Key Laboratory of Intelligence Computing and Novel Software Technology, Tianjin University of Technology, 300191, China; Tianjin Key Laboratory of Intelligence Computing and Novel Software Technology, Tianjin University of Technology, 300191, China; Tianjin Key Laboratory of Intelligence Computing and Novel Software Technology, Tianjin University of Technology, 300191, China; Tianjin Key Laboratory of Intelligence Computing and Novel Software Technology, Tianjin University of Technology, 300191, China; Tianjin Key Laboratory of Intelligence Computing and Novel Software Technology, Tianjin University of Technology, 300191, China","Proceedings of the 29th Chinese Control Conference","","2010","","","1727","1732","The berth allocation is the primary problem of the berth scheduling system, which is an important one in the container terminal. This paper regards total planning delay time of vessels in the terminals as service measure, introduces buffer time as robust measure and proposes the model of robust and discrete berth allocation based on ant colony optimization. In the model the physical conditions and handling capability of the berths is taken into account and the trade-off between the service and robustness of berth allocation planning. The paper applies ant colony optimization to solve the model. The simulation tests reveal that the robust berth allocation plan through the method in this dissertation has the significantly better ability of anti-disturbance than that by the traditional method under the uncertain environment.","1934-1768;2161-2927","978-7-8946-3104-6978-1-4244-6263-6978-7-8946-3104","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5573979","Container Terminal;Berth Allocation;Robust;Service;Ant Colony Algorithm","Robustness;Resource management;Time measurement;Sun;Electronic mail;Planning;Containers","containers;optimisation;scheduling;transportation","robust discrete berth allocation planning;ant colony algorithm;berth scheduling system;total planning delay time","","1","6","","","","","","IEEE","IEEE Conferences"
"General regression neural network forecasting model based on PSO algorithm in water demand","Juan Zhou; Kaiyun Yang","North China University of Water Conversancy and Hydroelectric Power, Zhengzhou, 450011, China; North China University of Water Conversancy and Hydroelectric Power, Zhengzhou, 450011, China","2010 Third International Symposium on Knowledge Acquisition and Modeling","","2010","","","51","54","There is a complicated non-linear relationship between the factors and water demand. General regression neural network (GRNN) was adopted to model the non-linear relationship in the study. The prediction performance of GRNN can vary considerably depending on smoothing parameter. The optimal smoothing parameter is usually determined empirically based on trial-and-error. Particle swarm optimization (PSO) algorithm, to improve GRNN prediction performance, was employed to optimize GRNN and determine an optimal value of smoothing parameter. At the same time, linear inertia weight and chaos variation operator are presented to improve traditional PSO algorithm searching capacity. GRNN forecasting model based on PSO algorithm was used to water demand in Yellow River Basin. The result shows that, compared with Back propagation based on Genetic algorithm model and GRNN based on Genetic algorithm prediction model, the new prediction model is reasonable.","","978-1-4244-8007-4978-1-4244-8004-3978-1-4244-8006","10.1109/KAM.2010.5646238","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5646238","general regression neural network;the improved particle swarm optimization;chaos variation operator;linear inertia weight;water demand forecasting model","Predictive models;Testing;Forecasting;Chaos;Computer aided software engineering","forecasting theory;genetic algorithms;neural nets;particle swarm optimisation;regression analysis;water supply","general regression neural network forecasting model;PSO algorithm;water demand;smoothing parameter;particle swarm optimization;chaos variation operator;yellow river basin;genetic algorithm","","","11","","","","","","IEEE","IEEE Conferences"
"Optimal design of video decoder based on PowerPC 405","Y. Fu; C. Deng; C. Li","Beijing Technology and Business University, Computer and Information Engineering Institute, Beijing, China; Beijing Technology and Business University, Computer and Information Engineering Institute, Beijing, China; Beijing Technology and Business University, Computer and Information Engineering Institute, Beijing, China","2012 2nd International Conference on Consumer Electronics, Communications and Networks (CECNet)","","2012","","","3353","3357","In order to improve the comprehensive performance of portable media player video decoding, adopts the mpeg-4 video decoder in software and hardware co-design. Through the division of software and hardware, the study is the IP core optimized design and implementation in two-dimensional inverse discrete cosine transform, motion compensation, color space conversion. It builds a video decoding model of hardware and software co-design based on hard-core PowerPC 405. As the core for Xilinx FPGA chip, the study builds SOPC test. The results show that the CIF video sequences are supported the real-time decoding at the operating frequency only 100 MHz. This system achieves real-time video playback in low power consumption, high reliability and flexibility.","","978-1-4577-1415-3978-1-4577-1414-6978-1-4577-1413","10.1109/CECNet.2012.6201874","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6201874","SOPC;FPGA;IP;video decoding;co-design","Decoding;Hardware;IP networks;Software;Motion compensation;Image color analysis;Acceleration","decoding;discrete cosine transforms;field programmable gate arrays;hardware-software codesign;image colour analysis;image sequences;inverse transforms;microprocessor chips;motion compensation;multimedia communication;reliability;video coding","optimal design;portable media player video decoding;mpeg-4 video decoder;software hardware codesign;IP core optimized design;two-dimensional inverse discrete cosine transform;motion compensation;color space conversion;hard-core PowerPC 405;Xilinx FPGA chip;SOPC test;CIF video sequence;real-time video playback;power consumption;reliability;frequency 100 MHz","","","5","","","","","","IEEE","IEEE Conferences"
"Software Assisted Digital RF Processor (DRP™) for Single-Chip GSM Radio in 90 nm CMOS","R. Staszewski; R. B. Staszewski; T. Jung; T. Murphy; I. Bashir; O. Eliezer; K. Muhammad; M. Entezari","Texas Instruments, Dallas, TX, USA; Texas Instruments, Dallas, TX, USA; Texas Instruments, Dallas, TX, USA; Texas Instruments, Dallas, TX, USA; Texas Instruments, Dallas, TX, USA; Texas Instruments, Dallas, TX, USA; Texas Instruments, Dallas, TX, USA; Texas Instruments, Dallas, TX, USA","IEEE Journal of Solid-State Circuits","","2010","45","2","276","288","This paper proposes and describes a new software and application programming interface view of an RF transceiver. It demonstrates benefits of using highly programmable digital control logic in an RF wireless system realized in a digital nanoscale CMOS process technology. It also describes a microprocessor architecture design in Digital RF Processor (DRP<sup>TM</sup>) and how it controls calibration and compensation for process, temperature and voltage variations of the analog and RF circuits to meet the required RF performance. A few calibration examples to reduce a DCO bias current and improve device reliability, as well as to optimize transmit modulation and receive performance, are given. The presented circuits and techniques have enabled successful implementation of a commercial single-chip GSM radio in 90 nm CMOS.","0018-9200;1558-173X","","10.1109/JSSC.2009.2036763","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5405143","All-digital phase-locked loop (ADPLL);application programming interface (API);built-in self-test (BIST);calibration;compensation;digitally-controlled oscillator (DCO);digital processor;digitally-assisted analog;mobile phones;nanometer scale CMOS;RF;software;software-defined radio (SDR);time-to-digital converter (TDC)","Radio frequency;GSM;CMOS process;Calibration;Application software;Transceivers;Digital control;CMOS logic circuits;Logic programming;Programmable control","cellular radio;CMOS logic circuits;microprocessor chips","software-assisted digital RF processor;single-chip GSM radio;application programming interface view;RF transceiver;programmable digital control logic;RF wireless system;digital nanoscale CMOS process technology;microprocessor architecture design;analog circuits;RF circuits;DCO bias current;device reliability;transmit modulation;size 90 nm","","31","35","","","","","","IEEE","IEEE Journals & Magazines"
"Quantum Ant Colony Optimization Algorithm and Its Application on Collision Detection","Y. Tian; J. Wu; L. Peng; L. Chen","NA; NA; NA; NA","2010 International Conference on Computational and Information Sciences","","2010","","","1150","1153","Collision detection is very important to improve the truth and immersion in the virtual environment. Firstly the paper analyzes the problems that exist in traditional algorithms. Secondly the paper analyses the problem of collision detection in theory, and then converts the problem of the collision detection to the non-linear programming problem with restricted conditions. And then the quantum ant colony optimization algorithm is brought forward to resolve the problem. A proof of convergence for the algorithm is developed. Finally, the simulation test shows that the quantum inspired immune algorithm has much more effective impact on solving the extreme-value problem compared to the traditional genetic algorithm. It is feasible to use the algorithm in collision detection.","","978-1-4244-8814-8978-0-7695-4270","10.1109/ICCIS.2010.284","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5709484","collision detection;quantum genetic algorithm;nonlinear programming;Bloch coordination","Quantum computing;Algorithm design and analysis;Optimization;Ant colony optimization;Quantum mechanics;Software algorithms;Convergence","genetic algorithms;virtual reality","quantum ant colony optimization algorithm;collision detection;virtual environment;quantum inspired immune algorithm;genetic algorithm","","","9","","","","","","IEEE","IEEE Conferences"
"Generic protocol for seamless control of test instrumentation towards realization of electro optical sensors","A. Dave; J. Sharma; A. Dutt; A. Sukheja","Space Applications Center, Indian Space Research Organization, Ahmedabad-380015, India; Space Applications Center, Indian Space Research Organization, Ahmedabad-380015, India; Space Applications Center, Indian Space Research Organization, Ahmedabad-380015, India; Space Applications Center, Indian Space Research Organization, Ahmedabad-380015, India","2011 IEEE Recent Advances in Intelligent Computational Systems","","2011","","","147","150","Space Applications Center (SAC) of Indian Space Research Organization (ISRO) designs and develops electro optical sensors for earth observations and inter-planetary exploration missions. The sensors are fairly complex systems involving linear/area imaging elements, optics, electronics having large number of spectral bands, making their development a highly challenging task. To ensure in-orbit performance, the sensors are exhaustively tested on ground before being flown. XSCoPE (UNIX based Software System for Payload Evaluation) caters to the evaluation requirement throughout the development cycle of the cameras accomplishing data acquisition, parametric evaluation and optimizations. The paper describes an instrument control protocol (ICP) developed as part of the system and provides an abstraction layer in order to seamlessly interface with the test instrumentation having different underlying hardware interfaces. The protocol specifically developed for tests involving repeated measurements and automation, is scalable, generic in nature and can be adopted for different situations. Details of implementation of the protocol are given citing spectral response measurement test as a specific case to explain the idea.","","978-1-4244-9477-4978-1-4244-9478-1978-1-4244-9475-0978-1-4244-9476","10.1109/RAICS.2011.6069291","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6069291","sensors;CCD;checkout;instrumentation;interfaces;automation;spectral measurement","Instruments;Protocols;Cameras;Software;Sensors;Data acquisition;Automation","aerospace instrumentation;CCD image sensors;optical sensors;protocols;Unix","generic protocol;seamless control;test instrumentation;electro optical sensors;Space Applications Center;Indian Space Research Organization;earth observations;interplanetary exploration mission;XSCoPE;UNIX based software system;payload evaluation;instrument control protocol;abstraction layer;camera development cycle","","","3","","","","","","IEEE","IEEE Conferences"
"Robotic test bed for autonomous surface exploration of Titan, Mars, and other planetary bodies","W. Fink; M. A. Tarbell; R. Furfaro; L. Powers; J. S. Kargel; V. R. Baker; J. Lunine","Visual and Autonomous Exploration Systems Research Laboratory, Departments of Electrical &amp; Computer Engineering, Biomedical Engineering, and Ophthalmology and Vision Science, University of Arizona, Tucson, 85721, USA; Visual and Autonomous Exploration Systems Research Laboratory, Departments of Electrical &amp; Computer Engineering, Biomedical Engineering, and Ophthalmology and Vision Science, University of Arizona, Tucson, 85721, USA; Systems and Industrial Engineering Department, University of Arizona, Tucson, USA; Department of Electrical &amp; Computer Engineering, University of Arizona, Tucson, USA; Department of Hydrology and Water Resources, University of Arizona, Tucson, USA; Department of Hydrology and Water Resources, University of Arizona, Tucson, USA; Lunar and Planetary Laboratory, University of Arizona, Tucson, USA","2011 Aerospace Conference","","2011","","","1","11","Tier-scalable robotic reconnaissance missions are called for in extreme space environments, including planetary atmospheres, surfaces (both solid and liquid), and subsurfaces (e.g., oceans), as well as in potentially hazardous or inaccessible operational areas on Earth. Such future missions will require increasing degrees of operational autonomy: (1) Automatic mapping of an operational area from different vantages (i.e., spaceborne, airborne, surface, subsurface); (2) automatic sensor deployment and sensor data gathering; (3) automatic feature extraction and target/region-of-interest/anomaly identification within the mapped operational area; (4) automatic target prioritization for follow-up or close-up (in-situ) examination; and (5) subsequent automatic, targeted deployment and navigation/relocation of agents/sensors (e.g., to follow up on transient events). We report on recent progress in developing an Earth-based (outdoors) robotic test bed for Tier-scalable Reconnaissance at the University of Arizona and Caltech for distributed, science-driven, and significantly less constrained (compared to state-of-the-art) reconnaissance of prime locations on a variety of planetary bodies, with particular focus on Saturn's moon Titan with its methane/hydrocarbon lakes and Mars. The test bed currently comprises several computer-controlled robotic surface vehicles, i.e., rovers and lake landers/boats equipped with a variety of sensors. To achieve a fully operational Tier-scalable Reconnaissance test bed, aerial platforms will be integrated as a next step. The robotic surface vehicles can be interactively or automatically controlled from anywhere in the world in near real-time via the Internet. The test bed enables the implementation, field-testing, and validation of algorithms and strategies for navigation, exploration, sensor deployment, sensor data gathering, feature extraction, anomaly detection, and science goal prioritization for autonomous planetary exploration. Furthermore, it permits field-testing of novel instruments and sensor technologies, as well as testing of cooperative multi-agent scenarios and distributed scientific exploration of operational areas. As such the robotic test bed enables the development, implementation, field-testing, and validation of software packages for inter-agent communication and coordination to navigate and explore operational areas with greatly reduced reliance on (ultimately without assistance from) ground operators, thus affording the degree of mission autonomy/flexibility necessary to support future missions to Titan, Mars, and other planetary bodies, including asteroids.","1095-323X;1095-323X","978-1-4244-7351-9978-1-4244-7350-2978-1-4244-7349","10.1109/AERO.2011.5747267","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5747267","","Robot sensing systems;Sea surface;Cameras;Reconnaissance;Boats;Lakes","Earth;geophysical prospecting;Internet;Mars;mobile robots;Moon;multi-agent systems;planetary rovers;remote sensing;Saturn;sensors;software packages","autonomous surface exploration;Titan;Mars;planetary body;tier scalable robotic reconnaissance mission;space environment;planetary atmosphere;operational autonomy;automatic mapping;automatic sensor deployment;sensor data gathering;automatic feature extraction;anomaly identification;mapped operational area;automatic target prioritization;Earth based robotic test bed;University of Arizona;Caltech;Saturn moon;computer controlled robotic surface vehicle;Internet;cooperative multiagent scenario;software package;inter agent communication","","12","24","","","","","","IEEE","IEEE Conferences"
"Numerical simulation and optimization of dust pollution treatment scheme with coal warehouse roof","Yan Sun; Shaocheng Ge; Deji Jing","College of Safety Science and Engineering, Liaoning Technical University, Fuxin 123000, China; College of Safety Science and Engineering, Liaoning Technical University, Fuxin 123000, China; College of Safety Science and Engineering, Liaoning Technical University, Fuxin 123000, China","2011 International Conference on Remote Sensing, Environment and Transportation Engineering","","2011","","","3297","3300","Due to restriction of product condition, dust pollution treatment scheme with coal warehouse roof in coal wahery is impossible to be determined after several installation commissioning and testing. In order to choose appropriate dust control scheme, according to numerical simulation method, this paper simulates two kinds of feasible dust control schemes with the fluid calculation software, analyses the two schemes dust control mechanism and their effect, and optimizes the chosen scheme. This paper puts forward a effective dust control scheme with the joint of setting head precipitator in coal warehouse top, setting airtight cover and precipitator inside the warehouse. According to the field application and measurement in ZhunGeer coal washery, the result shows that this method can effectively control dust escaping. It can provide a kind of effective technical implementation method for the on-site construction by determining optimal dust control scheme with numerical simulation method.","","978-1-4244-9171-1978-1-4244-9172-8978-1-4244-9170","10.1109/RSETE.2011.5965017","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5965017","coal warehouse;dust;dust removal scheme;coal off;numerical simulation","Coal;Numerical simulation;Mathematical model;Belts;Materials;Equations;Pollution","air pollution;coal;dust;numerical analysis;optimisation","numerical simulation;optimization;dust pollution treatment;coal warehouse roof;product condition;dust control;fluid calculation software;precipitator;coal washery","","","10","","","","","","IEEE","IEEE Conferences"
"Palimpsests of Time and Place","J. Fishenden; A. Hugill","NA; NA","2011 IEEE 35th Annual Computer Software and Applications Conference","","2011","","","336","345","This paper discusses ongoing research into the development of an original composition portfolio themed on the concept of palimpsests of time and place. The research involves the application of an iterative development methodology to refine creative computer-based techniques for the exploration and navigation of multi-dimensional aural and visual content operating within three-dimensional Cartesian space. Various computational functions for utilising a web browser-based system, oriented around a conceptual n-tier layer model, are being developed and refined. Visualisation computational processes related to images (still and moving) of the same place (such as a building) or other artefact (such as a map) over time utilise both authentic and synthetic content. Auralisation computational processes include authentic and synthetic impulse responses (the acquisition of the acoustic characteristics of particular spatial environments) and convolution reverb (recreating the acoustic reverberation characteristics of a physical or synthetic space based on an associated impulse response). The research is applying an iterative theory-composition-testing cycle to optimise intuitive computer-based processes for enabling users to navigate multiple layers of content, as well as in finding methods that evoke an increased emotional awareness of, and connection with, the past of place over time. Consequently an integral component of the research methodology involves the emergent computational techniques being subjected to formalised usability testing both to assist with their further refinement and to assess their value in evoking an increased awareness of time and place.","0730-3157;0730-3157;0730-3157","978-0-7695-4439-7978-1-4577-0544-1978-0-7695-4439","10.1109/COMPSAC.2011.51","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032361","creative computing;creative technologies;palimpsests;composition;rich internet applications;usability testing","Visualization;Usability;Lenses;Testing;Navigation;Portfolios;Mice","data visualisation;online front-ends;program testing;software reusability","palimpsests;iterative development methodology;computer-based techniques;three-dimensional Cartesian space;Web browser-based system;visualisation computational processes;auralisation computational processes;usability testing","","2","15","","","","","","IEEE","IEEE Conferences"
"Convergence dynamics of biochemical pathway steady state stochastic global optimization","I. Mozga; E. Stalidzans","Biosystems Group, Department of Computer Systems, Latvia University of Agriculture, Liela iela 2, LV3001, Latvia; Biosystems Group, Department of Computer Systems, Latvia University of Agriculture, Liela iela 2, LV3001, Latvia","2011 IEEE 12th International Symposium on Computational Intelligence and Informatics (CINTI)","","2011","","","231","235","The stochastic nature of convergence of steady state stochastic global optimization methods in design optimization tasks with steady state precondition is a hardly predictable step in development of industrially efficient strains of microorganisms. The properties of convergence dynamics of evolutionary programming (EP) and particle swarm (PS) depending on complexity of kinetic equations within the same model are studied optimizing yeast glycolysis for ethanol production by COPASI software adjusting parameters of three combinations of five reactions out of fifteen enzymatic reactions. Results indicate significant differences in the convergence dynamics between different combinations. 50-fold difference in convergence time as well as possible stagnation at local optima was observed. The choice of optimization method and duration of optimization runs should be based on number of tests on the convergence quality, speed and repeatability.","","978-1-4577-0045-3978-1-4577-0044-6978-1-4577-0043","10.1109/CINTI.2011.6108504","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6108504","","Convergence;Optimization methods;Steady-state;Biological system modeling;Stochastic processes;Mathematical model","biochemistry;biofuel;biotechnology;convergence;enzymes;evolutionary computation;microorganisms;particle swarm optimisation;reaction kinetics","convergence dynamics;biochemical pathway;steady state stochastic global optimization;design optimization;microorganisms;evolutionary programming;particle swarm optimization;ethanol production;yeast glycolysis;COPASI software;enzymatic reactions","","5","25","","","","","","IEEE","IEEE Conferences"
"Shader-based sensor simulation for autonomous car testing","S. Wang; S. Heinrich; M. Wang; R. Rojas","Artificial Intelligence Group, Department of Mathematics and Computer Science, Freie Universität Berlin, Germany; Artificial Intelligence Group, Department of Mathematics and Computer Science, Freie Universität Berlin, Germany; Artificial Intelligence Group, Department of Mathematics and Computer Science, Freie Universität Berlin, Germany; Artificial Intelligence Group, Department of Mathematics and Computer Science, Freie Universität Berlin, Germany","2012 15th International IEEE Conference on Intelligent Transportation Systems","","2012","","","224","229","During autonomous car system development, sensor simulation can help to test and evaluate algorithms such as sensor fusion and object tracking in simulated dynamic scenarios at an early stage; thus, time and cost can be spared and more reliable system can be guaranteed. In this paper, shader-based LiDAR and Radar simulations are extended into autonomous car testing. Besides realizing sensor simulations producing information of interest scan data, conceptual programming interfaces to full featured physical models are also provided. Simulation accuracy is discussed and corresponding improvement methods are proposed. Optimistic results are displayed with a software-in-loop test for autonomous car and the computational cost is reported. Comparison between ray-tracing based and shader-based LiDAR simulation in terms of computational cost is also carried out and discussed.","2153-0017;2153-0009;2153-0009","978-1-4673-3063-3978-1-4673-3064-0978-1-4673-3062","10.1109/ITSC.2012.6338904","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6338904","","Laser radar;Computational modeling;Cameras;Rendering (computer graphics);Lasers;Radar cross section","automatic test software;automobile industry;automotive electronics;object tracking;optical radar;ray tracing;sensor fusion","shader-based sensor simulation;autonomous car testing;autonomous car system development;sensor fusion;object tracking;simulated dynamic scenarios;reliable system;shader-based LiDAR;radar simulations;conceptual programming interfaces;physical models;simulation accuracy;software-in-loop test;computational cost;ray tracing;shader based LiDAR simulation","","5","17","","","","","","IEEE","IEEE Conferences"
"Wiring optimization for wired vehicle detector of Parking","Shaohua Wang; Jianzhen Liu; Zhiwei Guan; Lei Zhang","School of Automobile and Transportation, Tianjin University of Technology and Education, China; RIOH Transport, Consultants Ltd Research, institute of Highway, Ministry of Transport, Beijing, China; School of Automobile and Transportation, Tianjin University of Technology and Education, China; School of Automobile and Transportation, Tianjin University of Technology and Education, China","2011 International Conference on Remote Sensing, Environment and Transportation Engineering","","2011","","","5633","5636","To avoid the lack of empirical methods and reduce the cost of Parking Guidance and Information System, the determined method of vehicle detector wiring in the parking was studied. The control structure of the Parking Guidance and Information System was analyzed, the wiring optimization strategies were chosen, and then a minimum cost mathematical model of cable was set up. By measuring the Manhattan distance as the dissimilarity between objects, computing the initial cluster centers based on the density and selecting objects by comparing distance, the K-means clustering algorithms was improved, and the process was end up when the value of the object function never become smaller. The algorithm was programmed by VB6.0 and tested by an example. The result demonstrates that value of the object function counted by the new method is 3177.5m, more less than 5300m, estimated by the traditional way, so the location of the node controller determined by the algorithm is more reasonable and can reduce the expense of wiring.","","978-1-4244-9171-1978-1-4244-9172-8978-1-4244-9170","10.1109/RSETE.2011.5965630","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5965630","formatting;style;styling;insert","Clustering algorithms;Software algorithms;Wiring;Detectors;Automobiles;Educational institutions","object detection;optimisation;road vehicles;statistical analysis;traffic information systems;Visual BASIC;wiring","wiring optimization;wired vehicle detector;parking guidance and information system;Manhattan distance;K-means clustering algorithms;VB6.0;object function","","","","","","","","","IEEE","IEEE Conferences"
"BURN: Enabling Workload Burstiness in Customized Service Benchmarks","G. Casale; A. Kalbasi; D. Krishnamurthy; J. Rolia","Imperial College London, London; University of Calgary, Calgary; University of Calgary, Calgary; HP Labs, Palo Alto","IEEE Transactions on Software Engineering","","2012","38","4","778","793","We introduce BURN, a methodology to create customized benchmarks for testing multitier applications under time-varying resource usage conditions. Starting from a set of preexisting test workloads, BURN finds a policy that interleaves their execution to stress the multitier application and generate controlled burstiness in resource consumption. This is useful to study, in a controlled way, the robustness of software services to sudden changes in the workload characteristics and in the usage levels of the resources. The problem is tackled by a model-based technique which first generates Markov models to describe resource consumption patterns of each test workload. Then, a policy is generated using an optimization program which sets as constraints a target request mix and user-specified levels of burstiness at the different resources in the system. Burstiness is quantified using a novel metric called overdemand, which describes in a natural way the tendency of a workload to keep a resource congested for long periods of time and across multiple requests. A case study based on a three-tier application testbed shows that our method is able to control and predict burstiness for session service demands at a fine-grained scale. Furthermore, experiments demonstrate that for any given request mix our approach can expose latency and throughput degradations not found with nonbursty workloads having the same request mix.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.58","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5928353","Benchmarking;performance;burstiness;bottleneck migration;overdemand","Benchmark testing;Markov processes;Servers;Aggregates;Analytical models;Computational modeling;Linear regression","benchmark testing;Markov processes","BURN;workload burstiness;customized service benchmarks;customized benchmarks;multitier application;time-varying resource usage condition;controlled burstiness;software services;model-based technique;Markov models;resource consumption pattern;optimization program;target request mix;user-specified levels;three-tier application testbed;session service demands;fine-grained scale;latency;throughput degradation;nonbursty workloads","","8","38","","","","","","IEEE","IEEE Journals & Magazines"
"Optimization research for horizontal spacing of no-duct inducing fan system","Jincheng Xing; Yanqin Wang; Jianxing Li; Jihong Ling; Guozhen Lin","School of Environmental Science and Technology, Tianjin University, China; School of Environmental Science and Technology, Tianjin University, China; Tianjin Municipal Engineering Design &amp; Research Institute, China; School of Environmental Science and Technology, Tianjin University, China; Tianjin Water and Gas Information Technology Development co., Ltd, China","2011 International Conference on Electric Technology and Civil Engineering (ICETCE)","","2011","","","4253","4256","For the problem of reasonable longitudinal and horizontal spacing of inducing fan in ventilation system in underground garages, the test of single inducing fan was carried out. Comparing the results of the test and of the numerical simulation, boundary conditions of numerical simulation were determined. By using fluent software, appropriate longitudinal spacing of inducing fan was gotten. Thinking of the poly trope of volume capacity ratio and parking utilization factor, the concept of garage utilization coefficient was defined. On this basis, CO concentration is taken as control standard in this paper and numerical simulation is performed for kinds of small space models. As a result, we get the recommended values of reasonable horizontal spacing for inducing fans' arrangement. Reference data are given for the design of no-duct inducing fan system of underground parking garages in future.","","978-1-4577-0290-7978-1-4577-0289-1978-1-4577-0288","10.1109/ICETCE.2011.5775381","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5775381","inducing fan;longitudinal spacing;CO concentration;horizontal spacing","Ventilation;Numerical simulation;Carbon;Space technology;Architecture;Buildings;Computer architecture","fans;geotechnical engineering;numerical analysis;optimisation;structural engineering;ventilation","optimization;no-duct inducing fan system;ventilation system;underground parking garages;numerical simulation;Fluent software;garage utilization coefficient","","","","","","","","","IEEE","IEEE Conferences"
"An evolutionary algorithm with sorted race mechanism for global optimization","X. Li; Z. Hao; H. Huang","School of Compute Science and Engineering, South China University of Technology, Guangzhou, 510006, China; Faculty of Computer, Guangdong University of Technology, Guangzhou, 510006, China; School of Software Engineering, South China University of Technology, Guangzhou, 510006, China","2010 International Conference on Machine Learning and Cybernetics","","2010","3","","1550","1555","There are often problems of search effectiveness and maintaining the diversity of population in solving single objective optimization problems by evolutionary algorithm. In order to improve search efficiency, the algorithm in this paper regards the current optimal individual as a search starting point, and designs efficient crossover and mutation operator with simulated annealing to search optimal solutions. A sorted race-based selection mechanism is taken to update current population to overcome premature and maintaining the diversity of population. The selection compares the similar individuals to select the best one to keep the population diversity. At last, we test a large number of single-objective test functions to compare and analyze the numerical results with existing algorithms. The results show that our algorithm is very effective.","2160-133X;2160-1348","978-1-4244-6527-9978-1-4244-6526-2978-1-4244-6525","10.1109/ICMLC.2010.5580810","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5580810","Evolutionary algorithm;sorted race mechanism;single objective optimization;estimation;global optimization","Random access memory;Optimization","evolutionary computation;simulated annealing","evolutionary algorithm;sorted race-based selection mechanism;global optimization;crossover operator;mutation operator;simulated annealing","","","11","","","","","","IEEE","IEEE Conferences"
"An automatic framework for dynamic data structures optimization in C","C. Baloukas; L. Papadopoulos; R. Pyka; D. Soudris; P. Marwedel","VLSI Design and Testing Center-Democritus Univ., Thrace 67100, Xanthi, Greece; Microprocessors and Digital Systems Lab - Heroon Polytechneiou, Zographou Campus 9, 15780, Greece; Informatik Centrum Dortmund e.V. Joseph-von-Fraunhofer-Strasse 20, 44227, Germany; Microprocessors and Digital Systems Lab - Heroon Polytechneiou, Zographou Campus 9, 15780, Greece; Informatik Centrum Dortmund e.V. Joseph-von-Fraunhofer-Strasse 20, 44227, Germany","2010 18th IEEE/IFIP International Conference on VLSI and System-on-Chip","","2010","","","155","160","Modern embedded devices require highly optimized code in order to efficiently run the wide range of applications they are designed for. However, most modern applications are getting more and more dynamic, which at the software level, translates in the use of dynamic data structures like dynamic arrays and lists. State of the art solutions for the optimization of these dynamic structures operate with code written in C++ or higher level languages. This work presents an automatic framework for the dynamic data structure optimization of applications written in C. The major advantages of this framework are the rich set of ready-to-use data structures in C that a developer can use to focus on the application itself and the fact that it targets applications in C rather than a higher level language. Moreover, the communication with existing state of the art optimization mechanisms in C++ provides the flexibility in optimization and the customization in the final solutions, needed for modern applications from many domains. The real world applicability of the proposed framework is proved by integrating it with well-known benchmarks written in C. Experimental results show possible reduction of data accesses by 7% and memory footprint by 33%.","2324-8432;2324-8440","978-1-4244-6471-5978-1-4244-6469-2978-1-4244-6470","10.1109/VLSISOC.2010.5642605","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5642605","","Optimization;Benchmark testing;Libraries;Arrays;Embedded systems;Memory management","C language;data structures;embedded systems","dynamic data structure optimization;automatic framework;embedded device;optimized code;C language","","","10","","","","","","IEEE","IEEE Conferences"
"Optimized Barriers for Heterogeneous Systems Using MPI","J. C. Meyer; A. C. Elster","NA; NA","2011 IEEE International Symposium on Parallel and Distributed Processing Workshops and Phd Forum","","2011","","","20","33","The heterogeneous communication characteristics of clustered SMP systems create great potential for optimizations which favor physical locality. This paper describes a novel technique for automating such optimizations, applied to barrier operations. Portability poses a challenge when optimizing for locality, as costs are bound to variations in platform topology. This challenge is addressed through representing both platform structure and barrier algorithms as input data, and altering the algorithm based on benchmark results which can be easily obtained from a given platform. Our resulting optimization technique is empirically tested on two modern clusters, up to eight dual quad-core nodes on one, and up to ten dual hex-core nodes on another. Included test results show that the method captures performance advantages on both systems without any explicit customization, and produces specialized barriers of superior performance to a topology-neutral implementation.","1530-2075;1530-2075","978-1-61284-425-1978-0-7695-4577","10.1109/IPDPS.2011.124","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6008818","","Topology;Clustering algorithms;Optimization;Benchmark testing;Software;Prediction algorithms;Synchronization","application program interfaces;message passing;optimisation","heterogeneous systems;MPI;clustered SMP systems;barrier operations;platform structure;barrier algorithms;optimization technique;dual hex-core nodes;topology-neutral implementation;platform topology;parallel architectures","","2","12","","","","","","IEEE","IEEE Conferences"
"VNA Tools II: S-parameter uncertainty calculation","M. Wollensack; J. Hoffmann; J. Ruefenacht; M. Zeier","METAS Swiss Federal Office of Metrology, Lindenweg 50, CH-3003, Bern-Wabern, Switzerland; METAS Swiss Federal Office of Metrology, Lindenweg 50, CH-3003, Bern-Wabern, Switzerland; METAS Swiss Federal Office of Metrology, Lindenweg 50, CH-3003, Bern-Wabern, Switzerland; METAS Swiss Federal Office of Metrology, Lindenweg 50, CH-3003, Bern-Wabern, Switzerland","79th ARFTG Microwave Measurement Conference","","2012","","","1","5","This paper describes a software, METAS VNA Tools II, which is designed to compute uncertainties of coaxial S-parameter measurements. A bottom-up concept is used. Thus basic influence quantities are propagated through the calibration of the vector network analyzer to the S-parameters of a device under test. METAS UncLib is used for the linear propagation of uncertainties. The result is not only an uncertainty region but a list of uncertainty contributions with correlations. Thus the uncertainties can be propagated into eventual post-processing steps. In the present paper the concept has been verified by computing uncertainties of a calibration with a traditional quick short open load thru algorithm and an algorithm which involves optimization. The observed differences between the resulting uncertainties are lower than 0.3 percent, which can be explained by numerical inaccuracies.","","978-1-4673-1231-8978-1-4673-1229-5978-1-4673-1230","10.1109/ARFTG79.2012.6291183","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6291183","Vector Network Analyzer;S-parameters;Calibration;Uncertainty;Traceability","Uncertainty;Calibration;Measurement uncertainty;Standards;Optimization;Transmission line measurements","calibration;computerised instrumentation;measurement uncertainty;network analysers;optimisation;S-parameters","S-parameter uncertainty calculation;METAS VNA Tools II software;coaxial S-parameter measurement;calibration;vector network analyzer;METAS UncLib;device under test;uncertainties. linear propagation;eventual post-processing step;calibration computing uncertainty;quick short open load thru algorithm;optimization;numerical inaccuracy","","21","15","","","","","","IEEE","IEEE Conferences"
"Enhancing MOEA with component-emphasizing mechanism for multi-objective optimization","Song Yang; Ji Junzhong; Liu Chunnian","College of Computer Science and Technology, Beijing University of Technology, Beijing Municipal Key Laboratory ofMultimedia and Intelligent Software Technology, 100124, CHINA; College of Computer Science and Technology, Beijing University of Technology, Beijing Municipal Key Laboratory ofMultimedia and Intelligent Software Technology, 100124, CHINA; College of Computer Science and Technology, Beijing University of Technology, Beijing Municipal Key Laboratory ofMultimedia and Intelligent Software Technology, 100124, CHINA","2010 IEEE Fifth International Conference on Bio-Inspired Computing: Theories and Applications (BIC-TA)","","2010","","","380","384","Multi-objective optimization is an important and challenging topic in the field of industrial design and scientific research because real-world problems usually involve several conflicting objectives. Since a multi-objective evolutionary algorithms (MOEA) is able to obtain an approximation to the Pareto optimal set and provide substantial information of the tradeoff between objectives, it is becoming one of the most successful methods for multi-objective optimization. Usually, an MOEA generates new trial solutions (offspring) with some candidate decision vectors (parents) to search for the promising areas and make the population evolve towards the Pareto optimal set. Moreover, in the reproduction procedure, most of MOEAs view the decision vector as a whole, and do not recognize the effects of a single component on the new trial solutions. In this paper, we propose the component-emphasizing mechanism for enhancing the search ability of MOEAs. In this mechanism, each component of a decision vector is viewed as an independent factor affecting the quality of the solution. Based on the mechanism, a new MOEA is presented. Finally, the performance of this new algorithm is compared with two other promising MOEAs, namely, NSGA-II and GDE3, on a set of test instances. The experimental results have shown that the proposed algorithm outperforms the others in solution quality and time cost.","","978-1-4244-6440-1978-1-4244-6437-1978-1-4244-6439","10.1109/BICTA.2010.5645288","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5645288","MOEA;elite population;steady-state strategy;component-emphasizing mechanism","Artificial neural networks","evolutionary computation;Pareto optimisation;search problems","component-emphasizing mechanism;multiobjective optimization;industrial design;scientific research;multiobjective evolutionary algorithm;Pareto optimal set;trial solutions;decision vector;search ability;NSGA-II;GDE3","","","17","","","","","","IEEE","IEEE Conferences"
"Multi-objective optimization in dynamic environment: A review","R. Chen; W. Zeng","School of Software, Xiamen University, Xiamen, China; School of Software, Xiamen University, Xiamen, China","2011 6th International Conference on Computer Science & Education (ICCSE)","","2011","","","78","82","Dynamic multi-objective evolutionary algorithms (Dynamic MOEAs) use the evolutionary algorithms to solve the dynamic multi-objective optimization problems (DMOPs). It has become one of the hot areas of research. The challenge of DMOPs is that the objective functions, the constraints or the parameters may change over time. This paper tries to provide a comprehensive overview of the related work, which is organized by the common process of Dynamic MOEAs, such as, the detection of change, the maintenance of diversity, the prediction of change, the test problems and the performance metrics. Finally, topics for further research are suggested.","","978-1-4244-9718-8978-1-4244-9717-1978-1-4244-9716","10.1109/ICCSE.2011.6028589","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6028589","dynamic;multi-objective optimization;evolutionary algorithms;perfomance metric","Optimization;Heuristic algorithms;Genetic algorithms;Measurement;Evolutionary computation;Random access memory;Convergence","evolutionary computation","evolutionary algorithm;dynamic multiobjective optimization problem;dynamic MOEA;performance metrics","","1","33","","","","","","IEEE","IEEE Conferences"
"Visual Development Platform for White-Box Algorithmic Trading","C. Chou; A. Y. Chang","NA; NA","2010 Sixth International Conference on Intelligent Information Hiding and Multimedia Signal Processing","","2010","","","712","715","As with any software system development project, trading strategy generated by technical analysis, has its life cycle. Some only last for a few months, or even only a few days or hours. The trading strategy becomes ineffective after then. This study attempts to provide a visual based tool that allows traders using simple Drag &amp; Draw method to convert the idea of trading strategy to the program that is ready for execution. At the end of the life cycle of trading strategies, they can be modified, updated or deleted easily. This will make the trading strategies adapt to the fast changing market and let the traders generate more profit.","","978-1-4244-8378-5978-0-7695-4222","10.1109/IIHMSP.2010.180","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5636150","algorithmis trading;algorithmis trading;visual programming","Visualization;Programming;Testing;Algorithm design and analysis;Software algorithms;Optimization;Signal processing algorithms","financial data processing;software engineering;supervisory programs;visual programming","visual development platform;white box algorithm;software system development project;trading strategy;drag draw method;program execution","","1","14","","","","","","IEEE","IEEE Conferences"
"An introduction to Technology Forecasting with a TFDEA Excel add-in","D. Lim; T. R. Anderson","Dept. of Engineering and Technology Management, Portland State University, OR - USA; Dept. of Engineering and Technology Management, Portland State University, OR - USA","2012 Proceedings of PICMET '12: Technology Management for Emerging Technologies","","2012","","","1293","1298","This paper describes an Excel add-in program that can run TFDEA (Technology Forecasting using Data Envelopment Anlaysis) within the spreadsheet. It utilizes freely available statistical software R and its packages developed by Statconn. This add-in allows access to both user-friendly tools for data manipulation in spreadsheet available and the power and precision of the results via R.","2159-5100","978-1-890843-25-0978-1-4673-2853","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6304152","","Software;Technology management;Technology forecasting;Optimization;Benchmark testing;Computational modeling;Data envelopment analysis","data envelopment analysis;integrated software;statistical analysis","TFDEA Excel add-in program;technology forecasting using data envelopment analysis;spreadsheet;statistical software package;Statconn;user friendly tools;data manipulation","","1","12","","","","","","IEEE","IEEE Conferences"
"Design and Analysis of Packaging Boxes for Flat Panel Displays","F. C. Teng; F. T. Wu; C. Cheng; C. Sung","Department of Mechanical Engineering, National Chung Cheng University, Chia-Yi, Taiwan, R.O.C.; Department of Mechanical Engineering, National Chung Cheng University, Chia-Yi, Taiwan, R.O.C.; Department of Mechanical Engineering, National Chung Cheng University, Chia-Yi, Taiwan, R.O.C.; Department of Power Mechanical Engineering, National Tsing Hua University","IEEE Transactions on Advanced Packaging","","2010","33","1","106","114","A methodology of designing the packaging box based on the topology optimization technique is proposed. The packaging box is designed not only to protect the fragile liquid crystal display from damage but also take up a volume as small as possible in order to reduce the transport cost. In contrast to the traditional approach which minimizes the acceleration and/or the maximal stress of the panel contained in the packaging box, the proposed objective is to minimize simultaneously the predesignated natural frequency and the mean compliance of a packaging box subjected to a volume constraint. An algorithm which integrates the finite element software ANSYS used as the structural analysis tool, the optimization module based on the sequential linear programming, and a topology module, is developed to achieve the crashworthiness design of packaging box. The material properties as well as the finite element model of a packaging box are tested and validated experimentally. The packaging box after topology optimization is evaluated numerically by drop test simulations in terms of acceleration experienced by the panel and volume reduction of packaging box. For the specific example demonstrated, results show that an optimal layout of the packaging box not only has a smaller volume, 14.85% less than the packaging box used nowadays, but also reduce the maximal acceleration experienced by the panel by 8.9%.","1521-3323;1557-9980","","10.1109/TADVP.2009.2038790","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5393022","Crashworthiness design;liquid crystal display;packaging box;shock;topology optimization","Packaging;Flat panel displays;Topology;Design optimization;Acceleration;Finite element methods;Algorithm design and analysis;Vehicle crash testing;Design methodology;Protection","finite element analysis;flat panel displays;optimisation;packaging;topology","packaging box design;flat panel displays;topology optimization module;liquid crystal display;mean compliance;finite element software ANSYS;structural analysis;sequential linear programming;crashworthiness design;finite element model;drop test simulations","","","15","","","","","","IEEE","IEEE Journals & Magazines"
"ELITE: Ensemble of Optimal Input-Pruned Neural Networks Using TRUST-TECH","B. Wang; H. Chiang","School of Electrical and Computer Engineering, Cornell University, Ithaca, NY, USA; School of Electrical and Computer Engineering, Cornell University, Ithaca, NY, USA","IEEE Transactions on Neural Networks","","2011","22","1","96","109","The ensemble of optimal input-pruned neural networks using TRUST-TECH (ELITE) method for constructing high-quality ensemble through an optimal linear combination of accurate and diverse neural networks is developed. The optimization problems in the proposed methodology are solved by a global optimization a global optimization method called TRansformation Under Stability-reTraining Equilibrium Characterization (TRUST-TECH), whose main features include its capability in identifying multiple local optimal solutions in a deterministic, systematic, and tier-by-tier manner. ELITE creates a diverse population via a feature selection procedure of different local optimal neural networks obtained using tier-1 TRUST-TECH search. In addition, the capability of each input-pruned network is fully exploited through a TRUST-TECH-based optimal training. Finally, finding the optimal linear combination weights for an ensemble is modeled as a nonlinear programming problem and solved using TRUST-TECH and the interior point method, where the issue of non-convexity can be effectively handled. Extensive numerical experiments have been carried out for pattern classification on the synthetic and benchmark datasets. Numerical results show that ELITE consistently outperforms existing methods on the benchmark datasets. The results show that ELITE can be very promising for constructing high-quality neural network ensembles.","1045-9227;1941-0093","","10.1109/TNN.2010.2087354","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5634129","Feature selection;global optimization;neural network ensemble;optimal linear combination;transformation under stability-retaining equilibrium characterization (TRUST-TECH)","Artificial neural networks;Training;Optimization;Accuracy;Diversity reception;Systematics;Benchmark testing","feature extraction;learning (artificial intelligence);neural nets;nonlinear programming;pattern classification","optimal input pruned neural network;TRUST-TECH;ELITE;optimal linear combination;transformation under stability retraining equilibrium characterization;neural networks ensemble;global optimization method;feature selection;nonlinear programming;interior point method;pattern classification","Algorithms;Artificial Intelligence;Computer Simulation;Linear Models;Neural Networks (Computer);Neurons;Nonlinear Dynamics;Software;Software Design","11","44","","","","","","IEEE","IEEE Journals & Magazines"
"Branch penalty reduction on IBM Cell SPUs via software branch hinting","J. Lu; Y. Kim; A. Shrivastava; C. Huang","Compiler Microarchitecture Lab, Arizona State University, Tempe, AZ 85281; Compiler Microarchitecture Lab, Arizona State University, Tempe, AZ 85281; Compiler Microarchitecture Lab, Arizona State University, Tempe, AZ 85281; Compiler Microarchitecture Lab, Arizona State University, Tempe, AZ 85281","2011 Proceedings of the Ninth IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS)","","2011","","","355","364","As power-efficiency becomes paramount concern in processor design, architectures are coming up that completely do away with hardware branch prediction, and rely solely on software branch hinting. A popular example is the Synergistic Processing Unit (SPU) in the IBM Cell processor. To be able to minimize the branch penalty using branch hint instructions, in addition to estimating the branch probabilities (which has been looked at before [6, 25, 24]), it is important to carefully insert branch hints. Towards this, in this paper, we i) construct a branch penalty model for compiler, ii) formulate the problem of minimizing branch penalty using branch hinting and iii) propose a heuristic to solve this problem. The heuristic is based on three basic techniques that we introduce in this paper: NOP padding, hint pipelining, and nested loop restructuring. Experimental results on several benchmarks show that our solution can reduce the branch penalty as much as 35.4% over the previous approach.","","978-1-4503-0715-4978-1-4503-0715-4978-1-4503-0712","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062308","Branch hint;Cell processor;Compiler optimization","Computer architecture;Software;Pipeline processing;Hardware;Microprocessors;Pipelines;Benchmark testing","multiprocessing systems;pipeline processing;program compilers","branch penalty reduction;IBM cell SPU;software branch hinting;processor design;hardware branch prediction;synergistic processing unit;IBM cell processor;branch hint instructions;branch probability;branch penalty model;compiler;NOP padding;hint pipelining;nested loop restructuring","","","25","","","","","","IEEE","IEEE Conferences"
"Notice of Retraction<br>Simulation and test on braking performance of heavy-duty truck with multi-axis braking system","He Hao; Dengfeng Wang; Shuming Chen; Shengqiang Liu; Shaoxian Bu","State Key Laboratory of Automobile Dynamic Simulation, Jilin University, Changchun, 130022, China; State Key Laboratory of Automobile Dynamic Simulation, Jilin University, Changchun, 130022, China; State Key Laboratory of Automobile Dynamic Simulation, Jilin University, Changchun, 130022, China; China National Heavy Duty Truck Group Co. Ltd, Jinan, Shandong Province, 250002, China; China National Heavy Duty Truck Group Co. Ltd, Jinan, Shandong Province, 250002, China","2010 Second Pacific-Asia Conference on Circuits, Communications and System","","2010","1","","47","50","Based on multi-body dynamic method, a combined braking system model with two disc brakes and four drum brakes were built. On the basis of this model, a rigid-flexible coupling model of a heavy-duty truck with full-floating cab mount system, air suspension system with four air springs and taper-leaf-spring system was built. Some simulation calculations were performed in ADAMS software environment to analyze vehicle braking performance on high adhesive road surface under empty and full load conditions. Validity of the model was verified by comparing the simulation results with experiment data. The results show that the rigid-elastic coupling model of heavy-duty truck with multi-axis braking system established in this paper is valid, and it can simulate well vertical load distribution between axes generated by the inertia forces on braking. So it is feasible to analyze and optimize vehicle braking performance of a heavy-duty truck using virtual prototyping technology in product R&D stage.;Notice of Retraction<br><br>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<br><br>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<br><br>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<br><br>","","978-1-4244-7970-2978-1-4244-7969-6978-1-4244-7967","10.1109/PACCS.2010.5627018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5627018","Multi-axis braking system;Heavy-duty truck;Virtual Prototype;Braking performance;Simulation","Optimization;Software","brakes;braking;couplings;elasticity;road vehicles;shear modulus;springs (mechanical);suspensions (mechanical components);vehicle dynamics;virtual prototyping","heavy-duty truck;multiaxis braking system;multibody dynamic method;disc brake;drum brake;rigid-flexible coupling model;full-floating cab mount system;air suspension system;air spring;taper-leaf-spring system;ADAMS software;vehicle braking performance;high adhesive road surface;rigid-elastic coupling model;vertical load distribution;virtual prototyping","","1","6","","","","","","IEEE","IEEE Conferences"
"Optimizing Private Memory Performance by Dynamically Deactivating Cache Coherence","W. Shaogang; X. Weixia; P. Zhengbin; W. Dan; D. Yi; L. Pingjing","NA; NA; NA; NA; NA; NA","2012 IEEE 14th International Conference on High Performance Computing and Communication & 2012 IEEE 9th International Conference on Embedded Software and Systems","","2012","","","1112","1117","User Program has significant percentage of memory requests that target only private data, which does not need to resolve cache coherence conflicts. Yet traditional coherence protocol does not distinguish between shared and private blocks, which leaves much optimization space for memory reference latency and power consumption. This paper proposes an optimized cache coherence protocol, called PMESI, which dynamically deactivates cache coherence maintenance for private memory space. PMESI achieves two distinguishing features: the reduction of memory access latency and system power consumption. Simulation results on the cycle accurate simulator for wide range of industry benchmarks show that 54% memory references can be efficiently optimized and program execution time is reduced 9% on average.","","978-1-4673-2164-8978-0-7695-4749","10.1109/HPCC.2012.163","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6332298","PMESI protocol;private memory space;directory coherence protocol;operating system","Coherence;Protocols;Memory management;Kernel;Benchmark testing;Radiation detectors;Delay","cache storage;data privacy;optimisation;power aware computing;protocols","cache coherence conflicts;optimized cache coherence protocol;PMESI;cache coherence maintenance;dynamical deactivation;private memory space;memory access latency reduction;system power consumption reduction;program execution time;memory requests;private data;private memory performance optimization;user program","","","11","","","","","","IEEE","IEEE Conferences"
"Application of Genetic algorithm on IC substrate drilling path optimization","C. Y. Tsai; C. H. Liu; Y. C. Wang","Department of Industrial Engineering and Management at the Yuan-Ze University, Taoyuan, Taiwan; Department of Industrial Engineering and Management at the Yuan-Ze University, Chungli, Taoyuan, Taiwan; Department of Industrial Engineering and Management, Yuan-Ze University, Taoyuan, Taiwan","The 2012 International Conference on Advanced Mechatronic Systems","","2012","","","441","446","This paper proposes genetic algorithm (GA) based heuristic algorithms for the integrated circuit (IC) substrate drilling path optimization problem. IC substrates are cut from a piece of copper clad laminate which contains tens of thousands of holes. A path connecting these holes has to be plotted for the drilling operation. The objective in this research is minimizing the total path length. Thus, this problem is treated as a large scale traveling salesman problem. In this research, we developed three effective algorithms, unit optimization algorithm, two units incremental algorithm, and local fine-tune algorithm. This study employs a numerical experiment to determine the appropriate combination of algorithm parameter values. Test instances obtained from the case company were used to evaluate the performance of the proposed algorithms. It is shown that unit optimization algorithm is able to effectively obtain the approximate optimal value similar to the solutions of case company in fastest time. Furthermore, the two unit incremental algorithm and local fine-tune algorithm successfully improve execution time and solution stability.","2325-0690;2325-0682;1756-8412","978-0-9555293-8-2978-1-4673-1962","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6329620","","Integrated circuits;Genetics;Strips;Optimization;Companies;Lasers;Computer aided software engineering","circuit optimisation;circuit stability;copper;drilling;genetic algorithms;heuristic programming;integrated circuit modelling;laminates;travelling salesman problems","genetic algorithm;IC substrate drilling path optimization;heuristic algorithms;integrated circuit substrate drilling path optimization problem;copper clad laminate;large scale traveling salesman problem;unit optimization algorithm;unit incremental algorithm;local fine-tune algorithm;solution stability","","","13","","","","","","IEEE","IEEE Conferences"
"Optimal dispatch of wind farm based on particle swarm optimization algorithm","Xiaorong Zhu; Wentong Zhang; Yi Wang; Haifeng Liang","Department of Electrical Engineering, North China Electric Power University, Baoding, 071003 China; Department of Electrical Engineering, North China Electric Power University, Baoding, 071003 China; Department of Electrical Engineering, North China Electric Power University, Baoding, 071003 China; Department of Electrical Engineering, North China Electric Power University, Baoding, 071003 China","2012 IEEE International Conference on Power System Technology (POWERCON)","","2012","","","1","5","As installed capacity of wind power retains a significant proportion of generation in the power system, the dispatch of wind power brings some new problems to the system. It is an effective way to increase the capabilities of wind farms to regulate active power for grid optimal dispatch support. Particle swarm algorithm is an excellent optimization algorithm for its robustness and versatility, and it has been widely used in the field of power system optimization in recent years. In this paper, an active power dispatch model of wind turbine generators is presented, in which the optimization objective is to minimize the line loss of the wind farm, and the particle swarm optimization algorithm is applied to solve the optimization function. Testing results show that this calculation method could track the power dispatch upon operator's request more accurately than the conventional distribution method, and the active power loss of wind farm can be reduced.","","978-1-4673-2868-5978-1-4673-2868-5978-1-4673-2867","10.1109/PowerCon.2012.6401354","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6401354","wind farm;doubly fed induction generator (DFIG);particle swarm optimization (PSO);optimal dispatch;active power loss","Computer aided software engineering;Robustness;Educational institutions;IP networks;Turbines","AC generators;particle swarm optimisation;power generation dispatch;wind power plants","wind farm optimal dispatch;particle swarm optimization algorithm;power system;grid optimal dispatch;power system optimization;active power dispatch model;wind turbine generators;active power loss","","","15","","","","","","IEEE","IEEE Conferences"
"Using hybrid social emotional optimization algorithm with Metropolis rule to solve nonlinear equations","J. Wu; Z. Cui; J. Liu","Complex System and Computational Intelligence Laboratory, Taiyuan University of Science and Technology, Shanxi, China, 030024; Complex System and Computational Intelligence Laboratory, Taiyuan University of Science and Technology, Shanxi, China, 030024; Complex System and Computational Intelligence Laboratory, Taiyuan University of Science and Technology, Shanxi, China, 030024","IEEE 10th International Conference on Cognitive Informatics and Cognitive Computing (ICCI-CC'11)","","2011","","","405","411","Social emotional optimization algorithm (SEOA) is a novel swarm intelligent population-based optimization algorithm by simulating the human social behaviors. How- ever, as a stochastic optimization algorithm, it is easily trapped into local optima. In this paper, we propose a new hybrid algorithm combining with Metropolis rule to enhance the escaping capability from local optima. Further- more, the nonlinear equation solving problems are used to test, and compared with the standard version. Simulation results show it is effective.","","978-1-4577-1697-3978-1-4577-1695-9978-1-4577-1696","10.1109/COGINF.2011.6016173","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6016173","","Optimization;Indexes;Nonlinear equations;Humans;Mathematical model;Software algorithms","multi-agent systems;nonlinear equations;optimisation","hybrid social emotional optimization algorithm;metropolis rule;nonlinear equations;SEOA;swarm intelligent population-based optimization algorithm;human social behaviors;stochastic optimization algorithm","","4","26","","","","","","IEEE","IEEE Conferences"
"Hardware Particle Swarm Optimization Based on the Attractive-Repulsive Scheme for Embedded Applications","D. M. Munoz; C. H. Llanos; L. d. S. Coelho; M. Ayala-Rincon","NA; NA; NA; NA","2010 International Conference on Reconfigurable Computing and FPGAs","","2010","","","55","60","Particle Swarm Optimization (PSO) algorithms have been proposed to solve engineering problems that require to find an optimal point of operation. However, the PSO algorithm suffers from premature convergence and high elapsed time when solving multimodal and large scale engineering problems. This problem becomes an evident drawback for embedded applications in which the microcontrollers often operates at low computational capacity. This paper proposes a hardware implementation of a parallel self-adaptive PSO algorithm based on an attractive-repulsive scheme and using the efficient floating-point arithmetic which performs computations with large dynamic range and high precision. The parallel capabilities of the PSO are exploited by implementing parallel particles directly in hardware in order to decrease the running time. In addition, the attractive-repulsive technique avoids the premature convergence problem by self-adapting the swarm behavior according to its diversity. Synthesis and simulation results for benchmark test problems were performed, demonstrating the correctness of the proposed architectures. Finally, an elapsed time comparison between the hardware and software implementations shows the suitableness of the proposed architecture for embedded applications.","2325-6532","978-1-4244-9523-8978-0-7695-4314","10.1109/ReConFig.2010.73","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5695281","global optimization;swarm intelligence;FPGAs;floating-point arithmetic","Hardware;Computer architecture;Software;Field programmable gate arrays;Optimization;Mathematical model;Generators","embedded systems;floating point arithmetic;microcontrollers;particle swarm optimisation","particle swarm optimization;attractive-repulsive scheme;PSO algorithm;premature convergence;embedded applications;microcontrollers;hardware implementation;parallel self-adaptive algorithm;floating-point arithmetic","","5","14","","","","","","IEEE","IEEE Conferences"
"<emphasis emphasistype=""mono"">elastix</emphasis>: A Toolbox for Intensity-Based Medical Image Registration","S. Klein; M. Staring; K. Murphy; M. A. Viergever; J. P. W. Pluim","NA; NA; NA; NA; NA","IEEE Transactions on Medical Imaging","","2010","29","1","196","205","Medical image registration is an important task in medical image processing. It refers to the process of aligning data sets, possibly from different modalities (e.g., magnetic resonance and computed tomography), different time points (e.g., follow-up scans), and/or different subjects (in case of population studies). A large number of methods for image registration are described in the literature. Unfortunately, there is not one method that works for all applications. We have therefore developed elastix, a publicly available computer program for intensity-based medical image registration. The software consists of a collection of algorithms that are commonly used to solve medical image registration problems. The modular design of elastix allows the user to quickly configure, test, and compare different registration methods for a specific application. The command-line interface enables automated processing of large numbers of data sets, by means of scripting. The usage of elastix for comparing different registration methods is illustrated with three example experiments, in which individual components of the registration method are varied.","0278-0062;1558-254X","","10.1109/TMI.2009.2035616","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5338015","elastix;image registration;medical imaging;open source;software","MONOS devices;Biomedical imaging;Image registration;Cost function;Application software;Optimization methods;Spatial resolution;Open source software;Image processing;Image segmentation","biomedical MRI;computerised tomography;image registration;medical computing;medical image processing;public domain software","elastix;intensity-based medical image registration;magnetic resonance imaging;computed tomography;command-line interface;automated data processing;scripting","Diagnostic Imaging;Humans;Image Processing, Computer-Assisted;Models, Biological;Normal Distribution;Software","1175","73","","","","","","IEEE","IEEE Journals & Magazines"
"Application of Agile Requirement Engineering in Modest-Sized Information Systems Development","L. Jun; W. Qiuzhen; G. Lin","NA; NA; NA","2010 Second World Congress on Software Engineering","","2010","2","","207","210","Agile requirement engineering aims at applying agile thoughts to traditional requirement engineering. It is the optimization and improvement of traditional requirement engineering, getting it fit to the continuous changes of requirements. This paper contrasts the agile requirement engineering and the traditional one, studies how agile requirement engineering practices could be applied to modest-sized information systems development, and finally takes “Wensum Online” as an example to analyze the practical application of it.","","978-1-4244-9287","10.1109/WCSE.2010.105","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5718379","Requirement Engineering;Agile Requirement Engineering practices;Modest-sized Information Systems","Companies;Face;Software;Information systems;Programming;Interviews;Testing","formal specification;software engineering;systems analysis","agile requirement engineering;modest sized information systems development;Wensum Online","","2","5","","","","","","IEEE","IEEE Conferences"
"Multi-stage delivery of malware","M. Ramilli; M. Bishop","Dipartimento di Elettronica Informatica e Sistemistica, University of Bologna, Via Venezia, 52 - 47023 Cesena &#x00E2; ITALY; Department of Computer Science, University of California, Davis, 95616-8562, USA","2010 5th International Conference on Malicious and Unwanted Software","","2010","","","91","97","Malware signature detectors use patterns of bytes, or variations of patterns of bytes, to detect malware attempting to enter a systems. This approach assumes the signatures are both or sufficient length to identify the malware, and to distinguish it from non-malware objects entering the system. We describe a technique that can increase the difficulty of both to an arbitrary degree. This technique can exploit an optimization that many anti-virus systems use to make inserting the malware simple; fortunately, this particular exploit is easy to detect, provided the optimization is not present. We describe some experiments to test the effectiveness of this technique in evading existing signature-based malware detectors.","","978-1-4244-9356-2978-1-4244-9353-1978-1-4244-9355","10.1109/MALWARE.2010.5665788","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5665788","","Malware;Software;Grippers;Engines;Assembly;Detectors;Optimization","invasive software;optimisation","malware multistage delivery;malware signature detector;bytes pattern variation;optimization;antivirus system","","7","19","","","","","","IEEE","IEEE Conferences"
"Thermal-aware on-line task allocation for 3D multi-core processor throughput optimization","C. Lung; Y. Ho; D. Kwai; S. Chang","Department of Computer Science, National Tsing Hua University, HsinChu 30013, Taiwan; Department of Computer Science, National Tsing Hua University, HsinChu 30013, Taiwan; Information and Communications Research Laboratories, Industrial Technology Research Institute, HsinChu 31040, Taiwan; Department of Computer Science, National Tsing Hua University, HsinChu 30013, Taiwan","2011 Design, Automation & Test in Europe","","2011","","","1","6","Three-dimensional integrated circuit (3D IC) has become an emerging technology in view of its advantages in packing density and flexibility in heterogeneous integration. The multi-core processor (MCP), which is able to deliver equivalent performance with less power consumption, is a candidate for 3D implementation. However, when maximizing the throughput of 3D MCP, due to the inherent heat removal limitation, thermal issues must be taken into consideration. Furthermore, since the temperature of a core strongly depends on its location in the 3D MCP, a proper task allocation helps to alleviate any potential thermal problem and improve the throughput. In this paper, we present a thermal-aware on-line task allocation algorithm for 3D MCPs. The results of our experiments show that our proposed method achieves 16.32X runtime speedup, and 23.18% throughput improvement. These are comparable to the exhaustive solutions obtained from optimization modeling software LINGO. On average, our throughput is only 0.85% worse than that of the exhaustive method. In 128 task-to-core allocations, our method takes only 0.932 ms, which is 57.74 times faster than the previous work.","1558-1101;1530-1591;1530-1591","978-3-9810801-8-6978-1-61284-208-0978-3-9810801-7","10.1109/DATE.2011.5763008","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5763008","Multi-core processor;task allocation;thermal awareness;three-dimensional integration;throughput optimization;temperature uniformity","Resource management;Three dimensional displays;Throughput;Heat sinks;Mathematical model;Computational modeling;Runtime","microprocessor chips;multiprocessing systems;thermal management (packaging);three-dimensional integrated circuits","thermal-aware on-line task allocation;3D multicore processor throughput optimization;three-dimensional integrated circuit;3D IC;heterogeneous integration;heat removal;optimization modeling software;LINGO;task-to-core allocation;time 0.932 ms","","6","14","","","","","","IEEE","IEEE Conferences"
"Hybrid source-level simulation of data caches using abstract cache models","S. Stattelmann; G. Gebhard; C. Cullmann; O. Bringmann; W. Rosenstiel","FZI Forschungszentrum Informatik, Haid-und-Neu-Str. 10-14, D-76131 Karlsruhe, Germany; AbsInt Angewandte Informatik GmbH, Science Park 1, D-66123 Saarbruecken, Germany; AbsInt Angewandte Informatik GmbH, Science Park 1, D-66123 Saarbruecken, Germany; FZI Forschungszentrum Informatik, Haid-und-Neu-Str. 10-14, D-76131 Karlsruhe, Germany; FZI Forschungszentrum Informatik, Haid-und-Neu-Str. 10-14, D-76131 Karlsruhe, Germany","2012 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2012","","","376","381","This paper presents a hybrid cache analysis for the simulation-based evaluation of data caches in embedded systems. The proposed technique uses static analyses at the machine code level to obtain information about the control flow of a program and the memory accesses contained in it. Using the result of these analyses, a high-speed source-level simulation model is generated from the source code of the application, enabling a fast and accurate evaluation of its data cache behavior. As memory accesses are obtained from the binary-level control flow, which is simulated in parallel to the original functionality of the software, even complex compiler optimizations can be modeled accurately. Experimental results show that the presented source-level approach estimates the cache behavior of a program within the same level of accuracy as established techniques working at the machine code level.","1558-1101;1530-1591;1530-1591","978-1-4577-2145-8978-3-9810801-8","10.1109/DATE.2012.6176500","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6176500","System analysis and design;Timing;Modeling;Software performance;Cache memories","Analytical models;Data models;Software;Computer architecture;Registers;Optimization;Binary codes","cache storage;digital simulation;embedded systems;optimisation;program compilers;program diagnostics","hybrid source-level simulation;data caches;abstract cache models;hybrid cache analysis;simulation-based evaluation;static analyses;machine code level;high-speed source-level simulation model;binary-level control flow;complex compiler optimizations","","13","24","","","","","","IEEE","IEEE Conferences"
"Analysis and Optimization on the Torsional Rigidity and Its Effect Factors of Twist-Drill","C. Fang; F. Wang; X. Yang; Z. Hu","NA; NA; NA; NA","2010 International Conference on E-Product E-Service and E-Entertainment","","2010","","","1","4","The rigidity of twist-drill are determined by its structure parameters. In this paper, the FEM model of the twist-drill is established by software ABAQUS. With the methods of Orthogonal test and FEM, a quantitative analysis to the torsional rigidity of twist-drill have been developed. The optimization of the twist-drill structure parameters are carried out with Genetic Algorithm based on the objective function of the torsional rigidity. The research result is helpful to the design of twist-drill","","978-1-4244-7161-4978-1-4244-7159-1978-1-4244-7160","10.1109/ICEEE.2010.5660684","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5660684","","Finite element methods;Drilling;Laboratories;Vehicles;Optimization;Sun;Vibrations","drilling machines;finite element analysis;genetic algorithms;shear modulus;structural engineering computing;torsion","torsional rigidity;FEM model;software ABAQUS;orthogonal test;quantitative analysis;twist drill structure parameters;genetic algorithm;finite element analysis","","","","","","","","","IEEE","IEEE Conferences"
"An Evaluation Framework for Assessing and Optimizing Multimodal Fusion Engines Performance","P. Feiteira; C. Duarte","NA; NA","2012 Sixth International Conference on Complex, Intelligent, and Software Intensive Systems","","2012","","","560","566","The current development of interactive systems is shifting its focus into adding new features and capabilities, encompassing for example, new input devices and ways of interacting. Some applications make use of different modalities for both input and output, which adds great complexity to such systems. Due to a possible high number of input modalities and devices available, the task of combining all the information conveyed by users from these sources, becomes critical and troublesome. This process is commonly referred to, as multi modal fusion, and is performed by fusion engines, components of multi modal systems, that have the purpose of receiving multiple streams of input, combine them, and reaching an interpretation of user intent. Evaluating the efficiency of this process is a task that requires a considerate amount of effort, due to all the variables involved. In this paper we present an evaluation framework and methodology aimed towards the assessment and optimization of fusion engines performance. We begin by correlating our work to project GUIDE, explaining how fusion is achieved and how information about user and context are used for adaptation. Subsequent sections discuss the proposed framework, and how it enables developers to assess and improve their components performance. The article concludes by showing some results that demonstrate the benefits of using such an approach and the impact it can have on the development of fusion engines.","","978-1-4673-1233-2978-0-7695-4687","10.1109/CISIS.2012.165","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6245628","performance evaluation;multimodal fusion;sensor fusion;fusion engine;optimization","Engines;Context;Testing;Speech;Speech recognition;Adaptation models;Optimization","interactive systems;sensor fusion","multimodal fusion engine assessment;multimodal fusion engine optimization;interactive systems;multi modal fusion;fusion engines;multi modal systems;GUIDE project","","1","13","","","","","","IEEE","IEEE Conferences"
"Speeding Up Physical Synthesis with Transactional Timing Analysis","D. Papa; M. Moffitt; C. Alpert; I. Markov","University of Michigan, Ann Arbor, and IBM Austin; IBM Austin Research Lab; IBM Austin Research Lab; University of Michigan , Ann Arbor","IEEE Design & Test of Computers","","2010","27","5","14","25","Modern physical-synthesis flows operate on very large designs and perform increasingly aggressive timing optimizations. Traditional incremental timing analysis now represents the single greatest bottleneck in such optimizations and lacks the features necessary to support them efficiently. This article describes a paradigm of transactional timing analysis, which, together with incremental updates, offers an efficient, nested undo functionality that avoids significant timing calculations.","0740-7475;1558-1918","","10.1109/MDT.2010.76","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5499453","design and test;algorithms;static timing analysis;timing-driven placement;physical synthesis;transactional timing analysis","Timing;Algorithm design and analysis;Performance analysis;Design optimization;Circuit optimization;Software design;Software algorithms;Runtime;Engines;Design engineering","application specific integrated circuits;integrated circuit design;timing","physical synthesis;transactional timing analysis;incremental timing analysis;application specific integrated circuits","","2","18","","","","","","IEEE","IEEE Journals & Magazines"
"On Performance Modeling and Prediction in Support of Scientific Workflow Optimization","Q. Wu; V. V. Datla","NA; NA","2011 IEEE World Congress on Services","","2011","","","161","168","The computing modules in distributed scientific workflows must be mapped to computer nodes in shared network environments for optimal workflow performance. Finding a good workflow mapping scheme critically depends on an accurate prediction of the execution time of each individual computational module in the workflow. The time prediction of a scientific computation does not have a silver bullet as it is determined collectively by several dynamic system factors including concurrent loads, memory size, CPU speed, and also by the complexity of the computational program itself. This paper investigates the problem of modeling scientific computations and predicting their execution time based on a combination of both hardware and software properties. We employ statistical learning techniques to estimate the effective computational power of a given computer node at any point of time and estimate the total number of CPU cycles needed for executing a given computational program on any input data size. We analytically derive an upper bound of the estimation error for execution time prediction given the hardware and software properties. The proposed statistical analysis-based solution to performance modeling and prediction is validated and justified by experimental results measured on the computing nodes that vary significantly in terms of the hardware specifications.","2378-3818","978-1-4577-0879-4978-0-7695-4461","10.1109/SERVICES.2011.37","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6012708","Performance modeling;scientific computation;regression techniques","Hardware;Software;Computational modeling;Predictive models;Estimation;Benchmark testing;Complexity theory","computational complexity;learning (artificial intelligence);multiprocessing systems;optimisation;scientific information systems;statistical analysis;workflow management software","performance modeling;scientific workflow optimization;computer node;workflow mapping scheme;CPU speed;memory size;computational program complexity;statistical learning technique","","7","37","","","","","","IEEE","IEEE Conferences"
"From behaviour preservation to behaviour modification: constraint-based mutant generation","F. Steimann; A. Thies","Lehrgebiet Programmiersysteme, Fakult&#x0E4;t f&#x0FC;r Mathematik und Informatik, Fernuniversit&#x0E4;t in Hagen, Hagen; Lehrgebiet Programmiersysteme, Fakult&#x0E4;t f&#x0FC;r Mathematik und Informatik, Fernuniversit&#x0E4;t in Hagen, Hagen","2010 ACM/IEEE 32nd International Conference on Software Engineering","","2010","1","","425","434","The efficacy of mutation analysis depends heavily on its capability to mutate programs in such a way that they remain executable and exhibit deviating behaviour. Whereas the former requires knowledge about the syntax and static semantics of the programming language, the latter requires some least understanding of its dynamic semantics, i.e., how expressions are evaluated. We present an approach that is knowledgeable enough to generate only mutants that are both syntactically and semantically correct and likely exhibit non-equivalent behaviour. Our approach builds on our own prior work on constraint-based refactoring tools, and works by negating behaviour-preserving constraints. As a proof of concept we present an enhanced implementation of the Access Modifier Change operator for Java programs whose naive implementations create huge numbers of mutants that do not compile or leave behaviour unaltered. While we cannot guarantee that our generated mutants are non-equivalent, we can demonstrate a considerable reduction in the number of vain mutant generations, leading to substantial temporal savings.","1558-1225;0270-5257","978-1-60558-719","10.1145/1806799.1806862","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062110","accessibility;constraints;mutation analysis;object-oriented programming;refactoring;testing","Semantics;Java;Syntactics;Testing;Optimization;Program processors;Programming","constraint handling;Java;programming language semantics;software maintenance;software tools","behaviour preservation;behaviour modification;constraint-based mutant generation;mutation analysis;program mutation;static programming language semantics;dynamic semantics;constraint-based refactoring tools;access modifier change operator;Java programs","","3","21","","","","","","IEEE","IEEE Conferences"
"Analysis and Optimization of Data Import with Hadoop","W. Xu; W. Luo; N. Woodward","NA; NA; NA","2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops & PhD Forum","","2012","","","1058","1066","Data driven research has become an important part of scientific discovery in an increasing number of disciplines. In many cases, the sheer volume of data to be processed requires not only state-of-the-art computing resources but also carefully tuned and specifically developed software. These requirements are often associated with huge operational costs and significant expertise in software development. Due to its simplicity for the user and effectiveness at processing big data, Hadoop has become a popular software platform for large-scale data analysis. Using a Hadoop cluster in a remote shared infrastructure enables users to avoid the costs of maintaining a physical infrastructure. An inevitable step in using dynamically constructed Hadoop cluster is the initial importing of the data. This process is not trivial, particularly when the size of the data is large. In this paper, we evaluate the costs of importing large-scale data into a Hadoop cluster. We present a detailed analysis of the default data importing implementation in Hadoop and conduct a practical evaluation. Our evaluation includes tests with different hardware configurations, such as different network protocol and disk configurations. We also propose an implementation to improve the performance of importing data into a Hadoop cluster wherein the data is accessed directly by Data nodes during the import process.","","978-1-4673-0974","10.1109/IPDPSW.2012.129","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6270755","cloud computing;data import;disk I/O;data transfter;Hadoop","Pipelines;Sockets;Data models;File systems;Throughput;Hardware;Computational modeling","data analysis;optimisation;software engineering","optimization;data import;data driven research;scientific discovery;state-of-the-art computing resources;software development;software platform;large-scale data analysis;Hadoop cluster","","4","13","","","","","","IEEE","IEEE Conferences"
"Optimizing blocks in an SoC using symbolic code-statement reachability analysis","H. Chou; K. Chang; S. Kuo","Electrical Engineering Department, National Taiwan University, Taipei, Taiwan; Avery Design Systems, Inc., Andover, MA, USA; Electrical Engineering Department, National Taiwan University, Taipei, Taiwan","2010 15th Asia and South Pacific Design Automation Conference (ASP-DAC)","","2010","","","787","792","Optimizing blocks in a System-on-Chip (SoC) circuit is becoming more and more important nowadays due to the use of third-party Intellectual Properties (IPs) and reused design blocks. In this paper, we propose techniques and methodologies that utilize abundant external don't-cares that exist in an SoC environment for block optimization. Our symbolic code-statement reachability analysis can extract don't-care conditions from constrained-random testbenches or other design blocks to identify unreachable conditional blocks in the design code. Those blocks can then be removed before logic synthesis is performed to produce smaller and more power-efficient final circuits. Our results show that we can optimize designs under different constraints and provide additional flexibility for SoC design flows.","2153-6961;2153-697X","978-1-4244-5765-6978-1-4244-5767","10.1109/ASPDAC.2010.5419784","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5419784","","Reachability analysis;Design optimization;Circuit testing;Circuit optimization;Circuit synthesis;Analytical models;Circuit simulation;Constraint optimization;Performance evaluation;Software performance","industrial property;logic design;reachability analysis;system-on-chip","symbolic code-statement reachability analysis;system-on-chip circuit;third-party intellectual properties;reused design blocks;block optimization;constrained-random testbenches;logic synthesis;power-efficient final circuits;SoC design;system-on-chip","","5","25","","","","","","IEEE","IEEE Conferences"
"Controller optimization in real time for a morphing wing in a Wind Tunnel","A. V. Popov; L. T. Grigorie; R. M. Botez; M. Mamou; Y. Mebarki","École de Technologie Supérieure, LARCASE, www.larcase.etsmtl.ca, Montréal, Québec, H3C 1K3, Canada; École de Technologie Supérieure, LARCASE, www.larcase.etsmtl.ca, Montréal, Québec, H3C 1K3, Canada; École de Technologie Supérieure, LARCASE, www.larcase.etsmtl.ca, Montréal, Québec, H3C 1K3, Canada; Institute for Aerospace Research, NRC, Ottawa, Ontario, K1A 0R6, Canada; Institute for Aerospace Research, NRC, Ottawa, Ontario, K1A 0R6, Canada","Melecon 2010 - 2010 15th IEEE Mediterranean Electrotechnical Conference","","2010","","","107","112","Wind Tunnel Test results of a real time optimization of a morphing wing in wind tunnel for delaying the transition towards the trailing edge are presented. A morphing rectangular finite aspect ratio wing, having a reference airfoil cross-section, was considered with its upper surface made of a flexible composite material and instrumented with Kulite pressure sensors, and two smart memory alloys actuators. Several wind tunnel tests runs for various Mach numbers, angles of attack and Reynolds numbers were performed in the 6'×9' wind tunnel at the Institute for Aerospace Research at the National Research Council Canada (IAR/NRC). Unsteady pressure signals were recorded and used as feed back in real time control while the morphing wing was requested to reproduce various optimized airfoils by changing automatically the two actuators strokes. The new optimization method was implemented into the control software code that allowed the morphing wing to adjust its shape to an optimum configuration under the wind tunnel airflow conditions.","2158-8473;2158-8481","978-1-4244-5795-3978-1-4244-5793-9978-1-4244-5794","10.1109/MELCON.2010.5476329","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5476329","","Automotive components;Aerospace testing;Automatic control;Shape control;Delay effects;Composite materials;Instruments;Intelligent sensors;Intelligent actuators;Performance evaluation","actuators;aerodynamics;aerospace components;closed loop systems;composite materials;feedback;laminar flow;Mach number;optimisation;pressure sensors;shape memory effects;turbulence;wind tunnels","real time optimization;morphing wing;delay;trailing edge;rectangular finite aspect ratio wing;airfoil cross-section;flexible composite material;Kulite pressure sensors;smart memory alloys actuators;Mach numbers;Reynolds numbers;attack angles;Institute for Aerospace Research;National Research Council;Canada;unsteady pressure signals;feedback;actuators strokes;control software code;wind tunnel airflow conditions;wind tunnel test;real time control","","1","7","","","","","","IEEE","IEEE Conferences"
"Low-cost, high flexibility I–V curve tracer for photovoltaic modules","J. J. Maestro Ibirriaga; X. M. de Mendiluce Pena; A. Opritescu; D. Sera; R. Teodorescu","Aalborg University, Institute of Energy Technology, 9220, Denmark; Aalborg University, Institute of Energy Technology, 9220, Denmark; Aalborg University, Institute of Energy Technology, 9220, Denmark; Aalborg University, Institute of Energy Technology, 9220, Denmark; Aalborg University, Institute of Energy Technology, 9220, Denmark","2010 12th International Conference on Optimization of Electrical and Electronic Equipment","","2010","","","1210","1215","This work presents the design, construction and test of an in-door low cost, high flexibility I-V curve tracer for photovoltaic modules. The tracer is connected to a Xenon lamp based flashing solar simulator. The designed tracer is able to deal with the very fast changing irradiation conditions and its control software offers the flexibility to automatically adapt to the different irradiation conditions set by the flashing solar simulator. Simulation and experimental tests have been carried out, in order to verify the behaviour and performance of the designed I-V curve tracer.","1842-0133;1842-0133","978-1-4244-7020-4978-1-4244-7019-8978-1-4244-7018","10.1109/OPTIM.2010.5510467","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5510467","","Photovoltaic systems;Solar power generation;Voltage;Diodes;Circuit testing;Electronic equipment testing;Equations;Design optimization;Electronic equipment;Modular construction","photovoltaic power systems;power system interconnection","photovoltaic modules;Xenon lamp based flashing solar simulator;irradiation conditions;I-V curve tracer","","3","9","","","","","","IEEE","IEEE Conferences"
"A software tool for performance metaheuristics evaluation in real time alternative routing selection in random FMSs","M. Souier; Z. Sari","Automatic Control Laboratory of Tlemcen, Abou bekr Belka&#x00EF;d University, PoBox 230, 13000, Algeria; Automatic Control Laboratory of Tlemcen, Abou bekr Belka&#x00EF;d University, PoBox 230, 13000, Algeria","2011 International Conference on Communications, Computing and Control Applications (CCCA)","","2011","","","1","6","With the advance of research, design and operation of flexible manufacturing systems (FMS) which are designed to produce a variety of different part types with high machine utilization, short lead times and little work-in-progress inventory, new requirements like high modeling efficiency, high model validity and credibility and effectiveness and correct analysis of results are defined. Simulation is an efficient tool to verify design concepts, to select machinery, to evaluate alternative configurations and to test system control strategies of an FMS, etc. This paper presents a useful tool for simulating the behavior of random FMSs. This simulator is used for real time alternative routing selection based on a group of metaheuristics principles which include in particular simulated annealing (SA), genetic algorithm (GA), taboo search (TS), ant colony algorithms (ACO), particle swarm optimization (PSO) and electromagnetism like method (EM). This software is also can used for performance and sensitivity analysis of theses techniques jugged by the production rate, machines and material handling utilization rate, the cycle time and the work in process.","","978-1-4244-9796-6978-1-4244-9795-9978-1-4244-9794","10.1109/CCCA.2011.6031460","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6031460","software simulation;flexible manufacturing systems;alternative routing;real time routing selection;metaheuristics","Routing;Data models;Production;Load modeling;Real time systems;Genetic algorithms;Loading","design engineering;flexible manufacturing systems;genetic algorithms;machinery;materials handling;particle swarm optimisation;production engineering computing;search problems;sensitivity analysis;simulated annealing","software tool;performance metaheuristics evaluation;real time alternative routing selection;flexible manufacturing system;machine utilization;modeling efficiency;model validity;model credibility;correct analysis;design concept verification;machinery;system control strategy testing;simulated annealing;genetic algorithm;taboo search;ant colony algorithms;particle swarm optimization;electromagnetism like method;sensitivity analysis;production rate;material handling utilization rate","","2","18","","","","","","IEEE","IEEE Conferences"
"Optimization of forming process for transiting part of combustion chamber","Winn Wah Wah Aye; Li Xiao Xing","School of Mechanical and Automation, Beijing University of Aeronautics and Astronautics (BUAA), China; School of Mechanical and Automation, Beijing University of Aeronautics and Astronautics (BUAA), China","2010 International Conference on Mechanical and Electrical Technology","","2010","","","327","332","In deep drawing, one of the sheet metal forming processes is a widely used technique for producing parts from sheet metal blanks in a variety of fields such as aerospace and automobile. Predicting the behaviors of deformation during a forming process is one of the main challenges in cold forming. Instead of the traditional method called trial and error process, numerical simulation based on finite element analysis method could be used to achieve a better understanding of forming deformation during the process and to predict tools for several failure modes to reduce the number of costly experimental verification tests. In this paper, optimization of forming process for transiting part of combustion chamber is mainly performed by ABAQUS software. The numerical simulation model was constructed by CAD software. This part structure is very complex and the defects such as wrinkles and springback occur during forming. The present research work is aimed at avoiding the wrinkles. Using numerical simulation technology, the influences of friction coefficient and blankholder force in wrinkle have been investigated and optimized values for these two important process parameters are suggested in order to eliminate wrinkles. Moreover two criterions for fracture (FLD criterion and reduction of thickness criterion) are briefly presented.","","978-1-4244-8102-6978-1-4244-8100","10.1109/ICMET.2010.5598376","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5598376","Numerical Simulation;Blankholder force;Friction Coefficients;Wrinkle","Numerical models;Analytical models;Deformable models;Heating;Automobiles;Educational institutions","cold working;deformation;finite element analysis;forming processes;fracture mechanics;optimisation;sheet metal processing;stress-strain relations","sheet metal forming process;combustion chamber;sheet metal blanks;cold forming;deformation behavior;trial-and-error process;numerical simulation;finite element analysis;ABAQUS software;CAD software;wrinkles defect;springback defect;fracture criterion;FLD criterion;thickness reduction criterion","","","8","","","","","","IEEE","IEEE Conferences"
"Virtual model optimization and locomotion control of bionic hexapod robot","J. Zhang; J. Wang; W. Chen; W. Chen","School of Automation Science and Electrical Engineering, Beijing University of Aeronautics &amp; Astronautics, 100191, China; School of Automation Science and Electrical Engineering, Beijing University of Aeronautics &amp; Astronautics, 100191, China; School of Automation Science and Electrical Engineering, Beijing University of Aeronautics &amp; Astronautics, 100191, China; Singapore Institute of Manufacturing Technology, 71 Nanyang Drive, Singapore 638075, Singapore","2012 7th IEEE Conference on Industrial Electronics and Applications (ICIEA)","","2012","","","497","501","With reference to current biology research achievements of hexapod insects, this paper presents the design and optimization of structure of the bionic hexapod robot, imitating the cockroach body shape and structure features. Tripod Gaits is chosen as the robot's gait which is the most stable and fast gait for hexapod robots. We evaluate the performance of our design by testing it on the simulation system based on Virtools, which is an interactive integrating platform for virtual prototype and virtual environment. As a result of the adoption of the software, this paper relates in detail the message passing mechanism and a method of adjusting torque factor dynamic parameters, based on actuator's movement need. The result verifies the rationality of the structure and the locomotion flexibility of the robot.","2156-2318;2158-2297","978-1-4577-2119-9978-1-4577-2118-2978-1-4577-2117","10.1109/ICIEA.2012.6360779","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6360779","Bionic hexapod robot;mechanism optimization;tripod gaits;simulation system","Legged locomotion;Torque;Servomotors;Gears;Optimization;Biology","control engineering computing;legged locomotion;message passing;motion control;optimisation;performance evaluation;robot dynamics;virtual prototyping;virtual reality","virtual model optimization;locomotion control;bionic hexapod robot;biology research achievements;hexapod insects;cockroach body shape;structure features;tripod gaits;robot gait;performance evaluation;simulation system;Virtools;interactive integrating platform;virtual prototype;virtual environment;software adoption;message passing mechanism;torque factor dynamic parameters;actuator movement need;robot locomotion flexibility","","1","15","","","","","","IEEE","IEEE Conferences"
"Quantifying the value of risk-mitigation measures for launch vehicles","E. L. Morse; J. R. Fragola","Valador, Inc., 560 Herndon Parkway, Suite 300 Herndon VA 20170 USA; Valador, Inc., 265 Sunrise Highway, Suite 22, Rockville Centre, New York 11570 USA","2011 Proceedings - Annual Reliability and Maintainability Symposium","","2011","","","1","7","The efficient development of a highly reliable system, such as a new crew launch vehicle, cannot afford to ignore the lessons of history. A number of interesting studies of launch vehicle failures provide very valuable, albeit qualitative “lessons learned” on measures that a risk-informed program should take. If schedule and funds were unlimited, a very intensive and exhaustive test program would be the course to follow before the first flight of a new launcher. But when a program is faced with stringent schedule and cost constraints, it needs to optimize its test planning so as to meet constraints without sacrificing safety. Making such trade-offs intelligently requires having a way to quantify the relationship between the initial unreliability of a system, and the array of risk-mitigating measures on hand. This paper proposes several analysis steps beyond the existing studies of historical launch vehicle failures, which can form the basis for quantifying the lessons of history. Firstly, risk cannot be quantified accurately by summing all failures across history, because systems were not exposed to the same design deficiencies at each flight. Early failures typically represent sources of high risk, which are eliminated by corrective actions after the early flights, while late failures are often indicative of low-risk, design deficiencies that remain present for many flights. Thus failures occurring in the early launches of a system actually represent more risk than failures occurring later in history. Quantifying historical risk properly requires taking into account the reality of reliability growth. Secondly, knowing what failed in the past does not provide direct guidance as to how to reduce the risk of a new design. Of utmost relevance are the kinds of measures that could have prevented the failures in the first place. Simplistically put, knowing that the majority of launch vehicle failures originated in propulsion systems is of limited use to designers and managers, who already pay tremendous attention to that central subsystem. By contrast, a quantification of the potential risk reduction possible by submitting an engine to stress testing, for example, could be valuable in supporting the cost and schedule trade-offs that decision makers are unavoidably faced with. This paper proposes a method for re-considering the failures of historical launchers in that new light and illustrates its application to two historical examples, the Ariane and Centaur systems. The results provide an approximate quantification of the risk reduction potentially offered by improvements in areas such as: sufficient flight-like testing at the system level; definition of, and testing for, margins that consider all phases of flight, including not only steady-state but also transient conditions; stress testing and testing for variability at the component and engine levels; analysis of the results of every single flight with an eye towards uncovering design defects: “post-success investigations” re-examination of the margins of all components and systems (including software) and re-qualification after every single change in design, configuration, or mission profile; and maintenance of very rigorous levels of electrical and cabling parts control, quality assurance and contamination control in all phases of manufacturing, assembly and launch operations. The authors hope that the techniques and insights presented in this paper can be of use to the aerospace industry as it embarks on the flight certification program for the next-generation crewed launcher.","0149-144X;0149-144X","978-1-4244-8856-8978-1-4244-8857-5978-1-4244-8855","10.1109/RAMS.2011.5754436","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5754436","launch vehicle;reliability;growth;test;risk mitigation;analysis","Testing;Engines;Reliability;History;Vehicles;Modeling;Stress","aerospace industry;failure analysis;maintenance engineering;propulsion;quality assurance;testing;vehicles","risk mitigation measure;crew launch vehicle;risk-informed program;intensive test program;exhaustive test program;test planning;launch vehicle failure;propulsion system;risk reduction;stress testing;flight-like testing;mission profile;maintenance;quality assurance;contamination control;aerospace industry;flight certification program;next generation crewed launcher","","2","19","","","","","","IEEE","IEEE Conferences"
"Co-design techniques for distributed real-time embedded systems with communication security constraints","K. Jiang; P. Eles; Z. Peng","Department of Computer and Information Science, Linköping University, Sweden; Department of Computer and Information Science, Linköping University, Sweden; Department of Computer and Information Science, Linköping University, Sweden","2012 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2012","","","947","952","In this paper we consider distributed real-time embedded systems in which confidentiality of the internal communication is critical. We present an approach to efficiently implement cryptographic algorithms by using hardware/software co-design techniques. The objective is to find the minimal hardware overhead and corresponding process mapping for encryption and decryption tasks of the system, so that the confidentiality requirements for the messages transmitted over the internal communication bus are fulfilled, and time constraints are satisfied. Towards this, we formulate the optimization problems using Constraint Logic Programming (CLP), which returns optimal solutions. However, CLP executions are computationally expensive and, hence, efficient heuristics are proposed as an alternative. Extensive experiments demonstrate the efficiency of the proposed heuristic approaches.","1558-1101;1530-1591;1530-1591","978-1-4577-2145-8978-3-9810801-8","10.1109/DATE.2012.6176633","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6176633","","Field programmable gate arrays;Encryption;Hardware;Software;Schedules","constraint handling;cryptography;embedded systems;hardware-software codesign;optimisation;telecommunication security","distributed real-time embedded system;internal communication security constraint;cryptographic algorithm;hardware-software codesign technique;minimal hardware overhead;decryption mapping processing;encryption mapping processing;message transmission;internal communication bus;time constraint;optimization problem;constraint logic programming;CLP","","15","18","","","","","","IEEE","IEEE Conferences"
"Thread-level speculation as an optimization technique in Web Applications — Initial results","J. K. Martinsen; H. Grahn","School of Computing, Blekinge Institute of Technology, SE-371 79 Karlskrona, Sweden; School of Computing, Blekinge Institute of Technology, SE-371 79 Karlskrona, Sweden","2011 6th IEEE International Symposium on Industrial and Embedded Systems","","2011","","","83","86","Web Applications have become increasingly popular as they allow developers to use an uniform platform for user interactions. The dynamic programming language JavaScript used in most Web Applications has performance penalties, that have been addressed by traditional optimization techniques. We have found that while the performance gain of such techniques are positive for a set of established benchmarks, it often fails to improve the performance of real-life Web Applications. We suggest Thread-Level Speculation (TLS) at the JavaScript function level to automatically extract parallelism to gain performance. There have been multiple TLS proposals in both hardware and software, but little work has been done within JavaScript. Currently we are implementing our TLS ideas in a state-of-the-art JavaScript engine targeted for embedded mobile devices.","2150-3109;2150-3117","978-1-61284-820-4978-1-61284-818-1978-1-61284-819","10.1109/SIES.2011.5953686","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5953686","JavaScript;Multithreading;Parallel Computing;Speculative execution;Runtime environment","Benchmark testing;Parallel processing;Instruction sets;Optimization;Engines;Java","dynamic programming;Java;multiprocessing systems;multi-threading","thread-level speculation;optimization technique;Web applications;user interactions;dynamic programming language;JavaScript engine;embedded mobile devices","","","22","","","","","","IEEE","IEEE Conferences"
"Investment optimization methodology applied to investments on non-technical losses reduction actions","S. S. Ribeiro; T. Cazes; R. F. Mano; D. Maia","Choice Technologies S.A., Rio de Janeiro, Brazil; NA; NA; NA","2012 IEEE Symposium on Computers and Communications (ISCC)","","2012","","","000354","000360","This electronic document presents the research developed on a R&D Project for the Energy Recovery Department of Rio de Janeiro's distribution company, Light S.E.S.A. The main purpose is prioritizing the grid investments on non-technical losses reduction actions. The work was developed along eighteen months, and resulted in experimental software. It analyses the historical results of the actions, allocating optimally the resources to a pre-defined period of time. The methodology consists of a statistic model based on historical results processed through a decision three optimization algorithm in order to maximize the objective function. It was tested on decision making process regarding grid investments to reduce non-technical losses, and the Return-on-Investment results were quite satisfactory.","1530-1346;1530-1346","978-1-4673-2713-8978-1-4673-2712-1978-1-4673-2711","10.1109/ISCC.2012.6249321","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6249321","optimization;statistic models energy losses recovery;energy theft;decision support methods","Investments;Inspection;Optimization;Estimation;Software;Databases;Geospatial analysis","electricity supply industry;investment;power distribution economics;power grids;statistical analysis","investment optimization methodology;nontechnical losses reduction actions;electronic document;R&D Project;Rio de Janeiro;distribution company;grid investments;pre-defined period of time;statistic model;optimization algorithm","","2","15","","","","","","IEEE","IEEE Conferences"
"BlockChop: Dynamic squash elimination for hybrid processor architecture","J. Mars; N. Kumar","University of Virginia, USA; Intel Corporation, USA","2012 39th Annual International Symposium on Computer Architecture (ISCA)","","2012","","","536","547","Hybrid processors are HW/SW co-designed processors that leverage blocked-execution, the execution of regions of instructions as atomic blocks, to facilitate aggressive speculative optimization. As we move to a multicore hybrid design, fine grained conflicts for shared data can violate the atomicity requirement of these blocks and lead to expensive squashes and rollbacks. However, as these atomic regions differ from those used in checkpointing and transactional memory systems, the extent of this potentially prohibitive problem remains unclear, and mechanisms to mitigate these squashes dynamically may be critical to enable a highly performant multicore hybrid design. In this work, we investigate how multithreaded applications, both benchmark and commercial workloads, are affected by squashes, and present dynamic mechanisms for mitigating these squashes in hybrid processors. While the current wisdom is that there is not a significant number of squashes for smaller atomic regions, we observe this is not the case for many multithreaded workloads. With region sizes of just 200 - 500 instructions, we observe a performance degradation ranging from 10% to more than 50% for workloads with a mixture of shared reads and writes. By harnessing the unique flexibility provided by the software subsystem of hybrid processor design, we present BlockChop, a framework for dynamically mitigating squashes on multicore hybrid processors. We present a range of squash handling mechanisms leveraging retrials, interpretation, and retranslation, and find that BlockChop is quite effective. Over the current response to exceptions and squashes in a hybrid design, we are able to improve the performance of benchmark and commercial workloads by 1.4x and 1.2x on average for large and small region sizes respectively.","1063-6897;1063-6897;1063-6897","978-1-4673-0476-4978-1-4673-0475-7978-1-4673-0473-3978-1-4673-0474","10.1109/ISCA.2012.6237046","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6237046","","Multicore processing;Hardware;Registers;Optimization;Software;Benchmark testing","checkpointing;concurrency control;hardware-software codesign;multiprocessing systems;multi-threading;optimisation","BlockChop;dynamic squash elimination;hybrid processor architecture;HW/SW co-designed processors;atomic blocks;aggressive speculative optimization;multicore hybrid design;checkpointing;transactional memory systems;multithreaded applications;performance degradation;software subsystem;squash handling mechanisms","","","32","","","","","","IEEE","IEEE Conferences"
"Development of Shared Aperture Dual Polarized Microstrip Antenna at L-Band","S. Chakrabarti","SAMEER Kolkata Centre, Salt Lake Electronics Complex, Kolkata, India","IEEE Transactions on Antennas and Propagation","","2011","59","1","294","297","A simple technique to develop dual feed dual linearly polarized aperture coupled planar microstrip patch antenna at L-band frequency has been presented. A 4 × 2 array of aperture coupled square resonant patch antenna has been designed, fabricated and tested. The design optimization is carried out using commercial method of moment (MoM) based simulation software. The inter element spacing, inter-layer separation, isolation between the feeding ports, and other associated antenna parameters are optimized by successive simulations using the commercial software. Measured gain is better than 14 dBi. Measured results are also presented.","0018-926X;1558-2221","","10.1109/TAP.2010.2091249","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5624573","Antenna arrays;aperture coupled antenna;microstrip antenna","Antenna measurements;Microstrip antennas;Bandwidth;Feeds;Aperture coupled antennas;Microstrip","antenna testing;aperture-coupled antennas;method of moments;microstrip antennas;multifrequency antennas;optimisation;planar antennas","shared aperture dual polarized planar microstrip antenna;L-band;dual feed dual linearly polarized antenna;aperture coupled square resonant patch antenna;design optimization;method of moment;simulation software;interelement spacing;interlayer separation;antenna parameters","","10","10","","","","","","IEEE","IEEE Journals & Magazines"
"Application of multi-criteria optimization algorithms to numerical material extraction of thin layers through nanoindentaion technique","Ł. Dowhań; A. Wymysłowski; P. Janus; M. Ekwińska; O. Wittler","Faculty of Microsystem Electronics and Photonics, Wroclaw University of Technology, Poland; Faculty of Microsystem Electronics and Photonics, Wroclaw University of Technology, Poland; Institute of Electron Technology, Warsaw, Poland; Institute of Electron Technology, Warsaw, Poland; Fraunhofer Institute IZM, Micro Materials Center, Berlin, Germany","3rd Electronics System Integration Technology Conference ESTC","","2010","","","1","7","Current developments and trends in microelectronics are focused on thin layers and novel materials. This leads to application of different test and measurement methods, which are capable to measure basic mechanical properties of such materials on micro-scale and nano-scale. The presented paper focuses on application of the nanoindentation technique in order to extract the basic elastic and elasto-plastic mechanical properties through numerical approaches. In order to extract the elasto-plastic material data of the investigated thin layers the numerical process was designed. First of all, the nanoindentation process was elaborated in FEM Abaqus software. Then, the results were compared to the measurements and processed by the numerical optimization algorithms.","","978-1-4244-8555-0978-1-4244-8553-6978-1-4244-8554","10.1109/ESTC.2010.5642971","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5642971","","Strain;Strain measurement","elasticity;elastoplasticity;finite element analysis;mechanical testing;nanoindentation;optimisation;silver","multicriteria optimization algorithms;numerical material extraction;thin layers;nanoindentation;microelectronics;elasto-plastic mechanical properties;elastic mechanical properties;FEM Abaqus software;numerical optimization algorithms;Ag","","","9","","","","","","IEEE","IEEE Conferences"
"Mapping loops onto Coarse-Grained Reconfigurable Architectures using Particle Swarm Optimization","R. Gnanaolivu; T. S. Norvell; R. Venkatesan","Faculty of Electrical and Computer Engineering, Memorial University of Newfoundland, St. John's, NL, Canada A1B 3×5; Faculty of Electrical and Computer Engineering, Memorial University of Newfoundland, St. John's, NL, Canada A1B 3×5; Faculty of Electrical and Computer Engineering, Memorial University of Newfoundland, St. John's, NL, Canada A1B 3×5","2010 International Conference of Soft Computing and Pattern Recognition","","2010","","","145","151","Coarse-Grained Reconfigurable Architectures (CGRAs) have gained currency in recent years due to their abundant parallelism and flexibility. To utilize the abundant parallelism found in CGRAs, we propose a fast and efficient Modulo-Constrained Hybrid Particle Swarm Optimization (MCHPSO) scheduling algorithm to exploit loop level parallelism in applications. PSO has been proved to be successful in many applications in continuous optimization problems. In this paper, we show that PSO is capable of software pipelining loops by overlapping placement, scheduling and routing of successive loop iterations and executing them in parallel. Our proposed algorithm has been experimentally validated on various DSP benchmarks under two different architecture configurations. These experiments indicate that the proposed MCHPSO algorithm can find schedules with small initiation intervals within a reasonable amount of time. PSO is thus a promising alternative for obtaining near optimal solutions to this NP-hard scheduling problem.","","978-1-4244-7896-5978-1-4244-7897-2978-1-4244-7895","10.1109/SOCPAR.2010.5685969","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5685969","Coarse-Grained Reconfigurable Architectures;Particle Swarm Optimization;Modulo Scheduling;Loop level parallelism;Mapping","Schedules;Routing;Benchmark testing;Scheduling algorithm;Computer architecture;Scheduling","computational complexity;parallel architectures;particle swarm optimisation;pipeline processing;program control structures;reconfigurable architectures;scheduling","coarse grained reconfigurable architecture;modulo constrained hybrid particle swarm optimization;loop level parallelism;software pipelining loop;successive loop iteration routing;DSP;NP hard scheduling","","3","14","","","","","","IEEE","IEEE Conferences"
"Optimization of stateful hardware acceleration in hybrid architectures","X. Chang; Y. Ma; H. Franke; K. Wang; R. Hou; H. Yu; T. Nelms","IBM Research-China; Institute of Computing Technology, Chinese Academy of Science; IBM Watson Research Center; IBM Research-China; IBM Research-China; IBM Watson Research Center; IBM Software Group","2011 Design, Automation & Test in Europe","","2011","","","1","4","In many computing domains, hardware accelerators can improve throughput and lower power consumption, instead of executing functionally equivalent software on the general-purpose micro-processors cores. While hardware accelerators often are stateless, network processing exemplifies the need for stateful hardware acceleration. The packet oriented streaming nature of current networks enables data processing as soon as packets arrive rather than when the data of the whole network flow is available. Due to the concurrence of many flows, an accelerator must maintain and switch contexts between many states of the various accelerated streams embodied in the flows, which increases overhead associated with acceleration. We propose and evaluate dynamic reordering of requests of different accelerated streams in a hybrid on-chip/memory based request queue in order to reduce the associated overhead.","1558-1101;1530-1591;1530-1591","978-3-9810801-8-6978-1-61284-208-0978-3-9810801-7","10.1109/DATE.2011.5763285","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5763285","","Acceleration;Context;Throughput;Time factors;System-on-a-chip;Hardware;Switches","computer architecture;general purpose computers;microprocessor chips;multiprocessing systems;optimisation","optimization;stateful hardware acceleration;hybrid architectures;power consumption;general-purpose micro-processors cores;network processing;packet oriented streaming nature;data processing;dynamic reordering","","","5","","","","","","IEEE","IEEE Conferences"
"A DFT Methodology for Repairing Embedded Memories of Large MPSoCs","K. Ganeshpure; S. Kundu","NA; NA","2012 IEEE Computer Society Annual Symposium on VLSI","","2012","","","108","113","Memory Built-In Self-Test (MBIST) is used to test large memories embedded in Multi-Processor System on Chip (MPSoC). With increase in memory size, memory repair becomes necessary to improve yield. Memory repair consists of complex offline analysis requiring (i) fault diagnosis and (ii) optimizing reconfiguration based on failure map and available spare resources. This paper presents an embedded repair scheme that uses resources within a MPSoC. The main challenge involves establishing integrity of such internal resources before they are used for repair. We propose a layered approach to testing that (i) tests local processor cache first and uses (ii) software based self-testing of limited processor functions using (iii) a small program loaded into a cache from tester, which then (iv) serves as a vehicle for memory repair. This repaired memory can store and run larger software-based self-test programs to test the remaining systems. Software simulation is used to demonstrate feasibility of the proposed DFT scheme and test methodology. The main advantages of this approach are (i) avoidance of memory testers that are typically necessary for memory repair, (ii) avoiding additional hardware to support repair by using existing resources and (iii) testing all components using a logic tester.","2159-3477;2159-3469;2159-3469","978-1-4673-2234-8978-0-7695-4767","10.1109/ISVLSI.2012.17","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6296457","","Maintenance engineering;Redundancy;Hardware;Software;Built-in self-test;Computer aided manufacturing","memory architecture;microprocessor chips;multiprocessing systems;system-on-chip","DFT methodology;embedded memory;MPSoC;memory built-in self-test;multiprocessor system on chip;memory size;memory repair;complex offline analysis;fault diagnosis;reconfiguration;failure map;available spare resource;embedded repair;local processor cache;software based self-testing;limited processor function;software based self-test program;software simulation;memory tester;logic tester","","1","19","","","","","","IEEE","IEEE Conferences"
"Decision tree algorithm based on average Euclidean distance","Q. Liu; D. Hu; Q. Yan","JiangSu Province Support Software Engineering R&amp;D Center for Information Technology Application in Enterprise, Suzhou, Jiangsu, 215006, China; JiangSu Province Support Software Engineering R&amp;D Center for Information Technology Application in Enterprise, Suzhou, Jiangsu, 215006, China; School of Computer Science and Technology, Soochow University, Suzhou, Jiangsu, 215006, China","2010 2nd International Conference on Future Computer and Communication","","2010","1","","V1-507","V1-511","Traditionally, the algorithm of ID3 takes the information gain as a standard of expanding attributes. During the process of selection of expanded attributes, attributes with more values are usually preferred to be selected. To solve such problem, a kind of AED algorithm based on average Euclidean distance in decision tree is proposed in this paper. The algorithm uses the average Euclidean distance as heuristic information. The experiment results show that the improved AED algorithm can avoid the variety bias of ID3 algorithm, and has no worse classification precision and less time cost than ID3.","","978-1-4244-5824-0978-1-4244-5821-9978-1-4244-5823","10.1109/ICFCC.2010.5497736","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5497736","decision tress;ID3 algorithm;variety bias;average Euclidean distance","Decision trees;Euclidean distance;Classification tree analysis;Algorithm design and analysis;Costs;Merging;Testing;Information entropy;Software engineering;Research and development","decision trees;optimisation;pattern classification","decision tree algorithm;average Euclidean distance;ID3 algorithm;AED algorithm;heuristic information;classification algorithm","","1","10","","","","","","IEEE","IEEE Conferences"
"Automated Derivation of Application-Aware Error Detectors Using Static Analysis: The Trusted Illiac Approach","K. Pattabiraman; Z. Kalbarczyk; R. K. Iyer","Microsoft Research, Redmond; Center for Reliable and High-Performance Computing, Urbana; Center for Reliable and High-Performance Computing, Urbana","IEEE Transactions on Dependable and Secure Computing","","2011","8","1","44","57","This paper presents a technique to derive and implement error detectors to protect an application from data errors. The error detectors are derived automatically using compiler-based static analysis from the backward program slice of critical variables in the program. Critical variables are defined as those that are highly sensitive to errors, and deriving error detectors for these variables provides high coverage for errors in any data value used in the program. The error detectors take the form of checking expressions and are optimized for each control-flow path followed at runtime. The derived detectors are implemented using a combination of hardware and software and continuously monitor the application at runtime. If an error is detected at runtime, the application is stopped so as to prevent error propagation and enable a clean recovery. Experiments show that the derived detectors achieve low-overhead error detection while providing high coverage for errors that matter to the application.","1545-5971;1941-0018;2160-9209","","10.1109/TDSC.2009.23","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5089331","Error checking;reliability;testing;and fault tolerance;reconfigurable hardware;software engineering (reliability);fault tolerance.","Detectors;Computer crashes;Application software;Runtime;Hardware;Fault tolerance;Computer bugs;Protection;Program processors;Error correction","error detection;program compilers;program slicing;security of data;software fault tolerance;software reliability","application-aware error detectors;trusted Illiac approach;data error protection;compiler-based static analysis;backward program slice;control-flow path;error propagation;low-overhead error detection;software engineering reliability;fault tolerance","","14","40","","","","","","IEEE","IEEE Journals & Magazines"
"Link-time optimization for power efficiency in a tagless instruction cache","T. M. Jones; S. Bartolini; J. Maebe; D. Chanet","School of Informatics, University of Edinburgh, United Kingdom, Sandro Bartolini, Faculty of Engineering, University of Siena, Italy; Faculty of Engineering, University of Siena, Italy; ELIS Department, Ghent University, Belgium, Dominique Chanet, Gateway Architecture Group, Technicolor, Belgium; Gateway Architecture Group, Technicolor, Belgium","International Symposium on Code Generation and Optimization (CGO 2011)","","2011","","","32","41","The instruction cache is a critical component in any microprocessor. It must have high performance to enable fetching of instructions on every cycle. However, current designs waste a large amount of energy on each access as tags and data banks from all cache ways are consulted in parallel to fetch the correct instructions as quickly as possible. Existing approaches to reduce this overhead remove unnecessary accesses to the data banks or to the ways that are not likely to hit. However, tag hunks still need to be checked. This paper considers a new hybrid hardware and linker-assisted approach to tagless instruction caching. Our novel cache architecture, supported by the compilation toolchain, removes the need for tag checks entirely for the majority of cache accesses. The linker places frequently-executed instructions in specific program regions that are then mapped into the cache without the need for tag checks. This requires minor hardware modifications, no ISA changes and works across cache configurations. Our approach keeps the software and hardware independent, resulting in both backward and forward compatibility. evaluation on a superscalar processor with and without SMI' support shows power savings of 66% within the instruction cache with no loss of performance. This translates to a 49% saving when considering the combined power of the instruction cache and translation lookaside buffer, which is involved in managing our tagless scheme.","","978-1-61284-357-5978-1-61284-356-8978-1-61284-358","10.1109/CGO.2011.5764672","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5764672","","Program processors;Hardware;Benchmark testing;Clustering algorithms;Arrays;Logic gates;Aerospace electronics","cache storage;microprocessor chips;multi-threading;optimisation;power aware computing","microprocessor;tagless instruction caching;power efficiency;link time optimization;data banks;tag banks;compilation toolchain;superscalar processor;SMT;translation lookaside buffer;hybrid hardware-linker assisted approach","","2","38","","","","","","IEEE","IEEE Conferences"
"Software-defined Radio based On Cortex-A9","Xin Wei; Xin Qi; Limin Xiao; Zhiyuan Shi; Lianfen Huang","School of Information Science &amp;Technology, Xiamen University, Beijing China; State Key Laboratory on Microwave and Digital Communications Tsinghua National, Laboratory for Information Science and Technology, Tsinghua University, Beijing China; State Key Laboratory on Microwave and Digital Communications Tsinghua National, Laboratory for Information Science and Technology, Tsinghua University, Beijing China; School of Information Science &amp;Technology, Xiamen University, Beijing China; School of Information Science &amp;Technology, Xiamen University, Beijing China","7th International Conference on Communications and Networking in China","","2012","","","758","761","This paper presents an implementation of Software-defined Radio (SDR) system over ARM Cortex-A9 processor. Compared with conventionally used general purpose CPU, e.g. X86 CPU, ARM processors have advantages in both price and power consumption under the same computing capability. Two basic issues of SDR over ARM are investigated: the real-time operation and signal processing efficiency. For the former issue, we select the Linux-based real-time operating system Xenomai, which is built on Cortex-A9 platform. Testing results show that the performance of Xenomai on Cortex-A9 meets the real-time requirement of SDR. Furthermore, the NEON technology of Cortex-A9 is used to optimize the complex signal processing algorithms in communication systems. Soft Demodulation is taken as an example for the optimization, and the experimental results are provided to show the improvement of signal processing efficiency.","","978-1-4673-2699-5978-1-4673-2698-8978-1-4673-2697","10.1109/ChinaCom.2012.6417585","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6417585","SDR;Xenomai;Cortex-A9;Soft Demodulation;NEON","Real-time systems;Linux;Demodulation;Operating systems;Hardware;Signal processing algorithms","Linux;signal processing;software radio;telecommunication computing","software-defined radio;SDR system;ARM Cortex-A9 processor;general purpose CPU;X86 CPU;power consumption;computing capability;signal processing efficiency;Linux-based realtime operating system Xenomai;NEON technology;communication systems;soft demodulation","","1","9","","","","","","IEEE","IEEE Conferences"
"How software engineering can benefit from traditional industries — A practical experience report (Invited industrial talk)","T. Sprenger","AdNovum Informatik, Zurich, Switzerland","2012 34th International Conference on Software Engineering (ICSE)","","2012","","","1000","1000","To be competitive in today's market, the IT industry faces many challenges in the development and maintenance of enterprise information systems. Engineering these largescaled systems efficiently requires making decisions about a number of issues. In addition, customers expectations imply continuous software delivery in predictable quality. The operation such systems demands for transparency of the software in regard to lifecycle, change and incident management as well as cost efficiency. Addressing these challenges, we learned how to benefit from traditional industries. Contrary to the fact that the IT business calls itself gladly an industry, the industrialization of software engineering in most cases moves on a rather modest level. Industrialization means not only to build a solution or product on top of managed and well-defined processes, but also to have access to structured information about the current conditions of manufacturing at any time. Comparably with test series and assembly lines of the automobile industry, each individual component and each step from the beginning of manufacturing up to the final product should be equipped with measuring points for quality and progress. Even one step further the product itself, after it has left the factory, should be able to continuously provide analytic data for diagnostic reasons. Information is automatically collected and builds the basic essentials for process control, optimization and continuous improvement of the software engineering process. This presentation shows by means of a practical experience report how AdNovum managed to build its software engineering based on a well-balanced system of processes, continuous measurement and control — as well as a healthy portion of pragmatism. We implemented an efficient and predictable software delivery pipeline based on five cornerstones that enables us to ship more than 1500 customer deliveries per year.","1558-1225;0270-5257;0270-5257","978-1-4673-1067-3978-1-4673-1066-6978-1-4673-1065","10.1109/ICSE.2012.6227249","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227249","","","","","","","","","","","","","IEEE","IEEE Conferences"
"Fault identification with binary adaptive fireflies in parallel and distributed systems","R. Falcon; M. Almeida; A. Nayak","School of Information Technology and Engineering, University of Ottawa, 800 King Edward Ave., Ottawa ON, Canada K1N 6N5; School of Information Technology and Engineering, University of Ottawa, 800 King Edward Ave., Ottawa ON, Canada K1N 6N5; School of Information Technology and Engineering, University of Ottawa, 800 King Edward Ave., Ottawa ON, Canada K1N 6N5","2011 IEEE Congress of Evolutionary Computation (CEC)","","2011","","","1359","1366","The efficient identification of hardware and software faults in parallel and distributed systems still remains a serious challenge in today's most prolific decentralized environments. System-level fault diagnosis is concerned with the detection of all faulty nodes in a set of interconnected units. This is accomplished by thoroughly examining the collection of outcomes of all tests carried out by the nodes under a particular test model. Such task has non-polynomial complexity and can be posed as a combinatorial optimization problem, whose optimal solution has been sought through bio-inspired methods like genetic algorithms, ant colonies and artificial immune systems. In this paper, we employ a swarm of artificial fireflies to quickly and reliably navigate across the search space of all feasible sets of faulty units under the invalidation and comparison test models. Our approach uses a binary encoding of the potential solutions (fireflies), an adaptive light absorption coefficient to accelerate the search and problem-specific knowledge to handle infeasible solutions. The empirical analysis confirms that the proposed algorithm outperforms existing techniques in terms of convergence speed and memory requirements, thus becoming a viable approach for real-time fault diagnosis in large-size systems.","1941-0026;1089-778X","978-1-4244-7835-4978-1-4244-7834-7978-1-4244-7833","10.1109/CEC.2011.5949774","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5949774","fault diagnosis;firefly optimization;swarm intelligence;invalidation model;comparison model","Fault diagnosis;Optimization;Fires;Mathematical model;Encoding;Complexity theory;Memory management","artificial intelligence;encoding;parallel processing;program testing;search problems;software fault tolerance","binary adaptive fireflies;parallel systems;distributed systems;hardware-software fault identification;system-level fault diagnosis;interconnected units;nonpolynomial complexity;combinatorial optimization problem;bio-inspired methods;genetic algorithms;ant colonies;artificial immune systems;artificial swarm fireflies;binary encoding;adaptive light absorption coefficient;convergence speed;memory requirements;large-size systems","","21","22","","","","","","IEEE","IEEE Conferences"
"Simulations for wind, solar and pumping facilities and hybrid systems design with KOGAION","D. Capatina; S. Ignat; A. Calarasu; I. Stoian; O. Capatina","Software department, SC IPA SA, Cluj Subsidiary, Napoca, Romania; Software department, SC IPA SA, Cluj Subsidiary, Napoca, Romania; Renewable department, SC IPA SA, Cluj Subsidiary, Napoca, Romania; Renewable department, SC IPA SA, Cluj Subsidiary, Napoca, Romania; Renewable department, SC IPA SA, Cluj Subsidiary, Napoca, Romania","Proceedings of 2012 IEEE International Conference on Automation, Quality and Testing, Robotics","","2012","","","621","626","KOGAION is a computer modeling application that simplifies the task of designing distributed renewable energy generation systems - both on and off-grid. KOGAION's optimization and sensitivity analysis algorithms allows to evaluate the economic and technical feasibility of a photovoltaic, wind and hydro turbines technology options and to account for variations in technology costs and energy resource availability. It's adaptable to a wide variety of projects like wind, photovoltaic and hybrid energy systems, both for large on grid projects or small hybrid isolated projects (for a village, community-scale power systems or individual systems). KOGAION can model both the technical and economic factors involved in the project. For larger systems, it can provide an important overview that compares the cost and feasibility of different configurations. It is accessible to broad spectrum of users, from financial decision makers to engineers and others. More than other software applications, Kogaion supports all types of wind descriptions: by Weibull coefficients, probability of density function (pdf), time series (TS). Time references simulation is crucial for modeling variable resources, such as wind power. KOGAION's sensitivity analysis helps to determine the potential impact of uncertain factors wind speed on a given system, over time. It doesn't cover bio fuel, hydrogen, fuel cells.","","978-1-4673-0704-8978-1-4673-0701-7978-1-4673-0703","10.1109/AQTR.2012.6237785","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6237785","CAD;renewable;financial outputs","Wind turbines;Estimation;Solid modeling;Wind;Photovoltaic systems;Time series analysis","hybrid power systems;photovoltaic power systems;power generation economics;power grids;power system simulation;pumped-storage power stations;sensitivity analysis;time series;Weibull distribution;wind power plants","computer modeling application;distributed renewable energy generation system design;KOGAION optimization;sensitivity analysis algorithm;photovoltaic power system;wind turbines;hydro turbines technology;energy resource availability;hybrid energy system design;hybrid isolated project;grid project;economic factors;broad spectrum;Weibull coefficient;probability of density function;pdf;time series;TS;time reference simulation;wind speed","","","5","","","","","","IEEE","IEEE Conferences"
"A Dependability Solution for Homogeneous MPSoCs","X. Zhang; H. G. Kerkhoff","NA; NA","2011 IEEE 17th Pacific Rim International Symposium on Dependable Computing","","2011","","","53","62","Nowadays highly dependable electronic devices are demanded by many safety-critical applications. Dependability attributes such as reliability and availability/maintainability of a many-processor system-on-chip (MPSoC) should already be examined at the design phase. Design for dependability approaches such as using available fault-free processor-cores and introducing a dependability manager infrastructural IP for self-test and evaluation can greatly enhance the dependability of an MPSoC. This is further supported by subsequent software-based repair. Design choices such as test fault coverage, test and repair time are examined to optimize the dependability attributes. Utilizing existing infrastructures like a network-on-chip (NoC) and tile-wrappers are needed to ensure a test can be performed at application run-time. An example design following the proposed design for dependability approach is shown. The MPSoC has been processed and measurement results have validated the proposed dependability approach.","","978-1-4577-2005-5978-0-7695-4590","10.1109/PRDC.2011.16","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6133066","MPSoC;dependability;fault-tolerance;self-repair;reliability;availability;embedded instruments;NoC (TAM);self-test","Built-in self-test;Availability;Circuit faults;System-on-a-chip;Vectors;Maintenance engineering","automatic testing;integrated circuit design;integrated circuit reliability;microprocessor chips;system-on-chip","homogeneous MPSoC;electronic devices;safety-critical applications;many-processor system-on-chip;fault-free processor-cores;dependability manager infrastructural IP;self-test;software-based repair;test fault coverage;repair time;test time;dependability attributes;network-on-chip;tile-wrappers","","4","28","","","","","","IEEE","IEEE Conferences"
"Simulation for Wastewater Treatment Plant WWTP","A. Alalewi; C. Jiang","NA; NA","2010 4th International Conference on Bioinformatics and Biomedical Engineering","","2010","","","1","4","At present, activated sludge model has great value and can be used to improve the study, design and optimum operation of wastewater treatment around the world. It has been applied widely in the world. In this paper, the application and calibration of Activated Sludge Model No.l in the practical wastewater treatment plant had been implemented. The characterization and parameter of the outfluent from the rotational flow grit chamber had been measured in the WWTP. The actual operation procedure is simulated in case of both steady and dynamic status using simulation software named as WEST, which is from DHI. Where after different simulation analysis is done to analyses the problems exist in this factory, and hence based on the analysis result, the optimized suggestion is arised. The result shows that the method of characterization and parameter in this paper can is practical. The heterotrophic yield coefficient Y<sub>H</sub>, Heterotrophic max specific growth rateμ<sub>max</sub>, heterotrophic decay rate b<sub>H</sub>, readily biodegradable substrate Ss and slowly biodegradable substrate X<sub>S</sub>could be characterized by the method of batch OUR respirometric experiments which is simple to calculate, and especially the outcome date is stable.The WEST software was used to simulate the real operation of the WWTP, the result shows that using the combination of test mensuration and model calibration, good simulation results were achieved though changing of the model parameters.The activated sludge model which was promoted by the IAWQ can be used as a useful tool in operation management, technical improvement and scientific research in WWTPs. Nevertheless WEST software which was based on the activated sludge model could simulate the performance of activated sludge process and provide valuable advices for directing WWTPs.","2151-7622;2151-7614;2151-7614","978-1-4244-4712-1978-1-4244-4713","10.1109/ICBBE.2010.5518275","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5518275","","Wastewater treatment;Calibration;Biodegradable materials;Fluid flow measurement;Rotation measurement;Analytical models;Production facilities;Software tools;Software testing;Software performance","biodegradable materials;calibration;sludge treatment;wastewater treatment","wastewater treatment plant;activated sludge model;outfluent;rotational flow grit chamber;WEST simulation software;heterotrophic yield coefficient;heterotrophic max specific growth;heterotrophic decay rate;readily biodegradable substrate;slowly biodegradable substrate;OUR respirometric experiments;model calibration;operation management;technical improvement;scientific research;test mensuration","","","7","","","","","","IEEE","IEEE Conferences"
"Improving Performance of GPU Specific OpenCL Program on CPUs","Q. Lan; C. Xun; M. Wen; H. Su; L. Liu; C. Zhang","NA; NA; NA; NA; NA; NA","2012 13th International Conference on Parallel and Distributed Computing, Applications and Technologies","","2012","","","356","360","OpenCL provides unified programming interface for various parallel computing platforms. The OpenCL framework manifests good functional portability, the programs can be run on platforms supporting OpenCL programming without any modification. However, most of the OpenCL programs are optimized for massively parallel processors, such as GPU, it's hard to achieve good performance on general multi-core processors without sophisticate modification to the GPU specific OpenCL programs. The major reason is the immense gap between CPU and GPU architecture. In this paper, we evaluate the performance portability of OpenCL programs between CPU and GPU, and analyse the reasons why GPU specific OpenCL programs are not fit for CPU. Based on the profiling, we proposed three optimization strategies for improving performance of GPU specific OpenCL programs on CPU, including increasing the granularity of task partition, optimizing the usage of memory hierarchy and block-based data accessing. In addition, we applied the proposed techniques on several benchmarks. The experimental results show that the performance of the optimized OpenCL programs achieve high performance in terms of speedup ratio from 2 to 4 on CPUs, when compared with their corresponding GPU specific ones.","2379-5352","978-0-7695-4879-1978-0-7695-4879","10.1109/PDCAT.2012.81","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6589305","OpenCL;functional portability;performance portability;GPU specific","Graphics processing units;Benchmark testing;Programming;Computer architecture;Computational modeling;Optimization","graphics processing units;multiprocessing programs;optimisation;parallel programming;software performance evaluation;software portability","GPU specific OpenCL program performance improvement;unified programming interface;parallel computing platforms;functional portability;massively parallel processors;CPU architecture;OpenCL program portability performance evaluation;profiling;task partition granularity;memory hierarchy usage optimization strategies;block-based data access","","3","8","","","","","","IEEE","IEEE Conferences"
"Statistical debugging with elastic predicates","R. Gore; P. F. Reynolds; D. Kamensky","Dept. of Computer Science, University of Virginia, Charlottesville, USA; Dept. of Computer Science, University of Virginia, Charlottesville, USA; Institute of Computational Engineering and Sciences, University of Texas at Austin, USA","2011 26th IEEE/ACM International Conference on Automated Software Engineering (ASE 2011)","","2011","","","492","495","Traditional debugging and fault localization methods have addressed localization of sources of software failures. While these methods are effective in general, they are not tailored to an important class of software, including simulations and computational models, which employ floating-point computations and continuous stochastic distributions to represent, or support evaluation of, an underlying model. To address this shortcoming, we introduce elastic predicates, a novel approach to predicate-based statistical debugging. Elastic predicates introduce profiling of values assigned to variables within a failing program. These elastic predicates are better predictors of software failure than the static and uniform predicates used in existing techniques such as Cooperative Bug Isolation (CBI). We present experimental results for established fault localization benchmarks and widely used simulations that show improved effectiveness.","1938-4300","978-1-4577-1639-3978-1-4577-1638","10.1109/ASE.2011.6100107","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6100107","automated debugging;fault localization","Computational modeling;Software;Benchmark testing;Presses;Optimized production technology;Stochastic processes;Debugging","floating point arithmetic;program debugging;software fault tolerance","statistical debugging;elastic predicates;fault localization methods;software failures;floating point computations;continuous stochastic distributions;failing program;cooperative bug isolation","","8","23","","","","","","IEEE","IEEE Conferences"
"Bringing Introspection Into the BlobSeer Data-Management System Using the MonALISA Distributed Monitoring Framework","A. Carpen-Amarie; J. Cai; A. Costan; G. Antoniu; L. Bougé","NA; NA; NA; NA; NA","2010 International Conference on Complex, Intelligent and Software Intensive Systems","","2010","","","508","513","Introspection is the prerequisite of an autonomic behavior, the first step towards a performance improvement and a resource-usage optimization for large-scale distributed systems. In grid environments, the task of observing the application behavior is assigned to monitoring systems. However, most of them are designed to provide general resource information and do not consider specific information for higher-level services. More specifically, in the context of data-intensive applications, a specific introspection layer is required in order to collect data about the usage of storage resources, about data access patterns, etc. This paper discusses the requirements for an introspection layer in a data-management system for large-scale distributed infrastructures. We focus on the case of BlobSeer, a large-scale distributed system for storing massive data. The paper explains why and how to enhance BlobSeer with introspective capabilities and proposes a three-layered architecture relying on the MonALISA monitoring framework. This approach has been evaluated on the Grid'5000 testbed, with experiments that prove the feasibility of generating relevant information related to the state and the behavior of the system.","","978-1-4244-5918-6978-1-4244-5917-9978-0-7695-3967","10.1109/CISIS.2010.37","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5447468","Distributed system;storage management;large-scale system;monitoring;introspection","Large-scale systems;Computer science;Competitive intelligence;Software performance;Software systems;Computerized monitoring;Application software;System testing;Mesh generation;State feedback","distributed databases;grid computing;information retrieval systems;monitoring;optimisation;resource allocation","blobseer data management system;MonALISA distributed monitoring framework;resource-usage optimization;distributed systems;monitoring system;introspection layer;massive data storage","","3","14","","","","","","IEEE","IEEE Conferences"
"Exploring Query Optimization in Programming Codes by Reducing Run-Time Execution","V. K. S. Nerella; S. Surapaneni; S. K. Madria; T. Weigert","NA; NA; NA; NA","2010 IEEE 34th Annual Computer Software and Applications Conference","","2010","","","407","412","Object querying is an abstraction of operations over collections, whereas manual implementations are performed at low level which forces the developers to specify how a task must be done. Some object-oriented languages allow the programmers to express queries explicitly in the code, which are optimized using the query optimization techniques from the database domain. In this regard, Java Query Language has been developed that allows object querying and performs the query optimization at run-time. Therefore, only one problem is how to reduce the task of query optimization at run-time as much as possible within the Java Query Language system. In this paper, we have developed a technique that performs query optimization at compile-time to reduce the burden of optimization at run-time to improve the performance of the code execution. The proposed approach uses histograms that are computed from the data and these histograms are used to get the estimate of selectivity for query joins and predicates in a query at compile-time. With these estimates, a query plan is constructed at compile-time and executed it at run-time. The experimental trials show that our method performs better in terms of run time comparisons than the existing query optimization techniques used in the Java Query Language.","0730-3157;0730-3157;0730-3157","978-1-4244-7513-1978-1-4244-7512-4978-0-7695-4085","10.1109/COMPSAC.2010.48","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676286","selectivity;joins;compile time;run-time;histograms;query optimization","Histograms;Query processing;Java;Database languages;Benchmark testing;Frequency estimation","object-oriented programming;query languages","query optimization;programming codes;run time execution;object oriented languages;object querying;Java Query Language system","","2","22","","","","","","IEEE","IEEE Conferences"
"Validating Service Value Propositions Regarding Stakeholder Preferences","E. Wittern; C. Zirpins","NA; NA","2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops","","2011","","","294","297","In service science, the design of services' value propositions is a major concern. A service's value proposition needs to best possibly meet the customers' requirements and preferences. We envision an approach of validating value propositions regarding stakeholder preferences. The approach proposes the modeling of value propositions using Service Feature Modeling. Stakeholder preferences are then collected based on the created model. Finally, the model and the collected preferences are unitedly evaluated and optimizations are identified under consideration of potential adaptation expenses.","","978-0-7695-4345-1978-1-4577-0019","10.1109/ICSTW.2011.67","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954422","Service Science;Feature Modeling;Preferences","Adaptation models;Cognition;Servers;Software;Feature extraction;Throughput;Decision making","customer satisfaction;customer services","service value proposition;stakeholder preference;service science;customer requirement;customer preference;service feature modeling","","2","12","","","","","","IEEE","IEEE Conferences"
"A wireless sensor network deployment model with target localization constraints","S. Chamberland; L. Cobo; F. Mourchid; A. Quintero","Department of Computer and Software Engineering, École Polytechnique de Montréal, (Québec), Canada; Department of Computer and Software Engineering, École Polytechnique de Montréal, (Québec), Canada; Department of Computer and Software Engineering, École Polytechnique de Montréal, (Québec), Canada; Department of Computer and Software Engineering, École Polytechnique de Montréal, (Québec), Canada","2012 15th International Telecommunications Network Strategy and Planning Symposium (NETWORKS)","","2012","","","1","4","In this paper, we tackle the wireless sensor network (WSN) planning problem considering coverage, target localization and network connectivity constraints. First, a combinatorial optimization model is proposed for this problem and next, a starting greedy heuristic is proposed followed by a tabu search metaheuristic to find “good” feasible solutions rapidly. The solutions are compared to the optimal solutions found using CPLEX. The test results show the proposed approach finds good quality solutions.","","978-1-4673-1391-9978-1-4673-1390-2978-1-4673-1389","10.1109/NETWKS.2012.6381674","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6381674","Wireless sensor networks (WSN);network planning;combinatorial optimization model;tabu search","Wireless sensor networks;Atmospheric modeling;Biological system modeling;Optimization;Planning;Capacitive sensors","greedy algorithms;search problems;target tracking;wireless sensor networks","wireless sensor network deployment model;target localization constraints;wireless sensor network planning problem;WSN planning problem;network connectivity constraints;combinatorial optimization model;greedy heuristic;tabu search metaheuristic;CPLEX;good quality solutions","","1","11","","","","","","IEEE","IEEE Conferences"
"Divergence Analysis and Optimizations","B. Coutinho; D. Sampaio; F. M. Q. Pereira; W. Meira Jr.","NA; NA; NA; NA","2011 International Conference on Parallel Architectures and Compilation Techniques","","2011","","","320","329","The growing interest in GPU programming has brought renewed attention to the Single Instruction Multiple Data (SIMD) execution model. SIMD machines give application developers a tremendous computational power, however, the model also brings restrictions. In particular, processing elements (PEs) execute in lock-step, and may lose performance due to divergences caused by conditional branches. In face of divergences, some PEs execute, while others wait, this alternation ending when they reach a synchronization point. In this paper we introduce divergence analysis, a static analysis that determines which program variables will have the same values for every PE. This analysis is useful in three different ways: it improves the translation of SIMD code to non-SIMD CPUs, it helps developers to manually improve their SIMD applications, and it also guides the compiler in the optimization of SIMD programs. We demonstrate this last point by introducing branch fusion, a new compiler optimization that identifies, via a gene sequencing algorithm, chains of similarities between divergent program paths, and weaves these paths together as much as possible. Our implementation has been accepted in the Ocelot open-source CUDA compiler, and is publicly available. We have tested it on many industrial-strength GPU benchmarks, including Rodinia and the Nvidia's SDK. Our divergence analysis has a 34% false-positive rate, compared to the results of a dynamic profiler. Our automatic optimization adds a 3% speed-up onto parallel quick sort, a heavily optimized benchmark. Our manual optimizations extend this number to over 10%.","1089-795X;1089-795X","978-1-4577-1794-9978-0-7695-4566","10.1109/PACT.2011.63","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113840","CUDA;GPU;SIMD;Compiler;Static Analysis;Divergent Execution;Parallel Execution","Synchronization;Optimization;Graphics processing unit;Logic gates;Semantics;Algorithm design and analysis;Hardware","graphics processing units;optimisation;parallel architectures;program compilers;public domain software;synchronisation","open-source CUDA compiler;Ocelot;branch fusion;synchronization point;processing elements;SIMD execution model;single instruction multiple data;GPU programming;optimizations;divergence analysis","","22","39","","","","","","IEEE","IEEE Conferences"
"Optimisation of biometric ID tokens by using hardware/software co-design","J. Liu-Jimenez; R. Sanchez-Reillo; L. Mengibar-Pozo; O. Miguel-Hurtado","University Group for Identification Technologies, Carlos III University of Madrid, Leganes, Madrid, Spain; University Group for Identification Technologies, Carlos III University of Madrid, Leganes, Madrid, Spain; University Group for Identification Technologies, Carlos III University of Madrid, Leganes, Madrid, Spain; University Group for Identification Technologies, Carlos III University of Madrid, Leganes, Madrid, Spain","IET Biometrics","","2012","1","3","168","177","In current society, the necessity of recognising people is increasing every day. Logical or physical access is restricted to authorised users, which in many cases have to provide tokens where their personal information is stored. At the same time, biometrics proposes a feasible solution for the recognition problem. The combination of both solutions is coming up front. However, up till now, owing to processing restrictions, these tokens are just able to store data and perform the last steps of the biometric recognition process. In this study, the authors propose a new system where tokens are based on hardware/software (HW/SW) co-design, which allows computing most of the biometric process in them. This proposal covers several aspects which these systems are subject to, taking advantages of the two platforms they use for reducing computational time or HW area, and also to increase security or minimise misidentification errors. For testing this proposal, an Iris ID token has been implemented, showing different design alternatives adapted to different work scenarios.","2047-4938;2047-4946","","10.1049/iet-bmt.2012.0004","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6397037","","","hardware-software codesign;iris recognition","biometric ID token optimisation;hardware-software codesign;logical access;physical access;personal information;biometric recognition process;HW-SW;biometric process;misidentification errors;Iris ID token","","1","","","","","","","IET","IET Journals & Magazines"
"Hardware/software co-simulation for the rapid prototyping of an acceleration sensor system with force-feedback control","R. Khalilyulin; T. Steinhuber; G. Schrag; G. Wachutka","Institute for Physics of Electrotechnology, Munich University of Technology, Arcisstrasse 21, 80290, Germany; Institute for Physics of Electrotechnology, Munich University of Technology, Arcisstrasse 21, 80290, Germany; Institute for Physics of Electrotechnology, Munich University of Technology, Arcisstrasse 21, 80290, Germany; Institute for Physics of Electrotechnology, Munich University of Technology, Arcisstrasse 21, 80290, Germany","2010 Proceedings of the European Solid State Device Research Conference","","2010","","","186","189","We present hardware-in-the-loop simulations of a MEMS acceleration sensor system with view to reduce the development time and complexity especially regarding design and test of dedicated control algorithms. To this end, the physically-based high-level Simulink model of the mechanical sensor is connected to a FPGA hardware board, on which the corresponding control algorithms (in our case PID and fuzzy logic) are implemented. The different control algorithms are described on the level of behavioral modeling in Simulink using the Xilinx System Generator tool, which then automatically generates a VHDL code of the models and transfers it to the hardware board. The advantage of these hardware/software co-simulations compared to &#x201C;pure&#x201D; simulations of the entire system in Simulink is a realistic estimation of the overall performance (run times, power consumption, efficiency, etc.) of the control circuitry even before the transducer is realized in hardware itself. This, on the other side, allows also modifications and optimizations not only on the control circuit but also on the transducer design in a very early stage of the design process.","2378-6558;1930-8876;1930-8876","978-1-4244-6661-0978-1-4244-6658-0978-1-4244-6660","10.1109/ESSDERC.2010.5618398","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5618398","","Transducers;Biomembranes;Hardware;Integrated circuit modeling;Algorithm design and analysis;Acceleration;Generators","acceleration;acceleration control;acceleration measurement;force feedback;hardware-software codesign","hardware/software co-simulation;rapid prototyping;force feedback control;hardware-in-the-loop simulation;MEMS acceleration sensor system;dedicated control algorithm;high level Simulink model;mechanical sensor;FPGA hardware board;fuzzy logic;Xilinx system generator tool;VHDL code;control circuitry;transducer design","","","5","","","","","","IEEE","IEEE Conferences"
"SDiC: Context-based retrieval in Eclipse","B. Antunes; J. Cordeiro; P. Gomes","Centre for Informatics and Systems of the University of Coimbra, Coimbra, Portugal; Centre for Informatics and Systems of the University of Coimbra, Coimbra, Portugal; Centre for Informatics and Systems of the University of Coimbra, Coimbra, Portugal","2012 34th International Conference on Software Engineering (ICSE)","","2012","","","1467","1468","While working in an IDE, developers typically deal with a large number of different artifacts at the same time. The software development process requires that they repeatedly switch between different artifacts, which often depends on searching for these artifacts in the source code structure. We propose a tool that integrates context-based search and recommendation of source code artifacts in Eclipse. The artifacts are collected from the workspace of the developer and represented using ontologies. A context model of the developer is used to improve search and give recommendations of these artifacts, which are ranked according to their relevance to the developer. The tool was tested by a group of developers and the results show that contextual information has an important role in retrieving relevant information for developers.","1558-1225;0270-5257;0270-5257","978-1-4673-1067-3978-1-4673-1066-6978-1-4673-1065","10.1109/ICSE.2012.6227061","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227061","","Context;Context modeling;Ontologies;Knowledge based systems;Computational modeling;Programming;Switches","content-based retrieval;ontologies (artificial intelligence);recommender systems;software engineering","SDiC;context-based retrieval;Eclipse;IDE;software development process;source code structure;context-based search;source code artifact recommendation;ontologies;information retrieval;software development in context","","","4","","","","","","IEEE","IEEE Conferences"
"Validation and optimization of modular railgun model","Y. Hu; P. Ma; M. Yang; Z. Wang","Control and Simulation Center, Harbin Institute of Technology, 150001, China; Control and Simulation Center, Harbin Institute of Technology, 150001, China; Control and Simulation Center, Harbin Institute of Technology, 150001, China; Control and Simulation Center, Harbin Institute of Technology, 150001, China","2012 16th International Symposium on Electromagnetic Launch Technology","","2012","","","1","6","Considering the electromagnetic railgun structure features, an applied model has been established by modularization modeling method. Because model validation plays a key role in system simulation for analysis and optimization of railgun, a novel validation method is proposed in this paper. By utilizing the railgun dynamic test and predicted data, the simulation model is validated based on theil's inequality coefficient (TIC) method. To execute sufficient validation of the model and improve reliability of the validation results, similarity theory is adopted to analyze the credibility of simulation model by comparing the characteristics of the railgun. Inspired by analytic hierarchy process (AHP) for multiple attribute decision making, we define the muzzle velocity, current peak and discharging time as evaluation index for assessing the credibility of the simulation model and then evaluation structure is established. The final synthesis results demonstrate the feasibility of the adopted validation method and the validity of the modular model. The whole system model is run under MATLAB software platform, which is divided into several main components encapsulated into classes by object-oriented technology. On the basis of the validated model, an optimization pursuing highest velocity with the constraint of the current peak and the residual current in the rail is carried out to find a set of optimal parameters for the railgun using genetic algorithm.","","978-1-4673-0305-7978-1-4673-0306-4978-1-4673-0304","10.1109/EML.2012.6325055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6325055","modularized design;model validation;similarity theory;analytic hierarchy process;optimization;genetic algorithm;electromagnetic railgun","Mathematical model;Railguns;Optimization;Rails;Object oriented modeling;Projectiles;Analytical models","decision making;discharges (electric);genetic algorithms;railguns;reliability","modular railgun model;electromagnetic railgun structure features;modularization modeling method;model validation;system simulation;railgun analysis;railgun optimization;validation method;railgun dynamic test;Theil inequality coefficient;TIC method;reliability;similarity theory;analytic hierarchy process;AHP;multiple attribute decision making;muzzle velocity;current peak;discharging time;evaluation index;MATLAB software platform;object-oriented technology;residual current;genetic algorithm","","8","15","","","","","","IEEE","IEEE Conferences"
"Coordinated micro-generation and load management for energy saving policies","S. Bertolini; M. Giacomini; S. Grillo; S. Massucco; F. Silvestro","Department of Communication Computer and System Sciences of the University of Genoa, Italy; Department of Communication Computer and System Sciences of the University of Genoa, Italy; Electrical Engineering Department of the University of Genoa, Italy; Electrical Engineering Department of the University of Genoa, Italy; Electrical Engineering Department of the University of Genoa, Italy","2010 IEEE PES Innovative Smart Grid Technologies Conference Europe (ISGT Europe)","","2010","","","1","7","The paper describes a joint university-industry project aimed at defining hardware and software requirements for the development of a prototypical microgrid platform on existing sites able to manage generation resources and load. The paper describes an architecture developed for integrating existing software tools for on-line load monitoring and control and for management of in-site generation. The system acquires data from field and stores information on a server, from which an optimization tool gets data in order to perform its calculations and give the optimal set points to the programmable generation resources. Results are reported about experimentation at test sites with different generating units, such as cogeneration of heat and power (CHP) and/or renewable power generating units equipped with a real-time monitoring tool.","2165-4816;2165-4824","978-1-4244-8510-9978-1-4244-8508-6978-1-4244-8509","10.1109/ISGTEUROPE.2010.5638953","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5638953","Distributed generation;Energy Management;Energy efficiency;Load forecasting;RES optimization","Optimization;Monitoring;Generators;Computer architecture;Cogeneration;Energy consumption;Sensors","computerised monitoring;distributed power generation;energy conservation;load regulation;optimisation;power grids;software tools","coordinated microgeneration;load management;energy saving policy;university-industry project;prototypical microgrid platform;software tool;online load monitoring;load control;in-site generation;optimization tool;programmable generation resource;distributed generation","","4","6","","","","","","IEEE","IEEE Conferences"
"Optimized Discriminative Kernel for SVM Scoring and Its Application to Speaker Verification","S. Zhang; M. Mak","Department of Electronic and Information Engineering, Hong Kong Polytechnic University, Hong Kong; Department of Electronic and Information Engineering, Hong Kong Polytechnic University, Hong Kong","IEEE Transactions on Neural Networks","","2011","22","2","173","185","The decision-making process of many binary classification systems is based on the likelihood ratio (LR) scores of test patterns. This paper shows that LR scores can be expressed in terms of the similarity between the supervectors (SVs) formed by stacking the mean vectors of Gaussian mixture models corresponding to the test patterns, the target model, and the background model. By interpreting the support vector machine (SVM) kernels as a specific similarity (or discriminant) function between SVs, this paper shows that LR scoring is a special case of SVM scoring and that most sequence kernels can be obtained by assuming a specific form for the similarity function of SVs. This paper further shows that this assumption can be relaxed to derive a new general kernel. The kernel function is general in that it is a linear combination of any kernels belonging to the reproducing kernel Hilbert space. The combination weights are obtained by optimizing the ability of a discriminant function to separate the positive and negative classes using either regression analysis or SVM training. The idea was applied to both high-and low-level speaker verification. In both cases, results show that the proposed kernels achieve better performance than several state-of-the-art sequence kernels. Further performance enhancement was also observed when the high-level scores were combined with acoustic scores.","1045-9227;1941-0093","","10.1109/TNN.2010.2090893","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5643156","Kernel optimization;sequence kernels;speaker verification;support vector machines","Kernel;Support vector machines;Optimization;Adaptation model;Speech;Measurement;Computational modeling","decision making;Gaussian processes;Hilbert spaces;regression analysis;speaker recognition;support vector machines","optimized discriminative kernel;SVM scoring;speaker verification;decision-making process;binary classification systems;likelihood ratio scores;supervectors;Gaussian mixture models;support vector machine;similarity function;kernel Hilbert space;regression analysis;acoustic scores","Algorithms;Artificial Intelligence;Classification;Discrimination Learning;Humans;Linear Models;Neural Networks (Computer);Normal Distribution;Pattern Recognition, Automated;Speech Recognition Software;Voice","8","43","","","","","","IEEE","IEEE Journals & Magazines"
"A Hierarchical Analysis of phasor measurement unit placement optimization in transmission network","J. Zhong; K. L. Wong","Department of Electrical and Computer Engineering, RMIT University, Melbourne, Australia; Department of Electrical and Computer Engineering, RMIT University, Melbourne, Australia","AUPEC 2011","","2011","","","1","4","This paper presents a new phasor measurement unit (PMU) placement algorithm based on Hierarchical Analysis, a hybrid algorithm combining the breadth-first algorithm (BFA) and greedy algorithm. With the optimized number of PMU, the system is to maintain full network view during normal operation with and without conventional zero-injection effect. The IEEE standard 14-, 30-, 57- and the New England 39-bus test systems are investigated. Based on the comparison of the proposed method in this paper with other published methods, the proposed method has achieve good results with minimal PMU required and maximum system observability redundancy index (SORI).","","978-1-921897-07-8978-1-4577-1793","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6102542","Phasor Measurment Unit;Breadth-First Algorithm;Greedy Algorithm;Network Observability;Maximum Redundancy","Phasor measurement units;Greedy algorithms;Observability;Algorithm design and analysis;Computer aided software engineering;Redundancy;Indexes","greedy algorithms;IEEE standards;phasor measurement;transmission networks","hierarchical analysis;phasor measurement unit;placement optimization;transmission network;breadth-first algorithm;greedy algorithm;PMU;IEEE standard;New England 39-bus test systems;system observability redundancy index;SORI","","","11","","","","","","IEEE","IEEE Conferences"
"A discrete artificial bee colony algorithm for the traveling salesman problem with time windows","K. Karabulut; M. F. Tasgetiren","Software Engineering Department, Yasar University, Selcuk Yasar Campus, Izmir, Turkey; Industrial Engineering Department, Yasar University, Selcuk Yasar Campus, Izmir, Turkey","2012 IEEE Congress on Evolutionary Computation","","2012","","","1","7","This paper presents a discrete artificial bee colony algorithm (DABC) for solving the traveling salesman problem with time windows (TSPTW) in order to minimize the total travel cost of a given tour. TSPTW is a difficult optimization problem arising in both scheduling and logistic applications. The proposed DABC algorithm basically relies on the destruction and construction phases of iterated greedy algorithm to generate neighboring food sources in a framework of ABC algorithm. In addition, it also relies on a classical 1-opt local search algorithm to further enhance the solution quality. The performance of the algorithm was tested on a benchmark set from the literature. Experimental results show that the proposed DABC algorithm is very competitive to or even better than the best performing algorithms from the literature.","1089-778X;1941-0026","978-1-4673-1509-8978-1-4673-1510-4978-1-4673-1508","10.1109/CEC.2012.6252941","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6252941","traveling salesman problem with time windows;artificial bee colony algorithm;iterated greedy algorithm;swarm intelligence;heuristic optimization","Radiation detectors;Optimization;Equations;Benchmark testing;Heuristic algorithms;Particle swarm optimization;Algorithm design and analysis","greedy algorithms;iterative methods;logistics;optimisation;scheduling;search problems;travelling salesman problems","discrete artificial bee colony algorithm;traveling salesman problem with time windows;DABC;TSPTW;total travel cost minimization;scheduling;logistic applications;iterated greedy algorithm;classical 1-opt local search algorithm","","5","30","","","","","","IEEE","IEEE Conferences"
"An Evolutionary Multiobjective Sleep-Scheduling Scheme for Differentiated Coverage in Wireless Sensor Networks","S. Sengupta; S. Das; M. Nasir; A. V. Vasilakos; W. Pedrycz","Department of Electronics and Telecommunication Engineering, Jadavpur University, Kolkata, India; Electronics and Communication Sciences Unit, Indian Statistical Institute, Kolkata, India; Department of Electronics and Telecommunication Engineering, Jadavpur University, Kolkata, India; Department of Computer and Telecommunication Engineering, University of Western Macedonia, Kozani, Greece; Department of Electrical and Computer Engineering, University of Alberta, Edmonton, Canada","IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)","","2012","42","6","1093","1102","We propose an online, multiobjective optimization (MO) algorithm to efficiently schedule the nodes of a wireless sensor network (WSN) and to achieve maximum lifetime. Instead of dealing with traditional grid or uniform coverage, we focus on the differentiated or probabilistic coverage where different regions require different levels of sensing. The MO algorithm helps to attain a better tradeoff among energy consumption, lifetime, and coverage. The algorithm can be run every time a node failure occurs due to power failure of the node battery so that it may reschedule the network. This scheduling is modeled as a combinatorial, multiobjective, and constrained optimization problem with energy and noncoverage as the two objectives. The basic evolutionary multiobjective optimizer used is known as decomposition-based multiobjective evolutionary algorithm (MOEA/D) which is modified by integrating the concept of fuzzy Pareto dominance. The performance of the resulting algorithm, which is called MOEA/DFD, is compared with the performance of the original MOEA/D, which is another very well known MO algorithm called nondominated sorting genetic algorithm (NSGA-II), and an IBM optimization software package called CPLEX. In all the tests, MOEA/DFD is observed to outperform all other algorithms.","1094-6977;1558-2442","","10.1109/TSMCC.2012.2196996","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6330048","Density control;differentiated coverage;evolutionary multiobjective optimization (MO);node deployment;wireless sensor networks (WSNs)","Optimization;Vectors;Sensors;Linear programming;Wireless sensor networks;Event detection","fuzzy set theory;genetic algorithms;Pareto optimisation;scheduling;wireless sensor networks","evolutionary multiobjective sleep scheduling scheme;differentiated coverage;wireless sensor networks;multiobjective optimization;traditional grid coverage;uniform coverage;probabilistic coverage;node failure;power failure;node battery;constrained optimization problem;decomposition based multiobjective evolutionary algorithm;MOEA/D;fuzzy Pareto dominance;nondominated sorting genetic algorithm;NSGA-II;IBM optimization software package;CPLEX","","100","27","","","","","","IEEE","IEEE Journals & Magazines"
"Digital control of a fuel cell converter system: Verification, validation and test using a model-based design approach","O. A. Ahmed; J. A. M. Bleijs","Department of Engineering, University of Leicester, University Road, LE1 7RH, United Kingdom; Department of Engineering, University of Leicester, University Road, LE1 7RH, United Kingdom","4th European Education and Research Conference (EDERC 2010)","","2010","","","52","56","In this paper, digital control of a current-fed full bridge DC-DC converter for a fuel cell power system is investigated. The generated code for the designed digital PI compensators with anti-windup protection was implemented using Embedded Integrated Development Environment Link software with the Real-Time Workshop in MATLAB. Using the TMS320F2812 DSP the generated algorithm is validated, implemented and deployed. Rapid prototyping implementation has resulted in efficient code generation for the controller design by minimizing the overall time, optimizing the generated C-Code and speeding up the algorithm execution. After the correctness of the simulation results was confirmed, the functionality of the produced code was evaluated using processor-in-the-loop co-simulation. A 1.2kW prototype fuel cell converter system is implemented and tested. Experimental results have verified the theoretical and modelling analysis.","","978-0-9552047-4-6978-0-9552047-4","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6151406","","Mathematical model;Digital signal processing;Libraries;Pulse width modulation;Control systems;MATLAB;Algorithm design and analysis","compensation;control engineering computing;DC-DC power convertors;digital control;digital signal processing chips;formal verification;fuel cell power plants;PI control;power engineering computing;software prototyping","digital control;fuel cell converter system;model-based design approach;current-fed full bridge DC-DC converter;direct current-to-direct current converter;fuel cell power system;digital PI compensator;proportional-integral compensator;anti-windup protection;Embedded Integrated Development Environment Link software;RealTime Workshop;MATLAB;TMS320F2812 DSP;digital signal processor;rapid prototyping implementation;code generation;power 1.2 kW","","","7","","","","","","IEEE","IEEE Conferences"
"Annotation support for generic patches","G. Dotzler; R. Veldema; M. Philippsen","University of Erlangen-Nuremberg, Computer Science Department, Programming Systems Group, Germany; University of Erlangen-Nuremberg, Computer Science Department, Programming Systems Group, Germany; University of Erlangen-Nuremberg, Computer Science Department, Programming Systems Group, Germany","2012 Third International Workshop on Recommendation Systems for Software Engineering (RSSE)","","2012","","","6","10","In large projects parallelization of existing programs or refactoring of source code is time consuming as well as error-prone and would benefit from tool support. However, existing automatic transformation systems are not extensively used because they either require tedious definitions of source code transformations or they lack general adaptability. In our approach, a programmer changes code inside a project, resulting in before and after source code versions. The difference (the generated transformation) is stored in a database. When presented with some arbitrary code, our tool mines the database to determine which of the generalized transformations possibly apply. Our system is different from a pure compiler based (semantics preserving) approach as we only suggest code modifications. Our contribution is a set of generalizing annotations that we have found by analyzing recurring patterns in open source projects. We show the usability of our system and the annotations by finding matches and applying generated transformations in real-world applications.","2327-0934;2327-0942","978-1-4673-1759-7978-1-4673-1758","10.1109/RSSE.2012.6233400","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6233400","programming tools;optimizations;patches;code-refactoring","Benchmark testing;Pattern matching;Databases;Generators;Java;Semantics","automatic programming;program compilers;public domain software;software maintenance;software tools","annotation support;generic patches;automatic transformation system;generalized transformation;semantics preserving approach;code modification;generalizing annotation;open source project;system usability;generated transformation;programming tool;code refactoring","","","14","","","","","","IEEE","IEEE Conferences"
"All-values symbolic execution","G. Denaro","University of Milano-Bicocca, Department of Informatics, Systems and Communication, Viale Sarca, 336, 20126, Italy","2012 7th International Workshop on Automation of Software Test (AST)","","2012","","","138","144","This paper discusses and exemplifies our ideas on all-values symbolic execution, an alternative strategy to the traditional all-paths style of symbolic execution. All-values symbolic execution focuses on enumerating the (symbolic) values that may derive from the symbolic execution of program statements. It exploits program dependencies to optimize the symbolic execution of those statements that can be executed with the same symbolic inputs on multiple (up to infinite) paths. Although a fully working implementation and a thorough evaluation are yet to come, this paper illustrates with simple, but representative examples that the proposed technique can boost the efficiency of symbolic execution, and suite interesting new applications.","","978-1-4673-1822-8978-1-4673-1821","10.1109/IWAST.2012.6228982","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6228982","","Prototypes;Software;Engines;Testing;Concrete;Radio access networks;Java","software engineering;symbol manipulation","all-values symbolic execution;program statements;symbolic inputs","","","20","","","","","","IEEE","IEEE Conferences"
"Multi-objective immune algorithm with dynamic memetic Cauchy mutation","Y. Yang; H. Fang","College of Computer Science and Software Engineering, Shenzhen University, Shenzhen City, P.R. China 518060; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen City, P.R. China 518060","2011 IEEE Workshop on Memetic Computing (MC)","","2011","","","1","8","In this paper, a novel immune algorithm with dynamic memetic Cauchy mutation (DMCMIA) for multi-objective optimization is proposed. The idea of memetics is incorporated into the mutation process and a dynamic memetic Cauchy mutation (DMCM) operator is developed. The DMCM operator combines global exploration and local refinement efficiently, which adopts a generation-dependent parameter to guarantee a good balance between global search and local search. Comparison is made to another multi-objective optimization algorithm, nondominated neighbor immune algorithm, termed as NNIA, in solving five ZDT and five DTLZ standard test problems. Simulation results based on coverage of two set, convergence metric and spacing show that DMCMIA performs better than NNIA in generating approximations to the true Pareto front. In addition, the effectiveness of the novel dynamic memetic Cauchy mutation is verified by comparison to polynomial mutation and Gaussian mutation, the experimental results reinforce the advantage of the DMCM operator.","","978-1-61284-066-6978-1-61284-065-9978-1-61284-064","10.1109/MC.2011.5953629","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5953629","multiobjective;immune algorithm;clonal selection scheme;memetic mutation;adaptive Cauchy mutation","Heuristic algorithms;Memetics;Optimization;Cloning;Measurement;Convergence;Maintenance engineering","artificial immune systems;Pareto optimisation;search problems","multiobjective immune algorithm;dynamic memetic cauchy mutation;DMCMIA;multiobjective optimization;DMCM operator;generation-dependent parameter;local search;global search;nondominated neighbor immune algorithm;NNIA;ZDT;DTLZ standard test problem;Pareto front","","2","30","","","","","","IEEE","IEEE Conferences"
"Adaptive Differential Evolution with variable population size for solving high-dimensional problems","H. Wang; S. Rahnamayan; Z. Wu","State Key Laboratory of Software Engineering, Wuhan University, Wuhan 430072, China; Faculty of Engineering and Applied Science, University of Ontario Institute of Technology (UOIT), 2000 Simcoe Street North, Oshawa, ON L1H 7K4, Canada; Faculty of Engineering and Applied Science, University of Ontario Institute of Technology (UOIT), 2000 Simcoe Street North, Oshawa, ON L1H 7K4, Canada","2011 IEEE Congress of Evolutionary Computation (CEC)","","2011","","","2626","2632","In this paper, we present a novel Differential Evolution (DE) algorithm to solve high-dimensional global optimization problems effectively. The proposed approach, called DEVP, employs a variable population size mechanism, which adjusts population size adaptively. Experiments are conducted to verify the performance of DEVP on 19 high-dimensional global optimization problems with dimensions 50, 100, 200, 500 and 1000. The simulation results show that DEVP out performs classical DE, CHC (Crossgenerational elitist selection, Heterogeneous recombination, and Cataclysmic mutation), G CMA-ES (Restart Covariant Matrix Evolutionary Strategy) and GODE (Generalized Opposition-Based DE) on the majority of test problems.","1941-0026;1089-778X","978-1-4244-7835-4978-1-4244-7834-7978-1-4244-7833","10.1109/CEC.2011.5949946","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5949946","Differential Evolution (DE);variable population size;global optimization;large-scale;high-dimensional","Optimization;Convergence;Evolutionary computation;Algorithm design and analysis;Benchmark testing;Equations;Heuristic algorithms","evolutionary computation;optimisation","adaptive differential evolution algorithm;high-dimensional global optimization problem;DEVP;variable population size mechanism","","5","30","","","","","","IEEE","IEEE Conferences"
"Design aware scheduling of dynamic testbench controlled design element accesses in FPGA-based HW/SW co-simulation systems for fast functional verification","S. Banerjee; T. Gupta","Mentor Graphics India Pvt. Ltd; Mentor Graphics India Pvt. Ltd","2nd Asia Symposium on Quality Electronic Design (ASQED)","","2010","","","175","181","In HW/SW co-simulation based logic verification systems, the design under test (DUT) is executed on an FPGA based emulator and the behavioral testbench written in some high level language like C or HDL is run on a SW simulator or a general purpose CPU. In such systems it is essential to reduce the communication between SW and HW sides to enhance overall verification speed. Therefore it is of significant importance to efficiently schedule testbench controlled accesses to design elements like flip-flops, memories etc, which require access to FPGA HW. Such accesses by static HDL testbenches which are specified at design compilation time and are mainly to design IOs are optimized by synthesis of RTL transactors, which make use of specialized high speed links and standardized interfaces like SCE-MI for optimizing the transactions. It is seen that apart from static testbenches, verification engineers and designers often make use of dynamic random access to design elements (e.g a series of register sets) through C or TCL based dynamic testbenches typically specified post design compilation. Such dynamic accesses are not routed through the transaction link and follow a fixed cost scheduling, resulting in sub-optimal communication pattern between SW and HW verification engines. The paper presents a design aware scheduling system for optimizing such dynamic accesses, by extracting and making use of design characteristics for intelligently clubbing and scheduling various read/write accesses while maintaining modeling fidelity. This complements the streaming transaction based interfaces available for static accesses. It is seen that when this scheduling system is implemented in an industry standard FPGA based co-simulator, evaluation fidelity is maintained and significant enhancement in overall functional verification performance is achieved.","","978-1-4244-7808-8978-1-4244-7809-5978-1-4244-7807","10.1109/ASQED.2010.5548239","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5548239","HW-SW co-verification;Emulation;FPGA","Dynamic scheduling;System testing;Control systems;Job shop scheduling;Logic testing;Field programmable gate arrays;Design optimization;Hardware design languages;Logic design;High level languages","benchmark testing;field programmable gate arrays;formal verification;hardware-software codesign;scheduling","design aware scheduling;FPGA-based HW/SW co-simulation systems;logic verification systems;design under test;FPGA based emulator;dynamic random access;read/write accesses;streaming transaction based interfaces","","4","13","","","","","","IEEE","IEEE Conferences"
"Development of a new software-defined S-band radar and its use in the test of wavelet-based waveforms","L. Wang; S. Law; C. Fraker; R. Vela; Y. F. Zheng; R. Ewing; G. Scalzi","Baylor University; University of California, San Diego; Ohio State University; Pennslyvania State University; Ohio State University; Air Force Research Laboratory; Air Force Research Laboratory","Proceedings of the 2011 IEEE National Aerospace and Electronics Conference (NAECON)","","2011","","","162","166","A new software-defined S-band radar (SDSR) has recently been developed at the Air Force Research Laboratory. The system is built upon individual off-the-shelf components, devices, and instruments, and the center of the architecture is an Arbitrary Waveform Generator (AWG). The AWG can be programmed to generate various kinds of radar waveforms, which are mixed with a carrier to create S-band radar signals. The AWG can be interfaced with a computer which runs on MatLab or LabView for software definition of radar waveforms. Furthermore, the computer can be connected to the Internet for receiving data from remote users. As a result, the new SDSR can be used to support studies of radar waveforms in a large area by remote users. The characteristics of the radar system are studied and the performance of the system is optimized by selecting the parameters of the building components. The newly developed SDSR is used to test the wavelet-based radar waveforms, which verifies the theoretical results produced earlier.","2379-2027;0547-3578;0547-3578","978-1-4577-1041-4978-1-4577-1040-7978-1-4577-1039","10.1109/NAECON.2011.6183095","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6183095","","Wavelet transforms;Radar antennas;Radar imaging;Transmitting antennas;Receiving antennas","computerised instrumentation;Internet;parameter estimation;radar computing;radar signal processing;software radio;UHF radio propagation;waveform generators;wavelet transforms","software-defined S-band radar;SDSR;Air Force Research Laboratory;off-the-shelf components;arbitrary waveform generator;AWG;radar waveforms;S-band radar signals;MatLab;LabView;Internet;remote users;parameter selection;wavelet-based radar waveforms","","2","8","","","","","","IEEE","IEEE Conferences"
"Handling Conflicts with Compiler's Help in Software Transactional Memory Systems","S. Mannarswamy; R. Govindarajan","NA; NA","2010 39th International Conference on Parallel Processing","","2010","","","482","491","Atomic sections are supported in software through the use of optimistic concurrency by using Software Transactional Memory (STM). However STM implementations incur high overheads which reduce the wide-spread use of this approach by programmers. Conflicts are a major source of overheads in STMs. The basic performance premise of a transactional memory system is the optimistic concurrency principle wherein data updates executed by the transactions are to disjoint objects/memory locations, referred to as Disjoint Access Parallel (DAP). Otherwise, the updates conflict, and all but one of the transactions are aborted. Such aborts result in wasted work and performance degradation. While contention management systems in STM implementations try to reduce conflicts by various runtime feedback control mechanisms, they are not aware of the application's structure and data access patterns and hence typically act after the conflicts have occurred. In this paper we propose a scheme based on compiler analysis, which can identify static atomic sections whose instances, when executed concurrently by more than one thread always conflict. Such an atomic section is referred to as Always Conflicting Atomic Section (ACAS). We propose and evaluate two techniques Selective Pessimistic Concurrency Control (SPCC) and compiler inserted Early Conflict Checks (ECC) which can help reduce the STM overheads caused by ACAS. We show that these techniques help reduce the aborts in 4 of the STAMP benchmarks by up to 27.52% while improving performance by 1.24% to 19.31%.","0190-3918;0190-3918;2332-5690","978-1-4244-7913-9978-0-7695-4156","10.1109/ICPP.2010.56","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5599190","","Parallel processing;Concurrency control;Runtime;Concurrent computing;Software;Programming;Benchmark testing","concurrency control;data handling;program compilers;shared memory systems;transaction processing","handling conflict;software transactional memory system;atomic section;optimistic concurrency;optimistic concurrency principle;disjoint access parallel;contention management systems;feedback control mechanisms;data access patterns;compiler analysis;static atomic sections;always conflicting atomic section;selective pessimistic concurrency control;early conflict checks","","","29","","","","","","IEEE","IEEE Conferences"
"A Scilab based open source toolbox for global optimization using the bernstein form","D. B. Magare; B. V. Patil; V. A. Vyawahare; P. S. V. Nataraj","Systems and Control Engineering, Indian Institute of Technology Bombay, Mumbai, India; Systems and Control Engineering, Indian Institute of Technology Bombay, Mumbai, India; Systems and Control Engineering, Indian Institute of Technology Bombay, Mumbai, India; Systems and Control Engineering, Indian Institute of Technology Bombay, Mumbai, India","2010 2nd International Conference on Reliability, Safety and Hazard - Risk-Based Technologies and Physics-of-Failure Methods (ICRESH)","","2010","","","555","561","We present an open source toolbox in Scilab for multivariate polynomial optimization based on the Bernstein form. The developed toolbox finds the global minimum of unconstrained polynomial optimization problems. We first describe the method used by the toolbox, and then demonstrate its performance on several standard optimization examples. We also compare the quality of the results obtained using the developed toolbox with those of popular global optimization methods, such as genetic algorithms, simulated annealing, pattern search, and global search interior point methods. The tests show the superior performance of the presented toolbox.","","978-1-4244-8343-3978-1-4244-8344","10.1109/ICRESH.2010.5779610","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5779610","Bernstein polynomials;Genetic algorithm;global optimization;global search interior point algorithm;multivariate polynomial;pattern search algorithm;Scilab;Simulated annealing algorithm","Heart","genetic algorithms;high level languages;mathematics computing;polynomials;public domain software;search problems;simulated annealing","Scilab;open source toolbox;Bernstein form;multivariate polynomial optimization;unconstrained polynomial optimization problems;global optimization methods;genetic algorithms;simulated annealing;global search interior point methods;pattern search interior point methods","","1","12","","","","","","IEEE","IEEE Conferences"
"Algorithm Transformation for FPGA Implementation","D. G. Bailey; C. T. Johnston","NA; NA","2010 Fifth IEEE International Symposium on Electronic Design, Test & Applications","","2010","","","77","81","High level hardware description languages aim to make hardware design more like programming software. These languages are often used to accelerate legacy software algorithms by porting them to an FPGA based hardware implementation. Porting does not always result in efficient architectures as the original algorithms are usually developed and optimized to run on a serial processor. To obtain an efficient hardware architecture, one that makes use of the available parallelism, the algorithms need to be transformed. Eleven such transformations are identified and explained. While some of these are straightforward, and have been implemented by some compilers, many cannot be automated because they require detailed knowledge of the algorithm.","","978-1-4244-6026-7978-1-4244-6025-0978-0-7695-3978","10.1109/DELTA.2010.17","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5438710","FPGA;algorithm;architecture;automated design;compilation;hardware description languages","Field programmable gate arrays;Computer architecture;Image processing;Algorithm design and analysis;Hardware design languages;Software algorithms;Parallel processing;Acceleration;Application software;Electronic equipment testing","field programmable gate arrays;hardware description languages;logic design","FPGA based hardware implementation;high level hardware description language;legacy software algorithm;algorithm transformation","","3","18","","","","","","IEEE","IEEE Conferences"
"The Cold Extrusion Forming Process Optimization about the Hexagonal Bowl Auto Part","G. Hongying; Z. Hui; Z. Zhonghua; X. Xincheng","NA; NA; NA; NA","2011 Third International Conference on Measuring Technology and Mechatronics Automation","","2011","3","","729","732","Testing cold extrusion forming process of the hexagonal bowl auto part by DEFORM-3D on the paper. By comparing the feasibility obtained through numerical simulation testing and analysis of the two forming plans, according to the final test results to determine a reasonable extrusion program. At the same time, actual production the cold extrusion plan is feasible.","2157-1473;2157-1481","978-1-4244-9010","10.1109/ICMTMA.2011.754","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5721591","FEM Numerical Simulation;Cold Extrusion Forming;Auto Part;DEFORM-3D","Numerical models;Software;Metals;Numerical simulation;Production;Solid modeling;Shape","automotive components;automotive engineering;cold working;extrusion;forming processes;production engineering computing;solid modelling","cold extrusion forming process optimization;hexagonal bowl auto part;DEFORM-3D;numerical simulation testing;forming plan;extrusion program;cold extrusion plan","","","6","","","","","","IEEE","IEEE Conferences"
"Task implementation of synchronous finite state machines","M. Di Natale; H. Zeng","Scuola Superiore S. Anna, Italy; McGill University, Canada","2012 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2012","","","206","211","Model-based design of embedded control systems using Synchronous Reactive (SR) models is among the best practices for software development in the automotive and aeronautics industry. SR models allow to formally verify the correctness of the design and to automatically generate the implementation code. This improves productivity and, more importantly, can ensure a correct software implementation (preserving the model semantics). Previous research focuses on the concurrent implementation of the dataflow part of SR models, including the optimization of the block-to-task mapping and communication buffer sizing. When the system also consists of blocks implementing finite state machines, as in modern modeling tools like Simulink and SCADE, the task implementation can be further optimized with respect to time and memory. In this paper we analyze problems and opportunities in the implementation of finite state machine subsystems. We define the constraints and efficient policies for the task implementation of such systems.","1558-1101;1530-1591;1530-1591","978-1-4577-2145-8978-3-9810801-8","10.1109/DATE.2012.6176464","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6176464","","","aerospace industry;automobile industry;control engineering computing;embedded systems;finite state machines;optimisation;program compilers;software engineering","task implementation;synchronous finite state machines;model-based design;embedded control systems;synchronous reactive models;software development;automotive industry;aeronautics industry;implementation code generation;optimization;block-to-task mapping;communication buffer sizing;Simulink;SCADE","","7","12","","","","","","IEEE","IEEE Conferences"
"System design and signal processing of optimized GPS SR receiver","Zhang Lei; J. Wang","Shanghai Institute of Technical Physics, Chinese Academy of Sciences, China; Shanghai Institute of Technical Physics, Chinese Academy of Sciences, China","2010 International Conference On Computer Design and Applications","","2010","4","","V4-103","V4-107","In this paper, a high speed 12-channel real-time software GPS receiver is proposed. Firstly, the system architecture is presented and the block data processing mechanism is introduced. The SR receiver consists of a radio frequency (RF) front end, a data acquisition board, and software that runs on a laptop with a Pentium-M 1.5GHz processor running the Window<sup>®</sup>XP operating system. The RF front end down converts the GPS signal to intermediate frequency (IF) signal and samples the IF signal at 16.368MHz. Then the optimized bit-wise algorithm is reviewed and the signal acquisition and tracking algorithm is described. The data acquisition board packs the 2-bit samples with a 2-bit in/8-bit out shift register and transfers the packed samples to laptop through a USB port. The software running on the laptop reads and processes 4 milliseconds of IF data at a time. The architecture and implementation of the SR receiver is described in this paper, including block data processing mechanism, optimized-bit wise parallel algorithm for correlation, signal acquisition, and tracking. Finally, the real-time performance of the SR receiver is also test and presented. It is shown that the software consumes about 40% Central Processing Unit (CPU) capacity when it tracking 12 satellites in real-time.","","978-1-4244-7164-5978-1-4244-7164","10.1109/ICCDA.2010.5541472","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5541472","GPS;software radio;acquisition and tracking","Signal processing;Design optimization;Global Positioning System;Strontium;Receivers;Radio frequency;Portable computers;RF signals;Computer architecture;Data processing","data acquisition;Global Positioning System;radio receivers;signal detection;telecommunication computing;user interfaces","system design;signal processing;optimized GPS SR receiver;real-time software GPS receiver;radio frequency front end;Pentium-M processor;Window®XP operating system;bit-wise algorithm;tracking algorithm;signal acquisition;data acquisition;central processing unit","","","8","","","","","","IEEE","IEEE Conferences"
"Cross Comparison on C Compilers' Reliability Impact on 64-Bit Windows Vista OS","A. K. Karna; H. Zou","NA; NA","2010 2nd International Conference on Information Technology Convergence and Services","","2010","","","1","6","Software reliability has always been a concern for both software producers and consumers. Because of their role in binary code generation, compilers have significant impact on software performance. Although many works have been conducted on compilers, little is done on their impact on software reliability. Thus, people often face a void of useful knowledge when deciding which compiler to use in developing software to meet a specific reliability target. This paper tries to provide some guidance in users' compiler selection by performing cross-comparison study on popular C compilers: Intel, PGI, and Visual Studio using 64-bit Windows Vista ultimate edition. We have used SPEC CPU2006 benchmark suite as our test set. Our results show that Visual studio is the most efficient compiler and the binaries generated by Intel compiler seems to be the most reliable for this platform. Overall, Intel compiler has superior reliability impact compared to other compilers on Windows Vista OS. And this result is consistent with user experience from our survey feedback.","","978-1-4244-7584-1978-1-4244-7584","10.1109/ITCS.2010.5581288","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5581288","","Benchmark testing;Measurement;Software reliability;Software;Binary codes;Optimization","operating systems (computers);program compilers;software reliability","C compiler reliability impact;64-bit Windows Vista OS;software reliability;cross comparison study;PGI;Visual Studio;SPEC CPU2006 benchmark suite;Intel compiler","","","10","","","","","","IEEE","IEEE Conferences"
"Superheated steam temperature predictive optimal control based on external time-delay BP neural network and a simpler PSO algorithm","M. Liangyu; G. Yinping","Department of Automation, North China Electric Power University, Baoding 071003, China; Department of Automation, North China Electric Power University, Baoding 071003, China","Proceedings of the 31st Chinese Control Conference","","2012","","","4050","4055","Power industry is facing a rapid development toward larger-capacity supercritical and ultra-supercritical coal-fired power generating units. The big inertia, large time delay and nonlinear characteristics of the superheater system for a supercritical boiler are becoming more and more obvious. The conventional cascaded PID control scheme is often difficult to obtain satisfactory steam temperature control effect over wide-range loading conditions. In this paper, an intelligent predictive optimal controller based on neural network (NN) modeling and Particle Swarm Optimization (PSO) is presented for superheated steam temperature control. Aiming at the known drawbacks of the NN predictive optimal controller, a time-delay BP neural network is used to establish the nonlinear dynamic model of the superheater system to improve the model's real-time prediction accuracy and generalization ability. A new simplified high-efficiency PSO algorithm discarding the concept of velocity is adopted to search the optimal controls with faster convergence rate to meet the real-time control demand. An elastic search space is updated dynamically based on the real-time steam temperature control error to prevent oscillation and to enhance stability. By taking a full-scope simulator of a 600MW supercritical power unit as the object investigated, the neural network model of the superheater system is built and trained with historical wide-range operating data. The new control scheme is programmed with MATLAB software and it communicates two-way with the simulator. Extensive control simulation tests are made. It is shown the proposed intelligent predictive optimal control scheme can greatly improve the superheated steam temperature control with good application prospect.","2161-2927;1934-1768;1934-1768","978-988-15638-1-1978-1-4673-2581-3978-988-15638-1","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6390636","Supercritical Boiler Unit;Superheated Steam Temperature;Predictive Optimal Control;Time-Delay BP Neural Network;Particle Swarm Optimization","Temperature control;MATLAB;Optimal control;Artificial neural networks;Predictive models;Mathematical model","backpropagation;boilers;delays;neurocontrollers;nonlinear control systems;optimal control;particle swarm optimisation;predictive control;real-time systems;stability;temperature control","superheated steam temperature predictive optimal control;external time-delay BP neural network;PSO algorithm;power industry;ultra-supercritical coal-fired power generating units;superheater system nonlinear characteristics;superheater system time delay;superheater system inertia;supercritical boiler;cascaded PID control;steam temperature control effect;intelligent predictive optimal controller;neural network modeling;NN predictive optimal controller;nonlinear dynamic model;real-time prediction accuracy;generalization ability;convergence rate;real-time control demand;elastic search space;real-time steam temperature control error;stability;full-scope simulator;supercritical power unit;MATLAB software;control simulation tests;power 600 MW","","","15","","","","","","IEEE","IEEE Conferences"
"Frontier-based multi-robot map exploration using Particle Swarm Optimization","Y. Wang; A. Liang; H. Guan","School of Software, Shanghai Jiao, Tong University, Shanghai Key Laboratory of Scalable Computing and Systems, Shanghai, 200240, China; School of Software, Shanghai Jiao, Tong University, Shanghai Key Laboratory of Scalable Computing and Systems, Shanghai, 200240, China; Department of Computer Science, and Engineering, Shanghai Jiao, Tong University, Shanghai Key Laboratory of Scalable Computing and Systems, Shanghai, 200240, China","2011 IEEE Symposium on Swarm Intelligence","","2011","","","1","6","Exploring an unknown environment using team of autonomous mobile robots is an important task in many real-world applications. Many existing map exploration algorithms are based on frontier, which is the boundary between unexplored space and known open space. In the context of multiple robots, the main problem of frontier-based algorithm is to choose appropriate target points for the individual robots so that they can efficiently explore the different part of the common area. This paper proposed a novel distributed frontier-based map exploration algorithm using Particle Swarm Optimization model for robot coordination. In this algorithm, the robot keeps moving to the nearby frontier to reduce the size of the unknown region, and is navigated towards frontier far away based on the PSO model after exploring the local area. The exploration is completed when there are no frontier cells on the map. Our algorithm has been implemented and tested both in simulation runs and real world experiment. The result shows that our method has a good scalability and efficiency.","","978-1-61284-052-9978-1-61284-053-6978-1-61284-051","10.1109/SIS.2011.5952584","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5952584","Robot;distributed map exploration;PSO","Legged locomotion;Robot kinematics;Collision avoidance;Algorithm design and analysis;Robot sensing systems;Redundancy","mobile robots;multi-robot systems;particle swarm optimisation","frontier-based multirobot map exploration;autonomous mobile robots;distributed frontier-based map exploration algorithm;robot coordination;particle swarm optimization model","","13","25","","","","","","IEEE","IEEE Conferences"
"A software defined testbed for reconfigurable antenna cognitive radio","K. Wanuga; D. Pfeil; D. Gonzalez; K. Dandekar","ECE Department, Drexel University, Philadelphia, PA 19104; ECE Department, Drexel University, Philadelphia, PA 19104; ECE Department, Drexel University, Philadelphia, PA 19104; ECE Department, Drexel University, Philadelphia, PA 19104","2012 7th International ICST Conference on Cognitive Radio Oriented Wireless Networks and Communications (CROWNCOM)","","2012","","","1","6","A cognitive radio is capable of sensing local radio conditions and adapting its transmission and reception parameters to optimize performance in the evolving network environment. The use of software defined radio (SDR) to make communications more flexible has greatly facilitated the design of cognitive radios, by making communications reconfigurable at lower layers of the networking stack. The Cognitive Antenna Testbed described in this paper augments the flexibility of physical layer design through the use of reconfigurable antennas, which are capable of adapting frequency, pattern and polarization electrically, thereby expanding the design space of cognitive radio and networking algorithms. This testbed offers a platform for developing and field-testing reconfigurable antenna algorithms. Representative results of experiments performed with this testbed for antenna state selection, are provided to illustrate its relevance to cognitive networking research. Current research adapting this technology for use in dynamic spectrum access (DSA) applications is also discussed.","2166-5419;2166-5370;2166-5370","978-1-936968-55-8978-1-4673-2976-7978-1-936968-55","10.4108/icst.crowncom.2012.249582","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6481096","","Adaptive arrays;Sensors;Cognitive radio;OFDM;MIMO;Transmitting antennas","antenna testing;cognitive radio;electromagnetic wave polarisation;radio access networks;radio reception;radio transmitters;software radio;wireless sensor networks","software defined radio testbed;local radio conditions sensing;reception parameter;transmission parameter;SDR;physical layer design;field-testing reconfigurable antenna cognitive radio algorithm;cognitive antenna state selection testbed;dynamic spectrum access application;DSA","","","11","","","","","","IEEE","IEEE Conferences"
"Solving Multimodal problems by Coincidence Algorithm","K. Waiyapara; P. Chongstitvatana","Department of Computer Engineering, Faculty of Engineering, Chulalongkorn University, Bangkok, Thailand; Department of Computer Engineering, Faculty of Engineering, Chulalongkorn University, Bangkok, Thailand","2012 Ninth International Conference on Computer Science and Software Engineering (JCSSE)","","2012","","","45","48","In general, Multimodal optimization is hard problems even for Evolutionary Algorithm. Using a Genetic Algorithm (GA) to solve these problems, the algorithm cannot converge to solutions easily. This work presents a study of Coincidence Algorithm (COIN) to solve these problems. COIN has an ability to retain multiple solutions in its model; hence it is suitable for Multimodal optimization problems. The experiment is carried out to illustrate this capability. The benchmarks are designed for comparing the problem solving behavior of COIN against a Genetic Algorithm.","","978-1-4673-1921-8978-1-4673-1920-1978-1-4673-1919","10.1109/JCSSE.2012.6261923","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6261923","Coincidence Algorithm;Multimodal Problem;Genetic Algorithm","Genetic algorithms;Benchmark testing;Evolutionary computation;Algorithm design and analysis;Traveling salesman problems;Optimization;Probabilistic logic","genetic algorithms","multimodal problems;coincidence algorithm;multimodal optimization problem;evolutionary algorithm;genetic algorithm;GA;COIN","","4","7","","","","","","IEEE","IEEE Conferences"
"VMAD: A virtual machine for advanced dynamic analysis of programs","A. Jimborean; M. Herrmann; V. Loechner; P. Clauss","INRIA Nancy-Grand Est (CAMUS), LSIIT Lab., CNRS, University of Strasbourg, France; INRIA Nancy-Grand Est (CAMUS), LSIIT Lab., CNRS, University of Strasbourg, France; INRIA Nancy-Grand Est (CAMUS), LSIIT Lab., CNRS, University of Strasbourg, France; INRIA Nancy-Grand Est (CAMUS), LSIIT Lab., CNRS, University of Strasbourg, France","(IEEE ISPASS) IEEE International Symposium on Performance Analysis of Systems and Software","","2011","","","125","126","Runtime code analysis and optimization is becoming a main strategy used to face the ever extending and changing variety of processor architectures and execution environments that an application can meet. Particularly with the advent of multicore processors, efficient program optimizations, such as adaptive and speculative parallelism, require accurate and advanced runtime analyses, which inevitably incur a time overhead that has to be minimized. In this paper, we present VMAD, a virtual machine (VM) that handles x86_54 binary files, which are especially tailored at compile time to include instructions and data for code instrumentation and for the VM. VMAD enables low level profiling initiated by the programmer from the source code, through the insertion of a dedicated pragma delimiting the regions of interest. This approach provides the programmer a direct view of the actual execution behavior of the source code. To our knowledge, VMAD is the first proposal providing low-level instrumentation initiated from the source code, with almost negligible runtime overhead.","","978-1-61284-368-1978-1-61284-367","10.1109/ISPASS.2011.5762725","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5762725","","Instruments;Runtime;Optimization;Assembly;Linux;Benchmark testing;Virtual machining","multiprocessing systems;program compilers;program diagnostics;source coding;virtual machines","VMAD;virtual machine for advanced dynamic analysis;runtime code analysis;processor architectures;multicore processors;program optimization;binary files;code instrumentation;source code;low-level instrumentation;program analysis","","3","4","","","","","","IEEE","IEEE Conferences"
"Agent-in-the-loop simulation for testing swarm robots using an XML-based system of system framework in discrete-event simulation specification","Y. P. Dragunov; F. Sahin","Computer Engineering, Rochester Institute of Technology, NY, USA; Electrical and Microelectronic Engineering, Rochester Institute of Technology, NY, USA","2011 6th International Conference on System of Systems Engineering","","2011","","","293","298","This work builds upon our previous work, where a swarm robot called a Groundscout was tested for its emergent behavior(s) using a system of systems framework based on discrete-event simulation and XML-based hardware in the loop simulation (HIL). This paper presents performance optimizations, bug-fixes, and implementations of new features to further examine the benefits of using HIL simulations to develop and test swarm robots. In this work, the Groundscout's communications protocols were improved, a new obstacle-avoidance algorithm was added, and the environment and simulation engines have been either re-implemented or optimized for performance. Specifically for the obstacle-avoidance algorithm, the benefits of using HIL simulations, together with the principle of model continuity, are clearly demonstrated with testing experiments. Towards the eventual goal of creating simulations for testing emergent behaviors of much larger robot swarms, considerations that were learned are presented.","","978-1-61284-782-5978-1-61284-783-2978-1-61284-781","10.1109/SYSOSE.2011.5966613","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5966613","Swarm robots;XML;Agent-in-the-loop;Hardware-in-the-loop;Testing;System of systems","Testing;Modeling;Robot sensing systems;Time division multiple access;Software","collision avoidance;control engineering computing;discrete event simulation;multi-robot systems","agent-in-the-loop simulation;swarm robot testing;XML-based system-of-system framework;discrete event simulation specification;Groundscout swarm robot;hardware in the loop simulation;Groundscout communication protocol;obstacle avoidance algorithm;model continuity principle","","1","15","","","","","","IEEE","IEEE Conferences"
"Speech-Based Five Features Extraction and Hardware Trade-Off Design for Mental Fatigue Monitoring","Z. Li; W. Li; Z. Yao; Y. Li","NA; NA; NA; NA","2011 International Conference of Information Technology, Computer Engineering and Management Sciences","","2011","1","","68","71","After combing the known advantages and disadvantages of the normal methods on human mental fatigue test, we measure speech signals in condition of fatigue-free and fatigue, then attract key features, and then analyze their validities. More interesting works address corresponding feature-modules designed with EDA tools. The methods are outlined as: 4~5 seconds data are sampled by PC embedded MATLAB software platform, the mean-value ratio of pre- and post-features is defined into evaluation threshold. And with the help of EDA tools named of System Generator and Xilinx ISE 11, our feature-modules design optimization are marched onto complexities' report. The results show that five key features' validity-sequence can list as (1) short-time energy (mean-bias 298%), (2) short-time mean amplitude (41.9%), (3) short-time cross-zero ratio (15.9%), (4) short-time self-correlation (12%), and (5) pitch (10%), just about same as their modules' structure-sophistications. Compared with other advanced tricks on mental fatigue tests, our works may accelerate low power consumption application combined with trade-off design thinking for future study of cerebral health monitoring.","","978-1-4577-1419-1978-0-7695-4522","10.1109/ICM.2011.163","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113357","mental fatigue;speech signal;MATLAB;speech features;hardware modeling;optimization","Fatigue;Speech;Feature extraction;Hardware;Humans;MATLAB;Generators","circuit optimisation;electronic design automation;embedded systems;feature extraction;psychometric testing;speech recognition","speech-based feature extraction;hardware trade-off design;mental fatigue monitoring;human mental fatigue test;speech signals;fatigue-free speech signals;EDA tools;PC embedded MATLAB software platform;mean-value ratio;evaluation threshold;system generator;Xilinx ISE 11;feature-modules design optimization;short-time energy;short-time mean amplitude;short-time cross-zero ratio;short-time self-correlation;pitch","","1","16","","","","","","IEEE","IEEE Conferences"
"Analysis method on maneuvering motion pattern of trimaran unmanned surface vessel","C. Jian; W. Wei; Y. Songlin; W. Pengyu; Y. Jingpin; C. Peng","School of Naval Architecture and Ocean Engineering, Jiangsu University of Science and Technology, Zhenjiang, Jiangsu, China; School of Naval Architecture and Ocean Engineering, Jiangsu University of Science and Technology, Zhenjiang, Jiangsu, China; School of Naval Architecture and Ocean Engineering, Jiangsu University of Science and Technology, Zhenjiang, Jiangsu, China; School of Naval Architecture and Ocean Engineering, Jiangsu University of Science and Technology, Zhenjiang, Jiangsu, China; School of Naval Architecture and Ocean Engineering, Jiangsu University of Science and Technology, Zhenjiang, Jiangsu, China; School of Naval Architecture and Ocean Engineering, Jiangsu University of Science and Technology, Zhenjiang, Jiangsu, China","2010 International Conference on Computer and Information Application","","2010","","","186","189","A new type of trimaran unmanned surface vessel has been designed, we studied on its maneuverability, and finished 5°and 10°rudder angle maneuverability test. We selected the genetic algorithm as the optimization method, identified the test data of trimaran unmanned surface vessel maneuverability by software. Having compared the identified results with the test data and then analyzed the error, we made conclusion that the identified results well matched the test results from groups of compared data plots.","","978-1-4244-8598-7978-1-4244-8597","10.1109/ICCIA.2010.6141567","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6141567","trimaran unmanned surface vessel;identification system;genetic algorithm;Z type maneuverability test","Sea surface;Mathematical model;Marine vehicles;Optimization;Global Positioning System;System identification;Acceleration","control engineering computing;genetic algorithms;marine vehicles;mobile robots;motion control;remotely operated vehicles","analysis method;maneuvering motion pattern;trimaran unmanned surface vessel;rudder angle maneuverability test;genetic algorithm;optimization method","","","10","","","","","","IEEE","IEEE Conferences"
"Research on hardware-in-the-loop simulation for attitude control of vehicles with dSPACE emulator","X. Daocheng; W. Zhongwei","College of Aerospace and Material Engineering, National University of Defense Technology, Changsha 410073, P.R. China; College of Aerospace and Material Engineering, National University of Defense Technology, Changsha 410073, P.R. China","Proceedings of the 31st Chinese Control Conference","","2012","","","4550","4555","Lateral jet and grid fin combined control is a new attitude control method, and has advantage in reducing aerocraft weight and improving control efficiency. For checking the control capability and control method optimization, hardware-in-the-loop simulation (HILS) system should be established. Attitude stability of aerocraft and propellant consumption of lateral jet system can be analyzed based on the simulation data, then the engineers may evaluate the performance of combined control method. Hardware-in-the-loop simulation system of aerocraft control based on dSPACE is established in this paper, has real-time simulation capability that complicated model calculating needs, and solves the problems of high data communication and rapid development of simulation system in HILS. This paper introduces the hardware architecture and key technologies of the simulation system. Ground simulation of aerocraft attitude is realized with communication between DS4201s card and turn table, feedback voltage acquisition of two actuating mechanism and A/D conversion is solved using DS2002 card, the control algorithm is downloaded to DSP processor in the onboard computer using the RTW tool. Single instrument tests and interaction between two instruments are finished. Testing results of the hardware indicate that communication and signal transferring are proper. Attitude simulation is realized with serial-port communication between dSPACE and turn table, interference in A/D acquisition channel is reduced by adding filtering module in DS2002. Attitude control in the ascending phase of aerocraft is simulated under the HILS environment. Actuator deflection and on-off status of lateral jet system are derived by analyzing HILS results. We can examine and certify the control capability of lateral jet and grid fin combined control method and optimize the control algorithm.","2161-2927;1934-1768;1934-1768","978-988-15638-1-1978-1-4673-2581-3978-988-15638-1","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6390726","Combined Control;Hardware-in-the-loop simulation;dSPACE;Attitude Simulation;Feedback Acquisition","Attitude control;Data models;Mathematical model;Digital signal processing;Software packages;Vehicles;Optimization","aerospace propulsion;aircraft computers;aircraft instrumentation;aircraft testing;analogue-digital conversion;attitude control;digital signal processing chips;feedback;interference suppression;peripheral interfaces;stability","hardware-in-the-loop simulation;vehicle attitude control;dSPACE emulator;HILS;control method optimization;attitude stability;propellant consumption;lateral jet system;performance evaluation;real-time simulation;high-data communication problems;hardware architecture;aerocraft attitude ground simulation;DS4201s card;turn table;feedback voltage acquisition;A/D conversion;DS2002 card;serial-port communication;A/D acquisition channel interference;filtering module;aerocraft ascending phase;actuator deflection;on-off status;grid fin control;DSP processor;onboard computer;RTW tool;digital signal processing chips","","","19","","","","","","IEEE","IEEE Conferences"
"Implementation of a low-cost reconfigurable antenna array for SDR-based communication systems","M. Donelli; C. Sacchi","University of Trento, Dept. of Information Engineering, and Computer Science (DISI), Via Sommarive, 5, Trento (Italy), I-38123; University of Trento, Dept. of Information Engineering, and Computer Science (DISI), Via Sommarive, 5, Trento (Italy), I-38123","2012 IEEE Aerospace Conference","","2012","","","1","7","In the last years there has been a growing interest in software-defined radio communication systems (SDR). SDR technologies are attractive for communication systems because of their reconfigurable capabilities. Due to the complexity of real scenarios, usually characterized by multipath propagation and heterogeneous interfering sources, the antenna plays a key role. It is demonstrated by literature that the use of reconfigurable antennas (known also as smart antennas) could dramatically improve system performance. In this paper, we propose a compact and low-cost antenna array, based on microchip antennas, which will be integrated with a low-cost GNU-radio SDR testbed. The proposed antenna reconfigures its radiation characteristics, turning on/off the array elements, by means of radio frequency (RF) switches. The antenna is software controlled by means of a suitable optimization technique that maximizes the signal-to-noise ratio (SNR). Test results can prove the capability of the proposed antenna to improve the performance of SDR telecommunication systems in terms of increased SNR and reduced bit error rate.","1095-323X;1095-323X","978-1-4577-0557-1978-1-4577-0556-4978-1-4577-0555","10.1109/AERO.2012.6187130","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6187130","","Arrays;Antenna arrays;Dipole antennas;Software radio;Prototypes;Software","antenna arrays;multipath channels;optimisation;software radio","low-cost reconfigurable antenna array;SDR-based communication systems;software-defined radio communication systems;multipath propagation;heterogeneous interfering sources;microchip antennas;low-cost GNU-radio SDR testbed;radio frequency switches;RF switches;optimization technique;signal-to-noise ratio;SNR","","2","20","","","","","","IEEE","IEEE Conferences"
"Compiling SyncCharts to Synchronous C","C. Traulsen; T. Amende; R. von Hanxleden","Department of Computer Science, Christian-Albrechts-Universit&#x00E4;t zu Kiel; Department of Computer Science, Christian-Albrechts-Universit&#x00E4;t zu Kiel; Department of Computer Science, Christian-Albrechts-Universit&#x00E4;t zu Kiel","2011 Design, Automation & Test in Europe","","2011","","","1","4","SyncCharts are a synchronous Statechart variant to model reactive systems with a precise and deterministic semantics. The simulation and software synthesis for SyncCharts usually involve the compilation into Esterel, which is then further compiled into C code. This can produce efficient code, but has two principal drawbacks: 1) the arbitrary control flow that can be expressed with SyncChart transitions cannot be mapped directly to Esterel, and 2) it is very difficult to map the resulting C code back to the original SyncChart, which hampers traceability. This paper presents an alternative software synthesis approach for SyncCharts that compiles SyncCharts directly into Synchronous C (SC). The compilation preserves the structure of the original SyncChart, which is advantageous for validation and possibly certification. We present a static thread-scheduling scheme that reflects data dependencies and optimizes both the number of used threads as well as the maximal used priorities. This results in SC code with competitive speed and little memory requirements.","1558-1101;1530-1591;1530-1591","978-3-9810801-8-6978-1-61284-208-0978-3-9810801-7","10.1109/DATE.2011.5763284","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5763284","","Semantics;Computational modeling;Instruction sets;Writing;Concurrent computing;Schedules;Optimization","flowcharting;multi-threading;processor scheduling;program compilers;synchronisation","SyncChart compiling;Synchronous C;synchronous statechart variant;reactive systems;deterministic semantics;software synthesis;Esterel;C code;arbitrary control flow;SyncChart transition;static thread-scheduling scheme","","2","10","","","","","","IEEE","IEEE Conferences"
"Source Code Partitioning in Program Optimization","M. Bolat; K. Kelsey; X. Li; G. R. Gao","NA; NA; NA; NA","2011 IEEE 17th International Conference on Parallel and Distributed Systems","","2011","","","56","63","Program analysis and program optimization seek to improve program performance. There are optimization techniques which are applied to various scopes such as a source file, function or basic block. Inter-procedural program optimization techniques have the scope of source file and analyze the interaction and relationship between different program functions. The techniques analyze the entire translation unit (typically a source file) and optimize the whole translation unit globally instead of just optimizing inside a function. Analyzing and optimizing an entire translation unit increases compilation time drastically because many factors need to be considered during analysis and optimization. The translation unit size can be quite large, containing many functions. Another issue is that functions in different translation units can be more closely related to each other than to the functions within their translation unit. The main goal of this research is grouping or partitioning of closely related program functions into the same translation unit. Our method profiles an application, determines relationship information between program functions and groups closely related functions together. The source code partitioner method improves the processing time of inter-procedural optimization techniques by applying it to a subset of program functions. Partitioning of program functions by analyzing profiling output shows dramatic decrease in compilation time of programs. Our results show we can improve the compiling time in all tested real world benchmarks.","1521-9097","978-0-7695-4576-9978-1-4577-1875","10.1109/ICPADS.2011.125","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6121260","compiler;optimization;partitioning;instrumentation;betweenness;agglomerative;divisive","Clustering algorithms;Partitioning algorithms;Optimization;Communities;Mutual information;Software algorithms;USA Councils","program compilers;program diagnostics;program interpreters","source code partitioning;program analysis;program performance;interprocedural program optimization technique;translation unit;source file;program function;program compilation time","","1","8","","","","","","IEEE","IEEE Conferences"
"Optimization of Combination among Key Components at Air Side in Vehicle Thermal Management System","J. Ni; W. Yu; X. Shi; N. Gu","NA; NA; NA; NA","2010 International Conference on System Science, Engineering Design and Manufacturing Informatization","","2010","1","","310","313","In this paper, the basic data of vehicle thermal management system (VTMS) are obtained from VTMS bench test. A 1-D model of VTMS is built in KULI software, and comparison between simulation result and test data proves that the model can simulate practical situation well. Then several combinations of key components at air side (radiator, charge air cooler and fan) in VTMS, which are obtained by DoE (design of experiment) technology, are computed in KULI. Finally, the simulation result is analyzed in Microsoft Excel and SPSS, and an optimal combination is acquired, which can not only ensure enough cooling capacity but also reduce engine power consumption remarkably. The method of combination of software and design of experiment in this paper is very useful to design and optimize VTMS and other related systems.","","978-1-4244-8664","10.1109/ICSEM.2010.90","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5640303","vehicle thermal management system;optimization;design of experiment;simulation;power consumption","Manufacturing","automotive components;cooling;design of experiments;vehicle dynamics","vehicle thermal management system;VTMS;KULI software;air side;radiator;charge air cooler;fan;DoE;design-of-experiment technology;Microsoft Excel;SPSS","","","6","","","","","","IEEE","IEEE Conferences"
"A Image Thresholding Method Based on Binary Coded Ant Colony Algorithm","Z. Ye; Z. Hu; H. Wang; W. Liu","NA; NA; NA; NA","2010 2nd International Workshop on Intelligent Systems and Applications","","2010","","","1","4","Image segmentation is the most significant step in image analysis and is a long-term difficult problem, which hasn't been fully solved. Many segmentation methods have been brought forward to deal with image segmentation, among these methods thresholding is the simple and important method in image segmentation. In practical work, 2-dimension (2D) entropy method is often used. It segments images by using the gray value of the pixel and the local average gray value of it, and thus provides better results than that of one-dimension entropy. However, for more accurate thresholding, much more time has to pay. Thus, this paper employs a novel approach to 2D threshold selection based on binary coded ant colony optimization algorithm. The proposed approach has been implemented and tested on several real images. Experiments results indicate that proposed method performs well which is a good method to help select optimum 2D thresholds.","","978-1-4244-5874-5978-1-4244-5872","10.1109/IWISA.2010.5473306","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5473306","","Image segmentation;Entropy;Pixel;Ant colony optimization;Computer science;Image edge detection;Image analysis;Digital images;Software algorithms;Testing","binary codes;entropy codes;Gray codes;image segmentation;optimisation","image thresholding method;binary coded ant colony algorithm;image segmentation;image analysis;2-dimension entropy method;gray value;one-dimension entropy;optimum 2D threshold selection","","","11","","","","","","IEEE","IEEE Conferences"
"Influence Analysis of a Passenger Car's Universality Design on Ride Comfort","R. He; L. Sun","NA; NA","2010 2nd International Conference on Information Engineering and Computer Science","","2010","","","1","4","To strengthen the parts universality of a series of passenger cars and improve a passenger car ride comfort, the elastic components and the installation location of the vehicles are optimized. The optimization scheme of widening, lengthening the rear suspension leaf spring, and narrowing diameter of the front suspension torsion bar spring are analyzed, then the front and rear suspension stiffness match is done. The optimal passenger car model is built by ADAMS software to simulate, and the weighted RMS acceleration is calculated by MATLAB, the results meet the requirements. Then, the real vehicle test is done, and the comparative analysis of the performance with the original passenger car is done. The results show that, the optimization model' s ride comfort index than that of the original passenger cars have greatly increased, and comprehensive evaluation of steady static circular is also slightly higher scores.","2156-7387;2156-7379","978-1-4244-7941-2978-1-4244-7939","10.1109/ICIECS.2010.5678363","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5678363","","Suspensions;Vehicles;Springs;Mathematical model;Optimization;Stability analysis;Roads","automobiles;design;mechanical engineering computing;optimisation;suspensions (mechanical components)","passenger car universality design;ride comfort;optimization scheme;rear suspension leaf spring;front suspension torsion bar spring;ADAMS software;weighted RMS acceleration;MATLAB","","1","10","","","","","","IEEE","IEEE Conferences"
"Prototyping control and data acquisition for the ITER Neutral Beam Test Facility","A. Luchetta; G. Manduchi; C. Taliercio; A. Soppelsa; F. Paolucci; F. Sartori; P. Barbato; R. Capobianco; M. Breda; F. Molon; M. Moressa; S. Polato; P. Simionato; E. Zampiva","Consorzio RFX. Corso Stati Uniti 4, 35127 Padova Italy; Consorzio RFX. Corso Stati Uniti 4, 35127 Padova Italy; Consorzio RFX. Corso Stati Uniti 4, 35127 Padova Italy; Consorzio RFX. Corso Stati Uniti 4, 35127 Padova Italy; Fusion for Energy, Josep Pla, 2 07/30 Torres Diagonal Litoral B3, 08019 Barcelona Spain; Fusion for Energy, Josep Pla, 2 07/30 Torres Diagonal Litoral B3, 08019 Barcelona Spain; Consorzio RFX. Corso Stati Uniti 4, 35127 Padova Italy; Consorzio RFX. Corso Stati Uniti 4, 35127 Padova Italy; Consorzio RFX. Corso Stati Uniti 4, 35127 Padova Italy; Consorzio RFX. Corso Stati Uniti 4, 35127 Padova Italy; Consorzio RFX. Corso Stati Uniti 4, 35127 Padova Italy; Consorzio RFX. Corso Stati Uniti 4, 35127 Padova Italy; Consorzio RFX. Corso Stati Uniti 4, 35127 Padova Italy; Consorzio RFX. Corso Stati Uniti 4, 35127 Padova Italy","2012 18th IEEE-NPSS Real Time Conference","","2012","","","1","7","The ITER Neutral Beam Test Facility will be the project's R&D facility for heating neutral beam injectors (HNB) for fusion research operating with H/D negative ions. Its mission is to develop technology to build the HNB prototype injector meeting the stringent HNB requirements (16.5 MW injection power, -1 MVe acceleration energy, 40 A ion current and one hour continuous operation). Two test-beds will be built in sequence in the facility: first SPIDER, the ion source test-bed, to optimize the negative ion source performance, second MITICA, the actual prototype injector, to optimize ion beam acceleration and neutralization. The SPIDER control and data acquisition system is under design. To validate the main architectural choices, a system prototype has been assembled and performance tests have been executed to assess the prototype's capability to meet the control and data acquisition system requirements. The prototype is based on open-source software frameworks running under Linux. EPICS is the slow control engine, MDSplus is the data handler and MARTe is the fast control manager. The prototype addresses low and high-frequency data acquisition, 10 kS/s and 10 MS/s respectively, camera image acquisition, data archiving, data streaming, data retrieval and visualization, real time fast control with 100 μs control cycle and supervisory control. The paper shows how the proposed prototype architecture can address the most critical requirements of SPIDER control.","","978-1-4673-1084-0978-1-4673-1082-6978-1-4673-1083","10.1109/RTC.2012.6418191","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6418191","","Prototypes;Data acquisition;Synchronization;Servers;Structural beams;Memory;Ion sources","cameras;data acquisition;data handling;data visualisation;information retrieval;ion sources;Linux;negative ions;physical instrumentation control;plasma beam injection heating;plasma sources;plasma toroidal confinement;plasma transport processes;public domain software;research and development;Tokamak devices","ITER neutral beam test facility;R and D facility;neutral beam injection heating;H negative ions;D negative ions;HNB prototype injector meeting;ion current;ion source test-bed;negative ion source performance;MITICA;prototype control;SPIDER control;ion beam acceleration;ion beam neutralization;data acquisition system;open-source software frameworks;Linux;control engine;MDSplus;data handler;MARTe;high-frequency data acquisition;data streaming;data retrieval;data visualization;real time fast control;prototype architecture;camera image acquisition;data archiving;power 16.5 MW;current 40 A","","","17","","","","","","IEEE","IEEE Conferences"
"Multi-objective aware extraction of task-level parallelism using genetic algorithms","D. Cordes; P. Marwedel","TU Dortmund University, Germany; TU Dortmund University, Germany","2012 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2012","","","394","399","A large amount of research work has been done in the area of automatic parallelization for decades, resulting in a huge amount of tools, which should relieve the designer from the burden of manually parallelizing an application. Unfortunately, most of these tools are only optimizing the execution time by splitting up applications into concurrently executed tasks. In the domain of embedded devices, however, it is not sufficient to look only at this criterion. Since most of these devices are constraint-driven regarding execution time, energy consumption, heat dissipation and other objectives, a good trade-off has to be found to efficiently map applications to multiprocessor system on chip (MPSoC) devices. Therefore, we developed a fully automated multi-objective aware parallelization framework, which optimizes different objectives at the same time. The tool returns a Pareto-optimal front of solutions of the parallelized application to the designer, so that the solution with the best trade-off can be chosen.","1558-1101;1530-1591;1530-1591","978-1-4577-2145-8978-3-9810801-8","10.1109/DATE.2012.6176503","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6176503","Automatic Parallelization;Embedded Software;Multi-Objective;Genetic Algorithms;Task-Level Parallelism;Energy awareness","Energy consumption;Parallel processing;Genetic algorithms;Optimization;Data mining;Image edge detection;Benchmark testing","genetic algorithms;multiprocessing systems;parallel processing;Pareto optimisation;power aware computing;system-on-chip","embedded devices;constraint-driven execution time;energy consumption;heat dissipation;multiprocessor system on chip devices;MPSoC device;automated multiobjective aware parallelization framework;Pareto optimal front;genetic algorithm;task-level parallelism","","6","23","","","","","","IEEE","IEEE Conferences"
"The design and field application of high-resolution pushbroom imaging spectrometer","Q. Zhou; G. Wu; Q. Wang; C. Shi; Y. Tong; Z. Ma","University of Science and Technology of China, Anhui Hefei 230029, China; University of Science and Technology of China, Anhui Hefei 230029, China; University of Science and Technology of China, Anhui Hefei 230029, China; University of Science and Technology of China, Anhui Hefei 230029, China; Shanghai Institute of Applied Physics, Chinese Academy of Sciences, 201800, China; Beijing Research Center for Agrifood Testing and Farmland Monitoring, 100089, China","Proceedings of the 29th Chinese Control Conference","","2010","","","3031","3035","A design of a high-resolution pushbroom imaging spectrometer, including optical system, data acquisition system, image stitching algorithm and interactive application software. The instrument has been successfully applied in the field crop spectra acquisition, provide a new method of information acquisition for the precision agriculture.","1934-1768;2161-2927","978-7-8946-3104-6978-1-4244-6263-6978-7-8946-3104","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5572169","Imaging Spectrometer;Offner;Image Stitching;Field Application","Optical imaging;Optical sensors;Software;Hyperspectral imaging;Optimized production technology;Optical design","agriculture;crops;infrared imaging;spectrometers","high-resolution pushbroom imaging spectrometer;optical system;data acquisition;image stitching algorithm;interactive application software;field crop spectra acquisition;agriculture","","","15","","","","","","IEEE","IEEE Conferences"
"Industrial Case Study on Supporting the Comprehension of System Behaviour under Load","M. D. Syer; B. Adams; A. E. Hassan","NA; NA; NA","2011 IEEE 19th International Conference on Program Comprehension","","2011","","","215","216","Large-scale software systems achieve concurrency on enormous scales using a number of different design patterns. Many of these design patterns are based on pools of pre-existing and reusable threads that facilitate incoming service requests. Thread pools limit thread lifecycle overhead (thread creation and destruction) and resource thrashing (thread proliferation). Despite their potential for scalability, thread pools are hard to configure and test because of concurrency risks like synchronization errors and dead lock, and thread pool-specific risks like resource thrashing and thread leakage. Addressing these challenges requires a thorough understanding of the behaviour of the threads in the thread pool. We argue for a methodology to automatically identify and rank deviations in the behaviour of threads based on resource usage.","1092-8138;1092-8138","978-1-61284-308-7978-0-7695-4398","10.1109/ICPC.2011.38","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5970187","thread-pools;behaviour-based clustering;understanding ULS systems","Instruction sets;Software systems;Software engineering;Measurement;Performance analysis;Instruments","reverse engineering;software architecture;software maintenance","system behaviour comprehension;large-scale software systems;preexisting thread pool;reusable thread pool;thread lifecycle overhead;thread creation;thread destruction;thread proliferation","","3","7","","","","","","IEEE","IEEE Conferences"
"Fuzzy clustering and relevance ranking of web search results with differentiating cluster label generation","T. Matsumoto; E. Hung","Department of Computing, Hong Kong Polytechnic University, Hong Kong; Department of Computing, Hong Kong Polytechnic University, Hong Kong","International Conference on Fuzzy Systems","","2010","","","1","8","This paper introduces a prototype web search results clustering engine that enhances search results by performing fuzzy clustering on web documents returned by conventional search engines, as well as ranking the results and labeling the resulting clusters. This is done using a fuzzy transduction-based clustering algorithm (FTCA), which employs a transduction-based relevance model (TRM) to generate document relevance values. These relevance values are used to cluster similar documents, rank them, and facilitate a term frequency based label generator. The membership degrees of documents to fuzzy clusters also facilitates effective detection and removal of overly similar clusters. FTCA is compared against two other established web document clustering algorithms: Suffix Tree Clustering (STC) and Lingo, which are provided by the free open source Carrot<sup>2</sup> Document Clustering Workbench. To measure cluster quality, an extended version of the classic precision measurement is used to take into account relevance and fuzzy clustering, along with recall and F1 score. Results from testing on five different datasets show a considerable clustering quality and performance advantage over STC and Lingo in most cases.","1098-7584;1098-7584","978-1-4244-6921-5978-1-4244-6919-2978-1-4244-6920","10.1109/FUZZY.2010.5584771","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5584771","","Clustering algorithms;Search engines;Web search;Prototypes;Transmission line measurements;Labeling;Testing","fuzzy set theory;Internet;pattern clustering;public domain software;search engines;trees (mathematics)","relevance ranking;differentiating cluster label generation;Web search results clustering engine;search engines;fuzzy transduction-based clustering algorithm;transduction-based relevance model;term frequency;Web document clustering algorithms;suffix tree clustering;Lingo;open source carrot<sup>2</sup> document clustering workbench;precision measurement","","3","16","","","","","","IEEE","IEEE Conferences"
"Quantum-Inspired Evolutionary Algorithms applied to numerical optimization problems","A. V. A. da Cruz; M. M. B. R. Vellasco; M. A. C. Pacheco","Applied Computational Intelligence Lab - Electrical Engineering Department, Pontifical Catholic University of Rio de Janeiro, Brazil; Applied Computational Intelligence Lab - Electrical Engineering Department, Pontifical Catholic University of Rio de Janeiro, Brazil; Applied Computational Intelligence Lab - Electrical Engineering Department, Pontifical Catholic university of Rio de Janeiro, Brazil","IEEE Congress on Evolutionary Computation","","2010","","","1","6","Since they were proposed as an optimization method, the evolutionary algorithms have been successfully used for solving complex problems in several areas such as, for example, the automatic design of electronic circuits and equipments, task planning and scheduling, software engineering and data mining, among many others. However, some problems are computationally intensive when it concerns the evaluation of solutions during the search process, making the optimization by evolutionary algorithms a slow process for situations where a quick response from the algorithm is desired (for instance, in online optimization problems). Several ways to overcome this problem, by speeding up convergence time, were proposed, including Cultural Algorithms and Coevolutionary Algorithms. However, these algorithms still have the need to evaluate many solutions on each step of the optimization process. In problems where this evaluation is computationally expensive, the optimization can take a prohibitive time to reach optimal solutions. This work presents an evolutionary algorithm for numerical optimization problems (Quantum-Inspired Evolutionary Algorithm for Problems based on Numerical Representation - QIEA-R), inspired in the concept of quantum superposition, which allows the optimization process to be carried on with a smaller number of evaluations. It extends previous works by presenting a broader range of tests and improvements on the algorithm. The results show the good performance of this algorithm in solving numerical problems.","1089-778X;1941-0026","978-1-4244-6911-6978-1-4244-6909-3978-1-4244-6910","10.1109/CEC.2010.5586193","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5586193","","Optimization;Benchmark testing;Equations;Quantum computing;Particle swarm optimization;Mathematical model","convergence;evolutionary computation;quantum computing;search problems","quantum-inspired evolutionary algorithm;numerical optimization problem;search process;convergence time speeding;cultural algorithm;coevolutionary algorithm;numerical representation;quantum superposition","","7","10","","","","","","IEEE","IEEE Conferences"
"Research on multi-object clonal genetic algorithm for the 0-1 knapsack","Wang Xing; Zhang Wenpeng","Software Department Nanyang Normal University, China; Software Department Nanyang Normal University, China","2010 International Conference on Educational and Network Technology","","2010","","","82","85","Immune clonal algorithm (ICA) is applied to genetic algorithm (GA) to develop a class of multi-object clonal genetic algorithm (MOCGA) for combinatorial optimization. With the condition of preserving simulated annealing advantages, MOCGA takes advantage of ICA algorithm so as to avoid premature convergence. To demonstrate its effectiveness and applicability, experiments are carried out on the 0-1 knapsack problem. The results show that MOCGA performs well, without premature convergence as compared to GA.","","978-1-4244-7662-6978-1-4244-7660","10.1109/ICENT.2010.5532132","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5532132","knapsack problem;genetic algorithm;multi-object clonal genetic algorithm","Genetic algorithms;Convergence;Independent component analysis;Biological cells;Helium;Costs;Educational technology;Software algorithms;Simulated annealing;Testing","combinatorial mathematics;genetic algorithms;knapsack problems;simulated annealing","multiobject clonal genetic algorithm;immune clonal algorithm;combinatorial optimization;premature convergence","","","10","","","","","","IEEE","IEEE Conferences"
"An analysis model of botnet tracking based on ant colony optimization algorithm","Ping Wang; Tzu Chia Wang; Pu-Tsun Kuo; Chin Pin Wang","Department of Information Management, Kun Shan University, Taiwan; Department of Information Management, Kun Shan University, Taiwan; Department of Information Management, Kun Shan University, Taiwan; Department of Information Management, Kun Shan University, Taiwan","The 6th International Conference on Networked Computing and Advanced Information Management","","2010","","","606","611","Available botnet detection schemes all supposed that ISPs would be cooperative to record or generate the necessary routing information for path reconstruction. In practice, ISP's service constantly is a mutual benefit for intelligence exchange. Therefore the constraint, require cooperation between ISPs, ought to be relaxed. A new IP traceback scheme based on ant colony optimization (ACO) algorithm is proposed for incomplete routing logs are provided. The aim of our work is to develop an analysis model for reconstruction of attack paths to traceback the botnet C&C via ant-inspired collective intelligence by calculating the pheromone to find possible routes with support and confidence degree. The validation of model uses NS2 (Network Simulator, version2) complied by dark IP map, to simulate the scenario of fake IP attack, to test the effectiveness of model. Furthermore, sensitivity analysis is conducted to investigate significant parameters' effect on the output of attack paths. Experimental results show that the proposed approach effectively suggests the best attack path of botnet in a dynamic network environment.","","978-89-88678-26-8978-1-4244-7671","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5573232","Botnet;attack path;ant colony optimization","Nickel;Facsimile","optimisation;software agents","ant colony optimization algorithm;botnet detection;routing information;ISP service;sensitivity analysis","","1","6","","","","","","IEEE","IEEE Conferences"
"Molecular dynamics simulation: Implementation and optimization based on Hadoop","S. Jiao; C. He; Y. Dou; H. Tang","College of Computer Science and Technology, Chongqing University of Posts and Telecommunications, China; Department of Computer Science and Engineering, University of Nebraska-Lincoln, USA; Department of Physical Sciences, Nicholls State University, Thibodaux, USA; College of Computer Science and Technology, Chongqing University of Posts and Telecommunications, China","2012 8th International Conference on Natural Computation","","2012","","","1203","1207","At first the paper introduced the basic concept and mechanism of Hadoop architecture and the current parallel research situation of molecular dynamics simulation, then three solutions were proposed for implementing the parallel algorithm using atom-decomposition method on Hadoop to overcome shortcomings of fast iteration and communication among sub-tasks. After that tested and analyzed program's speed-up ratio, expansibility and cost-time of various parts, then did performance tuning in many ways, finally obtained good results as high as 28× speedup in large-scale system simulations. The test showing molecular dynamics simulation based on Hadoop is more economical and practical.","2157-9563;2157-9555;2157-9555","978-1-4577-2133-5978-1-4577-2130-4978-1-4577-2132","10.1109/ICNC.2012.6234529","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6234529","Hadoop;molecular dynamics;MapReduce;cloud computing;optimization","Computational modeling;Synchronization;Scientific computing;Load modeling;Educational institutions;Cloud computing","iterative methods;optimisation;parallel algorithms;parallel architectures;public domain software","molecular dynamics simulation;optimization;Hadoop architecture;parallel algorithm;atom decomposition method;iteration algorithm;performance tuning","","3","15","","","","","","IEEE","IEEE Conferences"
"LIN network simulation system based On LabVIEW","Feng Luo; Renjun Li","College of Automotive Engineering, Tongji University, Shanghai, China; College of Automotive Engineering, Tongji University, Shanghai, China","2010 International Conference On Computer Design and Applications","","2010","5","","V5-299","V5-303","LIN (LIN Interconnect Network) Network is widely used in automotive communication system. The widespread usage of LIN network makes the test on communication of LIN nodes and optimization of LIN application layer protocol necessary. Hence, a LIN network simulation system based on LabVIEW is introduced in this paper.The system is consisted of a PC software based on LabVIEW and a USB-LIN smart card. It can simulate up to 16 LIN nodes so as to test LIN communication in an incomplete network. The LIN driver in the smart card is built up with standard LIN 2.1 APIs, so the system can send configuration and identification commands to other LIN nodes when simulating a master node to modify the configuration in other nodes.The hardware of the smart card includes a USB communication module and a LIN communication module, the controller of the card is Freescale's 16-bit MCU. The imbedded program mainly handles the LIN communication while the PC software undertakes the simulation, configuration and monitoring function of system. Finally a test with the help of CANoe verifies the simulation function of the system.","","978-1-4244-7164-5978-1-4244-7164","10.1109/ICCDA.2010.5541113","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5541113","LIN2.1 API;LabVIEW;simulation system","Smart cards;Testing;Automotive engineering;Application software;Protocols;Communication standards;Hardware;Universal Serial Bus;Communication system control;Communication system software","application program interfaces;automotive engineering;controller area networks;device drivers;field buses;protocols;smart cards;virtual instrumentation","LIN network simulation system;LabVIEW;LIN interconnect network;automotive communication system;LIN nodes;LIN application layer protocol optimisation;PC software;USB-LIN smart card;LIN 2.1 API;LIN driver;USB communication module;LIN communication module;CANoe;Freescale MCU","","1","8","","","","","","IEEE","IEEE Conferences"
"Alternative hyper-heuristic strategies for multi-method global optimization","J. Grobler; A. P. Engelbrecht; G. Kendall; V. S. S. Yadavalli","Denel Dynamics Pty (Ltd), Department of Industrial and Systems Engineering at the University of Pretoria, South Africa; Department of Computer Science at the University of Pretoria, South Africa; School of Computer Science at the University of Nottingham, UK; Department of Industrial and Systems Engineering at the University of Pretoria, South Africa","IEEE Congress on Evolutionary Computation","","2010","","","1","8","The purpose of this paper is to investigate the use of meta-heuristics as low-level heuristics in a hyper-heuristic framework. A novel multi-method hyper-heuristic algorithm which makes use of a number of common meta-heuristics is presented. Algorithm performance is evaluated on a diverse set of real parameter benchmark problems and meaningful conclusions are drawn with respect to the selection of alternative low-level heuristics and the acceptance of the obtained solutions within the proposed multi-method meta-heuristic approach.","1089-778X;1941-0026","978-1-4244-6911-6978-1-4244-6909-3978-1-4244-6910","10.1109/CEC.2010.5585980","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5585980","","Optimization;Heuristic algorithms;Algorithm design and analysis;Benchmark testing;Evolutionary computation;Equations;Software algorithms","heuristic programming;optimisation","multimethod global optimization;multimethod hyper-heuristic algorithm;alternative low-level heuristics;common metaheuristics","","17","20","","","","","","IEEE","IEEE Conferences"
"Biomedical image processing with GPGPU using CUDA","A. R. Firstauthor; I. B. Secondauthor; G. V. Thirdauthor; P. B. Fourthauthor; M. K. Fifthauthor","Óbuda University/John von Neumann Faculty of Informatics, Budapest, Hungary; Óbuda University/John von Neumann Faculty of Informatics, Budapest, Hungary; Semmelweiss University/2nd Department of Internal Medicine, Budapest, Hungary; Óbuda University/John von Neumann Faculty of Informatics, Budapest, Hungary; MTA SZTAKI/Laboratory of Parallel and Distributed Computing, Budapest, Hungary","2011 Proceedings of the 34th International Convention MIPRO","","2011","","","291","294","The main aim of this work is to show, how the GPGPUs can be used to speed up certain image processing methods. The algorithm explained in this paper is used to detect nuclei on (HE - hematoxilin eosin) stained colon tissue sample images, and includes a Gauss blurring, an RGB-HSV color space conversion, a fixed binarization, an ultimate erode procedure and a local maximum search. Since the images retrieved from the digital slides require significant storage space (up to few hundred megapixels), the usage of GPGPUs to speed up image processing operations is necessary in the interest of achieving reasonable processing time. The CUDA software development kit was used to develop algorithms to GPUs made by NVIDIA. This work focuses on how to achieve coalesced global memory access when working with three-channel RGB images, and how to use the on-die shared memory efficiently. The exact test algorithm also included a linear connected component labeling, which was running on the CPU, and with iterative optimization of the GPU code, we managed to achieve significant speed up in well defined test environment.","","978-953-233-059-5978-1-4577-0996-8978-953-233-067","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5967067","","Graphics processing unit;Instruction sets;Image processing;Kernel;Optimization;Memory management;Pixel","biological tissues;biomedical optical imaging;coprocessors;image colour analysis;image recognition;iterative methods;medical image processing;optimisation","biomedical image processing;GPGPU;GPU based general purpose computation;image processing methods;nuclei detection;hematoxilin eosin stained colon tissue sample images;Gauss blurring;RGB-HSV color space conversion;fixed binarization;ultimate erode procedure;local maximum search;digital slides;CUDA software development kit;NVIDIA GPU;coalesced global memory access;three channel RGB images;iterative optimization;GPU code","","","6","","","","","","IEEE","IEEE Conferences"
"A methodology to use simulation at every stage of a hybrid vehicle design","V. Delafosse; S. Stanton; T. Sekisue; Junsik Yun","ANSYS, Inc., USA; ANSYS, Inc., USA; ANSYS, Japan; ANSYS, Korea","2012 IEEE Vehicle Power and Propulsion Conference","","2012","","","1134","1138","Accurate high level system simulation is difficult to obtain without first capturing the behavior of the individual components through physical testing and measurements. Early in the design phase the individual components have yet to be built and tested, thus compromising the accuracy of the simulation. In this early period of the design stage, the component models are created by relying solely on their constitutive relationships. Furthermore, it is difficult to incorporate physical effects such as spatial harmonic and nonlinearities into such constitutive based models. Once the design stage reaches the component level, there exists a number of physics simulators for every discipline involved: thermal, mechanical, electromagnetism and fluid dynamics; these simulators, based on numerical methods, provide high fidelity detailed information about the component. However, having an accurate multi-physics virtual model of a system at the 3D level and at the system 0D level remains a challenge. This paper will introduce a methodology to apply simulation at every step of the design cycle, from the early functional design stage to the detailed component optimization to the verification stage of the vehicle dynamics. It involves system simulation, component level multiphysics analysis using numerical methods, Reduce Order Models techniques and embedded software import. The example used to demonstrate this is the fuel consumption analysis of a hybrid electric vehicle.","1938-8756","978-1-4673-0954-7978-1-4673-0953-0978-1-4673-0952","10.1109/VPPC.2012.6422618","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6422618","","Friction;Inverters;Layout;Fluids;Optimization;Vehicles;Vehicle dynamics","electromagnetism;fluid dynamics;hybrid electric vehicles;optimisation","hybrid electric vehicle;physical effects;spatial harmonic;constitutive based models;physics simulators;thermal simulators;mechanical simulators;electromagnetism;fluid dynamics;multiphysics virtual model;3D level;detailed component optimization;verification stage;vehicle dynamics;component level multiphysics analysis;reduce order models;embedded software import;fuel consumption analysis","","","1","","","","","","IEEE","IEEE Conferences"
"Evaluating detection and treatment effectiveness of commercial anti-malware programs","J. A. Morales; R. Sandhu; Shouhuai Xu","Institute for Cyber Security, University of Texas at San Antonio, USA; Institute for Cyber Security, University of Texas at San Antonio, USA; Institute for Cyber Security, University of Texas at San Antonio, USA","2010 5th International Conference on Malicious and Unwanted Software","","2010","","","31","38","Commercial anti-malware programs consist of two main components: detection and treatment. Detection accuracy is often used to rank effectiveness of commercial anti-malware programs with less emphasis on the equally important treatment component. Effectiveness measures of commercial anti-malware programs should consider equally detection and treatment. This can be achieved by standardized measurements of both components. This paper presents a novel approach to evaluate the effectiveness of a commercial anti-malware program's detection and treatment components against malicious objects by partitioning true positives to incorporate detection and treatment. This new measurement is used to evaluate the effectiveness of four commercial anti-malware programs in three tests. The results show that several anti-malware programs produced numerous incorrectly treated or untreated true positives and false negatives leaving many infected objects unresolved and thereby active threats in the system. These results further demonstrate that our approach evaluates the detection and treatment components of commercial anti-malware programs in a more effective and realistic manner than currently accepted measurements which primarily focus on detection accuracy.","","978-1-4244-9356-2978-1-4244-9353-1978-1-4244-9355","10.1109/MALWARE.2010.5665797","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5665797","","Malware;Equations;Testing;Software;Mathematical model;Workstations","invasive software;software performance evaluation","commercial anti malware programs;detection evaluation;treatment evaluation","","3","21","","","","","","IEEE","IEEE Conferences"
"A LLVM Based Development Environment for Embedded Systems Software Targeting the RISCO Processor","G. Vilela; E. Correa; M. Kreutz","NA; NA; NA","2012 Brazilian Symposium on Computing System Engineering","","2012","","","77","82","In this paper we describe the design and implementation of a compilation and code analysis toolchain for embedded systems software targeting the RISCO processor, using the LLVM project. Small systems embedded in a larger device are by far the most common kind of computational system in use today, deployed in various types of equipments. Because of their nature, an embedded system presents interesting size, efficiency and energy consumption restrictions, among others, that impose unique challenges on a project. In that scenario, the RISCO processor, a RISC architecture similar to MIPS, was created as a simple, efficient, processor that could prove to be a practical alternative to the available commercial options in its price range. The toolchain we developed permit the development, simulation and analysis of software in C and C++ for the RISCO platform, with open source tools. Besides compiling and executing high level code, the environment supports emitting control flow graphs for each module, enabling further analysis. As a case study on using CFGs and generated machine code information we developed a worst case execution time analysis tool for RISCO code. We discuss the scope of the tools, the design decisions involved in the development of the compilation and analysis system, and the results obtained through testing.","2324-7894;2324-7886","978-0-7695-4929-3978-1-4673-5747","10.1109/SBESC.2012.23","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6473636","","Assembly;Optimization;Embedded systems;Registers;Reduced instruction set computing;Testing","embedded systems;power aware computing;program compilers;program diagnostics;public domain software;reduced instruction set computing","LLVM based development environment;embedded systems software;RISCO processor;compilation analysis toolchain;code analysis toolchain;energy consumption restrictions;RISC architecture;MIPS;C;C++;open source tools;CFG;machine code information;design decisions","","","26","","","","","","IEEE","IEEE Conferences"
"Practical Optimizations for Perceptron Algorithms in Large Malware Dataset","D. Gavrilut; R. Benchea; C. Vatamanu","NA; NA; NA","2012 14th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing","","2012","","","240","246","Due to the increasing number of malware samples in the past 4 years, machine learning algorithms emerged as an important tool in automated malware detection. This approach to create the detection model requires, however, a lot of time with a continually growing data-set. Often changes in malware families and the increasing training time makes the model less efficient and increases the probability of false alarms. This paper approaches this matter by reducing the time needed to create a detection model on very large databases and suggests three different optimization techniques. First, the perceptron algorithm was adjusted to use the map-reduce paradigm in order to make it run in a distribute manner. Second, hardware specific optimizations were applied for faster mathematical computations. Finally, a cache system was used to reduce the quantity of data processed by the algorithm. Even if these methods were designed and tested for malware databases they can easily be adjusted for other databases as well.","","978-1-4673-5026","10.1109/SYNASC.2012.33","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6481036","Perceptron;optimization;algorithm;database","Training;Optimization;Algorithm design and analysis;Malware;Machine learning algorithms;Databases;Adaptation models","cache storage;invasive software;learning (artificial intelligence);perceptrons;very large databases","perceptron algorithm;practical optimization;machine learning algorithm;automated malware detection;malware family;false alarm probability;map reduce paradigm;mathematical computation;cache system;data processing;malware database;large malware dataset;very large databases","","","15","","","","","","IEEE","IEEE Conferences"
"High performance smart expression template math libraries","K. Iglberger; G. Hager; J. Treibig; U. Rüde","Central Institute for Scientific Computing, University Erlangen-Nuremberg, Erlangen, Germany; Erlangen Regional Computing Center, University Erlangen-Nuremberg, Erlangen, Germany; Erlangen Regional Computing Center, University Erlangen-Nuremberg, Erlangen, Germany; Chair for System Simulation, University Erlangen-Nuremberg, Erlangen, Germany","2012 International Conference on High Performance Computing & Simulation (HPCS)","","2012","","","367","373","Performance is of utmost importance for linear algebra libraries since they usually are the core of numerical and simulation packages and use most of the available compute time and resources. However, especially in large scale simulation frameworks the readability and ease of use of mathematical expressions is essential for a continuous maintenance, modification, and extension of the software framework. Based on these requirements, in the last decade C++ Expression Templates have gained a reputation as a suitable means to combine an elegant, domain-specific, and intuitive user interface with “HPC-grade” performance. Unfortunately, many of the available ET-based frameworks fall short of the expectation to deliver high performance, adding to the general mistrust towards C++ math libraries. In this paper we present performance results for Smart Expression Template libraries, demonstrating that by proper combination of high-level C++ code and low-level compute kernels both requirements, an elegant interface and high performance, can be achieved.","","978-1-4673-2362-8978-1-4673-2359-8978-1-4673-2361","10.1109/HPCSim.2012.6266939","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6266939","","Libraries;Vectors;Kernel;Benchmark testing;Optimization;Standards;Syntactics","C++ language;libraries;linear algebra;mathematics computing;simulation;software maintenance;software packages;user interfaces","smart expression template;linear algebra libraries;numerical packages;simulation packages;large scale simulation;software maintenance;software modification;software framework;C++ expression templates;intuitive user interface;HPC-grade performance;C++ math libraries","","5","14","","","","","","IEEE","IEEE Conferences"
"FPGA-Optimised Uniform Random Number Generators Using LUTs and Shift Registers","D. B. Thomas; W. Luk","NA; NA","2010 International Conference on Field Programmable Logic and Applications","","2010","","","77","82","FPGA-optimised Random Number Generators (RNGs) are more resource efficient than software-optimised RNGs, as they can take advantage of bit-wise operations and FPGA-specific features. However, it is difficult to concisely describe FPGA-optimised RNGs, so they are not commonly used in real-world designs. This paper describes a new type of FPGA RNG called a LUT-SR RNG, which takes advantage of bit-wise XOR operations and the ability to turn LUTs into shift-registers of varying lengths. This provides a good resource-quality balance compared to previous FPGA-optimised generators, between the previous high-resource high-quality LUT-FIFO RNGs and low-resource low-quality LUT-OPT RNGs. The LUT-SR generators can also be expressed using a simple C++ algorithm contained within the paper, allowing 60 fully-specified LUT-SR RNGs with different characteristics to be embedded in the paper, backed up by an online set of VHDL generators and test-benches.","1946-1488;1946-147X;1946-1488","978-1-4244-7843-9978-1-4244-7842-2978-0-7695-4179","10.1109/FPL.2010.25","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5694224","FPGA;RNG;Random Number;Monte Carlo","Generators;Table lookup;Field programmable gate arrays;Loading;Logic gates;Software;Clocks","field programmable gate arrays;hardware description languages;Monte Carlo methods;optimisation;random number generation;shift registers","FPGA;uniform random number generator;shift register;software optimised RNG;bitwise operation;XOR operation;resource quality balance;LUT FIFO RNG;LUT OPT RNG;VHDL generator","","15","10","","","","","","IEEE","IEEE Conferences"
"Effect of fuel injector's parameters on HSDI diesel engine's performance","Z. Zhang; J. Deng; L. Li; F. Zhao","School of Automotive Studies, Tongji University, Shanghai, China; School of Automotive Studies, Tongji University, Shanghai, China; School of Automotive Studies, Tongji University, Shanghai, China; Automobile Engineering Institute, Zhejiang Geely Holding Group Co., LTD, Hangzhou, China","2011 International Conference on Electric Information and Control Engineering","","2011","","","2137","2142","Three schemes are adopted, which are combined with different spray angle of nozzles and spray penetration of the HSDI diesel. By means of FIRE software, the engine performances are predicated based on the key parameters, such as fuel-air equivalence ratio, flow velocity, NO<sub>x</sub> and soot concentration, and the combustion, NO<sub>x</sub> and soot's formation process are analyzed. If the fuel spray process could promote more air involving, the fuel spray would be atomized finer, then the combustion, power performance and fuel economy would be better, and the soot will be formed less but more NO<sub>x</sub>. Through comprehensive evaluation function, the compromise scheme is chosen which is compromised with better fuel economy, higher power performance, and low NO<sub>x</sub> and soot emissions. The best scheme which is validated by the 14 conditions test is the same with the simulation study's result. It's concluded that the simulation with software FIRE is very useful for the optimization of fuel injector's parameters.","","978-1-4244-8039-5978-1-4244-8036-4978-1-4244-8038","10.1109/ICEICE.2011.5776955","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5776955","high speed direct injection diesel;fuel injector;spray atomization;combustion and emissions;comprehensive evaluation function","Fuels;Combustion;Temperature distribution;Solid modeling;Fires;Software;Engines","combustion;computational fluid dynamics;diesel engines;fuel economy;fuel systems;impact (mechanical);nozzles;optimisation;soot;sprays","fuel injector;HSDI diesel engine performance;spray angle;nozzles;spray penetration;FIRE software;combustion;fuel spray process;power performance;fuel economy;soot emissions;optimization;high speed direction injection diesel engine","","","12","","","","","","IEEE","IEEE Conferences"
"Dual excited planar circular array antenna for direction agile applications","V. Ammula; S. M. Rao; S. M. Wentworth","Auburn University Dept. of ECE, Auburn, Alabama, USA; Naval Research Laboratory, Washington, DC, USA; Auburn University Dept. of ECE, Auburn, Alabama, USA","2010 42nd Southeastern Symposium on System Theory (SSST)","","2010","","","138","142","Smart antennas present a promising solution to the present day capacity and coverage shortage in mobile wireless communications. These intelligent antennas when used at the base station can avoid a lot of interference by transmitting and receiving signals only in desired directions. Circular array antennas have gained popularity among various antenna configurations used for direction agile applications. This paper presents a planar circular array antenna for base stations in mobile wireless communication systems. The antenna could be electronically steered to give a complete 360 coverage around the base station. The model was created using FEKO Suite 5.4, a Method of Moments (MoM) based electromagnetic simulation software. Particle Swarm Optimization algorithm was applied to maximize the gain of the antenna in a single azimuth direction. Ideal dimensions for the antenna structure were obtained from the optimization process. The designed antenna was fabricated and tested in an anechoic chamber to verify its radiation characteristics. The experimental results were found to be in good agreement with the simulation results.","2161-8135;0094-2898;0094-2898","978-1-4244-5691-8978-1-4244-5690-1978-1-4244-5692","10.1109/SSST.2010.5442853","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5442853","Smart antennas;beam steering;multiple excitations;antenna arrays","Antenna arrays;Directive antennas;Base stations;Mobile antennas;Wireless communication;Receiving antennas;Transmitting antennas;Interference;Application software;Electromagnetic modeling","adaptive antenna arrays;anechoic chambers (electromagnetic);electronic engineering computing;method of moments;mobile communication;particle swarm optimisation;planar antenna arrays","dual excited planar circular array antenna;direction agile application;smart antenna;mobile wireless communication;method of moments;electromagnetic simulation software;particle swarm optimization;anechoic chamber;radiation characteristic","","2","10","","","","","","IEEE","IEEE Conferences"
"Analysis of surge protection performance in low-voltage AC systems with capacitive load","S. Škuletią; V. Radulovią","Faculty of Electrical Engineering, University of Montenegro, Montenegro; Faculty of Electrical Engineering, University of Montenegro, Montenegro","45th International Universities Power Engineering Conference UPEC2010","","2010","","","1","6","The performances of one-stage and two-stage surge protection schemes for the case of capacitive load are analyzed and discussed in the paper. Simulation software ATP/EMTP and MATLAB have been used for simulation of surge testing of considered systems with different cable lengths between protection stages and equipment under test, as well as with different values of capacitive load, which can be found in real low-voltage AC power circuits. Combination Wave and Ring Wave, which IEEE Std. C62.41.2 defines as standard surge testing waveforms, have been used as representative stresses. Performed analysis should point on influences and characteristics of different loads and length of connecting cables, which provide proper surge protection. Obtained results and their analyses could be used for determination of patterns for protection zones, which are useful for techno-economical optimization of surge protection. In the cases when proper surge protection cannot be achieved, possible solutions are proposed, discussed and analyzed.","","978-0-9565570-2-5978-1-4244-7667-1978-0-9565570-0","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5648790","capacitive load;Combination Wave;low-voltage AC systems;Ring Wave;surge protection;surge protective devices;surge testing","Surges;Arresters;Load modeling;Integrated circuit modeling;Testing;Cable insulation","EMTP;load (electric);optimisation;power system economics;power system protection;surge protection","surge protection;low-voltage AC system;capacitive load;ATP-EMTP;MATLAB;surge testing simulation;low-voltage AC power circuit;combination wave;ring wave;surge testing waveform;techno-economical optimization;IEEE Std. C62.41.2","","","12","","","","","","IEEE","IEEE Conferences"
"Hardware-in-loop simulation of a common rail turbocharged diesel engine ECU","Enhua Wang; Hongguang Zhang; Boyuan Fan; Minggao Ouyang","College of Environmental and Energy Engineering, Beijing University of Technology, China; College of Environmental and Energy Engineering, Beijing University of Technology, China; College of Environmental and Energy Engineering, Beijing University of Technology, China; State Key Laboratory of Automotive Safety and Energy, Tsinghua University, Beijing, China","2011 International Conference on Electric Information and Control Engineering","","2011","","","5057","5060","To verify the hardware and software performance of a common rail turbocharged diesel engine ECU, a simulator was built up based on dSPACE hardware boards. The interface between the simulator and the ECU was developed. The real-time simulation model of the common rail turbocharged diesel engine was setup up in enDyna and successfully downloaded to the simulator. The monitor interface was designed in ControlDesk environment. The ECU hardware performance verification and failure mode test were conduct. The software integration verification was also carried out. The results indicate that the ECU could operate very well under various working conditions.","","978-1-4244-8039-5978-1-4244-8036-4978-1-4244-8038","10.1109/ICEICE.2011.5776806","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5776806","hardware-in-loop;common rail turbocharged diesel engine;simulator;enDyna","Fuels;Rails;Pulse width modulation;Diesel engines;Hardware;Optimized production technology","diesel engines;digital simulation;integrated software;mechanical engineering computing;performance evaluation;program verification","hardware-in-loop simulation;rail turbocharged diesel engine;dSPACE hardware boards;ECU hardware performance verification;failure mode test;software integration verification","","","","","","","","","IEEE","IEEE Conferences"
"A hardware in the loop design methodology for FPGA system and its application to complex functions","G. Liang; Danping He; J. Portilla; T. Riesgo","Centro de Electr&#x00F3;nica Industrial, Universidad Polit&#x00E9;cnica de Madrid, Spain; Centro de Electr&#x00F3;nica Industrial, Universidad Polit&#x00E9;cnica de Madrid, Spain; Centro de Electr&#x00F3;nica Industrial, Universidad Polit&#x00E9;cnica de Madrid, Spain; Centro de Electr&#x00F3;nica Industrial, Universidad Polit&#x00E9;cnica de Madrid, Spain","Proceedings of Technical Program of 2012 VLSI Design, Automation and Test","","2012","","","1","4","In this work, a unified algorithm-architecture-circuit co-design environment for complex FPGA system development is presented. The main objective is to find an efficient methodology for designing a configurable optimized FPGA system by using as few efforts as possible in verification stage, so as to speed up the development period. A proposed high performance FFT/iFFT processor for Multiband Orthogonal Frequency Division Multiplexing Ultra Wideband (MB-OFDM UWB) system design process is given as an example to demonstrate the proposed methodology. This efficient design methodology is tested and considered to be suitable for almost all types of complex FPGA system designs and verifications.","","978-1-4577-2081-9978-1-4577-2080-2978-1-4577-2079","10.1109/VLSI-DAT.2012.6212666","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6212666","","Field programmable gate arrays;Hardware design languages;Hardware;Algorithm design and analysis;Mathematical model;Application software;System analysis and design","fast Fourier transforms;field programmable gate arrays;formal verification;hardware-software codesign;logic design;multiplexing;OFDM modulation;optimisation;ultra wideband technology","hardware in the loop design methodology;complex functions;unified algorithm-architecture-circuit co-design environment;complex FPGA system development;configurable optimized FPGA system;verification stage;development period;iFFT processor;multiband orthogonal frequency division multiplexing ultra wideband system design process;MB-OFDM UWB system design process;complex FPGA system designs;complex FPGA system verifications","","3","12","","","","","","IEEE","IEEE Conferences"
"RoCoCo: Row and Column Compression for high-performance multiplication on FPGAs","F. Ugurdag; O. Keskin; C. Tunc; F. Temizkan; G. Fici; S. Dedeoglu","Dept. of Electrical &amp; Electronics Engineering, Ozyegin University, Istanbul, Turkey; Ericsson, Istanbul, Turkey; Dept. of Electrical &amp; Electronics Engineering, Ozyegin University, Istanbul, Turkey; Dept. of Electrical &amp; Electronics Engineering, Ozyegin University, Istanbul, Turkey; Ericsson, Istanbul, Turkey; Information Systems Dept. of the City Government of Basaksehir, Istanbul, Turkey","2011 9th East-West Design & Test Symposium (EWDTS)","","2011","","","98","101","Multiplication is, in no doubt, one of the top few frequently used operations in hardware and software. In high-performance hardware design, after high-level optimizations are exhausted, component level optimizations are employed such as building fast multipliers. Most fast multiplier architectures use some form of a Carry Save Adder (CSA) Tree, which is also called Column Compression (CC). We propose a new CC method called RoCoCo (Row and Column Compression), which also compresses the tree along rows so that the final adder is small and fast. Although CC results in faster multipliers in ASIC implementations, it is an assumption by designers that they are not the wisest choice on FPGAs. On the contrary, we were able to show through Xilinx synthesis results that RoCoCo (and sometimes Dadda CC) frequently offer faster multipliers than the built-in implementation of the multiply operation in Xilinx ISE synthesis tool.","","978-1-4577-1958-5978-1-4577-1957-8978-1-4577-1956","10.1109/EWDTS.2011.6116419","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6116419","","Field programmable gate arrays;Adders;Vegetation;Delay;Optimization;Routing","adders;application specific integrated circuits;field programmable gate arrays;integrated circuit design","row and column compression;FPGA;hardware design;high-level optimizations;component level optimizations;fast multiplier architectures;carry save adder;ASIC implementations;Dadda column compression;built-in implementation;Xilinx ISE synthesis tool","","3","10","","","","","","IEEE","IEEE Conferences"
"Design and development of power optimized satellite elephant collar with over the air programmability","B. I. Annasiwaththa; R. Munasinghe; P. Fernando; P. Leimgruber","Department of Electronic and Telecommunication Engineering, University of Moratuwa, Sri Lanka; Department of Electronic and Telecommunication Engineering, University of Moratuwa, Sri Lanka; Center for Conservation And Research, USA; Conservation Ecology Center, Smithsonian Conservation Biology Institute, U.S.A","2012 IEEE 6th International Conference on Information and Automation for Sustainability","","2012","","","13","18","A satellite elephant collar is a device mounted on an elephant's neck to track its position and movements. This information is essential for conservation and management of this highly endangered species. Currently available units are proprietary designs by few manufacturers and no literature can be found about their internal operation. This lack of published information hampers the evolution and development of future collaring systems with increased features and functionalities for research, conservation and management. Critical issues for improvement include live time of the collar, which is related to power consumption and operational modes of the unites, and remote programmability allowing for changes in data acquisition during deployment. Most commercially available collars lack the ability to change data acquisition schedules remotely over the air, and therefore data acquisition is un-optimized, which is directly responsible for un-optimized power consumption. This research was conducted to develop a satellite elephant collar with improved lifetime and remote programmability. A second purpose of our research was to make information on how to built elephant collars needs to become widely available to the user community,. This paper presents the particular issue of power optimization of satellite elephant collars. A functional test collar was designed using general purpose electronic components and a number of software and hardware optimization methods used during development which are presented in this paper. An expected life time over 580 days was achieved with eight D size Ultralife U10015 batteries for the prototype collar.","2151-1802;2151-1810","978-1-4673-1975-1978-1-4673-1976-8978-1-4673-1974","10.1109/ICIAFS.2012.6420036","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6420036","","Global Positioning System;Satellites;Batteries;Transceivers;Hardware;Satellite broadcasting;Base stations","artificial satellites;data acquisition;mobile radio;navigation;scheduling","power optimized satellite elephant collar development;power optimized satellite elephant collar design;elephant position tracking;elephant movement tracking;endangered species conservation;endangered species management;proprietary designs;internal operation;collar live time;operational modes;remote programmability;unoptimized data acquisition remotely scheduling;unoptimized power consumption;user community;functional test collar designed;general purpose electronic components;software optimization methods;hardware optimization methods;D-size ultralife U10015 batteries;over-the-air programmability","","1","9","","","","","","IEEE","IEEE Conferences"
"Optimization of Automatic Conversion of Serial C to Parallel OpenMP","D. Dheeraj; B. Nitish; S. Ramesh","NA; NA; NA","2012 International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery","","2012","","","309","314","This paper implements a technique that enhances parallel execution of auto-generated OpenMP programs by considering architecture of on chip cache memory. It avoids false-sharing in 'for-loops' by generating OpenMP code for dynamically scheduling chunks by placing each core's data cache line size apart. An open-source parallelization tool called Par4All has been analyzed and its power has been unleashed to achieve maximum hardware utilization. Some of the computationally intensive programs from Poly Bench have been tested on different architectures, with different data sets and the results obtained reveal that the OpenMP codes generated by the enhanced technique have resulted in considerable speedup.","","978-1-4673-2624-7978-0-7695-4810","10.1109/CyberC.2012.59","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6384986","on-chip cache;cache line size;false sharing;PolyBench;Par4All;PIPS;PoCC","Computers;Dynamic scheduling;Memory management;Cache memory;Optimization;Algorithm design and analysis","C language;cache storage;message passing;parallel processing;public domain software;scheduling","serial C program;parallel OpenMP program;automatic program conversion;on chip cache memory architecture;parallel execution;OpenMP code;message passing;chunk scheduling;data cache line size;Par4All tool;open-source parallelization tool;hardware utilization;Poly Bench","","","9","","","","","","IEEE","IEEE Conferences"
"Optimization of complex for high-loss substance permittivity measurements","P. S. Krasov; E. A. Arkhipova; A. I. Fisun","A. Ya. Usikov Institute of Radio Physics and Electronics, National Academy of Sciences of Ukraine, 12 Academician Proskura Str., Kharkiv 61085, Ukraine; A. Ya. Usikov Institute of Radio Physics and Electronics, National Academy of Sciences of Ukraine, 12 Academician Proskura Str., Kharkiv 61085, Ukraine; A. Ya. Usikov Institute of Radio Physics and Electronics, National Academy of Sciences of Ukraine, 12 Academician Proskura Str., Kharkiv 61085, Ukraine","2010 INTERNATIONAL KHARKOV SYMPOSIUM ON PHYSICS AND ENGINEERING OF MICROWAVES, MILLIMETER AND SUBMILLIMETER WAVES","","2010","","","1","2","Test experiments carried out in clinical terms on measuring of dielectric data of human blood and evaluation of erythrocyte adrenergic activity showed that measuring complex on the basis of the developed before reflectometer possesses some failures to complete. A cuvette for biological objects loses stability of measuring at the intensive continuous duty (6-8 hours), and also at measuring of time dependence of complex permittivity (by series for 6 minutes). It was also shown that some features of material of patients have an influence on measurements, i.e. device sensitivity was not enough for statistically reliable differentiation of blood cells reaction on bioactive substances effect. Thus, it was required to rework measuring cuvette. For this purpose the series of calculation experiments were conducted to optimize the waveguide flanges structure of cuvette as well as to fit optimal gaskets thicknesses limiting the sample. It should be take into account sensitivity and feasible implementation of gaskets thicknesses. To simulate the electromagnetic processes in cuvette we used a software package developed in the department of mathematical physics of Institute of Radio Physics and Electronics of NAS of Ukraine. This software product allow calculating and studying the electromagnetic waves propagation in complex structures consisting, inter alia, of the dielectric of arbitrary shape with the known parameters of permittivity and conductivity. The results of calculating experiments represented at present work have been carried out by the finite difference time domain method (FDTD). Two-dimensional initial-boundary value problems are examined for the Epolarized field. Time-domain data are transferred into the frequency domain ones by means of the Fourier transform. A cuvette in a model experiment was the two-dimensional layer structure consisted of the test sample with limiting gaskets. It is located in a short-circuited from one end waveguide and fully filling its section.","","978-1-4244-7899-6978-1-4244-7900-9978-1-4244-7898","10.1109/MSMW.2010.5546088","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5546088","","Permittivity measurement;Gaskets;Ethanol;Permittivity;Reflection;Limiting","bioelectric phenomena;biomedical equipment;biomedical measurement;blood;cellular biophysics;finite difference time-domain analysis;Fourier transforms;gaskets;initial value problems;permittivity;permittivity measurement","complex optimization;high-loss substance permittivity measurements;dielectric data;human blood;erythrocyte adrenergic activity;reflectometer;cuvette;blood cells reaction;bioactive substances effect;calculation experiments;waveguide flanges structure;gasket thicknesses;software package;software product;electromagnetic wave propagation;finite difference time domain method;FDTD;two-dimensional initial-boundary value problems;time-domain data;Fourier transform","","","6","","","","","","IEEE","IEEE Conferences"
"Research on Propulsion Motion Model System Identification Method of Trimaran","C. Peng; Y. Songlin; W. Wei; Y. Jingping; C. Jian","NA; NA; NA; NA; NA","2010 International Conference on Computational and Information Sciences","","2010","","","669","672","In this paper, we selected a kind of trimaran model to study its rapidity by theoretical research, ship model test and numerical simulation. Based on the rapidity test and ISIGHT-FD software platform, we set up mathematical model, selected the genetic algorithm method as system identification method. According to ship resistance and ship propelling, we analyzed the effect to its rapidity performance, hull, host, propeller match situation by lateral hulls arrangement. Finally, we made the residual resistance curve and open water efficiency curve, verified identify results and gave a prediction.","","978-1-4244-8814-8978-0-7695-4270","10.1109/ICCIS.2010.167","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5709174","system identification;trimaran;rapidity;ISIGHTFD;genetic algorithms","Resistance;Marine vehicles;Mathematical model;Propellers;Optimization;Navigation","genetic algorithms;mechanical engineering computing;mechanical testing;propulsion;ships","propulsion motion model system;system identification method;trimaran ship model;rapidity test;ISIGHT-FD software platform;genetic algorithm method;ship resistance;ship propelling;lateral hulls arrangement;residual resistance curve;open water efficiency curve","","","7","","","","","","IEEE","IEEE Conferences"
"Improving speculative loop parallelization via selective squash and speculation reuse","S. S. Ananthramu; D. Majeti; S. K. Aggarwal; M. Chaudhuri","Indian Institute of Technology, Kanpur, India; Dept. of Comp. Sci., Rice University, USA; Indian Institute of Technology, Kanpur, India; Indian Institute of Technology, Kanpur, India","2010 19th International Conference on Parallel Architectures and Compilation Techniques (PACT)","","2010","","","543","544","Speculative parallelization is a powerful technique to parallelize loops with irregular data dependencies. In this poster, we present a value-based selective squash protocol and an optimistic speculation reuse technique that leverages an extended notion of silent stores. These optimizations focus on reducing the number of squashes due to dependency violations. Our proposed optimizations, when applied to loops selected from standard benchmark suites, demonstrate an average (geometric mean) 2.5x performance improvement. This improvement is attributed to a 94% success in speculation reuse and a 77% reduction in the number of squashed threads compared to an implementation that, in such cases of squashes, would have squashed all the successors starting from the oldest offending one.","","978-1-4503-0178-7978-1-5090-5032","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7851554","Thread-level speculation;Mis-speculation overhead","Protocols;Optimization;Standards;Benchmark testing;Program processors;Programming","parallel programming;software reusability","speculative loop parallelization;selective squash;speculation reuse;value-based selective squash protocol;optimistic speculation reuse technique;dependency violations","","","2","","","","","","IEEE","IEEE Conferences"
"High efficiency and compact X band feed for Telemetry Applications","H. Diez","Centre National d'Etudes Spatiales, 18, Avenue Edouard Belin, 31401 Toulouse Cedex 9 France","2012 15 International Symposium on Antenna Technology and Applied Electromagnetics","","2012","","","1","5","This paper presents the design of a compact and high efficiency feed working in dual circular polarization, in X Band (8.0 GHz - 8.45 GHz). This feed was designed for working with a pointing mechanism typically for transmitting to a ground station, data coming from earth observation or science satellite missions. All the design steps are described, and main results are presented (including performances measured on an electrical model).","","978-1-4673-0292-0978-1-4673-0290-6978-1-4673-0291","10.1109/ANTEM.2012.6262344","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6262344","Antennas payload telemetry data subsystem;septum polarizer;scrimp horn;multimodal horn;Particle Swarm Optimization;multimodal analysis;compact test range","Feeds;Horn antennas;Antenna measurements;Satellite antennas;Software;Antenna arrays;Optimization","antenna feeds;microwave antennas;polarisation;satellite antennas;telemetry;transmitting antennas","telemetry applications;high efficiency compact X band feed;dual circular polarization;pointing mechanism;ground station transmission;science satellite missions;antenna feed;bandwidth 8 GHz to 8.45 GHz","","","8","","","","","","IEEE","IEEE Conferences"
"Modeling and simulation of the tracking mechanism for a PV string","N. Tatu; C. Alexandru","Department of Product Design, Mechatronics and Environment, Transilvania University of Brasov, Romania; Department of Product Design, Mechatronics and Environment, Transilvania University of Brasov, Romania","Proceedings of 2012 IEEE International Conference on Automation, Quality and Testing, Robotics","","2012","","","428","433","This paper presents the modeling and simulation of a solar tracker. The PV system is a string of PV modules which simultaneously change their daily position, being actuated by a rotative actuator. The system is designed using a virtual prototyping platform: CAD software (SolidWorks) - for the solid model, MBS software (Adams/View) - for the dynamic model, and DFC software (Matlab/Simulink) & Adams/Controls - for the control system model. The control system comprises two controllers, for position and velocity, whose factors are determined through an parametric optimization process. The simulation was performed in mechatronic concept. The system was optimized so that it has a maximum gain of solar energy, but keeping at minimum the energy consumed during tracking.","","978-1-4673-0704-8978-1-4673-0701-7978-1-4673-0703","10.1109/AQTR.2012.6237748","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6237748","photovoltaic;string;mechatronic system;virtual prototyping","Tracking;Solid modeling;Optimization;Mathematical model;Control systems;Joints;MATLAB","CAD;control engineering computing;mechatronics;photovoltaic power systems;position control;power engineering computing;power generation control;solar power stations;solid modelling;velocity control;virtual prototyping","tracking mechanism;PV module string;solar tracker modeling;solar tracker simulation;PV system;rotative actuator;virtual prototyping platform;CAD software;SolidWorks;solid model;MBS software;Adams-View;DFC software;Matlab-Simulink;Adams-Controls;control system model;dynamic model;position control;velocity control;parametric optimization process;mechatronic concept;solar energy","","","21","","","","","","IEEE","IEEE Conferences"
"On the Use of Term Rewriting for Performance Optimization of Legacy HPC Applications","A. Panyala; D. Chavarría-Miranda; S. Krishnamoorthy","NA; NA; NA","2012 41st International Conference on Parallel Processing","","2012","","","399","409","Preparing codes for next generation supercomputer systems is anticipated to require significant changes to the optimization strategies employed in established HPC applications. In this paper, we present our experience in applying term rewriting transformations for the optimization of such applications. We have designed application-specific term rewriting transformations to improve the scalability, enhance locality and reduce the communication overhead of the Self-Consistent Field computational chemistry benchmark. We present the rationale for the use of term rewriting in this manner, the design of our transformations, and the much enhanced performance of the resulting code.","0190-3918;0190-3918;2332-5690","978-1-4673-2508-0978-0-7695-4796","10.1109/ICPP.2012.44","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6337601","term rewriting;Global Arrays;TASCEL;load balancing","Tiles;Syntactics;Abstracts;Heuristic algorithms;Benchmark testing;Kernel;Semantics","optimisation;parallel machines;rewriting systems;software maintenance","performance optimization;legacy HPC application;next generation supercomputer system;application specific term rewriting transformation;scalability;self-consistent field computational chemistry benchmark","","4","35","","","","","","IEEE","IEEE Conferences"
"Soft-Sensing Modeling Method of Vinyl Acetate Polymerization Rate Based on BP Neural Network","J. Huang; H. Tao; Z. Zhu","NA; NA; NA","2010 International Conference on Measuring Technology and Mechatronics Automation","","2010","3","","410","413","Providing a soft-sensing modeling method of vinyl acetate (VAC) polymerization rate based on BP neural network. Solving the current problem that the VAC polymerization rate in the polyvinyl alcohol (PVA) producing process is hard to real-time measuring. Using the data samples collected from the scene to train the network. In the network learning process, using the Levenberg-Marquardt optimization algorithm. Finally, testing the network which has completed training. Test result shows that soft-sensing model of VAC polymerization rate based on BP neural network is accurate and effective.","2157-1473;2157-1481","978-1-4244-5739-7978-1-4244-5001","10.1109/ICMTMA.2010.326","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5458831","Soft-sensing;VAC polymerization rate;BP network;Modeling","Polymers;Neural networks;Production;Methanol;Multi-layer neural network;Software measurement;Temperature;Neurons;Layout;Testing","backpropagation;neural nets;optimisation;polymerisation;real-time systems;resins","soft sensing modeling method;vinyl acetate polymerization rate;BP neural network;VAC;PVA;polyvinyl alcohol;real-time measurement;network learning process;Levenberg-Marquardt optimization algorithm","","","7","","","","","","IEEE","IEEE Conferences"
"A Novel Chaining Approach for Direct Control Transfer Instructions","W. Xu; W. Chen; Q. Dou","NA; NA; NA","2010 IEEE 16th International Conference on Parallel and Distributed Systems","","2010","","","664","669","Software-based code cache systems are the key element in the dynamic translation system or optimization system to store the translated or optimized code for reuse. Translated code is organized in terms of code blocks in the code cache which transfers execution to the next code block through a control transfer instruction. As the target address of the control transfer instruction is in the form of its source program counter, the code cache system has to check the address mapping table for the translated program counter of the target address before entering the required code block. This will cause the performance degradation. As the target address of the direct control transfer instruction is fixed during the execution of a program, its source target address can be replaced with the translated target address. A direct control transfer chaining approach which occupies specific software assists is proposed in this paper. Evaluation of DCTC is conducted on a code cache simulator. The experiment results show the dramatic performance improvement brought by DCTC.","1521-9097;1521-9097","978-1-4244-9727-0978-0-7695-4307","10.1109/ICPADS.2010.114","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5695663","code cache;direct control transfer;chain;performance;simulator","Optimization;Benchmark testing;Software;Table lookup;Degradation;Radiation detectors","cache storage;codes;optimisation","direct control transfer instructions;software based code cache systems;dynamic translation system;optimization system;direct control transfer chaining approach","","","22","","","","","","IEEE","IEEE Conferences"
"Hybrid Scheduling Technology for Emergency Production under Job-Shop Environment","L. Jingsheng; W. Aimin; T. Chengtong","NA; NA; NA","2010 International Conference on Computing, Control and Industrial Engineering","","2010","1","","350","354","The manufactory must change the production style into flow-style for emergency production to meet the need of modern manufacture formal. This situation has made a contradiction between flow and discrete style. Thus hybrid scheduling technology is proposed to integrate these two styles under the job-shop environment. There are three crucial principles including in this technology: the logical production cell, dynamic priority and delaying strategy. They can be used to optimize the production resource and in the progress of scheduling. At last, the scheduling software is developed supporting by hybrid scheduling technology. The scheduling created by this software is tested by a real instance and is proved to improve the efficiency of production and shorten the make span than before.","","978-0-7695-4026","10.1109/CCIE.2010.95","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5492101","emergency production;delaying strategy;dynamic priority;hybrid scheduling technology;logical production cell","Job production systems;Job shop scheduling;Processor scheduling;Manufacturing;Optimized production technology;Mechanical engineering;Delay;Scheduling algorithm;Industrial engineering;Software testing","cellular manufacturing;job shop scheduling","hybrid scheduling technology;emergency production;job-shop environment;logical production cell;dynamic priority;delaying strategy;production resource;scheduling software","","","5","","","","","","IEEE","IEEE Conferences"
"Optimized control of an electric vehicle with functional actuator redundancy","P. Bergmiller; F. Schuldt; M. Maurer","Technische Universit&#x00E4;t Braunschweig, 38106, Germany; Technische Universit&#x00E4;t Braunschweig, 38106, Germany; Technische Universit&#x00E4;t Braunschweig, 38106, Germany","2012 IEEE International Conference on Vehicular Electronics and Safety (ICVES 2012)","","2012","","","25","30","This paper proposes an approach for optimized control of vehicle actuators as far as wear of individual components is concerned. The system is intended to improve the performance of existing balancing and protection systems working on component level. Well-tested and commonly known optimization algorithms are integrated into the onboard software to handle mutual dependencies of wear of components by generating an optimal coordination strategy. A suitable system architecture facilitates safe operation of the system and reduction of computational load. The approach especially focuses on electric vehicles with functional actuator redundancy and several power-supply units. Proper operation of the algorithm is demonstrated based on a full electric experimental vehicle. Thereby, tire wear is balanced while equalizing usage of the traction batteries and temperature levels of the drive motors.","","978-1-4673-0993-6978-1-4673-0992-9978-1-4673-0991","10.1109/ICVES.2012.6294327","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6294327","","Vehicles;Tires;Optimization;Batteries;Wheels;Torque;Actuators","actuators;electric vehicles;optimisation;tyres;wear","electric vehicle control;functional actuator redundancy;vehicle actuator control;performance improvement;balancing system;protection system;optimization algorithm;onboard software;optimal coordination strategy;system architecture;computational load reduction;power-supply unit;electric experimental vehicle;tire wear;traction battery;drive motor","","","15","","","","","","IEEE","IEEE Conferences"
"SafeBTW: A Scalable Optimistic Yet Non-risky Synchronization Algorithm","Y. Zhang; G. Li","NA; NA","2012 ACM/IEEE/SCS 26th Workshop on Principles of Advanced and Distributed Simulation","","2012","","","75","77","A new optimistic synchronization algorithm for Parallel Discrete Event Simulation (PDES) called Safe BTW is proposed in this paper. This new algorithm eliminates risky event processing in the Time Warp processing stage of the original BTW algorithm and is founded on a concept called ""safe causal relation"". In our new algorithm, the length of any chained rollback operations is limited to at most one and cascading rollback effects found in the original BTW algorithm can be completely avoided. Performance tests of Time Warp, Breathing Time Bucket, Breathing Time Warp and the new Safe BTW algorithms using PHOLD model show that the new algorithm obtains better speedups as the computation scales up. Its effective event processing time as a percentage of the total event processing time is higher than that of Time Warp and Breathing Time Warp, proving that excessive rollback operations are successfully controlled. And the extra synchronization overhead it introduces is still much lower than that of Breathing Time Bucket itself and is within tolerable and stable level as the computation scales up.","1087-4097","978-1-4673-1797","10.1109/PADS.2012.39","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6305888","","Synchronization;Algorithm design and analysis;Analytical models;Computational modeling;Process control;Parallel processing;Conferences","software performance evaluation;synchronisation;time warp simulation","SafeBTW algorithm;nonrisky synchronisation algorithm;scalable optimistic synchronization algorithm;parallel discrete event simulation;PDES;risky event processing elimination;time warp processing stage;safe causal relation concept;chained rollback operations;breathing time bucket algorithm;breathing time warp algorithm;PHOLD model","","","5","","","","","","IEEE","IEEE Conferences"
"Change management and workflow processing using Dynamics AX objects","I. Orosz; T. Orosz","Óbuda University - Alba Regia University Center, Budai str. 45, H-8000 Székesfehérvár, Hungary, Applied Informatics Doctoral School; Óbuda University - Alba Regia University Center, Budai str. 45, H-8000 Székesfehérvár, Hungary, Applied Informatics Doctoral School","2012 IEEE 10th Jubilee International Symposium on Intelligent Systems and Informatics","","2012","","","249","254","M Microsoft Dynamics AX continuously extends functional and technical approaches. Former Axapta and Navision applications are being replaced with the modern and smart AX and NAV solutions, like the other Dynamics modules and components. A brand new MS Dynamics implementation follows standard methodologies of Sure Step. However, companies, where one can find former Microsoft ERP systems, like Axapta or Navision, face to several change management issues and problems. One can figure out that business requirements for Software change management depend on specific business functions, industry, technologies and so on.","1949-047X;1949-0488","978-1-4673-4750-1978-1-4673-4751-8978-1-4673-4749","10.1109/SISY.2012.6339523","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6339523","Microsoft Dynamics AX;change management;CRM;GP;NAV","Vehicle dynamics;Companies;Software;Testing;Optimization","enterprise resource planning;management of change;software management;workflow management software","software change management;workflow processing;Microsoft Dynamics AX objects;NAV solutions;sure step standard methodology;Microsoft ERP systems;Axapta;Navision;business requirements","","","13","","","","","","IEEE","IEEE Conferences"
"A Fast Algorithm for Finding Community Structure Based on Community Closeness","X. Jiang; G. Liu; Z. Lin","NA; NA; NA","2010 Third International Joint Conference on Computational Science and Optimization","","2010","1","","436","439","Recently, the characterization of community structures in complex networks has received a considerable amount of attentions. Effective identification of these communities or clusters is a general problem in the field of data mining. In this paper we present a fast hierarchical agglomerative algorithm based on community closeness (FHACC) algorithm, for detecting community structure which is very efficient and faster than many other competing algorithms. FHACC tends to agglomerate such communities that share the most common vertices into larger ones. Its running time on a sparse network with n vertices and m edges is O(mk̅ + mt), where k̅ denotes the mean vertex degree, and t is the iteration times of community agglomeration in FHACC algorithm. The algorithm was tested on several real-world networks and proved to be high efficient and effective in community finding.","","978-1-4244-6813-3978-1-4244-6812","10.1109/CSO.2010.114","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5533069","community structure;complex network;data mining;community closeness","Clustering algorithms;Computational complexity;Computer networks;Complex networks;Data mining;Software algorithms;Computer science;Electronic mail;Testing;Density measurement","computational complexity;data mining;social sciences","community structure;community closeness;complex networks;data mining;fast hierarchical agglomerative algorithm;community finding","","","17","","","","","","IEEE","IEEE Conferences"
"Performance analysis and optimization of digital PWM controllers for surface-mounted PMSM drives","M. A. Khan; M. N. Uddin","Medium Voltage Technical Support, Rockwell Automation, Cambridge, ON, Canada; Department of Electrical Engineering, Lakehead University, Thundar Bay, ON, Canada","2012 IEEE Industry Applications Society Annual Meeting","","2012","","","1","8","The paper presents performance analysis and optimization of digital pulse width modulation (PWM) techniques for surface-mounted permanent magnet synchronous motors (SPMSM). Three specific PWM techniques such as sinusoidal PWM (SPWM), space vector PWM (SVPWM), and hysteresis PWM (HPWM) are designed and implemented for three-phase voltage source inverter-fed SPMSM drive. Each of the PWM technique is designed and modeled for three-phase inverters. The PWM models are integrated and implemented with the SPMSM drive using the Simulink software. The software is used for simulation tests of the SPMSM drive. The performance of the SPMSM drive using various PWM techniques is investigated for the ramp change in speed and for the step change in load. The power factor (PF) and total harmonic distortion (THD) of the drive are estimated for low and rated speed conditions at no load and rated load conditions. Based on the PF and THD analyses, two optimization algorithms are presented in order to select a PWM for the SPMSM drive with different operating conditions.","0197-2618;0197-2618","978-1-4673-0332-3978-1-4673-0330-9978-1-4673-0331","10.1109/IAS.2012.6374023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6374023","Harmonic analysis;pulse width modulation;permanent magnet synchronous motor;power factor;total harmonic distortion;voltage source inverter","Vectors;Switches;Inverters;Voltage control;Space vector pulse width modulation","harmonic distortion;hysteresis motor drives;machine theory;machine vector control;optimisation;permanent magnet motors;PWM invertors","performance analysis;digital PWM controller;surface-mounted PMSM drive;digital pulse width modulation controller;permanent magnet synchronous motor;sinusoidal PWM design;SPWM design;space vector PWM design;SVPWM design;hysteresis PWM design;HPWM design;three-phase voltage source inverter-fed SPMSM drive;Simulink software;simulation testing;power factor;PF;total harmonic distortion;THD;speed estimation;no load condition;rated load condition;optimization algorithm","","3","12","","","","","","IEEE","IEEE Conferences"
"How many individuals to use in a QA task with fixed total effort?","M. V. Mäntylä; K. Petersen; D. Pfahl","Lund University Department of Computer Science 22100 Lund, Sweden; Blekinge Institute of Technology School of Computing 37140 Karlskrona, Sweden; Lund University Department of Computer Science 22100 Lund, Sweden","Proceedings of the 2012 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement","","2012","","","311","314","Increasing the number of persons working on quality assurance (QA) tasks, e.g., reviews and testing, increases the number of defects detected - but it also increases the total effort unless effort is controlled with fixed effort budgets. Our research investigates how QA tasks should be configured regarding two parameters, i.e., time and number of people. We define an optimization problem to answer this question. As a core element of the optimization problem we discuss and describe how defect detection probability should be modeled as a function of time. We apply the formulas used in the definition of the optimization problem to empirical defect data of an experiment previously conducted with university students. The results show that the optimal choice of the number of persons depends on the actual defect detection probabilities of the individual defects over time, but also on the size of the effort budget. Future work will focus on generalizing the optimization problem to a larger set of parameters, including not only task time and number of persons but also experience and knowledge of the personnel involved, and methods and tools applied when performing a QA task.","1949-3770;1938-6451;1949-3789","978-1-4503-1056-7978-1-4503-1056-7978-1-4503-1056","10.1145/2372251.2372307","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6475432","Effectiveness;Fixed effort Budget;Effort;Review;People","Inspection;Optimization;Sociology;Statistics;Software;Probability;Educational institutions","optimisation;probability;quality assurance","QA task;quality assurance task;effort budget;optimization problem;defect detection probability;time function","","","8","","","","","","IEEE","IEEE Conferences"
"Making defect-finding tools work for you","M. G. Nanda; M. Gupta; S. Sinha; S. Chandra; D. Schmidt; P. Balachandran","IBM Research - India; IBM Research - India; IBM Research - India; IBM Research - TJ Watson Research Center; IBM Tivoli; IBM Rational","2010 ACM/IEEE 32nd International Conference on Software Engineering","","2010","2","","99","108","Given the high costs of software testing and fixing bugs after release, early detection of bugs using static analysis can result in significant savings. However, despite their many benefits, recent availability of many such tools, and evidence of a positive return-on-investment, static-analysis tools are not used widely because of various usability and usefulness problems. The usability inhibitors include the lack of features, such as capabilities to merge reports from multiple tools and view warning deltas between two builds of a system. The usefulness problems are related primarily to the accuracy of the tools: identification of false positives (or, spurious bugs) and uninteresting bugs among the true positives. In this paper, we present the details of an online portal, developed at IBM Research, to address these problems and promote the adoption of static-analysis tools. We report our experience with the deployment of the portal within the IBM developer community. We also highlight the problems that we have learned are important to address, and present our approach toward solving some of those problems.","1558-1225;0270-5257","978-1-60558-719","10.1145/1810295.1810310","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062143","defect differencing;defect merging;defect prioritization;defect views;static analysis portal","Computer bugs;Portals;Usability;Java;Open source software;Servers;Null value","portals;program diagnostics","defect-finding tools;software testing;bug fixing;static-analysis tools;usability problems;usefulness problems;online portal;IBM Research;IBM developer community","","10","14","","","","","","IEEE","IEEE Conferences"
"A Workflow Scheduling Algorithm for Optimizing Energy-Efficient Grid Resources Usage","F. Coutinho; L. A. V. d. Carvalho; R. Santana","NA; NA; NA","2011 IEEE Ninth International Conference on Dependable, Autonomic and Secure Computing","","2011","","","642","649","Grid computing represents the main solution to integrate distributed and heterogeneous resources in global scale. However, the infrastructure necessary for maintaining a global grid in production is huge. Such fact has led to excessive power consumption. On the other hand, most green strategies for data centers are DVS (Dynamic Voltage Scaling)-based and become difficult to implement them in global grids. This paper proposes the HGreen heuristic (Heavier Tasks on Maximum Green Resource) and defines a workflow scheduling algorithm in order to implement it on global grids. HGreen algorithm aims to prioritize energy-efficient resources and explores workflow application profiles. Simulation results have shown that the proposed algorithm can significantly reduce the power consumption in global grids.","","978-1-4673-0006-3978-0-7695-4612","10.1109/DASC.2011.115","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6119053","Grid computing;scheduling;scientific workflow;green computing","Green products;Energy efficiency;Air pollution;Benchmark testing;Algorithm design and analysis;Scheduling algorithm;Educational institutions","computer centres;grid computing;optimisation;power aware computing;scheduling;workflow management software","workflow scheduling algorithm;energy-efficient grid resource usage optimization;heterogeneous resource;global grid computing;power consumption;data center;dynamic voltage scaling;HGreen heuristic;HGreen algorithm","","7","40","","","","","","IEEE","IEEE Conferences"
"ERSA: Error Resilient System Architecture for probabilistic applications","L. Leem; H. Cho; J. Bau; Q. A. Jacobson; S. Mitra","Department of Electrical Engineering, Stanford University, CA, USA; Department of Electrical Engineering, Stanford University, CA, USA; Department of Electrical Engineering, Stanford University, CA, USA; Nokia Research Center, Palo Alto, CA, USA; Department of Electrical Engineering, Stanford University, CA, USA","2010 Design, Automation & Test in Europe Conference & Exhibition (DATE 2010)","","2010","","","1560","1565","There is a growing concern about the increasing vulnerability of future computing systems to errors in the underlying hardware. Traditional redundancy techniques are expensive for designing energy-efficient systems that are resilient to high error rates. We present Error Resilient System Architecture (ERSA), a low-cost robust system architecture for emerging killer probabilistic applications such as Recognition, Mining and Synthesis (RMS) applications. While resilience of such applications to errors in low-order bits of data is well-known, execution of such applications on error-prone hardware significantly degrades output quality (due to high-order bit errors and crashes). ERSA achieves high error resilience to high-order bit errors and control errors (in addition to low-order bit errors) using a judicious combination of 3 key ideas: (1) asymmetric reliability in many-core architectures, (2) error-resilient algorithms at the core of probabilistic applications, and (3) intelligent software optimizations. Error injection experiments on a multi-core ERSA hardware prototype demonstrate that, even at very high error rates of 20,000 errors/second/core or 2×10<sup>-4</sup> error/cycle/core (with errors injected in architecturally-visible registers), ERSA maintains 90% or better accuracy of output results, together with minimal impact on execution time, for probabilistic applications such as K-Means clustering, LDPC decoding and Bayesian networks. Moreover, we demonstrate the effectiveness of ERSA in tolerating high rates of static memory errors that are characteristic of emerging challenges such as Vccmin problems and erratic bit errors. Using the concept of configurable reliability, ERSA platforms may also be adapted for general-purpose applications that are less resilient to errors (but at higher costs).","1558-1101;1530-1591;1530-1591","978-3-9810801-6-2978-1-4244-7054-9978-3-9810801-6","10.1109/DATE.2010.5457059","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5457059","","Error correction;Computer architecture;Hardware;Error analysis;Resilience;Redundancy;Energy efficiency;Robustness;Control system synthesis;Degradation","belief networks;data mining;hardware-software codesign;optimisation;pattern clustering;probability;reliability theory","ERSA;error resilient system architecture;probabilistic applications;redundancy techniques;RMS;recognition mining and synthesis;asymmetric reliability;intelligent software optimizations;K-means clustering;LDPC decoding;Bayesian networks","","79","28","","","","","","IEEE","IEEE Conferences"
"Tile-based GPU optimizations through ESL full system simulation","H. Huang; C. Huang; C. Chen","Information and Communications Research Labs, Industrial Technology Research Institute, Hsinchu, Taiwan; Dept. of Electrical Engineering and Inst. of Computer &amp; Communication Engineering, National Cheng Kung University, Tainan, Taiwan; Dept. of Electrical Engineering and Inst. of Computer &amp; Communication Engineering, National Cheng Kung University, Tainan, Taiwan","2012 IEEE International Symposium on Circuits and Systems","","2012","","","1327","1330","We present a tile-based GPU design which is modeled in a full system simulation platform. The full system simulation platform includes a functional Linux-based system on which the GPU is incorporated for design explorations. To accurately estimate the execution time of the application graphics software, an execution time synchronization mechanism for the virtual platform is developed. We extend the Ericsson Texture Compression (ETC) scheme in our GPU to support alpha compression. In this way, we are able to reduce the external memory accesses to about one sixth, and speed up the rasterization engine (RE) by 35%. We also optimize the hardware-and-software data flow through the full system design platform and obtain significant improvements.","2158-1525;0271-4302;0271-4302","978-1-4673-0219-7978-1-4673-0218-0978-1-4673-0217","10.1109/ISCAS.2012.6271485","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6271485","","Graphics processing unit;Tiles;Rendering (computer graphics);Hardware;Optimization;Benchmark testing;Throughput","","","","","11","","","","","","IEEE","IEEE Conferences"
"Next generation emergency management common operating picture software/systems (COPSS)","R. E. Balfour","V.C.O.R.E. Solutions LLC, Bethpage, NY, USA","2012 IEEE Long Island Systems, Applications and Technology Conference (LISAT)","","2012","","","1","6","In the state-of-the-art Command Center at the Morrelly Homeland Security Center in Bethpage NY, VCORE Solutions has integrated, demonstrated, tested, deployed, and is operating powerful Emergency Management Common Operating Picture Software/Systems (COPSS). This Regional COPSS demonstrates in an operational environment the next generation of emergency management situational awareness, command & control, and information sharing in a natural, easy to use and understand four-dimensional (4D) common operating picture. This Regional COPSS is based on patented fourDscape® software technology, developed over the past decade by Long Island, NY-based Balfour Technologies. This powerful fourDscape augmented virtual reality technology has been effectively applied to local and regional emergency management operations and can be deployed nationally to deliver comprehensive situational awareness in support of safety, security and emergency preparedness, prevention, mitigation, response and recovery operations at all levels. A fourDscape browser/server-based COPSS is designed as an open, multi-layered service/resource oriented networked architecture (SOA/ROA, i.e. “cloud”) capable of (1) integrating and managing a multitude of disparate data sources of all types (including live and static data feeds); (2) interoperability with numerous other vendors information systems, COPs, information sharing frameworks, notification and alerting systems, analytics, etc.; and (3) sharing and passing information and comprehensive, timely situational awareness between first responders at the incident site, incident commanders, and emergency managers and decision makers at local, regional and national emergency operations centers across the country. And consistent with the recent Presidential Policy Directive on National Preparedness (PPD-8), this fourDscape COPSS capabilities and framework represents currently operational technology that can achieve an integrated, layered, and all-of-Nation [capabilities-based] preparedness approach that optimizes the use of available resources. Next Generation COPSS such as fourDscape need to be open and scalable to facilitate global information sharing; deliver information in an easy-to-understand augmented virtual reality common operating picture; be easy-to-use walk-up technology with a full complement of embedded training/simulation capability; provide for effective information assurance; and be compliant and effective in executing national preparedness goals. All this can and will be achieved by next generation COPSS.","","978-1-4577-1343-9978-1-4577-1342","10.1109/LISAT.2012.6223101","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6223101","common operating picture;fourDscape;virtual reality;telepresence;situational awareness;emergency response","Virtual reality;Solid modeling;Next generation networking;Browsers;Feeds;Sensors;Analytical models","augmented reality;cognition;command and control systems;data integration;decision making;electronic data interchange;emergency services;open systems;optimisation;service-oriented architecture","next generation emergency management;common operating picture software-system;COPSS;operational environment;situational awareness;command and control system;4D common operating picture;fourDscape® software technology;NY-based Balfour technology;augmented virtual reality;safety;security;recovery operation;emergency preparedness;fourDscape browser-server;resource oriented architecture;service oriented architecture;open architecture;data source management;interoperability;vendor information systems;information passing;decision making;national emergency operation;local emergency operation;regional emergency operation;Presidential Policy Directive on National Preparedness;PPD-8;optimization;global information sharing;walk-up technology;embedded training;information assurance;SOA;ROA;national preparedness goal execution;multilayered service;data integration","","3","5","","","","","","IEEE","IEEE Conferences"
"Parallelizing more Loops with Compiler Guided Refactoring","P. Larsen; R. Ladelsky; J. Lidman; S. A. McKee; S. Karlsson; A. Zaks","NA; NA; NA; NA; NA; NA","2012 41st International Conference on Parallel Processing","","2012","","","410","419","The performance of many parallel applications relies not on instruction-level parallelism but on loop-level parallelism. Unfortunately, automatic parallelization of loops is a fragile process, many different obstacles affect or prevent it in practice. To address this predicament we developed an interactive compilation feedback system that guides programmers in iteratively modifying their application source code. This helps leverage the compiler's ability to generate loop-parallel code. We employ our system to modify two sequential benchmarks dealing with image processing and edge detection, resulting in scalable parallelized code that runs up to 8.3 times faster on an eight-core Intel Xeon 5570 system and up to 12.5 times faster on a quad-core IBM POWER6 system. Benchmark performance varies significantly between the systems. This suggests that semi-automatic parallelization should be combined with target-specific optimizations. Furthermore, comparing the first benchmark to manually-parallelized, hand-optimized pthreads and OpenMP versions, we find that code generated using our approach typically outperforms the pthreads code (within 93-339%). It also performs competitively against the OpenMP code (within 75-111%). The second benchmark outperforms manually-parallelized and optimized OpenMP code (within 109-242%).","0190-3918;0190-3918;2332-5690","978-1-4673-2508-0978-0-7695-4796","10.1109/ICPP.2012.48","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6337602","Automatic Loop Parallelization;Compiler Feedback;Refactoring","Benchmark testing;Arrays;Kernel;Optimization;Image edge detection;Production;Radiation detectors","benchmark testing;edge detection;parallel programming;parallelising compilers;program control structures;software maintenance;software performance evaluation","compiler guided refactoring;parallel application performance;instruction-level parallelism;loop-level parallelism;automatic loop parallelization;interactive compilation feedback system;application source code modification;loop-parallel code generation;sequential benchmarks;image processing;edge detection;scalable parallelized code;eight-core Intel Xeon 5570 system;quad-core IBM POWER6 system;benchmark performance;semiautomatic parallelization","","4","26","","","","","","IEEE","IEEE Conferences"
"Autonomic University Administration System","B. McIntyre; M. Alsmarah; S. Sasi","Department of Computer and Information Science, Gannon University, Erie, PA 16541, USA; Department of Computer and Information Science, Gannon University, Erie, PA 16541, USA; Department of Computer and Information Science, Gannon University, Erie, PA 16541, USA","2010 International Conference on Computer Information Systems and Industrial Management Applications (CISIM)","","2010","","","377","382","An Autonomic computing system is a solution to circumvent the overhead associated with the usual network setup, manual updates, and continual general maintenance. In this research an in-depth analysis of the benefits of a novel Autonomic University Administration System (AUAS) is presented. An Autonomic Manager (AM) is responsible for the autonomic activities of self-configuration, self-healing and diagnosis, and optimization. A Policy Guided Self-Configuration Module (PGSCM) handles any new software or hardware to the proposed autonomic network (AN). The Self-Healing &amp; Diagnosis Module (SHDM) handles much of the testing of the AUAS for the AM. The Optimization Module (OM) handles self-optimization activities of the AUAS for the AM. A theoretical analysis is provided for cost, quality of service, failure avoidance, adaptivity, and response time.","","978-1-4244-7818-7978-1-4244-7817-0978-1-4244-7816","10.1109/CISIM.2010.5643632","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5643632","Autonomic Computing;Autonomic Manager;Self-Configuration Module;Self-Healing and Diagnosis Module;Optimization Module;Autonomic University Administration System","Optimization;Software;Servers;Least squares approximation;Quality of service;Testing;Computer architecture","configuration management;educational administrative data processing;educational institutions;fault tolerant computing;local area networks","autonomic university administration system;autonomic manager;self configuration module;self healing module;optimization module;learning management system;educational software","","","15","","","","","","IEEE","IEEE Conferences"
"Delegation-Based I/O Mechanism for High Performance Computing Systems","A. Nisar; W. Liao; A. Choudhary","University of California Santa Cruz, Santa Cruz; Northwestern University, Evanston; Northwestern University, Evanston","IEEE Transactions on Parallel and Distributed Systems","","2012","23","2","271","279","Massively parallel applications often require periodic data checkpointing for program restart and post-run data analysis. Although high performance computing systems provide massive parallelism and computing power to fulfill the crucial requirements of the scientific applications, the I/O tasks of high-end applications do not scale. Strict data consistency semantics adopted from traditional file systems are inadequate for homogeneous parallel computing platforms. For high performance parallel applications independent I/O is critical, particularly if checkpointing data are dynamically created or irregularly partitioned. In particular, parallel programs generating a large number of unrelated I/O accesses on large-scale systems often face serious I/O serializations introduced by lock contention and conflicts at file system layer. As these applications may not be able to utilize the I/O optimizations requiring process synchronization, they pose a great challenge for parallel I/O architecture and software designs. We propose an I/O mechanism to bridge the gap between scientific applications and parallel storage systems. A static file domain partitioning method is developed to align the I/O requests and produce a client-server mapping that minimizes the file lock acquisition costs and eliminates the lock contention. Our performance evaluations of production application I/O kernels demonstrate scalable performance and achieve high I/O bandwidths.","1045-9219;1558-2183;2161-9883","","10.1109/TPDS.2011.166","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5871604","Parallel I/O;I/O delegation;MPI-IO;non collective I/O;collaborative caching;parallel file systems;file locking.","Servers;Kernel;Bandwidth;Optimization;Benchmark testing;Multicore processing;Performance evaluation","client-server systems;data analysis;data integrity;file organisation;information retrieval;input-output programs;optimisation;parallel architectures;parallel programming;performance evaluation;software architecture","delegation-based I-O mechanism;high performance computing system;periodic data checkpointing;post-run data analysis;high end application;strict data consistency semantics;homogeneous parallel computing platform;high performance parallel application;parallel program;unrelated I-O access;large scale system;I-O serialization;lock contention;file system layer;I-O optimization;process synchronization;parallel I-O architecture;software design;parallel storage system;static file domain partitioning method;I-O request;client-server mapping;file lock acquisition cost;lock contention;performance evaluation;production application I-O kernel;high I-O bandwidth","","10","19","","","","","","IEEE","IEEE Journals & Magazines"
"Design of Composite Image Filters Using Interactive Genetic Programming","J. Ma; H. Takagi","NA; NA","2012 Third International Conference on Innovations in Bio-Inspired Computing and Applications","","2012","","","274","279","We combine a method for designing composite image filters with interactive genetic programming (IGP). Human subjective tests are used to comparitively evaluate the IGP-based filter design method to a manual filter design method and the multi-stage filtering feature of a software photo-retouching program. The composite image filter has a tree structure, with its nodes consisting of multiple simple image filters, arithmetic operators, arithmetic functions, constant values, and the pixel value of the input image. Genetic programming (GP) optimizes the tree structure based on the visual inspection of the IGP users, i.e. filter designers. Ten filter designers design composite filters using three methods: an IGP-based design method, a manual-based design method, and using the photo-retouching features of a commercial software program to time-sequentially apply ready-made filters. The designers make filters that output images corresponding to the given design concepts - relaxed and violent - based on their visual inspection. Twenty subjects compare the obtained images in pairs and evaluate which image is closer to achieving the give design concept. Wilcoxon signed-rank test demonstrates that the IGP-base filter design method can produce filters that create images with impressions that are closer to the given design concept than the other two methods.","","978-1-4673-2838-8978-0-7695-4837","10.1109/IBICA.2012.45","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6337677","interactive genetic programming;image processing;image filter design","Design methodology;Manuals;Humans;Convergence;Genetic programming;Educational institutions;Software","filtering theory;genetic algorithms;image processing","composite image filter design;interactive genetic programming;IGP-based filter design method;manual filter design method;multistage filtering;software photo-retouching program;tree structure;arithmetic operators;arithmetic functions;visual inspection;commercial software program;Wilcoxon signed-rank test;human subjective tests","","1","20","","","","","","IEEE","IEEE Conferences"
"Power swing detection for correct distance relay operation using S-transform and neural networks","A. F. Abidin; A. Mohamed; H. Shareef","The Faculty of Electrical Engineering, UiTM, Shah Alam, 40450, Selangor, Malaysia; Department of Electrical, Electronic and System Engineering, UKM Bangi, 43600, Selangor, Malaysia; Department of Electrical, Electronic and System Engineering, UKM Bangi, 43600, Selangor, Malaysia","2011 5th International Power Engineering and Optimization Conference","","2011","","","279","284","This paper presents an advanced technique for detecting power swings for distance relay operation. It uses a derivative of voltage and signal processing technique called as S-transform for feature extraction. Then an artificial neural network (ANN) is deployed in detecting the unstable swing in power systems. This approach overcomes the traditional relay scheme drawback, by distinguishing a fault, stable swing and unstable swing. To illustrate the effectiveness of the proposed approach, simulations were carried out on the IEEE 39 bus test system using the PSS/E software. Test results show that the proposed approach can effectively differentiate the fault, stable swing and unstable swing with a good accuracy.","","978-1-4577-0354-6978-1-4577-0355-3978-1-4577-0353","10.1109/PEOCO.2011.5970431","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5970431","Distance relay;Artificial Neural Network (ANN);S-transform;fault;stable swing and unstable swing","Artificial neural networks;Neurons;Protective relaying;Power system stability;Generators;Feature extraction","feature extraction;neural nets;power system analysis computing;relay protection;transforms","power swing detection;distance relay operation;S-transform;feature extraction;artificial neural network;IEEE 39 bus test system;PSS/E software","","1","9","","","","","","IEEE","IEEE Conferences"
"An Integrated Development Environment to Support the Multi-core Embedded Systems Development","C. Chang; C. Lu; W. C. Chu; P. Hsiung; N. Hsueh; C. Koong; C. Yang","NA; NA; NA; NA; NA; NA; NA","2012 12th International Conference on Quality Software","","2012","","","258","264","At now, multi-core processors are becoming prevalent rapidly in personal computing and embedded systems. But, the programming and supporting environment for multi-core processor systems is still unripe and lacks efficient tools. In this paper, we present a VERTAF/Multi-Core integration development environment and show how software code can be automatically generated from SysML/UML models of multi-core embedded systems. VERTAF/Multi-Core integrated requirement modeling, parallel design pattern modeling, multiview integrated model, automatic code generate, parallel code optimized, and system testing into a framework. We illustrate how model-driven design based on SysML/UML can be seamlessly integrated with Intel's threading building blocks (TBB) and the Quantum Platform middleware. We use a digital video recording system to show the benefits of the framework and it made the multi-core embedded system programming model-driven, easy, efficient, and effortless.","2332-662X;1550-6002;1550-6002","978-1-4673-2857-9978-0-7695-4833","10.1109/QSIC.2012.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6319258","Multi-core processors;Embedded system;Model-driven","Unified modeling language;Embedded software;Testing;Computer architecture;Streaming media;Embedded systems","embedded systems;middleware;multiprocessing systems;program compilers;Unified Modeling Language;video recording","integrated development environment;multicore embedded systems development;multicore processors;personal computing;VERTAF;software code;SysML/UML models;parallel design pattern modeling;multiview integrated model;automatic code generate;parallel code optimized;system testing;model-driven design;threading building blocks;TBB;quantum platform middleware;digital video recording system","","","7","","","","","","IEEE","IEEE Conferences"
"Experimental research on optimizing the layout of electronic components in closed electronic equipment with heat pipe","S. Shimei; Z. Liqiu","Jilin Institute of Architecture and Civil Engineering, Chang Chun China; China Northeast Central Design &amp; Research Institute of Municipal Engineering, Chang Chun China","2011 International Conference on Mechatronic Science, Electric Engineering and Computer (MEC)","","2011","","","2326","2331","Heat pipe heat exchanger is a kind of efficient heat recovery. There is a great development on system research of electronic equipment, because of its high heat transfer efficiency, small temperature drop, simple structure, easy to control and no energy dissipation characteristics. In this paper, SINDA/FLUINT which is a professional thermal analysis software researched performance of the heat pipe heat exchanger, established test-bed to test the heat exchanger performance of heat pipe, compared with the simulation results and measured data, depth analysis the use of heat pipe on cooling problem of electronics industry to provide a theoretical basis for large-scale application of heat pipe technology.","","978-1-61284-722-1978-1-61284-719-1978-1-61284-721","10.1109/MEC.2011.6025959","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6025959","heat pipe heat exchanger;electronic;equipment;numerical simulation;structural optimization","Electronic components;Temperature measurement;Space heating;Temperature distribution;Heat transfer;Electronic equipment","cooling;electronics industry;heat exchangers;heat pipes;heat recovery;temperature","electronic component;closed electronic equipment;heat pipe heat exchanger;heat recovery;heat transfer;temperature drop;thermal analysis software;cooling problem;electronics industry","","","1","","","","","","IEEE","IEEE Conferences"
"A new rule ranking model for Associative Classification using a hybrid Artificial Intelligence technique","M. M. Najeeb; A. E. Sheikh; M. Nababteh","Faculty of Information Systems and Technology, The Arab Academy for Banking &amp; Financial Sciences, Amman, Jordan; Faculty of Information Systems and Technology, The Arab Academy for Banking &amp; Financial Sciences, Amman, Jordan; Faculty of Information Systems and Technology, The Arab Academy for Banking &amp; Financial Sciences, Amman, Jordan","2011 IEEE 3rd International Conference on Communication Software and Networks","","2011","","","231","235","Rule ranking is a crucial step in Associative Classification (AC), AC algorithms proposed many ranking methods which aim to improve the accuracy of the classifier. In this paper we propose a new model in rule ranking, namely Hybrid-RuleRank, which employs a hybrid Artificial Intelligence (AI) technique that combines Simulated Annealing (SA) with Genetic Algorithm (GA), the new model tested against 11 data sets from UCI Machine Learning Repository, and the experimental results show that our model enhances the accuracy of the classifier.","","978-1-61284-486-2978-1-61284-485-5978-1-61284-484","10.1109/ICCSN.2011.6013816","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6013816","Associative Classification;Artificial Intelligence;Simulated Annealing;Genetic Algorithm","Accuracy;Iris;Annealing;Prediction methods;Biological system modeling;Machine learning;Breast","data mining;genetic algorithms;learning (artificial intelligence);pattern classification;simulated annealing","rule ranking model;associative classification;hybrid artificial intelligence technique;Hybrid-RuleRank;AI;simulated annealing;SA;genetic algorithm;GA;UCI machine learning repository;data mining","","4","29","","","","","","IEEE","IEEE Conferences"
"V-22 aircraft flight data mining","M. Burger; C. Jaworowski; R. Meseroll","Naval Air Systems Command, Lakehurst, NJ, USA; Naval Air Systems Command, Lakehurst, NJ, USA; Naval Air Systems Command, Lakehurst, NJ, USA","2011 IEEE AUTOTESTCON","","2011","","","443","447","The Naval Air Systems Command (NAVAIR) produces and supports highly complex aircraft weapons systems which provide advanced capabilities required to defend U.S. freedoms. Supporting said complex systems such as the MV-22/CV-22 aircraft requires being able to troubleshoot and mitigate complex failure modes in dynamic operational environments. Since an aircraft is comprised of multiple systems designed by specialty sub-vendors and subsequently brought together by an aircraft integrator, diagnostics at the aircraft level are usually “good enough” but not capable of 100% fault isolation to a single component. Today's system components must be highly integrated and are required to communicate via high speed data-bus conduits which require precise synchronization between systems. Failure modes of aircraft are identified via design, analysis and test prior to fielding of the weapon system. However, not all failure modes are typically known at the time of system Initial Operational Capability, but rather are found in the field by maintainers/pilots and then subsequently mitigated with aircraft engineering changes or system replacements. Also, the requirement for increased capabilities can drive the need for new systems to be integrated into an aircraft system that may not have been considered in the initial design and support concept. There is a plethora of maintenance action detail collected by pilots, maintenance officers (MO) and engineers that can and should be used to identify failure mode trends that come to light during the operational phase of an aircraft. New troubleshooting techniques can be developed to address underlying failure modes to increase efficiency of future maintenance actions thus reducing the logistics trail required to support the aircraft. The elements available for analysis are maintenance results input by the MO/pilot, (including free form comments regarding problems and resulting actions), Built-In-Test (BIT) fault codes recorded during a flight, and off-aircraft test equipment (such as Consolidated Automated Support System CASS) historical test results. The Integrated Support Environment (ISE) is collecting the data required to perform analysis of underlying maintenance trends that can be identified using some specialized software data mining tools such as text mining of corrective action and maintainer comments data fields from maintenance results. The findings or knowledge extracted from text mining can be correlated back to fault codes recorded during flight and historical maintenance results to help mitigate issues with broken troubleshooting procedures causing headaches to the our Sailors and Marines in the field. By tagging key phrases from the maintainer's/pilot's remarks, knowledge can be gleaned into how the aircraft fails in vigorous environments. The premise of this research is to first choose an apparent high failure avionics system on the V-22 aircraft that is experiencing a high removal rate from the aircraft but subsequently found to be fully operational when tested on CASS. The results of this analysis should present potential root causes for “Cannot Duplicate” situations by recommending an augmentation of diagnostics at the aircraft level to avoid removing and replacing a system that has not failed even though it has reported bad via the aircraft diagnostics. This research will utilize the Net-Centric Diagnostics Framework (NCDF) to retrieve past Smart Test Program Set (TPS) results/BIT sequence strings as a variable for identifying trends in V-22 aircraft maintenance actions. The results of the research will be socialized with the V-22 avionics Fleet Support Team and the Comprehensive Automated Maintenance Environment Optimized (CAMEO) for validation of findings before any troubleshooting changes are recommended. If required, the Integrated Diagnostics and Automated Test Systems group will perform an engineering analysis of problem and suggest an enhanced diagnostic technique to mitigate the issue.","1558-4550;1088-7725;1088-7725","978-1-4244-9363-0978-1-4244-9362-3978-1-4244-9361","10.1109/AUTEST.2011.6058773","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6058773","V-22;data-mining;text-mining;maintenance actions;diagnostics","Maintenance engineering;Aircraft;Text mining;Aerospace electronics;Predictive models;Aircraft propulsion","aerospace computing;aircraft maintenance;built-in self test;data mining;military aircraft;military computing;weapons","Naval Air Systems Command;NAVAIR;highly complex aircraft weapon systems;MV-22-CV-22 aircraft flight data mining;complex failure mode mitigation;aircraft integrator;high speed data-bus conduits;initial operational capability;aircraft engineering;maintenance action plethora;maintenance officers;MO;troubleshooting techniques;logistic reduction;built-in-test fault codes;BIT fault codes;off-aircraft test equipment;integrated support environment;ISE;specialized software data mining tools;text mining;fault codes;apparent high failure avionics system;CASS;aircraft diagnostics;net-centric diagnostic framework;NCDF;smart test program set;TPS;V-22 aircraft maintenance actions;V-22 avionics fleet support team;comprehensive automated maintenance environment optimized;CAMEO;engineering analysis;enhanced diagnostic technique;Integrated Diagnostics and Automated Test System group","","","6","","","","","","IEEE","IEEE Conferences"
"Fast and accurate resource conflict simulation for performance analysis of multi-core systems","S. Stattelmann; O. Bringmann; W. Rosenstiel","FZI Forschungszentrum Informatik, Haid-und-Neu-Str. 10-14, D-76131 Karlsruhe, Germany; FZI Forschungszentrum Informatik, Haid-und-Neu-Str. 10-14, D-76131 Karlsruhe, Germany; University of Tuebingen, Sand 13, D-72076 Tuebingen, Germany","2011 Design, Automation & Test in Europe","","2011","","","1","6","This work presents a SystemC-based simulation approach for fast performance analysis of parallel software components, using source code annotated with low-level timing properties. In contrast to other source-level approaches for performance analysis, timing attributes obtained from binary code can be annotated even if compiler optimizations are used without requiring changes in the compiler. To consider concurrent accesses to shared resources like caches accurately during a source-level simulation, an extension of the SystemC TLM-2.0 standard for reducing the necessary synchronization overhead is proposed as well. This enables the simulation of low-level timing effects without performing a full-fledged instruction set simulation and at speeds close to pure native execution.","1558-1101;1530-1591;1530-1591","978-3-9810801-8-6978-1-61284-208-0978-3-9810801-7","10.1109/DATE.2011.5763044","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5763044","System analysis and design;Timing;Modeling;Software performance","Synchronization;Binary codes;Analytical models;Kernel;Predictive models","digital simulation;multiprocessing systems;optimisation;parallel programming;performance evaluation;program compilers","resource conflict simulation;performance analysis;multicore systems;SystemC based simulation;parallel software components;source code;low level timing properties;compiler optimizations;SystemC TLM 2.0 standard","","13","19","","","","","","IEEE","IEEE Conferences"
"Recognition of abnormal vibrational responses of signposts using the Two-dimensional Geometric Distance and Wilcoxon test","M. Jinnai; Y. Akashi; S. Nakaya; F. Ren; M. Fukumi","Kagawa National College, Takamatsu, Japan; Nexco Engineering Ltd., Takamatsu, Kagawa, Japan; Nexco Engineering Ltd., Takamatsu, Kagawa, Japan; University of Tokushima, Japan; University of Tokushima, Japan","Proceedings of the 6th International Conference on Natural Language Processing and Knowledge Engineering(NLPKE-2010)","","2010","","","1","8","In expressway companies, workers have been impacting signposts using wooden hammers and estimating the degree of the corrosion by listening to the sound. In order to automate this, we have been developing software that recognizes an abnormal impact vibrational response due to corrosion. This software extracts sonograms from impact vibrational waves using the LPC spectrum analysis, and matches images of the sonogram between a standard and an input impact vibrations using the Two-dimensional Geometric Distance. Then, the software distinguishes the abnormality of the input impact vibration using Wilcoxon rank-sum test. We have measured the impact vibrations of five normal signposts and five abnormal signposts, and carried out the automatic recognition experiments. As a result, the software has recognized correctly in all cases. We have verified the effectiveness of the proposed method.","","978-1-4244-6899-7978-1-4244-6896-6978-1-4244-6898","10.1109/NLPKE.2010.5587837","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5587837","Similarity measures;distance functions;pattern matching","Equations;Noise","acoustic signal processing;impact testing;mechanical engineering computing;pattern recognition;ultrasonics;vibrations","abnormal vibrational response recognition;signpost recognition;2D geometric distance;sonogram extraction;LPC spectrum analysis;image matching;Wilcoxon rank-sum test;corrosion degree;corrosion","","1","16","","","","","","IEEE","IEEE Conferences"
"Benefits of building wireless sensor networks on commodity hardware and software stacks","N. B. Bajema; J. Trevathan; N. W. Bergmann; I. Atkinson; W. Read; A. Scarr; Y. J. Lee; R. Johnstone","eResearch Centre, James Cook University Townsville, Australia; eResearch Centre, James Cook University Townsville, Australia; School of Information Technology and Electrical Engineering, University of Queensland, Brisbane, Australia; eResearch Centre, James Cook University Townsville, Australia; School of Engineering and Physical Sciences, James Cook University, Townsville, Australia; eResearch Centre, James Cook University Townsville, Australia; eResearch Centre, James Cook University Townsville, Australia; School of Geography, Planning and Environmental Management - University of Queensland, Brisbane, Australia","2011 Seventh International Conference on Intelligent Sensors, Sensor Networks and Information Processing","","2011","","","282","287","The majority of wireless sensor networks are built on bespoke platforms, that is, custom designed and built hardware with a light weight software stack. There are a number of advantages to this approach. First, the ability to closely match and minimise the resource requirements (e.g., power consumption and communications protocols) to those that are suitable for the intended deployment. Second, as an entire hardware and software stack is often designed or at least optimised for each deployment, the latest advances can be quickly incorporated. However, this model generally requires the expertise of hardware and software engineers to design and build the system. In turn, this increases the cost and tends to shift the focus away from the initial science towards the development of the wireless sensor networks. This paper explores the utility and practicality of building wireless sensor networks based on commercially available embedded single board computing platforms using standard consumer operating systems. Our test bed was built using Gumstix computing platform, running a Linux Operating System (OS) with a java-based middleware coupled to low-cost scientific grade sensors. Test deployments have found this to be a highly versatile solution, able to leverage the flexibility of commodity hardware and software while maintaining reasonable utility.","","978-1-4577-0674-5978-1-4577-0675-2978-1-4577-0673","10.1109/ISSNIP.2011.6146554","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6146554","WSN;Commodity Stack","Wireless sensor networks;Software;Hardware;Educational institutions;Batteries;Sensor systems","embedded systems;Java;Linux;middleware;software engineering;wireless sensor networks","wireless sensor networks;commodity hardware;bespoke platforms;lightweight software stack;resource requirements;embedded single board computing platforms;standard consumer operating systems;Gumstix computing platform;Linux operating system;Java-based middleware","","","13","","","","","","IEEE","IEEE Conferences"
"A Framework for User Feedback Based Cloud Service Monitoring","Z. ur Rehman; O. K. Hussain; S. Parvin; F. K. Hussain","NA; NA; NA; NA","2012 Sixth International Conference on Complex, Intelligent, and Software Intensive Systems","","2012","","","257","262","The increasing popularity of the cloud computing paradigm and the emerging concept of federated cloud computing have motivated research efforts towards intelligent cloud service selection aimed at developing techniques for enabling the cloud users to gain maximum benefit from cloud computing by selecting services which provide optimal performance at lowest possible cost. Given the intricate and heterogeneous nature of current clouds, the cloud service selection process is, in effect, a multi criteria optimization or decision-making problem. The possible criteria for this process are related to both functional and nonfunctional attributes of cloud services. In this context, the two major issues are: (1) choice of a criteria-set and (2) mechanisms for the assessment of cloud services against each criterion for thorough continuous cloud service monitoring. In this paper, we focus on the issue of cloud service monitoring wherein the existing monitoring and assessment mechanisms are entirely dependent on various benchmark tests which, however, are unable to accurately determine or reliably predict the performance of actual cloud applications under a real workload. We discuss the recent research aimed at achieving this objective and propose a novel user-feedback-based approach which can monitor cloud performance more reliably and accurately as compared with the existing mechanisms.","","978-1-4673-1233-2978-0-7695-4687","10.1109/CISIS.2012.157","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6245621","Cloud Computing;Service Selection;Cloud Services;User Feedback;Cloud Monitoring","Monitoring;Cloud computing;Benchmark testing;Quality of service;Reliability;Educational institutions","cloud computing;decision making;optimisation;software performance evaluation;system monitoring","user feedback;cloud service monitoring;federated cloud computing;intelligent cloud service selection;multicriteria optimization;decision-making problem;functional attribute;nonfunctional attribute;criteria-set;cloud service assessment;continuous cloud service monitoring;user-feedback-based approach;cloud performance monitoring","","18","14","","","","","","IEEE","IEEE Conferences"
"Novel low level local features for 3D expression invariant face recognition","M. Hayat; M. Bennamoun; Y. Lei; A. El-Sallam","School of Computer Science and Software Engineering, The University of Western Australia, 35 Stirling Highway, Crawley, WA 6009; School of Computer Science and Software Engineering, The University of Western Australia, 35 Stirling Highway, Crawley, WA 6009; School of Computer Science and Software Engineering, The University of Western Australia, 35 Stirling Highway, Crawley, WA 6009; School of Computer Science and Software Engineering, The University of Western Australia, 35 Stirling Highway, Crawley, WA 6009","2012 12th International Conference on Control Automation Robotics & Vision (ICARCV)","","2012","","","194","198","In this paper, we present a system based on novel low level local features to recognize 3D faces under varying facial expressions. Our local features are obtained by combinatorially selecting two points from expression insensitive semi-rigid portions of the face. The curve length between the two points is computed and the distribution of such curve lengths is used as a feature vector to model the geometric shape distribution of the face. Our proposed features are very simple to compute yet highly distinctive and discriminating. Kernel Fisher discriminant analysis is used for feature optimization, followed by a linear support vector machine classifier for recognition. The system is extensively tested on 2500 facial scans of BU 3DFE dataset. Our experimental results show that the proposed system achieves a very high average classification rate of 99.17% and verification rates of 99.0% and above for a false acceptance rate of 0.001.","","978-1-4673-1872-3978-1-4673-1871-6978-1-4673-1870","10.1109/ICARCV.2012.6485157","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6485157","","Face;Feature extraction;Face recognition;Nose;Histograms;Shape;Vectors","face recognition;feature extraction;optimisation;support vector machines","low level local feature;3D expression invariant face recognition;expression insensitive semirigid portion;curve length;feature vector;geometric shape distribution;kernel Fisher discriminant analysis;feature optimization;linear support vector machine classifier","","","17","","","","","","IEEE","IEEE Conferences"
"Optimizing test-generation to the execution platform","A. Nahir; A. Ziv; S. Panda","IBM Research, Haifa, Israel; IBM Research, Haifa, Israel; IBM Systems and Technology Group, Bangalore, India","17th Asia and South Pacific Design Automation Conference","","2012","","","304","309","The role of stimuli generators is to reach all the dark corners of the design and expose the bugs hiding there. As such, stimuli generation is one of the cornerstones of dynamic verification. The quality of tools used for stimuli generation affect the outcome of the verification process. This paper discusses how differences between execution platforms, ranging from software simulators, through accelerators and emulators, to silicon affect the requirements of stimuli generators and how stimuli generators targeting different execution platforms address these differences. We demonstrate how the unique added value of the platforms are combined to guarantee the high quality of the silicon using examples of several IBM pre- and post-silicon stimuli generators with results from the verification of the IBM POWER7 processor chip.","2153-697X;2153-6961;2153-6961","978-1-4673-0772-7978-1-4673-0770-3978-1-4673-0771","10.1109/ASPDAC.2012.6164964","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6164964","","Generators;Silicon;Instruction sets;Acceleration;Observability;Computer architecture","formal verification;multiprocessing systems;program debugging","test-generation optimization;execution platform;bugs hiding;stimuli generation;dynamic verification;software simulator;accelerator;emulator;IBM pre-silicon stimuli generator;IBM post-silicon stimuli generator;IBM POWER7 processor chip verification","","4","18","","","","","","IEEE","IEEE Conferences"
"Study on the Syllogism-based construction of industrial products brands","P. Shuai; S. Cong-min; F. Zhi-gang","School of Business Administration South China University of Technology, Guangzhou, China, 510641; School of Business Administration South China University of Technology, Guangzhou, China, 510641; Jiangxi Institute of Metrology and Testing, Nanchang, Jiangxi, China, 330002","2011 IEEE 3rd International Conference on Communication Software and Networks","","2011","","","711","713","This paper, through the analysis on the three stages of brand rising, introduces the related points that should be operated emphatically in each stage, hence carrying on the optimization and integration of resources, and guides the enterprises manufacturing industrial products to establish brands with gracious reputations.","","978-1-61284-486-2978-1-61284-485-5978-1-61284-484","10.1109/ICCSN.2011.6014824","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014824","industrial products;brand construction;strategic study","Government;Educational institutions","manufacturing industries;marketing","syllogism-based construction;industrial product brands;optimization","","","5","","","","","","IEEE","IEEE Conferences"
"Application of SVC and single-phase shunt capacitor to improve voltage profiles and reduce losses of unbalanced multiphase smart grid with PEV charging stations","P. Juanuwattanakul; M. A. S. Masoum; S. Hajforoosh","Department of Electrical Engineering, Sripatum University, Bangkok, Thailand; Department of Electrical and Computer Engineering, Curtin University, Perth, WA, Australia; Department of Electrical Engineering, Damavand Branch, Islamic Azad University, Tehran, Iran","2012 22nd Australasian Universities Power Engineering Conference (AUPEC)","","2012","","","1","6","In smart grids, the conventional approach of locating compensation devices based on the forecasted daily load curves is not realistic as the locations, times and durations of some loads such as plug-in electric vehicles (PEVs) and smart appliances are randomly changing during the 24 hour period. This paper proposes a new approach to improve the performance of unbalanced multiphase distribution systems consisting of single-, two- and three-phase networks with PEV charging stations. The approach is designated to perform online VRI ranking, place SVCs and single-phase capacitors at the weakest three-phase and single-phase buses, respectively; and then switch these devices in and out of the service according to the lowest voltage ranking index (VRI) values in order to improve voltage profiles and reduce total system losses. Simulation results are performed and compared for an unbalanced multiphase 13 node test feeder with PEV charging stations using DIgSILENT PowerFactory software.","","978-979-18847-2-3978-1-4673-2933","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6360202","smart grid;PEV;charging stations;multiphase;bus voltage ranking;weakest bus","Static VAr compensators;Capacitors;Smart grids;Switches;Performance evaluation;Simulation;Indexes","electric vehicles;power capacitors;smart power grids","SVC;single-phase shunt capacitor;voltage profiles;unbalanced multiphase smart grid;PEV charging stations;compensation devices;plug-in electric vehicles;unbalanced multiphase distribution systems;online VRI ranking;single-phase buses;three-phase buses;single-phase capacitors;voltage ranking index;VRI values;total system losses;unbalanced multiphase 13 node test feeder;DIgSILENT PowerFactory software","","","22","","","","","","IEEE","IEEE Conferences"
"Study on the optimization and analysis of the Mixed Reflow soldering with lead and lead-free solder in the high density assembly","Y. Huang; Z. Wu; Z. Liu","School of Mechanical & Electrical Engineering of Guilin University of Electronic Technology, China; School of Mechanical & Electrical Engineering of Guilin University of Electronic Technology, China; CETC No. 10 Research Institute, Chengdu, 61000, China","2012 13th International Conference on Electronic Packaging Technology & High Density Packaging","","2012","","","884","889","With the continuous improvement of the packing density, and the continuous developments of the assembly material and lead-free craft, Electronic industry switches to a transitional phase of the lead to lead-free-mixed lead and lead-free solder joints. Due to that the melting point of lead-free solder is 217°C, which is nearly 34°C higher than lead solder 183°C. If the solder curve is used to implement welding, the lead-free pins are not molten while the component is being sent into cooling area, which must lead to delamination between solders and solder pastes. And if the solder-free curve is used, crystal grains of the solder pasts will be thick due to the high temperature. That will make the welding poor. In order to avoid the defects above in the mixed assembly, a set of process parameters are needed to determine a correct reflow curve. To direct at a high density mixed assembly, a component equivalent model is established using Ansys software. The reflow process is simulated under ERSA reflow-oven with 12 zones. L9 (34) orthogonal tests are designed to analysis the four main process parameters, which include Conveyor belt speed A, Preheating temperature B, Heat preservation temperature C, Reflow temperature D, on the influence of the five key indexes including heating rate V<sub>up</sub>, cooling rate V<sub>down</sub>, temperature difference ΔT, peak temperature T<sub>peak</sub>, time t<sub>1</sub>above 217°C. According to the temperature distribution, process parameters are adjusted. Finally, the optimal process parameters are determined. The results show that when the conveyor belt speed is set to 690mm/min, preheating temperature 160°C, and heat preservation temperature 170°C, reflow temperature 245°C, the reflow curve can meet the five key indexes, which can ensure the welding quality of the mixed assembly.","","978-1-4673-1681-1978-1-4673-1682-8978-1-4673-1680","10.1109/ICEPT-HDP.2012.6474751","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6474751","High density;Mixed assembly;Lead and lead-free;Orthogonal text;Optimization and Analysis","Indexes;Plastics;Heating;Assembly;Substrates;Arrays","belts;conveyors;cooling;delamination;electronics industry;lead;melting point;optimisation;printed circuit manufacture;soldering;solders;temperature distribution;welding","mixed reflow soldering;lead-free solder;lead solder;high density assembly;packing density;assembly material;lead-free craft;electronic industry switches;transitional phase;lead-free-mixed lead solder joints;solder curve;lead-free pins;cooling area;solder-free curve;correct reflow curve;process parameters;high density mixed assembly;Ansys software;ERSA reflow-oven;L9 orthogonal tests;process parameters;conveyor belt speed;preheating temperature;heat preservation temperature;reflow temperature;heating rate;cooling rate;temperature difference;peak temperature;temperature distribution process parameters;optimal process parameters;welding quality","","","9","","","","","","IEEE","IEEE Conferences"
"Can we certify systems for freedom from malware","N. V. N. Kumar; H. Shah; R. K. Shyamasundar","Tata Institute of Fundamental Research, Mumbai, India; Tata Institute of Fundamental Research, Mumbai, India; Tata Institute of Fundamental Research, Mumbai, India","2010 ACM/IEEE 32nd International Conference on Software Engineering","","2010","2","","175","178","Malicious code is any code that has been modified with the intention of harming its usage or the user. Typical categories of malicious code include Trojan Horses, viruses, worms etc. With the growth in complexity of computing systems, detection of malicious code is becoming horrendously complex. For security of embedded devices it is important to ensure the integrity of software running in it. The general virus detection is undecidable. However, in the case of embedded systems or personal systems, the software and hardware configurations are known a priori. We are experimenting to see whether we can certify such systems for malware freedom. Most of the current efforts on malware detection rely heavily on detection of syntactic patterns. Malware writers are resorting to simple syntactic transformations (which preserve the program semantics) such as various compiler optimizations and program obfuscation techniques to evade detection. Our work is based on semantic behaviour of programs. We are working towards developing a model of the behaviour of a program executing in an environment. Our approach to detect tampering is based on benchmarking the behaviour of a program executing in an environment, and then matching the observed behaviour of the program in a similar environment with the benchmark (a la translation validation in a sense or bisimulation that is widely used in model checking). Since execution behaviour remains the same in majority of obfuscations, our approach is resilient to such exploits. We have performed several experiments in this direction and obtained encouraging results. Differences between the benchmarked behaviour and the observed behaviour quantifies the damage due to a virus. This enables us to arrive at refined notions of ""harm"" done by a virus and appropriate measures for protection.","1558-1225;0270-5257","978-1-60558-719","10.1145/1810295.1810323","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062153","","Malware;Benchmark testing;Software;Syntactics;Hardware;USA Councils","invasive software","malware;Trojan horses;viruses;worms;computing systems;malicious code detection;security;embedded devices;virus detection;personal systems;syntactic patterns;compiler optimization;program obfuscation","","","19","","","","","","IEEE","IEEE Conferences"
"Optimal placement of voltage sag monitors based on monitor reach area and sag severity index","A. A. Ibrahim; A. Mohamed; H. Shareef; S. P. Ghoshal","Dept. of Electrical, Electronics and System Engineering, Universiti Kebangsaan Malaysia, 43600 Bangi, Selangor, Malaysia; Dept. of Electrical, Electronics and System Engineering, Universiti Kebangsaan Malaysia, 43600 Bangi, Selangor, Malaysia; Dept. of Electrical, Electronics and System Engineering, Universiti Kebangsaan Malaysia, 43600 Bangi, Selangor, Malaysia; Dept. of Electrical Engineering, National Institute of Technology, 713209 Durgapur, West Bengal, India","2010 IEEE Student Conference on Research and Development (SCOReD)","","2010","","","467","470","In the modern industry, most of the equipment use semiconductor devices and microprocessors which are sensitive against power disturbances. Among all power disturbances, voltage sags are considered as the most frequent type of disturbances and their impact on sensitive loads is severe. To monitor these disturbances, voltage sag recorders (VSR) must be installed. However, installation of VSRs at all buses in the system is not economical and it needs to be minimized. This paper presents a sag monitor positioning algorithm which finds the optimal number and location of VSRs in the power distribution system. The IEEE 34-node test system was modeled in DIgSILENT software in obtaining monitor reach area matrix for various types of faults. Then, an optimization problem is formulated and performed using genetic algorithm to guarantee the observability of the whole system with minimum number of VSRs. A new index has been used to find the best location to install monitor in the distribution system.","","978-1-4244-8648-9978-1-4244-8647-2978-1-4244-8646","10.1109/SCORED.2010.5704055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5704055","voltage sag recorder (VSR);monitor reach area (MRA);genetic algorithm(GA)","Radio access networks;Computer languages;Mathematical model","optimisation;power distribution faults;power supply quality;power system measurement;power system simulation;recorders","voltage sag monitor placement;sag severity index;semiconductor device;microprocessor;voltage sag recorder;VSR;sag monitor positioning algorithm;power distribution system;IEEE 34-node test system;DIgSILENT software;monitor reach area matrix;optimization","","6","7","","","","","","IEEE","IEEE Conferences"
"Optimization of heat sink EMI using Design of Experiments with numerical computational investigation and experimental validation","S. Manivannan; R. Arumugam; S. Prasanna Devi; S. Paramasivam; P. Salil; B. Subbarao","Electrical and Electronics Department, Anna University Chennai, India; Electrical and Electronics Department, SSN College of Engineering, Chennai, India; Department of Industrial Engineering, Anna University Chennai, India; ESAB, Chennai, India; CEM - Sameer, Chennai, India; CEM - Sameer, Chennai, India","2010 IEEE International Symposium on Electromagnetic Compatibility","","2010","","","295","300","This paper presents a technique for the minimization of electromagnetic radiation from the flat plate heat sink by optimizing the heat sink geometry parameters using Taguchi Design of Experiments technique. The heat sink is modelled and simulated using numerical method via Ansoft HFSS software. Experimental investigation on the heat sink was performed in a shielded semi anechoic chamber confirming to FCC/CISPR requirements for EMC measurements. The experimental results were found to have good concurrence with the simulated results. The L27 combinations (6 factors, 3 levels) were generated by the Taguchi's Design of Experiments with orthogonal array method using Minitab software. The L27 combinations generated were analysed for radiated emission via simulation. The parameters considered for optimization are the Length and Width of the heat sink, Fin height, Base height, Number of fins and Fin thickness. With the results available for each of the 27 combinations, ANOVA test was carried out for finding out the contribution and impact of each heat sink design parameter towards the radiated emission by the heat sink.","2158-1118;2158-110X;2158-110X","978-1-4244-6308-4978-1-4244-6305-3978-1-4244-6307","10.1109/ISEMC.2010.5711288","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5711288","","Heat sinks;Geometry;Frequency domain analysis;Analysis of variance;Computational modeling;Antenna measurements;Optimization","anechoic chambers (electromagnetic);design of experiments;electromagnetic interference;electronic engineering computing;heat sinks;Taguchi methods","flat plate heat sink;electromagnetic interference;Taguchi design of experiments;heat sink geometry;shielded semi anechoic chamber;orthogonal array method;fin thickness;Ansoft HFSS software;Minitab software;ANOVA test","","2","20","","","","","","IEEE","IEEE Conferences"
"A new iterative algorithm for the solution for the load deflection square membranes","H. Youssef; A. Ferrand; P. Pons; R. Plana","CNRS; LAAS; 7, Avenue du Colonel Roche, F-31077 Toulouse, France; CNRS; LAAS; 7, Avenue du Colonel Roche, F-31077 Toulouse, France; CNRS; LAAS; 7, Avenue du Colonel Roche, F-31077 Toulouse, France; CNRS; LAAS; 7, Avenue du Colonel Roche, F-31077 Toulouse, France","2010 11th International Thermal, Mechanical & Multi-Physics Simulation, and Experiments in Microelectronics and Microsystems (EuroSimE)","","2010","","","1","5","The reliability of micro electromechanical systems depends directly on the mechanical behavior of microstructures and their performances are strongly associated with a good knowledge of materials mechanical properties that compose them. Thus, it is important to precisely determine these properties. There are various methods for micro-materials' characterization. In this work, we are interested in the bulge test technique in order to extract the mechanical properties of square shaped membranes. First, a FEM model has been realized with ABAQUS software. Then, an iterative algorithm has been developed and implemented in Matlab, it controls and optimizes the Finite Element simulation parameters. Secondly, a bulge test bench was set up to carry out experimental measurements. The iterative algorithm was then interfaced with the Abaqus software solver and used to fit the simulation results with the experimental measurements. The investigation reveals that the algorithm converges faster and gives good results. Besides, the proposed method allows us to find the mechanical properties of any membrane geometry and thickness.","","978-1-4244-7027-3978-1-4244-7026-6978-1-4244-7025","10.1109/ESIME.2010.5464523","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5464523","","Iterative algorithms;Biomembranes;Mechanical factors;Testing;Materials reliability;Electromechanical systems;Microstructure;Mathematical model;Finite element methods;Software measurement","finite element analysis;iterative methods;mechanical properties;membranes;micromechanical devices;reliability","load deflection square membranes;iterative algorithm;reliability;microelectromechanical systems;microstructures;mechanical properties;bulge test technique;FEM model;ABAQUS software;Matlab;finite element simulation;membrane geometry;membrane thickness","","1","10","","","","","","IEEE","IEEE Conferences"
"Optimized embedded memory diagnosis","M. d. Carvalho; P. Bernardi; M. S. Reorda; N. Campanelli; T. Kerekes; D. Appello; M. Barone; V. Tancorre; M. Terzi","Politecnico di Torino, Dip. di Automatica e Informatica, Italy; Politecnico di Torino, Dip. di Automatica e Informatica, Italy; Politecnico di Torino, Dip. di Automatica e Informatica, Italy; NplusT, Semiconductor Application Center, Montecastrilli (TR), Italy; NplusT, Semiconductor Application Center, Montecastrilli (TR), Italy; STMicroelectronics, Agrate Brianza (MI), Italy; STMicroelectronics, Agrate Brianza (MI), Italy; STMicroelectronics, Agrate Brianza (MI), Italy; STMicroelectronics, Agrate Brianza (MI), Italy","14th IEEE International Symposium on Design and Diagnostics of Electronic Circuits and Systems","","2011","","","347","352","This paper describes an optimized embedded memory diagnosis flow that exploits many levels of knowledge to produce accurate failure hypothesis. The proposed post-processing analysis flow is composed of many steps investigating failure shapes as well as cell fail syndromes, and includes advanced techniques to tackle incomplete data possibly due to tester noise and/or by faults showing intermittent effects. The effectiveness of the technique is demonstrated on an automotive-oriented System-on-Chip (SoC) manufactured in a 90nm technology by STMicroelectronics, which includes embedded SRAM memory cores tested using a programmable BIST. Scrambled BITMAPS gives a visual feedback leading to quick physical defect identification. Such research is relevant to aid on the manufacturing, material and process enhancements raising silicon yield.","","978-1-4244-9756-0978-1-4244-9755-3978-1-4244-9754","10.1109/DDECS.2011.5783109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5783109","Embedded memory diagnosis;Fault models;Scrambling;Failure Bitmaps;Failure analysis","Shape;Built-in self-test;Software;System-on-a-chip;Arrays;Failure analysis;Circuit faults","built-in self test;embedded systems;system-on-chip","optimized embedded memory diagnosis;failure hypothesis;intermittent effects;automotive-oriented system-on-chip;STMicroelectronics;programmable BIST;scrambled BITMAPS","","5","6","","","","","","IEEE","IEEE Conferences"
"Quality Improvement of Video Codec by Rate-Distortion Optimized Quantization","T. Huang; P. Su; C. Kao; T. Ou; H. H. Chen","NA; NA; NA; NA; NA","2011 IEEE International Symposium on Multimedia","","2011","","","482","487","Conventional quantization methods consider only the distortion between original and reconstructed video as the cost of compression. Considering the time-varying nature of network bandwidth for multimedia services, we believe a video coding system can provide a better quality of experience if it takes the bit rate of the compressed bit stream into consideration as well when optimizing the quantization. In this paper we present a rate-distortion optimization approach to the quantization of video coding. This approach is able to balance between rate and distortion for quantization and enhance the overall quality of the entire coding system, with only a slight increase in computational overhead. We implement this method in H.264/AVC, and the extensive experimental data obtained under various test conditions show that the performance of the R-D optimized quantization is indeed better than the H.264 reference software.","","978-1-4577-2015-4978-0-7695-4589","10.1109/ISM.2011.85","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6123393","","Quantization;Transforms;Bit rate;Rate-distortion;Optimization;Video coding;Software","image reconstruction;multimedia communication;quantisation (signal);rate distortion theory;video codecs;video coding","quality improvement;video codec;video reconstruction;network bandwidth;multimedia service;video coding system;quality of experience;bit stream compression;rate distortion optimization approach;video coding quantization;H.264/AVC;R-D optimized quantization","","2","11","","","","","","IEEE","IEEE Conferences"
"OSARE: Opportunistic Speculation in Actively REplicated Transactional Systems","R. Palmieri; F. Quaglia; P. Romano","NA; NA; NA","2011 IEEE 30th International Symposium on Reliable Distributed Systems","","2011","","","59","64","In this work we present OSARE, an active replication protocol for transactional systems that combines the usage of Optimistic Atomic Broadcast with a speculative concurrency control mechanism in order to overlap transaction processing and replica synchronization. OSARE biases the speculative serialization of transactions towards an order aligned with the optimistic message delivery order. However, due to the lock-free nature of its concurrency control algorithm, at high concurrency levels, namely when the probability of mismatches between optimistic and final deliveries is higher, OSARE explores additional alternative transaction serialization orders in a lightweight and opportunistic fashion. A simulation study we carried out in the context of Software Transactional Memory systems shows that OSARE achieves robust performance also in scenarios characterized by non-minimal likelihood of reorder between optimistic and final deliveries, providing remarkable speed-up with respect to state of the art speculative replication protocols.","1060-9857;1060-9857","978-1-4577-1349-1978-0-7695-4450","10.1109/SRDS.2011.16","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6076762","active replication;transaction processing systems;speculation;software transactional memories","Protocols;History;Proposals;Benchmark testing;Optimized production technology;Concurrency control","concurrency control;transaction processing","OSARE;actively replicated transactional systems;optimistic atomic broadcast;concurrency control mechanism;transaction processing;replica synchronization;optimistic message delivery order;software transactional memory systems","","18","26","","","","","","IEEE","IEEE Conferences"
"Reli: Hardware/software Checkpoint and Recovery scheme for embedded processors","T. Li; R. Ragel; S. Parameswaran","School of Computer Science and Engineering, University of New South Wales, Sydney, Australia; Department of Computer Engineering, University of Peradeniya, Sri Lanka; School of Computer Science and Engineering, University of New South Wales, Sydney, Australia","2012 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2012","","","875","880","Checkpoint and Recovery (CR) allows computer systems to operate correctly even when compromised by transient faults. While many software systems and hardware systems for CR do exist, they are usually either too large, require major modifications to the software, too slow, or require extensive modifications to the caching schemes. In this paper, we propose a novel error-recovery management scheme, which is based upon re-engineering the instruction set. We take the native instruction set of the processor and enhance the microinstructions with additional micro-operations which enable checkpointing. The recovery mechanism is implemented by three custom instructions, which recover the registers which were changed, the data memory values which were changed and the special registers (PC, status registers etc.) which were changed. Our checkpointing storage is changed according to the benchmark executed. Results show that our method degrades performance by just 1.45% under fault free conditions, and incurs area overhead of 45% on average and 79% in the worst case. The recovery takes just 62 clock cycles (worst case) in the examples which we examined.","1558-1101;1530-1591;1530-1591","978-1-4577-2145-8978-3-9810801-8","10.1109/DATE.2012.6176621","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6176621","","Registers;Program processors;Hardware;Clocks;Checkpointing;Optimization","cache storage;checkpointing;embedded systems;fault diagnosis;instruction sets","Reli;embedded processors;hardware-software checkpoint and recovery scheme;computer systems;transient faults;error-recovery management scheme;instruction set;microinstructions;microoperations;custom instructions;data memory values;checkpointing storage;fault free conditions;caching schemes","","5","31","","","","","","IEEE","IEEE Conferences"
"A new operational low cost GNSS software receiver for microsatellites","M. Grondin; M. Belasic; L. Ries; J. Issler; P. Bataille; L. Jobey; G. Richard","Centre National d'Etudes Spatiales (CNES), Toulouse, France; Centre National d'Etudes Spatiales (CNES), Toulouse, France; Centre National d'Etudes Spatiales (CNES), Toulouse, France; Centre National d'Etudes Spatiales (CNES), Toulouse, France; TES Electronic Solutions (TES), Bruz, France; TES Electronic Solutions (TES), Bruz, France; TES Electronic Solutions (TES), Bruz, France","2010 5th ESA Workshop on Satellite Navigation Technologies and European Workshop on GNSS Signals and Signal Processing (NAVITEC)","","2010","","","1","5","This paper presents a new low cost GNSS Software (SW) receiver for Microsatellites. TES Electronic Solutions (Bruz, FRANCE) has been chosen by CNES for developing and manufacturing this equipment which should be tested on board the scientific microsatellites named TARANIS and MICROSCOPE that will be launched in 2013 and 2014. The GNSS receiver under development is a high-performance equipment specially designed and optimized for the needs and constraints of small platforms for which small volume, low mass and low power consumption are important parameters. This equipment is based on COTS (Commercials off The Shelf) in order to exploit the performance of the advanced technology developed for terrestrial applications and to reduce significantly the global cost of the equipment. The structure of this GNSS receiver is organized around a reconfigurable architecture with the use of one FPGA (Field Programmable Gate Array) associated with one DSP (Digital Signal Processor). The GNSS function is then split in two main parts according to the real time requirements of the processing and navigation operations. The hardware (HW) architecture has been optimized to be able to support a low power mode through a fractioned activity that has been implemented by introducing standby time frames during the mission. An orbital extrapolator is also implemented for propagating position and velocity without pseudo range measurements. This receiver is able to process GPS (L1) and GALILEO (E1) signals simultaneously in the first version of the software and will be able to evolve in a second step to a dual frequency mode, compatible with E5a/E1, or with E5b/E1. This GNSS receiver dedicated to low earth orbit (LEO) satellites will withstand the radiation environment therefore the hardware and software architecture has been defined to reduce the single event effects (SEE) and to maximize the service availability.","2325-5439;2325-5455","978-1-4244-8741-7978-1-4244-8740-0978-1-4244-8739","10.1109/NAVITEC.2010.5707991","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5707991","GPS;GALILEO;GNSS;Software receiver;orbit;satellites;space;CNES;TES Electronic Solutions","Receivers;Global Navigation Satellite Systems;Global Positioning System;Software;Radio frequency;Power demand;Hardware","artificial satellites;digital signal processing chips;field programmable gate arrays;Global Positioning System;radio receivers","microsatellites;GNSS software receiver;TES Electronic Solutions;Bruz;France;CNES;TARANIS;MICROSCOPE;commercials off the shelf;terrestrial applications;FPGA;field programmable gate array;DSP;digital signal processor;orbital extrapolator;GPS signal;GALILEO signal;low Earth orbit satellites;single event effects","","3","2","","","","","","IEEE","IEEE Conferences"
"Designing VM schedulers for embedded real-time applications","A. Masrur; T. Pfeuffer; M. Geier; S. Drössler; S. Chakraborty","Institute for Real-Time Computer Systems, TU Munich, Germany; Institute for Real-Time Computer Systems, TU Munich, Germany; Institute for Real-Time Computer Systems, TU Munich, Germany; ReliaTec GmbH, Garching, Germany; Institute for Real-Time Computer Systems, TU Munich, Germany","2011 Proceedings of the Ninth IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS)","","2011","","","29","38","Virtual Machines (VMs) allow for platform-independent software development and their use in embedded systems is increasing. In particular, VMs are rewarding in the context of mixed-criticality applications to provide isolation between critical and non-critical tasks running on the same processor. In this paper, we study the design of a real-time system based on a VM monitor/hypervisor that supports multiple VMs/domains. Since each VM in the system runs several real-time tasks, scheduling the VMs leads to a hierarchical scheduling problem. So far, most published techniques for analyzing hierarchical scheduling deal with the schedulabil-ity problem, i.e., for a given hierarchical scheduler, testing whether a set of real-time tasks meet their deadlines. In this paper, we are rather concerned with the synthesis of hier-archical/VM schedulers; that is, how to design a scheduler such that all real-time tasks running on the different VMs meet their deadlines. We consider a setup where the tasks are scheduled on multiple VMs under fixed priorities according to the Deadline Monotonic (DM) policy. The VMs are scheduled under fixed priorities on a Rate Monotonic (RM) basis using one or more processors. A partitioned scheduling of VMs is considered, i.e., VMs are not allowed to migrate from one processor to the other. In this context, we propose a method for selecting optimum time slices and periods for each VM in the system. Our goal is to configure the VM scheduler such that not only all tasks are schedulable but also the minimum possible resources are used. Finally, to illustrate the proposed design technique, we present a case study based on automotive control applications.","","978-1-4503-0715-4978-1-4503-0715-4978-1-4503-0712","10.1145/2039370.2039378","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062302","Real-Time Scheduling;Virtual Machines;Embedded Software","Real time systems;Scheduling;Program processors;Processor scheduling;Time factors;Delta modulation;Monitoring","automotive engineering;control engineering computing;embedded systems;processor scheduling;program testing;virtual machines","VM scheduler design;embedded real-time application;virtual machines;platform-independent software development;mixed-criticality application;noncritical tasks;critical tasks;VM monitor;hierarchical scheduling problem;schedulability problem;multiple VM;deadline monotonic policy;rate monotonic basis;partitioned scheduling;automotive control applications","","3","23","","","","","","IEEE","IEEE Conferences"
"Efficient Revised Simplex Method for SVM Training","C. Sentelle; G. C. Anagnostopoulos; M. Georgiopoulos","Department of Electrical Engineering and Computer Science, University of Central Florida, Orlando, FL, USA; Department of Electrical Engineering, Florida Institute of Technology, Melbourne, FL, USA; Department of Electrical Engineering and Computer Science, University of Central Florida, Orlando, FL, USA","IEEE Transactions on Neural Networks","","2011","22","10","1650","1661","Existing active set methods reported in the literature for support vector machine (SVM) training must contend with singularities when solving for the search direction. When a singularity is encountered, an infinite descent direction can be carefully chosen that avoids cycling and allows the algorithm to converge. However, the algorithm implementation is likely to be more complex and less computationally efficient than would otherwise be required for an algorithm that does not have to contend with the singularities. We show that the revised simplex method introduced by Rusin provides a guarantee of nonsingularity when solving for the search direction. This method provides for a simpler and more computationally efficient implementation, as it avoids the need to test for rank degeneracies and also the need to modify factorizations or solution methods based upon those rank degeneracies. In our approach, we take advantage of the guarantee of nonsingularity by implementing an efficient method for solving the search direction and show that our algorithm is competitive with SVM-QP and also that it is a particularly effective when the fraction of nonbound support vectors is large. In addition, we show competitive performance of the proposed algorithm against two popular SVM training algorithms, SVMLight and LIBSVM.","1045-9227;1941-0093","","10.1109/TNN.2011.2165081","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6009228","Active set method;null space method;quadratic programming;revised simplex method;support vector machine","Support vector machines;Training;Equations;Kernel;Pricing;Integrated circuits;Indexes","learning (artificial intelligence);quadratic programming;set theory;support vector machines","simplex method;SVM training;support vector machine;rank degeneracy;SVMLight algorithm;LIBSVM algorithm;active set method;quadratic programming","Algorithms;Artificial Intelligence;Models, Neurological;Neural Networks (Computer);Software;Software Design;Software Validation","10","26","","","","","","IEEE","IEEE Journals & Magazines"
"A Modified Conjugate Gradient Algorithm with Sufficient Descent","B. Jiao; J. Han; L. Chen","NA; NA; NA","2011 Fourth International Joint Conference on Computational Sciences and Optimization","","2011","","","175","177","A modification of the Neculai Andrei conjugate gradient algorithm is proposed. Using exact line search, the algorithm reduces to the original version of the Dai and Yuan computational scheme. For inexact line search the algorithm satisfies both sufficient descent and conjugate conditions. A global convergence result is proved when the Wolfe line search conditions are used. Computational results show that this new conjugate gradient algorithm substantially outperforms the Neculai Andrei conjugate gradient algorithm.","","978-1-4244-9712-6978-0-7695-4335","10.1109/CSO.2011.38","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5957636","conjugate gradient method;sufficient descent property;Wolfe line search;global convergence","Gradient methods;Convergence;Software;Level set;Testing;Linear systems","conjugate gradient methods;search problems","modified conjugate gradient algorithm;sufficient descent;Neculai Andrei conjugate gradient algorithm;exact line search;Wolfe line search conditions","","","8","","","","","","IEEE","IEEE Conferences"
"Using data mining in optimisation of building energy consumption and thermal comfort management","Y. Gao; E. Tumwesigye; B. Cahill; K. Menzel","Department of Civil and Environmental Engineering, University College Cork, College Road, Cork, Ireland; Department of Civil and Environmental Engineering, University College Cork, College Road, Cork, Ireland; Department of Civil and Environmental Engineering, University College Cork, College Road, Cork, Ireland; Department of Civil and Environmental Engineering, University College Cork, College Road, Cork, Ireland","The 2nd International Conference on Software Engineering and Data Mining","","2010","","","434","439","Performance monitoring using wireless sensors is now common practice in building operation and maintenance and generates a large amount of building specific data. However, it is difficult for occupants, owners and operators to explore such data and understand underlying patterns. This is especially true in buildings which involve complex interactions, such as ventilation, solar gains, internal gains and thermal mass. Performance monitoring requires collecting data concerning energy consumption and ambient environmental conditions to model and optimise buildings' energy consumption. This paper details the use of data mining techniques in understanding building energy performance of geothermal, solar and gas burning energy systems. The paper is part of an outgoing research into optimisation of building performance under hybrid energy regimes. The objective of the research presented in this paper is to predict comfort levels based on the Heating, Ventilating, and Air Conditioning (HVAC) system performance and external environmental conditions. A C4.5 classification methodology is used to analyse a combination of internal and external ambient conditions. The mining algorithms are used to determine comfort constraints and the influence of external conditions on a building's internal user comfort. To test the performance of classification and its use in prediction, different offices, one to the south and the other to the north of the building are used. Classification rules being developed are analysed for their application to modify control algorithms and to apply results to generalise hybrid system performance. The results of this study can be generalised for an entire building, or a set of buildings, under a single energy network subject to the same constraints.","","978-89-88678-22-0978-1-4244-7324","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5542881","sensors;HVAC;energy;performance;multi-dimension;classification;data mining","Thermal management;Data mining;Energy consumption;Energy management;System performance;Wireless sensor networks;Solar power generation;Ventilation;Condition monitoring;Heating","building management systems;data mining;energy consumption;HVAC;optimisation;pattern classification;power engineering computing;wireless sensor networks","data mining techniques;building energy consumption optimisation;thermal comfort management;performance monitoring;wireless sensors;ambient environmental conditions;geothermal energy systems;solar energy systems;gas burning energy systems;heating, ventilating, and air conditioning system;C4.5 classification methodology","","1","19","","","","","","IEEE","IEEE Conferences"
"Machine design optimization based on finite element analysis in a high-throughput computing environment","W. Jiang; T. M. Jahns; T. A. Lipo; W. Taylor; Y. Suzuki","Dept. of Electrical & Computer Engineering, University of Wisconsin-Madison, USA; Dept. of Electrical & Computer Engineering, University of Wisconsin-Madison, USA; Dept. of Electrical & Computer Engineering, University of Wisconsin-Madison, USA; Dept. of Computer Science, University of Wisconsin-Madison, USA; JSOL Corporation, Middleton, WI USA","2012 IEEE Energy Conversion Congress and Exposition (ECCE)","","2012","","","869","876","Although finite element (FE) analysis is a powerful analytical tool for electric machines, it is rarely used in iterative machine design optimization programs since it is computationally intensive, requiring excessive calculation times. This paper describes an approach for overcoming this obstacle using a high-throughput computing (HTC) environment that harnesses the parallel processing capabilities of large numbers of computers to evaluate many candidate designs simultaneously. Differential evolution has been selected as the optimization algorithm that applies FE analysis to maximize the electromagnetic performance according to an objective function in a computationally-efficient manner. This software has been applied using available HTC resources to optimize the design of a 30 kW (continuous) fractional-slot concentrated winding (FSCW) surface permanent magnet (SPM) machine for high torque density. Tests comparing the computational speeds achieved using the same optimization software with the HTC resources and a single computer have demonstrated a major reduction (approx. 30:1) of the computation time using the HTC approach.","2329-3721;2329-3748","978-1-4673-0803-8978-1-4673-0802-1978-1-4673-0800-7978-1-4673-0801","10.1109/ECCE.2012.6342727","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6342727","","Torque;Iron;Algorithm design and analysis;Computers;Design optimization;Power capacitors","computational electromagnetics;electric machine analysis computing;finite element analysis;iterative methods;machine windings;optimisation;parallel programming;permanent magnet machines","finite element analysis;electric machine;iterative machine design optimization;high-throughput computing;parallel processing;differential evolution;FE analysis;electromagnetic performance;HTC;fractional slot concentrated winding;FSCW;surface permanent magnet machine;SPM machine;torque density;power 30 kW","","26","11","","","","","","IEEE","IEEE Conferences"
"Real time acquisition of ESPI specklegram and automated analysis for NDE","A. Mujeeb; V. U. Nayar","University College of Engineering, CUSAT Campus, Kuttanadu, Alappuzha 688 504, Kerala, India; Indian Institute of Science Education and Research, CET Campus, Trivandrum-695016 Kerala, India","2011 3rd International Congress on Ultra Modern Telecommunications and Control Systems and Workshops (ICUMT)","","2011","","","1","3","Electronic Speckle Pattern Interferometry (ESPI) is a fast developing whole field optical technique used for the measurement of optical phase changes under object deformations and has evolved as a powerful on-line nondestructive evaluation (NDE) tool. In ESPI the speckle interferometry data of the object under different loading conditions are stored as digital images and then converted into interference fringes by digital subtraction or by addition method. These interference patterns correspond to surface displacements, which in turn relates to the strain variation in the specimen. Defects in the specimen produce a strain concentration on loading and generate a fringe anomaly in the interferogram. These anomalies in the interferogram are analyzed for the characterisation of defects in the material. This technique is used as an effective tool in Non-Destructive Evaluation (NDE). In this paper a method for on-line analysis of ESPI fringe patterns for the qualitative evaluation in the materials is presented. Continuous acquisition and simultaneous digital processing of specklegrams has been developed for generating the fringe patterns. The image acquisition and processing system used for the present study is an algorithm developed by using Imaging Graphs software. Image processing techniques are introduced for contrast enhancement, noise reduction and fringe sharpening for the on-line detection of different types of defects in the material.","2157-023X;2157-0221;2157-0221","978-963-8111-77-7978-1-4577-0682-0978-963-8111-75","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6079003","Laser Speckle;Interferometry;ESPI;NonDestructive Evaluation;Fringe Analysis;Image Processing","Speckle;Optical interferometry;Loading;Materials;Holography;Optimized production technology","electronic speckle pattern interferometry;image denoising;image enhancement;light interference;nondestructive testing;optical variables measurement;physics computing","real time image acquisition;ESPI specklegram;automated analysis;electronic speckle pattern interferometry;optical technique;optical phase measurement;object deformations;on-line nondestructive testing;interference fringes;digital subtraction;surface displacements;strain concentration;fringe patterns;digital image processing;imaging graph software;contrast enhancement;noise reduction;fringe sharpening;defect detection","","","16","","","","","","IEEE","IEEE Conferences"
"Universal rules guided design parameter selection for soft error resilient processors","L. Duan; Y. Zhang; B. Li; L. Peng","Department of Electrical and Computer Engineering, Louisiana State University, Baton Rouge, LA 70803; Department of Electrical and Computer Engineering, Louisiana State University, Baton Rouge, LA 70803; Department of Experimental Statistics, Louisiana State University, Baton Rouge, LA 70803; Department of Electrical and Computer Engineering, Louisiana State University, Baton Rouge, LA 70803","(IEEE ISPASS) IEEE International Symposium on Performance Analysis of Systems and Software","","2011","","","247","256","High-performance processors suffer from soft error vulnerability due to the increasing on-chip transistor density, shrinking processor feature size, lower threshold voltage, etc. In this paper, we propose to use a rule search strategy, i.e. Patient Rule Induction Method (PRIM), to optimize processor soft error robustness. By exploring a huge microarchitectural design space on the Architectural Vulnerability Factor (AVF), we are capable of generating a set of selective rules on key design parameters. Applying these rules at early design stage effectively identifies the configurations that are inherently reliable to soft errors. Furthermore, we propose a generic approach capable of generating a set of “universal” rules that achieves the optimization of the output variable for different programs in execution. The effectiveness of the universal rule set is validated on programs that are not used in training. This cross-program capability is very useful in the era of multi-threading. Finally, the proposed scheme is extended to multiprocessors where multiple design metrics including reliability, performance and power are balanced. Our proposed methodology is able to generate quantitative and universal solutions for both uniprocessors and multiprocessors.","","978-1-61284-368-1978-1-61284-367","10.1109/ISPASS.2011.5762741","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5762741","soft error;architectural vulnerability factor;cross-program;design parameter selection;multi-objective optimization","Program processors;Benchmark testing;Registers;Reliability engineering;Training;Input variables","logic design;microprocessor chips;multiprocessing systems;multi-threading;optimisation;query formulation","soft error resilient processors;high-performance processors;on-chip transistor density;patient rule induction method;microarchitectural design space;architectural vulnerability factor;optimization;universal rule set;cross-program capability;multithreading;multiprocessors;multiple design metrics;reliability","","4","36","","","","","","IEEE","IEEE Conferences"
"Domain specific architecture for next generation wireless communication","B. Zhang; H. Liu; H. Zhao; F. Mo; T. Chen","Institute of Microelectrics & Microprocessor, School of Computer Science, National University of Defense Technology, Changsha HN, China; Institute of Microelectrics & Microprocessor, School of Computer Science, National University of Defense Technology, Changsha HN, China; Institute of Microelectrics & Microprocessor, School of Computer Science, National University of Defense Technology, Changsha HN, China; Institute of Microelectrics & Microprocessor, School of Computer Science, National University of Defense Technology, Changsha HN, China; Institute of Microelectrics & Microprocessor, School of Computer Science, National University of Defense Technology, Changsha HN, China","2010 Design, Automation & Test in Europe Conference & Exhibition (DATE 2010)","","2010","","","1414","1419","In order to solve the challenges in processor design for the next generation wireless communication systems, this paper first proposes a system level design flow for communication domain specific processor, and then proposes a novel processor architecture for the next generation wireless communication named GAEA using this design flow. GAEA is a shared memory multi-core SoC based on Software Controlled Time Division Multiplexing Bus, with which programmers can easily explore memory-level parallelism of applications by proper instructions and scheduling algorithms. MPE, which is the kernel component of GAEA, adopts hybrid parallel processing scheme to explore instruction-level and data-level parallelism. The pipeline and instruction set of GAEA are also optimized for the next generation wireless communication systems. The evaluation and implementation results show that GAEA architecture is suitable for the next generation wireless communication systems.","1558-1101;1530-1591;1530-1591","978-3-9810801-6-2978-1-4244-7054-9978-3-9810801-6","10.1109/DATE.2010.5457034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5457034","","Wireless communication;Process design;System-level design;Computer architecture;Communication system control;Time division multiplexing;Programming profession;Application software;Scheduling algorithm;Kernel","integrated circuit design;scheduling;shared memory systems;software radio;system-on-chip;time division multiplexing","domain specific architecture;processor design;next generation wireless communication systems;system level design flow;communication domain specific processor;GAEA;shared memory multicore SoC;software controlled time division multiplexing bus;memory-level parallelism;scheduling algorithms;hybrid parallel processing scheme;instruction-level parallelism;data-level parallelism;software defined radio","","4","16","","","","","","IEEE","IEEE Conferences"
"A New Perry Conjugate Gradient Method with the Generalized Conjugacy Condition","D. Liu; Y. Shang","NA; NA","2010 International Conference on Computational Intelligence and Software Engineering","","2010","","","1","4","The Perry conjugate gradient method is generalized, from which a new descent algorithm, PDCGy, is presented and its global convergence is proven under Wolfe line searches. Preliminary numerical results for a set of 720 unconstrained optimization test problems verify the performance of the algorithm and show that the PDCGy algorithm is competitive with the CG_DESCENT algorithm.","","978-1-4244-5391-7978-1-4244-5392","10.1109/CISE.2010.5677114","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5677114","","Gradient methods;Convergence;Noise measurement;Matrices","conjugate gradient methods;convergence of numerical methods;optimisation","Perry conjugate gradient method;generalized conjugacy condition;global convergence;Wolfe line searches;unconstrained optimization;PDCGy algorithm;CG_DESCENT algorithm","","","17","","","","","","IEEE","IEEE Conferences"
"Hardware/software-based diagnosis of load-store queues using expandable activity logs","J. Carretero; X. Vera; J. Abella; T. Ramírez; M. Monchiero; A. González","Intel Barcelona Research Center, Intel Labs - UPC, Barcelona, Spain; Intel Barcelona Research Center, Intel Labs - UPC, Barcelona, Spain; Intel Barcelona Research Center, Intel Labs - UPC, Barcelona, Spain; Intel Barcelona Research Center, Intel Labs - UPC, Barcelona, Spain; Intel Barcelona Research Center, Intel Labs - UPC, Barcelona, Spain; Intel Barcelona Research Center, Intel Labs - UPC, Barcelona, Spain","2011 IEEE 17th International Symposium on High Performance Computer Architecture","","2011","","","321","331","The increasing device count and design complexity are posing significant challenges to post-silicon validation. Bug diagnosis is the most difficult step during post-silicon validation. Limited reproducibility and low testing speeds are common limitations in current testing techniques. Moreover, low observability defies full-speed testing approaches. Modern solutions like on-chip trace buffers alleviate these issues, but are unable to store long activity traces. As a consequence, the cost of post-Si validation now represents a large fraction of the total design cost. This work describes a hybrid post-Si approach to validate a modern load-store queue. We use an effective error detection mechanism and an expandable logging mechanism to observe the microarchitectural activity for long periods of time, at processor full-speed. Validation is performed by analyzing the log activity by means of a diagnosis algorithm. Correct memory ordering is checked to root the cause of errors.","2378-203X;1530-0897;1530-0897","978-1-4244-9435-4978-1-4244-9432-3978-1-4244-9434","10.1109/HPCA.2011.5749740","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5749740","","Optimization;Software;Testing;Microarchitecture;Computer bugs;Hardware;Latches","computer architecture;error detection;fault diagnosis;microprocessor chips;program debugging;program diagnostics;program verification;system recovery","hardware-software based diagnosis;load store queue;expandable activity logs;bug diagnosis;hybrid post-silicon validation approach;error detection mechanism;expandable logging mechanism;microarchitectural activity;processor full-speed;memory ordering","","3","25","","","","","","IEEE","IEEE Conferences"
"Image reconstruction for the partially immersed conductor by dynamic differential evolution","C. Chen; W. Hsiao; C. Chiu","Department of Computer and Communication Engineering, Taipei College of Maritime Technology Danshui Town. Taioei Countv. Taiwan. R.O.C.; Electrical Engineering Department, Tamkang University, Danshui Town, Taipei County, Taiwan, R.O.C.; Electrical Engineering Department, Tamkang University, Danshui Town, Taipei County, Taiwan, R.O.C.","2011 IEEE 3rd International Conference on Communication Software and Networks","","2011","","","433","436","The application of one technique for the reconstruction of shape reconstruction of a metallic cylinder from scattered field measurements is studied in this paper. Considering that the microwave imaging is recast as a nonlinear optimization problem, a cost functional is defined by the norm of a difference between the measured scattered electric field and that calculated for an estimated the shape of metallic cylinder. Thus, the shape of metallic cylinder can be obtained by minimizing the cost function. In order to solve this inverse scattering problem, dynamic differential evolution (DDE) is employed. The technique has been tested in the case of simulated measurement contaminated by additive white Gaussian noise. Numerical results demonstrate that, even when the initial guess is far away from the exact one, good reconstruction can be obtained.","","978-1-61284-486-2978-1-61284-485-5978-1-61284-484","10.1109/ICCSN.2011.6014929","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014929","inverse scattering problem;partially immersed conductor;dynamic differential evolution;transverse electric wave","Shape;Image reconstruction","AWGN;image reconstruction;microwave imaging;optimisation","image reconstruction;partially immersed conductor;dynamic differential evolution;shape reconstruction;scattered field measurements;microwave imaging;nonlinear optimization problem;metallic cylinder;cost function;additive white Gaussian noise","","","8","","","","","","IEEE","IEEE Conferences"
"The Design and Application of I2C Bus in the TD-LTE Comprehensive Test Instrument","C. Fatang; Y. Linyu","NA; NA","2011 International Conference on Business Computing and Global Informatization","","2011","","","622","625","Based on the development platform of the TD-LTE Comprehensive Test Instrument, to enable the system to adapt to the needs of a variety of configurations and be more flexible, the paper proposes a method of transferring the control information by I<sup>2</sup>C Bus from DSP to FPGA, then implementing I<sup>2</sup>C slave interface in FPGA. The experimental simulations show the correction of I<sup>2</sup>C interface. In the end, the paper proposes an optimized program.","2378-8941;2378-895X","978-1-4577-0788-9978-0-7695-4464","10.1109/BCGIn.2011.165","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6003976","TD-LTE;I2C;FPGA;DSP","Field programmable gate arrays;Writing;Digital signal processing;Hardware;Instruments;Timing;Software radio","digital signal processing chips;field programmable gate arrays;Long Term Evolution","I<sup>2</sup>C bus;TD-LTE comprehensive test instrument;DSP;FPGA;I<sup>2</sup>C slave interface","","2","6","","","","","","IEEE","IEEE Conferences"
"Optimized pre-copy live migration for memory intensive applications","K. Z. Ibrahim; S. Hofmeyr; C. Iancu; E. Roman","Lawrence Berkeley National Laboratory, One Cyclotron Road, Berkeley, USA; Lawrence Berkeley National Laboratory, One Cyclotron Road, Berkeley, USA; Lawrence Berkeley National Laboratory, One Cyclotron Road, Berkeley, USA; Lawrence Berkeley National Laboratory, One Cyclotron Road, Berkeley, USA","SC '11: Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis","","2011","","","1","11","Live migration is a widely used technique for resource consolidation and fault tolerance. KVM and Xen use iterative pre-copy approaches which work well in practice for commercial applications. In this paper, we study pre-copy live migration of MPI and OpenMP scientific applications running on KVM and present a detailed performance analysis of the migration process. We show that due to a high rate of memory changes, the current KVM rate control and target downtime heuristics do not cope well with HPC applications: statically choosing rate limits and downtimes is infeasible and current mechanisms sometimes provide sub-optimal performance. We present a novel on-line algorithm able to provide minimal downtime and minimal impact on end-to-end application performance. At the core of this algorithm is controlling migration based on the application memory rate of change.","2167-4329;2167-4337","978-1-4503-0771-0978-1-4503-0771","10.1145/2063384.2063437","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6114430","Virtual Machines;State Migration;Checkpoint Restart","Virtual machining;Bandwidth;Instruction sets;Monitoring;Benchmark testing;Random access memory;Measurement","application program interfaces;checkpointing;message passing;resource allocation;software fault tolerance;software performance evaluation;virtual machines;virtualisation","optimized pre-copy live migration;memory intensive applications;resource consolidation;fault tolerance;KVM rate control;Xen;MPI application;OpenMP scientific application;performance analysis;migration control;virtualization technologies;virtual machines;checkpoint restart;kernel-based virtual machine;end-to-end application performance","","19","20","","","","","","IEEE","IEEE Conferences"
"Simulation and test on virtual prototype of heavy-duty truck with full-floating cab and air suspension","Dengfeng Wang; He Hao; Bin Zhang; Shuming Chen; Shanpo Wang; Guoqiang Liu","State Key Laboratory of Automobile Dynamic Simulation, Jilin University, Changchun 130022, China; State Key Laboratory of Automobile Dynamic Simulation, Jilin University, Changchun 130022, China; State Key Laboratory of Automobile Dynamic Simulation, Jilin University, Changchun 130022, China; State Key Laboratory of Automobile Dynamic Simulation, Jilin University, Changchun 130022, China; China National Heavy Duty Truck Group Co. Ltd., Jinan 250002, China; China National Heavy Duty Truck Group Co. Ltd., Jinan 250002, China","2010 International Conference On Computer Design and Applications","","2010","4","","V4-209","V4-213","Based on multi-body dynamic method, a rigid-flexible coupling model of a heavy-duty truck with full-floating cab mount system, air suspension system with four airbags and taper-leaf-spring system was built. Some simulation calculations were performed in ADAMS software environment to analyze ride comfort on B-class road surface and steady-state steering characteristic with full load. Validity of the model was verified by comparing the simulation results with experiment data. The results show that the rigid-elastic coupling model established in this paper is accurate. So it is feasible to analyze and optimize the dynamic performance of a heavy-duty truck using virtual prototyping technology in production R&amp;D stage.","","978-1-4244-7164-5978-1-4244-7164","10.1109/ICCDA.2010.5541215","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5541215","Heavy-duty truck;Full-floating cab;Air suspension;Ride comfort;Steady-state steering characteristic","Testing;Virtual prototyping;Springs;Computational modeling;Vehicle dynamics;Computer simulation;Performance analysis;Steady-state;Vehicles;Analytical models","automobile manufacture;automotive components;road vehicles;suspensions (mechanical components);vehicle dynamics;virtual prototyping","virtual prototype;heavy-duty truck;multibody dynamic method;rigid-flexible coupling model;full-floating cab mount system;air suspension system;airbags;taper-leaf-spring system;simulation calculations;ADAMS software environment;B-class road surface;steady-state steering characteristic;rigid-elastic coupling model;virtual prototyping technology;production R&D stage","","1","5","","","","","","IEEE","IEEE Conferences"
"Kernel Dimensionality Reduction evaluation on various dimensions of effective subspaces for cancer patient survival analysis","Y. S. Chin; I. Wasito; S. Z. Mohd Hashim","Department of Software Engineering, Faculty of Computer Science and Information Systems, University Technology Malaysia, Skudai, Johor Bahru, Malaysia; Faculty of Computer Science, University of Indonesia, Jakarta, Indonesia; Department of Software Engineering, Faculty of Computer Science and Information Systems, University Technology Malaysia, Skudai, Johor Bahru, Malaysia","10th International Conference on Information Science, Signal Processing and their Applications (ISSPA 2010)","","2010","","","790","793","In this research, we have extended the use of Kernel Dimensionality Reduction (KDR) in the context of semi supervised learning in particular for micro-array DNA clustering application. We have proposed a new model call K-means-KDR for survival analysis which we aimed to improve the genes classification performance and study the dimension of effective subspaces in cancer patient survival analysis. KDR method was extended and combined with the K-means clustering technique, Cox's proportional hazards regression model and log rank test where KDR contributes in gene classification to determine subgroups from the patient's group. Results from the experiments have indicated that our model has outperformed Support Vector Machines (SVM) in gene classification. We also observed that the best value for dimension of effective subspaces (K) for microarray DNA data is between 10%-20% of the total patients.","","978-1-4244-7167-6978-1-4244-7165-2978-1-4244-7166","10.1109/ISSPA.2010.5605512","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5605512","Kernel Dimensionality Reduction (KDR);Dimension of Effective Subspaces (K)","Training;Kernel;Support vector machines;Immune system;Cancer;Bioinformatics","bioinformatics;cancer;DNA;genetics;learning (artificial intelligence);patient diagnosis;pattern classification;pattern clustering;regression analysis;set theory","kernel dimensionality reduction;semisupervised learning;DNA clustering;k-means clustering technique;genes classification;cancer patient;survival analysis;Cox proportional hazards regression model;log rank test;support vector machine;SVM;microarray DNA","","","15","","","","","","IEEE","IEEE Conferences"
"Notice of Retraction<br>Optimizing the sound radiation of oil pan based on ANSYS","Hu Ming-jiang; Qi Li-qiao","Department of Thermal energy Engineering, Henan University of Urban Construction, Pingdingshan, China; Department of Thermal energy Engineering, Henan University of Urban Construction, Pingdingshan, China","2010 International Conference on Computer Application and System Modeling (ICCASM 2010)","","2010","2","","V2-538","V2-541","Based on FEM/BEM method, the oil pan models on the finite element and the semi-anechoic chamber boundary element were established. Based on the oil pan models and the engine operating conditions, the oil pan structure had been improved. Using ANSYS software, the oil pan models were optimized, the effect on reducing vibration and sound radiation were predicted by increasing the stiffener and the damping material of the oil pan. The tests on sound intensity and sound power of the diesel engine were carried out; the test results indicated that the oil pan model was right; the methods on reducing the vibration and the sound reduction of the oil pan such as the stiffener and the damping material were reliable.;Notice of Retraction<br><br>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<br><br>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<br><br>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<br><br>","2161-9069;2161-9077","978-1-4244-7237-6978-1-4244-7235","10.1109/ICCASM.2010.5620693","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5620693","ANSYS;Oil Pan;Sound Radiation;Sound Power","Couplings","boundary-elements methods;damping;diesel engines;finite element analysis;noise abatement;vibration isolation","sound radiation optimization;oil pan structure;ANSYS software;FEM method;BEM method;semianechoic chamber boundary element;engine operating conditions;vibration;stiffener;damping material;diesel engine;sound reduction","","","7","","","","","","IEEE","IEEE Conferences"
"Vehicle windscreen wiper mathematical model development and optimisation for model based hardware-in-the-loop simulation and control","J. Wei; A. Mouzakitis; J. Wang; Hao Sun","School of Engineering, University of Warwick, Coventry CV4 7AL, UK; Jaguar Land Rover Engineering Centre, Whitley, Coventry, CV3 4LF, UK; School of Engineering, University of Warwick, Coventry CV4 7AL, UK; School of Engineering, University of Warwick, Coventry CV4 7AL, UK","The 17th International Conference on Automation and Computing","","2011","","","207","212","Hardware-in-the-loop (HIL) simulations have long been used to test electronic control units (ECUs) and software in car manufactures. Accurate Model based HIL simulation (AMHIL) is considered as a most efficient and cost effective way for exploration of new design and development of new products, particularly in calibration and parameterization of vehicle stability controller. The paper presents our recent work in developing an accurate real-time mathematical model for a Jaguar windscreen wiper system, which will be adopted for the HIL tests to the vehicle body control module (BCM). Based on the electro-mechanical engineering principles, the mathematical model of the windscreen wiper system is developed and the unknown model parameters are identified using Genetic Algorithms. Model has been tested by operating at different working conditions, e.g. varied voltage input levels. Real-time implementation of the mathematical model onto dSPACE Ecoline HIL simulator is explained, and the configurations of the laboratory test rig are reported.","","978-1-86218-098-7978-1-4673-0000","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6084928","Harware-in-the-Loop;Real-time Modelling & Control;Genetic Algorithm","Mathematical model;DC motors;Biological system modeling;Genetic algorithms;Couplings;Real time systems;Load modeling","automobile manufacture;automobiles;automotive electronics;calibration;digital simulation;electromechanical effects;embedded systems;genetic algorithms;mathematical analysis;product design;product development;stability","vehicle windscreen wiper mathematical model development;vehicle windscreen wiper mathematical model optimisation;model based hardware-in-the-loop simulation;model based control;electronic control units;ECU testing;software testing;car manufactures;accurate model based HIL simulation;AMHIL;calibration;vehicle stability controller;real-time mathematical model;Jaguar windscreen wiper system;vehicle body control module;electromechanical engineering principle;genetic algorithm;dSPACE Ecoline HIL simulator;laboratory test rig configuration","","1","12","","","","","","IEEE","IEEE Conferences"
"Heap access optimizations for a hardware-accelerated Java virtual machine","J. Tyystjärvi; T. Säntti; J. Plosila","Turku Centre for Computer Science, Finland; Dept. of Information Technology, University of Turku, Finland; Dept. of Information Technology, University of Turku, Finland","2010 International Symposium on System on Chip","","2010","","","125","128","The REALJava virtual machine consists of a software partition running on a general-purpose CPU and a hardware partition containing one or more Java co-processor units. The co-processor units execute most of the bytecode, while the software partition handles complex instructions and tasks such as class loading, input and output and memory management. The software partition and the co-processors communicate using a general communication channel such as a bus. By far the most common instructions executed in the software partition are heap accesses. Because executing instructions on software is relatively slow, code-improving transformations which reduce the number of interrupts generated and the amount of communication can have a large impact on performance. An improvement on an existing technique called supersequence transformation is presented which makes the technique more general and reduces the amount of communication required between the partitions and the number of interrupts generated. The improved technique is shown to improve performance over the original in many programs.","","978-1-4244-8278-8978-1-4244-8279-5978-1-4244-8277","10.1109/ISSOC.2010.5625548","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5625548","","Java;Benchmark testing;Virtual machining;Optimization;Generators;Instruction sets","coprocessors;data structures;Java;virtual machines","heap access optimizations;hardware accelerated Java virtual machine;REALJava virtual machine;software partition;Java coprocessor units;complex instructions;memory management;general communication channel;supersequence transformation","","","8","","","","","","IEEE","IEEE Conferences"
"Vision for cross-layer optimization to address the dual challenges of energy and reliability","A. DeHon; H. M. Quinn; N. P. Carter","Electrical and Systems Engineering, University of Pennsylvania, 200 S. 33rd St., Philadelphia, 19104, USA; Los Alamos National Laboratory, ISR-3 Space Data Systems, NM 87545, USA; Intel Corporation, 2200 Mission College Blvd, RNB6-61, Santa Clara, California 95054, USA","2010 Design, Automation & Test in Europe Conference & Exhibition (DATE 2010)","","2010","","","1017","1022","We are rapidly approaching an inflection point where the conventional target of producing perfect, identical transistors that operate without upset can no longer be maintained while continuing to reduce the energy per operation. With power requirements already limiting chip performance, continuing to demand perfect, upset-free transistors would mean the end of scaling benefits. The big challenges in device variability and reliability are driven by uncommon tails in distributions, infrequent upsets, one-size-fits-all technology requirements, and a lack of information about the context of each operation. Solutions co-designed across traditional layer boundaries in our system stack can change the game, allowing architecture and software (a) to compensate for uncommon variation, environments, and events, (b) to pass down invariants and requirements for the computation, and (c) to monitor the health of collections of devices. Cross-layer codesign provides a path to continue extracting benefits from further scaled technologies despite the fact that they may be less predictable and more variable. While some limited multi-layer mitigation strategies do exist, to move forward redefining traditional layer abstractions and developing a framework that facilitates cross-layer collaboration is necessary.","1558-1101;1530-1591;1530-1591","978-3-9810801-6-2978-1-4244-7054-9978-3-9810801-6","10.1109/DATE.2010.5456959","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5456959","","Integrated circuit noise;Semiconductor device noise;Space heating;Threshold voltage;Working environment noise;Convergence;Delay;Circuit noise;Integrated circuit yield;Semiconductor device manufacture","integrated circuit design;integrated circuit reliability;low-power electronics;optimisation","cross-layer optimization;device variability;reliability;traditional layer abstractions;cross-layer collaboration","","25","26","","","","","","IEEE","IEEE Conferences"
"JVM Virtual Method Invoking Optimization Based on CAM Table","S. Cai; Y. Yang; C. Lin; Q. Liu","NA; NA; NA; NA","2011 IEEE Sixth International Conference on Networking, Architecture, and Storage","","2011","","","122","129","In Java programs, it needs to use the information of the method type to resolve the virtual method dynamically, which restricts the performance greatly. Currently, the solution is mainly the technique of inline caching, which can be divided into two categories: monomorphic inline caching and polymorphic inline caching. Because of the simple implementation of monomorphic inline caching, it is more commonly used, but it cannot resolve the problem of frequently seeing different types of objects at one call site. Although polymorphic inline caching can solve this problem, it is costly. This paper presents the CAM hardware table, proposes a virtual method invoking mechanism based on software and hardware co-design. It solves the problem of frequently seeing different types of objects at one call site, and does not introduce additional overhead. The experimental evaluation shows that it improves the cached hit rate from 13.3% to 76.4% and improves 16.2% performance of the virtual test, it improves the performance of SPECjvm 98 by 6.4% on average.","","978-0-7695-4509-7978-1-4577-1172","10.1109/NAS.2011.13","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6005431","CAM table;JVM virtual method invocation;inline cache","Computer aided manufacturing;Java;Hardware;Software;Indexes;Virtual machining;Cryptography","cache storage;content-addressable storage;hardware-software codesign;Java;virtual machines","JVM virtual method invoking optimization;Java virtual machine;CAM hardware table;Java programs;monomorphic inline caching;polymorphic inline caching;software-and-hardware co-design;content associated memory","","","17","","","","","","IEEE","IEEE Conferences"
"Code-Change Impact Analysis Using Counterfactuals","M. Peralta; S. Mukhopadhyay","NA; NA","2011 IEEE 35th Annual Computer Software and Applications Conference","","2011","","","694","699","In this paper we present a framework for what-if analysis of programs based on Lewis' theory of counterfactuals. The framework can be used to statically perform change impact analysis for source code. It enables us to verify assertions about a changed version of the program without actually incorporating the changes. We present a logical calculus that precisely characterizes structural modifications to source code and their impact on the behavior of the program.","0730-3157;0730-3157;0730-3157","978-0-7695-4439-7978-1-4577-0544-1978-0-7695-4439","10.1109/COMPSAC.2011.96","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032420","formal verification;proof theory;theorem proving;software engineering","Calculus;Context;Semantics;Grammar;Cognition;Context modeling;Optimization","formal verification;program testing;theorem proving","code-change impact analysis;what-if analysis;counterfactuals;logical calculus;structural modifications;software development;formal verification;proof theory","","","14","","","","","","IEEE","IEEE Conferences"
"A software-based mobile DVB-T2 measurement receiver","M. Slimani; J. Robert; J. Zoellner","Researchers at Institut fuer Nachrichtentechnik of Technische Universitaet Braunschweig, Germany; Researchers at Institut fuer Nachrichtentechnik of Technische Universitaet Braunschweig, Germany; Researchers at Institut fuer Nachrichtentechnik of Technische Universitaet Braunschweig, Germany","IEEE international Symposium on Broadband Multimedia Systems and Broadcasting","","2012","","","1","6","The second generation DVB standard for digital terrestrial broadcasting (DVB-T2) is already in regular operation in several countries around the world. The networks operated in these countries are primarily optimized for stationary reception. Consequently, all currently available commercial receivers have been optimized for the stationary use case. However, thanks to a wide range of parameter configurations, the DVB-T2 standard is also well-suited for mobile reception, which is currently evaluated in a field trial with DVB-T2 in the northern part of Germany. As no mobile measurement receivers are available, we developed the world's first software-based mobile DVB-T2 measurement receiver for evaluating the performance of DVB-T2 within this field trial. Besides the decoding of the Transport Stream, the receiver is also able to measure detailed transmission channel characteristics and link them to their geographical position using GPS data.","2155-5052;2155-5044;2155-504","978-1-4673-0295-1978-1-4673-0293-7978-1-4673-0294","10.1109/BMSB.2012.6264238","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6264238","1.1 Mobile TV;1.5 Field trials and test results;3.1 Performance evaluation;5.1 Channel modeling and simulation","Digital video broadcasting;Receivers;OFDM;Decoding;Mobile communication;Quadrature amplitude modulation;Channel estimation","digital video broadcasting;Global Positioning System;mobile television;radio receivers;software radio","software-based mobile DVB-T2 measurement receiver;second generation DVB standard;digital terrestrial broadcasting;stationary reception;DVB-T2 standard;mobile reception;mobile measurement receivers;transport stream;transmission channel characteristics;GPS data;geographical position","","11","21","","","","","","IEEE","IEEE Conferences"
"Optimized Zero False Positives Perceptron Training for Malware Detection","D. Gavrilut; R. Benchea; C. Vatamanu","NA; NA; NA","2012 14th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing","","2012","","","247","253","The increasing number of malware in the past 4 years has determined researchers to test different machine learning techniques to automate the detection system. But because of the large size of the dataset and the need of having a high detection rate, the resulted models have often produced many false positives. This paper proposes a modified version of the perceptron algorithm able to detect malware samples while training at a low rate (even zero) of false positives. A very low number of false positives is crucial because in a real life situation detecting a clean file as malware can destroy the operating system or render other programs unusable. We also provide a method of optimizing the training speed for the algorithm while maintaining the same accuracy. The resulted algorithm can be used in an ensemble or voting system to increase detection and eliminate false positives.","","978-1-4673-5026","10.1109/SYNASC.2012.34","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6481037","Perceptron;one side class;reducing false positives;large dataset;data mining;distributed algorithms","Training;Malware;Databases;Classification algorithms;Machine learning algorithms;Optimization;Educational institutions","data mining;distributed algorithms;invasive software;learning (artificial intelligence);operating systems (computers)","optimized zero false positives perceptron training;malware detection;machine learning techniques;detection system automation;perceptron algorithm;operating system;clean file detection;training speed optimization;ensemble system;voting system;data mining;distributed algorithms","","12","14","","","","","","IEEE","IEEE Conferences"
"Optimized hybrid green power model for remote telecom sites","P. O. Akuon","School of Engineering, University of KwaZulu-Natal, King George V Avenue, 4041 Durban (DUR), South Africa","IEEE Power and Energy Society Conference and Exposition in Africa: Intelligent Grid Integration of Renewable Energy Resources (PowerAfrica)","","2012","","","1","5","This paper presents a standard hybrid power solution model for remote base transceiver stations (BTS) for application in remote telecommunication sites where grid connection is not present or unreliable. The load power sources are the diesel generator, solar panels, wind generator and the battery bank systems. The design, implementation and some test analysis are discussed based on a green power solution installation made in the Angolan coast (Latitude 8<sup>0</sup>15' 13"" S and Longitude 13<sup>0</sup>6' 46"" E), at the shores of South Atlantic Ocean. This model utilizes the “Hybrid Optimization Model for Electric Renewables” (HOMER) software for optimal costing evaluation procedures, while the input wind speeds and solar insolation data are adopted from NASA's long-term weather statistics without in situ measurements. The theoretical design parameters are seen to follow the practical observations over a period of one year, the limiting effects of shading and dust accumulation on the photo-voltaic panels is verified and the need to have optimum separation distance between wind turbines is confirmed and can be used to guide design engineers.","","978-1-4673-2550-9978-1-4673-2548","10.1109/PowerAfrica.2012.6498625","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6498625","Hybrid power;BTS loads","","diesel-electric generators;environmental factors;hybrid power systems;optimisation;solar cells;wind power","hybrid green power model;remote telecom sites;load power sources;diesel generator;solar panels;wind generator;battery bank systems;hybrid optimization model for electric renewables software;HOMER software;optimal costing evaluation;photovoltaic panels","","2","5","","","","","","IEEE","IEEE Conferences"
"Path-based scheduling in a hardware compiler","R. Gu; A. Forin; N. Pittman","Department of Electrical and Computer Engineering, University of Maryland at College Park, USA; Microsoft Research, Microsoft Corporation, Redmond, WA, USA; Microsoft Research, Microsoft Corporation, Redmond, WA, USA","2010 Design, Automation & Test in Europe Conference & Exhibition (DATE 2010)","","2010","","","1317","1320","Hardware acceleration uses hardware to perform some software functions faster than it is possible on a processor. This paper proposes to optimize hardware acceleration using path-based scheduling algorithms derived from dataflow static scheduling, and from control-flow state machines. These techniques are applied to the MIPS-to-Verilog (M2V) compiler, which translates blocks of MIPS machine code into a hardware design represented in Verilog for reconfigurable platforms. The simulation results demonstrate a factor of 22 in performance improvement for simple self-looped basic blocks over the base compiler.","1558-1101;1530-1591;1530-1591","978-3-9810801-6-2978-1-4244-7054-9978-3-9810801-6","10.1109/DATE.2010.5457011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5457011","compiler;MIPS;Verilog;FPGA;schedulling","Acceleration;Hardware design languages;Processor scheduling;Optimizing compilers;Educational institutions;Software performance;Field programmable gate arrays;High level languages;Data analysis;Software tools","field programmable gate arrays;hardware description languages;reconfigurable architectures;reduced instruction set computing","path-based scheduling;hardware compiler;hardware acceleration;software function;dataflow static scheduling;control-flow state machine;MIPS-to-Verilog compiler;M2V compiler;MIPS machine code;hardware design;reconfigurable platform;self-looped basic block;FPGA","","","20","","","","","","IEEE","IEEE Conferences"
"Assignments acceptance strategy in a Modified PSO Algorithm to elevate local optima in solving class scheduling problems","M. A. A. Aziz; M. N. Taib; N. M. Hussin","Department of System Engineering, Faculty of Electrical Engineering, Universiti Teknologi MARA, 40450 Shah Alam, Selangor, Malaysia; Department of System Engineering, Faculty of Electrical Engineering, Universiti Teknologi MARA, 40450 Shah Alam, Selangor, Malaysia; Universiti Teknologi MARA Perlis, Arau Campus, 02600 Arau, Perlis, Malaysia","2010 6th International Colloquium on Signal Processing & its Applications","","2010","","","1","5","Local optima in optimization problems describes a state where no small modification of the current best solution will produce a solution that is better. This situation will make the optimization algorithm unable to find a way to global optimum and finally the quality of the generated solution is not as expected. This paper proposes an assignment acceptance strategy in a Modified PSO Algorithm to elevate local optima in solving class scheduling problems. The assignments which reduce the value of objective function will be totally accepted and the assignment which increases or maintains the value of objective function will be accepted based on acceptance probability. Five combinations of acceptance probabilities for both types of assignments were tested in order to see their effect in helping particles moving out from local optima and also their effect towards the final penalty of the solution. The performance of the proposed technique was measured based on percentage penalty reduction (%PR). Five sets of data from International Timetabling Competition were used in the experiment. The experimental results shows that the acceptance probability of 1 for neutral assignment and 0 for negative assignments managed to produce the highest percentage of penalty reduction. This combination of acceptance probability was able to elevate the particle stuck at the local optima which is one of the unwanted situations in solving optimization problems.","","978-1-4244-7122-5978-1-4244-7121-8978-1-4244-7120","10.1109/CSPA.2010.5545252","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5545252","Particle Swarm Optimization;Timetabling;Scheduling","Scheduling algorithm;Signal processing algorithms;Particle swarm optimization;Application software;Processor scheduling;Systems engineering and theory;Computer science;Testing;Performance analysis;Algorithm design and analysis","particle swarm optimisation;probability;scheduling","assignment acceptance strategy;modified PSO algorithm;local optima;solving class scheduling problem;acceptance probability","","","26","","","","","","IEEE","IEEE Conferences"
"Recognition of pests based on compressive sensing theory","A. Han; H. Peng; J. Li; J. Han; X. Guo","Institute of Electrical Engineering and Electronic, Technology, China Jiliang University, Hangzhou 310018, China; Institute of Electrical Engineering and Electronic, Technology, China Jiliang University, Hangzhou 310018, China; Institute of Electrical Engineering and Electronic, Technology, China Jiliang University, Hangzhou 310018, China; Institute of Electrical Engineering and Electronic, Technology, China Jiliang University, Hangzhou 310018, China; Fair Friend Institute of Electromechanics, Hangzhou Vocational and Technical College, Hangzhou 310018, China","2011 IEEE 3rd International Conference on Communication Software and Networks","","2011","","","263","266","In order to improve the performance of the existing recognition methods of pests, the limitations of these methods are analyzed in this paper. Based on the analysis, the novel recognition method of pests by using compressive sensing theory is presented in this paper. In the proposed method, a large number of representative training samples of pests are used to construct the training samples matrix, then the sparse decomposition representation of the testing samples of pests is obtained by solving the L1-norm optimization problem, which contains distinct class information and could be used for the different species of pests recognition directly. The 12 species of stored-grain pests and the 110 species of common pests are separately recognized by the proposed method. The experimental results prove that the application of compressive sensing theory in the recognition of pests is practical and feasible.","","978-1-61284-486-2978-1-61284-485-5978-1-61284-484","10.1109/ICCSN.2011.6014437","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014437","pests;recognition;compressive sensing;feature parameters;sparse decomposition;recognition precision","Training;Testing;Sparse matrices;Optimization;Approximation methods;Matching pursuit algorithms;Vectors","agriculture;feature extraction;matrix decomposition;optimisation;sparse matrices","compressive sensing theory;pests recognition;representative training samples;training samples matrix;sparse decomposition representation;Ll-norm optimization problem","","1","8","","","","","","IEEE","IEEE Conferences"
"17.5um thin Cu wire bonding for fragile low-K wafer technology","Teo Chen Kim Jude","Infineon Technologies Asia Pacific Pte. Ltd., 168 Kallang Way Singapore 349253","2010 12th Electronics Packaging Technology Conference","","2010","","","355","358","This paper describes the challenges of a 17.5um thin bare Cu wire bonding on aluminum bond pads for a fragile low-k wafer technology, on a BGA package. Previous evaluations have so far focused on 20um and 25um bare Cu wires as a suitable low cost replacement for Au wires. To improve performance, more fragile low-k wafer technology is being developed. In the past, some key technical challenges experienced are the substantial aluminum splash and potential cracks underneath the bond pads. This paper will focus on using a thin Cu wire of 17.5um diameter to address the smaller BPO of 44um, with very minimal aluminum splash, and prevent any cracks on a fragile low-k construction. As copper is a harder material than aluminum, to make a good bonding interconnect between the two metals and yet addressing these concerns, is critical. Also, the bonded ball bond should have a sufficient amount of remnant aluminum to survive reliability test. As Cu oxidation is rampant, an inert environment of a forming gas of 95N<sub>2</sub>5H<sub>2</sub> is needed to contain this aspect in order to achieve a good wire bond. To address the small BPO and with the aluminum splash which will pose the likelihood of shorting between adjacent ball bonds, a small bonded ball bond size is important. Process parameter optimization coupled with wire bonder hardware and software capabilities are crucial. Acknowledging these concerns, the selection of the capillary was given much thought. Such a capillary must achieve the small bonded ball bond desired, and in association with process optimization, minimize the aluminum splash, ensure remnant aluminum, and finally to address any potential damage to the fragile lowk construction underneath the bond pads. The prescribed approach would be to use a lower set of bond process parameters. Hence, the selected capillary design must be able to achieve such criteria. Evaluations on a wire bonder equipped with a Cu kit, using a 17.5um thin bare Cu wire together with a special low-k capillary design, and with a bonding temperature of 150 deg C, showed promising initial data. At time zero, all traditional wire bond process buy-off criteria are met. However, some lifted ball bonds were observed during the wire pull, with the readings greater than 2gf criteria. Further optimization and refinement is needed. Of course, the necessary reliability test data must follow suit. Also, cross-sectional analysis did not reveal any damage or cracks to the fragile low-k regions below the bond pads. Cu wire to aluminum bond pad interface creates an intermetallics phase that is prone to corrosion after certain stress test conditions. Some studies have shown that the addition of Pd, or Palladium, can overcome this corrosion. With this in mind, the evaluations also include using a Pd coated Cu wire of similar size. Like the bare Cu wire, time zero data showed viability for the Pd coated wire as well. In fact, initial data showed an improvement over the bare Cu wire for standard wire bond process buy-off criteria. The above evaluations have demonstrated that a 17.5um thin bare Cu as well as Pd coated thin Cu wires showed somewhat similar positive initial results on a low-k wafer. Of course there are still many technical challenges as well as reliability considerations to overcome, before the proliferation of 17.5um Cu wire on such a fragile wafer technology.","","978-1-4244-8562-8978-1-4244-8560-4978-1-4244-8561","10.1109/EPTC.2010.5702662","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5702662","","Wire;Copper;Aluminum;Bonding;Reliability;Electronics packaging","aluminium alloys;ball grid arrays;copper alloys;corrosion;integrated circuit interconnections;integrated circuit reliability;integrated circuit testing;lead bonding;oxidation;wafer bonding","wire bonding;fragile low-K wafer technology;BGA package;bonding interconnect;reliability test;oxidation;aluminum splash;ball bond size;process parameter optimization;capillary design;cross-sectional analysis;aluminum bond pad interface;intermetallics phase;stress test condition;corrosion;size 17.5 mum;temperature 150 C","","1","2","","","","","","IEEE","IEEE Conferences"
"Digital microfluidic biochips: Functional diversity, More than Moore, and cyberphysical systems","K. Chakrabarty; P. Pop; T. Ho","Duke University, Durham, NC, USA; Technical University of Denmark, Kgs. Lyngby, Denmark; National Cheng Kung University, Tainan, Taiwan","2011 Proceedings of the Ninth IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS)","","2011","","","377","377","Summary form only given. The 2010 International Technology Roadmap for Semiconductors (ITRS) predicted that bio-medical chips will soon revolutionize the healthcare market. These bio-medical chips should be able to sense and actuate, store and manipulate data, and transmit information. To realize such bio-medical chips, the integration of embedded systems and microfluidics inevitably leads to a new research dimension for More than Moore and beyond. This tutorial will introduce attendees to the emerging technology of digital microfluidics, which is poised to play a key role in the transformation of healthcare and the interplay between biochemistry and embedded systems. Advances in droplet-based “digital” microfluidics have led to the emergence of biochip devices for automating laboratory procedures in biochemistry and molecular biology. These devices enable the precise control of nanoliter-volume droplets of biochemical samples and reagents. Therefore, integrated circuit (IC) technology can be used to transport and transport “chemical payload” in the form of micro/nanofluidic droplets. As a result, non-traditional biomedical applications and markets (e.g., high-throughout DNA sequencing, portable and point-of-care clinical diagnostics, protein crystallization for drug discovery), and fundamentally new uses are opening up for ICs and systems. However, continued growth (and larger revenues resulting from technology adoption by pharmaceutical and healthcare companies) depends on advances in chip integration and design-automation tools. In particular, design-automation tools are needed to ensure that biochips are as versatile as the macro-labs that they are intended to replace. This is therefore an opportune time for the semiconductor industry and circuit/system designers to make an impact in this emerging field. This tutorial offers attendees an opportunity to bridge the semiconductor ICs/systems industry with the biomedical and pharmaceutical industries. The audience will see how a “biochip compiler” can translate protocol descriptions provided by an end user (e.g., a chemist or a nurse at a doctor's clinic) to a set of optimized and executable fluidic instructions that will run on the underlying digital microfluidic platform. Testing techniques will be described to detect faults after manufacture and during field operation. Sensor integration and close coupling between the underlying hardware and the control software in a cyberphysical framework will also be described. A number of case studies based on representative assays and laboratory procedures will be interspersed in appropriate places throughout the tutorial. Commercial devices and advanced prototypes from the major company in this market segment (Advanced Liquid Logic, Inc.) will be described, and ongoing activity on newborn screening using digital microfluidic biochips at several large hospitals in Illinois will be highlighted. The topics covered in the tutorial include the following: 1) Technology and application drivers: Motivation and background, actuation methods, electrowetting and digital microfluidics, review of micro-fabrication processes, applications to biochemistry, medicine, and laboratory procedures. 2) System-level design automation: Synthesis techniques: scheduling of fluidic operations, resource binding (mapping of operations to on-chip resources), module placement. 3) Physical-level design automation: droplet routing, defect tolerance, chip-level design, and design of pin-constrained biochips. 4) Testing and design-for-testability: Defects, fault modeling, test planning, reconfiguration techniques, sensor integration and cyberphysical system design.","","978-1-4503-0715-4978-1-4503-0715-4978-1-4503-0712","10.1145/2039370.2039430","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062311","Microfluidics;biochips;system-on-chip;design automation","Microfluidics;Medical services;Tutorials;Laboratories;Integrated circuits;Educational institutions;Biochemistry","biochemistry;bioMEMS;drops;embedded systems;health care;lab-on-a-chip;microfabrication;microfluidics;molecular biophysics;monolithic integrated circuits;nanofluidics","digital microfluidic biochips;functional diversity;cyberphysical systems;biomedical chips;embedded systems;health care;biochemistry;droplet based digital microfluidics;biochip devices;laboratory procedure automation;molecular biology;nanoliter volume droplet control;biochemical samples;biochemical reagents;integrated circuit technology;chemical payload;microfluidic droplets;nanofluidic droplets;chip integration;design automation tools;semiconductor IC;biochip compiler;optimized fluidic instructions;executable fluidic instructions;sensor integration;control software;cyberphysical framework;actuation methods;electrowetting;microfabrication processes;system level design automation;synthesis techniques;droplet routing;defect tolerance;chip level design;pin constrained biochip design","","","","","","","","","IEEE","IEEE Conferences"
"Accurate Synthesis and Design of Wideband and Inhomogeneous Inductive Waveguide Filters","P. Soto; E. Tarin; V. E. Boria; C. Vicente; J. Gil; B. Gimeno","Grupo de Aplicaciones de las Microondas (GAM), Instituto de Telecomunicaciones y Aplicaciones Multimedia (iTEAM), Universidad Politécnica de Valencia, Valencia, Spain; Grupo de Aplicaciones de las Microondas (GAM), Instituto de Telecomunicaciones y Aplicaciones Multimedia (iTEAM), Universidad Politécnica de Valencia, Valencia, Spain; Grupo de Aplicaciones de las Microondas (GAM), Instituto de Telecomunicaciones y Aplicaciones Multimedia (iTEAM), Universidad Politécnica de Valencia, Valencia, Spain; Aurora Software and Testing S.L., Universidad Politécnica de Valencia, Valencia, Spain; Aurora Software and Testing S.L., Universidad Politécnica de Valencia, Valencia, Spain; Dpto. de Física Aplicada y Electromagnetismo-Instituto de Ciencas de los Materiales, Universidad de Valencia, Burjassot, Valencia, Spain","IEEE Transactions on Microwave Theory and Techniques","","2010","58","8","2220","2230","In this paper, a new synthesis and design methodology is presented and applied for the fast and accurate design of inductive rectangular waveguide filters. By using this technique, the dimensions of both homogeneous and inhomogeneous filters can be successfully synthesized for almost any practical filter bandwidth, return loss, or filter order. This novel technique is based on a prototype with additional degrees of freedom, able to match the response of the different filter components in a wideband frequency range, and an elaborated design procedure that fully exploits this flexibility. During the design procedure, the prototype and the real structure are continuously aligned in order to have the same electromagnetic behavior and jointly evolve to obtain an equiripple response. Once the final prototype has been synthesized, excellent filter dimensions can be extracted that, in most cases, do not require further optimization. Examples will show the outstanding performance of the proposed design technique in terms of versatility, accuracy, and CPU time.","0018-9480;1557-9670","","10.1109/TMTT.2010.2052668","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5499446","Circuit synthesis;design automation;design methodology;distributed parameter circuits;inmittance inverters;losses;stopband performance;waveguide filters","Wideband;Filters;Prototypes;Electromagnetic waveguides;Rectangular waveguides;Design methodology;Design automation;Microwave devices;Gas insulated transmission lines;Bandwidth","network synthesis;rectangular waveguides;waveguide filters","wideband inductive waveguide filter;inhomogeneous inductive waveguide filter;rectangular waveguide filters;electromagnetic behavior;equiripple response;filter design;filter synthesis","","15","28","","","","","","IEEE","IEEE Journals & Magazines"
"Model-based electrical energy analysis of industrial automation systems","A. Beck; N. Jazdi","Institute of Industrial Automation and Software Engineering, University of Stuttgart; Institute of Industrial Automation and Software Engineering, University of Stuttgart","2010 IEEE International Conference on Automation, Quality and Testing, Robotics (AQTR)","","2010","2","","1","6","Energy optimization is one of the key challenges of the next decades. A first step for mitigation was made by research in the field of optimized technologies for energy consuming components, energy generation and energy transport. Despite this, energy optimization of complex systems during their operation time, regarding human-machine-interaction, has remained a challenge. This contribution proposes a model-based approach for electrical energy analysis of industrial automation systems. The approach serves as basis for decisions related to energy optimization issues. It considers the influences on the system's energy consumption, so that it allows a holistic view on energy consumption causes. Hence, a targeted selection of energy optimization measures becomes possible.","","978-1-4244-6725-9978-1-4244-6724-2978-1-4244-6723","10.1109/AQTR.2010.5520831","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5520831","","Energy consumption;Electrical products industry;Computer industry;Washing machines;Actuators;Electrical equipment industry;Prototypes;Energy measurement;Home automation;Engines","energy consumption;factory automation;man-machine systems","model-based electrical energy analysis;industrial automation systems;energy optimization;energy generation;energy transport;human-machine-interaction","","4","16","","","","","","IEEE","IEEE Conferences"
"Selection of electronic health records software: Challenges, considerations, and recommendations","T. Piliouras; P. L. Yu; H. Huang; X. Liu; V. K. A. Siddaramaiah; N. Sultana","Polytechnic Institute of New York University, Electronic Health Records Task Force, Hawthorne, NY USA 10953; Polytechnic Institute of New York University, Electronic Health Records Task Force, Hawthorne, NY USA 10953; Polytechnic Institute of New York University, Electronic Health Records Task Force, Hawthorne, NY USA 10953; Polytechnic Institute of New York University, Electronic Health Records Task Force, Hawthorne, NY USA 10953; Polytechnic Institute of New York University, Electronic Health Records Task Force, Hawthorne, NY USA 10953; New York University College of Nursing, New York, NY USA 10003","2011 IEEE Long Island Systems, Applications and Technology Conference","","2011","","","1","5","The authors are engaged in on-going research on the functionality of Electronic Health Records (EHR) software, and challenges doctors and nurse practitioners face when they migrate from paper to electronic-based record keeping. Our goal is to assist small practices in the adoption of EHR software and in the optimization of their clinical and practice management functions. In this paper, we discuss challenges, considerations, and recommendations for identifying solutions suited to this health care delivery category. We present findings on installation, training and use of EHR software, based on our hands-on testing and experiences working with leading EHR vendors. We have conducted a detailed industry analysis of over 200 vendors and their offerings.","","978-1-4244-9877-2978-1-4244-9878","10.1109/LISAT.2011.5784239","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5784239","electronic health records software;selection;implementation;assessment;compliance with ARRA (American Recovery and Reinvestment Act) requirements;user inteface design;training and system documentation;challenges and obstacles to automated medical and health care recordkeeping","Software;Training;Medical services;Computers;Security;Documentation;Maintenance engineering","medical information systems","electronic health records software;EHR;practice management functions;clinical management functions","","4","11","","","","","","IEEE","IEEE Conferences"
"The Development of High Performance Pulsed Magnets of the Prototype Facility of WHMFC","L. Li; T. Peng; H. Ding; X. Han; T. Ding; L. Qiu; Y. Lv; Y. Song; X. Duan; F. Herlach; Y. Pan","Wuhan High Magnetic Field Center, Huazhong University of Science and Technology, Hubei, China; Wuhan High Magnetic Field Center, Huazhong University of Science and Technology, Hubei, China; Wuhan High Magnetic Field Center, Huazhong University of Science and Technology, Hubei, China; Wuhan High Magnetic Field Center, Huazhong University of Science and Technology, Hubei, China; Wuhan High Magnetic Field Center, Huazhong University of Science and Technology, Hubei, China; Wuhan High Magnetic Field Center, Huazhong University of Science and Technology, Hubei, China; Wuhan High Magnetic Field Center, Huazhong University of Science and Technology, Hubei, China; Wuhan High Magnetic Field Center, Huazhong University of Science and Technology, Hubei, China; Wuhan High Magnetic Field Center, Huazhong University of Science and Technology, Hubei, China; Institute for Nanoscale Physics and Chemistry, K. U. Leuven, Leuven, Belgium; Wuhan High Magnetic Field Center, Huazhong University of Science and Technology, Hubei, China","IEEE Transactions on Applied Superconductivity","","2010","20","3","676","679","Coils for pulsed magnets in the range of 50-75 T are developed with optimized combinations of conductors and reinforcement for the prototype laboratory of the Wuhan High Magnetic Field Center (WHMFC). The basic design tool is a Pulsed Magnet Design Software package (PMDS) for calculating the capacitor discharge, conductor heating, mechanical stresses and the residual stress, including plastic deformation and winding tension. A 1 MJ capacitor bank is equipped with a thyristor switch, a mechanical switch, polarity reversal switches, a current limiting inductor and a diode crowbar. The design and the test results of the magnets, of the protection inductance and of the capacitor bank are presented and discussed in this paper.","1051-8223;1558-2515;2378-7074","","10.1109/TASC.2010.2042585","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5439742","Coil;multistage;pulsed magnet;stress","Magnets;Prototypes;Switches;Capacitors;Software prototyping;Conductors;Coils;Laboratories;Magnetic fields;Software design","capacitors;current limiters;electronic engineering computing;internal stresses;plastic deformation;software packages;superconducting coils;superconducting magnets;switches;thyristors;windings","coils;conductors;pulsed magnet design software package;capacitor discharge;conductor heating;mechanical stresses;residual stress;plastic deformation;winding tension;thyristor switch;capacitor bank;polarity reversal switches;mechanical switch;current limiting inductor;diode crowbar;energy 1 MJ;magnetic flux density 50 T to 75 T","","5","7","","","","","","IEEE","IEEE Journals & Magazines"
"A new mechanism of selecting representative data samples for Parzen windows method","Jianhong Ni; J. Wang; X. Li","Modern Education Technology Center, Hebei Institute of Physical Education, Shijiazhuang, 050041, China; Modern Education Technology Center, Hebei Institute of Physical Education, Shijiazhuang, 050041, China; Department of Foreign Languages, Hebei Institute of Physical Education, Shijiazhuang 050041, China","2012 IEEE International Conference on Computer Science and Automation Engineering","","2012","","","177","180","Based on the experimental observations and theoretical analysis, we validate that the significant increase of data samples may not bring about the obvious improvement of estimation performance of Parzen windows method. Thus, in this paper, we discuss a new mechanism of selecting representative data samples for Parzen windows method. An importance degree function is defined to evaluate the importance of data sample. Then, a decision threshold is optimized based on particle swarm optimization (PSO) algorithm. The data samples whose importance degrees are larger than the optimized decision threshold will be selected as the representations to estimate the underlying probability density function (PDF). Finally, the experimental results on the designed datasets obeying Uniform, Normal, Exponential, and Rayleigh distributions show that the estimation of PDF by using the representative data samples can obtain the same estimation errors (the two-tailed t-test with 95% confidence level) compared with the estimation on whole dataset. Meanwhile, the computational complexity of using representative data samples to estimate PDF is decreased evidently.","2327-0586;2327-0594","978-1-4673-2008-5978-1-4673-2007-8978-1-4673-2006","10.1109/ICSESS.2012.6269434","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6269434","Decision threshold;importance degree function;particle swarm optimization;Parzen windows method;probability density function","Estimation;MATLAB;Delta modulation","data analysis;normal distribution;particle swarm optimisation;probability","Parzen windows method;importance degree function;particle swarm optimization algorithm;PSO algorithm;optimized decision threshold;probability density function;PDF;computational complexity;representative data samples;Rayleigh distributions;exponential distributions;normal distributions;uniform distribution","","","16","","","","","","IEEE","IEEE Conferences"
"Testing the Computational Capabilities of Mobile Device Processors: Some Interesting Benchmark Results","N. V. Uti; R. Fox","NA; NA","2010 IEEE/ACIS 9th International Conference on Computer and Information Science","","2010","","","477","481","Application development for mobile devices has become big business. Between the increasing computational capabilities of these devices, their ubiquity, and the availability of dozens of development environments, new applications for these devices are being produced daily by large companies and independent software entrepreneurs. As a result, many researchers are interested in the computational strength and potential of mobile devices. While benchmark tests for desktop and laptop processors are readily available, there are very few benchmarks available for the processors used in mobile devices. Rather, the data for mobile device capabilities are primarily limited to the usability of the devices, tests regarding overall features, and battery capacity. However, one of the major challenges of the on-going trend of researchers migrating computationally intensive algorithms originally designed for desktops to mobile devices is the lack of fundamental understanding of the computational capabilities of the processors of mobile devices. In order to address this significant gap in the research and to provide programmers with a better understanding of the computational capabilities and limitations of these devices, this research devised and ran a series of benchmark tests on several popular mobile phones. This paper describes the nature of the benchmark tests developed, presents the results, and highlights some of the interesting limitations which were identified in the process.","","978-1-4244-8198","10.1109/ICIS.2010.116","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5590492","mobile computing capability;testing mobile prosessors;mobile processor benchmarks","Mobile handsets;Program processors;Benchmark testing;Arrays;Optimization;Mobile communication;Performance evaluation","microprocessor chips;mobile handsets","computational capabilities;mobile device processors;interesting benchmark results;mobile phones","","3","11","","","","","","IEEE","IEEE Conferences"
"Research and Development of a Cross Flow Swirl Solid-Liquid Separate Equipment with High Efficiency","S. Sun; X. Liang; K. Cui; J. Qian","NA; NA; NA; NA","2010 International Conference on E-Product E-Service and E-Entertainment","","2010","","","1","4","A new cross flow solid-liquid separate equipment with high efficiency has been proposed, and comparative experiment investigations between the municipal sewage plant and the test device of the separator and the numerical simulation of the device flow field using FLUENT CFD software have been combined to define its basic operating parameters and precipitation characteristics, and has laid the foundation of further optimization design of the device, serial technology standard setting and equipment development.","","978-1-4244-7161-4978-1-4244-7159-1978-1-4244-7160","10.1109/ICEEE.2010.5660363","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5660363","","Particle separators;Computational modeling;Numerical models;Testing;Solid modeling;Software;Numerical simulation","computational fluid dynamics;flow separation;flow simulation;mechanical engineering computing;numerical analysis;production equipment;research and development;separation;two-phase flow;vortices;wastewater treatment","cross flow swirl solid-liquid separate equipment;municipal sewage plant;numerical simulation;device flow field;FLUENT CFD software;optimization design","","","12","","","","","","IEEE","IEEE Conferences"
"Optimizing Network Measurements through Self-adaptive Sampling","J. M. C. Silva; S. R. Lima","NA; NA","2012 IEEE 14th International Conference on High Performance Computing and Communication & 2012 IEEE 9th International Conference on Embedded Software and Systems","","2012","","","794","801","Traffic sampling techniques are crucial and extensively used to assist network management tasks. Nevertheless, combining accurate network parameters' estimation and flexible lightweight measurements is an open challenge. In this context, this paper proposes a self-adaptive sampling technique, based on linear prediction, which allows to reduce significantly the measurement overhead, while assuring that sampled traffic reflects the statistical characteristics of the global traffic under analysis. The technique is multiadaptive as several parameters are considered in the dynamic configuration of the traffic selection process. The devised test scenarios aim at exploring the proposed sampling technique ability to join accurate network estimates to reduced overhead, using throughput as reference parameter. The evaluation results, obtained resorting to real traffic traces representing wired and wireless aggregated traffic scenarios and actual network services, prove that the simplicity, flexibility and self-adaptability of this technique can be successfully explored to improve network measurements efficiency over distinct traffic conditions. For optimization purposes, this paper also includes a study of the impact of varying the order of prediction, i.e., of considering different degrees of past memory in the self-adaptive estimation mechanism. The significance of the obtained results is demonstrated through statistical benchmarking.","","978-1-4673-2164-8978-0-7695-4749","10.1109/HPCC.2012.112","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6332250","","Throughput;Estimation;Time measurement;Correlation;Accuracy;Prediction algorithms","computer network management;optimisation;parameter estimation;sampling methods;statistical analysis;telecommunication traffic","network measurement optimization;traffic sampling techniques;network management tasks;network parameter estimation;self-adaptive sampling technique;linear prediction;measurement overhead reduction;dynamic configuration;traffic selection process;network throughput;reference parameter;global traffic trace resorting;wired aggregated traffic scenarios;wireless aggregated traffic scenarios;network services;self-adaptive estimation mechanism;statistical benchmarking","","","18","","","","","","IEEE","IEEE Conferences"
"Module level thermal performance characterization and enhancement of chip stack and package stack memory devices","S. H. Ore; W. H. Zhu; W. L. Yuan; N. Suthiwongsunthorn","United Test and Assembly Center Ltd, 5 Serangoon North Ave 5 Singapore 554916; Tian Shui Hua Tian Technology Co.Ltd, No.14, Shuangqiao Road, Gansu, China; United Test and Assembly Center Ltd, 5 Serangoon North Ave 5 Singapore 554916; United Test and Assembly Center Ltd, 5 Serangoon North Ave 5 Singapore 554916","2010 12th Electronics Packaging Technology Conference","","2010","","","626","631","Technological advances driven by the DRAM market demands resulted in thermal challenges arising from increasing power and decreasing space for cooling. This is exacerbated by the packaging of multiple devices within the same footprint based on die stacking or package stacking. In view of the thermal concerns, we conducted a comprehensive thermal study of chip stack and package stack devices at both the component and module level using 3D Computational Fluid Dynamics (CFD) software FLOTHERM. The component level studies are performed under JEDEC standard conditions (JESD 51-2 and JESD 51-6). The simulation results were used to obtain thermal resistance matrixes and the Linear Superposition (LSP) principle was applied to estimate the temperature rise and thermal cross talk for several power split configurations. It was found that under the investigated conditions, there is an optimum power split configuration that will result in the smallest junction temperatures even when the same total package power is applied and the value of this power is influenced by the package structure. As the key application of DRAM chip stack and package stack is for high density module, studies were performed for both chip stack and package stack devices on high density memory modules under server application environments. Under the module level environment, when the top die is active with DDR3 power application, both the dual chip stack and package stack are able to achieve maximum junction temperatures less than 85°C when wind speed of 4m/s is applied, without the need for external heat spreader. To optimize the thermal performance of the investigated devices, the effects of various design variations were also studied. The addition of solder balls under the die area was found to improve the thermal performance of the investigated dual stack packages. The impact of increased stacking was evaluated where the thermal performance of quad stack devices were compared to the dual stack devices. The quad stack and dual stack devices exhibited similar thermal behaviors, where the chip stack devices showed minor temperature differences within the chip stack, while the temperature differences between the different dies are larger in the package stack devices. Valuable insights on the package structure, design and thermal performance at both component and module level thus obtained are presented.","","978-1-4244-8562-8978-1-4244-8560-4978-1-4244-8561","10.1109/EPTC.2010.5702714","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5702714","","Electronic packaging thermal management;Stacking;Junctions;Thermal resistance;Performance evaluation;Thermal conductivity","chip scale packaging;circuit analysis computing;computational fluid dynamics;cooling;crosstalk;DRAM chips;semiconductor device packaging;thermal management (packaging);thermal resistance","module level thermal performance characterization;chip stack;package stack memory device;DRAM market;cooling;multiple device packaging;package stacking;3D computational fluid dynamics software FLOTHERM;CFD software;JEDEC standard condition;thermal resistance matrix;linear superposition principle;LSP;thermal cross talk;power split configuration;DDR3 power application;quad stack device;dual stack device","","2","9","","","","","","IEEE","IEEE Conferences"
"Parametrized hardware architectures for the Lucas primality test","A. Le Masle; W. Luk; C. A. Moritz","Department of Computing, Imperial College London, UK; Department of Computing, Imperial College London, UK; BlueRISC, Inc, Amherst, MA, USA","2011 International Conference on Embedded Computer Systems: Architectures, Modeling and Simulation","","2011","","","124","131","We present our parametric hardware architecture of the NIST approved Lucas probabilistic primality test. To our knowledge, our work is the first hardware architecture for the Lucas test. Our main contributions are a hardware architecture for calculating the Jacobi symbol based on the binary Jacobi algorithm, a pipelined modular add-shift module for calculating the Lucas sequences, methods for dependence analysis and for scheduling of the Lucas sequences computation. Our architecture implemented on a Virtex-5 FPGA is 30% slower but 3 times more energy efficient than the software version running on a Intel Xeon W3505. Our fastest 45 nm ASIC implementation is 3.6 times faster and 400 times more energy efficient than the optimised software implementation in comparable technology. The performance scaling of our architecture is much better than linear in area. Different speed/area/energy trade-offs are available through parametrization. The cell count and the power consumption of our ASIC implementations make them suitable for integration into an embedded system whereas our FPGA implementation would more likely benefit server applications.","","978-1-4577-0801-5978-1-4577-0802","10.1109/SAMOS.2011.6045453","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6045453","","Pipelines;Jacobian matrices;Computer architecture;Hardware;Calculators;Adders;Field programmable gate arrays","application specific integrated circuits;embedded systems;field programmable gate arrays;integrated circuit testing","Lucas probabilistic primality test;parametric hardware architecture;American National Institute of Standards and Technology;Jacobi symbol;binary Jacobi algorithm;pipelined modular add-shift module;dependence analysis;Lucas sequences computation;Virtex-5 FPGA;software version running;Intel Xeon W3505;ASIC implementation;speed-area-energy trade-offs;cell count;power consumption;embedded system;server applications;size 45 nm","","2","13","","","","","","IEEE","IEEE Conferences"
"Optimal location and sizing of distributed generation based on gentic algorithm","A. Helal; M. Amer; H. Eldosouki","Electrical and Control Engineering Dept., Arab Academy for Sciences &amp; Technology and Maritime Transport, Alexandria, Egypt; LSIS, Aix-Marseille University, Marseille, France; Electrical and Control Engineering Dept., Arab Academy for Sciences &amp; Technology and Maritime Transport, Alexandria, Egypt","CCCA12","","2012","","","1","6","This paper presents a methodology for optimal distributed generation (DG) location and sizing in distribution systems. The main objective of the added DG units is minimizing the total electrical network losses with acceptable voltage profile. Genetic Algorithm (GA) Technique is used as the optimization searching algorithm due to its advantages over the other optimization techniques in this application. The system losses and voltage profile evaluation is based on a power flow analysis for the distribution network with the representation of the distributed generators using MATPOWER software package. Cost Benefit Factor (CBF) is used to evaluate the benefits of the added DG units to the system performance. This factor combined the cost of adding new DG unit with the saving gained from total power losses reduction and reserved power generation. The optimization algorithm is applied to two different test distribution systems; 13-Bus radial system and actual 66 kV distribution network of Alexandria, EGYPT. The results indicated that if the DG units are placed at their optimal location and have optimal sizing, the total distribution system losses will be reduced.","","978-1-4673-4695-5978-1-4673-4694-8978-1-4673-4693","10.1109/CCCA.2012.6417905","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6417905","Distributed Generation;Genetic Algorithm;optimization;Losses;CBF","Genetic algorithms;Distributed power generation;Optimization;Power systems;Generators;Sociology;Statistics","cost-benefit analysis;genetic algorithms;power engineering computing;power generation economics;software packages","distributed generation location;distributed generation sizing;genetic algorithm;electrical network loss;GA technique;optimization searching algorithm;voltage profile evaluation;power flow analysis;MATPOWER software package;cost benefit factor;Alexandria;Egypt;voltage 66 kV","","2","17","","","","","","IEEE","IEEE Conferences"
"Verification of CGRA Executable Code and Debugging of Memory Dependence Violation","H. Shim; M. Ahn; J. Jung; Y. Han; S. Ryu","NA; NA; NA; NA; NA","2012 13th International Workshop on Microprocessor Test and Verification (MTV)","","2012","","","65","69","We present verification and debugging of highly optimized executable code that is generated from C source code to run on CGRA (Coarse-Grained Reconfigurable Array). To generate the executable code, the CGRA compiler uses software pipelining technique that maps instructions in a loop body to multiple FUs (functional units) of CGRA for concurrent execution. Often, the programmer chooses to use aggressive optimization as a way to obtain highly performing executable code. For example, the programmer may turn off memory dependence check in order to suppress false dependence that would otherwise result in overly conservative, therefore poorly performing, executable code. A trouble is that it is not easy to verify correctness of the resulting executable code. In this paper, we propose a method to verify CGRA executable code and to detect memory dependence violation if there occurs such violation and to provide source code position where the violation occurs. We use the behavior of VLIW code as a reference and compare it with the behavior of CGRA code. In order to guide the comparison, compiler-generated mapping table information is used.","1550-4093;2332-5674","978-0-7695-4877-7978-1-4673-4441","10.1109/MTV.2012.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6519737","Verification;Debugging;CGRA","","C language;concurrency control;formal verification;pipeline processing;reconfigurable architectures;source coding","CGRA executable code verification;memory dependence violation debugging;highly optimized executable code;C source code;coarse-grained reconfigurable array;CGRA compiler;software pipelining technique;loop body;FUs;functional units;concurrent execution;memory dependence check;executable code;source code position;VLIW code;compiler-generated mapping table information","","1","5","","","","","","IEEE","IEEE Conferences"
"Efficient bytecode optimizations for a multicore Java co-processor system","J. Tyystjärvi; T. Säntti; J. Plosila","Turku Centre for Computer Science, Finland; Dept. of Information Technology, University of Turku, Finland; Dept. of Information Technology, University of Turku, Finland","2010 12th Biennial Baltic Electronics Conference","","2010","","","173","176","As the bytecode produced by the Java compiler is unoptimized, the bytecode generated from certain types of idiomatic Java code is inefficient for execution in an interpreter. This effect is amplified in a co-processor system, in which a single processor must process heap accesses and virtual method calls from multiple threads. Two types of optimizations are presented which can be performed directly on bytecode during class loading and which do not require a large amount of processing time. These optimizations are shown to improve performance greatly, up to 29 % in the Embedded Caffeinemark Benchmark suite. Even higher improvements were measured in multithreaded programs.","2382-820X;1736-3705;1736-3705","978-1-4244-7356-4978-1-4244-7358","10.1109/BEC.2010.5631144","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5631144","","Optimization;Java;Virtual machining;Software;Benchmark testing;Hardware;Multicore processing","coprocessors;Java;multiprocessing systems;multi-threading;program compilers;program interpreters","bytecode optimizations;multicore Java coprocessor system;Java compiler;idiomatic Java code;interpreter;virtual method calls;class loading;embedded caffeinemark benchmark suite;multithreaded programs","","4","16","","","","","","IEEE","IEEE Conferences"
"Towards a Formal Specification Framework for Manufacturing Execution Systems","M. Witsch; B. Vogel-Heuser","Institute of Automation and Information Systems, Technische Universität München, Garching bei, Germany; Institute of Automation and Information Systems, Technische Universität München, Garching bei, Germany","IEEE Transactions on Industrial Informatics","","2012","8","2","311","320","Manufacturing Execution Systems (MES) optimize production and business processes at the same time. However, the engineering and specification of MES is a challenging, interdisciplinary process. Especially IT and production experts with different views and background have to cooperate. For successful and efficient MES software projects, misunderstandings in the specification process have to be avoided. Therefore, textual specifications need to be complemented by unambiguous graphical models, reducing the complexity by integrating interdisciplinary views and domain specific terms based on different background knowledge. Today's modeling notations focus on the detailed modeling of a certain domain specific problem area. They do not support interdisciplinary discussion adequately. To bridge this gap a novel MES Modeling Language (MES-ML) integrating all necessary views important for MES and pointing out their interdependencies has been developed. Due to its formal basis, comparable and consistent MES-models can be created for specification, standardization, testing, and documentation of MES software. In this paper, the authors present the formal basis of the modeling language and its core notation. The application of MES-ML is demonstrated taking a yogurt production as an example. Finally, the authors give some evaluation results that underline the effectiveness and efficiency of this new modeling approach with reference to four applications in industrial MES-projects in the domain of discrete and hybrid manufacturing.","1551-3203;1941-0050","","10.1109/TII.2012.2186585","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6145656","Business process model and notation (BPMN);formal definition;graphical modeling notation;manufacturing execution systems (MES)","Production;Unified modeling language;Business;Data models;Automation;Manufacturing;Software","formal specification;manufacturing systems;production engineering computing;program testing;simulation languages","formal specification framework;manufacturing execution system;production process;business process;MES specification;MES software projects;textual specification;unambiguous graphical model;domain specific terms;MES modeling language;MES-ML;MES software documentation;MES software testing;yogurt production","","25","44","","","","","","IEEE","IEEE Journals & Magazines"
"CoNNeCT's approach for the development of three Software Defined Radios for space application","S. K. Johnson; R. C. Reinhart; T. J. Kacpura","NASA Glenn Research Center, MS 54-1, 21000 Brookpark Road, Cleveland, OH 44135; NASA Glenn Research Center, MS 54-1, 21000 Brookpark Road, Cleveland, OH 44135; NASA Glenn Research Center, MS 54-4, 21000 Brookpark Road, Cleveland, OH 44135","2012 IEEE Aerospace Conference","","2012","","","1","13","National Aeronautics and Space Administration (NASA) is developing an on-orbit, adaptable, Software Defined Radios (SDR)/Space Telecommunications Radio System (STRS)-based testbed facility to conduct a suite of experiments to advance technologies, reduce risk, and enable future mission capabilities. The flight system, referred to as the “SCAN Testbed” will be launched on an HTV-3 no earlier than May of 2012 and will operate on an external pallet on the truss of the International Space Station (ISS) for up to five years. The Communications, Navigation, and Networking reConfigurable Testbed (CoNNeCT) Project, developing the SCAN Testbed, will provide NASA, industry, other Government agencies, and academic partners the opportunity to develop and field communications, navigation, and networking applications in the laboratory and space environment based on reconfigurable, software defined radio platforms and the Space Telecommunications Radio System (STRS) Architecture. Three flight qualified SDRs platforms were developed, each with verified waveforms that are compatible with NASA's Tracking and Data Relay Satellite System (TDRSS). The waveforms and the Operating Environment are compliant with NASA's software defined radio standard architecture, STRS. Each of the three flight model (FM) SDRs has a corresponding breadboard and engineering model (EM) with lower fidelity than the corresponding flight unit. Procuring, developing, and testing SDRs differs from the traditional hardware-based radio approach. Methods to develop hardware platforms need to be tailored to accommodate a “software” application that provides functions traditionally performed in hardware. To accommodate upgrades, the platform must be specified with assumptions for broader application but still be testable and not exceed Size, Weight, and Power (SWaP) expectations. Ideally, the applications (waveforms) operating on the platform should be specified separately to accommodate portability to other platforms and support multiple entities developing the platform from the application. To support future flight upgrades to the flight SDRs, development and verification platforms are necessary in addition to the flight system. This paper provides details on the approach used to procure and develop the SDR systems for CoNNeCT and provide suggestions for similar developments. Unique development approaches for each SDR were used which provides a rare opportunity to compare approaches and provide recommendations for future space missions considering the use of an SDR. Three case studies were examined. In two cases, the SDR vendor (General Dynamics and Harris) was the integrated platform and waveform provider. In these cases, the platform and waveform requirements were considered together by the vendor using high level analysis to support the division of the requirements. In the Harris SDR case, the platform and waveform specification was then integrated into a single document. This case study was for a first generation platform, which offers significant processing and reconfigurablility, but is not optimized for SWaP. This provides a test bed platform for many investigations of future capabilities, but requires additional SWaP than optimized flight radios. In the GD case, the specifications were provided separately. The GD SDR leverages existing platforms with minor changes to the Radio Frequency (RF) portions. The most significant change to the CoNNeCT GD SDR from previous platforms was the addition of a reconfigurable processor. The capability tests the next generation SDR, but offers limited capacity and reconfigurability. In the case of the JPL SDR, the platform was developed by JPL and Cincinnati Electronics. Goddard Space Flight Center (GSFC) provided a waveform that was developed on a ground-based development platform, and Glenn Research Center (GRC) ported the waveform to the flight platform and performed the integrated test and acceptance of the subsystem. This last case also leverages an existing platform development, and offers more capacity for reconfigurability than the second case.","1095-323X;1095-323X","978-1-4577-0557-1978-1-4577-0556-4978-1-4577-0555","10.1109/AERO.2012.6187147","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6187147","","Field programmable gate arrays;Radio frequency;Computers;NASA;Aerospace electronics;Hardware;Modems","satellite navigation;software radio;space communication links","CoNNeCT approach;space telecommunications radio system;National Aeronautics and Space Administration;STRS-based testbed facility;flight system;SCAN testbed;HTV-3;International Space Station;ISS;communications-navigation-networking reconfigurable testbed project;field communications;STRS architecture;NASA tracking-data relay satellite system;SDR platforms;NASA software defined radio standard architecture;flight model SDR system;breadboard-engineering model;hardware-based radio approach;high level analysis;waveform specification;first generation platform;reconfigurable processor;Goddard Space Flight Center;GSFC;ground-based development platform;Glenn Research Center","","4","7","","","","","","IEEE","IEEE Conferences"
"Genetic Algorithms with Random Coordinates for Route Planning on a 3D Terrain","O. Ozdikis","NA","2011 Fifth International Conference on Genetic and Evolutionary Computing","","2011","","","146","149","In this paper, an approach using genetic algorithms (GA) is proposed to find a feasible and optimum route on a 3D Geographical Information System (GIS) platform. The approach is based on generating random coordinates on the region instead of using discrete grids. Specialized crossover and mutation operators are defined accordingly. The feasibility criterion is to prevent the route from passing through the water. The two criteria to be optimized are the distance of the route from starting point to the destination point and the amount of climb ups/downs through the route. Our method is tested on NASA's World Wind application, which is an open source 3D virtual globe developed in Java.","","978-1-4577-0817-6978-0-7695-4449","10.1109/ICGEC.2011.43","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6042738","Route Planning;Multi-criteria Optimization;Constraint Handling;3D GIS Environment","Biological cells;Genetic algorithms;Three dimensional displays;Planning;Path planning;Geographic Information Systems;Optimization","genetic algorithms;geographic information systems;grid computing;Java;public domain software;terrain mapping;virtual reality","genetic algorithms;route planning;3D terrain;geographical information system;GIS;discrete grids;NASA;World Wind application;open source 3D virtual globe;Java","","","11","","","","","","IEEE","IEEE Conferences"
"Greedy feature selection for ranking","H. Lai; Y. Tang; H. Luo; Y. Pan","School of Information Science and Technology, Sun Yat-sen University, Guangzhou, China 510006; School of Computer Science, South China Normal University, Guangzhou, China 510631; School of Information Science and Technology, Sun Yat-sen University, Guangzhou, China 510006; School of Software, Sun Yat-sen University, Guangzhou, China 510006","Proceedings of the 2011 15th International Conference on Computer Supported Cooperative Work in Design (CSCWD)","","2011","","","42","46","This paper is concerned with a study on the feature selection for ranking. Learning to rank is a useful tool for collaborative filtering and many other collaborative systems, which many algorithms have been proposed for dealing this issue. But feature selection methods receive little attention, despite of their importance in collaborative filtering problems: First, recommender systems always have massive data. Using all these data in learning to rank is unrealistic and impossible. Second, we discuss that not all the features are useful for a user's query. So choosing the most relevant data is necessary and useful. To amend this problem, we describe an algorithm called FBPCRank to choose the most relevant features for ranking. Our method combines two measures of good subsets of features, which not only can decrease the loss objective, but also reduce total similarity scores between any two features. We adopt forward and backward methods to choose the most relative features and use Pearson correlation coefficient to measure the similarity of two features. The experiments indicate that our method can outperform other state-of-the-art algorithms by selecting just small amounts of features.","","978-1-4577-0387-4978-1-4577-0386-7978-1-4577-0385","10.1109/CSCWD.2011.5960053","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5960053","","Correlation;Greedy algorithms;Machine learning;Loss measurement;Benchmark testing;Feature extraction;Recommender systems","greedy algorithms;recommender systems","greedy feature selection;collaborative filtering;collaborative systems;recommender systems;FBPCRank;backward methods;forward methods;Pearson correlation coefficient","","4","15","","","","","","IEEE","IEEE Conferences"
"More Investment in Conceptual Designers: Think about it!","S. Khouri; L. Bellatreche; I. Boukhari; S. Bouarar","NA; NA; NA; NA","2012 IEEE 15th International Conference on Computational Science and Engineering","","2012","","","88","93","Developing database (DB) and data warehouse (DW) applications passes through three main modeling phases imposed by the ANSI/SPARC architecture: conceptual, logical and physical. This architecture creates two different actors: (i) conceptual designers and Database Administrators (DBA). The first actor collects user requirements, chooses the relevant diagrams for designing the conceptual model (or the logical model). The DBA ensures the performance, the maintenance and the tuning of the final application. Most of the tasks performed by these two actors are complex and time consuming for a majority of companies. Recently, some academic and industry research efforts are moving towards truly zero-administration of DW by proposing tools (advisors) substituting some tasks of DBA. One of the functionalities of these tools is to propose recommendations in choosing optimization structures such as indexing, materialized views, etc. They do not guarantee robust solutions. In this paper, we propose a revolutionary economical model for DB /DW application. Instead of substituting the DBA by advisors, we propose to delegate some DBA tasks to conceptual designers like selecting optimization structures. First, we propose to connect user requirements to the conceptual model of the target application. Secondly, based on the analysis of requirements, SQL queries are identified and then used to select optimization structures. Finally, a feasibility of our approach is tested through the selection of bitmap join indexes based on user requirements.","","978-1-4673-5165-2978-0-7695-4914","10.1109/ICCSE.2012.22","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6417279","Requirements Engineering;ANSI/SPARC Ar-chitecture;Designer;Administrator;Data Warehouse;Optimization;Experiments","Optimization;Indexes;Unified modeling language;Data models;Benchmark testing;Amplitude modulation","data warehouses;formal specification;query processing;software architecture;SQL;systems analysis","conceptual designers;developing database applications;DB applications;data warehouse applications;DW applications;ANSI/SPARC architecture;conceptual architecture;logical architecture;physical architecture;database administrators;DBA;user requirements;conceptual model design;optimization structures;economical model;requirements analysis;SQL query identification;investment","","2","12","","","","","","IEEE","IEEE Conferences"
"System Identification and Application Based on Parameters Self-Adaptive SMO","Y. Zhai; L. Liu; Q. Li","NA; NA; NA","2010 International Conference on Computational Intelligence and Software Engineering","","2010","","","1","4","This paper studies the identification algorithm of parameters self adaptive SMO based on linear kernel function, and analyses its performance and advantages. For ARX model and long-term prediction model, the method is used to identify the model of main steam pressure of thermal system and dual-lane gas turbine engine of aero system. The simulation results show that the algorithm can effectively identify model parameters and has a higher accuracy, reducing the requirements of training data including quantity and quality, so that its engineering applications and implementation are easier.","","978-1-4244-5391-7978-1-4244-5392","10.1109/CISE.2010.5676952","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676952","","Support vector machines;Predictive models;Testing;Data models;Kernel;Training;Prediction algorithms","identification;optimisation;support vector machines","system identification;parameters self adaptive SMO;linear kernel function;ARX model;main steam pressure;thermal system;dual lane gas turbine engine;aero system","","","8","","","","","","IEEE","IEEE Conferences"
"A Parameters Selection Method of SVM","D. Kou; Y. Zhang; H. Zheng","NA; NA; NA","2010 International Conference on Computational Intelligence and Software Engineering","","2010","","","1","4","An improved artificial fish swarm algorithm called ASFSA is proposed. It could facilitate the selection of values for Step and Visual to meet the balance of algorithm speed and effect. A new SVM parameters selection method based on the ASFSA is described, and the kernel parameter γ and regularization parameter C can both be optimized well. The application case shows that the performance of SVM with optimized parameters is good, so the method is feasible and effective.","","978-1-4244-5391-7978-1-4244-5392","10.1109/CISE.2010.5676994","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676994","","Marine animals;Support vector machines;Kernel;Visualization;Optimization;Training;Testing","algorithm theory;support vector machines","artificial fish swarm algorithm;ASFSA;SVM parameter selection method;kernel parameter;regularization parameter;support vector machines","","","8","","","","","","IEEE","IEEE Conferences"
"New Approach for the Identification and Validation of a Nonlinear F/A-18 Model by Use of Neural Networks","N. Boely; R. M. Botez","Bombardier Aerospace, Montreal, Canada; Department of Automated Production Engineering, Laboratory of Applied Research in Active Controls, Avionics and Aeroservoelasticity, &#x00C8;cole de Technologie Sup&#x00E9;rieure, University of Quebec, Montreal, Canada","IEEE Transactions on Neural Networks","","2010","21","11","1759","1765","This paper presents a new approach for identifying and validating the F/A-18 aeroservoelastic model, based on flight flutter tests. The neural network (NN), trained with five different flight flutter cases, is validated using 11 other flight flutter test (FFT) data. A total of 16 FFT cases were obtained for all three flight regimes (subsonic, transonic, and supersonic) at Mach numbers ranging between 0.85 and 1.30 and at altitudes of between 5000 and 25 000 ft. The results obtained highlight the efficiency of the multilayer perceptron NN in model identification. Optimization of the NN requires mixing of two proprieties: the hidden layer size reduction and four-layered NN performances. This paper shows that a four-layer NN with only 16 neurons is enough to create an accurate model. The fit coefficients were higher than 92% for both the identification and the validation test data, thus demonstrating accuracy of the NN.","1045-9227;1941-0093","","10.1109/TNN.2010.2071398","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5582299","Aeroservoelasticity;aircraft validation and identification;flight flutter tests;neural network","Artificial neural networks;Neurons;Data models;Atmospheric modeling;Aircraft;MIMO;Robustness","aerospace computing;aerospace testing;elasticity;identification;Mach number;multilayer perceptrons","nonlinear F/A-18 aeroservoelastic model;neural networks;flight flutter tests;Mach numbers;multilayer perceptron NN","Aircraft;Algorithms;Artificial Intelligence;Aviation;Elasticity;Neural Networks (Computer);Neurons;Nonlinear Dynamics;Software Validation;Stress, Mechanical","22","19","","","","","","IEEE","IEEE Journals & Magazines"
"Fault-localization using dynamic slicing and change impact analysis","E. Alves; M. Gligoric; V. Jagannath; M. d'Amorim","Federal University of Pernambuco, Brazil; University of Illinois at Urbana-Champaign, USA; University of Illinois at Urbana-Champaign, USA; Federal University of Pernambuco, Brazil","2011 26th IEEE/ACM International Conference on Automated Software Engineering (ASE 2011)","","2011","","","520","523","Spectrum-based fault-localization tools, such as Tarantula, have been developed to help guide developers towards faulty statements in a system under test. These tools report statements ranked in order of suspiciousness. Unfortunately, the reported statements can often be unrelated to the error. This paper evaluates the impact of several approaches to ignoring such unrelated statements in order to improve the effectiveness of fault-localization tools.","1938-4300","978-1-4577-1639-3978-1-4577-1638","10.1109/ASE.2011.6100114","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6100114","","Inspection;Computational efficiency;Schedules;Measurement;Testing;Filtering;USA Councils","program slicing;program testing;software fault tolerance","dynamic slicing;change impact analysis;spectrum based fault localization tools;Tarantula;faulty statements","","7","9","","","","","","IEEE","IEEE Conferences"
"Comparison of Automatic DSM Generation Modules by Processing IKONOS Stereo Data of an Urban Area","S. Eckert; T. Hollands","European Commission, Institute for the Protection and Security of the Citizen, Support to External Security Unit, Ispra, Italy; Alfred Wegener Institute, Polarmeteorology/Earth Observing System, Bremerhaven, Germany","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2010","3","2","162","167","This study deals with the evaluation of four different image-processing software modules for the generation of digital surface models from very high-resolution stereo satellite data. The analysis was done in an urban area due to the growing interest in 3-D information over built-up areas. Depending on the different geometric model approaches used by the different software packages, shifts between 3.06-3.27 m between the digital surface models (DSMs) and the reference DSM were measured. The vertical RMSE of the four tested software packages range between 2.96-14.01 m. However, the visual evaluation resulted in a different ranking and does not confirm the quantitative results entirely. The results show that, depending on the building type to be extracted, the choice of software package may vary. The challenges of automatic DSM extraction in urban areas and the performance of current software package modules to address them are discussed. Potential improvements for automatic DSM extraction in urban areas are identified.","1939-1404;2151-1535","","10.1109/JSTARS.2010.2047096","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5456205","Image matching;remote sensing;software","Buildings;Software packages;Accuracy;Urban areas;Pixel;Remote sensing","geophysical image processing;geophysical techniques;image matching;remote sensing","image-processing software modules;digital surface models;high-resolution stereo satellite data;urban area;reference DSM;vertical RMSE;tested software packages;visual evaluation;automatic DSM extraction;image matching;remote sensing;automatic DSM generation modules;IKONOS stereo data","","15","12","","","","","","IEEE","IEEE Journals & Magazines"
"Performance issues of hospital system using MySQL","N. Kohli; N. K. Verma","Electrical Engineering Department, Indian Institute of Technology, Kanpur, India; Electrical Engineering Department, Indian Institute of Technology, Kanpur, India","2010 3rd International Conference on Computer Science and Information Technology","","2010","6","","497","501","To provide the better treatment in the hospitals, it is required to integrate all the hospitals of the country via internet. While visiting to hospital for treatment, a smart card with 10 digit unique patient-ID with his personal information is issued to the patient from the administrator of the hospital. As per unique patient-ID, all the doctor prescriptions, test reports, MRI, CT-scan images are stored in the database of the local server of the hospital and main centralized server of all the hospitals. Application of the smart card based on line hospital system has been developed as front end in .Net and back end in MySQL. In the development of application, all kinds of optimization have been covered at application level, database design level, memory utilization and optimization of queries for accessing the database. In this paper, some performance issues have been proposed for fast retrieval of patient data.","","978-1-4244-5540-9978-1-4244-5537-9978-1-4244-5539","10.1109/ICCSIT.2010.5564100","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5564100","Hospital;patient;smart card;MySQL","Databases","hospitals;information retrieval;Internet;medical information systems;optimisation;patient treatment;personal information systems;smart cards;software performance evaluation;SQL","hospital system;MySQL;Internet;smart card;patient-ID;personal information;doctor prescription;patient test report;MRI report;CT-scan image;hospital centralized server;.Net;patient data retrieval","","","11","","","","","","IEEE","IEEE Conferences"
"Optics optimization of 10µm resolution used for the smart phone's PCB and ITO lithography","Jinyun Zhou; Liang Lei; Qinghua Lin; Wenyan Pei","School of Physics & Optoelectronic Engineering, Guangdong University of Technology, Guangzhou 510006, China; School of Physics & Optoelectronic Engineering, Guangdong University of Technology, Guangzhou 510006, China; School of Physics & Optoelectronic Engineering, Guangdong University of Technology, Guangzhou 510006, China; School of Physics & Optoelectronic Engineering, Guangdong University of Technology, Guangzhou 510006, China","2011 International Conference on System science, Engineering design and Manufacturing informatization","","2011","1","","183","186","Aimed at 10μm resolution used for the smart phone's PCB and ITO lithography, an optical system based on 351 nm XeF excimer laser projection and scanning is designed. The optimized parameters such as maximum optical path difference, depth of focus (DOF), maximum field curvature and distortion, etc. is obtained by experiment and analysis. For the laser projection lithography system used to directly achieve patterning with higher resolutions as well as higher throughput, the projection lens for a unit-magnification is tentatively simulation-designed using ZEMAX optical design software. The optimized design results meet the demands in technical specifications. For the sake of optimizing the paraxial plane in the length of the DOF, a new aligning way is used to get the optimum figure of the developed substrates through a microscope. The results can be used to figure out the shortest path to the focal plane. By testing, the alignment precision is 2μm, adjustable range 2mm in the optical axis. This helps us obtain dependable theoretical evidence for our projection exposure system to align the substrate and the mask. All experiments demonstrate that this optimized system can satisfy the 10μm resolution lithography.","","978-1-4577-0246-4978-1-4577-0247","10.1109/ICSSEM.2011.6081178","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6081178","smart phone;Printed Circuit Board (PCB);Indium-Tin-Oxide (ITO);Lithography","Image resolution;Optical imaging;Lithography","excimer lasers;focal planes;indium compounds;integrated optoelectronics;lenses;lithography;masks;mobile handsets;optical design techniques;optical distortion;optical focusing;optical projectors;optical scanners;optical testing;physics computing;printed circuits;tin compounds","optics optimization;smart phone;ITO lithography;optical system;excimer laser projection;excimer laser scanning;optical path difference;focusing depth;maximum field curvature;laser projection lithography;projection lens;ZEMAX optical design software;paraxial plane;optical microscope;focal plane;printed circuit board;wavelength 351 nm;ITO","","","8","","","","","","IEEE","IEEE Conferences"
"A novel Bayesian network structure learning algorithm based on Maximal Information Coefficient","Y. Zhang; Q. Hu; W. Zhang; J. Liu","State Key Laboratory of Intelligent Control and Management of Complex Systems at Institute of Automation, Chinese Academy of Sciences, BeiJing 100190; International school of software, Wuhan University, Wuhan 430074; State Key Laboratory of Intelligent Control and Management of Complex Systems at Institute of Automation, Chinese Academy of Sciences, BeiJing 100190; State Key Laboratory of Software Engineering, Computer School, Wuhan University, Wuhan 430074","2012 IEEE Fifth International Conference on Advanced Computational Intelligence (ICACI)","","2012","","","862","867","Greedy Equivalent Search (GES) is an effective algorithm for Bayesian network problem, which searches in the space of graph equivalence classes. However, original GES may easily fall into local optimization trap because of empty initial structure. In this paper, An improved GES method is prosposed. It firstly makes a draft of the real network, based on Maximum Information Coefficient (MIC) and conditional independence tests. After this step, many independent relations can be found. To ensure correctness, then this draft is used to be a seed structure of original GES algorithm. Numerical experiment on four standard networks shows that NEtoGS (the number of graph structure, which is equivalent to the God Standard network) has big improvement. Also, the total of learning time are greatly reduced. Therefore, our improved method can relatively quickly determine the structure graph with highest degree of data matching.","","978-1-4673-1744-3978-1-4673-1743-6978-1-4673-1742","10.1109/ICACI.2012.6463292","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6463292","","Microwave integrated circuits;Bayesian methods;Algorithm design and analysis;Standards;Learning systems;Search problems;Complexity theory","belief networks;graph theory;learning (artificial intelligence);optimisation;search problems","Bayesian network structure learning algorithm;maximal information coefficient;greedy equivalent search;GES;graph equivalence classes;local optimization trap;MIC;conditional independence tests;NEtoGS;structure graph;data matching","","1","34","","","","","","IEEE","IEEE Conferences"
"Optimizing diagnostic verification processes","R. Shannon; J. Knecht","Naval Air Systems Command, Lakehurst, NJ; Naval Air Systems Command, Lakehurst, NJ","2010 IEEE AUTOTESTCON","","2010","","","1","5","Demonstrations of avionics system and subsystem diagnostic capability are performed before a system or subsystem is verified. This ordinarily happens during the system design and demonstration phase of a program. In the case of aircraft or ground vehicles, there are several subsystem demonstrations, followed by a single system-level event. By the time a system or subsystem is ready for a functional demonstration of its diagnostic capability, there is already significant programmatic inertia towards achieving the next programmatic or contractual milestone. There is typically not enough available manpower, time-on-system, or even funding to test every possible fault in a given system or subsystem. Indeed, testing only the ""relevant"" faults, which the system's diagnostics have been built to address, can be a hugely time-consuming effort. Due to these constraints, diagnostic demonstrations are sometimes not conducted in a scientifically robust manner. Sometimes, certain testing techniques are used in an effort to expedite testing. These techniques include: emulating hardware faults and their detection circuitry in software, selecting only those faults which are easy to test or guaranteed to work, and choosing faults which do not significantly stress the diagnostics system. This paper describes a survey of diagnostic program managers in an attempt to characterize, and suggest remedies for, the time and budget-constrained fashion in which avionics diagnostics systems are functionally demonstrated today.","1558-4550;1088-7725;1088-7725","978-1-4244-7961-0978-1-4244-7960-3978-1-4244-7959","10.1109/AUTEST.2010.5613617","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613617","Avionics diagnostics;built-in test;M-demo;verification","Circuit faults;Government;Personnel;Testing;Companies;Integrated circuit modeling","aircraft instrumentation;avionics;diagnostic expert systems","avionics diagnostic systems;diagnostic verification process;subsystem diagnostic capability;aircraft;ground vehicles;single system-level event;programmatic inertia;relevant faults;time-consuming effort;expedite testing;hardware faults;detection circuitry","","2","10","","","","","","IEEE","IEEE Conferences"
"A flexible Hardware-In-the-Loop test platform for physical resource sharing mechanisms in wireless networks","J. Luo; A. Kortke; W. Keusgen; J. Li; M. Haardt; P. Prochazka; J. Sykora","Fraunhofer Heinrich-Hertz-Institut, Einsteinufer 37, Berlin, 10587, Germany; Fraunhofer Heinrich-Hertz-Institut, Einsteinufer 37, Berlin, 10587, Germany; Fraunhofer Heinrich-Hertz-Institut, Einsteinufer 37, Berlin, 10587, Germany; Ilmenau University of Technology, Communications Research Laboratory, PO Box, 100565, Ilmenau, 98684, Germany; Ilmenau University of Technology, Communications Research Laboratory, PO Box, 100565, Ilmenau, 98684, Germany; Czech Technical University in Prague, FEE K13137, Technick&#x00E1; 2, 166 27 Praha 6, Czech Republic; Czech Technical University in Prague, FEE K13137, Technick&#x00E1; 2, 166 27 Praha 6, Czech Republic","2011 Future Network & Mobile Summit","","2011","","","1","8","Currently, a lot of research is being done to develop physical resource sharing mechanisms in wireless networks. These mechanisms are expected to enhance spectral efficiency, coverage, user satisfaction and operator revenue. In order to prove the feasibility of these mechanisms or to identify practical problems for mechanism optimization, a Hardware-in-The-Loop (HIL) test platform has been developed. This platform consists of multiple broadband wireless experimental devices and can be flexibly configured, allowing extensive signal transmission experiments. Furthermore, different transmission modes of the Radio-Frequency (RF) signals are available, allowing a variety of mechanism evaluation possibilities. Based on this platform, physical resource sharing mechanisms can be efficiently implemented and tested. First test results are shown in this paper.","","978-1-905824-25-0978-1-4577-0928-9978-1-905824-25","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6095197","Resource sharing;Hardware-In-the-Loop (HIL);testbed;demonstrator platform;verification of algorithm;proof of concept","Radio frequency;Resource management;Software;Wireless communication;Clocks;Propagation;Field programmable gate arrays","radio networks;test equipment","flexible hardware-in-the-loop test platform;physical resource sharing mechanisms;wireless networks;spectral efficiency;multiple broadband wireless experimental devices;signal transmission;radio-frequency signals","","2","9","","","","","","IEEE","IEEE Conferences"
"Meta-analysis of protein structural alignment","J. Havrilla; A. Saçan","School of Biomedical Engineering, Drexel University, Philadelphia, PA, United States of America; School of Biomedical Engineering, Drexel University, Philadelphia, PA, United States of America","2012 IEEE International Conference on Bioinformatics and Biomedicine Workshops","","2012","","","72","76","The three-dimensional structure of a protein molecule provides significant insight into its biological function. Structural alignment of proteins is an important and widely performed task in the analysis of protein structures, whereby functionally and evolutionarily important segments are identified. However, structural alignment is a computationally difficult problem and a large number of heuristics introduced to solve it do not agree on their results. Consequently, there is no widely accepted solution to the structure alignment problem. In this study, we present a meta-analysis approach to generate a re-optimized, best-of-all result using the alignments generated from several popular methods. Evaluations of the methods on a large set of benchmark pairwise alignments indicate that TM-align provides superior alignments (except for RMSD), compared to other methods we have surveyed. Smolign provides smaller cores than other methods with best RMSD values. The re-optimization of the alignments using TM-align's optimization method does not alter the relative performance of the methods. Additionally, visualization approaches to delineate the relationships of the alignment methods have been performed and their results provided.","","978-1-4673-2747-3978-1-4673-2746-6978-1-4673-2745-9978-1-4673-2744","10.1109/BIBMW.2012.6470218","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6470218","meta-analysis;protein alignment;structure comparison;benchmark test;meta-program","Proteins;Synthetic aperture sonar;Principal component analysis;Optimization;Protein engineering;Software;Clustering algorithms","molecular biophysics;molecular configurations;optimisation;proteins","protein structural alignment;three-dimensional structure;protein molecule;biological function;protein structural analysis;heuristics;meta-analysis approach;benchmark pairwise alignments;TM-align optimization method","","1","11","","","","","","IEEE","IEEE Conferences"
"A novel method for distributed generation and capacitor optimal placement considering voltage profiles","M. Wang; J. Zhong","Department of Electrical and Electronic Engineering, the University of Hong Kong, HKSAR, China; Department of Electrical and Electronic Engineering, the University of Hong Kong, HKSAR, China","2011 IEEE Power and Energy Society General Meeting","","2011","","","1","6","To ensure the quality of power supply in distribution systems, bus voltages should be maintained within limits. Shunt capacitor banks installed along distribution feeders can reduce voltage drops. Moreover, distributed generations (DGs) can improve system voltage profile as well as supply real power. The locations of DGs and capacitors play an important role in maintaining voltage profiles. In this paper, two optimization models are proposed to obtain the optimal placements of DGs and capacitor banks to maintain better voltage profiles in distribution systems. First, the optimal DG placement problem is formulated as a modified optimal power flow (OPF) problem, with an innovative mathematic representation of voltage profile optimization. Then the capacitor optimal placement problem is modeled and solved. Both models are tested on the IEEE 41 bus distribution system, which is a radial system. Discussions are provided based on the results of case studies.","1932-5517;1944-9925","978-1-4577-1002-5978-1-4577-1000-1978-1-4577-1001","10.1109/PES.2011.6039106","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6039106","Optimization;DG placement;capacitor placement;voltage profile;reactive power support","Capacitors;Indexes;Reactive power;Optimization;Substations;Mathematical model;Computer aided software engineering","distributed power generation;electric potential;optimisation;power capacitors;power supply quality","power supply quality;shunt capacitor banks;distribution feeders;DG;OPF;optimal power flow;innovative mathematic representation;voltage profile optimization;capacitor optimal placement problem;IEEE 41 bus distribution generation system;voltage drop reduction","","8","15","","","","","","IEEE","IEEE Conferences"
"Analysis of multi-domain scenarios for optimized dynamic power management strategies","J. Zimmermann; O. Bringmann; W. Rosenstiely","FZI Karlsruhe, Germany; FZI Karlsruhe, Germany; FZI Karlsruhe, Germany","2012 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2012","","","862","865","Synchronous dataflow (SDF) models are gaining increased attention in designing software-intensive embedded systems. Especially in the signal processing and multimedia domain, dataflow-oriented models of computation are commonly used by designers reflecting the regular structure of algorithms and providing an intuitive way to specify both sequential and concurrent system functionality. Furthermore, dataflow-oriented models are qualified for capturing dynamic behavior due to data-dependent execution. In this work, we extend those data-dependent dataflow models to include dynamic power management (DPM) aspects of a target platform while still meeting hard timing requirements. We capture different system states in a multi-domain scenario approach and develop a state space based on this SDF representation for system analysis and optimization. By traversing the state space of the power-aware scenario modeling we derive a power management configuration with minimized energy dissipation depending on dynamic system behavior.","1558-1101;1530-1591;1530-1591","978-1-4577-2145-8978-3-9810801-8","10.1109/DATE.2012.6176617","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6176617","","Vectors;Throughput;Switches;Computational modeling;Heuristic algorithms;Delay","data flow analysis;embedded systems;integrated circuit design;power aware computing;system-on-chip","multidomain scenario analysis approach;optimized dynamic power management strategy;synchronous data flow models;SDF models;software-intensive embedded systems;signal processing;multimedia domain;dataflow-oriented models;data-dependent execution;data-dependent data flow models;power-aware scenario modeling;energy dissipation;dynamic system behavior;concurrent system functionality;system-on-chip design","","2","22","","","","","","IEEE","IEEE Conferences"
"Self-optimization of dense wireless sensor networks based on simulated annealing","A. R. Pinto; A. Cansian; J. M. Machado; C. Montez","DCCE - UNESP - Universidade Estadual Paulista, S&#x00E3;o Jos&#x00E9; do Rio Preto, Brazil; DCCE - UNESP - Universidade Estadual Paulista, S&#x00E3;o Jos&#x00E9; do Rio Preto, Brazil; DCCE - UNESP - Universidade Estadual Paulista, S&#x00E3;o Jos&#x00E9; do Rio Preto, Brazil; UFSC - Universidade Federal de Santa Catarina, Florian&#x00F3;polis, Brazil","2012 13th Latin American Test Workshop (LATW)","","2012","","","1","6","Wireless sensor network (WSN) Is a technology that can be used to monitor and actuate on environments in a non-intrusive way. The main difference from WSN and traditional sensor networks is the low dependability of WSN nodes. In this way, WSN solutions are based on a huge number of cheap tiny nodes that can present faults in hardware, software and wireless communication. The deployment of hundreds of nodes can overcome the low dependability of individual nodes, however this strategy introduces a lot of challenges regarding network management, real-time requirements and self-optimization. In this paper we present a simulated annealing approach that self-optimize large scale WSN. Simulation results indicate that our approach can achieve self-optimization characteristics in a dynamic WSN.","2373-0862","978-1-4673-2355-0978-1-4673-2354","10.1109/LATW.2012.6261236","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6261236","Wireless Sensor Networks;Simulated Annealing;Self-Optimization","Peer to peer computing;Wireless sensor networks;Monitoring;Simulated annealing;Heating;Wireless personal area networks;Wireless communication","","","","","16","","","","","","IEEE","IEEE Conferences"
"To Strengthen Security, Change Developers' Incentives","J. A. Halderman","University of Michigan","IEEE Security & Privacy","","2010","8","2","79","82","Many of the most common software vulnerabilities, such as buffer overflows, cross-site scripting, and misapplications of cryptography, are wholly avoidable if software makers apply an appropriate level of training, testing, and care.Yet developers today have the ""wrong"" incentives, often leading them to underinvest in security or even to directly harm it. If we can understand these incentives and their causes, we might be able to reshape them and radically improve security.Software makers have shown a dramatic ability to strengthen their products' security given sufficient motivation.The most famous example is Microsoft's transformation over the past decade from a security laughingstock to a leader. In 2002, stung by several widely publicized vulnerabilities across its product line, the company began a major security initiative that produced lasting changes in its priorities, processes, and culture. Gone were the days of ""creating designs and code that emphasize features over security."" Yet changes like these are exceptional. Microsoft's shift was motivated by an intense level of scrutiny and withering global publicity that few firms experience, and it had the unusual luxury of responding with vast engineering resources paid for by monopoly rents. Most developers face far weaker security incentives.","1540-7993;1558-4046","","10.1109/MSP.2010.85","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5439535","security economics;developers' incentives;transparency;liability;security and privacy","Security;Buffer overflow;Cryptography;Software testing;Monopoly","cryptography;operating systems (computers)","strengthen security;change developers incentives;software vulnerabilities;cross site scripting;cryptography misapplications;security improvement;software makers;products security;Microsoft transformation;publicized vulnerabilities;engineering resources","","1","5","","","","","","IEEE","IEEE Journals & Magazines"
"On stochastic risk ordering of network services for proactive security management","M. Amezziane; E. Al-Shaer; M. Q. Ali","Department of Mathematical Sciences, Depaul University, Chicago, Illinois; Dept of Software and Information Systems, University of North Carolina Charlotte, Charlotte, North Carolina; Dept of Software and Information Systems, University of North Carolina Charlotte, Charlotte, North Carolina","2012 IEEE Network Operations and Management Symposium","","2012","","","994","1000","Contemporary network services don't have any statistical ranking mechanism for proactive security management. Since the emerging threats are actively exploiting the vulnerabilities in network services to compromise the system, not much attention has been paid to rank these services based on their vulnerability history. We argue in this paper that a reliable mechanism could be used to rank these services based on their vulnerability history. Such ranking will be significantly helpful for proactive network security management to partition services and deploy security countermeasures. We propose a framework using stochastic order alternatives to statistically rank network services based on time intervals between exploits as reported by National Vulnerability Database (NVD). We show that Statistical techniques can be used to rank these services by modeling the related metrics. We validated our technique using products of known ranking, and presented some case studies to confirm our result on real network services.","2374-9709;1542-1201;1542-1201","978-1-4673-0269-2978-1-4673-0267-8978-1-4673-0268","10.1109/NOMS.2012.6212020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6212020","","Stochastic processes;Security;Random variables;Software;Reliability;Testing;Data models","computer network management;computer network security;statistical analysis","stochastic risk ordering;network services;proactive network security management;vulnerability history;time intervals;National Vulnerability Database;NVD;statistical techniques;security countermeasures","","1","23","","","","","","IEEE","IEEE Conferences"
"Automobile anti-lock braking system simulation based on LabVIEW and ARM","C. Chu; X. Zhang; X. Jia; R. Zhu","Nanchang institute of technology, Department of mechanical and electrical engineering, 330099, China; Nanchang institute of technology, Department of mechanical and electrical engineering, 330099, China; Nanchang institute of technology, Department of mechanical and electrical engineering, 330099, China; Nanchang institute of technology, deanery, 330099, China","Proceedings of the 29th Chinese Control Conference","","2010","","","5372","5377","Based on establishment of full-car dynamic model, the easiest actualized logic threshold control method was adopted to develop ECU of automobile ABS system on the basis of software LabVIEW table. The results proved that the ECU developed could achieve preferable braking performance. Most importantly, hardware -in -the -loop test could be carried out on the basis of this table, which could make the development of this ECU not be limited in the software simulation, but could be applied to practical use.","1934-1768;2161-2927","978-7-8946-3104-6978-1-4244-6263-6978-7-8946-3104","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5572232","Automobile;Anti-lock Braking System;LabVIEW;ARM","Electronic mail;Hardware;Software;Optimized production technology;Automobiles;Vehicle dynamics;Electrical engineering","automotive engineering;braking;mechanical engineering computing;microprocessor chips;vehicle dynamics;virtual instrumentation","automobile anti-lock braking system;LabVIEW software;ARM microprocessors;electronic control unit;logic threshold control method;full-car dynamic model;software simulation","","","8","","","","","","IEEE","IEEE Conferences"
"Reliability Computing of Stochastic BEM Based on Grid Platform","L. Zhang; Y. Liu; J. Zhang","NA; NA; NA","2011 Fourth International Joint Conference on Computational Sciences and Optimization","","2011","","","187","189","Stochastic BEM provides a powerful tool for the reliability computing of complex or large scale structure. Though simple and applicable, AFOSM Method has shortcomings when distribution forms of random variable are concerned. In some cases, the distribution types of variables can be worked out through statistics analysis. Thus it is necessary to adopt appropriate methods to work out possible impacts of variable distribution. Stochastic BEM, based on Monte-Carlo numerical simulation, is capable to simulate such impacts effectively as a method of statistical approximation. But the coefficient matrix is asymmetric full-matrix in the solution of BEM matrix equation, the computational complexity increase by N3 with the increase of boundary node number. On solving the complex large-scale three dimensions structure problem, the large computational scale has been a barrier in the application of boundary element method, especially for the Stochastic BEM computation. Grid computing is a new style of distributed computing technology after the cluster computing. Grid is a integrated environment of computing and resource, which realize sharing resource and cooperative teamwork .It can absorb various computing resources and translate the min to convenient and reliable computer power. On the campus grid platform, the single or multiple large-scale BEM computing could carry out expediently and efficiently. This paper took the case of a campus grid in Hohai university, and introduced the characteristic of grid computing and the three-layer frame of the campus grid such as resource layer, grid layer and application service layer, and the campus grid platform is a system based on two-level scheduler, respectively grid layer scheduler and resource layer scheduler, thus a two-level parallel strategy in the above two layers can be adopted for the computing of stochastic BEM which has two parallel specialties in both BEM computing and Monte-Carlo method. Then the realization of stochastic BEM based on grid platform was put forward. Finally, there is a test which shows the high performance and efficiency of the two-level parallel strategy for the stochastic BEM computing on the grid platform.","","978-1-4244-9712-6978-0-7695-4335","10.1109/CSO.2011.218","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5957639","grid computation;hpc;stochastic bem","Reliability;Portals;Computers;Grid computing;Monte Carlo methods;Computer architecture;Teamwork","boundary-elements methods;grid computing;mathematics computing;matrix algebra;Monte Carlo methods;software reliability;statistical analysis","stochastic BEM;reliability computing;AFOSM Method;Monte Carlo numerical simulation;statistical approximation;BEM matrix equation;grid computing;cluster computing;campus grid;resource layer;grid layer;application service layer","","","5","","","","","","IEEE","IEEE Conferences"
"Test cases for unit commitment and hydrothermal scheduling problems","A. L. Diniz","CEPEL, the Brazilian Electric Power Research Center, and UERJ - State University of Rio de Janeiro, Brazil","IEEE PES General Meeting","","2010","","","1","8","Generation scheduling and other types of optimization problems for hydrothermal systems have been studied for more than 60 years. A large number of algorithms and solving strategies have already been proposed in the literature. Despite of this huge amount of publications, we still lack the existence of standardized test systems that can be used to benchmark the performance and solution quality of so many proposed techniques. In this sense, this paper proposes some test cases that can be used to unit commitment and hydrothermal scheduling problems both in regulated and deregulated markets. Electrical network data are taken from the 24, 57, 118 and 300-bus IEEE-based test systems, which were fictitiously linked to a set of up to 20 thermal units and more than 100 hydro units. Hydro data are based on the real Brazilian system and include cascaded reservoirs, water delay times, and hydro plants input-output curves. Thermal data are taken from the literature.","1932-5517;1944-9925;1944-9925","978-1-4244-6551-4978-1-4244-6549-1978-1-4244-6550-7978-1-4244-8357","10.1109/PES.2010.5589757","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5589757","Hydrothermal scheduling;Unit commitment;Test cases","Reservoirs;Computer aided software engineering;Economics;Delay;Scheduling;Upper bound","hydrothermal power systems;power generation dispatch;power generation scheduling","unit commitment;hydrothermal scheduling;standardized test systems;deregulated markets;cascaded reservoirs;water delay times;hydro plants input-output curves","","11","27","","","","","","IEEE","IEEE Conferences"
"Design and Implementation of a Library Management System Based on the Web Service","Y. Li; H. Zheng; T. Yang; Z. Liu","NA; NA; NA; NA","2012 Fourth International Conference on Multimedia Information Networking and Security","","2012","","","433","436","At present, the level of library management way in most universities is still a manual operation. For the problem of the traditional manual operation is time-consuming and inconvenience, this paper proposes a library management system is designed and implemented based on the web service. In this system, the three-layer architecture is employed, applying model building language that UML carries on needs analysis and design, using the JSP technique to build the system front interface, and using SQL Server 2005 technology to build the back-end database. In order to timely feedback of relevant information for the use of the reader, we add the Guest Book sub module to the system in implementation process. Finally, to improve the operating efficiency of the database, we use the stored procedures and triggers technology to optimize the database performance. By actual test, we correct the system bug, and further improve the system performance.","2162-8998","978-1-4673-3093-0978-0-7695-4852","10.1109/MINES.2012.94","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405716","ibrary manegement system;use case Analysis;database optimization;performance test","Libraries;Databases;Educational institutions;Computers;Unified modeling language;Servers;Software","libraries;SQL;Unified Modeling Language;Web services","library management system;Web service;three-layer architecture;UML;JSP technique;system front interface;SQL Server 2005 technology;back-end database;guest book submodule;operating efficiency improvement;system bug","","1","15","","","","","","IEEE","IEEE Conferences"
"Design method of cymbal transducer structure parameters based on finite element analysis","Peng Haijun; Zhang Kai; Du Yiqun","Depart. of Weaponry Eng., Naval Univ. of Engineering, Wuhan, China; Depart. of Weaponry Eng., Naval Univ. of Engineering, Wuhan, China; Navy Equipment and Technology Institute, Beijing, China","2010 International Conference On Computer Design and Applications","","2010","4","","V4-269","V4-273","Structure parameters of cymbal transducer play an impotent role in its performance. It is complicated to analysis the cymbal transducer with traditional methods, such as equivalent circuit method. The results of traditional methods are difficult to be used for structure optimization. Finite element method (FEM) takes structure parameters into account, and result data are more accurate and direct. Admittance, transmitting voltage response (TVR) and free field voltage sensitivity (FFVS) of cymbal transducers with different structure parameters were researched with FEM. Conclusions were drew out from results of finite element analysis (FEA) about how structure parameters affect transducer's performance. A sample transducer was developed and tested in an anechoic tank. Data from measurement agreed well with data calculated from FEA software ANSYS. Methods discussed in this article are useful to optimize cymbal transducers and develop cymbal transducer array.","","978-1-4244-7164-5978-1-4244-7164","10.1109/ICCDA.2010.5541159","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5541159","transducer;structure optimization;finite element analysis","Design methodology;Transducers;Finite element methods;Optimization methods;Voltage;Field emitter arrays;Equivalent circuits;Admittance;Performance analysis;Testing","finite element analysis;piezoelectric transducers","cymbal transducer structure parameter;finite element analysis;admittance;transmitting voltage response;free field voltage sensitivity;ANSYS;cymbal transducer array","","","12","","","","","","IEEE","IEEE Conferences"
"Ranking of technology transfer barriers in developing countries; case study of Iran's biotechnology industry","K. Yazdani; K. Y. Rashvanlouei; K. Ismail","Faculty of Management and Human Resource Development, University Technology Malaysia, Johor Bahru, Malaysia; Graduate School of Management and Economics, Sharif University of Technology, Tehran, Iran; Faculty of Management and Human Resource Development, University Technology Malaysia, Johor Bahru, Malaysia","2011 IEEE International Conference on Industrial Engineering and Engineering Management","","2011","","","1602","1606","In this paper, first we shall define a list of technology transfer barriers in the biotechnology field through interviews and literature reviews. Next, we shall categorize them into four major categories of technology, and at last introduce all of the hypotheses on the existence of relationship between each part of technology and failure in the technology transfer process. Questionnaires consisting of two parts were created and handed out to all of the specialists and biotechnology idea-holders in the Iranian National Research center of Biotechnology, Lidco Co, Iran Pastor Institute, Biotechnology Department of University of Tehran, etc. Going on, we've evaluated the factors' validity through statistical tests and the recognized barriers' priorities in each category, which were generally, identified by Analytical Hierarchy Process (AHP) via the software Expert Choice. These priorities describe the high importance of organization-ware (34.1%), information-ware (27%), technique-ware (20.7%) and eventually human-ware (18.1%) respectively.","2157-362X;2157-3611;2157-3611","978-1-4577-0739-1978-1-4577-0740-7978-1-4577-0738","10.1109/IEEM.2011.6118187","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6118187","Management of Technology;Technology Transfer;Developing Countries;Analytic Hierarchy Process (AHP)","Technology transfer;Biotechnology;Reliability;Industries;Educational institutions;Humans;Organizations","biotechnology;decision making;technology transfer","technology transfer barrier ranking;biotechnology industry;Iran;technology transfer process;analytical hierarchy process;Expert Choice software;organization-ware;information-ware;technique-ware;human-ware","","2","10","","","","","","IEEE","IEEE Conferences"
"A Unified Approach for Localizing Non-deadlock Concurrency Bugs","S. Park; R. Vuduc; M. J. Harrold","NA; NA; NA","2012 IEEE Fifth International Conference on Software Testing, Verification and Validation","","2012","","","51","60","This paper presents UNICORN, a new automated dynamic pattern-detection-based technique that finds and ranks problematic memory access patterns for non-deadlock concurrency bugs. UNICORN monitors pairs of memory accesses, combines the pairs into problematic patterns, and ranks the patterns by their suspiciousness scores. UNICORN detects significant classes of bug types, including order violations and both single-variable and multi-variable atomicity violations, which have been shown to be the most important classes of non-deadlock concurrency bugs. The paper also describes implementations of UNICORN in Java and C++, along with empirical evaluation using these implementations. The evaluation shows that UNICORN can effectively compute and rank the patterns that represent concurrency bugs, and perform computation and ranking with reasonable efficiency.","2159-4848","978-0-7695-4670-4978-1-4577-1906","10.1109/ICST.2012.85","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200096","Concurrency;Debugging;Fault-Localization;Multi-varialbe Bug;Atomicity Violation","Computer bugs;Concurrent computing;Instruction sets;Indexes;Instruments;Algorithm design and analysis;Monitoring","C++ language;concurrency control;Java;program debugging;system monitoring","nondeadlock concurrency bugs localization;UNICORN;automated dynamic pattern-detection-based technique;problematic memory access pattern ranking;memory access monitoring;order violation;single-variable atomicity violation;multi-variable atomicity violation;Java;C++","","9","25","","","","","","IEEE","IEEE Conferences"
"Does the Past Say It All? Using History to Predict Change Sets in a CMDB","S. Nadi; R. Holt; S. Mankovskii","NA; NA; NA","2010 14th European Conference on Software Maintenance and Reengineering","","2010","","","97","106","To avoid unnecessary maintenance costs in large IT systems resulting from poorly planned changes, it is essential to manage and control changes to the system and to verify that all items impacted by each change are updated as needed. This paper presents a method of decision support that helps guarantee that each change set (those items to be updated in the change) contains all the software or hardware components impacted by the proposed change. Today, many IT systems are managed by a Configuration Management Database (CMDB), which can be represented as a large graph in which the nodes are configuration items (CIs), such as software applications or servers, and the edges record dependencies between these items. In this paper we present a new approach to suggesting change sets based on our conjecture that each new change set is likely to be similar to instances of previous change sets. Accordingly, if the analyst determines that CI x is in a new change set, our method essentially searches for previous change sets, stored in the CMDB, that contain x, and suggests that CIs in those sets (appropriately ranked) should be considered for inclusion in the new change set. Our model uses support and confidence measures to estimate how closely nodes x and y are related, based on how often they have appeared together in past change sets. Based on these measures, we implement a prototype that suggests likely items to an analyst who is composing a change set. Based on a history of three years of a particular industrial CMDB, and several filtering techniques, the observed recall and precision values were as high as 69.8% and 88.5% respectively.","1534-5351","978-0-7695-4321-5978-1-61284-369","10.1109/CSMR.2010.14","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5714424","Data mining;change management;maintenance;configuration management","Software;Testing;Geographic Information Systems;History;Servers;Symmetric matrices;Prototypes","database management systems;information technology;software maintenance","maintenance costs;IT systems;configuration management database;CMDB;configuration items","","","16","","","","","","IEEE","IEEE Conferences"
"NETbalance: Reducing the Runtime of Network Emulation Using Live Migration","A. Grau; K. Herrmann; K. Rothermel","NA; NA; NA","2011 Proceedings of 20th International Conference on Computer Communications and Networks (ICCCN)","","2011","","","1","6","Network emulation is an efficient method for evaluating distributed applications and communication protocols by combining the benefits of real world experiments and network simulation. The process of network emulation involves the execution of connected instances of the software under test (called virtual nodes) in a controlled environment. In previous work, we introduced an approach to minimize the runtime of network emulation experiments based on prior known average resource requirements of virtual nodes. In this paper, we introduce NETbalance, a novel approach to runtime reduction for experiments with unknown or varying resource requirements. NETbalance migrates virtual nodes during an experiment to distribute the load evenly across the physical nodes, avoiding overloaded nodes and exploiting the idle resources on underloaded nodes for speeding up the experiment execution. We make the following contributions: First, we present an emulation architecture for efficiently supporting live migration of virtual nodes. Second, we propose a cost model for determining the runtime reduction achieved through the migration. Third, we introduce an algorithm for calculating placements that minimize the experiment runtime. Our evaluations of the NETbalance prototype show, that it is able to reduce the experiment runtime by up to 70%.","1095-2055;1095-2055","978-1-4577-0638-7978-1-4577-0637-0978-1-4577-0636","10.1109/ICCCN.2011.6005793","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6005793","","Runtime;Emulation;Network topology;Load modeling;Topology;Optimization;Computer architecture","program testing;software performance evaluation","NETbalance;network emulation testbed;distributed application;communication protocol;live migration;software under test;runtime reduction;emulation architecture;virtual node placement","","2","22","","","","","","IEEE","IEEE Conferences"
"Checking Compliance to Coding Standards for x86 Executables","Z. Dai; X. Mao; D. Wang; D. Liu; J. Zhang","NA; NA; NA; NA; NA","2010 7th International Conference on Ubiquitous Intelligence & Computing and 7th International Conference on Autonomic & Trusted Computing","","2010","","","449","455","COTS component evaluation is one of the most important steps in component-based development. Enforcing the coding standard within the coding phase is one important aspect for the quality of safety-critical software. This paper addresses the problem of carrying out COTS component evaluation of its compliance to the coding standard in case when only the executables with neither source code nor debugging information are available. The static binary analysis techniques are employed to discuss the feasibilities and possible approaches of checking compliance to coding standard rules for x86 executables. This work is our known first attempt to address this problem. About 27% of the MISRA-C 2004 rules can be checked or partially checked for x86 executables. Experiments are presented to conclude that for the rules related to unstructuredness and pointer cast, violations found from executables can be a reasonable approximation of the extent of compliance to the coding standard of the corresponding source code.","","978-1-4244-9043-1978-0-7695-4272","10.1109/UIC-ATC.2010.118","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5667114","static binary analysis;coding standard checking","Encoding;Flow graphs;Libraries;Software;Registers;Optimization;Arrays","approximation theory;conformance testing;encoding;object-oriented programming;program debugging;program diagnostics;safety-critical software","x86 executables;coding standards;compliance checking;COTS component evaluation;component based development;safety-critical software;debugging information;source code;static binary analysis techniques;MISRA-C 2004 rules;commercial off-the-shell","","","13","","","","","","IEEE","IEEE Conferences"
"Automatic transition between structural system views in a safety relevant embedded systems development process","C. Ellen; C. Etzien; M. Oertel","OFFIS e.V., Oldenburg, Germany; OFFIS e.V., Oldenburg, Germany; OFFIS e.V., Oldenburg, Germany","2012 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2012","","","820","823","It is mandatory to design safety relevant embedded systems in multiple structural system views. A typical example is the usage of a functional and technical system representation. A transition between these system views not only comprises the allocation of components but also copes with multiple design aspects and constraints that need to be transferred to the target perspective. Optimization goals regarding arbitrary design artifacts complicate this problem. In this paper we present a novel comprehensive approach integrating common allocation techniques together with a partial design generation in a system wide process to optimize complex system view transitions. We demonstrate our approach using the CESAR design methodology. The original system models and requirements are used as input for our procedure and the results are directly applied to the same models.","1558-1101;1530-1591;1530-1591","978-1-4577-2145-8978-3-9810801-8","10.1109/DATE.2012.6176607","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6176607","","Resource management;Sensors;Safety;Optimization;Actuators;Software;Minimization","circuit optimisation;embedded systems;network synthesis;safety","structural system;safety relevant embedded systems development process;design safety relevant embedded systems;multiple structural system views;technical system representation;functional system representation;multiple design aspects;arbitrary design artifacts;common allocation techniques;partial design generation;complex system view transitions;CESAR design methodology","","","8","","","","","","IEEE","IEEE Conferences"
"Feature selection based on Rough set and modified genetic algorithm for intrusion detection","Y. Guo; B. Wang; X. Zhao; X. Xie; L. Lin; Q. Zhou","Software School of Xiamen University Xiamen, China; Software School of Xiamen University Xiamen, China; Software School of Xiamen University Xiamen, China; Software School of Xiamen University Xiamen, China; Software School of Xiamen University Xiamen, China; Software School of Xiamen University Xiamen, China","2010 5th International Conference on Computer Science & Education","","2010","","","1441","1446","In the Network Intrusion Detection, the large number of features increases the time and space cost, besides the irrelative redundant characteristics make the detection accuracy dropped. In order to improve detection accuracy and efficiency, a new Feature Selection method based on Rough Sets and improved Genetic Algorithms is proposed for Network Intrusion Detection. Firstly, the features are filtered by virtue of the Rough Sets theory; then in the remaining feature subset, the Optimal subset will be found out through the Genetic Algorithm improved with Population Clustering approach for the best ultimate optimized results. Finally, the effectiveness of the algorithm is tested on the classical KDD CUP 99 data sets, using the SVM classifier for performance evaluation. The experiment shows that the new method improves the accuracy and efficiency in Network Intrusion Detection compared with the related researches of the intrusion detection system.","","978-1-4244-6005-2978-1-4244-6002-1978-1-4244-6004","10.1109/ICCSE.2010.5593765","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5593765","Intrusion Detection;Feature Selection;Rough Sets;Genetic Algorithm","Classification algorithms;Feature extraction;Intrusion detection;Accuracy;Support vector machines;Algorithm design and analysis;Clustering algorithms","data mining;feature extraction;genetic algorithms;performance evaluation;rough set theory;security of data;support vector machines","feature selection;rough set theory;modified genetic algorithm;network intrusion detection;irrelative redundant characteristics;detection accuracy;detection efficiency;population clustering approach;KDD CUP 99 data sets;SVM classifier;performance evaluation;intrusion detection system","","8","13","","","","","","IEEE","IEEE Conferences"
"Dynamic Economic Emission Dispatch problem with valve-point effect","C. Rani; D. P. Kothari","School of Electrical Engineering, VIT University, Veliere, India; JB Group of Institutions, Hyderabad, India","2012 International Conference on Emerging Trends in Electrical Engineering and Energy Management (ICETEEEM)","","2012","","","109","114","The advancement in power systems has led to the development of Dynamic Economic Emission Dispatch (DEED)that is difficult to solve by classical optimization method. The proposed paper work is to evolve a simple and effective method for an optimum generation dispatch to minimize the fuel cost and emission of power networks. This paper presents a Chaotic Self Adaptive Particle Swarm Optimization (CSAPSO) algorithm to solve DEED problem. Many non-linear characteristics of the generator such as value point effect, non smooth cost functions of fuel and emission and ramp rate limits are considered in this proposed method. The cost function of the generator exhibits the non convex characteristics, as the valve point effects are modeled and imposed as rectified sinusoid components. The velocity, which is popularly known as sensitive parameter of CSAPSO is adjusted dynamically in order to increase its precision. A chaotic local search operator is introduced in the proposed algorithm to avoid premature convergence. The effectiveness of the proposed optimization is verified in simulation studies using MAT LAB software. In this proposed work, fuel cost and emission are treated as competing objectives. The applicability and high feasibility of the proposed method is validated on 10 unit test systems. Results of the application of the proposed method are presented in this reported work.","","978-1-4673-4633-7978-1-4673-4632","10.1109/ICETEEEM.2012.6494514","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6494514","Dynamic Economic Emission Dispatch (DEED);Chaotic Self Adaptive Particle Swarm Optimization (CSAPSO);ramp rate limits;valve point effect","","load dispatching;mathematics computing;particle swarm optimisation;power system economics;search problems","fuel emission;fuel cost;Matlab software;chaotic local search operator;rectified sinusoid components;nonconvex characteristics;nonsmooth cost functions;nonlinear characteristics;CSAPSO algorithm;chaotic self adaptive particle swarm optimization algorithm;power networks;optimum generation dispatch;optimization method;DEED;power systems;valve-point effect;dynamic economic emission dispatch problem","","3","27","","","","","","IEEE","IEEE Conferences"
"Binary particle swarm optimisation for feature selection: A filter based approach","L. Cervante; Bing Xue; M. Zhang; Lin Shang","School of Engineering and Computer Science, Victoria University of Wellington, PO Box 600, 6140, New Zealand; School of Engineering and Computer Science, Victoria University of Wellington, PO Box 600, 6140, New Zealand; School of Engineering and Computer Science, Victoria University of Wellington, PO Box 600, 6140, New Zealand; State Key Laboratory of Novel Software Technology, Department of Computer Science and Technology, Nanjing University, 210046, China","2012 IEEE Congress on Evolutionary Computation","","2012","","","1","8","Based on binary particle swarm optimisation (BPSO) and information theory, this paper proposes two new filter feature selection methods for classification problems. The first algorithm is based on BPSO and the mutual information of each pair of features, which determines the relevance and redundancy of the selected feature subset. The second algorithm is based on BPSO and the entropy of each group of features, which evaluates the relevance and redundancy of the selected feature subset. Different weights for the relevance and redundancy in the fitness functions of the two proposed algorithms are used to further improve their performance in terms of the number of features and the classification accuracy. In the experiments, a decision tree (DT) is employed to evaluate the classification accuracy of the selected feature subset on the test sets of four datasets. The results show that with proper weights, two proposed algorithms can significantly reduce the number of features and achieve similar or even higher classification accuracy in almost all cases. The first algorithm usually selects a smaller feature subset while the second algorithm can achieve higher classification accuracy.","1089-778X;1941-0026","978-1-4673-1509-8978-1-4673-1510-4978-1-4673-1508","10.1109/CEC.2012.6256452","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6256452","","Redundancy;Equations;Entropy;Mutual information;Mathematical model;Accuracy;Rough sets","decision trees;entropy;particle swarm optimisation;pattern classification","binary particle swarm optimisation;filter based approach;BPSO;information theory;filter feature selection methods;classification problems;mutual information;feature group entropy;fitness functions;decision tree;DT;feature subset selection","","26","26","","","","","","IEEE","IEEE Conferences"
"Model Checking a Lazy Concurrent List-Based Set Algorithm","S. J. Zhang; Y. Liu","NA; NA","2010 Fourth International Conference on Secure Software Integration and Reliability Improvement","","2010","","","43","52","Concurrent objects are notoriously difficult to design correctly, and high performance algorithms that make little or no use of locks even more so. In this paper, we present a formal verification of a lazy concurrent list-based set using model checking techniques. The algorithm supports insertion, removal, and membership testing of a list entry under optimistic locking scheme. The algorithm has nonfixed linearization points and is highly non-trivial. We have proved that the algorithm satisfies linearizability, by showing a trace refinement relation from the concrete implementation to its abstract specification. These models are specified in CSP# and verified automatically using our home grown model checker PAT.","","978-1-4244-7434-9978-1-4244-7435-6978-0-7695-4086","10.1109/SSIRI.2010.37","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5502856","Linearizability;Concurrent List-Based Set Algorithm;Refinement Checking;PAT","Formal verification;Software performance;Software algorithms;Algorithm design and analysis;Concrete;Reliability engineering;Design engineering;Concurrent computing;High performance computing;Testing","communicating sequential processes;concurrency control;list processing;program diagnostics;program verification","formal verification;lazy concurrent list based set algorithms;optimistic locking scheme;nonfixed linearization points;abstract specification;CSP#;home grown model checker PAT;trace refinement relation","","2","19","","","","","","IEEE","IEEE Conferences"
"A Karatsuba-Based Montgomery Multiplier","G. C. T. Chow; K. Eguro; W. Luk; P. Leong","NA; NA; NA; NA","2010 International Conference on Field Programmable Logic and Applications","","2010","","","434","437","Modular multiplication of long integers is an important building block for cryptographic algorithms. Although several FPGA accelerators have been proposed for large modular multiplication, previous systems have been based on O(N<sup>2</sup>) algorithms. In this paper, we present a Montgomery multiplier that incorporates the more efficient Karatsuba algorithm which is O(N<sup>(log 3/log 2)</sup>). This system is parameterizable to different bitwidths and makes excellent use of both embedded multipliers and fine-grained logic. The design has significantly lower LUT-delay product and multiplier-delay product compared with previous designs. Initial testing on a Virtex-6 FPGA showed that it is 60-190 times faster than an optimized multi-threaded software implementation running on an Intel Xeon 2.5 GHz CPU. The proposed multiplier system is also estimated to be 95-189 times more energy efficient than the software-based implementation. This high performance and energy efficiency makes it suitable for server-side applications running in a datacenter environment.","1946-1488;1946-147X;1946-1488","978-1-4244-7843-9978-1-4244-7842-2978-0-7695-4179","10.1109/FPL.2010.89","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5694289","Montgomery multiplication;Karatsuba multiplication","Adders;Field programmable gate arrays;Algorithm design and analysis;Software;Complexity theory;Computer architecture;Clocks","digital arithmetic;field programmable gate arrays;multi-threading;optimisation;public key cryptography","Karatsuba based Montgomery multiplier;cryptographic algorithm;FPGA accelerator;Virtex 6 FPGA;multithreaded software;Intel Xeon 2.5 GHz CPU;modular multiplication","","17","14","","","","","","IEEE","IEEE Conferences"
"Multi-objective performance analysis of one-machine manufacturing systems","P. Bikfalvi; F. Erdélyi; T. Tóth","Department of Information Engineering, University of Miskolc, 3515, Hungary; Department of Information Engineering, University of Miskolc, 3515, Hungary; Department of Information Engineering, University of Miskolc, 3515, Hungary","Proceedings of 2012 IEEE International Conference on Automation, Quality and Testing, Robotics","","2012","","","38","43","Achieving of the desired performance for manufacturing systems is one very important goal when solving of planning and scheduling tasks. Corresponding management of orders and invested assets demands for multi-level and multi-objective optimization. Software systems of Enterprise Resource Planning (ERP) and Manufacturing Execution System (MES) applications used presently in practice are based on different modeling approaches. However, most of involved mathematical models use a set of complicated production equations. In these equations, released jobs and operation rates (as input) can be considered as the most important parameters that may play the controlling role of the production. The considered mathematical models usually evaluate three key performance indices (KPIs) of great importance. These indices are as follows: 1. Readiness for delivery, 2. Stock or WIP (work-in-progress) level, 3.Utilization rate of resources. These indices can be considered as objective functions not only for manufacturing systems, but for the entire enterprise, too. Unfortunately these indices depend on each other and influence each other in an opposite way. A basic requirement of control policy is production stability. In the case of stable production, there is a general correspondence among the average performance indices together with the average released job set or production-rate parameters. These are analyzed for one-machine manufacturing systems.","","978-1-4673-0704-8978-1-4673-0701-7978-1-4673-0703","10.1109/AQTR.2012.6237672","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6237672","performance indices;production planning and control;long-term stability;one-machine systems","Mathematical model;Performance analysis;Production planning;Manufacturing systems;Optimization","asset management;enterprise resource planning;manufacturing systems;production planning;resource allocation;single machine scheduling;work in progress","multiobjective performance analysis;one-machine manufacturing systems;planning tasks;scheduling tasks;order management;invested asset management;multilevel optimization;multiobjective optimization;software system;enterprise resource planning;manufacturing execution system;MES;mathematical models;production equations;operation rates;key performance indices;stock level;WIP level;work in progress;resource utilization rate;control policy;production stability;production control;average released job set;production rate parameter","","","18","","","","","","IEEE","IEEE Conferences"
"Multi-physics analysis of heat-structure on surface resistance","Jianxu Hu; Maozhou Meng","Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Graduate University of Chinese Academy of Sciences, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, CnTech Company, China","2011 IEEE International Conference on Information and Automation","","2011","","","886","889","The failure of electronic systems and equipment mostly happens in the welding parts between components and circuit board while the cycles of thermal expansion achieve a certain number, because the solder comes to fatigue cracking. In this paper, a sample about surface resistance was chosen to explain what relationship among the electric field, heat transfer, and structure expansion by using the multi-physics modeling and simulation software, COMSOL Multiphysics, to model these three physical fields in fully coupling. The components of the electric field, temperature field and stress field distribution were extracted to explain the interaction among these three physics fields. According to the results of numerical simulation, the engineer will be inspired to obtain an optimization structure and process.","","978-1-4577-0270-9978-1-4577-0268-6978-1-4577-0269","10.1109/ICINFA.2011.5949121","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5949121","Full coupling;FEA;Multi-physics;COMSOL;Optimization design","Mathematical model;Resistors;Stress;Equations;Computational modeling;Heat transfer","computational electromagnetics;electric fields;electrical faults;electronic equipment testing;failure analysis;fatigue cracks;heat transfer;solders;surface resistance;temperature;thermal expansion","multiphysics analysis;heat structure;surface resistance;electronic system failure;equipment failure;welding part;circuit board;thermal expansion;solder;fatigue cracking;electric field;heat transfer;structure expansion;multiphysics modeling;COMSOL simulation software;temperature field;stress field distribution","","","9","","","","","","IEEE","IEEE Conferences"
"Biologically-inspired massively-parallel architectures — Computing beyond a million processors","S. Furber","Computer Engineering, School of Computer Science, Manchester U, UK","2011 Design, Automation & Test in Europe","","2011","","","1","1","Moore's Law continues to deliver ever-more transistors on an integrated circuit, but discontinuities in the progress of technology mean that the future isn't simply an extrapolation of the past. For example, design cost and complexity constraints have recently caused the microprocessor industry to switch to multi-core architectures, even though these parallel machines present programming challenges that are far from solved. Moore's Law now translates into ever-more processors on a multi-, and soon many-core chip. The software challenge is compounded by the need for increasing fault-tolerance as near-atomic-scale variability and robustness problems bite harder. We look beyond this transitional phase to a future where the availability of processor resource is effectively unlimited and computations must be optimised for energy usage rather than load balancing, and we look to biology for examples of how such systems might work. Conventional concerns such as synchronisation and determinism are abandoned in favour of real-time operation and adapting around component failure with minimal loss of system efficacy.","1558-1101;1530-1591;1530-1591","978-3-9810801-8-6978-1-61284-208-0978-3-9810801-7","10.1109/DATE.2011.5763006","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5763006","","","multiprocessing systems;parallel architectures;parallel machines;software fault tolerance;transistors","Moores law;transistor;integrated circuit;design cost;complexity constraint;microprocessor industry;multicore architecture;parallel machine;near-atomic-scale variability;fault tolerance;synchronisation;real-time operation;biologically inspired massively parallel architecture","","1","","","","","","","IEEE","IEEE Conferences"
"The control of a positioning system using bi-axial variable reluctance actuator","C. D. Comeaga; C. G. Alionte","University “Politehnica” of Bucharest, Faculty of Mechanical Engineering and Mechatronics; University “Politehnica” of Bucharest, Faculty of Mechanical Engineering and Mechatronics","2010 IEEE International Conference on Automation, Quality and Testing, Robotics (AQTR)","","2010","1","","1","6","This article presents a study about a new positioning system using variable reluctance electro-magnetic actuators. The positioning system can produce movements in a plan. The system is composed of two moving tables series connected and guided using leaf springs, with a pair of opposite actuators acting on each table. In the paper is presented the design, control and testing of this system. The controlling signals applied to the coils are pulse width modulation type. The control algorithm has two parts: the predictive positioning algorithm and the stability algorithm. To achieve a desired position the system use tabular data (starting and ending position) to extract command parameters (fill factors). This tabular data is obtained through several simulations using a FEM analysis. Because the position control is realized in PWM and not in DC current the transitory behavior was evaluated when is applied a PWM signal up to stabilization of the mobile armature at a position. This second step is performed using a finite difference model. The finite difference model implied at each iteration, data from FEM simulation. Because the FEM software (FEMM) can't solve finite difference equation, a software routine was build in LUA scripting program which call the FEM simulation software at each step. Due to the low mechanically stiffness another algorithm was proposed to stabilize the structure around the target point, using a linearised model. The results of the experimental test confirmed the functioning of the system and allowed to optimize the coefficients of the command algorithm.","","978-1-4244-6725-9978-1-4244-6724-2978-1-4244-6723","10.1109/AQTR.2010.5520868","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5520868","","Control systems;Actuators;Pulse width modulation;Finite difference methods;Electric variables control;System testing;Springs;Coils;Space vector pulse width modulation;Prediction algorithms","coils;electromagnetic actuators;finite difference methods;finite element analysis;position control;pulse width modulation;reluctance machines;springs (mechanical);stability","positioning system control;biaxial variable reluctance electro-magnetic actuator;moving tables;leaf springs;system design;system testing;controlling signal;coil;pulse width modulation;predictive positioning algorithm;stability algorithm;starting position;ending position;command parameter;FEM analysis;transitory behavior;PWM signal;stabilization;mobile armature;finite difference model;FEM simulation;software routine;LUA scripting program","","2","5","","","","","","IEEE","IEEE Conferences"
"“Feeling” young modulus of materials","M. T. Restivo; A. M. Lopes; P. J. Xia","UISPA, IDMEC-Pólo FEUP, Faculdade de Engenharia, Universidade do Porto, Portugal; UISPA, IDMEC-Pólo FEUP, Faculdade de Engenharia, Universidade do Porto, Portugal; School of Mechanical and Electronic Engineering, Harbin Institute of Technology, China","2012 9th International Conference on Remote Engineering and Virtual Instrumentation (REV)","","2012","","","1","4","Stiffness is a structural property to be considered in product design and optimization, operations that are, at present, the industry's universal goals. The stiffness of a body is associated with the Young modulus of its material. Therefore, the Young modulus is of great relevance and the evaluation of this material property is important at education or at training levels. A flexible material is associated to a low Young modulus and a stiff material to a high Young modulus. So, in the first case, low loads are able to cause an elastic deformation and in the second case, higher loads are needed for inducing it. The association of tactile information to a bending test using similar specimens of different materials will provide to the user sensorial information on their Young modulus. A virtual reality software application with haptic interaction has been developed for training users “to feel the Young modulus of materials”. The virtual reality application and its structure will be described in this work. Finally, perspectives on future work will be presented.","","978-1-4673-2542-4978-1-4673-2540-0978-1-4673-2541","10.1109/REV.2012.6293173","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6293173","virtual reality;haptics interface;training;computer aided instruction","Haptic interfaces;Young's modulus;Force;Instruction sets;Education;Rendering (computer graphics)","bending;computer aided instruction;elastic deformation;haptic interfaces;mechanical engineering computing;mechanical testing;optimisation;product design;virtual reality;Young's modulus","body stiffness;structural property;product design;optimization;Young modulus;material property;education;training level;flexible material;stiff material;elastic deformation;tactile information;bending test;sensorial information;virtual reality software;haptic interaction","","4","12","","","","","","IEEE","IEEE Conferences"
"Optimal size and location of Distributed Generation and KVAR support in unbalanced 3-Φ distribution system using PSO","S. S. Reddy; S. H. N. Dey; S. Paul","Dept. of Electrical Engg., Jadavpur University, Kolkata, India-700032; Dept. of Electrical Engg., Jadavpur University, Kolkata, India-700032; Dept. of Electrical Engg., Jadavpur University, Kolkata, India-700032","2012 International Conference on Emerging Trends in Electrical Engineering and Energy Management (ICETEEEM)","","2012","","","77","83","This paper proposes a Particle Swarm Optimization based methodology for finding Optimal size and location of Distributed Generation and unbalanced Reactive power support for unbalanced three phase distribution network. The improvement in voltage profile and loss saving are presented. The proposed technique is tested on IEEE 37 node radial test feeder which is an actual feeder in California. The program is developed using MATLAB programming software. The results obtained show the effectiveness of the method for unbalanced network.","","978-1-4673-4633-7978-1-4673-4632","10.1109/ICETEEEM.2012.6494447","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6494447","Distributed generation (DG);power losses;Particle Swarm Optimization (PSO);Optimal placement","","distributed power generation;losses;particle swarm optimisation;reactive power","KVAR;unbalanced 3-Φ distribution system;PSO;particle swarm optimization;optimal distributed generation location;optimal distributed generation size;unbalanced reactive power;distribution network;voltage profile;loss;IEEE 37 node radial test feeder;Matlab programming software;unbalanced network","","2","19","","","","","","IEEE","IEEE Conferences"
"Enabling technologies for self-aware adaptive systems","M. D. Santambrogio; H. Hoffmann; J. Eastep; A. Agarwal","Massachusetts Institute of Technology, Computer Science and Artificial Intelligence Laboratory, Cambridge, MA 02139; Massachusetts Institute of Technology, Computer Science and Artificial Intelligence Laboratory, Cambridge, MA 02139; Massachusetts Institute of Technology, Computer Science and Artificial Intelligence Laboratory, Cambridge, MA 02139; Massachusetts Institute of Technology, Computer Science and Artificial Intelligence Laboratory, Cambridge, MA 02139","2010 NASA/ESA Conference on Adaptive Hardware and Systems","","2010","","","149","156","Self-aware computer systems will be capable of adapting their behavior and resources thousands of times a second to automatically find the best way to accomplish a given goal despite changing environmental conditions and demands. Such a capability benefits a broad spectrum of computer systems from embedded systems to supercomputers and is particularly useful for meeting power, performance, and resource-metering challenges in mobile computing, cloud computing, multicore computing, adaptive and dynamic compilation environments, and parallel operating systems. Some of the challenges in implementing self-aware systems are a) knowing within the system what the goals of applications are and if they are meeting them, b) deciding what actions to take to help applications meet their goals, and c) developing standard techniques that generalize and can be applied to a broad range of self-aware systems. This work presents our vision for self-aware adaptive systems and proposes enabling technologies to address these three challenges. We describe a framework called Application Heartbeats that provides a general, standardized way for applications to monitor their performance and make that information available to external observers. Then, through a study of a self-optimizing synchronization library called Smartlocks, we demonstrate a powerful technique that systems can use to determine which optimization actions to take. We show that Heartbeats can be applied naturally in the context of reinforcement learning optimization strategies as a reward signal and that, using such a strategy, Smartlocks are able to significantly improve performance of applications on an important emerging class of multicore systems called asymmetric multicores.","","978-1-4244-5889-9978-1-4244-5887-5978-1-4244-5888","10.1109/AHS.2010.5546266","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5546266","","Adaptive systems;Monitoring;Multicore processing;Benchmark testing;Biomedical monitoring;Computers","embedded systems;learning (artificial intelligence);multiprocessing systems;software libraries;software performance evaluation;system monitoring","self-aware adaptive system;self-aware computer system;environmental condition;embedded system;mobile computing;cloud computing;multicore computing;adaptive compilation environment;dynamic compilation environment;parallel operating system;Application Heartbeats;self-optimizing synchronization library;Smartlocks;reinforcement learning optimization strategy;multicore system;asymmetric multicore","","17","39","","","","","","IEEE","IEEE Conferences"
"Performance evaluation and optimization of random memory access on multicores with high productivity","V. Saxena; Y. Sabharwal; P. Bhatotia","IBM Research - India, New Delhi; IBM Research - India, New Delhi; Max Planck Institute for Software Systems, Germany","2010 International Conference on High Performance Computing","","2010","","","1","10","The slow progress in memory access latencies in comparison to CPU speeds has resulted in memory accesses dominating code performance. While architectural enhancements have benefited applications with data locality and sequential access, random memory access still remains a cause for concern. Several benchmarks have been proposed to evaluate the random memory access performance on multicore architectures. However, the performance evaluation models used by the existing benchmarks do not fully capture the varying types of random access behaviour arising in practical applications. In this paper, we propose a new model for evaluating the performance of random memory access that better captures the random access behaviour demonstrated by applications in practice. We use our model to evaluate the performance of two popular multicore architectures, the Cell and the GPU. We also suggest novel optimizations on these architectures that significantly boost the performance for random accesses in comparison to conventional architectures. Performance improvements on these architectures typically come at the cost of reduced productivity considering the extra programming effort involved. To address this problem, we propose libraries that incorporate these optimizations and provide innovatively designed programming interfaces that can be used by the applications to achieve good performance without loss of productivity.","1094-7256","978-1-4244-8520-8978-1-4244-8518-5978-1-4244-8519","10.1109/HIPC.2010.5713168","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5713168","","Libraries;Computer architecture;Benchmark testing;Graphics processing unit;Optimization;Instruction sets;Microprocessors","memory architecture;multiprocessing systems;performance evaluation;random-access storage","performance evaluation models;random memory access;multicore architectures;memory access latencies;CPU speeds;data locality;sequential access;Cell;GPU;programming interfaces","","5","12","","","","","","IEEE","IEEE Conferences"
"Fast, robust quadruped locomotion over challenging terrain","M. Kalakrishnan; J. Buchli; P. Pastor; M. Mistry; S. Schaal","Computational Learning and Motor Control Lab, University of Southern California, Los Angeles, 90089, USA; Computational Learning and Motor Control Lab, University of Southern California, Los Angeles, 90089, USA; Computational Learning and Motor Control Lab, University of Southern California, Los Angeles, 90089, USA; Disney Research, Pittsburgh, PA 15213, USA; Computational Learning and Motor Control Lab, University of Southern California, Los Angeles, 90089, USA","2010 IEEE International Conference on Robotics and Automation","","2010","","","2665","2670","We present a control architecture for fast quadruped locomotion over rough terrain. We approach the problem by decomposing it into many sub-systems, in which we apply state-of-the-art learning, planning, optimization and control techniques to achieve robust, fast locomotion. Unique features of our control strategy include: (1) a system that learns optimal foothold choices from expert demonstration using terrain templates, (2) a body trajectory optimizer based on the Zero-Moment Point (ZMP) stability criterion, and (3) a floating-base inverse dynamics controller that, in conjunction with force control, allows for robust, compliant locomotion over unperceived obstacles. We evaluate the performance of our controller by testing it on the LittleDog quadruped robot, over a wide variety of rough terrain of varying difficulty levels. We demonstrate the generalization ability of this controller by presenting test results from an independent external test team on terrains that have never been shown to us.","1050-4729","978-1-4244-5038-1978-1-4244-5040","10.1109/ROBOT.2010.5509805","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5509805","","Robustness;Legged locomotion;Force control;Foot;Leg;Size control;Robust control;Mobile robots;Software testing;Optimal control","force control;learning systems;legged locomotion;optimisation;path planning;stability criteria","fast robust quadruped locomotion;challenging terrain;control architecture;learning;planning;optimization;control strategy;optimal foothold choices;body trajectory optimizer;zero-moment point stability criterion;floating-base inverse dynamics controller;force control","","41","14","","","","","","IEEE","IEEE Conferences"
"The research and application of GSM network coverage based on high-speed railway","Y. Du; J. Wang","College of Electronics Science and Engineering, Nanjing University of Posts and Telecommunications, Nanjing, China; College of Electronics Science and Engineering, Nanjing University of Posts and Telecommunications, Nanjing, China","2011 International Conference on Electrical and Control Engineering","","2011","","","2751","2754","In China, China Mobile has totally 400 thousands GSM base stations and the network coverage is the best in the world. This research analyzes the problems of GSM network coverage in high-speed railway and puts forward effective solutions of each problem. Moreover, it introduces the hardware and software used in DT Testing. Meanwhile through the test in intercity railway from Nanjing to Shanghai, it shows specific analysis methods of data and parameters. Finally, based on a layout example of high-speed railway from Beijing to Shanghai, the research gives standards and planning of high-speed special network from several aspects.","","978-1-4244-8165-1978-1-4244-8162-0978-1-4244-8164","10.1109/ICECENG.2011.6057653","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6057653","GSM Network;High-Speed Railway;Optimization;Drive Test","Rail transportation;Base stations;GSM;Doppler effect;Optical switches","cellular radio;railway communication;telecommunication network planning","GSM network coverage;high-speed railway;GSM base stations;DT testing;intercity railway;high-speed special network planning","","","7","","","","","","IEEE","IEEE Conferences"
"Herb 2.0: Lessons Learned From Developing a Mobile Manipulator for the Home","S. S. Srinivasa; D. Berenson; M. Cakmak; A. Collet; M. R. Dogar; A. D. Dragan; R. A. Knepper; T. Niemueller; K. Strabala; M. Vande Weghe; J. Ziegler","The Robotics Institute, Carnegie Mellon University; EECS Department, University of California Berkeley, Berkeley, CA , USA; School of Interactive Computing, Georgia Institute of Technology , Atlanta, GA, USA; The Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; The Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; The Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; The Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Knowledge-based Systems Group, RWTH Aachen University, Aachen, Germany; The Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; The Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Department of Measurement and Control Systems, Karlsruhe Institute of Technology, Karlsruhe, Germany","Proceedings of the IEEE","","2012","100","8","2410","2428","We present the hardware design, software architecture, and core algorithms of Herb 2.0, a bimanual mobile manipulator developed at the Personal Robotics Lab at Carnegie Mellon University, Pittsburgh, PA. We have developed Herb 2.0 to perform useful tasks for and with people in human environments. We exploit two key paradigms in human environments: that they have structure that a robot can learn, adapt and exploit, and that they demand general-purpose capability in robotic systems. In this paper, we reveal some of the structure present in everyday environments that we have been able to harness for manipulation and interaction, comment on the particular challenges on working in human spaces, and describe some of the lessons we learned from extensively testing our integrated platform in kitchen and office environments.","0018-9219;1558-2256","","10.1109/JPROC.2012.2200561","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6222306","Human-robot interaction;mobile manipulation;motion planning;perception;personal robotics;trajectory optimization","Mobile communication;Human factors;Sensors;Manipulators;Intelligent systems;Hardware design languages;Service robots;Quality assessment","home computing;manipulators;mobile robots;software architecture","hardware design;software architecture;core algorithms;bimanual mobile manipulator;personal robotics lab;Carnegie Mellon University;Herb 2.0;general-purpose capability;robotic systems;kitchen environments;office environments","","46","80","","","","","","IEEE","IEEE Journals & Magazines"
"A Framework for Automated Performance Tuning and Code Verification on GPU Computing Platforms","A. S. Gehrke; I. Ra; D. A. Connors","NA; NA; NA","2011 IEEE International Symposium on Parallel and Distributed Processing Workshops and Phd Forum","","2011","","","2113","2116","Emerging multi-core processor designs create a computing paradigm capable of advancing numerous scientific areas, including medicine, data mining, biology, physics, and earth sciences. However, the trends in multi-core hardware technology have advanced far ahead of the advances in software technology and programmer productivity. For the most part, current scientists only leverage multi-core and GPU (Graphical Processing Unit) computing platforms after painstakingly uncovering the inherent task and data-level parallelism in their application. In many cases, the development does not realize the full potential of the parallel hardware. There exists an opportunity to meet the challenges in optimally mapping scientific application domains to multi-core computer systems through the use of compile-time and link-time optimization strategies. We are exploring a code compilation framework that automatically generates and tunes numerical solver codes for optimal performance on graphical processing units. The framework advances computational simulation in kinetic modeling by significantly reducing the execution time of scientific simulations and enabling scientists to compare results to previous models and to extend, modify, and test new models without code changes.","1530-2075;1530-2075","978-1-61284-425-1978-0-7695-4577","10.1109/IPDPS.2011.390","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6009101","","Computational modeling;Numerical models;Graphics processing unit;Kinetic theory;Optimization;Registers;Data models","computer graphic equipment;coprocessors;multiprocessing systems;optimisation;performance evaluation;program compilers","automated performance tuning;code verification;GPU computing platforms;multicore processor designs;data level parallelism;parallel hardware;compile time;link time optimization strategies;code compilation framework;kinetic modeling","","","12","","","","","","IEEE","IEEE Conferences"
"NBTI mitigation by optimized NOP assignment and insertion","F. Firouzi; S. Kiamehr; M. B. Tahoori","Dependable Nano-Computing (CDNC), Karlsruhe Institute of Technology (KIT), 76131, Germany; Dependable Nano-Computing (CDNC), Karlsruhe Institute of Technology (KIT), 76131, Germany; Dependable Nano-Computing (CDNC), Karlsruhe Institute of Technology (KIT), 76131, Germany","2012 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2012","","","218","223","Negative Bias Temperature Instability (NBTI) is a major source of transistor aging in scaled CMOS, resulting in slower devices and shorter lifetime. NBTI is strongly dependent on the input vector. Moreover, a considerable fraction of execution time of an application is spent to execute NOP (No Operation) instructions. Based on these observations, we present a novel NOP assignment to minimize NBTI effect, i.e. maximum NBTI relaxation, on the processors. Our analysis shows that NBTI degradation is more impacted by the source operands rather than instruction opcodes. Given this, we obtain the instruction, along with the operands, with minimal NBTI degradation, to be used as NOP. We also proposed two methods, software-based and hardware-based, to replace the original NOP with this maximum aging reduction NOP. Experimental results based on SPEC2000 applications running on a MIPS processor show that this method can extend the lifetime by 37% in average while the overhead is negligible.","1558-1101;1530-1591;1530-1591","978-1-4577-2145-8978-3-9810801-8","10.1109/DATE.2012.6176465","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6176465","","Program processors;Delay;Degradation;Logic gates;Registers;Hazards;Transistors","ageing;CMOS integrated circuits;integrated circuit reliability;MOSFET","NBTI mitigation;optimized NOP assignment;optimized NOP insertion;negative bias temperature instability mitigation;NOP assignment;NBTI relaxation;NBTI degradation;instruction opcodes;maximum aging reduction NOP;MIPS processor;CMOS technology","","21","18","","","","","","IEEE","IEEE Conferences"
"Research on printing machinery design based on virtual prototype technology","X. Zheng; F. Wang","Department of Packaging and printing, Zhongshan Torch Polytechnic, Guangdong Zhongshan, China; School of Mechanical &amp; Electonic, Xi'an Technological University, Shaanxi, Xi'an, China","2012 2nd International Conference on Consumer Electronics, Communications and Networks (CECNet)","","2012","","","1247","1250","Die-cutting machine is an important postpress processing equipment for surface decoration of printed matter. After speed of die-cutting machine increased, insufficient position time during the course of paper transferring is an important factor constraining die-cutting accurate improvement. For original shortcoming about paper-transferring mechanism design, performance of new paper-transferring mechanism may be predicted based on virtual prototype technology. By solidworks software, solid model of the paper-transferring mechanism can be established and the model can be imported into simulation analysis software ADAMS. With the mechanism simulated and the performance tested, the maximum positioning error of the paper-transferring mechanism at high-speed is 0.01 mm that it is far lower than the present error, so the design can meet the requirements of the machine. By applying virtual design method, the performance of the mechanism will be designed, analyzed and evaluated in early design in order to set and optimize prototype parameters. Thus the risk and the development cycle of new product exploitation will be reduced and the product performance will be increased.","","978-1-4577-1415-3978-1-4577-1414-6978-1-4577-1413","10.1109/CECNet.2012.6201727","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6201727","printing machinery;die-cutting machine;virtual prototype;ADAMS","Prototypes;Software;Analytical models;Accuracy;Printing machinery;Mechanical systems;Force","cutting tools;dies (machine tools);printing machinery;solid modelling;virtual prototyping","printing machinery design;virtual prototype technology;die-cutting machine;postpress processing equipment;printed matter surface decoration;paper-transferring mechanism design;solidworks software;solid model;simulation analysis software;ADAMS software;virtual design method;prototype parameter optimization;product exploitation","","1","7","","","","","","IEEE","IEEE Conferences"
"Identification and analysis the change points of hydrological process of Yingluoxia station in the Heihe river basin","Yue Zou; Bo Zhang","College of Geography and Environmental Science, Northwest Normal University, 730070, Lanzhou, China; College of Geography and Environmental Science, Northwest Normal University, 730070, Lanzhou, China","World Automation Congress 2012","","2012","","","1","5","In order to distinguish the change-point of hydrological process accurately, this paper adopted the annual runoff series data for 60 years of the hydrological monitoring stations of the Yingluoxia in Heihe river basin during 1947 to 2006. Difference plot curve-combined method of rank test is used to identify the change-point of hydrological process, which combined with Brown-Forsythe method, sequential clustering method, sliding F test method to verify the accuracy of variation points. At the same time, it applied for the statistical software of SPSS to analyze the changes in hydrological characteristics of the data before and after the variation points. The results showed that 1959 and 1979 were the change points in Yingluoxia station. The characteristics of trends and mutations in hydrological series reflected that natural conditions and human activities had a significant impact on the natural state of the river hydrological sequence.","2154-4824;2154-4824","978-1-889334-47-9978-1-4673-4497-5978-1-889335-47","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6321321","Heihe River;hydrological sequences;change point;differencel plot curve;rank tests","Rivers;Humans;Meteorology;Educational institutions;Gaussian distribution;Water resources;Standards","","","","","15","","","","","","IEEE","IEEE Conferences"
"Constrained Power Management: Application to a multimedia mobile platform","P. Bellasi; S. Bosisio; M. Carnevali; W. Fornaciari; D. Siorpaes","Dipartimento di Elettronica e Informazione, Politecnico di Milano, P.zza Leonardo da Vinci, 32. 20133, Italy; Dipartimento di Elettronica e Informazione, Politecnico di Milano, P.zza Leonardo da Vinci, 32. 20133, Italy; Dipartimento di Elettronica e Informazione, Politecnico di Milano, P.zza Leonardo da Vinci, 32. 20133, Italy; Dipartimento di Elettronica e Informazione, Politecnico di Milano, P.zza Leonardo da Vinci, 32. 20133, Italy; Advanced System Technology, STMicroelectronics, Via C. Olivetti, 2. 20041 - Agrate Brianza, Italy","2010 Design, Automation & Test in Europe Conference & Exhibition (DATE 2010)","","2010","","","989","992","In this paper we provide an overview of CPM, a cross-layer framework for Constrained Power Management, and we present its application on a real use case. This framework involves different layers of a typical embedded system, ranging from device drivers to applications. The main goals of CPM are (i) to aggregate applications' QoS requirements and (ii) to exploit them to support an efficient coordination between different drivers' local optimization policies. This role is supported by a system-wide and multi-objective optimization policy which could be also changed at run-time. In this paper we mostly focus on a real use case to show the very low overhead of CPM both on the management of QoS requirements and on the tracking of hardware crossdependencies, which cannot be directly considered by local optimization policies.","1558-1101;1530-1591;1530-1591","978-3-9810801-6-2978-1-4244-7054-9978-3-9810801-6","10.1109/DATE.2010.5456905","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5456905","","Energy management;Power system management;Aggregates;Hardware;Resource management;Linux;Power generation;Embedded system;Application software;Energy consumption","device drivers;embedded systems;mobile computing;multimedia computing;optimisation;power aware computing;quality of service","constrained power management;multimedia mobile platform;embedded system;device drivers;QoS requirements;optimization policies;hardware crossdependencies","","2","7","","","","","","IEEE","IEEE Conferences"
"Exploitation of Multiple Hyperspace Dimensions to Realize Coexistence Optimized Wireless Automation Systems","K. Ahmad; P. Ostfeld; U. Meier; H. Kwasnicka","Institute Industrial IT, OWL University of Applied Sciences, Lemgo, Germany; Institute Industrial IT, OWL University of Applied Sciences, Lemgo, Germany; Institute Industrial IT, OWL University of Applied Sciences, Lemgo, Germany; Wroc&#x0142;aw University of Technology, Wroc&#x0142;aw, Poland","IEEE Transactions on Industrial Informatics","","2010","6","4","758","766","The need for multiple radio systems in overlapping regions of a factory floor introduces a coexistence problem. The current research challenge is to design and realize radio systems that should be able to achieve a desired quality-of-service (QoS) in harsh, time-varying, coexisting industrial environments. Conventional coexistence solutions attempt to accommodate coexisting systems in a single dimension, mostly in the frequency dimension. The concept of multidimensional electromagnetic (EM) space utilization provides optimal opportunities to achieve coexistence optimized solutions. It can revolutionarily augment the shareable capacity of the resource space and provide optimal coexisting capabilities of radio systems. A software defined radio (SDR)-based cognitive radio (CR) is realized which can exploit the frequency, time, and power dimensions of the EM space to improve coexistence in the 2.4 GHz industrial, scientific, and medical (ISM) band. Furthermore, a conventional hardware defined radio (HDR) and additional simulations are used to test and prove the feasibility of the triple EM space utilization. Joint results of these experiments are presented in this contribution. Additionally, we present a novel computational efficient algorithm to detect cyclic properties of industrial wireless systems.","1551-3203;1941-0050","","10.1109/TII.2010.2062191","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5556023","Coexistence;cognitive radio;software defined radio;spectrum sensing","Cognitive radio;Quality of service;Software radio;Tuning;Wireless communication","quality of service;radiocommunication","multiple hyperspace dimensions;coexistence optimized wireless automation systems;multiple radio systems;factory floor;coexistence problem;quality-of-service;industrial environments;multidimensional electromagnetic space utilization","","8","27","","","","","","IEEE","IEEE Journals & Magazines"
"Research on Dynamic Modeling and Simulation of Complex Mechanical-Electrical-Hydraulic Coupling System","G. Liu; B. Xu; T. Zheng; J. Yang","NA; NA; NA; NA","2012 Spring Congress on Engineering and Technology","","2012","","","1","4","Taking Special Vehicle as an example, the research is made on dynamic virtual prototyping modeling and simulation of complex mechanical-electrical-hydraulic coupling system. Based on mechanical dynamic model and control and hydraulic schematic diagram, a mechanical-electrical-hydraulic coupling dynamic virtual prototyping model of Special Vehicle is built capitalizing on ADAMS, MATLAB/Simulink and Easy5 and validated with frequency spectrum analysis. On the basis of this model, the virtual tests are carried out in various typical working conditions, which include vehicle driving stability, vehicle unfolding and folding stability, etc. From these virtual tests, complete data are obtained and performance indexes achievability and adaptability to the environment of Special Vehicle are evaluated. Meanwhile, the research results have important significance for the simulation and optimization of complex mechanical-electrical-hydraulic coupling system.","","978-1-4577-1964-6978-1-4577-1965-3978-1-4577-1963","10.1109/SCET.2012.6342151","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6342151","","Vehicles;Mathematical model;Vehicle dynamics;Virtual prototyping;Couplings;Integrated circuit modeling;Springs","hydraulic systems;mechanical engineering computing;mechanical stability;mechanical testing;military vehicles;vehicle dynamics;virtual prototyping;weapons","complex mechanical-electrical-hydraulic coupling system;special vehicle;dynamic virtual prototyping modeling;hydraulic schematic diagram;mechanical dynamic model;ADAMS software;Matlab-Simulink;Easy5 software;frequency spectrum analysis;virtual test;vehicle driving stability;vehicle unfolding stability;vehicle folding stability;performance index;weapon system","","1","8","","","","","","IEEE","IEEE Conferences"
"Real-time software GNSS signal simulator accelerated by CUDA","Z. Bo; L. Guang-bin; L. Dong; F. Zhi-liang","Laboratory of Satellite Navigation, Xi'an Research Institute of Hi-tech, Xi'an, P. R. China; Laboratory of Satellite Navigation, Xi'an Research Institute of Hi-tech, Xi'an, P. R. China; Laboratory of Satellite Navigation, Xi'an Research Institute of Hi-tech, Xi'an, P. R. China; Laboratory of Satellite Navigation, Xi'an Research Institute of Hi-tech, Xi'an, P. R. China","2010 2nd International Conference on Future Computer and Communication","","2010","1","","V1-100","V1-104","Software GNSS signal simulators produce the digital IF signal to test the algorithms of GNSS receiver. Because of the limitation of parallel computing capability that general CPU can provide, software GNSS simulator cannot generate signal synchronously when the sample rate is tens of MSPS. To solve this problem, a DDFS algorithm accelerated by CUDA technology is introduced. By optimizing the data transitions and execution path, the real-time simulation can run at sample rate up to 58MSPS. The simulator has been proved functional by general GNSS acquisition and tracking software.","","978-1-4244-5824-0978-1-4244-5821-9978-1-4244-5823","10.1109/ICFCC.2010.5497829","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5497829","global navigation satellites system;signal simulator;parallel computation;direct digital frequency synthesis","Satellite navigation systems;Acceleration;Computational modeling;Signal processing;Concurrent computing;Multicore processing;Parallel processing;Central Processing Unit;Signal generators","computer graphics;coprocessors;satellite navigation;telecommunication computing","real-time software;GNSS signal simulator;CUDA;parallel computing capability;global navigation satellites system;compute unified device architecture","","1","5","","","","","","IEEE","IEEE Conferences"
"An Optimized Checkpointing Based Learning Algorithm for Single Event Upsets","S. Sharanyan; A. Kumar","NA; NA","2010 IEEE 34th Annual Computer Software and Applications Conference","","2010","","","395","400","With the arrival of the CMOS technology, the sizes of the transistors are anything but increasing. Due to the current transistor sizes single event upsets, which were over looked for the previous generation are not so anymore. With memories and other peripherals well protected from single event upsets, processors are in a critical state. Hard errors too have a higher probability of occurrence. This work is aimed at detection of soft errors (SEUs) and making programs more resilient to them and to detect hard errors and eliminate them. SEUs are transient errors and hard errors are permanent in nature. The idea is to use the concept of CFGs, DFGs and data dependency graphs with Integer Linear Programming to improve the program and testing it on fault induced architectures.","0730-3157;0730-3157;0730-3157","978-1-4244-7513-1978-1-4244-7512-4978-0-7695-4085","10.1109/COMPSAC.2010.47","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676284","Computer Architecture;Single Event Upsets;Control Flow Graphs;Integer Linear Programming;Graph theory","Program processors;Single event upset;Hardware;Accuracy;Resilience;Computational modeling;Testing","checkpointing;CMOS integrated circuits;error detection;integer programming;linear programming","optimized checkpointing based learning algorithm;single event upset;CMOS technology;soft error detection;transient errors;hard errors;CFG;DFG;data dependency graphs;integer linear programming;fault induced architectures","","","13","","","","","","IEEE","IEEE Conferences"
"Visualization of Real-World Web Services Based on Fuzzy Logic","M. Almulla; H. Yahyaoui; K. Almatori","NA; NA; NA","2012 IEEE Eighth World Congress on Services","","2012","","","330","335","This paper investigates techniques for supporting individual users in choosing appropriate Web services according to their specific needs, and accessing their favorite Web services. It also examines the provision of visualization of Web services that can be made available to service consumers as well as service providers who promote their Web services. To achieve these goals, we propose a three-layer architecture: a data layer for handling measurements of quality attribute of Web services; a Web service middleware processing layer, which computes quality attribute weights for ranking Web services; and finally, a visualization layer for monitoring the quality of Web services. The proposed architecture is implemented and tested on real world Web services drawn from the QWS database of Web services. Our Web services visualization technique is more convenient and user-friendly compared to current basic text based Web services discovery techniques.","2378-3818","978-1-4673-3053-4978-0-7695-4756","10.1109/SERVICES.2012.68","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6274070","Web services;Visualization;Fuzzy logic;Ranking","Data visualization;Equations;Computer architecture;Service oriented architecture;IP networks;Google","fuzzy logic;middleware;program visualisation;software architecture;software metrics;software quality;Web services","real-world Web services visualization technique;fuzzy logic;three-layer architecture;data layer;quality attribute measurement handling;Web service middleware processing layer;quality attribute weight computation;visualization layer;quality monitoring;QWS database","","3","24","","","","","","IEEE","IEEE Conferences"
"An FPGA-based implementation of HW/SW architecture for CFAR radar target detector","R. Djemal; K. Belwafi; W. Kaaniche; S. A. Alshebeili","Electrical Engineering Department College of Engineering, King Saud University, Box 800 CP 11421 KSA; Electrical Engineering Department College of Engineering, King Saud University, Box 800 CP 11421 KSA; ENISO, Avenue 18 Janvier 1952 Sousse Tunisia; Electrical Engineering Department College of Engineering, King Saud University, Box 800 CP 11421 KSA","ICM 2011 Proceeding","","2011","","","1","6","This paper presents an efficient HW/SW Codesign FPGA-based architecture of B-ACOSD CFAR target detector in log normal distribution for radar system. All CFAR system modules are analyzed in order to identify the critical ones to be optimized so that the detection process will be conducted in realtime. To compel the design optimization of CFAR Architecture, we have considered the custom instruction approach offered by Altera environment. Furthermore HW/SW architecture of the CFAR detector is carried out where the NIOS II execute the software part and communicate via the Avalon switch fabric with the hardware modules represented by the custom logic components, on-chip memories, UART and JTAG interfaces. The proposed system-on-chip is validated and tested using the Stratix IV EP4SGX230KF4C2 of Altera operating at 250MHz. Using the HW/SW approach for our embedded target detection system, we improved the performance of the architecture compared to the pure software one with a total delay of 0.45 μs.","2159-1660;2159-1679","978-1-4577-2209-7978-1-4577-2207-3978-1-4577-2208","10.1109/ICM.2011.6177358","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6177358","","Computer architecture;Clutter;Detectors;System-on-a-chip;Hardware;Microprocessors;Software","field programmable gate arrays;hardware-software codesign;log normal distribution;object detection;optimisation;radar computing;radar detection;radar signal processing;system-on-chip","HW-SW codesign FPGA-based architecture;B-ACOSD CFAR radar target detector;log normal distribution;radar system;design optimization;CFAR architecture;custom instruction approach;Altera environment;NIOS II;Avalon switch fabric;custom logic components;on-chip memories;UART;JTAG interfaces;system-on-chip;Stratix IV EP4SGX230KF4C2;embedded target detection system;frequency 250 MHz","","1","26","","","","","","IEEE","IEEE Conferences"
"The procedure of a camera calibration using Camera Calibration Toolbox for MATLAB","A. Fetić; D. Jurić; D. Osmanković","Faculty of Electrical Engineering, Department for Automatic Control and Electronics, Sarajevo, Bosnia and Herzegovina; Faculty of Electrical Engineering, Department for Automatic Control and Electronics, Sarajevo, Bosnia and Herzegovina; Faculty of Electrical Engineering, Department for Automatic Control and Electronics, Sarajevo, Bosnia and Herzegovina","2012 Proceedings of the 35th International Convention MIPRO","","2012","","","1752","1757","This paper describes the calibration procedure of a CCD digital camera based on the perspective projection method. The idea of this method is to calibrate the camera in order to extract precise three dimensional information from the images so that they can be used for the computer vision and similar applications which take the images acquired from the camera as an input. The image capturing process reduces the dimensions of data from three to two. The camera calibration procedure determines which light source is associated with each pixel on the resulting image. Since the camera used in the process is a complex CCD camera, errors resulting from the misaligned lenses and deformations in their structures result in more complex distortions in the acquired image. In order to associate the points in the cameras image space with the locations in a 3D world space the camera projection matrix consisting of the estimated intrinsic and extrinsic parametars is calculated. The calibration process is comprised of capturing calibration images and processing them in order to get the intrinsic and extrinsic camera parameters. The results of the calibration are tested using the test images captured with the same camera. The calibration process was implemented using a MATLAB software package.","","978-953-233-068-7978-1-4673-2577-6978-953-233-072","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6240932","calibration;CCD camera;perspective projection method;pixel error;intrinsic parametars;extrinsic parametars","Calibration;Cameras;Charge coupled devices;Lenses;Nonlinear distortion;Optimization;Computers","calibration;computer vision;deformation;feature extraction;matrix algebra;software packages;video cameras","Matlab;camera calibration toolbox;CCD digital camera;perspective projection method;information extraction;computer vision;images acquisition;image capturing proccess;misaligned lens;deformation;image distortion;3D world space;camera projection matrix;camera parameter estimation;MATLAB;software package","","7","9","","","","","","IEEE","IEEE Conferences"
"Benchmark synthesis for architecture and compiler exploration","L. Van Ertvelde; L. Eeckhout","Ghent University, Belgium; Ghent University, Belgium","IEEE International Symposium on Workload Characterization (IISWC'10)","","2010","","","1","11","This paper presents a novel benchmark synthesis framework with three key features. First, it generates synthetic benchmarks in a high-level programming language (C in our case), in contrast to prior work in benchmark synthesis which generates synthetic benchmarks in assembly. Second, the synthetic benchmarks hide proprietary information from the original workloads they are built after. Hence, companies may want to distribute synthetic benchmark clones to third parties as proxies for their proprietary codes; third parties can then optimize the target system without having access to the original codes. Third, the synthetic benchmarks are shorter running than the original workloads they are modeled after, yet they are representative. In summary, the proposed framework generates small (thus quick to simulate) and representative benchmarks that can serve as proxies for other workloads without revealing proprietary information; and because the benchmarks are generated in a high-level programming language, they can be used to explore both the architecture and compiler spaces. The results obtained with our initial framework are promising. We demonstrate that we can generate synthetic proxy benchmarks for the MiBench benchmarks, and we show that they are representative across a range of machines with different instruction-set architectures, microarchitectures, and compilers and optimization levels, while being 30 times shorter running on average. We also verify using software plagiarism detection tools that the synthetic benchmark clones hide proprietary information from the original workloads.","","978-1-4244-9296-1978-1-4244-9297","10.1109/IISWC.2010.5650208","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5650208","","Benchmark testing;Optimization;Computer architecture;Cloning;Computer languages;Program processors;Hardware","C language;instruction sets;program compilers;software architecture","benchmark synthesis;compiler exploration;three key feature;high level programming language;synthetic benchmark;target system;proprietary information;compiler space;synthetic proxy;MiBench benchmark;instruction set architecture;microarchitecture;optimization level;software plagiarism detection tool","","12","27","","","","","","IEEE","IEEE Conferences"
"Strategies for HIRF simulations using FDTD","G. Gutiérrez; E. Pascual; J. Á. González; S. F. Romero; S. G. García","EMC & MW Department, AIRBUS MILITARY, Getafe (Madrid), Spain; EMC & MW Department, AIRBUS MILITARY, Getafe (Madrid), Spain; EMC/EMI Department, Cassidian Getafe (Madrid), Spain; EMC - Aircraft Test Group, INTA, Torrejón de Ardoz (Madrid), Spain; Dept. Electromagnetism and Material Physics, University of Granada, Granada, Spain","International Symposium on Electromagnetic Compatibility - EMC EUROPE","","2012","","","1","6","Preparing the 3D-geometry models needed to perform computational electromagnetics simulations can be very tedious and time consuming. Furthermore, the need to include the test setup in the models in order to validate the software by comparing the numerical results with the measured data often makes the models extremely big and, therefore, the computational cost of the simulation becomes unaffordable. A test case from the European FP7 HIRF-SE project intended to provide the aeronautics industry with an integrated electromagnetic compatibility simulation tool set has been used in the present study in order to identify the unnecessary elements for the simulation while retaining the essential physics of the problem. Strategies for optimizing the modeling process and guidelines for achieving the worst possible level so as to use the simulation in a certification process can be drawn from this study.","2325-0364;2325-0356;2325-0356","978-1-4673-0717-8978-1-4673-0718-5978-1-4673-0716","10.1109/EMCEurope.2012.6396705","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6396705","computational electromagnetics (CEM);electromagnetic compatibility (EMC);finite difference time domain (FDTD);electromagnetic (EM);high-intensity radiated field (HIRF);aircraft;radio frequency (RF);low-level swept current (LLSC);equipment under test (EUT);perfectly matched layers (PML)","Computational modeling;Probes;Numerical models;Finite difference methods;Atmospheric modeling;Time domain analysis;Antenna measurements","computational electromagnetics;electromagnetic compatibility;finite difference time-domain analysis","HIRF simulation;FDTD;3D-geometry model;computational electromagnetics;European FP7 HIRF-SE project;aeronautics industry;electromagnetic compatibility;high-intensity radiated field","","1","8","","","","","","IEEE","IEEE Conferences"
"A convex optimization approach to GPS receiver tracking loop design","H. Wang","Department of Communications and Guidance Engineering, National Taiwan Ocean University, Keelung 202, Taiwan","Proceedings of SICE Annual Conference 2010","","2010","","","349","354","In this paper, the application of convex optimization theory to GPS (Global Positioning System) receiver tracking loops is investigated. We design an H<sub>∞</sub>controller for the receiver tracking loop based on an LMI (Linear Matrix Inequality) approach. The H<sub>∞</sub>controller is particularly attractive because it is a robust design in the sense that small disturbances lead to small tracking errors. Furthermore, it easily accommodates the inclusion of plant uncertainties as part of the plant model. By adding unstructured or structured perturbations to the plant model, it is possible to design controller that ensure stability robustness and performance robustness of the closed-loop system. In order to apply the H<sub>∞</sub>optimal design, we first rewrite the GPS receiver tracking loop into a two-input two-output generalized plant model, and then the H<sub>∞</sub>controller is design by using an LMI approach. The LMI is a convex optimization problem in which a local solution is guaranteed to be a global minimizer. By using a software-based GPS L5 signal generator, various levels of disturbances are derived as test inputs to evaluate the performance of the resulting tracking loop.","","978-4-907764-36-4978-1-4244-7642-8978-4-907764-35","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5602681","GPS;Linear Matrix Inequality;Convex Optimization;J-Co Control;Delayed-Lock Loop","Global Positioning System;Tracking loops;Receivers;Noise;Mathematical model;Equations;Jamming","closed loop systems;control system synthesis;convex programming;Global Positioning System;H∞control;linear matrix inequalities;perturbation techniques;radio receivers;radio tracking;robust control;signal generators;telecommunication control","convex optimization approach;GPS receiver tracking loop design;linear matrix inequality;H∞controller;stability robustness;closed-loop system;H∞optimal design;software-based GPS L5 signal generator","","","7","","","","","","IEEE","IEEE Conferences"
"Less Effort, More Outcomes: Optimising Debt Recovery with Decision Trees","Y. Zhao; H. Bohlscheid; S. Wu; L. Cao","NA; NA; NA; NA","2010 IEEE International Conference on Data Mining Workshops","","2010","","","655","660","This paper presents a real-world application of data mining techniques to optimise debt recovery in social security. The traditional method of contacting a customer for the purpose of putting in place a debt recovery schedule has been an out-bound phone call, and by and large, customers are chosen at random. This obsolete and inefficient method of selecting customers for debt recovery purposes has existed for years and in order to improve this process, decision trees were built to model debt recovery and predict the response of customers if contacted by phone. Test results on historical data show that, the built model is effective to rank customers in their likelihood of entering into a successful debt recovery repayment schedule. If contacting the top 20 per cent of customers in debt, instead of contacting all of them, approximately 50 per cent of repayments would be received.","2375-9232;2375-9259","978-1-4244-9244-2978-0-7695-4257","10.1109/ICDMW.2010.114","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5693359","data mining application;decision tree;debt recovery","Decision trees;Data models;Data mining;Predictive models;Security;Software;Schedules","data mining;decision trees;financial data processing","decision trees;data mining;social security;out-bound phone call;debt recovery repayment schedule","","","6","","","","","","IEEE","IEEE Conferences"
"Development of Building Automation and Control Systems","Y. Yang; Q. Zhu; M. Maasoumy; A. Sangiovanni-Vincentelli","EECS Department, University of California, Berkeley; EE Department, University of California , Riverside; EECS Department, University of California , Berkeley; EECS Department, University of California, Berkeley","IEEE Design & Test of Computers","","2012","29","4","45","55","In this paper, we proposed a design flow for BAC systems that enables integrating heterogeneous input models, conducts automatic design space exploration, and performs software synthesis on distributed platforms while guaranteeing correctness and reducing communication load. We believe these capabilities can enable the building designers to better adopt model-based design methodologies, and facilitate them to improve design productivity, optimize system performance, and reduce cost.","0740-7475;1558-1918","","10.1109/MDT.2012.2201130","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6204325","Building Automation and Control;design space exploration;intermediate format;software synthesis","Computational modeling;Green buildings;Mathematical model;Computer architecture;Control systems;Program processors;Software engineering","building management systems;control systems","building automation;control systems;heterogeneous input model;automatic design space exploration;software synthesis;distributed platform;building designer;model based design methodology;design productivity","","11","12","","","","","","IEEE","IEEE Journals & Magazines"
"X-ray Cone-Beam Breast Computed Tomography: Phantom Studies","P. Russo; G. Mettivier; A. Lauria; M. C. Montesi","Laboratory of Medical Physics at Dipartimento di Scienze Fisiche, Sezione di Napoli, Università di Napoli Federico II, INFN, Napoli, Italy; Laboratory of Medical Physics at Dipartimento di Scienze Fisiche, Sezione di Napoli, Università di Napoli Federico II, INFN, Napoli, Italy; Laboratory of Medical Physics at Dipartimento di Scienze Fisiche, Sezione di Napoli, Università di Napoli Federico II, INFN, Napoli, Italy; Laboratory of Medical Physics at Dipartimento di Scienze Fisiche, Sezione di Napoli, Università di Napoli Federico II, INFN, Napoli, Italy","IEEE Transactions on Nuclear Science","","2010","57","1","160","172","We present the design and preliminary imaging evaluation of a bench-top apparatus for X-ray cone-beam breast computed tomography (CBBCT), assembled for technical evaluation and for laboratory tests of various optimization techniques for CBBCT. The prototype is characterized by the computer control of the X-ray tube (W anode, 35-80 kVp, 0.25 mA, 50  ¿m focal spot size), CsI:Tl CMOS flat panel detector (12×12 cm<sup>2</sup>area, 50 ¿m pitch, up to 9 fps at 4 ×  4 binning), motorized translation and rotation stages with eight degrees of freedom, and fan-beam or cone-beam backprojection software. First cone-beam tomographic images of two 14-cm diameter hemiellipsoidal PMMA breast phantoms are presented.","0018-9499;1558-1578","","10.1109/TNS.2009.2034373","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5410015","Biomedical X-ray imaging;breast computed tomography;CMOS flat panel detector","Breast;Computed tomography;X-ray imaging;Imaging phantoms;Optical imaging;Assembly;Laboratories;Testing;Design optimization;Size control","biological organs;CMOS image sensors;computerised tomography;flat panel displays;gynaecology;optimisation;phantoms","X-ray cone-beam breast computed tomography;phantom;imaging evaluation;bench-top apparatus;optimization techniques;X-ray tube;CsI:Tl CMOS flat panel detector;motorized translation;rotation stages;fan-beam backprojection software;hemiellipsoidal PMMA breast phantoms;voltage 35 kV to 80 kV;current 0.25 mA;size 14 mum","","20","28","","","","","","IEEE","IEEE Journals & Magazines"
"High-Speed Data-Injection for Data-Flow Verification at LHCb","O. Callot; M. Frank; J. Garnier; C. Gaspar; G. Liu; N. Neufeld; A. Sambade Varela; A. C. Smith; D. Sonnick","LAL, Orsay, France; CERN, Geneva, Switzerland; CERN, Geneva, Switzerland; CERN, Geneva, Switzerland; CERN, Geneva, Switzerland; CERN, Geneva, Switzerland; CERN, Geneva, Switzerland; CERN, Geneva, Switzerland; CERN, Geneva, Switzerland","IEEE Transactions on Nuclear Science","","2010","57","2","497","502","The High Level Trigger (HLT) and Data Acquisition System select about 2 kHz of events out of the 40 MHz of beam crossings. The selected events are consolidated into files in onsite storage and then sent to permanent storage for subsequent analysis on the Grid. For local and full-chain tests a method to exercise the data-flow through the High Level Trigger is needed in the absence of real data. In order to test the system as much as possible under identical conditions as for data-taking, the solution would be to inject data at the input of the HLT at a minimum rate of 2 kHz. This is done via a software implementation of the trigger system which sends data to the HLT. The application has to simulate that the data it sends come from real LHCb readout-boards. Data can come from several input streams, which are selected according to probabilities or frequencies. Therefore the emulator offers runs which are not only identical data-flows coming from a sequence on tape, but physics-like pseudo-indeterministic data-flow, including lumi events and candidate b-quark events. Both simulation data and previously recorded real data can be re-played through the system in this manner. As the data rate is high (100 MB/s), care has been taken to optimize the emulator for throughput from the Storage Area Network. The emulator can be run in stand-alone mode, but even more interesting is that it can emulate any partition of LHCb in parallel with the real hardware partition. In this mode it is fully integrated into the standard run-control. The architecture, implementation, and performance results of the emulator and full tests will be presented. This emulator is a crucial part of the ongoing data-challenges in LHCb. Results from these Full System Integration Tests (FEST) will be presented, which helped to verify and benchmark the entire LHCb data-flow.","0018-9499;1558-1578","","10.1109/TNS.2009.2038216","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5446503","Benchmark;data acquisition;high level trigger;simulation","Data acquisition;Detectors;System testing;Application software;Frequency;Throughput;Storage area networks;Hardware;Benchmark testing","data acquisition;nuclear electronics;readout electronics;trigger circuits","high-speed data-injection;data-flow verification;high level trigger;data acquisition system;onsite storage;subsequent analysis;Grid;software implementation;trigger system;LHCb readout-boards;pseudoindeterministic data-flow;storage area network;stand-alone mode;hardware partition;standard run-control;full system integration tests;FEST","","1","21","","","","","","IEEE","IEEE Journals & Magazines"
"SoftAccel: A Software-Only IP Network Application Accelerator","H. Zhang; Y. Bai; Z. Li; H. Peng; X. Liu; L. Zhao; W. Wu; L. Luo","NA; NA; NA; NA; NA; NA; NA; NA","2010 IEEE Asia-Pacific Services Computing Conference","","2010","","","411","415","Internet technologies are making rapid progress, but achieving high performance of network application over the WAN remains a challenge. When branch office's workers access enterprise applications locating in data center, the performances drop dramatically. To solve this problem, many hardware accelerators are produced. In this paper, we introduce a Software-only IP Network Application Accelerator (Soft Accel), for improving network applications performance over the WAN. To overcome the performance-limiting factors associated with WAN, transport protocol and application protocol, Soft Accelcombines application proxy, delta compress and TCPoptimization techniques. We have implemented Soft Accel in the form of framework, so people can design and test their optimization technologies based on it. Experiments in WAN environment show that Soft Accel can improve the performance of IP network applications at least by 2x.","","978-1-4244-9396","10.1109/APSCC.2010.47","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5708599","WAN optimization;delta compression;application acceleration","Conferences","computer centres;Internet;IP networks;transport protocols;wide area networks","software-only IP network application accelerator;Internet technology;WAN;data center;hardware accelerator;performance-limiting factor;transport protocol;application protocol;application proxy technique;delta compress technique;TCP optimization technique","","1","12","","","","","","IEEE","IEEE Conferences"
"A new input-output based model coverage paradigm for control blocks","C. Chethan; Y. Jeppu; S. Hariram; N. Narayan Murthy; P. R. Apte","Moog India Technology Center, Plot No 1,2 and 3, Electronic City, Hosur Road, Bangalore, 560100, India; Moog India Technology Center, Plot No 1,2 and 3, Electronic City, Hosur Road, Bangalore, 560100, India; Moog India Technology Center, Plot No 1,2 and 3, Electronic City, Hosur Road, Bangalore, 560100, India; Moog India Technology Center, Plot No 1,2 and 3, Electronic City, Hosur Road, Bangalore, 560100, India; Department of Electrical Engineering, Indian Institute of Technology, Bombay, Mumbai, 400076, India","2011 Aerospace Conference","","2011","","","1","12","Safety critical control systems, especially flight control systems, have failed in trials and actual flights sometimes killing people. Independence is a key word in the verification and validation activity of such systems. Model based testing is used today in all flight control programs. Test cases are generated using qualified automated tools or manually to ensure code and requirements coverage. A new paradigm of coverage metric is developed here for the independent verification. This has been developed for control systems represented as Simulink blocks. The coverage metrics are defined as pairs of cells. One cell has a True/False discrete metric, which indicates if a particular functionality has been covered. The other cell has the continuous metric, which indicates a distance to nominal, or coverage. The efficacy of these metrics is determined by tests using mutant code. Matlab code corresponding to the model is mutated to generate several mutant files. It is shown that the test cases developed using this coverage metric bring out the errors introduced in the mutant files. It was possible to use these metrics to optimize test cases successfully using the Taguchi Design of Experiments methodology.","1095-323X;1095-323X","978-1-4244-7351-9978-1-4244-7350-2978-1-4244-7349","10.1109/AERO.2011.5747530","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5747530","","Measurement;Mathematical model;Unified modeling language;Control systems;Safety;Software;Certification","aerospace control;aerospace safety;design of experiments;program testing;safety-critical software;Taguchi methods","input output based model coverage paradigm;control block;safety critical control system;flight control system;model based testing;Simulink blocks;coverage metrics;Matlab code;Taguchi design of experiment method","","2","32","","","","","","IEEE","IEEE Conferences"
"A survey on bee colony algorithms","S. Bitam; M. Batouche; E. Talbi","Computer science department, Mohamed Khider University, Biskra, Algeria; COEIA - CCIS, King Saud University, Riyadh, Saudi Arabia; COEIA - CCIS, King Saud University, Riyadh, Saudi Arabia","2010 IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum (IPDPSW)","","2010","","","1","8","This paper presents a survey of current research activities inspired by bee life. This work is intended to provide a broad and comprehensive view of the various principles and applications of these bio-inspired systems. We propose to classify them into two major models. The first one is based on the foraging behavior in the bee quotidian life and the second is inspired by the marriage principle. Different original studies are described and classified along with their applications, comparisons against other approaches and results. We then summarize a review of their derived algorithms and research efforts.","","978-1-4244-6534-7978-1-4244-6533-0978-1-4244-6532","10.1109/IPDPSW.2010.5470701","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5470701","metaheurstic;optimization;bee inspiration;foraging behavior;marriage behavior","Biological system modeling;Insects;Evolution (biology);Ant colony optimization;Radiofrequency interference;Computer science;Application software;Testing;Biology computing;Computational biology","optimisation","bee colony algorithms;foraging behavior;bee quotidian life;marriage principle;bio-inspired systems","","19","42","","","","","","IEEE","IEEE Conferences"
"Evaluation and optimization of Java object ordering schemes","A. A. Ilham; K. Murakami","Institute of Systems, Information Technologies and Nanotechnologies, Department of Advanced Information Technology, Kyushu University, Fukuoka, Japan; Institute of Systems, Information Technologies and Nanotechnologies, Department of Advanced Information Technology, Kyushu University, Fukuoka, Japan","Proceedings of the 2011 International Conference on Electrical Engineering and Informatics","","2011","","","1","6","Java is gaining popularity in software development. It is widely used in network computing and embedded systems because it offers several key advantages such as safe programming, code verification and checking, automatic memory management, and significant support from the computing industry. As the popularity of Java has rapidly increased, research on Java application performance is continuously of interest to the research community. This paper aims to evaluate and optimize the order of Java objects in memory in order to give a good impact on Java application performance. We evaluate how the order of Java objects in memory affects cache performance, DTLB performance and Java application execution time. This work is motivated by the facts that Java programs create many objects dynamically on the heap, access and mutate them during runtime. These objects might be spread across the memory since they are not necessarily resided in adjacent memory locations. To show how the order of objects in memory affects Java application performance, we use the garbage collector, automatic memory management in Java Virtual Machine (JVM), to reorganize the order of live objects during garbage collection time. We implemented two different object ordering schemes upon the invocation of copying garbage collector: Bread First (BF) scheme and Depth First (DF) scheme. Our experiment results show that Java execution time, cache misses and DTLB misses vary by 3-16%, 5-20% and 9-21% respectively due to BF and DF schemes. We optimize the order of frequently accessed objects and the results show that this optimized scheme has a better impact on Java application performance compared with the DF scheme.","2155-6830;2155-6822;2155-6822","978-1-4577-0752-0978-1-4577-0753-7978-1-4577-0751","10.1109/ICEEI.2011.6021558","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6021558","Java;object;performance","Java;Runtime;Memory management;Optimization;Object recognition;Benchmark testing;Virtual machining","Java;object-oriented programming;storage management;virtual machines","Java object ordering scheme;software development;Java application performance;cache performance;DTLB performance;Java application execution time;heap object;access object;mutate object;Java virtual machine;garbage collector;memory management;bread first scheme;depth first scheme","","1","15","","","","","","IEEE","IEEE Conferences"
"Stepper motor drives for robotic applications","B. Aranjo; P. K. Soori; P. Talukder","Electronic and Electrical Engineering student at Heriot Watt University, Dubai, U.A.E.; Department of Electrical Engineering, School of Engineering &amp; Physical Sciences at Heriot Watt University, Dubai, U.A.E.; Electronic and Electrical Engineering student at Heriot Watt University, Dubai, U.A.E.","2012 IEEE International Power Engineering and Optimization Conference Melaka, Malaysia","","2012","","","361","366","Robotics is one area where the use of stepper motors is of paramount importance. Robotic applications require precise movements. This paper presents an efficient and versatile drive system for stepper motors for use in robotic applications. The proposed drive system delivers precise control with the smallest possible step angle. A model of the permanent magnet stepper motor is simulated using MATLAB Simulink simulation software. The software and the hardware circuits of the stepper motor drive is designed and tested for different types of movements of the stepper motor. The proposed circuitry consists of the design of power supply circuit, microcontroller, buffer and a driver circuit. Unlike a manual controller, the AT89C51 microcontroller is programmed to have user friendliness so that the user can select the required movement of the stepper motor. The drive system allows the movement of the stepper motor to be controlled by selecting options of half or full step, forward or reverse movement and the speed in RPM or the fixed number of steps that it should move.","","978-1-4673-0662-1978-1-4673-0660-7978-1-4673-0661","10.1109/PEOCO.2012.6230890","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6230890","Drive system;Microcontroller;Robotic applications;Speed control;Stepper motor","Permanent magnet motors;Pins;Microcontrollers;Robots;Integrated circuit modeling;Power supplies","buffer circuits;driver circuits;microcontrollers;motor drives;permanent magnet motors;power supply circuits;robots;stepping motors","stepper motor drives;robotic applications;drive system;step angle;permanent magnet stepper motor;MATLAB Simulink simulation software;hardware circuits;power supply circuit;buffer circuit;driver circuit;manual controller;AT89C51 microcontroller;forward movement;reverse movement","","6","4","","","","","","IEEE","IEEE Conferences"
"Hybrid Genetic Algorithm fuzzy rule based guidance and control for launch vehicle","U. Ansari; S. Alam","Aeronautics &amp; Astronautics Dept, Institute of Space Technology, Karachi, Pakistan; Aeronautics &amp; Astronautics Dept, Institute of Space Technology, Karachi, Pakistan","2011 11th International Conference on Intelligent Systems Design and Applications","","2011","","","178","185","This paper presents a methodology of designing guidance and control law for four stage launch vehicle. Design of Guidance and Control system for nonlinear dynamic systems is an arduous task and various approaches have been attempted in the past to address coupled system dynamics and nonlinearities. In this paper, the approach of Guidance and Control is exclusively based on fuzzy rule-based mechanism. Since fuzzy logics are more identical to human decision making so, this strategy enhances robustness and reliability in guidance and control mechanism and meet the flight objectives tactfully and manage vehicle energy. For inner loop, fuzzy rule based autopilot is designed which precisely follows the reference pitch attitude profile. The reference pitch profile is constantly reshaped online by a fuzzy rule based guidance to achieve desired altitude. Since reshaping attitude has no great influence on velocity; therefore, an engine shutoff mechanism has been implemented depending on the magnitude of semi major axis during the 3rd and final stage to gain the required orbital velocity. To design optimal fuzzy rule based control and guidance algorithm, the distribution of the membership functions of fuzzy inputs and output are obtained by solving constraint optimization problem using Genetic Algorithms (GA). To get nominal trajectory profiles for proposed scheme, offline trajectory optimization is performed primarily. To acquire optimal AOA profile, Optimization problem is solved by using Genetic Algorithm. To analyze the flight path of the vehicle, a point mass trajectory model is developed. In this model constant thrust and mass flow rate are assumed while the aerodynamic coefficients are calculated by DATCOM. For performance evaluation and validation of proposed guidance and control algorithm, a Six Degree of Freedom software is developed and simulated in SIMULINK. Numerous simulations are conducted to test the proposed scheme for a variety of disturbances and modeling uncertainties.","2164-7151;2164-7143;2164-7143","978-1-4577-1676-8978-1-4577-1676-8978-1-4577-1675","10.1109/ISDA.2011.6121651","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6121651","Trajectory Optimization;Fuzzy Control;Fuzzy Guidance;Genetic Algorithms;Launch Vehicle Guidance and Control;GA","Vehicles;Trajectory;Genetic algorithms;Optimization;Fuzzy systems;Niobium;Aerodynamics","aerodynamics;aerospace engines;aircraft control;attitude control;control nonlinearities;control system synthesis;decision making;fuzzy control;fuzzy logic;genetic algorithms;nonlinear dynamical systems;uncertain systems","hybrid genetic algorithm;fuzzy rule based guidance;launch vehicle control;four stage launch vehicle;guidance system design;control system design;nonlinear dynamic system;coupled system dynamics;nonlinearities;fuzzy logics;human decision making;fuzzy rule based autopilot;reference pitch attitude profile;reshaping attitude;engine shutoff mechanism;optimal fuzzy rule based control design;membership function;constraint optimization problem;nominal trajectory profiles;offline trajectory optimization;angle-of-attack profile;flight path analysis;point mass trajectory model;constant thrust;mass flow rate;aerodynamic coefficients;DATCOM;six degree of freedom software;SIMULINK;modeling uncertainties","","7","22","","","","","","IEEE","IEEE Conferences"
"Speed control of induction motor drive using universal controller","P. Talukder; P. K. Soori; B. Aranjo","Electronic and Electrical Engineering student at Heriot Watt University, Dubai, U.A.E.; Department of Electrical Engineering, School of Engineering &amp; Physical Sciences at Heriot Watt University, Dubai, U.A.E.; Electronic and Electrical Engineering student at Heriot Watt University, Dubai, U.A.E.","2012 IEEE International Power Engineering and Optimization Conference Melaka, Malaysia","","2012","","","509","514","When connected to main power supply, induction motors run at their rated speed, however there are many applications where variable speed operations are required. Although a range of induction motor control techniques are available, generating variable frequency supply is a popular control technique, having a constant voltage to frequency ratio in order to attain constant (maximum) torque throughout the operating period. This control technique is called as variable frequency control. The main aim of this honours degree project paper emphasizes on the development of a general purpose universal board that is capable of controlling the speed of single or three phase induction motor with minor software and hardware modifications. The absolute system consists of the control, driver and the power circuits. The control circuit includes the power supply circuit and the microcontroller. The power circuit includes the full-bridge single-phase Pulse Width Modulation inverter. Simulation was done using MATLAB Simulink software. The system was implemented, tested and the experimental results are examined and discussed.","","978-1-4673-0662-1978-1-4673-0660-7978-1-4673-0661","10.1109/PEOCO.2012.6230919","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6230919","Induction Motor;Microcontroller;Pulse Width Modulation Inverter;Speed Control","Inverters;Induction motors;Microcontrollers;Stators;Power supplies;Frequency control;Integrated circuit modeling","angular velocity control;frequency control;induction motor drives;machine control","speed control;induction motor drive;universal controller;power supply;induction motor control techniques;variable frequency supply;constant voltage;frequency ratio;constant torque;variable frequency control;general purpose universal board;three phase induction motor;power circuits;driver;power supply circuit;control circuit;microcontroller;full-bridge single-phase pulse width modulation inverter;Matlab Simulink software;absolute system","","1","5","","","","","","IEEE","IEEE Conferences"
"Optimization of the magnetic circuit of the homopolar inductor machine with non-overlapping concentrated windings","N. Levin; S. Orlova; V. Pugachov; L. Ribickis","Institute of Physical Energetics, Riga, Latvia; Institute of Physical Energetics, Riga, Latvia; Institute of Physical Energetics, Riga, Latvia; Riga Technical university, Riga, Latvia","Proceedings of 14th International Power Electronics and Motion Control Conference EPE-PEMC 2010","","2010","","","T4-77","T4-81","The present work considers issues about optimization of the tooth zone of homopolar inductor generator with non-overlapping concentrated coils. The recommendations for the choice of optimal parameters of the tooth zone are made on the basis of the analysis of magnetic field in the machine's cross-section. The results of the experiment data of the inductor generator's test sample with the power of 32 kW are also presented in the work.","","978-1-4244-7855-2978-1-4244-7856-9978-1-4244-7854","10.1109/EPEPEMC.2010.5606551","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5606551","AC machine;design;electrical machine;modelling;software","Teeth;Rotors;Windings;Generators;Inductors;Stator windings","asynchronous generators;machine windings;magnetic circuits","magnetic circuit;homopolar inductor machine;nonoverlapping concentrated windings;tooth zone;homopolar inductor generator;nonoverlapping concentrated coils;magnetic field;power 32 kW","","2","7","","","","","","IEEE","IEEE Conferences"
"Software, electronics and mechanical components co-simulation for efficient design","Yicheng Zhang; Nuo Li; S. Zahrai; Hui Zhang","R &amp; D Department of Shanghai ABB Engineering Ltd., China 201319; ABB Corporate Research Center China, Shanghai, China 201319; R &amp; D Department of Shanghai ABB Engineering Ltd., China 201319; R &amp; D Department of Shanghai ABB Engineering Ltd., China 201319","2011 IEEE International Conference on Robotics and Automation","","2011","","","1","4","A method and a tool for simulation of a control system are presented. The tool allows integration of software components, electronics and mechanical devices into the same package and makes it possible to simulate the complete system accounting for all time-discretizations and physical delays in an accurate way. The software used for control of the virtual system is exactly the same as that used in the final product. The complete system can be prototyped in a virtual world, be tested and optimized long before a physical prototype is built.","1050-4729;1050-4729;1050-4729","978-1-61284-385-8978-1-61284-386-5978-1-61284-380","10.1109/ICRA.2011.5980589","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5980589","","","control engineering computing;delays;discrete time systems;mechanical products;mechatronics;virtual prototyping","control system simulation tool;software components;electronic devices;mechanical devices;time-discretizations;physical delays;virtual system","","","6","","","","","","IEEE","IEEE Conferences"
"Application specific sensor node architecture optimization—Experiences from field deployments","Wei Liu; Xiaotian Fei; Tao Tang; Pengjun Wang; Hong Luo; Beixing Deng; Huazhong Yang","Department of Electronic Engineering, Tsinghua University, Beijing, China 100084; Department of Electronic Engineering, Tsinghua University, Beijing, China 100084; Department of Electronic Engineering, Tsinghua University, Beijing, China 100084; Department of Electronic Engineering, Tsinghua University, Beijing, China 100084; Department of Electronic Engineering, Tsinghua University, Beijing, China 100084; Department of Electronic Engineering, Tsinghua University, Beijing, China 100084; Department of Electronic Engineering, Tsinghua University, Beijing, China 100084","17th Asia and South Pacific Design Automation Conference","","2012","","","389","394","The Mote architecture is the most popular platform used in wireless sensor network applications. In this architecture, microcontroller is responsible for all jobs, such as scheduling, sampling, computing, and communication. In the past one year, two practical applications: bridge structural health monitoring system and rare animal monitoring system are developed and deployed in Wuxi and Beijing, China. It is found that Mote architecture faces many problems in these applications. First, sampling, computing, and communication conflicts with each other if they are not carefully scheduled; second, some jobs are very difficult even impossible to be implemented in the microcontroller; third, low power, one of the most fundamental design principles in wireless sensor networks, is sometimes violated with all jobs implemented in the microcontroller. Software optimization is attempted to solve these problems. However, the effect is very limited. Application specific sensor node architecture is necessary for implementing these applications efficiently. In this paper, we propose new application specific sensor node architecture and corresponding design principles and then applied them in the field deployments. Experimental and field tests show that these architectures are more efficient than Mote architecture in these applications.","2153-697X;2153-6961;2153-6961","978-1-4673-0772-7978-1-4673-0770-3978-1-4673-0771","10.1109/ASPDAC.2012.6164979","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6164979","","Computer architecture;Monitoring;Wireless sensor networks;Microcontrollers;Wireless communication;Bridges;Vibrations","microcontrollers;optimisation;wireless sensor networks","field deployments;Mote architecture;wireless sensor network applications;microcontroller;bridge structural health monitoring system;rare animal monitoring system;software optimization;application specific sensor node architecture","","6","21","","","","","","IEEE","IEEE Conferences"
"Balance and single line to ground fault location in electrical distribution system","Misagh Alaie Faradonbeh; Hazlie Bin Mokhlis; M. Karimi; A. Shahariari","Electrical Energy and Power System, Malaya University, Malaysia; Electrical Engineering Department, Malaysia; University of Malaya, Malaysia; University of Malaya, Malaysia","2011 5th International Power Engineering and Optimization Conference","","2011","","","1","5","This paper outlines impedance based technique and its application for determining the location of a fault on a distribution system. A few well-know fault location methods are first discussed and analyzed to highlight its practicality as compared to impedance based technique. The practicality of the impedance based technique in locating a fault is shown by testing it on a Canadian distribution network. The algorithm of the impedance based technique is developed and integrated into PSCAD/EMTDC software by using C-interface and FORTRAN Language. Various test cases showed the practicality of the impedance based technique in locating a fault.","","978-1-4577-0354-6978-1-4577-0355-3978-1-4577-0353","10.1109/PEOCO.2011.5970382","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5970382","Fault Location;Balanced Fault;Distribution Network;Single Line to Ground Fault","Impedance;Circuit faults;Voltage measurement;Current measurement;Fault location;Power systems;Impedance measurement","fault location;FORTRAN;power distribution faults;power system CAD","electrical distribution system;impedance based technique;Canadian distribution network;PSCAD/EMTDC software;C-interface;FORTRAN language;single line to ground fault location","","","15","","","","","","IEEE","IEEE Conferences"
"Notice of Retraction<BR>The prediction model of graded gravel resilient modulus</BR>","X. Wang; G. Li","Postdoctoral workstation, Chinese First Institute Limited Corporation of, Highway Survey & Design, Xi'an, China; Institute of pavement, Chinese First Institute Limited Corporation of Highway Survey & Design, Xi'an, China","2011 International Conference on Electric Technology and Civil Engineering (ICETCE)","","2011","","","7015","7018","According to the nonlinear characteristic of graded gravel resilient modulus, Based on the optimization of the three kinds of graded gravel as the research object, and combined with repeated dynamic triaxial test data, compared k~θ model and Uzan model, using statistical analysis software lstopt carries on the experimental parameters of nonlinear regression, obtained the correlative coefficient of three graded gravel resilient modulus, which is under the two models. Experimental results show that: Uzan model can more accurately reflects the nonlinear deformation behavior of graded gravel resilient, so the reliability of the regression modulus prediction equation for graded gravel is higher, can be used to forecast China's Graded Aggregate Base modulus.;Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR/></BR></BR></BR></BR></BR></BR></BR>","","978-1-4577-0290-7978-1-4577-0289-1978-1-4577-0288","10.1109/ICETCE.2011.5775762","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5775762","graded gravel;nonlinear regression;resilient modulus;prediction model","Mathematical model;Stress;Predictive models;Data models;Load modeling;Aggregates","aggregates (materials);building materials;deformation;materials testing;regression analysis","graded gravel resilient modulus;dynamic triaxial test data;statistical analysis software;correlative coefficient;Uzan model;nonlinear deformation behavior;nonlinear regression modulus prediction equation;China graded aggregate base modulus;k~θ model","","","5","","","","","","IEEE","IEEE Conferences"
"Modified real-value negative selection algorithm and its application on fault diagnosis","Y. F. Li; G. H. Chang; C. J. Zhang; S. H. Liang","College of Naval Architecture and Power, Naval University of Engineering, Wuhan, China; College of Naval Architecture and Power, Naval University of Engineering, Wuhan, China; College of Naval Architecture and Power, Naval University of Engineering, Wuhan, China; College of Naval Architecture and Power, Naval University of Engineering, Wuhan, China","2011 IEEE 2nd International Conference on Software Engineering and Service Science","","2011","","","216","219","Analyze the drawbacks of common real-value negative selection algorithm applied on fault diagnosis, and the modified real-value negative selection algorithm is presented based on the corresponding innovations. Firstly, the fault detector set is partitioned into remember-detector set covering known-fault space and random-detector set covering unknown-fault space. Secondly, taking all known states including normal state as self set in training period, get the random-detector set through negative selection and distribution optimization. Lastly, in order to avoid `Fail to Alarm' event caused by the Hole, the two-time-matching method is presented in detecting period which takes the normal state as self set. A resistance circuit fault diagnosis experiment shows that compared with the common real-value negative selection algorithm, the modified real-value negative algorithm can effectively avoid `Fail to Alarm' event, and has higher diagnostic accuracy.","2327-0586;2327-0594","978-1-4244-9698-3978-1-4244-9699-0978-1-4244-9697","10.1109/ICSESS.2011.5982293","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5982293","Negative Selection;Artificial Immune System;Fault Diagnosis","Detectors;Fault detection;Circuit faults;Fault diagnosis;Training;Testing;Immune system","artificial immune systems;fault diagnosis","modified real value negative selection algorithm;fault detector set;remember detector set;random detector set;unknown fault space;distribution optimization;two time matching method;resistance circuit fault diagnosis experiment;real value negative selection algorithm;modified real value negative algorithm;fail to alarm event","","1","7","","","","","","IEEE","IEEE Conferences"
"Model-based dependency analysis in service delivery process management","F. Liu; Q. Ma; K. Ratakonda; H. Wang; L. Liu; Y. Chen","IBM Research - China, Building 19 Zhongguancun Software Park, 8 Dongbeiwang West Road, Haidian District, Beijing 100193, China; IBM Research - China, Building 19 Zhongguancun Software Park, 8 Dongbeiwang West Road, Haidian District, Beijing 100193, China; IBM Thomas J. Watson Research Center, Hawthorne, New York, US; IBM Research - China, Building 19 Zhongguancun Software Park, 8 Dongbeiwang West Road, Haidian District, Beijing 100193, China; IBM Research - China, Building 19 Zhongguancun Software Park, 8 Dongbeiwang West Road, Haidian District, Beijing 100193, China; IBM Research - China, Building 19 Zhongguancun Software Park, 8 Dongbeiwang West Road, Haidian District, Beijing 100193, China","Proceedings of 2010 IEEE International Conference on Service Operations and Logistics, and Informatics","","2010","","","17","22","Build up well-defined and optimized service process is the key to deliver good service quality and service satisfaction. An effective method of dependency analysis in the service delivery process is the core to construct an optimized process. However, with increasingly complicated services, there is the large amount of the tasks elements with extreme complex dependency in the service process. Under this situation, set up correct relationship among tasks becomes time consuming and error prone. In this paper, we propose a model-based dependency analysis to automatically build up the dependency relationship among tasks in the service delivery process. It addresses the problem in analyzing dependency firstly and our approach is given. Based on the dependency analysis, we also propose several advanced analysis features on the process to guide user to optimize the process via reducing the process cost. Also a tool adopting our approach has been implemented and introduced. Based on the tool, a case study about the test service delivery process is illustrated to show the results.","","978-1-4244-7119-5978-1-4244-7118-8978-1-4244-7117","10.1109/SOLI.2010.5551624","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5551624","","","customer satisfaction;customer services","model-based dependency analysis;service delivery process management;service satisfaction;good service quality","","1","12","","","","","","IEEE","IEEE Conferences"
"Automotive EMC case study: HMI graphics influence on radiated emissions","S. Mainville","Johnson Controls, Inc., Automotive Experience, Holland, MI USA","2010 IEEE International Symposium on Electromagnetic Compatibility","","2010","","","620","624","A case study describing the influence of TFT LCD graphics on automotive radiated emissions testing and its impact to the FM band receiver. The underlying root-cause of the EMI issue is determined using a novel technique that decodes the display's graphics into the transmitted RGB data and predicts the data's impact to radiated emissions. The countermeasure implemented to resolve the issue is equally novel and does not involve any hardware optimizations. The use of test images to be used in component level EMC testing is also discussed.","2158-1118;2158-110X;2158-110X","978-1-4244-6308-4978-1-4244-6305-3978-1-4244-6307","10.1109/ISEMC.2010.5711348","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5711348","","Graphics;Pixel;Image color analysis;Software;Electromagnetic compatibility;Clocks;Hardware","computer graphics;electromagnetic compatibility;electromagnetic interference;frequency modulation;liquid crystal displays;man-machine systems;optimisation;thin film transistors","EMC;HMI;TFT;LCD graphics;automotive radiated emissions;FM;receiver;EMI;RGB;optimization","","4","5","","","","","","IEEE","IEEE Conferences"
"Google's C/C++ toolchain for smart handheld devices","Doug Kwan; Jing Yu; B. Janakiraman","Google Inc., 1600 Amphitheatre Parkway, Mountain View, CA, USA; Google Inc., 1600 Amphitheatre Parkway, Mountain View, CA, USA; Google Inc., 1600 Amphitheatre Parkway, Mountain View, CA, USA","Proceedings of Technical Program of 2012 VLSI Design, Automation and Test","","2012","","","1","4","Smart handheld devices are ubiquitous today and software plays an important role on them. Therefore a compiler and related tools can improve devices by generating efficient, compact and secure code. In this paper, we share our experience of applying various compilation techniques at Google to improve software running on smart handheld devices, using our mobile platforms as examples. At Google we use the GNU toolchain for generating code on different platforms and for conducting compiler research and development. We have developed new techniques, added features and functionality in the GNU tools. Some of these results are now used for smart handheld devices.","","978-1-4577-2081-9978-1-4577-2080-2978-1-4577-2079","10.1109/VLSI-DAT.2012.6212583","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6212583","","Optimization;Google;Handheld computers;Benchmark testing;Software;Mobile communication;Performance evaluation","C++ language;mobile computing;program compilers;smart phones","Google;C/C++ toolchain;smart handheld devices;compilation techniques;mobile platforms;GNU toolchain;compiler research","","","29","","","","","","IEEE","IEEE Conferences"
"Model predictive control for energy and leakage management in water distribution systems","P. Skworcow; B. Ulanicki; H. AbdelMeguid; D. Paluszczyszyn","Process Control - Water Software System, De Montfort University Leicester, UK; Process Control - Water Software System, De Montfort University Leicester, UK; Process Control - Water Software System, De Montfort University Leicester, UK; Process Control - Water Software System, De Montfort University Leicester, UK","UKACC International Conference on Control 2010","","2010","","","1","6","This paper considers development of a methodology for an on-line energy and leakage management in water distribution systems, formulated within a model predictive control framework. The approach involves calculation of control actions, i.e. time schedules for pumps, valves and sources, to minimize the costs associated with energy used for water pumping and treatment and water losses due to leakage, whilst satisfying all operational constraints. The process of computing the control action utilises EPAnet hydraulic simulator, a mathematical modelling language called GAMS and a non-linear programming solver called CONOPT. The proposed control scheme has been integrated with an industrial SCADA system from ABB and interfaced with an actual medium-scale water distribution systems being part of Yorkshire Water Services. The scheme is currently being tested using on-line telemetry data. It has been operational for over 1 month with 1 hour sampling time and the preliminary results described in this paper indicate a potential for savings of 30% of the cost of electrical energy.","","978-1-84600-038","10.1049/ic.2010.0416","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6490874","Model predictive control;water distribution systems;pump control;SCADA;nonlinear optimisation","","control engineering computing;costing;hydraulic control equipment;nonlinear programming;predictive control;pumps;SCADA systems;specification languages;valves;water supply","online energy and leakage management;model predictive control;control action;time schedule;valve;cost minimization;water pumping;water loss;operational constraint;EPAnet hydraulic simulator;mathematical modelling language;GAMS;nonlinear programming solver;CONOPT;industrial SCADA system;ABB;medium-scale water distribution system;Yorkshire Water Services;online telemetry data;electrical energy cost","","1","","","","","","","IET","IET Conferences"
"Impact of Markov Random Field optimizer on MRI-based tissue segmentation in the aging brain","C. G. Schwarz; A. Tsui; E. Fletcher; B. Singh; C. DeCarli; O. Carmichael","Computer Science Department, University of California, Davis, CA 95616; Computer Science Department, University of California, Davis, CA 95616; Neurology Department, University of California, Davis, CA 95616; Neurology Department, University of California, Davis, CA 95616; Neurology Department, University of California, Davis, CA 95616; Computer Science Department, University of California, Davis, CA 95616","2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society","","2011","","","7812","7815","Automatically segmenting brain magnetic resonance images into grey matter, white matter, and cerebrospinal fluid compartments is a fundamentally important neuroimaging problem whose difficulty is heightened in the presence of aging and neurodegenerative disease. Current methods overlap greatly in terms of identifiable algorithmic components, and the impact of specific components on performance is generally unclear in important real-world scenarios involving serial scanning, multiple scanners, and neurodegenerative disease. Therefore we evaluated the impact that one such component, the Markov Random Field (MRF) optimizer that encourages spatially-smooth tissue labelings, has on brain tissue segmentation performance. Two challenging elderly data sets were used to test segmentation consistency across scanners and biological plausibility of tissue change estimates; and a simulated young brain data set was used to test accuracy against ground truth. Belief propagation (BP) and graph cuts (GC), used as the MRF optimizer component of a standardized segmentation system, provide high segmentation performance on aggregate that is competitive with end-to-end systems provided by SPM and FSL (FAST) as well as the more traditional MRF optimizer iterated conditional modes (ICM). However, the relative performance of each method varied strongly by performance criterion and differed between young and old brains. The findings emphasize the unique difficulties involved in segmenting the aging brain, and suggest that optimal algorithm components may depend in part on performance criteria.","1558-4615;1094-687X;1557-170X;1557-170X","978-1-4577-1589-1978-1-4244-4121-1978-1-4244-4122","10.1109/IEMBS.2011.6091925","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6091925","","Image segmentation;Aging;Magnetic resonance imaging;Brain models;Labeling","belief networks;biological tissues;biomedical MRI;brain;graph theory;image segmentation;iterative methods;Markov processes;medical image processing;neurophysiology","Markov random field optimizer;MRI;tissue segmentation;aging brain;image segmentation;magnetic resonance images;grey matter;white matter;cerebrospinal fluid compartments;neuroimaging;neurodegenerative disease;serial scanning;multiple scanners;spatially-smooth tissue labelings;tissue change estimates;belief propagation;graph cuts;iterated conditional modes","Aged;Aging;Algorithms;Brain;Computer Simulation;Humans;Image Processing, Computer-Assisted;Magnetic Resonance Imaging;Markov Chains;Software","","23","","","","","","IEEE","IEEE Conferences"
"Occurrence probability analysis of a path at the architectural level","D. Jayaraman; S. Tragoudas","ECE Dept., Southern Illinois University Carbondale, Carbondale, IL-62901; ECE Dept., Southern Illinois University Carbondale, Carbondale, IL-62901","2011 12th International Symposium on Quality Electronic Design","","2011","","","1","5","In this paper, we propose an algorithm to compute the occurrence probability for a given path precisely in an acyclic synthesizable VHDL or software code. This can be useful for the ranking of critical paths and in a variety of problems that include compiler-level architectural optimization and static timing analysis for improved performance. Functions that represent condition statements at the basic blocks are manipulated using Binary Decision Diagrams (BDDs). Experimental results show that the proposed method outperforms the traditional Monte Carlo simulation approach. The later is shown to be non-scalable as the number of inputs increases.","1948-3295;1948-3287;1948-3287","978-1-61284-914-0978-1-61284-913-3978-1-61284-912","10.1109/ISQED.2011.5770768","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770768","BDD;timing analysis;timing optimization;execution time analysis","Data structures;Boolean functions;Monte Carlo methods;Software;Benchmark testing;Timing;Probability","binary decision diagrams;data structures;hardware description languages;Monte Carlo methods;probability;program compilers","path occurrence probability analysis;architectural level;acyclic synthesizable VHDL;software code;compiler-level architectural optimization;static timing analysis;critical paths;binary decision diagrams;Monte Carlo simulation approach","","1","21","","","","","","IEEE","IEEE Conferences"
"A multi Start adaptive Variable Neighborhood Search metaheuristic for the aircraft landing problem","S. Dhouib","Research Unity of Logistic, Superior Institute of Industrial management, University of Sfax, TUNISIA","2011 4th International Conference on Logistics","","2011","","","197","200","In this paper, a variable neighborhood search metaheuristic is enriched with a multi start technique and an adaptive taboo memory: the SVNS metaheuristic. In the proposed SVNS method, the diversification phase is ameliorated by launching the VNS metaheuristic in multi start technique and by archiving each blocked solution in taboo memory with a three different shames. This taboo memory governs the selection move for the neighborhood. The selection is performed in the neighborhood of each current solution. The SVNS metaheuristic is used to optimize the multiple runways aircraft landing problem. Computational experiments in several problems collected from the literature, instances from OR-Library, demonstrate that the proposed SVNS metaheuristic reaches high-quality solutions using very few user-defined parameters: the Kruskal-Wallis statistic test is used to prove that.","2162-9021","978-1-4577-0324-9978-1-4577-0322-5978-1-4577-0323","10.1109/LOGISTIQUA.2011.5939426","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5939426","metaheuristic;aircraft landing;sequence dependent scheduling","Aircraft;Search problems;Barium;Resource management;Europe;Software;Programming","aircraft;search problems;statistical testing;transportation","multistart adaptive variable neighborhood search metaheuristic;multiple runways aircraft landing problem;adaptive taboo memory;SVNS metaheuristic;OR-Library;Kruskal-Wallis statistic test","","1","12","","","","","","IEEE","IEEE Conferences"
"Oil pressure characteristic of automatic transmission's shift control unit and clutch failure analysis","Dong Peng; Liu Yanfang; Jiang Enqiao; Xu Xiangyang; Shui Lin; Zhang Guoru","School of Transportation Science &amp; Engineering, Beihang University, Beijing, China; School of Transportation Science &amp; Engineering, Beihang University, Beijing, China; School of Transportation Science &amp; Engineering, Beihang University, Beijing, China; School of Transportation Science &amp; Engineering, Beihang University, Beijing, China; Beijing Research &amp; Development Centre, Lubricant Company, Sinopec Corp., China; Beijing Research &amp; Development Centre, Lubricant Company, Sinopec Corp., China","2010 International Conference on Computer Application and System Modeling (ICCASM 2010)","","2010","3","","V3-42","V3-46","This paper researches on oil pressure characteristic in the shift control unit of a heavy vehicle's automatic transmission. The simulation model of its control unit was founded in the ITI-SimulationX software. Through a bench experiment, the correctness and effectiveness of the model has been checked. To solve the early failure problem of the clutch in the road test, optimizing its oil characteristic in the model by adjusting parameters and improving the model structure. The optimization results indicated that two factors, impacts between hydraulic control units when shifting and long time of sliding friction, were the major reasons leading to early failure. Modifying the structure of the AT prototype according to its simulation model, the early failure problem has been solved effectively.","2161-9069;2161-9077","978-1-4244-7237-6978-1-4244-7235","10.1109/ICCASM.2010.5620269","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5620269","shift control unit;oil pressure characteristic;dynamic simulation modeling","Valves;Petroleum;Mathematical model;Object oriented modeling;Simulation;Vehicle dynamics;Gears","clutches;failure analysis;hydraulic control equipment;oils;optimisation;sliding friction;vehicle dynamics","oil pressure characteristic;shift control unit;clutch failure analysis;vehicle automatic transmission;ITI-SimulationX software;optimization;hydraulic control units;sliding friction","","1","8","","","","","","IEEE","IEEE Conferences"
"OMAN: A Mobile Ad Hoc Network Design System","A. Fridman; S. Weber; C. Graff; D. E. Breen; K. R. Dandekar; M. Kam","Drexel University, Philadelphia; Drexel University, Philadelphia; U.S. Army, Ft. Monmouth; Drexel University, Philadelphia; Drexel University, Philadelphia; Drexel University, Philadelphia","IEEE Transactions on Mobile Computing","","2012","11","7","1179","1191","We present a software library that aids in the design of mobile ad hoc networks (MANET). The OMAN design engine works by taking a specification of network requirements and objectives, and allocates resources which satisfy the input constraints and maximize the communication performance objective. The tool is used to explore networking design options and challenges, including: power control, adaptive modulation, flow control, scheduling, mobility, uncertainty in channel models, and cross-layer design. The unaddressed niche which OMAN seeks to fill is the general framework for optimization of any network resource, under arbitrary constraints, and with any selection of multiple objectives. While simulation is an important part of measuring the effectiveness of implemented optimization techniques, the novelty and focus of OMAN is on proposing novel network design algorithms, aggregating existing approaches, and providing a general framework for a network designer to test out new proposed resource allocation methods. In this paper, we present a high-level view of the OMAN architecture, review specific mathematical models used in the network representation, and show how OMAN is used to evaluate tradeoffs in MANET design. Specifically, we cover three case studies of optimization. The first case is robust power control under uncertain channel information for a single physical layer snapshot. The second case is scheduling with the availability of directional radiation patterns. The third case is optimizing topology through movement planning of relay nodes.","1536-1233;1558-0660;2161-9875","","10.1109/TMC.2011.176","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6202816","Cross-layer design;optimization;mobile ad hoc networks;simulation;software systems.","Optimization;Resource management;Throughput;Receivers;Signal to noise ratio;Power control;Interference","adaptive modulation;mobile ad hoc networks;resource allocation;telecommunication network planning;telecommunication network topology","OMAN design engine;mobile ad hoc network design system;software library;mobile ad hoc networks;MANET;resource allocation;power control;adaptive modulation;flow control;channel models;cross-layer design;network representation;channel information;single physical layer snapshot;directional radiation patterns;movement planning;relay nodes","","6","36","","","","","","IEEE","IEEE Journals & Magazines"
"Experimental comparison of impairment-aware RWA algorithms in a GMPLS-controlled dynamic optical network","M. Angelou; F. Agraz; P. Kokkinos; J. Perello; S. Azodolmolky; E. Varvarigos; S. Spadaro; I. Tomkos","Universitat Polit&#x00E8;cnica de Catalunya (UPC), C/ Jordi Girona 1-3, 08034 Barcelona, Spain; Universitat Polit&#x00E8;cnica de Catalunya (UPC), C/ Jordi Girona 1-3, 08034 Barcelona, Spain; Research Academic Computer Technology Institute (RACTI), Patras, Greece; Universitat Polit&#x00E8;cnica de Catalunya (UPC), C/ Jordi Girona 1-3, 08034 Barcelona, Spain; Universitat Polit&#x00E8;cnica de Catalunya (UPC), C/ Jordi Girona 1-3, 08034 Barcelona, Spain; Research Academic Computer Technology Institute (RACTI), Patras, Greece; Universitat Polit&#x00E8;cnica de Catalunya (UPC), C/ Jordi Girona 1-3, 08034 Barcelona, Spain; Athens Information Technology, 19.5km Markopoulou Ave., 19002 Athens, Greece","2011 Future Network & Mobile Summit","","2011","","","1","5","The European research project DICONET proposed and implemented a multi-plane impairment-aware solution for flexible, robust and cost-effective core optical networks. The vision of DICONET was realized via a set of cross-layer optimization algorithms designed to serve the network during planning and operation. The cross-layer modules were incorporated in a common software platform forming a planning and operation tool that takes into account physical-layer impairments in the decision making. The overall solution relies on a GMPLS-based control plane that was extended to disseminate the physical layer information required by the cross-layer modules. One of the key activities in DICONET concerns the routing and wavelength assignment of traffic demands that arrive dynamically during the network operation. Identifying the important role of dynamic lightpath provisioning, in this work we focused on the performance of routing algorithms in dynamic optical networks. We tested the suitability and performance of two different online IA-RWA algorithms in a 14-node experimental test-bed that employed centralized control-plane architecture under the same network and traffic conditions. The parameters used to evaluate the two routing engines included the lightpath setup time and the blocking ratio in a traffic scenario where connections arrive and depart from the network dynamically. Results for different traffic loads showed that optimum impairment-aware decisions are made at the expense of higher lightpath setup times.","","978-1-905824-25-0978-1-4577-0928-9978-1-905824-25","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6095250","Networks;Optical Communications","Engines;Routing;Optical fiber networks;Heuristic algorithms;Algorithm design and analysis;Planning;Protocols","decision making;optical fibre communication;optimisation;telecommunication network routing;telecommunication traffic","impairment-aware RWA algorithm;GMPLS-controlled dynamic optical network;European research project;DICONET;multiplane impairment-aware solution;cost-effective core optical network;cross-layer optimization algorithm;software platform;operation tool;physical-layer impairment;decision making;GMPTS-based control plane;physical layer information;wavelength assignment;network operation;dynamic lightpath provisioning;online IA-RWA algorithm;centralized control-plane architecture;routing engine;optimum impairment-aware decision","","","6","","","","","","IEEE","IEEE Conferences"
"802.11 Handover optimization to improve communications of mobile workers","K. Kuladinithi; A. Udugama; A. Timm-Giel; C. Görg","Communications Networks, TZI, University of Bremen, Germany; Communications Networks, TZI, University of Bremen, Germany; Institute of Communication Networks, Hamburg University of Technology, Hamburg, Germany; Communications Networks, TZI, University of Bremen, Germany","2010 Fifth International Conference on Information and Automation for Sustainability","","2010","","","156","162","Seamless performance of communication applications is critical for today's mobile workers who are dependent on their mobile computing devices. One area of such criticality is the smoothness of handovers between Access Points (APs) in 802.11 based communications. In this work, two heuristic handover algorithms based on indoor positioning are designed and tested on 802.11 based networks. The two algorithms, named “position based” and “prediction based” utilizes a previously populated position based RSSI database to determine the optimal AP to connect at a particular location. The goal is to select the best AP at each identified location in order to optimize the performances of an application (in terms of lesser number of packets lost and to achieve the optimum throughput). To test these algorithms, we build a testing environment with an indoor based mobile factory worker. This worker utilizes a Microsoft Windows based operating environment considering the fact that a vast majority of today's mobile computing devices are run on Microsoft Windows based platforms. The indoor environment is populated with 802.11b APs which are distributed along the mobile worker's path. The performance is measured in terms of the performance of the applications run on the mobile computing device generating TCP and UDP based traffic. The results conclude that both algorithms outperform the algorithm used conventionally by the Microsoft Windows operating environment considering the maximum throughput achieved.","2151-1802;2151-1810","978-1-4244-8552-9978-1-4244-8549-9978-1-4244-8551","10.1109/ICIAFS.2010.5715652","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5715652","Seamless handover;mobile worker;prediction based","Wireless LAN;Prediction algorithms;Manganese;Throughput;Mobile communication;Software;Software algorithms","mobile computing;mobility management (mobile radio);operating systems (computers);telecommunication computing;telecommunication traffic;transport protocols;wireless LAN","802.11 handover optimization;mobile factory worker communication;mobile computing device;access point;AP;RSSI database;Microsoft window;TCP;UDP-based traffic","","1","10","","","","","","IEEE","IEEE Conferences"
"Simulation and parameters optimization of power DMOS Trench Field Effect Transistors","V. V. Baranov; A. I. Belous; M. M. Krechko; I. I. Roubtsevich; A. S. Tourtsevich","Belarusian State University of Informatics and Radioelectronics, Belarus, Minsk; Corporation “Integral”, Belarus, Minsk; Corporation “Integral”, Belarus, Minsk; Corporation “Integral”, Belarus, Minsk; Corporation “Integral”, Belarus, Minsk","3rd Electronics System Integration Technology Conference ESTC","","2010","","","1","3","DMOS Trench Field Effect Transistors (TrenchFETs) have been taken as an object for electrical parameters optimization with the special developed software based on Excel table processor. Using this software the values of resistance between the source areas and the drain area of the open DMOS TrenchFET have been minimized as well as the values of the output and the internal capacitance of the cell transistor structure. The efficiency of the developed software is proved by the results of testing the manufactured series of DMOS TrenchFETs as their electrical parameters correspond to the best world known analogues.","","978-1-4244-8555-0978-1-4244-8553-6978-1-4244-8554","10.1109/ESTC.2010.5642831","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5642831","","Epitaxial growth;Lead;Transistors;Logic gates","capacitance;field effect transistors;isolation technology;MOS integrated circuits","power DMOS trench field effect transistor;electrical parameter optimization;Excel table processor;DMOS trenchFET;internal capacitance;cell transistor structure","","","2","","","","","","IEEE","IEEE Conferences"
"An Evaluation Research on Usability of Taobao's Homepage and Main Search Engine Based on Eye Tracking","M. Hua; F. Qian","NA; NA","2010 International Conference on System Science, Engineering Design and Manufacturing Informatization","","2010","2","","23","26","The real-time nature can be reflected in the interaction between users and webpages, Eye Tracking can record these data, with which the researcher can find out the key points, areas of interests, difficulties to understand and browsing habits, thereby providing objective and visual guidance to optimize the information architecture of webpages. This paper describes the process of eye tracking test by Taobao's UED and analyzes the methods and results of evaluation on user habits by use of the generated diagrams of eye tracking. Through Eye tracking test, design researchers can find a lot of interesting content that cannot be easily found in other ways. These valuable research data and reports will be shared to the front-end engineers, interaction designers and other related personnel in the way of internal sharing.","","978-1-4244-8664","10.1109/ICSEM.2010.96","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5640287","usability;evaluation;Eye tracking;user research","Testing;Usability;Tracking;Search engines;Visualization;Indexes","human computer interaction;search engines;software reusability","main search engine;eye tracking test;information architecture;Taobao homepage;Webpages","","1","4","","","","","","IEEE","IEEE Conferences"
"HFSS simulation, experimental investigation and optimisation of heat sink EMI","S. Manivannan; R. Arumugam; S. Paramasivam; P. Salil; B. S. Rao","Department of Electrical and Electronics Engineering, Anna University, Chennai, Tamil Nadu, India; Department of Electrical and Electronics Engineering, SSN College of Engineering, Chennai, Tamil Nadu, India; ESAB, Chennai, Tamil Nadu, India; Sameer CEM, Chennai, Tamil Nadu, India; Sameer CEM, Chennai, Tamil Nadu, India","IET Power Electronics","","2010","3","6","881","891","This study presents the approach for the minimisation of electromagnetic radiation from the heat sink by optimising the heat sink geometry parameters using Taguchi's design of experiments (DOE) technique. The heat sink is modelled using Ansoft HFSS software version 12 and the value of the emitted radiation is obtained by simulation. Experimental investigation was performed in a semi-anechoic chamber for a selected heat sink to observe the radiated emissions from it at a distance of 3 m. The simulation model was thus validated with the experimental results and hence, the simulation was continued for the combinations generated by the L27 (six factors, three levels) orthogonal array generated using Taguchi's DOE using the Minitab software. The factors considered for optimisation are the length and width of the heat sink, fin height, base height, number of fins and fin thickness. The analysis of variance test was carried out for finding out the contribution and impact of each heat sink design factor towards the radiations emitted by the heat sink. Additionally, a decision support for selecting the heat sink parameters for predicting the emitted radiations has been presented using the linear regression analysis and the results are compared with HFSS simulation results.","1755-4535","","10.1049/iet-pel.2010.0017","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5618877","","","design of experiments;electromagnetic interference;electromagnetic waves;heat sinks;minimisation;regression analysis","HFSS simulation;optimisation;heat sink EMI;minimisation;electromagnetic radiation;heat sink geometry parameter;design of experiment;DOE technique;radiation emission;semianechoic chamber;Minitab software;variance test;heat sink design;decision support;linear regression analysis","","4","","","","","","","IET","IET Journals & Magazines"
"Research and Implementation of the Portable LED Stage Lighting Control System","H. Ren; L. Yang; Y. Li; H. Wang","NA; NA; NA; NA","2012 Fifth International Joint Conference on Computational Sciences and Optimization","","2012","","","566","568","With the development of stage lighting technology, people have increasing demands for the lighting control, such as real-time and portable, However, existing LED dimming control equipments usually have large size and complicated functions, so they are not easy to move and operate. In order to improve these problems, this paper proposes a portable LED stage lighting control system and designed its software and hardware. Its hardware is composed of three parts: stage lighting manipulator, the communication lines based on DMX512 protocol and LED dimming control circuit. ATmega16 is the core of the control system. Data transmission follows the DMX512 protocol. The test results showed this system had stable performance and high reliability, which can completely met the requirement of stage lighting control. Its simple structure and convenient operation made it easy to carry and use. Meanwhile, using LED lamps instead of traditional ones can effectively save energy.","","978-1-4673-1365-0978-0-7695-4690","10.1109/CSO.2012.129","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6274790","stage;led;dmx512;atmega16;constant-current driver","Light emitting diodes;Pulse width modulation;Lighting;Manipulators;Lighting control;Hardware;Protocols","control engineering computing;data communication;energy conservation;field buses;LED lamps;lighting control;microcontrollers;portable instruments;protocols;reliability;stability","portable LED stage lighting control system;software design;hardware design;stage lighting manipulator;communication lines;DMX512 protocol;LED dimming control circuit;ATmega16;data transmission;performance stability;reliability;energy saving","","2","5","","","","","","IEEE","IEEE Conferences"
"Sequential and Cooperative Distributed SA-Type Algorithms for Energy Optimization in Embedded Systems","M. I. Aouad; L. Idoumghar; R. Schott; O. Zendra","NA; NA; NA; NA","2010 International Conference on Computational Intelligence and Software Engineering","","2010","","","1","4","Reducing memory energy consumption in embedded systems is crucial. In this paper, we propose new sequential and distributed algorithms based on Simulated Annealing (SA) in order to reduce memory energy consumption in embedded systems. Our algorithms outperform the Tabu Search (TS) approach. In fact, our algorithms manage to consume nearly from 76% up to 98% less memory energy than TS. Execution time savings for the distributed SA version (from 65.63% up to 75.34% for a cluster of 6 PCs) are also recorded.","","978-1-4244-5391-7978-1-4244-5392","10.1109/CISE.2010.5676945","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676945","","Random access memory;Memory management;Writing;Program processors;Signal processing algorithms;Embedded systems;Benchmark testing","cache storage;embedded systems;energy consumption;search problems;simulated annealing","sequential distributed SA-type algorithms;cooperative distributed SA-type algorithms;energy optimization;embedded systems;memory energy consumption;simulated annealing;tabu search","","","13","","","","","","IEEE","IEEE Conferences"
"CBCD: Cloned buggy code detector","J. Li; M. D. Ernst","DNV Research & Innovation H⊘vik, Norway; University of Washington, Seattle, WA, USA","2012 34th International Conference on Software Engineering (ICSE)","","2012","","","310","320","Developers often copy, or clone, code in order to reuse or modify functionality. When they do so, they also clone any bugs in the original code. Or, different developers may independently make the same mistake. As one example of a bug, multiple products in a product line may use a component in a similar wrong way. This paper makes two contributions. First, it presents an empirical study of cloned buggy code. In a large industrial product line, about 4% of the bugs are duplicated across more than one product or file. In three open source projects (the Linux kernel, the Git version control system, and the PostgreSQL database) we found 282, 33, and 33 duplicated bugs, respectively. Second, this paper presents a tool, CBCD, that searches for code that is semantically identical to given buggy code. CBCD tests graph isomorphism over the Program Dependency Graph (PDG) representation and uses four optimizations. We evaluated CBCD by searching for known clones of buggy code segments in the three projects and compared the results with text-based, token-based, and AST-based code clone detectors, namely Simian, CCFinder, Deckard, and CloneDR. The evaluation shows that CBCD is fast when searching for possible clones of the buggy code in a large system, and it is more precise for this purpose than the other code clone detectors.","1558-1225;0270-5257;0270-5257","978-1-4673-1067-3978-1-4673-1066-6978-1-4673-1065","10.1109/ICSE.2012.6227183","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227183","Validation;Debugging aids","Cloning;Computer bugs;Optimization;Linux;Complexity theory;Kernel;Detectors","configuration management;graph theory;Linux;operating system kernels;program debugging;public domain software;SQL","CBCD;cloned buggy code detector;industrial product line;open source projects;Linux kernel;Git version control system;PostgreSQL database;CBCD tests graph isomorphism;program dependency graph representation;PDG;buggy code segments;text-based code clone detectors;token-based code clone detectors;AST-based code clone detectors;Simian;CCFinder;Deckard;CloneDR","","27","34","","","","","","IEEE","IEEE Conferences"
"Time-triggered implementations of mixed-criticality automotive software","D. Goswami; M. Lukasiewycz; R. Schneider; S. Chakraborty","Alexander von Humboldt Research Fellow, TU Munich, Germany; TUM CREATE, Singapore; Institute for Real-Time Computer Systems, TU Munich, Germany; Institute for Real-Time Computer Systems, TU Munich, Germany","2012 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2012","","","1227","1232","We present an automatic schedule synthesis framework for applications that are mapped onto distributed time-triggered automotive platforms where multiple Electronic Control Units (ECUs) are synchronized over a FlexRay bus. We classify applications into two categories (i) safety-critical control applications with stability and performance constraints, and (ii) time-critical applications with only deadline constraints. Our proposed framework can handle such mixed constraints arising from timing, control stability, and performance requirements. In particular, we synthesize schedules that optimize control performance and respects the timing requirements of the real-time applications. An Integer Linear Programming (ILP) problem is formulated by modeling the ECU and bus schedules as a set of constraints for optimizing both linear or quadratic control performance functions.","1558-1101;1530-1591;1530-1591","978-1-4577-2145-8978-3-9810801-8","10.1109/DATE.2012.6176680","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6176680","","Delay;Schedules;Linear approximation;Real time systems;Stability analysis","automotive electronics;automotive engineering;control engineering computing;integer programming;linear programming;system buses","time-triggered implementations;mixed-criticality automotive software;automatic schedule synthesis framework;distributed time-triggered automotive platforms;electronic control units;FlexRay bus;safety-critical control applications;time-critical applications;stability constraints;performance constraints;deadline constraints;integer linear programming problem;bus schedules;linear control performance functions;quadratic control performance functions","","31","12","","","","","","IEEE","IEEE Conferences"
"LCU-based adaptive interpolation filter","S. Matsuo; S. Takamura; H. Jozawa","NTT Cyber Space Laboratories, NTT Corporation 1-1 Hikari-no-oka, Yokosuka-shi, Kanagawa, 239-0847 Japan; NTT Cyber Space Laboratories, NTT Corporation 1-1 Hikari-no-oka, Yokosuka-shi, Kanagawa, 239-0847 Japan; NTT Cyber Space Laboratories, NTT Corporation 1-1 Hikari-no-oka, Yokosuka-shi, Kanagawa, 239-0847 Japan","2012 Picture Coding Symposium","","2012","","","393","396","Interpolation filter that supports motion estimation with fractional-pel accuracy is one of important coding tools for improving efficiency of inter frame coding. A lot of interpolation filters were proposed to standardization activity of Joint Collaborative Team on Video Coding (JCT-VC) to establish a next video coding standard called High Efficiency Video Coding (HEVC). DCT-based interpolation filter (DCT-IF) is currently employed in the HEVC test model (HM) software. However, the values of the filter coefficients are constant regardless of the characteristic of the input video. The authors proposed a region-based adaptive interpolation filter (RBAIF) that optimizes the filter coefficients on a region-by-region basis. In this paper, some modifications are added to the conventional RBAIF. Simulation results showed that, compared to the HM software, the proposed filter offers about 0.5% and 0.4% average coding gains for luminance and chrominance, respectively. Runtime of the encoder and the decoder were 101.4% and 102.5% on average, respectively.","","978-1-4577-2049-9978-1-4577-2047-5978-1-4577-2048","10.1109/PCS.2012.6213372","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6213372","","Encoding;Interpolation;Proposals;Indexes;Software;Decoding;Video coding","adaptive filters;data compression;interpolation;motion estimation;video coding","LCU-based adaptive interpolation filter;motion estimation;fractional-pel accuracy;inter frame coding;joint collaborative team;JCT-VC;high efficiency video coding;DCT-based interpolation filter;DCT-IF;HEVC test model software;HM software;region-based adaptive interpolation filter;conventional RBAIF","","","14","","","","","","IEEE","IEEE Conferences"
"Randomized testing for Robotic plan execution for autonomous systems","Z. Saigol; F. Py; K. Rajan; C. McGann; J. Wyatt; R. Dearden","School of Computer Science University of Birmingham, UK; Monterey Bay Aquarium Research Institute Moss Landing, California; Monterey Bay Aquarium Research Institute Moss Landing, California; Wizbots, LLC San Carlos, California; School of Computer Science University of Birmingham, UK; School of Computer Science University of Birmingham, UK","2010 IEEE/OES Autonomous Underwater Vehicles","","2010","","","1","9","Autonomous underwater vehicles (AUVs) are commonly used for carrying out pre-planned oceanographic surveys, but there is increasing interest in optimizing these surveys by performing onboard re- planning. MBARI has developed an advanced AUV control system, the Teleo Reactive Executive (T-REX) that enables the vehicle to survey areas in more detail if biogeochemical markers indicate the presence of a target feature, and even to follow dynamic ocean phenomena such as fronts. T-REX uses artificial intelligence (AI) techniques in constraint-based temporal planning together with a layered control architecture that allows plans to be generated and executed onboard. One challenge of onboard plan synthesis and execution is that the power of the system to generate different behaviors makes it hard to test in simulation, and failures at sea are costly. We introduce a randomized Monte-Carlo method based test approach that executes hundreds of simulated missions with each mission presenting different inputs to the planner, and checks each output plan for validity. The approach sets environmental parameters to exercise T-REX's domain model, and it is fully configurable. We describe how the Monte-Carlo tester integrates with T-REX, how we have incorporated it into our testing process, and the benefits for system reliability that have resulted. We also highlight our experiences in discovering bugs both in simulation and for science surveys in waters off Northern California.","2377-6536;1522-3167;1522-3167","978-1-61284-981-2978-1-61284-980-5978-1-61284-979","10.1109/AUV.2010.5779648","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5779648","","Argon;Software;Gold;Educational institutions","mobile robots;Monte Carlo methods;oceanographic techniques;planning (artificial intelligence);reliability;remotely operated vehicles;underwater vehicles","randomized testing;robotic plan execution;autonomous underwater vehicle;preplanned oceanographic survey;onboard replanning;MBARI;AUV control system;teleoreactive executive;biogeochemical marker;dynamic ocean phenomena;artificial intelligence technique;constraint based temporal planning;layered control architecture;simulation testing;randomized Monte Carlo method;environmental parameter;T-REX domain model;system reliability","","1","","","","","","","IEEE","IEEE Conferences"
"MicroTools: Automating Program Generation and Performance Measurement","J. C. Beyler; N. Triquenaux; V. Palomares; F. Chabane; T. Fighiera; J. Halimi; W. Jalby","NA; NA; NA; NA; NA; NA; NA","2012 41st International Conference on Parallel Processing Workshops","","2012","","","424","433","Tuning an application to a given architecture has become a complex procedure. Sophisticated hardware obfuscates the path to easily writing peak-performance applications. During the optimization process, before utilizing the hardware correctly, the user must understand out-of-order execution and micro-operations. Understanding the software's performance on a given target architecture is the goal of the Micro Creator and Micro Launcher tools. Micro Creator automatically generates a set of benchmark programs from a XML file, whereas Micro Launcher executes them in a stable and closed environment. With these tools, the user has a better understanding of the underlying architecture. The two programs, through the execution of hundreds of micro-programs and from a single-core execution to the parallel world using OpenMP, give insight on performance issues. Looking into unrolling, strided memory accesses, vectorized programs, and parallel programs allow quick and efficient calculations of the latencies and bottlenecks of the architecture.","0190-3918;1530-2016;2332-5690","978-1-4673-2509-7978-0-7695-4795","10.1109/ICPPW.2012.60","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6337510","Program generation;Micro-benchmarks;Performance tools","Kernel;Registers;Computer architecture;Assembly;Benchmark testing;Optimization;Hardware","software performance evaluation;XML","MicroTools;program generation;performance measurement;sophisticated hardware;peak performance application;optimization process;micro operation;software performance;target architecture;Micro Creator;Micro Launcher tools;benchmark programs;XML file;microprograms;single core execution;parallel world;OpenMP;strided memory;vectorized programs;parallel programs","","1","20","","","","","","IEEE","IEEE Conferences"
"Programming a Topological Quantum Computer","S. Devitt; K. Nemoto","NA; NA","2012 IEEE 21st Asian Test Symposium","","2012","","","55","60","Topological quantum computing has recently proven itself to be a powerful computational model when constructing viable architectures for large scale computation. The topological model is constructed from the foundation of a error correction code, required to correct for inevitable hardware faults that will exist for a large scale quantum device. It is also a measurement based model of quantum computation, meaning that the quantum hardware is responsible only for the construction of a large, computationally universal quantum state. This quantum state is then strategically consumed, allowing for the realisation of a fully error corrected quantum algorithm. The number of physical qubits needed by the quantum hardware and the amount of time required to implement an algorithm is dictated by the manner in which this universal quantum state is consumed. In this paper we examine the problem of algorithmic optimisation in the topological lattice and introduce the required elements that will be needed when designing a classical software package to compile and implement a large scale algorithm on a topological quantum computer.","2377-5386;1081-7735;1081-7735","978-1-4673-4555-2978-0-7695-4876","10.1109/ATS.2012.52","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6394172","Quantum Computing;Error Correction;Fault-tolerance;Classical Compiler","Lattices;Logic gates;Quantum computing;Hardware;Computational modeling;Computers;Computer architecture","optimisation;quantum computing","topological quantum computer;error correction code;hardware fault;large scale quantum device;measurement based model;physical qubits;quantum hardware;universal quantum state;software package;algorithmic optimisation;topological lattice","","3","20","","","","","","IEEE","IEEE Conferences"
"A New Formulation for Feedforward Neural Networks","S. Razavi; B. A. Tolson","Department of Civil and Environmental Engineering, University of Waterloo, Waterloo, Canada; Department of Civil and Environmental Engineering, University of Waterloo, Waterloo, Canada","IEEE Transactions on Neural Networks","","2011","22","10","1588","1598","Feedforward neural network is one of the most commonly used function approximation techniques and has been applied to a wide variety of problems arising from various disciplines. However, neural networks are black-box models having multiple challenges/difficulties associated with training and generalization. This paper initially looks into the internal behavior of neural networks and develops a detailed interpretation of the neural network functional geometry. Based on this geometrical interpretation, a new set of variables describing neural networks is proposed as a more effective and geometrically interpretable alternative to the traditional set of network weights and biases. Then, this paper develops a new formulation for neural networks with respect to the newly defined variables; this reformulated neural network (ReNN) is equivalent to the common feedforward neural network but has a less complex error response surface. To demonstrate the learning ability of ReNN, in this paper, two training methods involving a derivative-based (a variation of backpropagation) and a derivative-free optimization algorithms are employed. Moreover, a new measure of regularization on the basis of the developed geometrical interpretation is proposed to evaluate and improve the generalization ability of neural networks. The value of the proposed geometrical interpretation, the ReNN approach, and the new regularization measure are demonstrated across multiple test problems. Results show that ReNN can be trained more effectively and efficiently compared to the common neural networks and the proposed regularization measure is an effective indicator of how a network would perform in terms of generalization.","1045-9227;1941-0093","","10.1109/TNN.2011.2163169","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5993545","Feedforward neural networks;generalization;geometrical interpretation;internal behavior;measure of regularization;reformulated neural network;training","Biological neural networks;Training;Nickel;Optimization;Function approximation;Neurons","feedforward neural nets;function approximation;generalisation (artificial intelligence);learning (artificial intelligence);optimisation","function approximation techniques;black box model;neural network functional geometry;geometrical interpretation;reformulated neural network;feedforward neural network;error response surface;learning ability;training method;derivative free optimization algorithm;generalization ability;ReNN approach","Algorithms;Artificial Intelligence;Feedback;Models, Neurological;Neural Networks (Computer);Software Design","55","35","","","","","","IEEE","IEEE Journals & Magazines"
"Performance Portability with the Chapel Language","A. Sidelnik; S. Maleki; B. L. Chamberlain; M. J. Garzar'n; D. Padua","NA; NA; NA; NA; NA","2012 IEEE 26th International Parallel and Distributed Processing Symposium","","2012","","","582","594","It has been widely shown that high-throughput computing architectures such as GPUs offer large performance gains compared with their traditional low-latency counterparts for many applications. The downside to these architectures is that the current programming models present numerous challenges to the programmer: lower-level languages, loss of portability across different architectures, explicit data movement, and challenges in performance optimization. This paper presents novel methods and compiler transformations that increase programmer productivity by enabling users of the language Chapel to provide a single code implementation that the compiler can then use to target not only conventional multiprocessors, but also high-throughput and hybrid machines. Rather than resorting to different parallel libraries or annotations for a given parallel platform, this work leverages a language that has been designed from first principles to address the challenge of programming for parallelism and locality. This also has the advantage of providing portability across different parallel architectures. Finally, this work presents experimental results from the Parboil benchmark suite which demonstrate that codes written in Chapel achieve performance comparable to the original versions implemented in CUDA on both GPUs and multicore platforms.","1530-2075;1530-2075","978-1-4673-0975-2978-0-7695-4675","10.1109/IPDPS.2012.60","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6267860","","Graphics processing unit;Arrays;Multicore processing;Parallel processing;Benchmark testing;Reactive power","parallel architectures;parallel programming;parallelising compilers;software libraries;software performance evaluation;software portability","performance portability;Chapel Language;high-throughput computing architectures;programming models;lower-level languages;explicit data movement;performance optimization;compiler transformation;single code implementation;hybrid machines;parallel libraries;parallel platform;parallel architectures;Parboil benchmark suite","","8","41","","","","","","IEEE","IEEE Conferences"
"Design of a tunable S-band narrow-band coaxial cavity filter","P. Wang; L. Li; Shaowei","Communication Institute of PLA University of Science and Technology; Nanjing; China; 210007, NO. 1 Biaoyin, Nanjing JIangsu China; Communication Institute of PLA University of Science and Technology; Nanjing; China; 210007, NO. 1 Biaoyin, Nanjing JIangsu China; Communication Institute of PLA University of Science and Technology; Nanjing; China; 210007, NO. 1 Biaoyin, Nanjing JIangsu China","2012 International Conference on Microwave and Millimeter Wave Technology (ICMMT)","","2012","1","","1","4","An electromagnetic model for coaxial cavity filter is theoretically analyzed in this paper. Based on this model, the calculation expressions of cavity coupling factors are derived, and the eigenmode value of each resonator unit is obtained, which are significant parameters to filter design. The 3D electromagnetic simulation software is used to analysis and optimization for the narrow-band cavity filter. Finally, the filter is fabricated and tested. The experiment and simulation results are in coincidence. In comparison with the traditional C-band cavity or waveguide narrow-band band-pass filters, the designed coaxial cavity filter is minimized in size.","","978-1-4673-2185-3978-1-4673-2184","10.1109/ICMMT.2012.6229908","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6229908","Narrow-band coaxial cavity filter;Full wave analysis;couple coefficient;Eigenmode value","Couplings;Cavity resonators;Band pass filters;Resonator filters;Microwave filters;Resonant frequency;Magnetic separation","cavity resonator filters;circuit optimisation;eigenvalues and eigenfunctions;microwave filters;UHF filters","tunable S-band narrowband coaxial cavity filter design;electromagnetic model;cavity coupling factors;eigenmode value;resonator unit;3D electromagnetic simulation software;narrowband cavity filter optimization;C-band cavity;waveguide narrowband band-pass filters","","1","5","","","","","","IEEE","IEEE Conferences"
"Anomaly detection using baseline and K-means clustering","M. F. Lima; B. B. Zarpelão; L. D. H. Sampaio; J. J. P. C. Rodrigues; T. Abrão; M. L. Proença","Computing Science Department, State University of Londrina (UEL), Brazil; School of Elect. & Comp. Engineering, University of Campinas (UNICAMP), Brazil; Computing Science Department, State University of Londrina (UEL), Brazil; Instituto de Telecomunicaç ões, University of Beira Interior, Covilhã, Portugal; Computing Science Department, State University of Londrina (UEL), Brazil; Computing Science Department, State University of Londrina (UEL), Brazil","SoftCOM 2010, 18th International Conference on Software, Telecommunications and Computer Networks","","2010","","","305","309","Anomaly detection refers to methods that provide warnings of unusual behaviors which may compromise the security and performance of communication networks. In this paper it is proposed a novel model for network anomaly detection combining baseline, K-means clustering and particle swarm optimization (PSO). The baseline consists of network traffic normal behavior profiles, generated by the application of Baseline for Automatic Backbone Management (BLGBA) model in SNMP historical network data set, while K-means is a supervised learning clustering algorithm used to recognize patterns or features in data sets. In order to escape from local optima problem, the K-means is associated to PSO, which is a meta-heuristic whose main characteristics include low computational complexity and small number of input parameters dependence. The proposed anomaly detection approach classifies data clusters from baseline and real traffic using the K-means combined with PSO. Anomalous behaviors can be identified by comparing the distance between real traffic and cluster centroids. Tests were performed in the network of State University of Londrina and the obtained detection and false alarm rates are promising.","","978-1-4244-8663-2978-953-290-004","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5623690","","Clustering algorithms;Particle swarm optimization;Monitoring;Alarm systems;Data mining;Unsupervised learning","computer network management;computer network security;learning (artificial intelligence);particle swarm optimisation;pattern clustering;telecommunication traffic","k-means clustering;communication network security;network anomaly detection approach;baseline clustering algorithm;particle swarm optimization;PSO;network traffic normal behavior profiles;automatic backbone management model;SNMP historical network data set;BLGBA model;local optima problem;supervised learning clustering algorithm;meta-heuristic;low computational complexity;input parameter dependence;data cluster classification;cluster centroids;false alarm rates","","2","16","","","","","","IEEE","IEEE Conferences"
"MPIActor - A Multicore-Architecture Adaptive and Thread-Based MPI Program Accelerator","Z. Liu; K. Ren; J. Song","NA; NA; NA","2010 IEEE 12th International Conference on High Performance Computing and Communications (HPCC)","","2010","","","98","107","Improving MPI foundational software to suit multicore systems is a key issue for developing effective parallel software on high performance communication domain. Towards this issue, in this paper, we propose a novel technique, called MPI Accelerator or MPIActor in short, which is a transparent middleware to enhance conventional MPI libraries. The main idea is to optimize MPI routines for multicore systems by adopting threaded MPI mechanism and multicore architecture aware collectives in MPIActor. With the join of MPIActor, on one hand, all MPI processes in each node are mapped to several threads in one process. As a result, the overhead of intra-node point-to-point communications can greatly decrease. On the other hand, the collective routines are implemented by the cooperation of individual intra - and inter-node collective subroutines, and the intra-node collective subroutines can be further optimized by multicore architecture aware collective algorithms. Based on above idea, a framework involving an MPI_Reduce routine and a set of point-to-point communication routines has been implemented and evaluated on a 256 cores Nehalem platform. When compared to the performance of MVAPICH2, the final experimental results show that the performance by MPIActor can be significantly improved whatever by using OSU_LATENCY benchmark for point-to-point communications or IMB Reduce benchmark for reduction collectives. Especially, the performance results of using OSU_LATENCY benchmark even can be improved up to 321%.","","978-1-4244-8335-8978-0-7695-4214","10.1109/HPCC.2010.89","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5581329","Threaded MPI;MPI Accelerator;Communication;Multicore-Architecture Adaptive Collective;MPIActor","Multicore processing;Message systems;Libraries;Algorithm design and analysis;Receivers;Instruction sets","benchmark testing;message passing;middleware;multiprocessing systems;multi-threading;parallel architectures;software libraries;subroutines","MPIActor;multicore architecture adaptive;thread based MPI program accelerator;MPI foundational software;parallel software;high performance communication domain;transparent middleware;MPI library;intranode point-to-point communication;internode collective subroutine;intranode collective subroutine;MPI reduce routine;MVA PICH2;OSU LATENCY benchmark;IMB reduce benchmark","","","30","","","","","","IEEE","IEEE Conferences"
"Optimization of GPS L1 acquisition using Radix-4 FFT","S. R. Babu; P. Selvam; G. S. Rao; J. Wang","Advanced Technology Labs, Wipro Technologies 105 Anna Salai, Guinday, Chennai - 32, India; Advanced Technology Labs, Wipro Technologies 105 Anna Salai, Guinday, Chennai - 32, India; Advanced Technology Labs, Wipro Technologies 105 Anna Salai, Guinday, Chennai - 32, India; Advanced Technology Labs, Wipro Technologies 105 Anna Salai, Guinday, Chennai - 32, India","2011 International Conference on Recent Trends in Information Technology (ICRTIT)","","2011","","","875","879","The implementation of an acquisition loop for software GPS receiver is still a challenge because of the large number of arithmetic operations involved in an FFT algorithm. Oversampling of the composite signals further compounds this problem. A Radix-4 FFT algorithm was proposed to improve the performance and the results were compared with Radix-2 FFT. To validate the algorithms a software GPS baseband processor was developed in Simulink and tested with real satellite data set. The analysis shows that the Radix-4 FFT is about 23% more efficient than the Radix-2 algorithm.","","978-1-4577-0590-8978-1-4577-0588-5978-1-4577-0589","10.1109/ICRTIT.2011.5972353","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5972353","GPS Acquisition;FFT;Radix-4;Simulink","Signal processing algorithms;Algorithm design and analysis;Global Positioning System;Satellites;Receivers;Frequency domain analysis;Mathematical model","data acquisition;fast Fourier transforms;Global Positioning System;radio receivers","GPS L1 acquisition;Radix-4 FFT;acquisition loop;software GPS receiver;satellite data set;software GPS baseband processor","","3","10","","","","","","IEEE","IEEE Conferences"
"A Nested Loop Fusion Algorithm Based on Cost Analysis","Z. Jie; Z. Rongcai; Y. Yuan","NA; NA; NA","2012 IEEE 14th International Conference on High Performance Computing and Communication & 2012 IEEE 9th International Conference on Embedded Software and Systems","","2012","","","1096","1101","When applying Loop Fusion technology in the existing parallelizing compiler systems, the combined loop may not gain better parallel efficiency due to the lack of the consideration to the cost analysis of parallel loops. By improving the execution process of parallelizing compiler systems, we proposed a nested loop fusion algorithm based on cost analysis. It can not only optimize the execution process of parallelizing compiler system but also ensure that the combined loop having been applied Loop Fusion technology can get positive speedup. We implemented this algorithm on a specific parallelizing compiler and compiled some benchmark programs by the compiler. The experimental results show that the efficiency of the parallel programs increases by 5% to 20%.","","978-1-4673-2164-8978-0-7695-4749","10.1109/HPCC.2012.160","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6332296","parallelizing compiler;loop fusion;cost analysis;nested loop","Program processors;Algorithm design and analysis;Optimization;Vectors;Parallel processing;Benchmark testing;Computer architecture","parallel programming;parallelising compilers;sensor fusion;software cost estimation","nested loop fusion algorithm;cost analysis;execution process;parallel programs;parallelizing compiler systems","","","7","","","","","","IEEE","IEEE Conferences"
"Study of Making Model and Simulating for DC Motor Windings Based on ACA","J. Liu; X. Wang; N. Jiang","NA; NA; NA","2010 International Conference on Computational Intelligence and Software Engineering","","2010","","","1","4","The Electromagnetic Compatibility (EMC) of direct current (DC) motor windings is a system model which is able to reflect the functional characters of the system in the whole EMC specified frequency (150 KHz ~ 30 MHz). For most motor designing process, it always evaluated the inductance of windings in lower or working frequency; however, when analyzing the conducted interference, it is necessary to take some parameters in high frequency into account in building up the EMC model, such as the noticeable distributed capacitance among the windings or between windings and shells. Past research neglected the common-mode current generated by the high frequency interference through motor bearings coupled with shells, since the parasitic capacitance of rotor core comes from armature windings supplied sufficient paths. In EMC modeling for direct current (DC) motor problem, first test the impedance of windings by experiments, then generate the equivalent circuit with total parameters. At present, it is a difficulty that how to choose the parameters. Most researchers preferred to adopt analytical calculation, however, it could not reflect the essence of the model since it requires many simplification. As a result, this paper adopted ant colony algorithm (ACA) with positive feedback to intelligent search and global optimize parameters of equivalent circuit. Simulation result showed that impedance of equivalent circuit calculated by this algorithm was the same as experimental result in the whole EMC frequency.","","978-1-4244-5391-7978-1-4244-5392","10.1109/CISE.2010.5676972","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676972","","DC motors;Windings;Equivalent circuits;Integrated circuit modeling;Impedance;Analytical models;Electromagnetic compatibility","DC motors;electromagnetic compatibility;machine bearings;machine windings;optimisation","DC motor windings;ACA;electromagnetic compatibility;direct current motor windings;EMC model;distributed capacitance;frequency interference;motor bearings;rotor core;parasitic capacitance;armature windings;equivalent circuit;ant colony algorithm;frequency 150 kHz to 30 MHz","","1","10","","","","","","IEEE","IEEE Conferences"
"A post-manufacturing language-adaptive embedded processor system","Y. Jung","Electrical and Computer Engineering, Gannon University, USA","2011 NASA/ESA Conference on Adaptive Hardware and Systems (AHS)","","2011","","","161","168","Language is a continuously adapting entity. As new modes of expression and ideas arise, humans are capable of extending the boundaries of language despite time-consuming and costly processes. Yet, the boundaries for processors are rigidly established upon manufacturing. A processor utilizing a static language such as an instruction set is unlikely to last long amongst rapidly advancing technologies, and valuable time and energy are lost due to the use of outmoded forms to express new ideas. In attempts to overcome such limitations and inefficiencies, new instructions are continuously added to the instruction set for greater adaptability to support swiftly evolving applications. Unfortunately, this process requires time-consuming and costly manufacturing, and even this level of adaptability is unsatisfactory and results in a processor that falls short of its intended potential. Since post-manufacturing instruction set customization is generally impossible, an instruction set usually comprises hundreds of instructions while only about 20% of instructions are typically used in most of applications. We have developed a unique language-adaptive embedded processor system for post-manufacturing code optimization and synthesis, resulting in unlimited programmability, innovative instruction cache usage, and significant enhancement (i.e., 5.67× smaller space, 3.14× less energy, and 2.32× faster) alongside simultaneously increased performance.","","978-1-4577-0599-1978-1-4577-0598-4978-1-4577-0597","10.1109/AHS.2011.5963931","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5963931","","Software;Hardware;Decoding;Assembly;Optimization;Benchmark testing;Microarchitecture","cache storage;embedded systems;instruction sets;microprocessor chips;optimisation;program diagnostics","post manufacturing language adaptive embedded processor system;static language;instruction set;language adaptive embedded processor system;post manufacturing code optimization;post manufacturing code synthesis;instruction cache usage","","","18","","","","","","IEEE","IEEE Conferences"
"Clearance optimization of piston/cylinder pair based on virtual prototype of axial piston pump","Z. Junhui; X. Bing","Mechanical Department, Zhejiang University, The State Key Lab of Fluid Power Transmission and control, Hangzhou, Zhejiang, 310058, China; Mechanical Department, Zhejiang University, The State Key Lab of Fluid Power Transmission and control, Hangzhou, Zhejiang, 310058, China","Proceedings of 2012 IEEE/ASME 8th IEEE/ASME International Conference on Mechatronic and Embedded Systems and Applications","","2012","","","166","171","The virtual prototype of axial piston pump is discussed in details through its application in the investigation on piston/cylinder pair. Three sub-models are introduced firstly. The data are transferred between three sub-models through the software interfaces. The liquid-solid coupling and rigid-flexible coupling of piston/cylinder pair model are achieved through the co-simulation model. Then several related test rigs are mentioned. The comparisons of simulation results and experimental results demonstrate that the virtual prototype of axial piston pump has a satisfying accuracy and a great potential in axial piston pump design. At last, the influence the average clearance height of piston/cylinder pair is analyzed. The simulation results indicate that the reduction of the average clearance height between piston and cylinder bore contributes to the reduction of leakage and friction force of piston/cylinder pair, and the improvement of the carrying ability of the lubricating oil film.","","978-1-4673-2349-9978-1-4673-2347-5978-1-4673-2348","10.1109/MESA.2012.6275556","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6275556","axial piston pump;piston/cylinder pair;virtual prototype","Pistons;Friction;Mathematical model;Force;Films;Prototypes;Pumps","digital simulation;engine cylinders;friction;lubricating oils;mechanical engineering computing;optimisation;pistons;pumps;virtual prototyping","clearance optimization;piston-cylinder pair;virtual prototype;axial piston pump design;liquid-solid coupling;rigid-flexible coupling;lubricating oil film;software interfaces;cosimulation model;leakage reduction;friction force reduction","","","13","","","","","","IEEE","IEEE Conferences"
"The optimization of speed hump design: A case study in Malaysia residential streets","N. I. Zainuddin; J. M. Diah; M. A. Adnan; N. Sulaiman","Faculty of Civil Engineering, Universiti Teknologi MARA (UiTM), 40450 Shah Alam, Selangor, Malaysia; Faculty of Civil Engineering, Universiti Teknologi MARA (UiTM), 40450 Shah Alam, Selangor, Malaysia; Faculty of Civil Engineering, Universiti Teknologi MARA (UiTM), 40450 Shah Alam, Selangor, Malaysia; Faculty of Civil Engineering, Universiti Teknologi MARA (UiTM), 40450 Shah Alam, Selangor, Malaysia","2012 IEEE Colloquium on Humanities, Science and Engineering (CHUSER)","","2012","","","368","373","Speed humps have been positively received by local residents in Malaysia but as to date no systematic and ideal guideline were introduce or tested for the effectiveness and were never properly enforced by the local authorities. The purpose of this paper is to develop the 85<sup>th</sup> percentile speed reduction in relation to speed hump geometric design. This research paper presents the experimental design of the research and the empirical model to predict 85<sup>th</sup> percentile speed reduction in relation to speed hump geometric for residential streets in Malaysia. The speed data measurement is performed on the spot speed data at specified points and location using instrumentation called Pro Laser III Laser Gun Meter Detector. The data are prepared and analyzed using the statistical software called Minitab version 16.0. The geometric parameters such as speed hump height, length and width are used to ascertain the effect of 85<sup>th</sup> percentile speed reduction. After several analyses were conducted one model with R-Sq value of 80.6% was developed using multiple linear regressions. If the model using the linear regression analysis is implemented and enhanced into current guideline and standard in Malaysia, it could play an important role in designing or redesigning the speed humps geometric in relation to the desired speed limit for residential streets in Malaysia.","","978-1-4673-4617-7978-1-4673-4615-3978-1-4673-4616","10.1109/CHUSER.2012.6504341","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6504341","Traffic calming;Speed hump and Hump Geometric design","","design engineering;design of experiments;optimisation;regression analysis;shapes (structures);structural engineering;velocity measurement","optimization;speed hump design;Malaysia;residential streets;speed reduction;experimental design;speed data measurement;instrumentation;Pro Laser III Laser Gun Meter Detector;statistical software;multiple linear regressions;linear regression analysis","","","27","","","","","","IEEE","IEEE Conferences"
"Runtime extraction of memory access information from the application source code","S. A. Ostadzadeh; M. Corina; C. Galuzzi; K. Bertels","Computer Engineering Group, Delft University of Technology, Delft, the Netherlands; Computer Engineering Group, Delft University of Technology, Delft, the Netherlands; Computer Engineering Group, Delft University of Technology, Delft, the Netherlands; Computer Engineering Group, Delft University of Technology, Delft, the Netherlands","2011 International Conference on High Performance Computing & Simulation","","2011","","","647","655","The shift towards using increasing numbers of processing elements has placed new burdens on the programming community to fully exploit the potential performance gain of multiprocessor systems. The programming problem is even more complex in the case of systems that utilize reconfigurable devices. The increased complexity of programming necessitates the use of tools that can support programmers in migrating existing applications to these emerging systems. Programmers need increasingly sophisticated tools for profiling and analysis of applications. Particularly, tools to inspect the memory access behavior of applications become crucial due to the processor/memory communication bottleneck. In this paper, we present xQUAD, a unique extension to the QUAD dynamic profiling toolset, which augments the memory access analysis of an application by providing detailed, fine-grained intra-function information. xQUAD provides detailed memory access information on the application source code data object granularity. This information can help programmers for application optimization and revision. We also present a ranking method based on the memory access intensity of a function, which reveals more accurately the suitability of the function for hardware implementation. xQUAD is tested on a real application from the multimedia domain to describe the capabilities of the proposed toolset.","","978-1-61284-383-4978-1-61284-380-3978-1-61284-382","10.1109/HPCSim.2011.5999888","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5999888","Dynamic profiling;Instrumentation;Performance analysis;Code tuning;Reconfigurable architectures;Hardware/Software partitioning","Instruments;Debugging;Hardware;Data mining;Kernel;Runtime;Data structures","inspection;program debugging;software tools;storage management;systems analysis","runtime extraction;memory access information;application source code;processing elements;multiprocessor system;programming problem;reconfigurable device;application profiling;application analysis;memory access behavior inspection;processor-memory communication bottleneck;xQUAD;QUAD dynamic profiling toolset;application optimization;application revision;ranking method;memory access intensity;hardware implementation;multimedia domain;DWARF debugging","","1","26","","","","","","IEEE","IEEE Conferences"
"A Solution for Fault-Tolerance Based on Adaptive Replication in MonALISA","A. Costan; M. I. Andreica; V. Cristea; C. Grigoras","NA; NA; NA; NA","2010 International Conference on P2P, Parallel, Grid, Cloud and Internet Computing","","2010","","","375","380","The domains of usage of large-scale distributed systems have been extending during the past years from scientific to commercial applications. Together with the extension of the application domains, new requirements have emerged for large-scale distributed systems. Among these, fault tolerance is needed by more and more modern distributed applications, not only by the critical ones. In this paper we present a solution aiming at fault tolerant monitoring of the distributed systems within the MonALISA framework. Our approach uses replication and guarantees that all processing replicas achieve state consistency, both in the absence of failures and after failure recovery. We achieve consistency in the former case by implementing a module that ensures that the order of monitoring tuples is the same at all the replicas. To achieve consistency after failure recovery, we rely on check pointing techniques. We address the optimization problem of the replication architecture by dynamically monitoring and estimating inter-replica link throughputs and real-time replica status. We demonstrate the strengths of our solution using the MonALISA monitoring application in a distributed environment. Our tests show that the proposed approach outperforms previous solutions in terms of latency and that it uses system resources efficiently by carefully updating replicas, while keeping overhead very low.","","978-1-4244-8538-3978-0-7695-4237","10.1109/3PGCIC.2010.63","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5662762","fault tolerance;replication;monitoring;distributed systems;Grid computing","Monitoring;Computer architecture;Distributed databases;Fault tolerance;Fault tolerant systems;Availability","distributed processing;optimisation;software architecture;software fault tolerance","fault-tolerance;adaptive replication;MonALISA;large-scale distributed systems;fault tolerant monitoring;failure recovery;optimization problem;replication architecture;inter-replica link throughputs;real-time replica status","","","15","","","","","","IEEE","IEEE Conferences"
"The Research of Web Service Selection Based on the Ant Colony Algorithm","R. Wang; L. Ma; Y. Chen","NA; NA; NA","2010 International Conference on Artificial Intelligence and Computational Intelligence","","2010","3","","551","555","The Web service composition is one of the most important ways to satisfy the users' personalized requirements and supply the high quality service for users, and the foundation of service composition is the selection of Web service. Firstly, expounded the basic principle of Ant Colony Algorithm and analyzed the model of Web service selection and brought the algorithm into the fields of Web service selection and transformed the problem of Web service selection based on the QoS into the problem of finding the optimization path. Secondly, given the steps for solving the problem of Web service selection based on the Ant Colony Algorithm and contrasted the results under different parameters. At last, used the virtual scene, the author tested the validity of Ant Colony Algorithm in solving the problem of Web Service selection.","","978-1-4244-8432","10.1109/AICI.2010.354","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5656760","ant colony algorithm;swarm intelligence;Web service selection;quality of service","Web services;Quality of service;Heuristic algorithms;Cities and towns;Software algorithms;Convergence;Computers","optimisation;quality of service;Web services","Web service selection;ant colony algorithm;users' personalized requirements;QoS;optimization;virtual scene","","3","9","","","","","","IEEE","IEEE Conferences"
"Multidisciplinary Modeling Based on Model Unified Description Methods for Complex System","W. Jiangyun; T. Liang","NA; NA","2010 First International Conference on Pervasive Computing, Signal Processing and Applications","","2010","","","743","746","The model of complex system concerns mechanical, optical, electrical, hydraulic, control, thermal and aerodynamic disciplines. Multidisciplinary modeling technology can provide an effective method for design, optimization and test of complex system. Research status of multidisciplinary modeling is analyzed and its definition is given. The multidisciplinary modeling method based on model unified description is presented and model specifications are provided. Models are expressed as atomic model and compound model form that includes such elements as attributes, interface variables and behavior. Multidisciplinary model of complex satellite is then studied based on model unified description method.","","978-1-4244-8043-2978-0-7695-4180","10.1109/PCSPA.2010.185","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635486","multidisciplinary modeling;model unified description;atomic model;compound model","Mathematical model;Object oriented modeling;Analytical models;Satellites;Load modeling;Compounds;Software","artificial satellites;formal specification;large-scale systems;optimisation;Unified Modeling Language","multidisciplinary modeling;model unified description method;complex system;optimization;model specification;atomic model;compound model;interface variable;complex satellite;model unified description method","","","8","","","","","","IEEE","IEEE Conferences"
"Miniature robot BeBot: Mechatronic test platform for self-x properties","J. Gausemeier; T. Schierbaum; R. Dumitrescu; S. Herbrechtsmeier; A. Jungmann","Product Engineering, Heinz Nixdorf Institute, University of Paderborn, Germany; Product Engineering, Heinz Nixdorf Institute, University of Paderborn, Germany; Product Engineering, Heinz Nixdorf Institute, University of Paderborn, Germany; System and Circuit Technology, Heinz Nixdorf Institute, University of Paderborn, Germany; C-LAB, University of Paderborn, Germany","2011 9th IEEE International Conference on Industrial Informatics","","2011","","","451","456","Machines are omnipresent. They produce, they transport. Machines facilitate work and assist. The increasing penetration of mechanical engineering by information technology enables considerable benefits. We refer to such systems as advanced mechatronic systems, which relay on the close interaction of mechanics, electric/electronics, control engineering and software engineering. Hence, the design and production of such systems is an interdisciplinary and complex task. Our ambition is a new school for the design of advanced mechatronic systems. Consequently, we need an avant-garde basic system which can be used to develop and to test future applications. The miniature robot BeBot is such a basic system. This robot constitutes the test bench for the applications, being based on modern approaches, such as self-optimization, self-organization and self-coordination as well as on the use of new manufacturing technologies.","1935-4576;2378-363X","978-1-4577-0434-5978-1-4577-0435-2978-1-4577-0433","10.1109/INDIN.2011.6034921","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6034921","","Mechatronics;Robot kinematics;Robot sensing systems;Batteries;Cameras","mechatronics;microrobots;sensors","BeBot miniature robot;mechatronic test platform;robot self-x property;mechanical engineering;information technology;mechatronic system;control engineering;software engineering;avant-garde basic system;robot self-optimization;robot self-organization;robot self-coordination","","2","15","","","","","","IEEE","IEEE Conferences"
"The measurement of foreign indebtedness in the selected transition countries (multivariate and multicriterial analysis) using SPSS and Decision Lab Softwares","M. Pečarić; S. Pivac; M. Jurun","Faculty of Economics Split, University of Spli, Matice hrvatske 31, 21000 Split, Croatia; Faculty of Economics Split, University of Split, Matice hrvatske 31, 21000 Split, Croatia; Undergraduate student, Faculty of Economics Split, University of Spli, Matice hrvatske 31, 21000 Split, Croatia","2011 Proceedings of the 34th International Convention MIPRO","","2011","","","1183","1188","The aim of this paper is statistical analysis of foreign indebtedness in the selected transition countries using appropriate software support. The paper stress out the benefit of using SPSS and Decision Lab in the process of education and research. There is a constant evidence of the growth of the foreign indebtedness in all the countries in transition, both EU member states and (pre)accession countries. The status and trends of external debt are important indicators of potential macroeconomic problems, determining that the management of foreign debt should be a task for all governments. After appropriate testing, classification of selected countries in transition (Bosnia and Herzegovina, Croatia, Czech, Estonia, Hungary, Latvia, Latvia, Macedonia, Poland, Slovenia) is done using multivariate cluster analysis (SPSS) according to the key indicators of the state and trends of foreign indebtedness. In addition, ranking (Decision Lab) of those countries will be done in relation to the indebtedness indicators by the multicriteria analysis method. Comparative analysis of the results will be also done. The additional advantage of such analysis is that classification and ranking can be done for all countries, based on all indicators of external indebtedness at the same time using SPSS and Decision Lab software.","","978-953-233-059-5978-1-4577-0996-8978-953-233-067","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5967236","","Economic indicators;Software;Couplings;Statistical analysis;Macroeconomics;Investments","decision support systems;macroeconomics;mathematics computing;public finance;statistical analysis","foreign indebtedness measurement;selected transition countries;multivariate analysis;multicriterial analysis;SPSS;Decision Lab softwares;statistical analysis;macroeconomic problems;foreign debt management;multivariate cluster analysis","","","11","","","","","","IEEE","IEEE Conferences"
"Compilation of stream programs for multicore processors that incorporate scratchpad memories","W. Che; A. Panda; K. S. Chatha","Faculty of Computer Science and Engineering, Arizona State University, Tempe, 85287, USA; Faculty of Computer Science and Engineering, Arizona State University, Tempe, 85287, USA; Faculty of Computer Science and Engineering, Arizona State University, Tempe, 85287, USA","2010 Design, Automation & Test in Europe Conference & Exhibition (DATE 2010)","","2010","","","1118","1123","The stream processing characteristics of many embedded system applications in multimedia and networking domains have led to the advent of stream based programming formats. Several multicore processors aimed at embedded domains incorporate scratchpad memories (SPM) due to their superior power consumption characteristics. The paper addresses the problem of compiling stream programs on to multi-core processors that incorporate SPM. Performance optimization on SPM based processors requires effective schemes for software based management of code and/or data overlay. In the context of our problem instance the code overlay scheme impacts both the stream element to core mapping and memory available for inter-processor communication. The paper presents an integer linear programming (ILP) formulation and heuristic approach that effectively exploit the SPM to maximize the throughput of stream programs when mapped to multicore processors. The experimental results demonstrate the effectiveness of the proposed techniques by compiling StreamIt based benchmark applications on the IBM Cell processor and comparing against existing approach.","1558-1101;1530-1591;1530-1591","978-3-9810801-6-2978-1-4244-7054-9978-3-9810801-6","10.1109/DATE.2010.5456976","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5456976","","Multicore processing;Streaming media;Scanning probe microscopy;Embedded system;Multimedia systems;Energy consumption;Optimization;Software performance;Context;Integer linear programming","embedded systems;integer programming;linear programming;multiprocessing systems;program compilers","stream program compilation;multicore processors;embedded system applications;performance optimization;code overlay scheme;integer linear programming;heuristic approach;Streamit based benchmark applications;IBM cell processor;scratchpad memories;power consumption","","4","21","","","","","","IEEE","IEEE Conferences"
"Research of State Exact Feedback Linearization Control of Shunt Single-Phase Active Power Filter","F. Chen; Z. Chen; H. Wang; J. Le","NA; NA; NA; NA","2010 Asia-Pacific Power and Energy Engineering Conference","","2010","","","1","4","The paper presents a nonlinear control strategy for single-phase active power filter applying exact feedback linearization theory, since this technique has been successfully applied to other area of power electronic. Using the state-space average modeling method, a nonlinear affine mode of single-phase shunt active power filter is set up. It is testified that the model is satisfied with the condition of exact feedback linearization. Further, the nonlinear output function is derived. So the system model can be linearized by means of a nonlinear transformation. After that, the feedback coefficients are selected using optimization theory. In order to validate the correctness and validity, the operation of control circuit has been explained using MATLAB software and simulation. The simulation result shows that the single-phase active power filter applied the proposed control strategy has well stability and dynamic character.","2157-4839;2157-4847","978-1-4244-4812-8978-1-4244-4813","10.1109/APPEEC.2010.5449133","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5449133","","State feedback;Linear feedback control systems;Active filters;Mathematical model;Power system modeling;Circuit simulation;Power electronics;Testing;MATLAB;Circuit stability","active filters;linearisation techniques;mathematics computing;nonlinear control systems;optimisation;power electronics;power filters;state feedback","state exact feedback linearization control;shunt single-phase active power filter;nonlinear control;power electronic;state-space average modeling;nonlinear affine mode;exact feedback linearization;nonlinear output function;nonlinear transformation;feedback coefficients;optimization theory;MATLAB","","","10","","","","","","IEEE","IEEE Conferences"
"Realtime simulation of control algorithms including the dedicated control hardware","M. Gorski; R. Bartelt; C. Heising; V. Staudt; A. Steimel","Ruhr-University of Bochum, D-44780, Germany; Ruhr-University of Bochum, D-44780, Germany; Ruhr-University of Bochum, D-44780, Germany; Ruhr-University of Bochum, D-44780, Germany; Ruhr-University of Bochum, D-44780, Germany","2010 12th International Conference on Optimization of Electrical and Electronic Equipment","","2010","","","532","537","A number of different steps are necessary to develop power-electronic systems: After preparation of a concept, the layout of the controller is made based on simulation. The process is finalised by implementation and test on the power-electronic system itself. A successful final test requires, however, that the controller hardware controlling the power-electronic system functions properly. Additionally, the control software must be implemented correctly and function properly. At this point, a real-time simulator becomes useful which allows to verify the function of the controller hard- and software in real-time. This is realized by simulating the entire controlled system - including the power electronics - by means of state equations in a simulation algorithm. This method offers the possibility to test the control hardware without endangering the costly power electronics.","1842-0133;1842-0133","978-1-4244-7020-4978-1-4244-7019-8978-1-4244-7018","10.1109/OPTIM.2010.5510540","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5510540","","Hardware;Real time systems;Control systems;Differential equations;Power electronics;Circuit simulation;Power semiconductor switches;Field programmable gate arrays;System testing;Electronic equipment testing","digital simulation;electronic engineering computing;power electronics;real-time systems","control algorithms real-time simulation;dedicated control hardware;power electronic system;real-time simulator;simulation algorithm","","1","13","","","","","","IEEE","IEEE Conferences"
"Visual Amortization Analysis of Recompilation Strategies","S. Zimmer; S. Diehl","NA; NA","2010 14th International Conference Information Visualisation","","2010","","","509","514","Dynamic recompilation tries to produce more efficient code by exploiting runtime information. Virtual machines like the Jikes RVM use recompilation heuristics to decide how to recompile the program, i.e. what parts are recompiled at what level of optimization. In this paper we present our post-mortem amortization analysis based on improved call stack sampling. Our tool presents the results of the analysis as an interactive visualizations to help both virtual machine implementors improve their recompilation strategies, as well as programmers assess whether these recompilation strategies pay off not only for their application as a whole, but also for individual methods.","2375-0138;1550-6037","978-1-4244-7846","10.1109/IV.2010.76","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5571165","software visualization;recompilation;dynamic analysis","Context;Optimization;Estimation;Visualization;Instruments;Runtime;Benchmark testing","data visualisation;program compilers;virtual machines","visual amortization analysis;recompilation strategy;virtual machines;Jikes RVM use recompilation heuristics;improved call stack sampling;interactive visualizations","","1","10","","","","","","IEEE","IEEE Conferences"
"Optimizating one-class techniques applied to verify information extractors","I. F. de Viana; P. J. Abad; J. L. Álvarez; J. L. Arjona","Departamento de Tecnologías de le Información, Universidad de Huelva, Huelva, Spain; Departamento de Tecnologías de le Información, Universidad de Huelva, Huelva, Spain; Departamento de Tecnologías de le Información, Universidad de Huelva, Huelva, Spain; Departamento de Tecnologías de le Información, Universidad de Huelva, Huelva, Spain","7th Iberian Conference on Information Systems and Technologies (CISTI 2012)","","2012","","","1","6","One-class techniques are classification algorithms, largely unsupervised, that learn using a single class. The problem of verify the information obtained by an informaction extractor, could be considered as a one-class problem because the verifier is build only using instances of classes we want to extract. We propose the use of a multi-level classifier based on One-class techniques to solve the problem of verify information. The need for this new proposal arises from the bad behavior of One-class techniques when they use categorical characteristics. As we shall see, its use significantly improves the performance of all of the algorithms studied. To evaluate the performance obtained by these techniques and modifications, we use different databases proposed in the literature as well as nonparametric statistical test that will help us strengthen the statistical significance of the results achieved.","2166-0735;2166-0727;2166-0727","978-989-96247-7-1978-1-4673-2843-2978-989-96247-7","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6263212","Palabras Clave-Detección de Outlier;Clasificadores One Class;Reconocimiento de Novedades;Web Wrapper","Silicon;Data mining;Classification algorithms;Media;Proposals;Databases;Software","natural language processing;optimisation;pattern classification","optimizating one class techniques;information extractors verification;classification algorithms;multilevel classifier;information verification;natural language processing","","","24","","","","","","IEEE","IEEE Conferences"
"Optimizing Incremental Scope-Bounded Checking with Data-Flow Analysis","D. Shao; D. Gopinath; S. Khurshid; D. E. Perry","NA; NA; NA; NA","2010 IEEE 21st International Symposium on Software Reliability Engineering","","2010","","","408","417","We present a novel approach to optimize incremental scope-bounded checking of programs using a relational constraint solver. Given a program and its correctness specification, scope-bounded checking encodes control-flow and data-flow of bounded code segments into declarative formulas and uses constraint solvers to search for correctness violations. For non-trivial programs, the formulas are often complex and represent a heavy workload that can choke the solvers. To scale scope-bounded checking, our previous work introduced an incremental approach that uses the program's control-flow as a basis of partitioning the program and generating several sub-formulas, which represent simpler problem instances for the underlying solvers. This paper introduces a new approach that uses the program's dataflow, specifically variable-definitions, as a basis for incremental checking. Experimental results show that the use of data-flow provides a significant reduction in the number of variables in the encoded formulas over the previous control-flow-based approach, thereby further improving scalability of scopebounded checking.","1071-9458;1071-9458;2332-6549","978-1-4244-9056-1978-0-7695-4255","10.1109/ISSRE.2010.27","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635077","Scope-bounded checking;Alloy;first-order logic;SAT;lightweight formal method;computation graph;white-box testing;data-flow analysis;JML","Complexity theory;Metals;Encoding;Scalability;Java;Semantics;Engines","data flow analysis;program verification;search problems","incremental scope bounded checking;data flow analysis;relational constraint solver;correctness specification;control flow;bounded code segment","","5","39","","","","","","IEEE","IEEE Conferences"
"Components and Aspects Composition Planning for Ubiquitous Adaptive Services","M. Alia; M. Beauvois; Y. Davin; R. Rouvoy; F. Eliassen","NA; NA; NA; NA; NA","2010 36th EUROMICRO Conference on Software Engineering and Advanced Applications","","2010","","","231","234","In ubiquitous environments, resources limitations and fluctuations combined with device mobility requires the dynamic adaptation of mobile applications. This paper reports on an extension of the MUSIC adaptation middleware to support aspect-oriented programming in order to handle cross-cutting adaptations. Basically, this extension specifies an architectural model for defining applications as a composition of aspects and components. The dynamic adaptation of an application in a given context is realised by selecting the appropriate component and aspect implementations using utility functions as a mean of optimising the overall QoS. Our approach and middleware are implemented and tested on top of OSGi framework.","2376-9505;1089-6503","978-1-4244-7901","10.1109/SEAA.2010.45","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5598102","sel-adaptive software systems;ubiquitous computing;services-orientation;aspect-weaving;components","Middleware;Weaving;Adaptation model;Multiple signal classification;Quality of service;Planning;Security","aspect-oriented programming;middleware;mobile computing;quality of service;utility programs","aspect composition planning;ubiquitous adaptive service;device mobility;MUSIC adaptation middleware;aspect oriented programming;cross-cutting adaptation;dynamic adaptation;utility function;QoS;OSGi framework","","2","20","","","","","","IEEE","IEEE Conferences"
"Mapping farmland organic matter using HSI image and its effects of land-use types","G. Xiaohe; D. Yansheng; W. Kun","Beijing Research Center for Information Technology in Agriculture, Beijing, China; Beijing Research Center for Information Technology in Agriculture, Beijing, China; Physics and Software Engineering, Lanzhou Jiaotong University, Lanzhou, China","2012 First International Conference on Agro- Geoinformatics (Agro-Geoinformatics)","","2012","","","1","4","Spatial distribution of farmland organic is essential for soil fertility adjustment, land-use change and sustainable development of agriculture. It is important to develop a rapid method for mapping farmland organic matter at county scale using remote sensing technology. The HJ-1A HSI image used in the paper has 115 bands, which result in good response to soil organic matter. With the support of in-situ sample data, the correlation between organic matter and characteristic variants of HSI image was analyzed. Then the optimized response wavebands and feature algorithm was selected. Through the application of multiple linear regressions, the model of retrieving farmland organic matter at a county scale was developed. Results indicated that the visible and near infrared bands of HSI image, especially 540-860 nm bands, had good response to farmland organic matter. The model of first order differential logarithm of HSI reflectivity could reach best accuracy, of which the determination coefficients (R) of training and testing samples were both higher than 0.7, while the RMSEs all around 0.2%. The spatial distribution of farmland organic matter was mapped by the model and HSI image. It is concluded that the HJ1A-HSI image has good response and coverage ability for farmland organic matter, which could provide an effective model of mapping organic matter on a county scale. The study also shows that land-use types have certain influence to farmland organic matter.","","978-1-4673-2496-0978-1-4673-2495-3978-1-4673-2494","10.1109/Agro-Geoinformatics.2012.6311723","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6311723","organic matter;HSI;multiple linear regressions;mapping;land-use type","Soil;Remote sensing;Correlation;Reflectivity;Linear regression;Testing","geophysical image processing;regression analysis;soil;sustainable development;terrain mapping","farmland organic matter mapping;land use type;spatial distribution;soil fertility adjustment;land use change;agriculture sustainable development;remote sensing technology;HJ-1A HSI image;optimized response waveband;feature algorithm;multiple linear regression;first order differential logarithm","","1","11","","","","","","IEEE","IEEE Conferences"
"Scalable Performance Predictions of Distributed Peer-to-Peer Applications","B. F. Cornea; J. Bourgeois; T. T. Nguyen; D. El-Baz","NA; NA; NA; NA","2012 IEEE 14th International Conference on High Performance Computing and Communication & 2012 IEEE 9th International Conference on Embedded Software and Systems","","2012","","","193","201","Recently, a new environment for high performance peer-to-peer distributed computing was proposed. This environment, named P2PDC, addresses stable or volatile systems communicating in a decentralized manner using the self-adaptive protocol P2PSAP. P2PDC is devoted to task parallel applications like numerical simulation problems or optimization problems solved via parallel or distributed iterative algorithms. For distributed applications meant to run with P2PDC, a performance prediction tool named dPerf was proposed. dPerf combines static and dynamic analysis with trace-based simulation to provide scientist with information about the execution of their large scale numerical simulation applications. dPerf addresses real parallel and distributed numerical simulation and optimisation applications written in C, C++ or Fortran for P2PDC. This paper introduces an enhancement of the dPerf tool which provides scalable performance prediction results. Scaling is done with respect to (i) network configuration and (ii) number of peers. Scaling predictions based on network configuration is achieved through trace-based simulation, where various architectures can be studied. Scaling predictions based on the number of peers implies analyzing the communication topology and modifying trace files prior to simulation. We present experimental results obtained for the obstacle problem, a C/P2PDC implementation of the code used in mechanics and finance. Prediction for this application is computed under real conditions, with a reduced slowdown and by providing user with scalable results.","","978-1-4673-2164-8978-0-7695-4749","10.1109/HPCC.2012.34","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6331994","Scalability;performance prediction;static analysis;dynamic analysis;peer-to-peer computing;high performance computing;task parallel model;numerical simulation","Peer to peer computing;Topology;Computer architecture;Predictive models;Scalability;Computational modeling;Benchmark testing","C++ language;iterative methods;numerical analysis;optimisation;parallel architectures;parallel processing;peer-to-peer computing;protocols","scalable performance prediction;peer-to-peer distributed computing;P2PDC;self-adaptive protocol;P2PSAP;task parallel application;parallel iterative algorithm;dPerf;static analysis;dynamic analysis;trace-based simulation;numerical simulation;optimisation;C;C++;dPerf tool;network configuration","","3","32","","","","","","IEEE","IEEE Conferences"
"Browsing in the virtual museum of the sarcophagi in the Basilica of St. Silvestro at the Catacombs of Priscilla in Rome","L. Pecchioli; M. Pucci; F. Mohamed; B. Mazzei","Seminar for the Languages and Cultures of the Near East Ruprecht-Karls-University Heidelberg, Heidelberg, Germany; Dipartimento di Architettura, Disegno Storia e Progetto, University of Florence, Florence, Italy; Department of Chemistry, Humboldt University, Berlin, Germany; Responsabile Settore Restauro, Musei e Archivi PCAS, Pontificia Commissione di Archeologia Sacra, Rome, Italy","2012 18th International Conference on Virtual Systems and Multimedia","","2012","","","413","420","This paper outlines a program (ISEE) to visualise interactive 3D environments and access information through the Web. ISEE had been prototyped as content management tool with Internet Explorer since 2008 and currently supports the major browsers. Its database of information is stored in MySQL. The basic idea of the software is to enable information retrieval by simply looking inside a 3D environment, since moving and looking in the real world are basic interactions, which all viewers use. It ranks the relevant information by means of its position/orientation of the viewer in 3D space. The ranking algorithm that we developed matches the intuitive expectation of users as verified by means of formal usability tests. We present its application in the project in progress for the Museum of the sculpture in the Basilica of St. Silvestro at the Catacombs of Priscilla in Rome, where the user will be able to retrieve information through the virtual visit and using QR-code in situ for each fragment of sarcophagi. An important aspect with internet usage is the optimisation of the 3D model. This has been achieved creating a low poly 3D mesh with the application of normal and displacement maps generated with a baking process of the high poly 3D model. In this way we have optimized for the Web and the interactive use. To reduce the chromatic aberration a massive photographic campaign was used to texturize the 3D model through a camera matching process. The result is a low poly 3D model fully texturized ready to use and imported in many 3D viewers. Normally we use the Unity 3D technology (http://unity3d.com) to visualize and interactively navigate 3D models. The Unity plug-in is available for all the major browsers (IE, Firefox, Safari) and platforms (Windows, OsX). Our interdisciplinary collaboration is a contribute to try an innovative solution for the accessing and managing the information in its context.","","978-1-4673-2564-6978-1-4673-2563","10.1109/VSMM.2012.6365953","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6365953","web-based visualization;sarcophagi;relevant information;catacomb;3D Gaussian","Solid modeling;Context;Cultural differences;Software;Navigation;Educational institutions;Internet","cameras;data visualisation;image matching;Internet;mesh generation;museums;online front-ends;relevance feedback;solid modelling;SQL;virtual reality;visual databases","virtual museum browsing;interactive 3D environment visualisation;Web information access;content management tool;Internet Explorer;Web browsers;MySQL;information database;3D space viewer orientation;ranking algorithm;formal usability tests;sculpture;Priscilla catacomb;St. Silvestro basilica;Rome;virtual visit;QR-code;Internet usage;3D model optimisation;low poly 3D mesh creation;displacement maps;baking process;high poly 3D model;chromatic aberration reduction;camera matching process;Unity 3D technology;interactive 3D model navigation;Unity plug-in;information management;relevant information retrieval","","1","14","","","","","","IEEE","IEEE Conferences"
"A hardware/software co-design model for face recognition using Cognimem Neural Network chip","S. Sardar; G. Tewari; K. A. Babu","DRDO, Hyderabad, India; DRDO, Hyderabad, India; DRDO, Hyderabad, India","2011 International Conference on Image Information Processing","","2011","","","1","6","Automated Face recognition is a technique employed in wide-range of practical applications, which include access control, identification systems, surveillance and law enforcement applications to name a few, and future improvements promise to spread the use of face recognition further still. Radial Basis Function Networks (RBFN) have proven effective approach for face recognition. Software implementations fail to capture the inherent parallelism of RBFN and incur long training time. Although, hardware implementations can speed up the training process, they may lead to inflexible solution. The main challenges of Face Recognition today are broad lightning variations, handling rotation in depth, together with personal appearance changes. A highly accurate face recognition system requires a number of complex sub-operations to be performed. To balance the flexibility of the involved sub-modules and to achieve high accuracy in face recognition, we propose an embedded computing system, consisting of a processor and dedicated fully parallelized Cognimem Neural Network chip based board. We will also identify the optimized algorithm for each of the involved sub-operations. Results obtained after testing our proposed system, with standard databases, show promising performances in terms of Recognition accuracy, False acceptance rate (FAR), False rejection rate (FRR), training time and testing time.","","978-1-61284-861-7978-1-61284-859-4978-1-61284-860","10.1109/ICIIP.2011.6108866","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6108866","Radial basis function Networks (RBFN);Cognimem Neural Network chip;accuracy;FAR;FRR","Face;Face recognition;Principal component analysis;Neurons;Databases;Accuracy;Training","discrete wavelet transforms;embedded systems;face recognition;hardware-software codesign;image classification;principal component analysis;radial basis function networks","hardware-software co-design model;Cognimem neural network chip;automated face recognition;radial basis function networks;embedded computing system;recognition accuracy;false acceptance rate;false rejection rate;PCA method;ANN classifier;DWT","","12","23","","","","","","IEEE","IEEE Conferences"
"Autoadaptivity and Optimization in Distributed ECG Interpretation","P. Augustyniak","Akademia Górniczo-Hutnicza University of Science and Technology, Kraków, Poland","IEEE Transactions on Information Technology in Biomedicine","","2010","14","2","394","400","This paper addresses principal issues of the ECG interpretation adaptivity in a distributed surveillance network. In the age of pervasive access to wireless digital communication, distributed biosignal interpretation networks may not only optimally solve difficult medical cases, but also adapt the data acquisition, interpretation, and transmission to the variable patient's status and availability of technical resources. The background of such adaptivity is the innovative use of results from the automatic ECG analysis to the seamless remote modification of the interpreting software. Since the medical relevance of issued diagnostic data depends on the patient's status, the interpretation adaptivity implies the flexibility of report content and frequency. Proposed solutions are based on the research on human experts behavior, procedures reliability, and usage statistics. Despite the limited scale of our prototype client-server application, the tests yielded very promising results: the transmission channel occupation was reduced by 2.6 to 5.6 times comparing to the rigid reporting mode and the improvement of the remotely computed diagnostic outcome was achieved in case of over 80% of software adaptation attempts.","1089-7771;1558-0032","","10.1109/TITB.2009.2038151","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5373927","Distributed systems;e-health;home care;pervasive ECG monitoring;ubiquitous computing","Electrocardiography;Medical diagnostic imaging;Surveillance;Wireless communication;Digital communication;Data acquisition;Availability;Frequency;Humans;Statistics","biomedical telemetry;client-server systems;electrocardiography;health care;medical diagnostic computing;medical signal processing;patient diagnosis;patient monitoring;ubiquitous computing;wireless sensor networks","distributed ECG interpretation autoadaptivity;distributed ECG interpretation optimisation;distributed surveillance network;distributed biosignal interpretation networks;automatic ECG analysis;diagnostic data;report content flexibility;report frequency flexibility;human experts behavior;procedure reliability;usage statistics;client-server application;transmission channel occupation;remotely computed diagnostics;wireless digital communication;electrocardiography","Algorithms;Diagnosis;Electrocardiography;Humans;Signal Processing, Computer-Assisted;Software;Telemedicine;Telemetry","4","35","","","","","","IEEE","IEEE Journals & Magazines"
"SMOS L1 algorithms","A. Gutiérrez; J. Barbosa; N. Catarino; R. Castro; S. Freitas; B. Lucas; H. Candeias; J. Freitas; M. Ventura; M. Zundo","Ground Segment Systems, Deimos Engenharia, S.A.; Lisboa, Portugal; Ground Segment Systems, Deimos Engenharia, S.A.; Lisboa, Portugal; Ground Segment Systems, Deimos Engenharia, S.A.; Lisboa, Portugal; Ground Segment Systems, Deimos Engenharia, S.A.; Lisboa, Portugal; Ground Segment Systems, Deimos Engenharia, S.A.; Lisboa, Portugal; Ground Segment Systems, Deimos Engenharia, S.A.; Lisboa, Portugal; Ground Segment Systems, Deimos Engenharia, S.A.; Lisboa, Portugal; Command and Control, Critical Software S.A.; Lisboa, Portugal; Command and Control, Critical Software S.A.; Lisboa, Portugal; European Space Agency; Noordwijk, The Netherlands","2010 IEEE International Geoscience and Remote Sensing Symposium","","2010","","","3154","3157","The Level 1 Processing of SMOS transforms the data acquired by MIRAS (Microwave Imaging Radiometer with Aperture Synthesis) into geolocated TOA Brightness Temperatures, providing observation angles and additional parameters for the Level 2 Processor. Prior to SMOS launch in November 2009 the Level 1 Prototype Processor (L1PP) lead the way for specifying product types and contents, as well as to define, implement and validate all processing algorithms. During the six months of Commissioning, L1PP continued to be the testing environment for all new algorithms and proposed modifications to the Level 1 products. Particular emphasis should be given to LIPP's capability to produce the first image from SMOS. Within less than three hours after the data acquisition at ESAC, L1PP generated images. L1PP has also been tuned to identify unforeseen hardware problems that have been spotted only with the satellite in-orbit. This paper is divided in 3 sections: I) a high level description of the LI processing strategy and functional blocks of the processor (called processing units); II) important results obtained during the In-Orbit Commissioning Phase (IOCP), namely for calibration optimization, image reconstruction improvement, geolocation assessment and the impact on scientific results, in particular, to insure optimal input to Level 2 Soil Moisture and Ocean Salinity retrieval; and III) conclusions from the Commissioning Phase.","2153-7003;2153-6996;2153-6996","978-1-4244-9566-5978-1-4244-9565-8978-1-4244-9564","10.1109/IGARSS.2010.5650097","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5650097","SMOS;L1 processing;software;algorithms;commissioning","Calibration;Sensitivity;Accuracy;Radiometry;Brightness temperature;Instruments;Image reconstruction","calibration;data acquisition","SMOS L1 algorithms;processing algorithms;data acquisition;high level description;calibration optimization;image reconstruction improvement;geolocation assessment","","1","11","","","","","","IEEE","IEEE Conferences"
"Synthesis of Multitask Implementations of Simulink Models With Minimum Delays","M. Di Natale; L. Guo; H. Zeng; A. Sangiovanni-Vincentelli","Scuola Superiore S. Anna, Pisa, Italy; Department of Electrical Engineering and Computer Sciences, University of California at Berkeley, Berkeley, CA, USA; NA; Department of Electrical Engineering and Computer Sciences, University of California at Berkeley, Berkeley, CA, USA","IEEE Transactions on Industrial Informatics","","2010","6","4","637","651","Model-based design of embedded control systems using Synchronous Reactive (SR) models is among the best practices for software development in the automotive and aeronautic industry. SR models allow to formally verify the correctness of the design and automatically generate the implementation code. This feature is a major productivity enhancement and, more importantly, can ensure correct-by-design software provided that the code generator is provably correct. This paper presents an improvement of code generation technology for SR obtained via a novel algorithm for optimizing the multitask implementation of Simulink models on single-processor platforms with limited availability of memory. Existing code generation tools require the addition of zero-order hold (ZOH) blocks, and therefore additional memory, and possibly also additional functional delays whenever there is a rate transition in the computation and communication flow. Our algorithm leverages a novel efficient encoding of the scheduling feasibility region to find the task implementation of function blocks with minimum additional functional delays within timing and memory constraints. The algorithm is applied to an automotive case study with tens of function blocks and very high utilization to test its applicability to complex systems.","1551-3203;1941-0050","","10.1109/TII.2010.2072511","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5582169","Code generation;mixed-integer linear programming (MILP);real-time programming;Simulink;schedulability;software models","Delay;Computational modeling;Multitasking;Real time systems;Mixed integer linear programming;Random access memory;Embedded systems","aerospace industry;automobile industry;embedded systems;program compilers;software engineering","embedded control systems;synchronous reactive model;correct-by-design software;code generation technology;zero-order hold blocks;multitask implementation synthesis;Simulink models;functional delays;software development;automotive industry;aeronautic industry","","32","25","","","","","","IEEE","IEEE Journals & Magazines"
"On deciding between conservative and optimistic approaches on massively parallel platforms","C. D. Carothers; K. S. Perumalla","Department of Computer Science, Rensselaer Polytechnic Institute, 110 8th Street, Troy, New York 12180, USA; Computer Sciences and Engineering Division, Oak Ridge National Laboratory, P.O. Box 2008 MS-6085, TN 37831-6085, USA","Proceedings of the 2010 Winter Simulation Conference","","2010","","","678","687","Over 5000 publications on parallel discrete event simulation (PDES) have appeared in the literature to date. Nevertheless, few articles have focused on empirical studies of PDES performance on large supercomputer-based systems. This gap is bridged here, by undertaking a parameterized performance study on thousands of processor cores of a Blue Gene supercomputing system. In contrast to theoretical insights from analytical studies, our study is based on actual implementation in software, incurring the actual messaging and computational overheads for both conservative and optimistic synchronization approaches of PDES. Complex and counter-intuitive effects are uncovered and analyzed, with different event timestamp distributions and available levels of concurrency in the synthetic benchmark models. The results are intended to provide guidance to the PDES community in terms of how the synchronization protocols behave at high processor core counts using a state-of-the-art supercomputing systems.","1558-4305;0891-7736;0891-7736","978-1-4244-9865-9978-1-4244-9866-6978-1-4244-9864","10.1109/WSC.2010.5679119","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5679119","","Synchronization;Computational modeling;Supercomputers;Protocols;Hardware;Benchmark testing;Optimized production technology","discrete event simulation;mainframes;microcomputers;parallel machines;synchronisation","parallel discrete event simulation;processor cores;Blue Gene supercomputing system;conservative synchronization approach;optimistic synchronization approach;counter intuitive effect;synchronization protocol","","19","26","","","","","","IEEE","IEEE Conferences"
"Robustness Against the Decision-Maker's Attitude to Risk in Problems With Conflicting Objectives","L. T. Bui; H. A. Abbass; M. Barlow; A. Bender","Department of Software Engineering, Le Quy Don University, Hanoi, Vietnam; Defense and Security Applications Research Center, University of New South Wales, Australian Defense Force Academy, Canberra, Australia; Defense and Security Applications Research Center, University of New South Wales, Australian Defense Force Academy, Canberra, Australia; Land Operations Division, Defense, Science and Technology Organization, Edinburgh, Australia","IEEE Transactions on Evolutionary Computation","","2012","16","1","1","19","In multiobjective optimization problems (MOPs), the Pareto set consists of efficient solutions that represent the best trade-offs between the conflicting objectives. Many forms of uncertainty affect the MOP, including uncertainty in the decision variables, parameters or objectives. A source of uncertainty that is not studied in the evolutionary multiobjective optimization (EMO) literature is the decision-maker's attitude to risk (DMAR) even though it has great significance in real-world applications. Often the decision-makers change over the course of the decision-making process and thus, some relevant information about preferences of future decision-makers is unknown at the time a decision is made. This poses a major risk to organizations because a new decision-maker may simply reject a decision that has been made previously. When an EMO technique attempts to generate the set of nondominated solutions for a problem, then DMAR-related uncertainty needs to be reduced. Solutions generated by an EMO technique should be robust against perturbations caused by the DMAR. In this paper, we focus on the DMAR as a source of uncertainty and present two new types of robustness in MOP. In the first type, dominance robustness (DR), the robust Pareto solutions are those which, if perturbed, would have a high chance to move to another Pareto solution. In the second type, preference robustness (PR), the robust Pareto solutions are those that are close to each other in configuration space. Dominance robustness captures the ability of a solution to move along the Pareto optimal front under some perturbative variation in the decision space, while PR captures the ability of a solution to produce a smooth transition (in the decision variable space) to its neighbors (defined in the objective space). We propose methods to quantify these robustness concepts, modify existing EMO techniques to capture robustness against the DMAR, and present test problems to examine both DR and PR.","1089-778X;1089-778X;1941-0026","","10.1109/TEVC.2010.2051443","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5557781","Decision making;evolutionary;multi-objective;risk;robustness","Robustness;Uncertainty;Optimization;Noise;Decision making;Organizations;Biological system modeling","decision making;evolutionary computation;Pareto optimisation;risk management","decision-maker attitude-to-risk;conflicting objectives;evolutionary multiobjective optimization problems;Pareto set;decision variables;decision-making process;EMO technique;DMAR-related uncertainty;dominance robustness;robust Pareto solutions;preference robustness;configuration space","","13","54","","","","","","IEEE","IEEE Journals & Magazines"
"A multi-objective genetic algorithm for the QoS based routing and wavelength allocation problem","Hongyi Zhang; Zhidong Shen","International School of Software, Wuhan University, China; International School of Software, Wuhan University, China","2012 8th International Conference on Computing and Networking Technology (INC, ICCIS and ICMIC)","","2012","","","306","310","To tackle the QoS based routing and wavelength allocation problem (QRWA), a multi-objective genetic algorithm which is based on the ideas of SPEA-II is applied to solve it. The chromosome coding scheme, crossover and mutation operators are redefined, and a repair method is proposed to guarantee the generated offspring are valid. The proposed algorithm is evaluated on a set of different scale test problems and compared with the recently proposed related GA based multi-objective optimization algorithms. The experimental results reveal very encouraging results in terms of the solution quality and diversity.","","978-89-94364-18-6978-1-4673-1326","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6418673","multi-objective optimization;genetic algorithm;QoS;QRWA","Sociology;Statistics;Frequency modulation;Artificial neural networks","computational complexity;genetic algorithms;graph theory;quality of service;telecommunication network routing","multiobjective genetic algorithm;QoS based routing;quality of service;wavelength allocation problem;QRWA problem;chromosome coding scheme;crossover operator;mutation operator;repair method","","","16","","","","","","IEEE","IEEE Conferences"
"The lung diseases diagnosis software: Influenza and Tuberculosis case studies in the cloud computing environment","R. Lestari; M. Ahmad; B. Alisjahbana; T. Djatmiko","School of Electrical Engineering and Informatics, Bandung Institute of Technology, Indonesia; School of Electrical Engineering and Informatics, Bandung Institute of Technology, Indonesia; Hasan Sadikin Hospital, Bandung, Indonesia; Telkom Institut of Management, Bandung, Indonesia","2012 International Conference on Cloud Computing and Social Networking (ICCCSN)","","2012","","","1","7","This research paper describes study of the lung diseases diagnosis software with Influenza and Tuberculosis as the cases study. Influenza is a highly contagious infection of the respiratory tract. In Indonesia, numerous number of Avian-Influenza (H5N1) cases occurred in rural area, such as cases in Garut, Jawa Barat in 2006 which started in Cikelet district causing 14 infected-cases and 5 death-cases and has put Garut in an emergency situation until 2010. The H5N1 detection in rural area requires fast yet accurate diagnosis result to enable immediate curative actions. The most accurate result is provided by RT-PCR (Reverse-Transcription Polymerase-Chain-Reaction), which is high investment, and usually locates in urban area. Another diagnostic tool, rapid diagnostic test, is most common tool but gives inaccurate result because of its low sensitivity and specificity. In case of Tuberculosis, Indonesia ranks number 3, after India and China, as a country with most number of infected-people. Symptoms are chronic-cough with blood-tinged-sputum, fever, night-sweats and weight-loss. Tuberculosis is diagnosed by identifying Mycobacterium-tuberculosis organism in clinical sample. When this is not possible, probable diagnosis may be made using imaging X-rays or scans, tuberculin skin test and/or Interferon-Gamma-Release-Assay (IGRA). The main problem with tuberculosis diagnosis is difficulty in culturing this slow-growing organism in laboratory, which may take 4 to 12 weeks for blood or sputum-culture. The requirement is to create a first-hand diagnostic tool which allows paramedic mobility to rural areas for Influenza/Tuberculosis diagnostic purposes. The study first introduces lung auscultation method, then analysis and preliminary design of lung diseases diagnosis software. To enable each Lung Health for Public Council (Balai Besar Kesehatan Paru Masyarakat) and Health Service Office (Dinas Kesehatan) within province/region/country as data owner to maintain its own data, and to provide the data confidentiality as well, the cloud computing environment is then applied.","","978-1-4673-1816-7978-1-4673-1815-0978-1-4673-1814","10.1109/ICCCSN.2012.6215758","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6215758","Breath sound;Breath frequency;Cloud computing;Influenza;Lung Auscultation;Lung sound waveform;Tuberculosis (TBC)","Lungs;Influenza;Diseases;Software;Databases;Servers;Cellular phones","cloud computing;diseases;medical computing;patient diagnosis","lung diseases diagnosis software;Influenza;tuberculosis;cloud computing environment;Indonesia;avian-influenza cases;emergency situation;H5N1 detection;RT-PCR;reverse-transcription polymerase-chain-reaction;chronic-cough;blood-tinged-sputum;mycobacterium-tuberculosis organism;interferon-gamma-release-assay;IGRA;first-hand diagnostic tool;paramedic mobility;lung auscultation method;health service office;lung health for public council","","6","17","","","","","","IEEE","IEEE Conferences"
"Bezier curve-based generic shape encoder","F. A. Sohel; G. C. Karmakar; L. S. Dooley; M. Bennamoun","School of Computer Science and Software Engineering, The University of Western Australia; Gippsland School of Information Technology, Monash University; Department of Communication and Systems, The Open University; School of Computer Science and Software Engineering, The University of Western Australia","IET Image Processing","","2010","4","2","92","102","Existing Bezier curve-based shape description techniques primarily focus upon determining a set of pertinent control points (CP) to represent a particular shape contour. While many different approaches have been proposed, none adequately consider domain-specific information about the shape contour like its gradualness and sharpness, in the CP generation process which can potentially result in large distortions in the object's shape representation. This study introduces a novel Bezier curve-based generic shape encoder (BCGSE) that partitions an object contour into contiguous segments based upon its cornerity, before generating the CP for each segment using relevant shape curvature information. In addition, although CP encoding has generally been ignored, BCGSE embeds an efficient vertex-based encoding strategy exploiting the latent equidistance between consecutive CP. A non-linear optimisation technique is also presented to enable the encoder is automatically adapt to bit-rate constraints. The performance of the BCGSE framework has been rigorously tested on a variety of diverse arbitrary shapes from both a distortion and requisite bit-rate perspective, with qualitative and quantitative results corroborating its superiority over existing shape descriptors.","1751-9659;1751-9667","","10.1049/iet-ipr.2008.0128","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5440741","","","curve fitting;encoding;object recognition;shape recognition","Bezier curve-based generic shape encoder;object contour;shape curvature information;vertex-based encoding strategy;non-linear optimisation technique;bit-rate constraints;diverse arbitrary shapes;shape descriptors;object shape representation;control point encoding","","2","","","","","","","IET","IET Journals & Magazines"
"Optimizing Constrained Search Patterns for Remote Mine-Hunting Vehicles","M. Couillard; J. Fawcett; M. Davison","Centre for Operational Research and Analysis (CORA), Defence Research and Development Canada (DRDC), Halifax, Canada; Defence Research and Development Canada—Atlantic (DRDC Atlantic), Dartmouth, Canada; Richard Ivey School of Business, University of Western Ontario, London, Canada","IEEE Journal of Oceanic Engineering","","2012","37","1","75","84","When conducting remote mine-hunting operations with a sidescan-sonar-equipped vehicle, a lawn-mowing search pattern is standard if no prior information on potential target locations is available. Upon completion of this initial search, a list of contacts is obtained. The overall classification performance can be significantly improved by revisiting these contacts to collect additional looks. This paper provides, for the first time, a link between the recent literature which finds optimal secondary looks and optimal route planning software. Automated planning algorithms are needed to generate multiaspect routes to improve the performance of mine-hunting systems and increase the capability of navies to efficiently clear potential mine fields. This paper introduces two new numerical techniques designed to enable current remote mine-hunting systems to achieve secondary paths minimizing the total distance to be traveled and satisfying all motion and imaging constraints. The first ""local"" approach is based on a sequential algorithm dealing with more tractable subproblems, while the second is ""global"" and based on simulated annealing. These numerical techniques are applied to two test sites created for the Mongoose sea trial held at the 2007 Autonomous Underwater Vehicle (AUV) Fest, Panama City, FL. Highly satisfactory planning solutions are obtained.","0364-9059;1558-1691;2373-7786","","10.1109/JOE.2011.2173833","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6104186","Mine countermeasures;path optimization;route planning;traveling salesman","Sonar;Vehicles;Imaging;Shape;Traveling salesman problems;Rocks;Image segmentation","autonomous underwater vehicles;numerical analysis;path planning;simulated annealing;sonar detection;sonar imaging;weapons","constrained search pattern optimization;remote mine-hunting vehicles;sidescan-sonar-equipped vehicle;lawn-mowing search pattern;classification performance;optimal route planning software;automated planning algorithm;potential mine fields;numerical techniques;imaging constraints;motion constraints;sequential algorithm;simulated annealing;autonomous underwater vehicle;AUV","","11","17","","","","","","IEEE","IEEE Journals & Magazines"
"CGLive - A Real Time Power Monitoring Solution for Enterprises","S. V. Rao; S. Ramesh; V. A. Muthuraj; K. Sundararaman; J. Parthasarathi","NA; NA; NA; NA; NA","2011 IEEE/ACM International Conference on Green Computing and Communications","","2011","","","212","215","CGLive is a real time energy monitoring software tool that could be used to monitor and reduce energy consumption of PC's in a enterprise network environment. Working of CGLive has been tested in a lab environment and was found to be successful. Dashboard of CGLive can show current status of a work station, energy consumed by it and also the Process which consume higher percentage of CPU. CGLive can issue commands towards ensuring shutdown, logon, logoff and hibernate a particular machine.","","978-1-4577-1006-3978-0-7695-4466","10.1109/GreenCom.2011.43","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6061329","Green IT;Intelligent Green IT Management;Energy audit;M2M technology;Sensor Networks;Performance Optimization;Remote monitoring","Workstations;Green products;Monitoring;Power demand;Temperature sensors;Databases;Software","business data processing;computerised monitoring;power aware computing","CGLive;realtime power monitoring solution;energy monitoring software tool;energy consumption;PC;enterprise network environment","","","5","","","","","","IEEE","IEEE Conferences"
"Modelling the Power and Energy Consumption of NIOS II Softcores on FPGA","L. Senn; E. Senn; C. Samoyeau","NA; NA; NA","2012 IEEE International Conference on Cluster Computing Workshops","","2012","","","179","183","This paper discusses the consumption of NIOS I embedded processors on different FPGA (Field-Programmable Gate Array) platforms. Two FPGA embedded platform have been studied (based on the Cyclone III LS and Aria II GX from ALTERA). Power and energy consumption have been measured using an automated test bench. From these measurements, parameters with the greatest influence on the power and energy consumption are identified, and power and energy models are developed. Those models will be included in high level tools for early design space exploration. They exhibit a relatively low error(3% in average). They were validated against measurements on a video analysis application. The works which are presented here have been undertaken in the frame of the Open-PEOPLE research project, which goal is to provide a platform dedicated to power and energy measurement, estimation and optimization for complete embedded systems.","","978-0-7695-4844-9978-1-4673-2893","10.1109/ClusterW.2012.41","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6355862","","Field programmable gate arrays;Computer architecture;Power demand;Mathematical model;Software;Estimation;Temperature measurement","automatic test equipment;energy measurement;field programmable gate arrays;integrated circuit design;operating systems (computers);power measurement;video signal processing","power consumption modelling;energy consumption modelling;NIOS II softcores;FPGA;field-programmable gate array;NIOS I embedded processors;Cyclone III LS;Aria II GX;ALTERA;automated test bench;power consumption measurement;energy consumption measurement;power models;energy models;video analysis application;Open-PEOPLE research project;power estimation;power optimization;energy estimation;energy optimization;complete embedded systems;early design space exploration","","4","19","","","","","","IEEE","IEEE Conferences"
"Order tracking based on Gabor and Viterbi algorithm","Z. Xiaoping; Z. Xiuli; W. Yan; Z. Yuhui","Jiangsu Engineering Center of Network Monitoring, Nanjing University of Information Science &amp; Technology, China Nanjing, 210044; Hebei Software Institute, China Baoding, 071000; School of Electrical Engineering and Information, Anhui University of Technology Maanshan, China; Jiangsu Engineering Center of Network Monitoring, Nanjing University of Information Science &amp; Technology, China Nanjing, 210044","2011 International Conference on Mechatronic Science, Electric Engineering and Computer (MEC)","","2011","","","1918","1921","The study proposes an improved Gabor order tracking technique based on Viterbi algorithm (V_GOT) to cope with close orders and crossing orders that cannot be effectively separated by using the original Gabor order tracking scheme. The dual function of the Gabor elementary function can effect the precision of tracked orders. In the paper, its influence on the computed Gabor expansion coefficients is investigated. For applying V_GOT in practical works, the separation and extraction of close orders components of vibration signals measured from an aircraft engine test is illustrated.","","978-1-61284-722-1978-1-61284-719-1978-1-61284-721","10.1109/MEC.2011.6025862","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6025862","Rotating machinery;orde tracking;Instantaneous frequency estimation","Vibrations;Frequency estimation;Time frequency analysis;Viterbi algorithm;Educational institutions;Optimized production technology","aerospace engines;condition monitoring;maximum likelihood estimation;mechanical engineering computing;source separation;vibrations","Gabor Algorithm;Viterbi Algorithm;improved Gabor order tracking technique;V_GOT;crossing orders;Gabor elementary function;Gabor expansion coefficients;close-order component separation;close-order component extraction;vibration signals;aircraft engine test","","","7","","","","","","IEEE","IEEE Conferences"
"Risks of unrecognized commonalities in information technology supply chains","C. W. Axelrod","Delta Risk LLC, Great Neck, New York","2010 IEEE International Conference on Technologies for Homeland Security (HST)","","2010","","","495","499","In this paper we examine the interdependencies and common points of failure (and attack) that plague commonly-used system and network hardware and software. The proposed approach requires not only generating inventories of acquiring organizations' equipment and software products, and clear and detailed descriptions of every link in the supply chain, but also the identification of common components and their sources. This information is required not only for manufacturer and OEM supply chains, but also for the services supply chains of maintenance and repair organizations. When such critical components and services have been identified, one must prioritize their importance and apply appropriate security and testing. Such an identification and tracking system is only as good as its ability to incorporate up-to-the-minute changes and additions. This requires extensive real-time reporting and information sharing. The author presents a general description of a proprietary tool that facilitates the collaboration needed for such an approach to be effective.","","978-1-4244-6048-9978-1-4244-6047-2978-1-4244-6046","10.1109/THS.2010.5654970","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5654970","supply chain;dependencies;complexity;risk mitigation;common points of failure;IT outsourcing;computer hardware;computer software","Supply chains;Software;Complexity theory;Security;Organizations;Hardware;Testing","information technology;production engineering computing;risk management;supply chain management","unrecognized commonalities;information technology supply chains;plague commonly-used system;network hardware;network software;OEM supply chains;repair organizations;tracking system;identification system;information sharing","","1","7","","","","","","IEEE","IEEE Conferences"
"POSS<sup>®</sup>: Railway Condition Monitoring developed by a maintainer","F. R. Redeker","Strukton Rail, Netherlands","5th IET Conference on Railway Condition Monitoring and Non-Destructive Testing (RCM 2011)","","2011","","","1","5","Railway Condition Monitoring (RCM) can be a great tool to improve the availability of your railways and lower the maintenance costs. Key factor for success of RCM is the implementation of the system in the organisation and the appreciation of the system by the users. This is where the POSS<sup>®</sup>philosophy distinguishes itself by its special approach. The hardware design focuses on simplicity and reliability. The software focuses on user friendliness for all layers in the maintenance organisation. For the implementation of RCM Strukton Rail has a large number of experienced maintenance engineers, able to advise the management, to train the users and to avoid the usual pitfalls. Important also is the selection of assets to be monitored. RCM can only pay itself back when it contributes significantly to a higher availability or an optimised maintenance process. The development of POSS<sup>®</sup>started in 1999 with Point Condition Monitoring which is still the most import RCM item. All experience acquired over the last 12 years is used in the design of the newest MicroPOSS data loggers that can handle any type of point drive on the market. The new software release 6.0 offers a further improved alarm generation. Other POSS<sup>®</sup>applications include monitoring of: track circuits, wheel shunt, level crossings, expansion joints, rail temperature and rolling stock. A new concept is the monitoring of axle loads and wheel defect detection. The required maintenance activities on a railway line strongly depend on the axle load and the wheel defects of the passing trains. The new concept gives more transparency to the infra manager and can help to optimize the maintenance of the rolling stock. The new trend on railways to use more axle counters has led to the decision of the development of POSS8","","978-1-84919-558","10.1049/cp.2011.0590","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6191858","","","condition monitoring;data loggers;human computer interaction;maintenance engineering;railway engineering;rolling","railway condition monitoring;maintenance costs;RCM success;organisation system;hardware design;user friendliness;maintenance organisation;RCM Strukton rail;experienced maintenance engineers;users training;point condition monitoring;MicroPOSS data loggers;software release 6.0;alarm generation;track circuits;wheel shunt;level crossings;expansion joints;rail temperature;rolling stock;axle loads monitoring;wheel defect detection;infra manager transparency;POSS broken rail detection development;track circuits;POSS management tool","","","","","","","","","IET","IET Conferences"
"Hardware acceleration of Scatter Search","M. Walton; G. Grewal; G. Darlington","School of Comp. Science, University of Guelph, Ontario, Canada; School of Comp. Science, University of Guelph, Ontario, Canada; Mathematics and Statistics, University of Guelph, Ontario, Canada","2010 International Conference on High Performance Computing & Simulation","","2010","","","436","443","In this paper, we share our experience implementing the well-known meta-heuristic, Scatter Search, on a Field-Programmable Gate-Array (FPGA). Our objective is to improve the runtime of scatter search by exploiting the potential performance benefits that are available through the native parallelism in hardware. When implementing scatter search we employ Handel-C - a programming language specifically designed to enable software developers to easily synthesize C-like programs into synchronous hardware. A total of 31 different Handel-C implementations, all based on different language level optimizations, are considered. A full-factorial experiment is performed to test the interactions between different combinations of language-level optimizations with respect to the performance of scatter search compared with software.","","978-1-4244-6830-0978-1-4244-6827-0978-1-4244-6829","10.1109/HPCS.2010.5547101","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5547101","Hardware Acceleration;FPGA;Scatter Search;0–1Knapsack Problem;Parallelism","Optimization;Clocks;Hardware;Field programmable gate arrays;Search problems;Runtime;Random access memory","C language;field programmable gate arrays;optimisation","hardware acceleration;scatter search;field-programmable gate-array;FPGA;Handel-C;programming language;software developers;C-like programs;synchronous hardware;language level optimizations;full-factorial experiment","","","23","","","","","","IEEE","IEEE Conferences"
"Optimized Handover Scheme Using IEEE 802.21 MIH Service in Multi-Service Environment","J. Yuan; Y. Wang; F. Liu; L. Zheng","NA; NA; NA; NA","2010 IEEE 71st Vehicular Technology Conference","","2010","","","1","5","The scenario that different RATs are used to satisfy mobile user's two or more application requirements simultaneously in heterogeneous network is called multi-service environment. Recently, powerful multi-mode wireless terminal, which is suitable for multi-service situation, has become the test product. The mobility management (MM) issues, especially handover, of different interfaces in the test product are resolved independently with a network layer solution FMIPv6. The packet loss of FMIPv6 could become an MM bottleneck. A potential approach of improving packet loss performance jointly between interfaces in multi-service scenario exists in our research. Therefore, this paper proposes a novel scheme using IEEE 802.21 MIH services to improve packet loss performance by utilizing the active links to maintain the data flow. The MIH services, Link_Action and MIH_Link_Action, are extended and an MIH event named Link_PDU_Receive_Status is added. A complete message exchange in handover procedure is provided. Numerical analysis shows that the proposed scheme performs better in terms of packet loss comparing with the traditional independent FMIPv6 scheme.","1550-2252;1550-2252","978-1-4244-2518-1978-1-4244-2519","10.1109/VETECS.2010.5494040","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5494040","","Rats;Wireless LAN;Mobile radio mobility management;Testing;Performance loss;Application software;Satellite broadcasting;Ground penetrating radar;Costs;Land mobile radio cellular systems","IP networks;mobility management (mobile radio);radio access networks","optimized handover scheme;IEEE 802.21 MIH service;multiservice environment;multimode wireless terminal;mobility management;network layer solution;FMIPv6;packet loss performance;Link_Action;MIH_Link_Action;Link_PDU_Receive_Status;radio access technologies","","","11","","","","","","IEEE","IEEE Conferences"
"Differential Evolution enhanced by neighborhood search","H. Wang; Z. Wu; S. Rahnamayan","State Key Laboratory of Software Engineering, Wuhan University, Wuhan, 430072 China; State Key Laboratory of Software Engineering, Wuhan University, Wuhan, 430072 China; Faculty of Engineering and Applied Science, University of Ontario Institute of Technology (UOIT), 2000 Simcoe Street North, Oshawa, ON L1H 7K4, Canada","IEEE Congress on Evolutionary Computation","","2010","","","1","8","This paper presents a novel Differential Evolution (DE) algorithm, called DE enhanced by neighborhood search (DENS), which differs from pervious works of utilizing the neighborhood search in DE, such as DE with neighborhood search (NSDE) and self-adaptive DE with neighborhood search (SaNSDE). In DENS, we focus on searching the neighbors of individuals, while the latter two algorithms (NSDE and SaNSDE) work on the adaption of the control parameters F and CR. The proposed algorithm consists of two following main steps. First, for each individual, we create two trial individuals by local and global neighborhood search strategies. Second, we select the fittest one among the current individual and the two created trial individuals as a new current individual. Experimental studies on a comprehensive set of benchmark functions show that DENS achieves better results for a majority of test cases, when comparing with some other similar evolutionary algorithms.","1089-778X;1941-0026","978-1-4244-6911-6978-1-4244-6909-3978-1-4244-6910","10.1109/CEC.2010.5586418","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5586418","Differential evolution;neighborhood search;local search;global optimization","Benchmark testing;Chromium;Search problems;Topology;Evolutionary computation;Nearest neighbor searches;Book reviews","evolutionary computation;search problems","differential evolution;neighborhood search;control parameters;evolutionary algorithms","","5","23","","","","","","IEEE","IEEE Conferences"
"Skel: Generative Software for Producing Skeletal I/O Applications","J. Logan; S. Klasky; J. Lofstead; H. Abbasi; S. Ethier; R. Grout; S. Ku; Q. Liu; X. Ma; M. Parashar; N. Podhorszki; K. Schwan; M. Wolf","NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA","2011 IEEE Seventh International Conference on e-Science Workshops","","2011","","","191","198","Massively parallel computations consist of a mixture of computation, communication, and I/O. Of these three components, implementing an effective parallel I/O solution has often been overlooked by application scientists and has typically been added to large scale simulations only when existing serial techniques have failed. As scientists' teams scaled their codes to run on hundreds of processors, it was common to call on an I/O expert to implement a set of more scalable I/O routines. These routines were easily separated from the calculations and communication, and in many cases, an I/O kernel was derived from the application which could be used for testing I/O performance independent of the application. These I/O kernels developed a life of their own used as a broad measure for comparing different I/O techniques. Unfortunately, as years passed and computation and communication changes required changes to the I/O, the separate I/O kernel used for benchmarking remained static, no longer providing an accurate indicator of the I/O performance of the simulation, and making I/O research less relevant for the application scientists. In this paper we describe a new approach to this problem where I/O kernels are replaced with skeletal I/O applications that are automatically generated from an abstract set of simulation I/O parameters. We realize this abstraction by leveraging the ADIOS [1] middleware's XML I/O specification with additional runtime parameters. Skeletal applications offer all of the benefits of I/O kernels including allowing I/O optimizations to focus on useful I/O patterns. Moreover, since they are automatically generated, it is easy to produce an updated I/O skeleton whenever the simulation's I/O changes. In this paper we analyze the performance of automatically generated I/O skeletal applications for the S3D and GTS codes. We show that these skeletal applications achieve performance comparable to that of the production applications. We wrap up the paper with a discussion of future changes to make the skeletal application better approximate the actual I/O performed in the simulation.","","978-1-4673-0026-1978-0-7695-4598","10.1109/eScienceW.2011.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6130712","","XML;Kernel;Arrays;Benchmark testing;Educational institutions;Reactive power;Hardware","input-output programs;parallel processing","Skel;generative software;producing skeletal I/O applications;parallel computations;parallel I/O solution;I/O routines;skeletal I/O applications;XML I/O specification;I/O optimizations;I/O patterns","","4","21","","","","","","IEEE","IEEE Conferences"
"Optimizing system monitoring configurations for non-actionable alerts","L. Tang; T. Li; F. Pinel; L. Shwartz; G. Grabarnik","School of Computer Science, Florida International University, Miami, FL, USA; School of Computer Science, Florida International University, Miami, FL, USA; IBM T.J. Watson Research Center, Hawthorne, NY, USA; IBM T.J. Watson Research Center, Hawthorne, NY, USA; Dept. Math & Computer Science, St. John's University, Queens, NY, USA","2012 IEEE Network Operations and Management Symposium","","2012","","","34","42","Today's competitive business climate and the complexity of IT environments dictate efficient and cost effective service delivery and support of IT services. This is largely achieved through automating of routine maintenance procedures including problem detection, determination and resolution. System monitoring provides effective and reliable means for problem detection. Coupled with automated ticket creation, it ensures that a degradation of the vital signs, defined by acceptable thresholds or monitoring conditions, is flagged as a problem candidate and sent to supporting personnel as an incident ticket. This paper describes a novel methodology and a system for minimizing non-actionable tickets while preserving all tickets which require corrective action. Our proposed method defines monitoring conditions and the optimal corresponding delay times based on an off-line analysis of historical alerts and the matching incident tickets. Potential monitoring conditions are built on a set of predictive rules which are automatically generated by a rule-based learning algorithm with coverage, confidence and rule complexity criteria. These conditions and delay times are propagated as configurations into run-time monitoring systems.","2374-9709;1542-1201;1542-1201","978-1-4673-0269-2978-1-4673-0267-8978-1-4673-0268","10.1109/NOMS.2012.6211880","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6211880","","Monitoring;Prediction algorithms;Servers;Transient analysis;Delay;Accuracy;Testing","computational complexity;knowledge based systems;learning (artificial intelligence);software maintenance;system monitoring","system monitoring configuration optimization;nonactionable alerts;IT environments;cost effective service delivery;IT services;automating routine maintenance procedures;problem detection;problem determination;problem resolution;automated ticket creation;monitoring conditions;incident ticket;nonactionable ticket minimization;corrective action;delay times;offline analysis;historical alerts;incident ticket matching;rule-based learning algorithm;rule complexity criteria;coverage criteria;confidence criteria;run-time monitoring systems","","14","29","","","","","","IEEE","IEEE Conferences"
"Augmented Reality for Construction Control","K. Kirchbach; C. Runde","NA; NA","2012 16th International Conference on Information Visualisation","","2012","","","440","444","The development of a physical building control center has got the purpose of supporting the management and control of a project to optimize the flow of the complex processes at a construction site. The concept consists of equipping vehicles with sensors, using virtual reality and augmented reality-techniques to visualize these realtime information and allow an optical adaption to current circumstances. A more efficient utilization and enhanced cost effectiveness will be the result. Based on a previous performed requirement analysis a software architecture (AR4CC, ""Augmented Reality for Construction Control"") has been developed, which allows the use of virtual reality (vr) and augmented reality (ar) at a construction site. The basic idea of information transparency and the generated software is introduced. Two test-phases were performed and will be presented, one of them in a computer on a virtual construction site and the other one on a construction site mock-up, which was built especially for this purpose.","2375-0138;1550-6037;1550-6037","978-1-4673-2260-7978-0-7695-4771","10.1109/IV.2012.76","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6295851","virtual reality;augmented reality;control center;information transparency;process optimization;construction site;information visualization","Buildings;Vehicles;Augmented reality;Cameras;Argon;Planning","augmented reality;civil engineering computing;construction industry;data visualisation;formal specification;project management;real-time systems;software architecture","physical building control center;project control;project management;complex process flow optimization;virtual reality;real-time information visualization;sensors-equipped vehicles;requirement analysis;software architecture;AR4CC;augmented reality for construction control;information transparency;virtual construction site;construction site mock-up","","1","15","","","","","","IEEE","IEEE Conferences"
"Optimizing the Calculation of Conditional Probability Tables in Hybrid Bayesian Networks Using Binary Factorization","M. Neil; X. Chen; N. Fenton","Queen Mary, University of London, London and Agena Ltd.; Queen Mary, University of London, London; Queen Mary, University of London, London and Agena Ltd.","IEEE Transactions on Knowledge and Data Engineering","","2012","24","7","1306","1312","Reducing the computational complexity of inference in Bayesian Networks (BNs) is a key challenge. Current algorithms for inference convert a BN to a junction tree structure made up of clusters of the BN nodes and the resulting complexity is time exponential in the size of a cluster. The need to reduce the complexity is especially acute where the BN contains continuous nodes. We propose a new method for optimizing the calculation of Conditional Probability Tables (CPTs) involving continuous nodes, approximated in Hybrid Bayesian Networks (HBNs), using an approximation algorithm called dynamic discretization. We present an optimized solution to this problem involving binary factorization of the arithmetical expressions declared to generate the CPTs for continuous nodes for deterministic functions and statistical distributions. The proposed algorithm is implemented and tested in a commercial Hybrid Bayesian Network software package and the results of the empirical evaluation show significant performance improvement over unfactorized models.","1041-4347;1558-2191;2326-3865","","10.1109/TKDE.2011.87","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5740894","Bayesian networks;binary factorization;dynamic discretization.","Clustering algorithms;Algorithm design and analysis;Heuristic algorithms;Inference algorithms;Approximation algorithms;Bayesian methods;Junctions","approximation theory;belief networks;computational complexity;pattern clustering;software packages;statistical distributions;tree data structures","conditional probability tables;binary factorization;computational complexity;junction tree structure;BN nodes;time exponential complexity;cluster size;CPT;approximation algorithm;dynamic discretization;arithmetical expressions;deterministic functions;statistical distributions;hybrid Bayesian network software package;unfactorized models","","11","20","","","","","","IEEE","IEEE Journals & Magazines"
"L1 - An Intermediate Language for Mixed-Protocol Secure Computation","A. Schropfer; F. Kerschbaum; G. Muller","NA; NA; NA","2011 IEEE 35th Annual Computer Software and Applications Conference","","2011","","","298","307","Secure Computation (SC) enables secure distributed computation of arbitrary functions of private inputs. It has many useful applications, e.g. benchmarking or auctions. Several general protocols for SC have been proposed and recently been implemented in a number of compilers and frameworks. These compilers or frameworks implement one general SC protocol and then require the programmer to implement the function he wants the protocol to compute. Performance remains a challenge for this approach and it has been realized early on that special protocols for important problems can deliver superior performance. In this paper we propose a new intermediate language (L1) for optimizing SC compilers which enables efficient implementation of special protocols potentially mixing several general SC protocols. We show by three case studies - one for computation of the median, one for weighted average, one for division - that special protocols and mixed-protocol implementations in our language L1 can lead to superior performance. Moreover, we show that only a combined view on algorithm and cryptographic protocol can discover SCs with best run-time performance.","0730-3157;0730-3157;0730-3157","978-0-7695-4439-7978-1-4577-0544-1978-0-7695-4439","10.1109/COMPSAC.2011.46","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032356","Multi-party Computation;Compiler","Protocols;Java;Benchmark testing;Instruction sets;Encryption","distributed processing;formal languages;optimisation;program compilers;protocols;security of data","intermediate language;mixed-protocol secure computation;secure computation;distributed computation;SC protocol;SC compiler optimization;cryptographic protocol;run-time performance","","9","34","","","","","","IEEE","IEEE Conferences"
"A self-adaptive differential evolution algorithm for dynamic economic dispatch with valve-point effects","T. Sum-Im","Department of Electrical Engineering, Faculty of Engineering, Srinakharinwirot University","2012 47th International Universities Power Engineering Conference (UPEC)","","2012","","","1","6","In this paper, a self-adaptive differential evolution algorithm (SaDEA) is proposed for solving dynamic economic dispatch (DED) problem with valve-point effects consideration. The purpose of DED problem is to minimize the total generation costs of thermal power plants associated with the technical and economical constraints. The software development has been performed within the mathematical programming environment of MATLAB in this work. The efficiency and effectiveness of the proposed technique is initially demonstrated via the analysis of 3-unit and 10-unit test systems considering valve-point loading and ramp rate constraints. A detailed comparative study among an evolutionary programming (EP), a particle swarm optimization (PSO), an enhanced particle swarm optimization (EPSO), an enhanced particle swarm optimization with Gaussian mutation (EPSO-GM), a hybrid method between evolutionary programming and sequential quadratic programming (EP-SQP), a modified hybrid EP-SQP (MHEP-SQP) and the proposed method is presented. From the experimental results, the proposed method has the achieved solutions with good accuracy, stable convergence characteristics, simple implementation and satisfactory computational time.","","978-1-4673-2856-2978-1-4673-2854-8978-1-4673-2855","10.1109/UPEC.2012.6398551","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6398551","Power generation;dynamic economic dispatch;self-adaptive differential evolution algorithm","Vectors;Generators;Sociology;Statistics;Heuristic algorithms;Cost function","evolutionary computation;particle swarm optimisation;power generation dispatch;power generation economics;quadratic programming;thermal power stations","self-adaptive differential evolution algorithm;dynamic economic dispatch;valve-point effects;SaDEA;DED problem;total generation costs;thermal power plants;Matlab;mathematical programming environment;unit test systems;ramp rate constraints;evolutionary programming;particle swarm optimization;PSO;enhanced particle swarm optimization;EPSO;enhanced particle swarm optimization with Gaussian mutation;EPSO-GM;sequential quadratic programming;modified hybrid EP-SQP method","","1","26","","","","","","IEEE","IEEE Conferences"
"Reliability Implications of Register Utilization: An Empirical Study","P. Romer; P. Troger","NA; NA","2011 IEEE Ninth International Conference on Dependable, Autonomic and Secure Computing","","2011","","","105","112","Physical memory faults are one of the accepted reliability threats for modern processor technology. Even though error correction for memory hardware can deal with this class of fault, many processor designs such as X86 do not invest semiconductor complexity and space to deal with multi-bit faults in caches and registers. In this paper, we analyze how the right choice of compiler options can act as software-based error prevention strategy for register file faults. This investigation is based on a classification scheme for GCC compiler options and a new fault injection environment for automated experiments. The study proves the the initial hypothesis that reliability improvements mainly depend on the algorithmic structure of the application under test. A small number of compiler options have proven to be a feasible application-independent error mitigation strategy.","","978-1-4673-0006-3978-0-7695-4612","10.1109/DASC.2011.41","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6118360","reliability;compiler;register;soft error;X86","Registers;Circuit faults;Reliability;Optimization;Hardware;Computer architecture;Error correction codes","cache storage;optimising compilers;software reliability","reliability implication;register utilization;physical memory fault;processor technology;error correction;memory hardware;processor design;X86;semiconductor complexity;multibit fault;caches;software-based error prevention;register file fault;classification scheme;GCC compiler option;fault injection","","1","20","","","","","","IEEE","IEEE Conferences"
"Design method of X band coaxial duplexer","W. Qiang; H. Ying; W. Bin; T. Jinsong","Sichuan Institute of Piezoelectric and Acousto-optic Technology, Chongqing, 400060, China; Sichuan Institute of Piezoelectric and Acousto-optic Technology, Chongqing, 400060, China; Sichuan Institute of Piezoelectric and Acousto-optic Technology, Chongqing, 400060, China; Sichuan Institute of Piezoelectric and Acousto-optic Technology, Chongqing, 400060, China","2012 International Conference on Microwave and Millimeter Wave Technology (ICMMT)","","2012","4","","1","4","Based on the design of the general Chebyshev bandpass filter with finite transmission zeros, the filter simulation model has been established according to the calculated couple matrix. Combining EM software with an optimization method, the size of the duplexer has been optimized and designed by using the group delay fitting method. The test results show that the insertion loss of the proposed duplexer is less than 1.5 dB, the VSWR is less than 1.5, the band-stop-rejection is greater than 85 dB. The measured curves agree well with the design specifications and the correctness of this method has been verified.","","978-1-4673-2185-3978-1-4673-2184","10.1109/ICMMT.2012.6230257","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6230257","general Chebyshev filter;duplexer;couple matrix;co-simulation","Microwave filters;Optimization;Couplings;Cavity resonators;Integrated circuit modeling;Delay;Microwave circuits","band-pass filters;Chebyshev filters;microwave filters;multiplexing equipment;poles and zeros","coaxial duplexer;Chebyshev bandpass filter;transmission zeros;filter simulation model;EM software;optimization method;group delay fitting method;insertion loss;band-stop-rejection","","","5","","","","","","IEEE","IEEE Conferences"
"MATPOWER: Steady-State Operations, Planning, and Analysis Tools for Power Systems Research and Education","R. D. Zimmerman; C. E. Murillo-Sanchez; R. J. Thomas","Department of Applied Economics and Management and the School of Electrical and Computer Engineering, Cornell University, Ithaca, NY, USA; Universidad Autónoma de Manizales, Universidad Nacional de Colombia, Manizales, Manizales, ColombiaColombia; Department of Applied Economics and Management and the School of Electrical and Computer Engineering, Cornell University, Ithaca, NY, USA","IEEE Transactions on Power Systems","","2011","26","1","12","19","MATPOWER is an open-source Matlab-based power system simulation package that provides a high-level set of power flow, optimal power flow (OPF), and other tools targeted toward researchers, educators, and students. The OPF architecture is designed to be extensible, making it easy to add user-defined variables, costs, and constraints to the standard OPF problem. This paper presents the details of the network modeling and problem formulations used by MATPOWER, including its extensible OPF architecture. This structure is used internally to implement several extensions to the standard OPF problem, including piece-wise linear cost functions, dispatchable loads, generator capability curves, and branch angle difference limits. Simulation results are presented for a number of test cases comparing the performance of several available OPF solvers and demonstrating MATPOWER's ability to solve large-scale AC and DC OPF problems.","0885-8950;1558-0679","","10.1109/TPWRS.2010.2051168","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5491276","Load flow analysis;optimal power flow;optimization methods;power engineering;power engineering education;power system economics;power system simulation;power systems;simulation software;software tools","Power system planning;Steady-state;Power system analysis computing;Power system simulation;Load flow;Open source software;Computer languages;Packaging;Costs;Mathematical model","load flow;mathematics computing;piecewise linear techniques;power generation dispatch;power generation planning;power system simulation","MATPOWER;steady-state operation;power system planning;power system research;power system education;Matlab-based power system simulation package;optimal power flow;OPF architecture;user defined variable;network modeling;piecewise linear cost function;load dispatch;branch angle difference;DC OPF problem","","1842","19","","","","","","IEEE","IEEE Journals & Magazines"
"Logistic Regression by Means of Evolutionary Radial Basis Function Neural Networks","P. A. Gutierrez; C. Hervas-Martinez; F. J. Martinez-Estudillo","Department of Computer Science and Numerical Analysis, University of Córdoba, Córdoba, Spain; Department of Computer Science and Numerical Analysis, University of Córdoba, Córdoba, Spain; Department of Management and Quantitative Methods, Faculty of Economics and Business Sciences, University of Córdoba, Córdoba, Spain","IEEE Transactions on Neural Networks","","2011","22","2","246","263","This paper proposes a hybrid multilogistic methodology, named logistic regression using initial and radial basis function (RBF) covariates. The process for obtaining the coefficients is carried out in three steps. First, an evolutionary programming (EP) algorithm is applied, in order to produce an RBF neural network (RBFNN) with a reduced number of RBF transformations and the simplest structure possible. Then, the initial attribute space (or, as commonly known as in logistic regression literature, the covariate space) is transformed by adding the nonlinear transformations of the input variables given by the RBFs of the best individual in the final generation. Finally, a maximum likelihood optimization method determines the coefficients associated with a multilogistic regression model built in this augmented covariate space. In this final step, two different multilogistic regression algorithms are applied: one considers all initial and RBF covariates (multilogistic initial-RBF regression) and the other one incrementally constructs the model and applies cross validation, resulting in an automatic covariate selection [simplelogistic initial-RBF regression (SLIRBF)]. Both methods include a regularization parameter, which has been also optimized. The methodology proposed is tested using 18 benchmark classification problems from well-known machine learning problems and two real agronomical problems. The results are compared with the corresponding multilogistic regression methods applied to the initial covariate space, to the RBFNNs obtained by the EP algorithm, and to other probabilistic classifiers, including different RBFNN design methods [e.g., relaxed variable kernel density estimation, support vector machines, a sparse classifier (sparse multinomial logistic regression)] and a procedure similar to SLIRBF but using product unit basis functions. The SLIRBF models are found to be competitive when compared with the corresponding multilogistic regression methods and the RBFEP method. A measure of statistical significance is used, which indicates that SLIRBF reaches the state of the art.","1045-9227;1941-0093","","10.1109/TNN.2010.2093537","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5659484","Artificial neural networks;classification;evolutionary algorithms;evolutionary programming;logistic regression;radial basis function neural networks","Logistics;Kernel;Algorithm design and analysis;Maximum likelihood estimation;Training;Artificial neural networks;Support vector machines","covariance analysis;learning (artificial intelligence);maximum likelihood estimation;probability;radial basis function networks;regression analysis","evolutionary radial basis function neural networks;hybrid multilogistic methodology;radial basis function covariates;RBF covariates;evolutionary programming algorithm;EP algorithm;RBF neural network;RBF transformations;attribute space;logistic regression literature;nonlinear transformations;maximum likelihood optimization method;multilogistic regression model;augmented covariate space;multilogistic regression algorithms;initial covariates;cross validation;automatic covariate selection;simplelogistic initial-RBF regression;SLIRBF;regularization parameter;benchmark classification;machine learning problems;agronomical problems;multilogistic regression methods;probabilistic classifiers;RBFNN design methods;relaxed variable kernel density estimation;support vector machines;sparse classifier;sparse multinomial logistic regression;product unit basis functions;statistical significance","Algorithms;Artificial Intelligence;Logistic Models;Mathematical Computing;Neural Networks (Computer);Pattern Recognition, Automated;Pattern Recognition, Automated;Regression Analysis;Software Design;Software Validation","30","57","","","","","","IEEE","IEEE Journals & Magazines"
"A Differential Evolution Algorithm for the University course timetabling problem","K. Shaker; S. Abdullah; A. Hatem","Department of Software Engineering, Faculty of Computer, Science and Information Technology, Universiti Malaya, 50603 Kuala Lumpur, Malaysia; Center for Artificial Intelligence Technology, Universiti Kebangsaan Malaysia, 43600 Bangi, Selangor, Malaysia; Center for Artificial Intelligence Technology, Universiti Kebangsaan Malaysia, 43600 Bangi, Selangor, Malaysia","2012 4th Conference on Data Mining and Optimization (DMO)","","2012","","","99","102","The University course timetabling problem is known as a NP-hard problem. It is a complex problem wherein the problem size can become huge due to limited resources (e.g. amount of rooms, their capacities and number availability of lecturers) and the requirements for these resources. The university course timetabling problem involves assigning a given number of events to a limited number of timeslots and rooms under a given set of constraints; the objective is to satisfy the hard constraints and minimize the violation of soft constraints. In this paper, a Differential Evolution (DE) algorithm is proposed. DE algorithm relies on the mutation operation to reduce the convergence time while reducing the penalty cost of solution. The proposed algorithm is tested over eleven benchmark datasets (representing one large, five medium and five small problems). Experimental results show that our approach is able to generate competitive results when compared with previous available approaches. Possible extensions upon this simple approach are also discussed.","2155-6938;2155-6946","978-1-4673-2718-3978-1-4673-2717-6978-1-4673-2716","10.1109/DMO.2012.6329805","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6329805","differential evolution;course timetabling","Educational institutions;Sociology;Statistics;Optimization;Genetic algorithms;Prediction algorithms;Evolutionary computation","computational complexity;educational institutions;evolutionary computation","differential evolution algorithm;university course timetabling problem;NP-hard problem;hard constraints;soft constraints;DE algorithm","","","22","","","","","","IEEE","IEEE Conferences"
"Analog computation and experimental study of small gasoline engine","Zhang Zhenning; Liu Shengji","School of automobile and traffic engineering, Jiangsu University, Zhen Jiang, China; School of automobile and traffic engineering, Jiangsu University, Zhen Jiang, China","2011 International Conference on Electric Information and Control Engineering","","2011","","","1993","1996","This paper uses AVL-FIRE software to do CFD analog computation for the intake port of 168F engine. The microscopic distribution of velocity field and etc are analysed. Meanwhile, this paper studies the relationship between the intake port structure and velocity field, and find that the bend intersection's structure is the key effect for the flow characteristics of the intake port. Finally, the results indicate that this method can be used to analyse the intake and exhaust process and it can also provide a basic method for optimization design of intake and exhaust systems in the future.","","978-1-4244-8039-5978-1-4244-8036-4978-1-4244-8038","10.1109/ICEICE.2011.5776884","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5776884","Small Gasoline Engine;intake port;flow coefficient;steady flow test rig;velocity field","Valves;Engines;Petroleum;Resistance;Numerical models;Fluids;Solid modeling","computational fluid dynamics;design;exhaust systems;internal combustion engines;mechanical engineering computing","analog computation;small gasoline engine;AVL-FIRE software;CFD analog computation;168F engine;microscopic distribution;bend intersection structure;flow characteristics;exhaust process;optimization design","","","5","","","","","","IEEE","IEEE Conferences"
"Experiences with UPC on TILE-64 processor","O. Serres; A. Anbar; S. Merchant; T. El-Ghazawi","NSF Center for High-Performance Reconfigurable Computing (CHREC), Dept. of Electrical and Computer Engineering, The George Washington University, 801 22nd St NW, 20052, USA; NSF Center for High-Performance Reconfigurable Computing (CHREC), Dept. of Electrical and Computer Engineering, The George Washington University, 801 22nd St NW, 20052, USA; NSF Center for High-Performance Reconfigurable Computing (CHREC), Dept. of Electrical and Computer Engineering, The George Washington University, 801 22nd St NW, 20052, USA; NSF Center for High-Performance Reconfigurable Computing (CHREC), Dept. of Electrical and Computer Engineering, The George Washington University, 801 22nd St NW, 20052, USA","2011 Aerospace Conference","","2011","","","1","9","Partitioned global address space (PGAS) programming model presents programmers with a globally shared address space with locality awareness and one-sided communication constructs. The shared address space and the one-sided communication constructs enhance ease-of-use of PGAS based languages and the locality awareness enables programmers and the runtime systems to achieve higher performance. Thus PGAS programming model may help address the escalating software complexity issues resulting from the proliferation of many-core processor architectures in aerospace and computing systems in general. This paper presents our experiences with Unified parallel C (UPC), a PGAS language, on the Tile64™ processor, a 64-core processor from Tilera Corporation. We ported Berkeley UPC compiler and runtime system on the Tilera architecture and evaluated two separate runtime implementation conduits of the underlying GASNet communication library, a pThreads based conduit and an MPI based conduit. Each conduit uses different on-chip, inter-core communication networks providing different latencies and bandwidths for inter-process communications. The paper presents the implementation details and empirical analyses of both approaches by comparing and evaluating results from NAS Parallel Benchmark suite. The analyses reveal various optimization opportunities based on specific many-core architectural features which are also discussed in the paper.","1095-323X;1095-323X","978-1-4244-7351-9978-1-4244-7350-2978-1-4244-7349","10.1109/AERO.2011.5747452","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5747452","","Electronics packaging;Tiles;Benchmark testing;Programming;Computer architecture;Instruction sets;Runtime","aerospace computing;application program interfaces;benchmark testing;C language;multiprocessing systems;optimisation;parallel languages;parallel programming;program compilers","TILE-64 processor;partitioned global address space;shared address space;PGAS based language;software complexity;many core processor architecture;aerospace system;computing system;unified parallel C;64 core processor;Tilera corporation;Berkeley UPC compiler;GASNet communication library;pThreads based conduit;MPI based conduit;intercore communication network;NAS parallel benchmark suite","","8","19","","","","","","IEEE","IEEE Conferences"
"Localizing failure-inducing program edits based on spectrum information","L. Zhang; M. Kim; S. Khurshid","Electrical and Computer Engineering, The University of Texas at Austin, USA; Electrical and Computer Engineering, The University of Texas at Austin, USA; Electrical and Computer Engineering, The University of Texas at Austin, USA","2011 27th IEEE International Conference on Software Maintenance (ICSM)","","2011","","","23","32","Keeping evolving systems fault free is hard. Change impact analysis is a well-studied methodology for finding faults in evolving systems. For example, in order to help developers identify failure-inducing edits, Chianti extracts program edits as atomic changes between different program versions, selects affected tests, and determines a subset of those changes that might induce test failures. However, identifying real regression faults is challenging for developers since the number of affecting changes related to each test failure may still be too large for manual inspection. This paper presents a novel approach FAULTTRACER which ranks program edits in order to reduce developers' effort in manually inspecting all affecting changes. FAULTTRACER adapts spectrum-based fault localization techniques and applies them in tandem with an enhanced change impact analysis that uses Extended Call Graphs to identify failure-inducing edits more precisely. We evaluate FAULTTRACER using 23 versions of 4 real-world Java programs from the Software Infrastructure Repository. The experimental results show that FAULTTRACER outperforms Chianti in selecting affected tests (slightly better, but handles safety problems of Chianti) as well as in determining affecting changes (with an improvement of approximately 20%). By ranking the affecting changes using spectrum-based test behavior profile, for 14 out of 22 studied failures, FAULTTRACER places a real regression fault within top 3 atomic changes, significantly reducing developers' effort in inspecting potential failure-inducing edits.","1063-6773;1063-6773","978-1-4577-0664-6978-1-4577-0663-9978-1-4577-0662","10.1109/ICSM.2011.6080769","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080769","","Electrocardiography;IEEE Potentials;Image edge detection;Debugging;Delta modulation;Inspection;Manuals","fault tolerant computing;Java","failure-inducing program edits;spectrum information;evolving systems;change impact analysis;failure-inducing edits;regression fault;FAULTTRACER;spectrum-based fault localization;extended call graphs;Java programs;software infrastructure repository;spectrum-based test behavior profile","","20","24","","","","","","IEEE","IEEE Conferences"
"Compression performance of high efficiency video coding (HEVC) working draft 4","B. Li; G. J. Sullivan; J. Xu","University of Science and Technology of China, Hefei, Anhui, China; Microsoft Corporation, Redmond, Washington, USA; Microsoft Corporation, Redmond, Washington, USA","2012 IEEE International Symposium on Circuits and Systems","","2012","","","886","889","This paper presents the results of compression comparison tests between the current state of the emerging High Efficiency Video Coding (HEVC) draft standard and the current dominant standard H.264/MPEG-4 AVC (High Profile) as an anchor reference. The conditions used for the comparison tests were designed to reflect relevant application scenarios and to enable a fair comparison to the maximum extent feasible, i.e. using comparable quantization settings, reference frame buffering, etc. The testing was generally configured in favour of using a relatively strong H.264/MPEG-4 AVC anchor reference. Several of the encoder optimizations currently found in the HEVC software are tested and shown to be helpful to improve the H.264/MPEG-4 AVC anchor performance. When compared to the improved anchor encoder configurations, the HEVC draft design currently provides a bit rate savings for equal PSNR of about 39% for random access applications, 44% for low-delay use, and 25% for all-intra use.","2158-1525;0271-4302;0271-4302","978-1-4673-0219-7978-1-4673-0218-0978-1-4673-0217","10.1109/ISCAS.2012.6272183","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6272183","","Video coding;Encoding;Transform coding;Delay;Software;Standards;PSNR","data compression;encoding;optimisation;quantisation (signal);video coding","high efficiency video coding;HEVC working draft 4;compression performance;current dominant standard;H.264/MPEG-4 AVC;anchor reference;quantization;reference frame buffering;encoder optimizations;HEVC software;anchor encoder configurations;bit rate savings;random access applications","","14","14","","","","","","IEEE","IEEE Conferences"
"Hierarchical neural model for workflow scheduling in Utility Management Systems","S. Vukmirovic; A. Erdeljan; L. Imre; N. Nedic","Faculty of Technical Sciences, University of Novi Sad, Serbia; Faculty of Technical Sciences, University of Novi Sad, Serbia; Faculty of Technical Sciences, University of Novi Sad, Serbia; Faculty of Technical Sciences, University of Novi Sad, Serbia","4th International Workshop on Soft Computing Applications","","2010","","","51","56","The emerging computational grid infrastructure consists of heterogeneous resources in widely distributed autonomous domains, which makes job scheduling very challenging. Although there is much work on static scheduling approaches for workflow applications in parallel environments, little work has been done on a real-world Grid environment for industrial systems. Utility Management Systems (UMS) are executing very large numbers of workflows with very high resource requirements. Unlike the grid approach for standard scientific workflows, UMS workflows have different set of computation requirements and thereby optimization of resource usage has to be made in a different way. This paper proposes architecture for a new scheduling mechanism that dynamically executes a scheduling algorithm using near real-time feedback about current status Grid nodes. Two Artificial Neural Networks (ANN) were created in order to solve scheduling problem. First ANN predicts future state of Grid based on current state and types of workflows that are currently executing. Second ANN output is optimal workflow type that should be executed. Inputs for second ANN are current state of the Grid and predicted future state (output of first ANN). Performance tests show that significant improvement of overall execution time can be achieved by this Hierarchical Artificial Neural Networks.","","978-1-4244-7984-9978-1-4244-7985-6978-1-4244-7983","10.1109/SOFA.2010.5565626","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5565626","","Artificial neural networks;Computer architecture;Databases;Optimization;Processor scheduling;Monitoring;Job shop scheduling","grid computing;neural nets;optimisation;scheduling;workflow management software","hierarchical neural model;workflow scheduling;utility management systems;computational grid infrastructure;job scheduling;resource optimization;feedback;artificial neural networks","","1","15","","","","","","IEEE","IEEE Conferences"
"Bandwidth optimizations for standards-based publish/subscribe in disadvantaged grids","E. Skjervold; K. Lund; T. H. Bloebaum; F. T. Johnsen","Norwegian Defence Research Establishment (FFI), Kjeller, Norway; Norwegian Defence Research Establishment (FFI), Kjeller, Norway; Norwegian Defence Research Establishment (FFI), Kjeller, Norway; Norwegian Defence Research Establishment (FFI), Kjeller, Norway","MILCOM 2012 - 2012 IEEE Military Communications Conference","","2012","","","1","8","NATO has identified Web services as a key enabler for its network enabled capability. Web services facilitate interoperability, easy integration and use of commercial off-the-shelf components, and while request/response-based schemes have hitherto been predominant, publish/subscribe-based services are gaining ground. SOAP-based Web services, however, introduce considerable communication overhead, and optimization must be done to enable use on the tactical level. Data compression is one such optimization, and it works well for large messages. We claim that the inherent characteristics of publish/subscribe-based Web services are such that using difference-based compression will allow effective compression also for small messages. In this paper we present the design and implementation of a proof-of-concept mechanism called ZDiff, which we have tested on several types of military data formats. Together with our SOAP-based proxy system it can be used together with commercial off-the-shelf Web services software. The results show that difference-based compression outperforms traditional compression for small messages, at the same time as it never performs worse than traditional compression for larger messages.","2155-7586;2155-7578;2155-7578","978-1-4673-1731-3978-1-4673-1729-0978-1-4673-1730","10.1109/MILCOM.2012.6415635","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6415635","Compression;Web services;publish/subscribe;WS-Notification","XML;Simple object access protocol;Standards;Bandwidth;Encoding","access protocols;data compression;military communication;military computing;open systems;optimisation;Web services","bandwidth optimization;publish-subscribe based Web services;interoperability;simple object access protocols;SOAP-based proxy system;communication overhead;data compression;difference-based compression;ZDiff;military data formats;network enabled capability","","3","15","","","","","","IEEE","IEEE Conferences"
"Multi-attribute automatic negotiation model based on niche genetic algorithm","Chen Pei-you; Li Yi-ling","College of Economic and Management, Heilongjiang Institute of Science and Technology, Harbin 150027, China; College of Economic and Management, Heilongjiang Institute of Science and Technology, Harbin 150027, China","2010 Chinese Control and Decision Conference","","2010","","","3380","3384","A multi-criteria ranking method about interval-valued intuitionistic fuzzy sets is introduced into the automatic negotiation model to weaken the subjectivity of preference information report from negotiation Agents. Using evidential reasoning algorithms, the criteria values are aggregated and the interval intuitionist fuzzy sets of alternatives are obtained; considering the incomplete certain information on weight, a nonlinear programming model is developed and the weight of various attribute is obtained. The niche genetic algorithm is proposed to search optimization and improve astringency. Finally, a simulation testing shows the model is a feasible and effective, which can help negotiation Agents to reach quickly a mutually beneficial agreement objectively.","1948-9439;1948-9447","978-1-4244-5181-4978-1-4244-5182","10.1109/CCDC.2010.5498579","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5498579","automatic negotiation;bargaining zone;interval-valued intuitionistic fuzzy number;niche genetic algorithm","Genetic algorithms;Fuzzy sets;Uncertainty;Educational institutions;Power generation economics;Technology management;Electronic mail;Testing;Electronic commerce;Decision theory","case-based reasoning;fuzzy set theory;genetic algorithms;negotiation support systems;nonlinear programming;search problems;software agents","multiattribute automatic negotiation model;niche genetic algorithm;multicriteria ranking method;interval valued intuitionistic fuzzy sets;negotiation agents;evidential reasoning algorithms;nonlinear programming model;search optimization","","","12","","","","","","IEEE","IEEE Conferences"
"Performance analysis of Android underlying virtual machine in mobile phones","E. Azimzadeh; M. Sameki; M. Goudarzi","Dept. of Computer Engineering, Sharif University of Technology, Tehran, Iran; Dept. of Computer Engineering, Sharif University of Technology, Tehran, Iran; Dept. of Computer Engineering, Sharif University of Technology, Tehran, Iran","2012 IEEE Second International Conference on Consumer Electronics - Berlin (ICCE-Berlin)","","2012","","","292","295","In recent years, Android is widely used in cell phones. Dalvik is the virtual machine which is embedded inside the Android operating system, and executes the Android-based applications. Thus, improving efficiency of the Dalvik virtual machine plays an important role in optimizing performance of android-based mobile phones. In this paper, we present a comprehensive analysis of the Dalvik bytecodes and their frequency of use in common Android applications and use the results to determine the most frequently used bytecodes in Dalvik virtual machine to identify best targets for improvement. Our analysis showed that over 82% of total execution time of our Android benchmarks is spent by onl.y 5 families of instructions.","2166-6822;2166-6814;2166-6814","978-1-4673-1547-0978-1-4673-1546-3978-1-4673-1545","10.1109/ICCE-Berlin.2012.6336470","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6336470","Android;Performance;Dalvik","Virtual machining;Smart phones;Program processors;Java;Operating systems;Benchmark testing","Linux;mobile computing;software performance evaluation;virtual machines","performance analysis;Android operating system;Dalvik virtual machine;efficiency improvement;performance optimization;Android-based mobile phones;Dalvik bytecode frequency;execution time","","","10","","","","","","IEEE","IEEE Conferences"
"Methodology &amp; tools for performance evaluation of IEC 61850 GOOSE based protection schemes","I. Ali; M. S. Thomas; S. Gupta","Department of Electrical Engineering FET, Jamia Millia Islamia New Delhi, India; Department of Electrical Engineering FET, Jamia Millia Islamia New Delhi, India; Department of Electrical Engineering FET, Jamia Millia Islamia New Delhi, India","2012 IEEE Fifth Power India Conference","","2012","","","1","6","IEC 61850 based high speed peer-to-peer communication called GOOSE (Generic Object Oriented Substation Event) has an impact on the development and testing of IEC 61850 relays and the associated protection schemes in modern power system. GOOSE not only provides an opportunity to optimize the system performance at reduced hardware cost but also eliminates various constraints in traditional protection schemes. Performance testing for GOOSE is an important item for the correctness and the real time performance of substation applications. The primary objective of this paper is to analyze the factors affecting the performance of GOOSE based protection schemes. The paper then discusses the methodology and advanced hardware and software tools available for performance measurement of lEC 61850 GOOSE.","","978-1-4673-0766-6978-1-4673-0763-5978-1-4673-0765","10.1109/PowerI.2012.6479511","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6479511","Substation Automation System (SAS);Power System Protection;Switched Ethernet;IEC 61850;GOOSE","IEC standards;Substations;Switches;Communication networks;Performance evaluation;Reliability;Testing","IEC standards;local area networks;object-oriented methods;peer-to-peer computing;real-time systems;relay protection;substation automation;substation protection","performance evaluation;IEC 61850 GOOSE-based protection scheme;high speed peer-to-peer communication;generic object oriented substation event;IEC 61850 relay development;IEC 61850 relay testing;power system;system performance optimization;hardware cost reduction;constraint elimination;performance testing;real time performance;substation applications;software tools;hardware tools;performance measurement","","5","25","","","","","","IEEE","IEEE Conferences"
"Probabilistic duration of power estimation for Nickel- metal- hydride (NiMH) battery under constant load using Kalman filter on chip","A. Kumar; J. Roychaudhury; S. Mandal","CSIR- Central Mechanical Engineering Research Institute, Durgapur, West Bengal, India; CSIR- Central Mechanical Engineering Research Institute, Durgapur, West Bengal, India; CSIR- Central Mechanical Engineering Research Institute, Durgapur, West Bengal, India","IEEE-International Conference On Advances In Engineering, Science And Management (ICAESM -2012)","","2012","","","641","646","For a battery powered safety critical system the safe duration of power for executing a specific task is extremely important. It is necessary to avoid unacceptable consequences due to unwanted battery power failure. An early stage estimation of this duration reduces the overall risk through optimization of current consumption by switching off noncritical load ahead of delivery of power to a critical load. In order to address this issue, an online battery state of charge estimator on chip is conceived and implemented using Kalman filter. The Kalman filter estimates the true values of measurements by predicting a value, considering the estimated uncertainty of the predicted value, and then computing a weighted average of the predicted value and the measured value. The basic idea is more accurate state prediction is possible when the state predicted value is fused with sensor prediction under any uncertain disturbance. The state estimator is developed in the form of an algorithm and stored into a single chip microcontroller. It is finally used to generate an early stage warning signal against battery failure. The paper presents a methodology for creating energy aware system that would avoid sudden system failure due to power outage. The authors used a generalized state space model of the battery to estimate the effect of unobserved battery parameters for duration estimation. An experiment was conducted in this regard through discharging the battery under constant load. Subsequently the internal parameters of battery were calculated. The model was simulated through MATLAB/simulink R2008a software and efficiency was tested. The program for prediction was finally emulated in a microcontroller and found satisfactory result.","","978-81-909042-2-3978-1-4673-0213-5INAVLID IS","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6216075","Kalman filter;Microcontroller;Recovery effect in battery;State of charge","Mathematical model;Estimation;Kalman filters;Resistance;Discharges (electric);Microcontrollers;Noise measurement","battery chargers;Kalman filters;microcontrollers;optimisation;power consumption;power harmonic filters;power system faults;power system reliability;safety-critical software;secondary cells","nickel-metal-hydride battery;NiMH battery;probabilistic duration;power estimation;constant load;Kalman filter;battery powered safety critical system;unwanted battery power failure;optimization;current consumption;charge estimator;uncertain disturbance;single chip microcontroller;early stage warning signal;power outage;generalized state space model;MATLAB/simulink R2008a software;Ni","","2","9","","","","","","IEEE","IEEE Conferences"
"Enhance Virtualized HPC System Based on I/O Behavior Perception and Asymmetric Scheduling","Y. Hu; X. Long; J. Zhang","NA; NA; NA","2012 IEEE 14th International Conference on High Performance Computing and Communication & 2012 IEEE 9th International Conference on Embedded Software and Systems","","2012","","","169","178","In virtualized HPC system such as virtual cluster and science cloud, CPU-intensive jobs are always companied by high-intensive I/O operations since different computing nodes perform periodic inter-VM communication to transfer data or synchronize computing result. Since traditional VMM schedulers cannot handle the scheduling scenario with mixed workloads efficiently, inter-VM communication always suffers serious performance regression from scheduling competition and then reduces the performance of entire virtualized HPC system. In order to address this issue, this paper proposes an asymmetric scheduling model based on I/O behavior perception. In this mode, we schedule I/O and computing jobs under isolated cpu subsets to erase their performance interference while optimizing the inter-VM communication through short period round robin scheduling. At the same time, we characterize the runtime I/O behavior of applications at fine temporal granularity and predict their I/O load state using specific online predictor. We will replan the scheduling scheme dynamically through migrating VMs across different cpu subsets if we predict a coming I/O intensity variation and decide the system performance could benefit from this scheduling adjustment. We build a prototype based on Xen-4.1.0 virtual and preliminary test results demonstrate that our approach could efficiently promote the performance of inter-VM communication under virtualized HPC environment while reducing the computing performance degradation caused by I/O-prior scheduling.","","978-1-4673-2164-8978-0-7695-4749","10.1109/HPCC.2012.31","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6332173","HPC;Virtualization;Cloud;Scheduling;I/O","Processor scheduling;Optimization;Benchmark testing;Virtual machining;Time frequency analysis;Dynamic scheduling;Runtime","cloud computing;input-output programs;multiprocessing systems;parallel processing;processor scheduling;regression analysis;synchronisation;virtual machines;virtualisation","I-O behavior perception-based virtualized HPC system;high-intensive I-O operations;periodic interVM communication;data transfer;VMM schedulers;performance regression;scheduling competition;asymmetric scheduling model;performance interference;short period round robin scheduling;temporal granularity;I-O load state;specific online predictor;I-O intensity variation;Xen-4.1.0 virtual test;VM communication;virtualized HPC environment;I-O-prior scheduling;isolated CPU subsets","","","20","","","","","","IEEE","IEEE Conferences"
"Graph partitioning using a Simulated Bee Colony algorithm","J. D. McCaffrey","Microsoft, USA","2011 IEEE International Conference on Information Reuse & Integration","","2011","","","400","405","Graph partitioning is a problem which has great practical importance. Because graph partitioning is an NP-complete problem, there has been much research attention focused on developing heuristics which find reasonably good approximations to optimal solutions. This study presents a Simulated Bee Colony (SBC) graph partitioning algorithm which is based on the foraging behavior of honey bees. A computer program which implemented the SBC algorithm was executed against 12 benchmark graph partitioning problems. The SBC algorithm produced partitions with better quality than the best published results for 10 of the 12 benchmark problems. The results suggest that Simulated Bee Colony algorithms are a highly effective technique for partitioning graphs in situations where partition quality is more important than real-time performance and that using SBC graph partitioning algorithms may be particularly useful in problem scenarios where the partition result is intended for reuse such as analyses of large communication graphs.","","978-1-4577-0966-1978-1-4577-0964-7978-1-4577-0965","10.1109/IRI.2011.6009581","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6009581","Combinatorial optimization;graph partitioning;multi-agent systems;simulated bee colony algorithm;swarm intelligence","Partitioning algorithms;Algorithm design and analysis;Approximation algorithms;Benchmark testing;Optimization;Software algorithms;Approximation methods","computational complexity;graph theory;multi-agent systems","simulated bee colony graph partitioning algorithm;NP-complete problem;honey bees foraging behavior;SBC graph partitioning algorithms;communication graphs;multiagent systems;combinatorial optimization;swarm intelligence","","2","22","","","","","","IEEE","IEEE Conferences"
"Scalable multi-core simulation using parallel dynamic binary translation","O. Almer; I. Böhm; T. E. von Koch; B. Franke; S. Kyle; V. Seeker; C. Thompson; N. Topham","Institute for Computing Systems Architecture, University of Edinburgh, United Kingdom; Institute for Computing Systems Architecture, University of Edinburgh, United Kingdom; Institute for Computing Systems Architecture, University of Edinburgh, United Kingdom; Institute for Computing Systems Architecture, University of Edinburgh, United Kingdom; Institute for Computing Systems Architecture, University of Edinburgh, United Kingdom; Software Engineering for Embedded Systems Group, Technical University of Berlin, Germany; Institute for Computing Systems Architecture, University of Edinburgh, United Kingdom; Institute for Computing Systems Architecture, University of Edinburgh, United Kingdom","2011 International Conference on Embedded Computer Systems: Architectures, Modeling and Simulation","","2011","","","190","199","In recent years multi-core processors have seen broad adoption in application domains ranging from embedded systems through general-purpose computing to large-scale data centres. Simulation technology for multi-core systems, however, lags behind and does not provide the simulation speed required to effectively support design space exploration and parallel software development. While state-of-the-art instruction set simulators (ISS) for single-core machines reach or exceed the performance levels of speed-optimised silicon implementations of embedded processors, the same does not hold for multi-core simulators where large performance penalties are to be paid. In this paper we develop a fast and scalable simulation methodology for multi-core platforms based on parallel and just-in-time (JIT) dynamic binary translation (DBT). Our approach can model large-scale multi-core configurations, does not rely on prior profiling, instrumentation, or compilation, and works for all binaries targeting a state-of-the-art embedded multi-core platform implementing the ARCompact instruction set architecture (ISA). We have evaluated our parallel simulation methodology against the industry standard Splash-2 and EEMBC MULTIBENCH benchmarks and demonstrate simulation speeds up to 25,307 Mips on a 32-core ×86 host machine for as many as 2048 target processors whilst exhibiting minimal and near constant overhead.","","978-1-4577-0801-5978-1-4577-0802","10.1109/SAMOS.2011.6045461","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6045461","","Program processors;Multicore processing;Random access memory;Hardware;Computational modeling;Field programmable gate arrays;Benchmark testing","embedded systems;general purpose computers;instruction sets;just-in-time;multiprocessing systems;parallel programming;software engineering","scalable multicore simulation;parallel dynamic binary translation;multicore processors;embedded systems;general-purpose computing;large-scale data centres;parallel software development;instruction set simulators;just-in-time;JIT;DBT;ARCompact;instruction set architecture;ISA","","7","41","","","","","","IEEE","IEEE Conferences"
"The 65nm PACDSP subsystem with embedded thermal sensors","H. Hsieh; S. Wen; C. Liao; H. Lin; P. Huang; S. Tung","Information and Communications Research Laboratories, Industrial Technology Research Institute, Hsinchu, Taiwan; Information and Communications Research Laboratories, Industrial Technology Research Institute, Hsinchu, Taiwan; Information and Communications Research Laboratories, Industrial Technology Research Institute, Hsinchu, Taiwan; Information and Communications Research Laboratories, Industrial Technology Research Institute, Hsinchu, Taiwan; Information and Communications Research Laboratories, Industrial Technology Research Institute, Hsinchu, Taiwan; Information and Communications Research Laboratories, Industrial Technology Research Institute, Hsinchu, Taiwan","Proceedings of 2011 International Symposium on VLSI Design, Automation and Test","","2011","","","1","4","Today's embedded systems mostly target on portable devices, which are expected not only to be small, lightweight, fully functional, and real-time, but also to provide extremely long battery lifetime. Therefore, energy-efficiency has become a new challenge. Due to this reason, the power consumption is more and more important for the system on chip (SoC) design. In this paper, we discuss the 65nm DSP subsystem which provides a power optimized DSP subsystem for dual-core software development and SoC prototyping. The most important component of 65nm DSP subsystem is the 65nm Media Platform IC. In the 65nm Media Platform IC, the dynamic voltage &amp; frequency scaling (DVFS) and power gating mechanism have been applied to reduce power dissipations with a novel Unified Power Format (UPF) flow. The 65nm Media Platform IC is fabricated in the TSMC 65nm CMOS technology, of which the estimated power dissipations are 40.97mW for 240MHz @1.0V and 56.47mW for 342MHz @1.2V respectively.","","978-1-4244-8499-7978-1-4244-8500-0978-1-4244-8498","10.1109/VDAT.2011.5783539","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5783539","","Thermal sensors;Media;Integrated circuits;Digital signal processing;Power demand;VLIW","digital signal processing chips;embedded systems;intelligent sensors;system-on-chip","PACDSP subsystem;embedded thermal sensors;embedded systems;energy-efficiency;system on chip;dual-core software development;SoC prototyping;unified power format flow","","1","7","","","","","","IEEE","IEEE Conferences"
"Extending an agent-based FMS scheduling approach with parallel genetic algorithms","G. Abaza; I. Badr; P. Goehner; S. Jeschke","Institute of Industrial Automation and Software Engineering, Stuttgart University; Institute of Industrial Automation and Software Engineering, Stuttgart University; Institute of Industrial Automation and Software Engineering, Stuttgart University; Institute of Information Management in Mechanical Engineering &amp; Center for Learning and Knowledge Management, ZLW/IMA, RWTH Aachen University","IECON 2010 - 36th Annual Conference on IEEE Industrial Electronics Society","","2010","","","2689","2694","Flexible manufacturing systems (FMS) aim at efficiently reacting to changing market needs to stand the increasing competitiveness. This imposes efficiency and flexibility requirements on FMS scheduling. Manufacturing scheduling is the process of allocating available manufacturing resources to the set of planned jobs over time. It is an optimization process by which limited manufacturing resources are to be allocated to several jobs of different products efficiently. The agent-based scheduling approach has shown the ability to fulfill the flexibility requirement. Although this approach emphasizes flexibility, it lacks the optimization support. In this paper, an agent-based scheduling approach is extended with parallel genetic algorithms (PGA) to provide the required optimization support. Test results have shown a remarkable enhancement to the optimality of the generated schedules with respect to the predefined set of manufacturing objectives. The extended approach fulfils both flexibility and efficiency requirements on manufacturing scheduling.","1553-572X;1553-572X","978-1-4244-5225-5978-1-4244-5226","10.1109/IECON.2010.5675132","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5675132","","Biological cells;Job shop scheduling;Electronics packaging;Schedules;Manufacturing;Computer architecture","flexible manufacturing systems;genetic algorithms;multi-agent systems;scheduling","agent based FMS scheduling approach;parallel genetic algorithms;flexible manufacturing system;manufacturing scheduling;optimization process","","1","11","","","","","","IEEE","IEEE Conferences"
"Integrated system for the monitoring and diagnosis used to provide an optimum operation of the internal network of a power source","P. M. Nicolae; M. Motocu; G. Vlăduţ; C. Constantinescu","University of Craiova, Craiova, Romania; Turceni Power Plant, Turceni, Romania; IPACIFAT S.A.; IPACIFAT S.A.","2010 IEEE International Conference on Automation, Quality and Testing, Robotics (AQTR)","","2010","3","","1","6","One presents a complex data acquisition structure used for the monitoring and diagnosis of a system used for electric power transmission in the zone of large power source. The structure is also used for the optimization of power network operation in order to use the network also for the transmission of electric signals other than those of 50 Hz. The schematic of the monitored system is presented, the test points are specified. The tests are performed with three independent synchronized data acquisition systems that can acquire up to 24 electrical quantities. The hardware and software solutions are described. Examples of using the presented solutions to record some electric quantities are provided. Real time information are obtained using the data acquisition structure along with the software packages dedicated to the data analysis and processing. At the end waveforms recorded in various points of the power group are provided.","","978-1-4244-6725-9978-1-4244-6724-2978-1-4244-6723","10.1109/AQTR.2010.5520675","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5520675","","Data acquisition;Synchronous generators;Power system protection;Power system harmonics;Condition monitoring;Hardware;Signal processing;Power system stability;Power generation;System testing","data acquisition;power engineering computing;power system measurement","integrated system;monitoring;diagnosis;optimum operation;internal network;power source;complex data acquisition structure;hardware solutions;software solutions;end waveforms","","8","10","","","","","","IEEE","IEEE Conferences"
"Reconstructing control flow graph for control flow checking","Jianli Li; Qingping Tan; Jianjun Xu","School of Computer, National University of Defense Technology, Changsha, China; School of Computer, National University of Defense Technology, Changsha, China; School of Computer, National University of Defense Technology, Changsha, China","2010 IEEE International Conference on Progress in Informatics and Computing","","2010","1","","527","531","In the space radiation environment, a large number of cosmic rays often lead to transient faults on the on-board computer. These transient faults result in data flow errors or control flow errors during program running. The present software implemented hardware fault tolerant technology mainly uses the signature analysis method to realize the control flow checking, namely, through assigning signature for each basic block and inserting some instructions into every basic block to realize the control flow checking. Because the size of different basic blocks in one program usually exist obvious difference, applying unified checking method for these basic blocks will reduce the protection efficiency. To solve this problem, this paper has proposed a control flow checking optimization method named RCFG by reconstructing control flow graph. RCFG firstly merges basic blocks into larger logic blocks, then cuts the logic blocks into basic logic blocks with similar size. At last, control flow detection algorithm can be applied based on the control flow graph composed with the basic logic blocks. RCFG can effectively improve the protection efficiency of algorithm, and user can regulate the balance between performance and reliability by configuring the size of basic logic block. This paper has finished the fault injection experiment for a typical signature analysis algorithm named CFCSS. According to the experiment result, compared with the original CFCSS algorithm, the average performance expense of the CFCSS algorithm implemented based on RCFG increased by 16.6%, and the average memory expense increased by 13.5%, but the number of the faults resulting in the program outputting wrong result reduced by 47.67% equally.","","978-1-4244-6789-1978-1-4244-6788-4978-1-4244-6787","10.1109/PIC.2010.5687403","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5687403","control flow errors;logic block;basic logic block;fault injection","Lead;Aerospace electronics","cosmic rays;data flow analysis;data flow graphs;fault tolerant computing;logic testing;optimisation","control flow graph reconstruction;control flow checking;space radiation environment;cosmic rays;transient faults;on-board computer;data flow errors;hardware fault tolerant technology;signature analysis method;control flow detection algorithm;fault injection;CFCSS algorithm","","2","12","","","","","","IEEE","IEEE Conferences"
"Java Support Packages and Benchmarks for Multi-core Processors","V. Olaru; A. Hangan; G. Sebestyen-Pal","NA; NA; NA","2011 IEEE International Conference on High Performance Computing and Communications","","2011","","","528","535","This paper presents Java support packages that help optimize the program performance by improving the coordination with the underlying software and hardware (operating system and CPU). The software support exports low-level information about processor features (cache sizes and sharing, number of logical CPUs per chip/core, Simultaneous Multi-Threading, etc) to the application level and addresses issues such as CPU/interrupt affinity, thread scheduling and synchronization. The paper also shows how to use the support packages to develop micro-benchmarks for Java VMs and Real-Time Specification for Java (RTSJ) implementations running on multi-core CPUs. A benchmark suite consisting of memory, asynchronous event handling (for RTSJ implementations only) and locking tests is described and evaluated on Java and Jamaica VM [15].","","978-1-4577-1564-8978-0-7695-4538","10.1109/HPCC.2011.75","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6063035","","Instruction sets;Java;Arrays;Hardware;Real time systems;Benchmark testing;Synchronization","benchmark testing;cache storage;Java;multiprocessing systems;multi-threading;processor scheduling;real-time systems","Java support packages;multicore processors;program performance;underlying software;hardware;operating system;software support;low-level information;processor features;cache sizes;cache sharing;logical CPU per chip;logical CPU per core;simultaneous multithreading;CPU affinity;interrupt affinity;thread scheduling;synchronization;microbenchmarks;Java VM;real-time specification for Java;RTSJ implementations;multicore CPU;benchmark suite;memory;asynchronous event handling;Jamaica VM","","1","23","","","","","","IEEE","IEEE Conferences"
"Improving estimation algorithms based on field data for a hybrid energy system","C. Vigu; D. I. Gota; D. Capatana","IPA R&amp;D Institute for Automation, Cluj Subsidiary; IPA R&amp;D Institute for Automation, Cluj Subsidiary; IPA R&amp;D Institute for Automation, Cluj Subsidiary","2010 IEEE International Conference on Automation, Quality and Testing, Robotics (AQTR)","","2010","3","","1","4","The production of green energy is a topic of great importance nowadays and of wide interest throughout the world. The system described in this paper allows an in-depth evaluation of the energetic efficiency of the green power solution proposed to power a remote location. Our goal is to study the difference between the values proposed with the aid of theoretical algorithms for energy production estimation and the field data. By studying this differences new solutions can be proposed to optimize the algorithms.","","978-1-4244-6725-9978-1-4244-6724-2978-1-4244-6723","10.1109/AQTR.2010.5520762","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5520762","","Control systems;SCADA systems;Programmable control;Production systems;Inverters;Process control;Displays;Centralized control;Application software;Ground penetrating radar","environmental factors;hybrid power systems","hybrid energy system;green energy production;energy production estimation","","","5","","","","","","IEEE","IEEE Conferences"
"Improving ranking of electric power system dynamic behavior in DSA system by applying VSC based HVDC technology","C. Nguyen Mau; N. T. Trinh; K. Rudion; E. Lerch; Z. A. Styczynski","Electric Power Networks and Renewable Energy Sources at the Otto-von-Guericke-University Magdeburg, Germany; Siemens AG, IC SG SE PTI NC, Erlangen, Germany; Electric Power Networks and Renewable Energy Sources at the Otto-von-Guericke-University Magdeburg, Germany; Siemens AG, IC SG SE PTI NC, Erlangen, Germany; Electric Power Networks and Renewable Energy Sources at the Otto-von-Guericke-University Magdeburg, Germany","2012 3rd IEEE PES Innovative Smart Grid Technologies Europe (ISGT Europe)","","2012","","","1","7","This paper first discusses the ranking of electric power system dynamic behavior in a dynamic security assessment system. This ranking is calculated by combining seven performance indices based on fuzzy logic. These security indices are applied to monitor the security level of the power system. Secondly, the paper focuses on the application of voltage source converter based high voltage direct current (VSC HVDC) transmission for improving this ranking. Due to the advantages that VSC HVDC has, such as fast and decoupled active and reactive power control, the power system stability can receive many benefits after a disturbance such as damping the oscillation of the load angle of the generators as well as voltage support. The validity of the technique is tested on an exemplary 11-bus system using PSS<sup>®</sup>NETOMAC software simulation. This procedure can be applied to both off-line as well as on-line applications.","2165-4816;2165-4816;2165-4824","978-1-4673-2597-4978-1-4673-2595-0978-1-4673-2596","10.1109/ISGTEurope.2012.6465732","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6465732","damping;dynamic security assessment (DSA);high voltage direct current (HVDC);load angle;power system stability;power swing;voltage source converter (VSC)","Indexes;HVDC transmission;Power conversion;Security;Voltage control;Power system stability","damping;electric generators;fuzzy logic;HVDC power transmission;power system dynamic stability;power system security;reactive power control","electric power system dynamic behavior;DSA system;VSC based HVDC technology;dynamic security assessment system;fuzzy logic;voltage source converter based high voltage direct current transmission;decoupled active power control;decoupled reactive power control;power system stability;PSS NETOMAC software simulation;voltage support;generators;load angle oscillation;11-bus system","","1","12","","","","","","IEEE","IEEE Conferences"
"Open source algorithm for storage area and temporally optimized run length coding for image compression technology used in biomedical imaging","M. B. Akhtar; Qamar-ul-Islam","Department of Communication Systems Engineering, Institute of Space Technology, Islamabad, Pakistan; Department of Communication Systems Engineering, Institute of Space Technology, Islamabad, Pakistan","2012 International Conference on Open Source Systems and Technologies","","2012","","","16","21","The objective of this paper is to prove the significance of the optimized run length coding algorithm for biomedical imaging technology and open source the idea behind the optimized algorithm in a comprehensive way. An optimized scheme for entropy encoding part of JPEG image compression by modifying the run length encoding method has been provided by the authors for a Space Research Program at Institute of Space technology (IST). The same has been observed to produce a large amount of saving in terms of memory required for Biomedical compressed images. In JPEG (Joint Photographic Experts Group) image compression algorithm run length coding performs the actual compression by removing the redundancy from transformed and quantized image data. Using the fact that the preceding processes of run length coding produces a large number of zeros the original run length coding uses an ordered pair (a, b), where `a' is the length of consecutive zeros preceding the ASCII character `b'. The proposed run length encoding scheme removes the unintended redundancy by using an ordered pair only when a zero occurs. The proposed encoding scheme does not alter the PSNR value for the algorithm. Using Matlab simulation, the proposed scheme has been tested on various biomedical images over a range of quantization (quality) factor and the results confirmed the effectiveness of the new run length encoding scheme in reducing the run length encoded data size and processing time delay.","","978-1-4673-3097-8978-1-4673-3094-7978-1-4673-3096","10.1109/ICOSST.2012.6472821","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6472821","compression;JPEG;entropy coding;run length coding;Biomedical Imaging","Image coding;Transform coding;Quantization;Discrete cosine transforms;Algorithm design and analysis;Biomedical imaging;Compression algorithms","data compression;image coding;medical image processing;public domain software;quantisation (signal)","IST;quantized image data;transformed image data;redundancy removal;quantization factor;peak signal-to-noise ratio;PSNR value;image data quantization;Joint Photographic Experts Group;Institute of Space Technology;Space Research Program;JPEG image compression;biomedical imaging;image compression technology;storage area;temporally optimized run length coding;open source algorithm","","","11","","","","","","IEEE","IEEE Conferences"
"A SimPLR method for routability-driven placement","M. Kim; J. Hu; D. Lee; I. L. Markov","University of Michigan, Department of EECS, 2260 Hayward St, Ann Arbor, MI 48109-2121; University of Michigan, Department of EECS, 2260 Hayward St, Ann Arbor, MI 48109-2121; University of Michigan, Department of EECS, 2260 Hayward St, Ann Arbor, MI 48109-2121; University of Michigan, Department of EECS, 2260 Hayward St, Ann Arbor, MI 48109-2121","2011 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)","","2011","","","67","73","Highly-optimized placements may lead to irreparable routing congestion due to inadequate models of modern interconnect stacks and the impact of partial routing obstacles. Additional challenges in routability-driven placement include scalability to large netlists and limiting the complexity of software integration. Addressing these challenges, we develop lookahead routing to give the placer advance, firsthand knowledge of trouble spots, not distorted by crude congestion models. We also extend global placement to (i) spread cells apart in congested areas, and (ii) move cells together in less-congested areas to ensure short, routable interconnects and moderate runtime. While previous work adds isolated steps to global placement, our SIMultaneous PLace-and-Route tool SimPLR integrates a layer- and via-aware global router into a leading-edge, force-directed placer. The complexity of integration is mitigated by careful design of simple yet effective optimizations. On the ISPD 2011 Contest Benchmark Suite, with the official evaluation protocol, SimPLR outperforms every contestant on every benchmark.","1558-2434;1092-3152;1092-3152","978-1-4577-1400-9978-1-4577-1399-6978-1-4577-1398","10.1109/ICCAD.2011.6105307","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6105307","","Routing;Benchmark testing;Runtime;Optimization;Metals;Estimation;Layout","integrated software;telecommunication computing;telecommunication network routing","SimPLR method;routability-driven placement;routing congestion;interconnect stack;partial routing obstacle;software integration;simultaneous place-and-route tool;layer-aware global router;via-aware global router;look-ahead routing","","35","42","","","","","","IEEE","IEEE Conferences"
"A Block Motion Compensation Fast Algorithm Designed for RV40 Codec","H. Yu; Y. Yang; S. Liu; J. Chen","NA; NA; NA; NA","2010 International Conference on Computational Intelligence and Software Engineering","","2010","","","1","3","This paper proposed a whole new design for RV40(realvideo40) video decoder from FFmpeg(fast forward Moving Picture Experts Group),which is in lack of the ability to decode smoothly when decoding a period of high bit rate frames within some video streams. The design specially optimized the sub pixel interpolation algorithm, which is the core of motion compensation. The design uses detailed, different algorithms based on different frame types. Meanwhile a temporal related, flexible algorithm usage method is introduced to deal with the jamming in decoding sequence. The time consumed by calculating is reduced with an acceptable reduced frame appearance. This design is used on a test platform for testify. The result returned shows that this design can greatly promote decoding performance up to 50% while reducing a little picture performance.","","978-1-4244-5391-7978-1-4244-5392","10.1109/CISE.2010.5677221","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5677221","","Decoding;Motion compensation;Pixel;Streaming media;Optimization;Codecs;Interpolation","decoding;motion compensation;video codecs;video coding;video streaming","block motion compensation fast algorithm;RV40 codec;video decoder;FFmpeg;fast forward moving picture experts group;decoding;video streams","","","5","","","","","","IEEE","IEEE Conferences"
"Cooperative extended rough attribute reduction algorithm based on improved PSO","W. Ding; J. Wang; Z. Guan","College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing 210016, P. R. China; School of Computer Science and Technology, Nantong University, Nantong 226019, P. R. China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing 210016, P. R. China; School of Computer Science and Technology, Nantong University, Nantong 226019, P. R. China","Journal of Systems Engineering and Electronics","","2012","23","1","160","166","Particle swarm optimization (PSO) is a new heuristic algorithm which has been applied to many optimization problems successfully. Attribute reduction is a key studying point of the rough set theory, and it has been proven that computing minimal reduction of decision tables is a non-derterministic polynomial (NP)-hard problem. A new cooperative extended attribute reduction algorithm named Co-PSAR based on improved PSO is proposed, in which the cooperative evolutionary strategy with suitable fitness functions is involved to learn a good hypothesis for accelerating the optimization of searching minimal attribute reduction. Experiments on Benchmark functions and University of California, Irvine (UCI) data sets, compared with other algorithms, verify the superiority of the Co-PSAR algorithm in terms of the convergence speed, efficiency and accuracy for the attribute reduction.","1004-4132","","10.1109/JSEE.2012.00020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6161708","rough set;extended attribute reduction;particle swarm optimization (PSO);cooperative evolutionary strategy;fitness function","Optimization;Heuristic algorithms;Evolutionary computation;Benchmark testing;Particle swarm optimization;Convergence","","","","1","","","","","","","BIAI","BIAI Journals & Magazines"
"A Comparison of Phase Transitions Produced by PARAMICS, TransModeler, and VISSIM","L. Lu; T. Yun; L. Li; Y. Su; D. Yao","Department of Automation, TNList, Tsinghua University, Beijing, China 100084.; An undergraduate student at the Department of Automation, Tsinghua University, mainly working on traffic data analysis.; Currently an Associate Professor at Department of Automation, Tsinghua University; He is now a PostDoctor with Research Institute of Information Technology, -Tsinghua University, mainly working on modeling, optimization and automatic control, and their applications to traffic and transportation systems.; He is a Full Professor of control science and engineering with his current research interests in intelligent transportation system engineering, urban traffic control and management, automatic detection and control, and advanced control theory and application.","IEEE Intelligent Transportation Systems Magazine","","2010","2","3","19","24","Increasing efforts are devoted to test and improve the performance of commercial traffic simulation software now. However, most previous approaches emphasized microscopic car-following and lane-changing features but neglect the macroscopic traffic dynamics. In this paper, we compare the traffic flow phase transition behaviors of three popular simulation software: PARAMICS 6.5, TransModeler 2.0 and VISSIM 5.1 on a single yet typical scenario. Our testing re-sults show that their slight differences in microscopic models can be appropriately magnified and intuitively displayed in the phase diagram.","1939-1390;1941-1197","","10.1109/MITS.2010.939193","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5668148","","Simulation;Roads;Microscopy;Performance evaluation;Vehicle dynamics","traffic engineering computing","commercial traffic simulation software;microscopic car following feature;microscopic lane changing feature;macroscopic traffic dynamics;traffic flow phase transition;PARAMICS 6.5;TransModeler 2.0;VISSIM 5.1","","17","20","","","","","","IEEE","IEEE Journals & Magazines"
"A review of the research on quantitative reliability Prediction and Assessment for electronic components","Yang Zhao; Xiaoyan Yin; R. Kang; K. S. Trivedi","School of Reliability and Systems Engineering, Beihang University, China; Department of Electrical and Computer Engineering, Duke University, Durham, USA; School of Reliability and Systems Engineering, Beihang University, China; Department of Electrical and Computer Engineering, Duke University, Durham, USA","2011 Prognostics and System Health Managment Confernece","","2011","","","1","7","A review is carried out on how quantitative approaches have been applied so far to the Reliability Prediction and Assessment (RPA) for computer and communication systems. A series of the reliability evaluation technology based on analytic models and computer simulations are developed for use in product design and test, shape, system operation and maintenance, during a research initiative towards understanding the quantitative characteristics of hardware and software systems. The implementation of these techniques guarantees that the sophisticated system satisfying the high standard on reliability, availability, performability, maintainability and safety. Such approaches conduct quantitative assessment for reliability at the system level. Such reliability quantitative assessments are effectively used in the system decision-making for fault detection, failures elimination, optimization, maintenance and safety.","2166-563X;2166-5656","978-1-4244-7950-4978-1-4244-7951-1978-1-4244-7949","10.1109/PHM.2011.5939553","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5939553","Assessment;Prediction;Reliability","Software reliability;Companies;Continuous wavelet transforms;Aerospace engineering;Random access memory;Software","decision making;failure analysis;life testing;reliability;standards","electronic components;reliability prediction and assessment;reliability evaluation technology;system decision-making;fault detection;failures elimination;optimization;maintenance;safety","","","55","","","","","","IEEE","IEEE Conferences"
"UML design for dynamically reconfigurable multiprocessor embedded systems","J. Vidal; F. de Lamotte; G. Gogniat; J. Diguet; P. Soulard","Lab-STICC - European University of Brittany - UBS - CNRS, UMR 3192, Centre de Recherche - BP 92116 - F-56321 Lorient Cedex - FRANCE; Lab-STICC - European University of Brittany - UBS - CNRS, UMR 3192, Centre de Recherche - BP 92116 - F-56321 Lorient Cedex - FRANCE; Lab-STICC - European University of Brittany - UBS - CNRS, UMR 3192, Centre de Recherche - BP 92116 - F-56321 Lorient Cedex - FRANCE; Lab-STICC - European University of Brittany - UBS - CNRS, UMR 3192, Centre de Recherche - BP 92116 - F-56321 Lorient Cedex - FRANCE; SODIUS - 6 rue de Cornouaille - F-44300 NANTES - FRANCE","2010 Design, Automation & Test in Europe Conference & Exhibition (DATE 2010)","","2010","","","1195","1200","In this paper we propose a design methodology to explore partial and dynamic reconfiguration of modern FPGAs. We improve an UML based co-design methodology to allow dynamic properties in embedded systems. Our approach targets MPSoPC (Multiprocessor System on Programmable Chip) which allows area optimization through partial reconfiguration without performance penalty. In our case area reduction is achieved by reconfiguring co-processors connected to embedded processors. Most of the system is automatically generated by means of MDE techniques. Our modeling approach allows designers to target dynamic reconfiguration without being expert of modern FPGAs as many implementation details are hidden during the modeling step. Such a methodology allows design time speedup and a significant reduction of the gap between hardware and software modeling. In order to validate our approach, an object tracking application has been implemented on a reconfigurable system composed of 4 embedded processors and 3 co-processors. Dynamic reconfiguration has been performed for one co-processor which dynamically implements 3 different computations.","1558-1101;1530-1591;1530-1591","978-3-9810801-6-2978-1-4244-7054-9978-3-9810801-6","10.1109/DATE.2010.5456989","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5456989","","Unified modeling language;Embedded system;Coprocessors;Design methodology;Field programmable gate arrays;Multiprocessing systems;Model driven engineering;Hardware;Target tracking;Application software","embedded systems;field programmable gate arrays;logic design;optimisation;system-on-chip;Unified Modeling Language","UML design;reconfigurable multiprocessor embedded;FPGA;multiprocessor system on programmable chip;MPSoPC;field programmable gate array","","13","21","","","","","","IEEE","IEEE Conferences"
"An Approach for Classifying Program Failures","B. Ozcelik; K. Kalkan; C. Yilmaz","NA; NA; NA","2010 Second International Conference on Advances in System Testing and Validation Lifecycle","","2010","","","93","98","In this work, we leverage hardware performance counters-collected data to automatically group program failures that stem from closely related causes into clusters, which can in turn help developers prioritize failures as well as diagnose their causes. Hardware counters have been used for performance analysis of software systems in the past. By contrast, in this paper they are used as abstraction mechanisms for program executions. The results of our feasibility studies conducted on two widely-used applications suggest that hardware counters-collected data can be used to reliably classify failures.","","978-1-4244-7784-5978-0-7695-4146","10.1109/VALID.2010.10","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5617204","failure classification;debugging aids;hardware performance counters","Radiation detectors;Hardware;Clustering algorithms;Sockets;Accuracy;Algorithm design and analysis;Flexible printed circuits","pattern classification;program debugging;software reliability","program failure classification approach;hardware performance counters;program diagnosis;software systems","","2","17","","","","","","IEEE","IEEE Conferences"
"Computer based virtual instrument technology developed practical ability of undergraduates","B. Yang; Jianxin Li","Department of computer science and technology, Suzhou University, 234000, China; Department of computer science and technology, Suzhou University, 234000, China","2010 2nd International Conference on Education Technology and Computer","","2010","4","","V4-43","V4-47","On information times with the scale expansion situation in university, in order to develop undergraduates' practical ability and meet needs of talent for social development, the teacher should explore new ideas for practical teaching. This work inspires undergraduates to design their own comprehensive experiment by using the advantages of virtual instrument technology. Virtual instrument is a demand for the organization in accordance with instrument data acquisition system. It is a computer instrumentation systems with the general-purpose computer as the core hardware platform, designed by the user who defined functions, with a virtual panel, the test functions was realized by test software. This method optimizes practical teaching, develop the innovation ability and shape the quality of modern engineering of undergraduates.","2155-1812","978-1-4244-6370-1978-1-4244-6367-1978-1-4244-6369","10.1109/ICETC.2010.5529738","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5529738","experiment teaching;virtual instrument;innovation;pracitical ablity;computing platform","Instruments;Technological innovation;Education;Laboratories;Educational technology;Software testing;System testing;Optimized production technology;Data acquisition;Computer science","computer aided instruction;data acquisition;laboratory techniques;student experiments;user interfaces;virtual instrumentation","virtual instrument technology;practical ability;practical teaching;data acquisition system;computer instrumentation systems","","","12","","","","","","IEEE","IEEE Conferences"
"Adaptation in Dynamic Environments: A Case Study in Mission Planning","L. T. Bui; Z. Michalewicz; E. Parkinson; M. B. Abello","Department of Software Engineering, Le Quy Don Technical University, Hanoi, Vietnam; School of Computer Science, University of Adelaide, Adelaide, Australia; School of Computer Science, University of Adelaide, Adelaide, Australia; School of Computer Science, University of Adelaide, Adelaide, Australia","IEEE Transactions on Evolutionary Computation","","2012","16","2","190","209","Many random events usually are associated with executions of operational plans at various companies and organizations. For example, some tasks might be delayed and/or executed earlier. Some operational constraints can be introduced due to new regulations or business rules. In some cases, there might be a shift in the relative importance of objectives associated with these plans. All these potential modifications create a huge pressure on planning staff for generating plans that can adapt quickly to changes in environment during execution. In this paper, we address adaptation in dynamic environments. Many researchers in evolutionary community addressed the problem of optimization in dynamic environments. Through an overview on applying evolutionary algorithms for solving dynamic optimization problems, we classify the paper into two main categories: 1) finding/tracking optima, and 2) adaptation and we discuss their relevance for solving planning problems. Based on this discussion, we propose a computational approach to adaptation within the context of planning. This approach models the dynamic planning problem as a multiobjective optimization problem and an evolutionary mechanism is incorporated; this adapts the current solution to new situations when a change occurs. As the multiobjective model is used, the proposed approach produces a set of non-dominated solutions after each planning cycle. This set of solutions can be perceived as an information-rich data set which can be used to support the adaptation process against the effect of changes. The main question is how to exploit this set efficiently. In this paper, we propose a method based on the concept of centroids over a number of changing-time steps; at each step we obtain a set of non-dominated solutions. We carried out a case study on this proposed approach. Mission planning was used for our experiments and experimental analysis. We selected mission planning as our test environment because battlefields are always highly dynamic and uncertain and can be conveniently used to demonstrate different types of changes, especially time-varying constraints. The obtained results support the significance of our centroid-based approach.","1089-778X;1089-778X;1941-0026","","10.1109/TEVC.2010.2104156","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6095344","Adaptation;dynamic environments;evolutionary algorithms;multiobjective optimization","Planning;Dynamic scheduling;Vectors;Optimal scheduling;Genetic algorithms","dynamic programming;evolutionary computation;military systems;tracking","dynamic environments adaptation;mission planning;operational constraints;business rules;planning staff;evolutionary community;dynamic environments optimization;evolutionary algorithms;tracking optima;computational approach;multiobjective optimization problem;nondominated solutions;planning cycle;information-rich data set;adaptation process;changing-time steps;time-varying constraints;centroid-based approach","","18","55","","","","","","IEEE","IEEE Journals & Magazines"
"Option space exploration using distributed computing for efficient benchmarking of FPGA cryptographic modules","B. Brewster; E. Homsirikamol; R. Velegalati; K. Gaj","ECE Department, George Mason University, 4400 University Drive, Fairfax, VA 22030, USA; ECE Department, George Mason University, 4400 University Drive, Fairfax, VA 22030, USA; ECE Department, George Mason University, 4400 University Drive, Fairfax, VA 22030, USA; ECE Department, George Mason University, 4400 University Drive, Fairfax, VA 22030, USA","2012 International Conference on Field-Programmable Technology","","2012","","","113","118","Benchmarking of digital designs targeting FPGAs is a time intensive and challenging process. Benchmarking results depend on a myriad of variables beyond the properties inherent to the designs being evaluated, encompassing the tools, tool options, FPGA families, and languages used. In this paper we will be discussing enhancements made to the ATHENa benchmarking tool to utilize distributed computing as well as option space exploration techniques to increase the efficiency of the pre-existing process. The capabilities of our environment are demonstrated using two example designs from the SHA-3 cryptographic hash function competition, BLAKE and JH.","","978-1-4673-2845-6978-1-4673-2846-3978-1-4673-2844","10.1109/FPT.2012.6412121","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6412121","","Benchmark testing;Optimized production technology;Algorithm design and analysis;Field programmable gate arrays;Space exploration;Cryptography","benchmark testing;cryptography;distributed processing;field programmable gate arrays;logic design;public domain software","option space exploration techniques;distributed computing;FPGA cryptographic module benchmarking;field programmable gate array;digital design benchmarking;ATHENa benchmarking tool;SHA-3 cryptographic hash function competition;BLAKE competition;JH competition;open-source benchmarking environment;automated tool-for-hardware evaluation","","","10","","","","","","IEEE","IEEE Conferences"
"Avalon","H. Erckens; G. Beusser; C. Pradalier; R. Y. Siegwart","ETH Zurich; Federal Institute of Technology,Zurich; Autonomous Systems Lab at the ETH Zurich; ETH Zurich","IEEE Robotics & Automation Magazine","","2010","17","1","45","54","This article describes the design and implementation of a navigation and control system for the autonomous sailing vessel Avalon. This boat, designed for participating in the Microtransat, is engineered to autonomously cross the Atlantic Ocean. We present here a special robust mechanical design and the navigation software that will plan an optimal navigation course and efficiently control the boat on this course. The path planner uses an A* algorithm to generate the fastest path to a given destination. It is able to avoid both static and dynamic obstacles. By using a given polar diagram and measured wind data, it takes into account maneuverability constraints of a sailboat. The control system takes care of rudder and sail to follow a given heading while optimizing speed. Additionally, the system is capable of automatically conducting maneuvers such as tack and jibe. At the time of this writing, Avalon and its software systems have been successfully tested more than two weeks in short deployments lasting several hours with winds ranging from zero to 30 kn.","1070-9932;1558-223X","","10.1109/MRA.2010.935792","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5430391","","Navigation;Control systems;Boats;Design engineering;Oceans;Robust control;Optimal control;Sea measurements;Automatic control;Software systems","boats;marine engineering;mobile robots;navigation;optimisation;path planning;underwater vehicles;vehicle dynamics;velocity control","autonomous sailing vessel;Avalon;navigation system;control system;Microtransat;robust mechanical design;navigation software;optimal navigation course;boat control;path planner;obstacles avoidance;polar diagram;wind data;maneuverability constraints;rudder;speed optimization","","16","16","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic Recognition of Performance Idioms in Scientific Applications","J. He; A. E. Snavely; R. F. V. d. Wijngaart; M. A. Frumkin","NA; NA; NA; NA","2011 IEEE International Parallel & Distributed Processing Symposium","","2011","","","118","127","Basic data flow patterns that we call performance idioms, such as stream, transpose, reduction, random access and stencil, are common in scientific numerical applications. We hypothesize that a small number of idioms can cover most programming constructs that dominate the execution time of scientific codes and can be used to approximate the application performance. To check these hypotheses, we proposed an automatic idioms recognition method and implemented the method, based on the open source compiler Open64. With the NAS Parallel Benchmark (NPB) as a case study, the prototype system is about 90% accurate compared with idiom classification by a human expert. Our results showed that the above five idioms suffice to cover 100% of the six NPB codes (MG, CG, FT, BT, SP and LU). We also compared the performance of our idiom benchmarks with their corresponding instances in the NPB codes on two different platforms with different methods. The approximation accuracy is up to 96.6%. The contribution is to show that a small set of idioms can cover more complex codes, that idioms can be recognized automatically, and that suitably defined idioms may approximate application performance.","1530-2075;1530-2075","978-1-61284-372-8978-0-7695-4385","10.1109/IPDPS.2011.21","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6012830","","Benchmark testing;Liquefied natural gas;Indexes;Prototypes;Optimization;Program processors;Hardware","abstract data types;benchmark testing;natural sciences computing;program compilers;public domain software","performance idiom;scientific application;data flow pattern;scientific codes;automatic idioms recognition;open source compiler Open64;parallel benchmark;prototype system;idiom benchmarks;approximation accuracy;complex codes","","5","35","","","","","","IEEE","IEEE Conferences"
"RAT: Robust animal tracking","R. Farah; J. M. P. Langlois; G. Bilodeau","Computer and Software Engineering Department, École Polytechnique de Montréal, Quebec, Canada; Computer and Software Engineering Department, École Polytechnique de Montréal, Quebec, Canada; Computer and Software Engineering Department, École Polytechnique de Montréal, Quebec, Canada","2011 IEEE International Symposium on Robotic and Sensors Environments (ROSE)","","2011","","","65","70","Determining the motion pattern of laboratory animals is very important in order to monitor their reaction to various stimuli. In this paper, we propose a robust method to track animals, and consequently determine their motion pattern. The method is designed to work under uncontrolled normal laboratory conditions. It consists of two steps. The first step tracks the animal coarsely, using the combination of four features, while the second step refines the boundaries of the tracked area, in order to fit more precisely the boundaries of the animal. The method achieves an average tracking error smaller than 5% for our test videos.","","978-1-4577-0820-6978-1-4577-0819-0978-1-4577-0818","10.1109/ROSE.2011.6058509","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6058509","Computer vision;animal tracking;optimizing score;dimension refinement","Animals;Videos;Target tracking;Image edge detection;Reflection;Noise","image motion analysis;object tracking;video signal processing","robust animal tracking;motion pattern;RAT;laboratory animals","","5","18","","","","","","IEEE","IEEE Conferences"
"Performance Evaluation by Ant Colony Algorithm","J. Chen; J. Wu","NA; NA","2010 Second International Conference on Information Technology and Computer Science","","2010","","","138","141","Besides traditional verification of system functions, performance evaluation in modern hardware and software systems has received extensive attention. Performance evaluation aims at obtaining quantitative measures of the system to test whether reliability-related properties are warranted. In this paper, model of the system is expressed in the form of IMC, a mixed model for describing both action-based and state-based systems. And by converting performance properties of the system into aCSL formulae, we can formally verify them by model checking. To accelerate the process of IMC performance evaluation, a modified version of ant colony algorithm is proposed in the paper. Compared with other numerical methods, the ant colony algorithm reduces unnecessary model elements and the execution efficiency is largely improved to tackle more complex system performance evaluation problems.","","978-1-4244-7294-9978-1-4244-7293","10.1109/ITCS.2010.40","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5557312","performance evaluation;model checking;ant colony algorithm;interactive markov chains","Markov processes;Performance evaluation;Algorithm design and analysis;Complexity theory;Telescopes;Numerical models;Analytical models","formal verification;Markov processes;optimisation;software performance evaluation","ant colony algorithm;state-based systems;action based continuous stochastic logic;model checking;interactive Markov chain;performance evaluation","","","7","","","","","","IEEE","IEEE Conferences"
"Integrating Resource Consumption and Allocation for Infrastructure Resources on-Demand","Y. Zhang; G. Huang; X. Liu; H. Mei","NA; NA; NA; NA","2010 IEEE 3rd International Conference on Cloud Computing","","2010","","","75","82","Infrastructure resources on-demand requires resource provision (e.g., CPU and memory) to be both sufficient and necessary, which is the most important issue and a challenge in Cloud Computing. Platform as a service (PaaS) encapsulates a layer of software that includes middleware, and even development environment, and provides them as a service for building and deploying cloud applications. In PaaS, the issue of on-demand infrastructure resource management becomes more challenging due to the thousands of cloud applications that share and compete for resources simultaneously. The fundamental solution is to integrate and coordinate the resource consumption and allocation management of a cloud application. The difficulties of such a solution in PaaS are essentially how to maximize the resource utilization of an application, and how to allocate resources to guarantee adequate resource provision for the system. In this paper, we propose an approach to managing infrastructure resources in PaaS by leveraging two adaptive control loops: the resource consumption optimization loop and the resource allocation loop. The optimization loop improves the resource utilization of a cloud application via management functions provided by the corresponding middleware layers of PaaS. The allocation loop provides or reclaims appropriate amounts of resources to/from the application system while guaranteeing its performance. The two loops are integrated to run consecutively and repeatedly to provide infrastructure resources on-demand by first trying to improve resource utilization, and then allocating more resources when necessary. We implement a framework, SmartRod, to investigate our approach. The experiment on SmartRod proves its effectiveness on infrastructure resource management.","2159-6182;2159-6190","978-1-4244-8207-8978-0-7695-4130","10.1109/CLOUD.2010.11","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5558007","Resource Consumption and Allocation;Infrastructure Resources on-Demand;SmartRod;Integrated Control Loop;Middleware;Virtual Machine;Platform as a Service;PaaS","Resource management;Optimization;Clouds;Middleware;System performance;Monitoring;Benchmark testing","Internet;middleware;optimisation;resource allocation","resource consumption;resource allocation;infrastructure resources on-demand;platform-as-a-service;cloud computing;on-demand infrastructure resource management;PaaS service;optimization loop;middleware layers;SmartRod framework","","20","25","","","","","","IEEE","IEEE Conferences"
"Agent-based integration of an electric car sharing fleet into a smart distribution feeder","D. Freund; A. F. Raab; T. Küster; S. Albayrak; K. Strunz","DAI-Labor, Technische Universität Berlin, Germany; Technische Universität Berlin, Germany; DAI-Labor, Technische Universität Berlin, Germany; DAI-Labor, Technische Universität Berlin, Germany; Technische Universität Berlin, Germany","2012 3rd IEEE PES Innovative Smart Grid Technologies Europe (ISGT Europe)","","2012","","","1","8","The paper presents an agent-based scheduling and energy management system for a smart distribution feeder which is installed on a test site and includes an electric car sharing fleet. Distributed Energy storage systems provide flexibility in the operation of the test site, where the integration of multiple power sources including Renewable Energy Sources and Distributed Generators is implemented. A software agent control architecture is introduced, which is divided into distinct subsystems in order to consider market roles of the Micro Smart Grid Operator, Car Sharing Operator and Distribution System Operator. Within this architecture, an optimization mechanism with the objective to maximize the utilization of Renewable Energy Sources for charging the electric vehicles is implemented and the functionality of the agent-based system is tested in response to a basic electric vehicle booking and charging scenario.","2165-4816;2165-4816;2165-4824","978-1-4673-2597-4978-1-4673-2595-0978-1-4673-2596","10.1109/ISGTEurope.2012.6465889","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6465889","Communication system control;Control design;Multi-agent systems;Distributed information systems;Power system control;Smart grids;Electric vehicles;Battery management systems;Local activities;Optimization methods","Energy management;Computer architecture;Vehicles;Optimization;Batteries;Schedules;Electricity","distributed power generation;electric vehicles;energy management systems;multi-agent systems;power engineering computing;renewable energy sources;scheduling","agent-based integration;electric car sharing fleet;smart distribution feeder;agent-based scheduling;energy management system;test site;electric car sharing fleet;distributed energy storage systems;multiple power sources;renewable energy sources;distributed generators;microsmart grid operator;car sharing operator;distribution system operator;optimization mechanism;electric vehicle booking;charging scenario","","4","23","","","","","","IEEE","IEEE Conferences"
"Automated Assessment in a Programming Tools Course","J. L. Fernandez Aleman","Informatics and Systems Facultad de Informática Campus de Espinardo, University of Murcia, Espinardo, Spain","IEEE Transactions on Education","","2011","54","4","576","581","Automated assessment systems can be useful for both students and instructors. Ranking and immediate feedback can have a strongly positive effect on student learning. This paper presents an experience using automatic assessment in a programming tools course. The proposal aims at extending the traditional use of an online judging system with a series of assignments related to programming tools. Some empirical results on how students use an automated assessment system in a CS2 course are presented. Research suggested that automated assessment systems promoted the students' interest and produced statistically significant differences in the scores between experimental and control groups.","0018-9359;1557-9638","","10.1109/TE.2010.2098442","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5675756","Active learning;competitive learning;e-learning;programming tools;Web tool","Programming profession;Testing;Debugging;Electronic learning;Software;Runtime","computer science education;courseware;feedback;software tools","automated assessment systems;programming tools course;immediate feedback;ranking;student learning;online judging system","","25","23","","","","","","IEEE","IEEE Journals & Magazines"
"Dynamic model of a small hydropower plant","C. Jaliu; I. Visa; D. Diaconescu; R. Saulescu; M. Neagoe; O. Climescu","Transilvania University of Brasov, Romania; Transilvania University of Brasov, Romania; Transilvania University of Brasov, Romania; Transilvania University of Brasov, Romania; Transilvania University of Brasov, Romania; Transilvania University of Brasov, Romania","2010 12th International Conference on Optimization of Electrical and Electronic Equipment","","2010","","","1216","1223","The selection of the best turbine for a particular hydro site depends on the site characteristics and on the power required. For the particular values of the flow and head, the turbine tends to run most efficient at a particular speed. Often, the generator needs to be rotated at a speed greater than the turbine optimum speed. Therefore, a speed increaser is needed in many cases to link the turbine to the generator. The paper analyzes the influence of introducing a speed increaser between a Turgo turbine and the generator. The generator is tested on experimental stands in order to find the mechanical characteristics. Then, the synthesis of the chosen planetary speed increaser is performed. The paper presents the dynamic modeling of the turbine-speed increaser-generator assembly; the working point in steady state regime is established. The validation of the theoretical results using Matlab - Simulink software guarantees the good functioning of the physical prototype of small hydropower plant in certain conditions.","1842-0133;1842-0133","978-1-4244-7020-4978-1-4244-7019-8978-1-4244-7018","10.1109/OPTIM.2010.5510517","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5510517","","Hydroelectric power generation;Assembly;Rivers;Mathematical model;Testing;Software prototyping;Hydraulic turbines;Character generation;Steady-state;Power generation","hydraulic turbines;hydroelectric generators;hydroelectric power stations","hydropower plant;turbogenerator;Turgo turbine;dynamic modeling;steady state regime;Matlab;Simulink","","1","11","","","","","","IEEE","IEEE Conferences"
"IRETE: An improved RETE multi-entity match algorithm","P. Yang; Y. Yang; N. Wang","Foundation department, Jiangsu University of Science and Technology, ZhangJiaGang, China; School of Computer, Northwestern Polytechnical University, Xi'an, China; School of Computer, Northwestern Polytechnical University, Xi'an, China","2011 International Conference on Electronics, Communications and Control (ICECC)","","2011","","","4363","4366","This paper analyzes the processing characteristics and execution mechanisms of the classical RETE algorithm and points out its limitations when performing multi-entity match and multi-rule match. By employing several technologies such as nodes sharing, types preprocessing, and index-based searching optimization, we propose an improved version of the RETE algorithm, IRETE, which is tested under multi-entity and multi-rule circumstances to be a much more efficient match algorithm, at the cost of a bit of loss in network construction efficiency.","","978-1-4577-0321-8978-1-4577-0320-1978-1-4577-0319","10.1109/ICECC.2011.6067863","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6067863","mode match;RETE algorithm;rule network;entity","Algorithm design and analysis;Memory management;Engines;Business;Inference algorithms;Educational institutions;Testing","business data processing;middleware","IRETE;improved RETE;multientity match algorithm;classical RETE algorithm;multirule match;index-based searching optimization;network construction efficiency;business software information system;software middleware","","2","14","","","","","","IEEE","IEEE Conferences"
"Hardware Support for Safety-Critical Java Scope Checks","J. R. Rios; M. Schoeberl","NA; NA","2012 IEEE 15th International Symposium on Object/Component/Service-Oriented Real-Time Distributed Computing","","2012","","","31","38","Memory management in Safety-Critical Java (SCJ) is based on time bounded, non garbage collected scoped memory regions used to store temporary objects. Scoped memory regions may have different life times during the execution of a program and hence, to avoid leaving dangling pointers, it is necessary to check that reference assignments are performed only from objects in shorter lived scopes to objects in longer lived scopes (or between objects in the same scoped memory area). SCJ offers, compared to the RTSJ, a simplified memory model where only the immortal and mission memory scoped areas are shared between threads and any other scoped region is thread private. In this paper we present how, due to this simplified model, a single scope nesting level can be used to check the legality of every reference assignment. We also show that with simple hardware extensions a processor can see some improvement in terms of execution time for applications where cross-scope references are frequent. Our proposal was implemented and tested on the Java Optimized Processor (JOP).","2375-5261;1555-0885","978-0-7695-4643-8978-1-4673-0499","10.1109/ISORC.2012.13","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6195858","Certification;Safety-Critical Java;Scoped Memory;Reference Assignment Checks;Java Optimized Processor","Hardware;Arrays;Java;Software;Memory management;Object oriented modeling;Sensors","Java;safety-critical software;storage management","safety-critical Java scope checks;memory management;garbage collection;single scope nesting level;Java optimized processor","","3","14","","","","","","IEEE","IEEE Conferences"
"Fast rate distortion optimization for the emerging HEVC standard","M. B. Cassa; M. Naccari; F. Pereira","Universit&#x00E0; degli Studi di Brescia; Instituto de Telecomunica&#x00E7;&#x00F5;es; Instituto de Telecomunica&#x00E7;&#x00F5;es","2012 Picture Coding Symposium","","2012","","","493","496","The under development High Efficiency Video Coding (HEVC) standard employs several powerful coding tools to obtain improved compression efficiency regarding the state-of-the-art H.264/AVC standard. To efficiently exploit the temporal and spatial redundancies, HEVC adopts a very flexible quadtree coding structure, allowing the encoder to use a block partition better matching the image features. The best combination of HEVC block partitioning and coding modes is found by means of a rate distortion optimization process. During this minimization, the encoder tests all the possible coding modes and block partitions and keeps those providing the smallest RD cost. Due to the large number of available modes and partitions, the minimization involves extremely high complexity, which may not be suitable for real-time application scenarios. To reduce the complexity, two novel fast RDO techniques are proposed in this paper: Top Skip and Early Termination. The experimental evaluation reveals that the proposed techniques significantly reduce the encoding time, notably up to 45% regarding the high complexity HEVC reference software codec, with a negligible quality loss never larger than 0.1 dB.","","978-1-4577-2049-9978-1-4577-2047-5978-1-4577-2048","10.1109/PCS.2012.6213262","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6213262","","Encoding;Standards;Complexity theory;Codecs;Video coding;Force;Rate-distortion","data compression;optimisation;video coding","fast rate distortion optimization process;HEVC standard;high efficiency video coding standard;powerful coding tools;improved compression efficiency;H.264-AVC standard;spatial redundancies;flexible quad-tree coding structure;image features;smallest RD cost;block partitions;fast RDO techniques","","45","11","","","","","","IEEE","IEEE Conferences"
"Benchmarking of a TLM solver on mobile devices sharing a common hardware architecture","D. R. Browne; S. C. Pomeroy; J. A. Flint","Dept. of Electronic &amp; Electrical Engineering, Loughborough University Loughborough, Leicestershire, LE11 3TU, United Kingdom; Dept. of Electronic &amp; Electrical Engineering, Loughborough University Loughborough, Leicestershire, LE11 3TU, United Kingdom; Dept. of Electronic &amp; Electrical Engineering, Loughborough University Loughborough, Leicestershire, LE11 3TU, United Kingdom","2011 Loughborough Antennas & Propagation Conference","","2011","","","1","3","Modern mobile communication devices share a common set of hardware and software features. Within a brand, it is also common for similar hardware architectures to be used, and to be paired with the same operating system. Whilst mathematically trivial in nature, the Transmission Line Matrix (TLM) method is computationally-intensive, and therefore varies in performance dependant on available resources. This paper investigates how the performance of a mobile Computational Electromagnetics (mCEM) laboratory iPhone application is effected when executed on devices with different hardware configurations, but that are available from the same manufacturer. Hardware differences across the chosen platform are first discussed, with performance predictions made based on the device specifications. TLM is suitable for parallel computing implementations, and therefore potential multi-threading optimisation strategies are discussed for applicable devices. Results from the application performance benchmarking are presented, and demonstrated that the iPad 2 offered the best processing performance of the devices tested.","","978-1-4577-1016-2978-1-4577-1014-8978-1-4577-1015","10.1109/LAPC.2011.6114079","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6114079","","Performance evaluation;Random access memory;Benchmark testing;Clocks;Hardware;Instruction sets;Computer architecture","computational electromagnetics;mobile communication;multi-threading;operating systems (computers);transmission line matrix methods","TLM solver benchmarking;mobile device;hardware architecture;mobile communication device;hardware feature;software feature;operating system;transmission line matrix method;mobile computational electromagnetics laboratory iPhone application;hardware configuration;parallel computing;multithreading optimisation strategy","","","6","","","","","","IEEE","IEEE Conferences"
"Enhanced self-adaptive evolutionary algorithm for numerical optimization","Y. Xue; Y. Zhuang; T. Ni; J. Ouyang; Z. Wang","School of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing 210016, P. R. China; School of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing 210016, P. R. China; No. 723 Institute of China Shipbuilding Industry Corporation, Yangzhou 225001, P. R. China; School of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing 210016, P. R. China; Science and Technology on Electron-optic Control Laboratory, Luoyang 471000, P. R. China","Journal of Systems Engineering and Electronics","","2012","23","6","921","928","There are many population-based stochastic search algorithms for solving optimization problems. However, the universality and robustness of these algorithms are still unsatisfactory. This paper proposes an enhanced self-adaptive evolutionary algorithm (ESEA) to overcome the demerits above. In the ESEA, four evolutionary operators are designed to enhance the evolutionary structure. Besides, the ESEA employs four effective search strategies under the framework of the self-adaptive learning. Four groups of the experiments are done to find out the most suitable parameter values for the ESEA. In order to verify the performance of the proposed algorithm, 26 state-of-the-art test functions are solved by the ESEA and its competitors. The experimental results demonstrate that the universality and robustness of the ESEA outperform its competitors.","1004-4132","","10.1109/JSEE.2012.00113","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6404003","self-adaptive;numerical optimization;evolutionary algorithm;stochastic search algorithm","Sociology;Statistics;Algorithm design and analysis;Optimization;Stochastic processes;Vectors;Evolutionary computation","","","","","","","","","","","BIAI","BIAI Journals & Magazines"
"A Fuzzy Logic Approach for Scheduling Preventive Maintenance in ERP System","R. H. Fouad; M. Samhouri","NA; NA","2011 International Conference on Management and Service Science","","2011","","","1","4","Many firms have proceeded to the adoption of Enterprise Resources Planning ERP solutions to maintain competitiveness. ERP is a packaged software system that enables enterprises to integrate operations, business processes and functions through common database. However, the majority of ERP systems do not support Preventive Maintenance (PM) scheduling process. The objective of PM is to minimize equipment downtime using the limited resources of an organization. Therefore, prioritizing PM activities for equipment is essential. In this paper, a fuzzy logic-based system for PM scheduling is proposed to interpret the linguistic variables extracted from expert's knowledge for determining equipment priorities, which could be incorporated as a custom module in ERP systems. The system was tested and proved to be reliable in solving PM scheduling problem.","","978-1-4244-6581-1978-1-4244-6579-8978-1-4244-6580","10.1109/ICMSS.2011.5999330","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5999330","","Job shop scheduling;Preventive maintenance;Availability;Fuzzy logic","enterprise resource planning;fuzzy logic;fuzzy reasoning;preventive maintenance;scheduling;software packages","fuzzy logic approach;preventive maintenance scheduling;ERP system;enterprise resources planning;packaged software system;database;equipment downtime minimization;PM scheduling problem;fuzzy inference system","","","15","","","","","","IEEE","IEEE Conferences"
"A Wireless Communications Laboratory on Cellular Network Planning","Z. Dawy; A. Husseini; E. Yaacoub; L. Al-Kanj","Department of Electrical and Computer Engineering, American University of Beirut, Riad El-Solh/Beirut, Lebanon; Department of Electrical and Computer Engineering, American University of Beirut, Riad El-Solh/Beirut, Lebanon; Department of Electrical and Computer Engineering, American University of Beirut, Riad El-Solh/Beirut, Lebanon; Department of Electrical and Computer Engineering, American University of Beirut, Riad El-Solh/Beirut, Lebanon","IEEE Transactions on Education","","2010","53","4","653","661","The field of radio network planning and optimization (RNPO) is central for wireless cellular network design, deployment, and enhancement. Wireless cellular operators invest huge sums of capital on deploying, launching, and maintaining their networks in order to ensure competitive performance and high user satisfaction. This work presents a lab course composed of 10 experiments that cover the different phases of RNPO for various state-of-the-art wireless technologies such as GSM, UMTS, and WiMAX. Each experiment constitutes a complete entity including the necessary theoretical background and references, the lab tasks based on real-world scenarios, and a research component consisting of general questions. The proposed lab course represents a novel initiative to increase interactive learning by integrating communications theory fundamental knowledge with state-of-the-art wireless communications software tools and measurement equipment. The experiments are carefully designed to enhance the analytical skills and to advance the academic and practical knowledge of the students.","0018-9359;1557-9638","","10.1109/TE.2009.2039935","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5406029","Cellular network planning;drive testing;GSM;UMTS;WiMAX;wireless communications","Cellular networks;GSM;3G mobile communication;WiMAX;Wireless communication;Student experiments","3G mobile communication;cellular radio;optimisation;telecommunication network planning;WiMax","cellular network planning;wireless communications laboratory;radio network planning;optimization;wireless cellular network;user satisfaction;lab course;GSM;UMTS;WiMAX;interactive learning;software tools;measurement equipment","","8","28","","","","","","IEEE","IEEE Journals & Magazines"
"Energy Efficiency Analysis of GPUs","J. M. Cebri'n; G. D. Guerrero; J. M. García","NA; NA; NA","2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops & PhD Forum","","2012","","","1014","1022","In the last few years, Graphics Processing Units (GPUs) have become a great tool for massively parallel computing. GPUs are specifically designed for throughput and face several design challenges, specially what is known as the Power and Memory Walls. In these devices, available resources should be used to enhance performance and throughput, as the performance per watt is really high. For massively parallel applications or kernels, using the available silicon resources for power management was unproductive, as the main objective of the unit was to execute the kernel as fast as possible. However, not all the applications that are being currently ported to GPUs can make use of all the available resources, either due to data dependencies, bandwidth requirements, legacy software on new hardware, etc, reducing the performance per watt. This new scenario requires new designs and optimizations to make these GPGPU's more energy efficient. But first comes first, we should begin by analyzing the applications we are running on these processors looking for bottlenecks and opportunities to optimize for energy efficiency. In this paper we analyze some kernels taken from the CUDA SDK<sup>2</sup>in order to discover resource underutilization. Results show that this underutilization is present, and resource optimization can increase the energy efficiency of GPU-based computation. We then discuss different strategies and proposals to increase energy efficiency in future GPU designs.","","978-1-4673-0974","10.1109/IPDPSW.2012.124","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6270749","Power Dissipation;Energy Efficiency;GPU;GPGPU","Graphics processing unit;Kernel;Bandwidth;Power dissipation;Benchmark testing;Clocks;Hardware","energy conservation;graphics processing units;parallel architectures;power aware computing;resource allocation;software maintenance","energy efficiency analysis;graphics processing units;parallel computing;power;memory walls;silicon resources;power management;data dependencies;bandwidth requirements;legacy software;GPGPU;CUDA SDK2;resource underutilization discovery;resource optimization;GPU-based computation","","10","25","","","","","","IEEE","IEEE Conferences"
"Intelligent system for freeway ramp metering control","K. Veljanovska; Z. Gacovski; S. Deskovski","Faculty of Administration and Information Systems Management, University St Kliment Ohridski, Bitola, Macedonia; Faculty for ICT FON University, Skopje, Macedonia; Technical Faculty - Bitola, University St Kliment Ohridski, Bitola, Macedonia","2012 6th IEEE International Conference Intelligent Systems","","2012","","","279","282","The number of techniques implemented in subsystems of the Intelligent System of infrastructure in transportation system in terms of agents for signal control, ramp metering, detecting incidents is numerous. Challenges, however, are still there for the researchers to optimize traffic operations. The aim of this paper is to prove the ability of artificial intelligence technique known as reinforcement learning implemented in intelligent system for freeway control. Intelligent agents are implemented as controllers in order to provide optimal performance on the freeway corridor via ramp metering control on a corridor. The algorithm used in the research was Q learning algorithm. The results are promising proving that the technique is capable for optimal control of entrance freeway ramps and suitable for building the intelligent system of the freeway.","1541-1672;1941-1294","978-1-4673-2278-2978-1-4673-2276-8978-1-4673-2277","10.1109/IS.2012.6335230","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6335230","Artificial intelligence;reinforcement learning;intelligent control","Traffic control;Vehicles;Learning;Testing;Intelligent systems","automated highways;learning (artificial intelligence);optimal control;optimisation;road traffic control;software agents","intelligent system;freeway ramp metering control;transportation system;signal control;traffic operations optimization;artificial intelligence technique;reinforcement learning;intelligent agents;optimal performance;freeway corridor;Q learning algorithm;optimal control;entrance freeway ramps","","","18","","","","","","IEEE","IEEE Conferences"
"Performance modeling for systematic performance tuning","T. Hoefler; W. Gropp; W. Kramer; M. Snir","National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, 1205 W. Clark Street, Urbana, IL; National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, 1205 W. Clark Street, Urbana, IL; National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, 1205 W. Clark Street, Urbana, IL; National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, 1205 W. Clark Street, Urbana, IL","SC '11: Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis","","2011","","","1","12","The performance of parallel scientific applications depends on many factors which are determined by the execution environment and the parallel application. Especially on large parallel systems, it is too expensive to explore the solution space with series of experiments. Deriving analytical models for applications and platforms allow estimating and extrapolating their execution performance, bottlenecks, and the potential impact of optimization options. We propose to use such ""performance modeling"" techniques beginning from the application design process throughout the whole software development cycle and also during the lifetime of supercomputer systems. Such models help to guide supercomputer system design and re-engineering efforts to adopt applications to changing platforms and allow users to estimate costs to solve a particular problem. Models can often be built with the help of well-known performance profiling tools. We discuss how we successfully used modeling throughout the proposal, initial testing, and beginning deployment phase of the Blue Waters supercomputer system.","2167-4329;2167-4337","978-1-4503-0771-0978-1-4503-0771","10.1145/2063348.2063356","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6114475","","Analytical models;Computational modeling;Optimization;Runtime;Predictive models;Bandwidth;Computer architecture","natural sciences computing;parallel processing;software performance evaluation;systems re-engineering","systematic performance tuning;parallel scientific application;execution environment;parallel application;execution performance estimation;bottleneck estimation;optimization options;performance modeling technique;application design process;software development cycle;supercomputer system lifetime;supercomputer system design;re-engineering efforts;cost estimation;performance profiling tools;Blue Waters supercomputer system","","26","34","","","","","","IEEE","IEEE Conferences"
"ReClick - A Modular Dataplane Design Framework for FPGA-Based Network Virtualization","D. Unnikrishnan; J. Lu; L. Gao; R. Tessier","NA; NA; NA; NA","2011 ACM/IEEE Seventh Symposium on Architectures for Networking and Communications Systems","","2011","","","145","155","Network virtualization has emerged as a powerful technique to deploy novel services and experimental protocols over shared network infrastructures. Although recent research has highlighted field programmable gate arrays (FPGAs) as attractive platforms for high performance network virtualization, these devices remain inaccessible to the larger networking research community due to the absence of user-friendly programming models. A programming model that can abstract the intricacies of the hardware platform while being aware of the underlying resource constraints is highly desirable. In this paper, we present ReClick, a framework to efficiently design and deploy reconfigurable data planes for FPGA-based network virtualization systems. A hardware-agnostic programming model is described that allows developers to focus on the virtual data plane semantics rather than the implementation details. The framework exposes interfaces similar to the popular software router development framework, Click, and promotes design reuse. Optimization strategies are included in ReClick which use similarities between virtual data plane configurations to implement multiple planes in an area-efficient manner. Data planes exhibiting up to 1 Gbps data rate have been automatically compiled and tested in hardware in a Net FPGA platform.","","978-1-4577-1454-2978-0-7695-4521","10.1109/ANCS.2011.31","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062727","Network Virtualization;FPGA;Programming models;Click modular router;NetFPGA","Hardware;Field programmable gate arrays;Software;Programming;Hardware design languages;Computer architecture;Pipelines","computer networks;field programmable gate arrays;software reusability;virtual machines;virtualisation","ReClick;modular dataplane design framework;shared network infrastructures;field programmable gate arrays;high performance network virtualization;user-friendly programming models;reconfigurable data plane design;reconfigurable data plane deployment;FPGA-based network virtualization systems;hardware-agnostic programming model;virtual data plane semantics;Click software router development framework;software reuse;Net FPGA platform","","3","28","","","","","","IEEE","IEEE Conferences"
"A Method for Generating Models of Black-Box Components","L. B. Cuong; P. N. Hung","NA; NA","2012 Fourth International Conference on Knowledge and Systems Engineering","","2012","","","217","222","The model-based approaches are difficult to be applied in practice due to the lack of formal models describing behaviors of systems. This paper proposes a method for generating accurate models of components in order to solve this issue in the context of the component-based systems. The key idea of this method is to generate minimal deterministic finite automata as the accurate models of the corresponding components. For this purpose, the proposed method first computes a set of traces as a regular language of a given component by executing all possible experiments over the alphabet of the component. This method then constructs a regular expression to represent this set. After that, a minimal deterministic finite automaton as an accurate model of the component is generated by applying the Thompson algorithm and some optimized activities. The generated models are useful for the existing model-based approaches, e.g., model checking and model-based testing in improving quality of component-based software. An implemented tool supporting the method and experimental results are also presented.","","978-0-7695-4760-2978-1-4673-2171","10.1109/KSE.2012.15","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6299422","model generation;black-box components;Thompson algorithm;model-based approaches","Unified modeling language;Doped fiber amplifiers;Software;Automata;Modeling;Adaptation models","deterministic automata;finite automata;formal verification;object-oriented programming;software quality","model generation;black-box components;formal models;component-based systems;component alphabet;minimal deterministic finite automaton;Thompson algorithm;model-based approaches;model checking;model-based testing;component-based software quality","","","17","","","","","","IEEE","IEEE Conferences"
"A Flexible Approach to Improving System Reliability with Virtual Lockstep","C. M. Jeffery; R. J. O. Figueiredo","University of Florida, Gainesville; University of Florida, Gainesville","IEEE Transactions on Dependable and Secure Computing","","2012","9","1","2","15","There is an increasing need for fault tolerance capabilities in logic devices brought about by the scaling of transistors to ever smaller geometries. This paper presents a hypervisor-based replication approach that can be applied to commodity hardware to allow for virtually lockstepped execution. It offers many of the benefits of hardware-based lockstep while being cheaper and easier to implement and more flexible in the configurations supported. A novel form of processor state fingerprinting is also presented, which can significantly reduce the fault detection latency. This further improves reliability by triggering rollback recovery before errors are recorded to a checkpoint. The mechanisms are validated using a full prototype and the benchmarks considered indicate an average performance overhead of approximately 14 percent with the possibility for significant optimization. Finally, a unique method of using virtual lockstep for fault injection testing is presented and used to show that significant detection latency reduction is achievable by comparing only a small amount of data across replicas.","1545-5971;1941-0018;2160-9209","","10.1109/TDSC.2010.53","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5590258","Virtualization;fault injection;dependable architectures;software reliability;autonomic computing.","Hardware;Fingerprint recognition;Fault detection;Prototypes;Reliability engineering","electronic engineering computing;fault diagnosis;fault tolerance;integrated circuit reliability;logic circuits;optimisation;transistors","system reliability;fault tolerance capabilities;logic devices;transistors scaling;hypervisor based replication approach;virtually lockstepped execution;processor state fingerprinting;fault detection latency;rollback recovery;optimization;fault injection testing;detection latency reduction","","13","60","","","","","","IEEE","IEEE Journals & Magazines"
"The UAE wind turbine performance prediction using wind tunnel airfoil data","C. Zhang; J. Hu","College of Energy and Power, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Energy and Power, Nanjing University of Aeronautics and Astronautics, Nanjing, China","2010 World Non-Grid-Connected Wind Power and Energy Conference","","2010","","","1","5","It's vitally important for wind turbine aerodynamic performance predictions to design and optimize wind turbines. In view of the BEM method's fast speed and widespread use, this paper applies WT_Perf software of NREL to predict the performance on a NREL Phase VI upwind wind turbine. Measurements from the Phase VI of the NREL/NASA Ames wind tunnel test were used for validating the predictions. Performance predictions will vary greatly when data selection changes. Also three-dimensional rotational stall delay should be accounted for in the predictions. The final obtained prediction methods show good agreement with the experiments. Because of their accuracy and straightforward manufacturing, derived methods can be widely used in wind turbine optimization and design of the future research.","2162-1055;2162-1063","978-1-4244-8921-3978-1-4244-8920","10.1109/WNWEC.2010.5673181","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5673181","BEM (Blade Element Momentum) method;WT_PERF;NREL Phase VI wind turbine rotor;stall delay","Wind turbines;Automotive components;Delay;Rotors;Blades;Wind speed;Mathematical model","aerodynamics;power engineering computing;wind turbines","UAE wind turbine aerodynamic performance prediction;wind tunnel airfoil data;BEM method;WT_Perf software;NREL Phase VI upwind wind turbine;NREL-NASA Ames wind tunnel test;three-dimensional rotational stall delay;wind turbine optimization;blade element momentum method","","6","12","","","","","","IEEE","IEEE Conferences"
"Combination of adaptive and intelligent load shedding techniques for distribution network","M. Karimi; H. Mokhlis; A. H. A. Bakar; J. A. Laghari; A. Shahriari; M. M. Aman","Department of Electrical Engineering, Faculty of Engineering, University of Malaya, Malaysia; Department of Electrical Engineering, University of Malaya, Malaysia; Department of Electrical Engineering, University of Malaya, Malaysia; Department of Electrical Engineering, Faculty of Engineering, University of Malaya, Malaysia; Department of Electrical Engineering, Faculty of Engineering, University of Malaya, Malaysia; Department of Electrical Engineering, Faculty of Engineering, University of Malaya, Malaysia","2012 IEEE International Power Engineering and Optimization Conference Melaka, Malaysia","","2012","","","57","61","One of the challenge in islanding operation is to maintain frequency stability when generation is less than demand. This paper presents a new Under-Frequency Load shedding scheme for an islanded distribution network. The scheme is based on an adaptive and intelligent load shedding techniques. This proposed scheme is able to conserve power system collapse even for large disturbance and events in the system. The proposed scheme is evaluated through simulation in PSCAD/EMTDC software. A distribution network connected with mini-hydro generation in Malaysia is chosen for the test. Various test scenarios show the effectiveness of the proposed load shedding scheme to shed optimum load.","","978-1-4673-0662-1978-1-4673-0660-7978-1-4673-0661","10.1109/PEOCO.2012.6230835","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6230835","Mini Hydro;Distributed Management System;Under-Frequency Load Shedding;Distributed Generation;Islanding","Generators;Power system stability;Educational institutions;Frequency response;Electrical engineering;Conferences;Adaptive systems","distributed power generation;distribution networks;frequency stability;hydroelectric power stations;load shedding","intelligent load shedding techniques;adaptive techniques;distribution network;islanding operation;frequency stability;under-frequency load shedding scheme;islanded distribution network;power system collapse;PSCAD/EMTDC software;minihydro generation;Malaysia;test scenarios","","1","15","","","","","","IEEE","IEEE Conferences"
"A ventricular assist device designed by use of Computational Fluid Dynamics","C. Lin; G. Wu; L. Shu; Y. Wang; W. Wu","Biomedical Engineering Department, Beijing Anzhen Hospital Affiliated to Capital University of Medical Sciences, Beijing 100029, China; Biomedical Engineering Department, Beijing Anzhen Hospital Affiliated to Capital University of Medical Sciences, Beijing 100029, China; Biomedical Engineering Department, Beijing Anzhen Hospital Affiliated to Capital University of Medical Sciences, Beijing 100029, China; Biomedical Engineering Department, Beijing Anzhen Hospital Affiliated to Capital University of Medical Sciences, Beijing 100029, China; Biomedical Engineering Department, Beijing Anzhen Hospital Affiliated to Capital University of Medical Sciences, Beijing 100029, China","2011 IEEE 3rd International Conference on Communication Software and Networks","","2011","","","288","292","Aims: the purpose of this study was to develop a new left ventricular assist device which causes less blood damage. Methods: During design of the device, the method of Computational Fluid Dynamics (CFD) was used to optimize the device shape, number of impeller vanes, the structure and position of the guide vanes, and the size of the impeller screw-pitch. After that the device, that was an axial flow blood pump, was produced without an absolute brushless DC motor and was constructed with a housing, an impeller with two blades, three guide vanes, ceramic bearings, and an inlet and outlet. We measured the dynamic and hemolysis performances and the surface temperature in an in vitro experiment. We also detected the trauma inflicted by this pump on the blood by testing to failure on a goat heart. Results: the experimental tests indicated that the dynamic performance of the axial flow pump included a nearly 6 L/min flow rate against a pressure of 100 mmHg with rotation at 10000 rpm, that output could satisfy demand during use as a left ventricular assistant device (LVAD). The normalized index of hemolysis (NIH) values from in vitro and in vivo experiments were 0.047±0.017 mg/ 100 mL and 0.016±0.011mg/100 mL, respectively. The results showed that the blood pump designed using CFD showed far less damage to blood than the previous pumps and also showed improved durability as tested in vivo in animals.","","978-1-61284-486-2978-1-61284-485-5978-1-61284-484","10.1109/ICCSN.2011.6014898","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014898","computational fluid dynamics;ventricular asssist device;dynamic performance;blood damage","Pumps;Monitoring;Biomedical monitoring;Atmospheric measurements;Blood;Particle measurements","biomedical equipment;blood;cardiology;cellular biophysics;computational fluid dynamics;ducts;haemodynamics;haemorheology;injuries;optimisation;pumps","computational fluid dynamics;left ventricular assist device;blood damage reduction;CFD;optimization;impeller vanes;guide vanes;impeller screw-pitch;axial flow blood pump;ceramic bearings;hemolysis;trauma;goat heart","","","16","","","","","","IEEE","IEEE Conferences"
"Highly scalable distributed dataflow analysis","J. L. Greathouse; C. LeBlanc; T. Austin; V. Bertacco","Advanced Computer Architecture Laboratory, University of Michigan, Ann Arbor; Advanced Computer Architecture Laboratory, University of Michigan, Ann Arbor; Advanced Computer Architecture Laboratory, University of Michigan, Ann Arbor; Advanced Computer Architecture Laboratory, University of Michigan, Ann Arbor","International Symposium on Code Generation and Optimization (CGO 2011)","","2011","","","277","288","Dynamic dataflow analyses find software errors by tracking meta-values associated with a program's runtime data. Despite their benefits, the orders-of-magnitude slowdowns that accompany these systems limit their use to the development stage; few users would tolerate such overheads. This work extends dynamic dataflow analyses with a novel sampling system which ensures that runtime slowdowns do not exceed a user-defined threshold. While previous sampling methods are inadequate for dataflow analyses, our technique efficiently reduces the number and size of analyzed dataflows. In doing so, it allows individual users to test large, stochastically chosen sets of a process's dataflows. Large populations can therefore, in aggregate, analyze a larger portion of the program than is possible by any single user running a complete, but slow, analysis. In our experimental evaluation, we show that 1 out of every 10 users expose a number of security exploits while only experiencing a 10% performance slowdown, in contrast with the 100× overhead caused by a complete analysis that exposes the same problems.","","978-1-61284-357-5978-1-61284-356-8978-1-61284-358","10.1109/CGO.2011.5764695","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5764695","","Software;Runtime;Security;Performance analysis;Virtual machine monitors;Prototypes;Dynamic scheduling","data flow analysis;distributed processing;sampling methods","scalable distributed dataflow analysis;software error;program runtime data;sampling system;user defined threshold","","3","33","","","","","","IEEE","IEEE Conferences"
"Retiming multi-rate DSP algorithms to meet real-time requirement","X. Zhu","State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, China, Beijing 100190","2010 Design, Automation & Test in Europe Conference & Exhibition (DATE 2010)","","2010","","","1785","1790","Multi-rate digital signal processing(DSP) algorithms are usually modeled by synchronous dataflow graphs(SDFGs). Performing with high enough throughput is a key real-time requirement of a DSP algorithm. Therefore how to decrease the iteration period of an SDFG to meet the real-time requirement of the system under consideration is a very important problem. Retiming is a prominent graph transformation technique for performance optimizing. In this paper, by proving some useful properties about the relationship between an SDFG and its equivalent homogeneous SDFG(HSDFG), we present an efficient retiming algorithm, which needn't convert the SDFG to HSDFG, for finding a feasible retiming to reduce the iteration period of an SDFG as required.","1558-1101;1530-1591;1530-1591","978-3-9810801-6-2978-1-4244-7054-9978-3-9810801-6","10.1109/DATE.2010.5457103","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5457103","","Digital signal processing;Signal processing algorithms;Iterative algorithms;Throughput;Software algorithms;Signal processing;Real time systems;Laboratories;Computer science;Computational modeling","digital signal processing chips","multi-rate digital signal processing algorithms;synchronous dataflow graphs;graph transformation technique","","3","12","","","","","","IEEE","IEEE Conferences"
"A generative solution for ATM cash management","R. Armenise; C. Birtolo; E. Sangianantoni; L. Troiano","Poste Italiane S.p.A. - TI - SSI - Centro Ricerca e Sviluppo 80133 Napoli, Italy; Poste Italiane S.p.A. - TI - SSI - Centro Ricerca e Sviluppo 80133 Napoli, Italy; Poste Italiane S.p.A. - TI - SSI - Centro Ricerca e Sviluppo 80133 Napoli, Italy; University of Sannio - Department of Engineering 82100 Benevento, Italy","2010 International Conference of Soft Computing and Pattern Recognition","","2010","","","349","356","Optimizing cash in automatic teller machines (ATM) is challenging due unpredictability of withdrawals, but profitable because of the large number of tellers. Generally ATM cash management and optimization is performed manually, according to corporate policies and personnel experience. A non-optimal cash upload can lead to poor service when cash demand is underestimated and to unnecessary costs when demand is overestimated. Therefore, finding the best match between cash stock and demand becomes crucial to improve. Recently, some authors attempted to optimize the cash by modeling and forecasting the demand. However, the high variance and non-stationarity of the underlying stochastic process can affect reliability of such an approach. In this paper we suggest the application of genetic algorithms as means for searching and generating optimal upload strategies, able at the same time to minimize the daily amount of stocked money and to assure cash dispensing service. Experimentation led at Poste Italiane S.p.A. makes this promising and worth to be further investigated.","","978-1-4244-7896-5978-1-4244-7897-2978-1-4244-7895","10.1109/SOCPAR.2010.5686730","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5686730","","Asynchronous transfer mode;Artificial neural networks;Biological cells;Testing;Forecasting;Software;Genetic algorithms","automatic teller machines;optimisation;stochastic processes","ATM cash management;cash optimization;demand forecasting;stochastic process","","2","13","","","","","","IEEE","IEEE Conferences"
"Mechanical energy optimization in trajectory planning for six DOF robot manipulators based on eighth-degree polynomial functions and a genetic algorithm","W. Pérez Bailón; E. Barrera Cardiel; I. Juárez Campos; A. Ramos Paz","School of Electrical Engineering, Universidad Michoacana de San Nicolás de Hidalgo, Morelia, Michoacán, México; School of Electrical Engineering, Universidad Michoacana de San Nicolás de Hidalgo, Morelia, Michoacán, México; School of Mechanical Engineering, Universidad Michoacana de San Nicolás de Hidalgo, Morelia, Michoacán, México; School of Electrical Engineering, Universidad Michoacana de San Nicolás de Hidalgo, Morelia, Michoacán, México","2010 7th International Conference on Electrical Engineering Computing Science and Automatic Control","","2010","","","446","451","Optimal trajectory planning for robot manipulators is a very important issue in the research field of robotics. Many applications require smooth trajectories and the minimization of a performance index, usually the traveling time or the mechanical energy of the actuators. This paper presents a novel method that uses eighth-degree polynomial functions to generate smooth trajectories for the parametric representation of a given path. The optimization algorithm presented in this paper minimizes the mechanical energy consumed in the robot manipulator. To solve the optimization model, a genetic algorithm is implemented. A software platform has been developed to test this optimal trajectory-planning algorithm. The software includes modules to solve the direct kinematics, the inverse kinematics, and the dynamics of the robot manipulator.","","978-1-4244-7314-4978-1-4244-7312-0978-1-4244-7314","10.1109/ICEEE.2010.5608583","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5608583","robot manipulator;trajectory planning;eighth-degree polynomials;genetic algorithm","Trajectory;Polynomials;Joints;Planning;Manipulator dynamics","genetic algorithms;manipulator dynamics;manipulator kinematics;minimisation;path planning;polynomials;position control","mechanical energy optimization;six DOF robot manipulator dynamics;eighth-degree polynomial functions;genetic algorithm;optimal trajectory planning algorithm;smooth trajectories generation;path parametric representation;optimization model;direct kinematics;inverse kinematics","","","8","","","","","","IEEE","IEEE Conferences"
"A method for fast jitter tolerance analysis of high-speed PLLs","S. Erb; W. Pribyl","Institute of Electronics - Graz University of Technology, Inffeldgasse 12/1, 8010 Graz, Austria; Institute of Electronics - Graz University of Technology, Inffeldgasse 12/1, 8010 Graz, Austria","2011 Design, Automation & Test in Europe","","2011","","","1","6","We propose a fast method for identifying the jitter tolerance curves of high-speed phase locked loops. The method is based on an adaptive recursion and uses known tail fitting methods to realize a fast optimization combined with a small number of jitter samples. It allows for efficient behavioral simulations, and can also be applied to hardware measurements. A typical modeling example demonstrates applicability to both software and hardware scenarios and achieves simulated measurement times in the range of few hundred milliseconds.","1558-1101;1530-1591;1530-1591","978-3-9810801-8-6978-1-61284-208-0978-3-9810801-7","10.1109/DATE.2011.5763182","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5763182","","Jitter;Phase locked loops;Bit error rate;Extrapolation;Hardware;Polynomials;Voltage-controlled oscillators","jitter;optimisation;phase locked loops","fast jitter tolerance analysis;high-speed PLL;jitter tolerance curves;high-speed phase locked loops;adaptive recursion;tail fitting;fast optimization","","","13","","","","","","IEEE","IEEE Conferences"
"Design techniques for cross-layer resilience","N. P. Carter; H. Naeimi; D. S. Gardner","Intel Corporation, 2200 Mission College Blvd., RNB6-61, Santa Clara, California 95054, USA; Intel Corporation, 2200 Mission College Blvd., RNB6-61, Santa Clara, California 95054, USA; Intel Corporation, 2200 Mission College Blvd., RNB6-61, Santa Clara, California 95054, USA","2010 Design, Automation & Test in Europe Conference & Exhibition (DATE 2010)","","2010","","","1023","1028","Current electronic systems implement reliability using only a few layers of the system stack, which simplifies the design of other layers but is becoming increasingly expensive over time. In contrast, cross-layer resilient systems, which distribute the responsibility for tolerating errors, device variation, and aging across the system stack, have the potential to provide the resilience required to implement reliable, high-performance, low-power systems in future fabrication processes at significantly lower cost. These systems can implement less-frequent resilience tasks in software to save power and chip area, can tune their reliability guarantees to the needs of applications, and can use the information available at each level in the system stack to optimize performance and power consumption. In this paper, we outline an approach to cross-layer system design that describes resilience as a set of tasks that systems must perform in order to detect and tolerate errors and variation. We then present strawman examples of how this task-based design process could be used to implement general-purpose computing and SoC systems, drawing on previous work and identifying key areas for future research.","1558-1101;1530-1591;1530-1591","978-3-9810801-6-2978-1-4244-7054-9978-3-9810801-6","10.1109/DATE.2010.5456960","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5456960","","Resilience;Power system reliability;Fabrication;Costs;Circuits;Error correction codes;Aging;Energy consumption;Computer architecture;Error correction","general purpose computers;logic design;optimisation;power consumption;reliability;system-on-chip","cross layer resilience design techniques;reliability;power consumption;performance optimize;strawman examples;general purpose computing;SoC systems","","31","51","","","","","","IEEE","IEEE Conferences"
"Keynote: Software engineering for a smarter planet situation-aware applications","","","2012 19th Working Conference on Reverse Engineering","","2012","","","xvii","xvii","Summary form only given, as follows. The growing popularity of smart devices and applications is accelerating the convergence of the physical and the digital worlds. Smart apps allow users, with the help of sensors and networks, to do a great variety of things, from tracking their friends to controlling remote devices. At the core of such smart systems are self-adaptive systems. Self-adaptive systems optimize their own behaviour according to high-level objectives and constraints to address changes in functional and nonfunctional requirements as well as environmental conditions. Self-adaptive systems are implemented using four key technologies: runtime models, context management, feedback control theory, and run-time verification and validation. The proliferation of smart applications challenges the software engineering community in re-thinking the boundary between development time and run time and developing techniques for adapting systems at run time. The key challenge is to automate traditional software engineering, maintenance and evolution techniques to adapt and evolve systems at run time with minimal or no human interference. Hitherto, most developers did not instrument their software with sensors and effectors to observe whether requirements are satisfied in an evolving environment at run time. One way to break out of this mold is to make the four key technologies readily accessible at run time. Challenges for the reverse engineering community are to instrument existing systems with manageability endpoints (i.e., sensors and effectors), extract design time models that are useful as run-time models (e.g., to manage dynamic context), and investigate analysis techniques for run-time testing and validation (e.g., to regulate requirements).","2375-5369;1095-1350","978-0-7695-4891-3978-1-4673-4536","10.1109/WCRE.2012.10","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6385090","Security;Reverse Engineering","","","","","","","","","","","","IEEE","IEEE Conferences"
"Perceptual Rate-Distortion Optimization Using Structural Similarity Index as Quality Metric","Y. Huang; T. Ou; P. Su; H. H. Chen","MediaTek, Hsinchu, Taiwan; Graduate Institute of Communication Engineering, National Taiwan University, Taipei, Taiwan; Graduate Institute of Communication Engineering, National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering, the Graduate Institute of Communication Engineering, and the Graduate Institute of Networking and Multimedia, National Taiwan University, Taipei, Taiwan","IEEE Transactions on Circuits and Systems for Video Technology","","2010","20","11","1614","1624","The rate-distortion optimization (RDO) framework for video coding achieves a tradeoff between bit-rate and quality. However, objective distortion metrics such as mean squared error traditionally used in this framework are poorly correlated with perceptual quality. We address this issue by proposing an approach that incorporates the structural similarity index as a quality metric into the framework. In particular, we develop a predictive Lagrange multiplier estimation method to resolve the chicken and egg dilemma of perceptual-based RDO and apply it to H.264 intra and inter mode decision. Given a perceptual quality level, the resulting video encoder achieves on the average 9% bit-rate reduction for intra-frame coding and 11% for inter-frame coding over the JM reference software. Subjective test further confirms that, at the same bit-rate, the proposed perceptual RDO indeed preserves image details and prevents block artifact better than traditional RDO.","1051-8215;1558-2205","","10.1109/TCSVT.2010.2087472","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5604279","H.264;lagrange multiplier;perceptual quality;rate-distortion optimization;structural similarity index;video codec","Indexes;Measurement;Encoding;Estimation;Approximation methods;Video coding;Fitting","mean square error methods;rate distortion theory;video coding","perceptual rate-distortion optimization;structural similarity index;quality metric;RDO framework;video coding;objective distortion metrics;mean squared error;predictive Lagrange multiplier estimation;chicken and egg dilemma;H.264 intra mode decision;H.264 inter mode decision;video encoder;bit-rate reduction;intraframe coding;interframe coding","","81","28","","","","","","IEEE","IEEE Journals & Magazines"
"A Real-Time FPGA-Based 20 000-Word Speech Recognizer With Optimized DRAM Access","Y. Choi; K. You; J. Choi; W. Sung","LG Electronics, Seoul, Korea; Department of Electrical Engineering, Seoul National University, Seoul, Korea; Department of Electrical Engineering, Seoul National University, Seoul, Korea; Department of Electrical Engineering, Seoul National University, Seoul, Korea","IEEE Transactions on Circuits and Systems I: Regular Papers","","2010","57","8","2119","2131","A real-time hardware-based large vocabulary speech recognizer requires high memory bandwidth. We have developed a field-programmable-gate-array (FPGA)-based 20 000-word speech recognizer utilizing efficient dynamic random access memory (DRAM) access. This system contains all the functional blocks for hidden-Markov-model-based speaker-independent continuous speech recognition: feature extraction, emission probability computation, and intraword and interword Viterbi beam search. The feature extraction is conducted in software on a soft-core-based CPU, while the other functional units are implemented using parallel and pipelined hardware blocks. In order to reduce the number of memory access operations, we used several techniques such as bitwidth reduction of the Gaussian parameters, multiframe computation of the emission probability, and two-stage language model pruning. We also employ a customized DRAM controller that supports various access patterns optimized for each functional unit of the speech recognizer. The speech recognition hardware was synthesized for the Virtex-4 FPGA, and it operates at 100 MHz. The experimental result on Nov 92 20 k test set shows that the developed system runs 1.52 and 1.39 times faster than real time using the bigram and trigram language models, respectively.","1549-8328;1558-0806","","10.1109/TCSI.2010.2041501","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5418887","Field-programmable gate array (FPGA) implementation;large vocabulary continuous speech recognition (LVCSR);memory access optimization;speech recognition","Speech recognition;Random access memory;Feature extraction;Hardware;Vocabulary;Bandwidth;DRAM chips;Viterbi algorithm;Pattern recognition;Control system synthesis","feature extraction;field programmable gate arrays;random-access storage;speech recognition","real-time FPGA-based speech recognizer;optimized DRAM access;field-programmable-gate-array;dynamic random access memory;feature extraction;emission probability computation;intraword Viterbi beam search;interword Viterbi beam search;bitwidth reduction;Gaussian parameters;multiframe computation;two-stage language model pruning","","19","22","","","","","","IEEE","IEEE Journals & Magazines"
"The case for hardware transactional memory in software packet processing","M. Labrecque; J. G. Steffan","Department of Electrical and Computer Engineering, University of Toronto, Canada; Department of Electrical and Computer Engineering, University of Toronto, Canada","2010 ACM/IEEE Symposium on Architectures for Networking and Communications Systems (ANCS)","","2010","","","1","11","Software packet processing is becoming more important to enable differentiated and rapidly-evolving network services. With increasing numbers of programmable processor and accelerator cores per network node, it is a challenge to support sharing and synchronization across them in a way that is scalable and easy-to-program. In this paper, we focus on parallel/threaded applications that have irregular control-flow and frequently-updated shared state that must be synchronized across threads. However, conventional lock-based synchronization is both difficult to use and also often results in frequent conservative serialization of critical sections. Alternatively, we propose that Transactional memory (TM) is a good match to software packet processing: it both (i) can allow the system to optimistically exploit parallelism between the processing of packets whenever it is safe to do so, and (ii) is easy-to-use for a programmer. With the NetFPGA platform and four network packet processing applications that are threaded and share memory, we evaluate hardware support for TM (HTM) using the reconfigurable FPGA fabric. Relative to NetThreads, our two-processor four-way-multithreaded system with conventional lock-based synchronization, we find that adding HTM achieves 6%, 54% and 57% increases in packet throughput for three of four packet processing applications studied, due to reduced conservative serialization.","","978-1-4244-9127-8978-1-4503-0379","10.1145/1872007.1872053","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5623817","Algorithms;Design;Performance","Synchronization;Pipeline processing;Benchmark testing;Instruction sets;Process control","concurrency control;field programmable gate arrays;multi-threading;shared memory systems;synchronisation","hardware transactional memory;software packet processing;network services;synchronization;parallel-threaded applications;NetFPGA platform;network packet processing applications;NetThreads","","2","49","","","","","","IEEE","IEEE Conferences"
"Multi-ring compound fiber optic rotation measuring approach based on the Rayleigh Backscattering","Liu Yanlei; Jiang Haili; Yuan Libo","College of Science, Harbin Engineering University, China 150001; College of Science, Harbin Engineering University, China 150001; College of Science, Harbin Engineering University, China 150001","Proceedings of 2012 International Conference on Measurement, Information and Control","","2012","1","","107","112","A new structure of multi-ring compound fiber optic rotation velocity measurement approach was researched, in this paper, based on the single ring Rayleigh backscattering type fiber optic rotation sensing principle. N fiber optic ring cavities and N 2×2 single-mode optical fiber coupler were used in this new structure to build a model for measuring changes of scattering. The output signal of optical rotation sensor detected by optical time domain reflectometer was deduced and analyzed in digitization by MA TLAB software. And then the structure parameters of the multi-ring cavity lengths and coupling ratios were optimized according to the characteristic of output signal. When the system was constructed based on the optimization simulation results in the same condition that parameters setting were N=3, range of rotate speed measure 0-0.18rad/s and measurement accuracy 0.01rad/s, the feasibility of the system was verified by measuring the Rayleigh backscattering signal in different speed. The use of compound multiple ring cavities, compared with single ring, improved the measuring signal strength, increased the effective test data, which is helpful to identify.","","978-1-4577-1604-1978-1-4577-1601-0978-1-4577-1603","10.1109/MIC.2012.6273310","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6273310","fiber optic rotation measurement;Rayleigh backscattering;multi-ring resonators;optical time domain reflectometer","Optical variables measurement;Optical coupling;Optical reflection;Optical sensors;Cavity resonators","computerised instrumentation;fibre optic sensors;mathematics computing;optical fibre couplers;optimisation;Rayleigh scattering;time-domain reflectometry;velocity measurement","multiring compound fiber optic rotation measuring approach;multiring compound fiber optic rotation velocity measurement approach;Rayleigh backscattering type fiber optic rotation sensing principle;fiber optic ring cavities;single-mode optical fiber coupler;optical time domain reflectometer;Matlab software;multiring cavity lengths;multiring coupling ratios;output signal characteristic;optimization simulation;measurement accuracy;Rayleigh backscattering signal;signal strength measurement;effective test data","","","13","","","","","","IEEE","IEEE Conferences"
"A new tracking method of symmetrical fault during Power Swing Based on S-Transform","Nor Zulaily Mohamad; A. Farid Abidin; Wan Noraishah Wan Abdul Munim","Centre for Electrical Power Engineering Studies, (CEPES), Universiti Teknologi MARA, Shah Alam, MALAYSIA; Centre for Electrical Power Engineering Studies, (CEPES), Universiti Teknologi MARA, Shah Alam, MALAYSIA; Centre for Electrical Power Engineering Studies, (CEPES), Universiti Teknologi MARA, Shah Alam, MALAYSIA","2012 IEEE International Power Engineering and Optimization Conference Melaka, Malaysia","","2012","","","141","146","Current distance relay is accommodated with Power Swing Blocking (PSB) scheme. However, this blocking scheme prove to vulnerable to distance relay operation as it could block the trip signals if the symmetrical fault occur during power swing. Hence, it is important to develop the proper fault detection scheme during power swing to avoid such undesirable circumstances. This paper presents a new detection technique to detect symmetrical fault during power swing by using S-Transform analysis based on the current, voltage, and three-phase active and reactive power signals waveform. To evaluate the effectiveness of the proposed technique, testing has been conducted under IEEE 9 bus test system. Simulation results show that the proposed technique can reliably detect symmetrical fault occurring during power swing.","","978-1-4673-0662-1978-1-4673-0660-7978-1-4673-0661","10.1109/PEOCO.2012.6230850","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6230850","distance protection;power swing;symmetrical fault;S-Transform","Impedance;Protective relaying;Reactive power;Voltage measurement;Power measurement;Current measurement;Computer aided software engineering","fault diagnosis;power system faults;power system protection;reactive power","current distance relay;power swing blocking scheme;fault detection scheme;symmetrical fault;S-transform analysis;reactive power signals waveform;IEEE 9 bus test system","","3","15","","","","","","IEEE","IEEE Conferences"
"Design, implementation and monitoring of a screw order handling process using business process management tools","A. Gîrbea; F. Sisak; L. Perniu","Electrical Engineering and Computer Science, Transilvania University of Brasov, Romania; Electrical Engineering and Computer Science, Transilvania University of Brasov, Romania; Electrical Engineering and Computer Science, Transilvania University of Brasov, Romania","2010 12th International Conference on Optimization of Electrical and Electronic Equipment","","2010","","","760","767","This article describes a new approach for the modeling of an enterprise application. This approach supposes a clear division of the application into three components: web front-end, business process and database. The article will particularly describe the development and testing of the business process. This new technique assures a perfect combination between the automated and the human tasks. The order placed by a client will be processed by the business process and as a result the raw materials used, the processing mode, the working timetable for the treatment, the galvanization will be determined. Finally the client will receive a notification including the acceptance/refusal of the order and the calculated delivery date for an accepted order. This approach comes with great advantages: greater flexibility for the application, automated monitoring of all the activities and greater reusability of the code. As a result the company is able to react much faster to market changes and the costs of the changes will be minimum.","1842-0133;1842-0133","978-1-4244-7020-4978-1-4244-7019-8978-1-4244-7018","10.1109/OPTIM.2010.5510565","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5510565","factory automation;business monitoring;business process;human-machine interface;flexibility;reusability","Fasteners;Application software;Databases;Companies;Service oriented architecture;Computerized monitoring;Computer architecture;Flowcharts;Software tools;Buildings","business data processing;production engineering computing","screw order handling process;business process management tool;Web front end;business process;activities automated monitoring","","","12","","","","","","IEEE","IEEE Conferences"
"Extending advanced failure effects analysis to support Prognostics and Health Management","S. Rudov-Clark; A. Ryan; C. Stecki; J. Stecki; A. Hess","PHM Technology Pty Ltd., 1/15 Pickering Rd, Mulgrave, Victoria, Australia 3071, 613-9561-0700; PHM Technology Pty Ltd., 1/15 Pickering Rd, Mulgrave, Victoria, Australia 3071, 613-9561-0700; PHM Technology Pty Ltd., 1/15 Pickering Rd, Mulgrave, Victoria, Australia 3071, 613-9561-0700; PHM Technology Pty Ltd., 1/15 Pickering Rd, Mulgrave, Victoria, Australia 3071, 613-9561-0700; PHM Technology Pty Ltd., 1/15 Pickering Rd, Mulgrave, Victoria, Australia 3071, 613-9561-0700","2010 Prognostics and System Health Management Conference","","2010","","","1","5","Anticipating failure modes and their system-wide symptoms enables the assessment and comparison of conceptual system designs with respect to their likely monitoring requirements. A model-based, qualitative simulation approach has been adopted in the development of a software tool (MADe) to support the Prognostics and Health Management (PHM) of the Joint Strike Fighter.","2166-563X;2166-5656","978-1-4244-4756-5978-1-4244-4758","10.1109/PHM.2010.5413407","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5413407","","Failure analysis;Prognostics and health management;Fault detection;System analysis and design;Design optimization;Fault diagnosis;Algorithm design and analysis;System testing;Hardware;Analytical models","failure analysis;production engineering computing","advanced failure effects analysis;prognostics;health management;failure modes;conceptual system design;monitoring requirements;model based approach;qualitative simulation approach;software tool","","2","4","","","","","","IEEE","IEEE Conferences"
"An Automatic Configuration Approach to Improve Real-Time Application Throughput While Attaining Determinism","X. Zhang; D. Cao; Y. Gao; X. Chen; H. Mei","NA; NA; NA; NA; NA","2010 IEEE 34th Annual Computer Software and Applications Conference","","2010","","","443","452","Determinism and throughput are two important performance measures for Java-based real-time applications, but they often conflict. Therefore, it is significant to improve throughput for Java-based real-time applications while guaranteeing its execution time determinism. In this paper, we propose an automatic configuration approach to assign real-time thread priorities to solve the above-mentioned problem. In this approach, we propose an innovative representation of determinism related with real-time thread priorities using stochastic process. Java-based real-time application's throughput is quantified with thread priorities as parameters. The algorithm of integer programming is used to optimize throughput with boundary conditions of the level of determinism. Finally, the Sweet Factory application is tested to evaluate the effect of our approach. Experiment results show that throughput for Java-based real-time applications could be efficiently improved while keeping the execution time determinism with our approach.","0730-3157;0730-3157;0730-3157","978-1-4244-7513-1978-1-4244-7512-4978-0-7695-4085","10.1109/COMPSAC.2010.73","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676292","real-time applications;throughput;determinism;stochastic process;integer programming","Real time systems;Throughput;Time factors;Instruction sets;Stochastic processes;Production facilities;Java","integer programming;Java;real-time systems;software architecture;virtual machines","determinism;Java;real-time thread priority;stochastic process;integer programming","","","15","","","","","","IEEE","IEEE Conferences"
"Integrated scheduling and configuration caching in dynamically reconfigurable systems","A. Ahmadinia","School of Engineering and Computing, Glasgow Caledonian University, UK","2010 International Conference on Intelligent and Advanced Systems","","2010","","","1","6","In order to use the dynamic reconfiguration possibility on FPGAs efficiently, one needs a support in the form of an operating system to manage both software and reconfigurable hardware. For this support, a suitable reconfigurable hardware model and optimization methods are required. This paper considers the problem of executing a dynamically changing set of tasks on a reconfigurable system, made upon a processor and a reconfigurable device. Task execution on such a platform is managed by a scheduler that can allocate tasks either to the processor or to the reconfigurable device. To reduce configuration overhead of modules, a caching algorithm is integrated with the scheduler. Simulation results demonstrate the reduction of configuration overhead up to 40% by using integrated caching algorithm in the scheduler and compare the effectiveness of different caching methods.","","978-1-4244-6625-2978-1-4244-6623-8978-1-4244-6624","10.1109/ICIAS.2010.5716227","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5716227","","Hardware;Software;Field programmable gate arrays;Heuristic algorithms;Processor scheduling;Signal processing algorithms;Benchmark testing","cache storage;field programmable gate arrays;operating systems (computers);optimisation;processor scheduling;reconfigurable architectures;task analysis","integrated scheduling;configuration caching;dynamically reconfigurable systems;dynamic reconfiguration possibility;FPGA;operating system;reconfigurable hardware model;optimization methods;dynamically changing set;reconfigurable device;task execution;task allocation;configuration overhead;integrated caching algorithm","","","16","","","","","","IEEE","IEEE Conferences"
"Topology structure and centrality in a java source code","L. Ying; D. Ding","Education department, national university of defense technology, Changsha, China; Dept. of mathematics and computer science, chizhou college, China","2012 IEEE International Conference on Granular Computing","","2012","","","787","789","Call graph plays a very important role in program analysis, and it has been widely used in software engineering (e.g., program understanding, compiling optimization, regression test, etc). In the present paper, we studied the source code structure of a Java program from a call graph perspective. We check the basal network properties of the call graph. By comparing several familiar centrality measures (betweenness, closeness, eccentricity, HITS-Authority and PageRank) and applying them to the call graph, we also give the centrality analysis of the call graph.","","978-1-4673-2311-6978-1-4673-2310","10.1109/GrC.2012.6468617","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6468617","call graph;centrality;complex network;Java program","Education;Biological information theory","complex networks;directed graphs;Java;program diagnostics","topology structure;topology centrality analysis;Java source code structure;call graph;program analysis;software engineering;program understanding;compiling optimization;regression test;Java program;basal network properties;complex network","","1","7","","","","","","IEEE","IEEE Conferences"
"K-means implementation on FPGA for high-dimensional data using triangle inequality","Z. Lin; C. Lo; P. Chow","Electrical and Computer Engineering, University of Toronto, ON, Canada M5S 3G4; Electrical and Computer Engineering, University of Toronto, ON, Canada M5S 3G4; Electrical and Computer Engineering, University of Toronto, ON, Canada M5S 3G4","22nd International Conference on Field Programmable Logic and Applications (FPL)","","2012","","","437","442","One of the challenges to data mining raised by technology development is that both data size and dimensionality is growing rapidly. K-means, one of the most popular clustering algorithms in data mining, suffers in computational time when used for large data sets and data with high dimensionality. In this paper, we propose a hardware architecture for K-means with triangle inequality optimization on FPGA. An optimal 8-bit square calculator for 6-LUT architectures is described to minimize the hardware cost and an approximation solution is proposed to avoid square root calculation in the original triangle inequality optimization. Our software and hardware experiments are tested with the MNIST benchmark and uniform random numbers of various size. This approximation results in 2% more distance calculations for MNIST and 5% for uniform random numbers than the original optimization. Compared to the baseline hardware system without optimization, our approach achieves up to 77% improvement in processing time with about 10% logic overhead. We demonstrate that the hardware can achieve 55-fold speed up compared to software for the 1024 MNIST.","1946-147X;1946-1488","978-1-4673-2256-0978-1-4673-2257-7978-1-4673-2255","10.1109/FPL.2012.6339141","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6339141","","Hardware;Optimization;Approximation methods;Field programmable gate arrays;Adders;Calculators;Clustering algorithms","approximation theory;data mining;field programmable gate arrays;optimisation;pattern clustering;unsupervised learning","k-means implementation;FPGA;high-dimensional data;triangle inequality optimization;data mining;technology development;data size;data dimensionality;unsupervised clustering algorithms;data sets;hardware architecture;optimal 8-bit square calculator;6-LUT architectures;hardware cost minimization;approximation solution;MNIST benchmark;uniform random numbers","","13","7","","","","","","IEEE","IEEE Conferences"
"Design of a multivariable controller for an engine eddy dynamometer system","Ge Huamin; Zhou Xingpeng","Key Laboratory of Measurement and Control of CSE Ministry of Education, School of Automation, Southeast University, Nanjing, China, 210096; Key Laboratory of Measurement and Control of CSE Ministry of Education, School of Automation, Southeast University, Nanjing, China, 210096","2010 International Conference on Computer, Mechatronics, Control and Electronic Engineering","","2010","4","","476","479","The ever increasing demand for reducing fuel consumption and emission has driven the improvement and innovation of new technology in an engine and an engine dynamometer system for an engine testing equipment. This paper presents a novel control method of the engine eddy dynamometer system for the automobile engine tester, an engine eddy dynamometer measurement and control system is developed based on Embedded PC and NI (National Instrument) real-time module by Labview software. Aiming at a multiple-input-multiple-output engine-dynamometer complex system,a multi-loop optimized subdivision PID controller of engine eddy dynamometer is designed. The experimental results indicated that the controller owns excellent performance and adaptability.","2159-6026;2159-6034","978-1-4244-7958-0978-1-4244-7957-3978-1-4244-7955","10.1109/CMCE.2010.5610107","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5610107","engine tester;multi-loop control;engine dynamometer system","Torque;Instruments;Engines","control system synthesis;dynamometers;engines;multivariable control systems;three-term control","multivariable controller design;engine eddy dynamometer measurement system;fuel consumption;engine testing equipment;automobile engine tester;embedded PC instrument real-time module;national instrument real-time module;Labview software;multiple input multiple output engine dynamometer complex system;multiloop optimized subdivision PID controller","","1","11","","","","","","IEEE","IEEE Conferences"
"An Improved Code Cache Management Scheme from I386 to Alpha In Dynamic Binary Translation","F. Yue; J. Pang; X. Han; J. Cui","NA; NA; NA; NA","2010 Second International Conference on Computer Modeling and Simulation","","2010","2","","321","324","Dynamic binary translate is one of the main methods in software migration cross platform. How to optimize it has been the research key points for long time. This paper presents a better scheme of translation cache management based on the research of translation from i386 to alpha. It divides the translation cache into two levels and adopts the advantages of FIFO scheme. The balance of speed and consume is accomplished smartly. We take the experiments on the improved dynamic binary translation software QEMU [1]. And we get the conclusion from the test results that the average translation speed can increase about 3% with this scheme.","","978-1-4244-5643-7978-1-4244-5642","10.1109/ICCMS.2010.97","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5421070","dynamic binary translation;translation optimizations;cache management;QEMU","Computer aided instruction;Computer architecture;Computational modeling;Computer simulation;Conference management;Technology management;Computer science;Information science;Embedded system;Algorithm design and analysis","cache storage","improved code cache management scheme;dynamic binary translation;software migration cross platform;translation cache management;FIFO scheme;improved dynamic binary translation software QEMU;average translation speed","","","7","","","","","","IEEE","IEEE Conferences"
"Design of mice maze based on PLC control unit","J. Ding; S. Zhang","School of Information and Electrical, Zhejiang University City College, Hangzhou, Zhejiang province, China; School of Information and Electrical, Zhejiang University City College, Hangzhou, Zhejiang province, China","2011 International Conference on Electronics, Communications and Control (ICECC)","","2011","","","2007","2010","In recent years, there are a number of mouse maze devices developed. In this paper, we introduce the Y-maze which is based on the traditional programmable logic controller as a control center to the new Y-maze. In the subject, the maze of automatic control system will be in addition to the sensor controller sub- section also part and actuator part. This paper mainly discusses the new Maze automatic control system, describes the software design process and priorities, hardware selection and layout. Among them, the interface circuit of the programmable controller, I/O port assignments, infrared sensor module and the maze rotation module are described in detail The maze of automatic control system to achieve the experimental data in mice can reduce errors, improve test efficiency and reduce human labor, side by side, in addition to the traditional Y-maze caused by the interference of some unavoidable factors, such as mice, leaving the smell of mice in the experimental results, the reliability of the experiment can be further improved.","","978-1-4577-0321-8978-1-4577-0320-1978-1-4577-0319","10.1109/ICECC.2011.6067621","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6067621","Y-maze;PLC;infrared sensors;automatic control","Mice;Noise;Educational institutions;Presses;Switches","biocontrol;control engineering computing;programmable controllers;software engineering","mice maze design;PLC control unit;mouse maze devices;Y-maze;programmable logic controller;maze automatic control system;software design process;hardware selection;hardware layout;I-O port assignments;infrared sensor module;maze rotation module","","","7","","","","","","IEEE","IEEE Conferences"
"Research and development of self-energy SF<inf>6</inf>circuit breaker on the breaking capacity","W. Gao; W. Zhang; Ma Zhiying; Wang Haibin; Peng Lei; Du Wenjuan; Jiao Qiuzhong","Xian High Voltage Apparatus Research Institute Co., Ltd., 710077, China; Xian High Voltage Apparatus Research Institute Co., Ltd., 710077, China; Xian Jiaotong University, 710049, China; Xian High Voltage Apparatus Research Institute Co., Ltd., 710077, China; Xian High Voltage Apparatus Research Institute Co., Ltd., 710077, China; Xian High Voltage Apparatus Research Institute Co., Ltd., 710077, China; Xian High Voltage Apparatus Research Institute Co., Ltd., 710077, China","2011 1st International Conference on Electric Power Equipment - Switching Technology","","2011","","","119","122","By improving of the dielectric performance between the breakers, medium condition and arc extinguishing ability, the breaking capacity of circuit breaker is increased. The main architecture of the 252kV circuit breaker is determined in which double chamber of interrupter construction + dual-direction moving of contacts mode are introduced. The key structure design parameter have been optimized combined with the design experiences of the finalized product: The volume of thermal expand chamber and puffer chamber as well as the cooperation between them are determined which ensures the breaking of the high current. Electric field analysis software is used to calculate, analyze and optimize the electric field strength of the circuit breaker contacts in some sensitive positions. Improving airflow channel for the decrease of the gas density is to be restrained and the hot gas exhaust efficiency is to be increased. As a result, the medium condition is improved. All the test verification has achieved for the researched and developed circuit breaker, and the product have been on stream. In the meantime, it provides the new technical foundation for the various following switchgears.","","978-1-4577-1272-2978-1-4577-1273-9978-1-4577-1271","10.1109/ICEPE-ST.2011.6122949","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6122949","Circuit breaker;Self-energy interrupter;Optimization;Breaking capacity","Circuit breakers;Optimization;Electric fields;Interrupters;Contacts;Sulfur hexafluoride;Thermal expansion","interrupters;research and development;switchgear","research and development;self-energy SF6 circuit breaker;breaking capacity;dielectric performance;interrupter;thermal expand chamber;puffer chamber;electric field strength;switchgears;voltage 252 kV","","","5","","","","","","IEEE","IEEE Conferences"
"Dynamic opening-book in computer games","L. Li; H. Huang; L. Deng","Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; School of Automation, School of Software, Beijing Institute of Technology, Beijing, China; School of Automation, School of Software, Beijing Institute of Technology, Beijing, China","2011 Chinese Control and Decision Conference (CCDC)","","2011","","","3451","3454","Opening-book is an important collection of human knowledge which would play a crucial role in computer games. For the shortcomings of ordinary opening-book, this paper proposes Dynamic Opening-book. Compared with ordinary one, dynamic opening-book introduces the concept of move reliability, processes information feedback, and has self-optimizing and self-learning functions. In addition, we carry out experiments using connect 6 to test dynamic opening-book in this paper. The results demonstrate its superiority.","1948-9439;1948-9447","978-1-4244-8738-7978-1-4244-8737-0978-1-4244-8736","10.1109/CCDC.2011.5968713","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5968713","Dynamic opening-book;Move reliability;Machine learning;Connect 6 emulation","Games;Reliability;Computers;Heuristic algorithms;Computational efficiency;Humans;Databases","computer games","dynamic opening-book;computer games;human knowledge;move reliability;information feedback;self-optimizing functions;self-learning functions","","1","5","","","","","","IEEE","IEEE Conferences"
"An Experiment on Performing DSTM Applications in a Public Cloud","Y. Yoshino; M. Aritsugi","NA; NA","2012 41st International Conference on Parallel Processing Workshops","","2012","","","179","187","Performing distributed software transactional memory (DSTM) applications in a public cloud is investigated in this paper. Transactions are introduced in DSTM for simplifying parallel programming in distributed environments. DSTM is thus a promising alternative to lock-based programming models. Cloud computing attracts attention as a new way for commercial applications and for processing large-scale data. However, to our knowledge, there is no study of executing DSTM applications using public clouds. In this paper, we report an experiment on performing DSTM applications in a public cloud. We also try to construct a performance model by adapting a TM performance model to DSTM in order to decide which cloud resources to be chosen in executing DSTM applications. Experimental results show that there are strong and weak points our DSTM model has in deciding machine types and the number of machines for performance.","0190-3918;1530-2016;2332-5690","978-1-4673-2509-7978-0-7695-4795","10.1109/ICPPW.2012.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6337478","programming models for HPC clouds;modeling and optimizations;distributed software transactional memory","Cloud computing;IP networks;Throughput;Benchmark testing;Adaptation models;Bandwidth","cloud computing;parallel programming;storage management","DSTM applications;distributed software transactional memory;public cloud;parallel programming;distributed environments;lock-based programming models;cloud computing;large-scale data processing;TM performance model","","","28","","","","","","IEEE","IEEE Conferences"
"Optimum electrostatic force control for fabricating a hybrid UV-curable aspheric lens","K. Hung; L. Chang; F. Tseng; N. T. M. Hang","Institute of Mechcnical and Electrical Engineering, Ming-Chi University of Technology, Hsinchu, Taiwan; Engineering and System Science Department, National Tsing Hua University, Hsinchu, Taiwan; Engineering and System Science Department, National Tsing Hua University, Hsinchu, Taiwan; Engineering and System Science Department, National Tsing Hua University, Hsinchu, Taiwan","2010 IEEE 5th International Conference on Nano/Micro Engineered and Molecular Systems","","2010","","","404","408","The purpose of this paper is to use a hybrid structure and the electrostatic force to fabricate aspheric lenses with high Blu-Ray transmittance (95% at 405 nm). The hybrid structure is composed of Norland Optical Adhesive 63 (NOA63) (refractive index: 1.5802 at 405 nm) and BK-7 glass (refractive index: 1.5302). OSLO and CFD software was used to simulate the gradient of the electric field between the top and bottom electrodes and to produce the optimum bottom electrode design. Different electrode designs were also tested in order to optimize the morphology of the lens profile design. The resulting lenses have a clear aperture of approximately 0.92 mm, the maximum shape error is less than 0.18%, and the spot size of the fabricated aspheric lenses can be controlled to approximately 0.504 μm. This technology can be used to fabricate lenses for applications in micro-optical systems.","","978-1-4244-6545-3978-1-4244-6543-9978-1-4244-6544","10.1109/NEMS.2010.5592417","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5592417","Aspheric lens;Polymer;Electrostatic","Lenses;Electrodes;Optical device fabrication;Adaptive optics;Optical variables control;Optical refraction;Electric fields","aspherical optics;light refraction;light transmission;microlenses;optical control;optical design techniques;optical glass;optical polymers;optical testing","optimum electrostatic force control;hybrid UV-curable aspheric lens;optical fabrication;high blu-ray transmittance;Norland Optical Adhesive 63;refractive index;BK-7 glass;CFD software;OSLO software;optimum bottom electrode design;optical testing;morphology;maximum shape error;wavelength 405 nm;size 0.92 mm","","","10","","","","","","IEEE","IEEE Conferences"
"A globally optimal data-driven approach for image distortion estimation","Y. Tian; S. G. Narasimhan","The Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; The Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA","2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","2010","","","1277","1284","Image alignment in the presence of non-rigid distortions is a challenging task. Typically, this involves estimating the parameters of a dense deformation field that warps a distorted image back to its undistorted template. Generative approaches based on parameter optimization such as Lucas-Kanade can get trapped within local minima. On the other hand, discriminative approaches like Nearest-Neighbor require a large number of training samples that grows exponentially with the desired accuracy. In this work, we develop a novel data-driven iterative algorithm that combines the best of both generative and discriminative approaches. For this, we introduce the notion of a “pull-back” operation that enables us to predict the parameters of the test image using training samples that are not in its neighborhood (not ϵ-close) in parameter space. We prove that our algorithm converges to the global optimum using a significantly lower number of training samples that grows only logarithmically with the desired accuracy. We analyze the behavior of our algorithm extensively using synthetic data and demonstrate successful results on experiments with complex deformations due to water and clothing.","1063-6919;1063-6919","978-1-4244-6985-7978-1-4244-6984-0978-1-4244-6983","10.1109/CVPR.2010.5539822","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5539822","","Parameter estimation;Back;Clothing;Optical character recognition software;Biomedical imaging;Layout;Robots;Electronic mail;Iterative algorithms;Testing","image classification;image registration;iterative methods;optimisation;parameter estimation","globally optimal data-driven approach;image distortion estimation;image alignment;nonrigid distortions;parameter estimation;dense deformation field;distorted image;parameter optimization;Lucas-Kanade;data-driven iterative algorithm;pull-back operation","","19","27","","","","","","IEEE","IEEE Conferences"
"The research on double row V-type inertial gas-liquid separator","W. Zhongyi; L. Beibei; S. Tao; Y. Yunliang","College of Power and Energy Engineering, Harbin Engineering University, Harbin, China; College of Power and Energy Engineering, Harbin Engineering University, Harbin, China; College of Power and Energy Engineering, Harbin Engineering University, Harbin, China; College of Power and Energy Engineering, Harbin Engineering University, Harbin, China","2011 IEEE 3rd International Conference on Communication Software and Networks","","2011","","","317","320","The performance of double row V-type inertial gas-liquid separator has been studied by numerical computation and experimental studied. The aerodynamics performance and separation efficiency of double row V-type inertial gas-liquid separator with different blade spacing are acquired by numerical simulation. The experimental research on aerodynamics performance of separator with specific blade spacing has been done on the special wind tunnel test bench. The experimental results agree well with the numerical simulation results. The researches can help to make prediction for performances in engineering and provide theoretical footing for optimizing the design.","","978-1-61284-486-2978-1-61284-485-5978-1-61284-484","10.1109/ICCSN.2011.6014730","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014730","gas-liquid separator;double row V-type blade;numerical simulation;experimental study","Educational institutions;Sun;Particle separators;Blades","aerodynamics;blades;flow separation;numerical analysis;two-phase flow","v-type inertial gas-liquid separator;aerodynamics performance;separation efficiency;blade spacing;numerical simulation;wind tunnel test bench","","","5","","","","","","IEEE","IEEE Conferences"
"An Approach to Optimized Resource Scheduling Algorithm for Open-Source Cloud Systems","H. Zhong; K. Tao; X. Zhang","NA; NA; NA","2010 Fifth Annual ChinaGrid Conference","","2010","","","124","129","Based on the deep research on Infrastructure as a Service (IaaS) cloud systems of open-source, we propose an optimized scheduling algorithm to achieve the optimization or sub-optimization for cloud scheduling problems. In this paper, we investigate the possibility to allocate the Virtual Machines (VMs) in a flexible way to permit the maximum usage of physical resources. We use an Improved Genetic Algorithm (IGA) for the automated scheduling policy. The IGA uses the shortest genes and introduces the idea of Dividend Policy in Economics to select an optimal or suboptimal allocation for the VMs requests. The simulation experiments indicate that our dynamic scheduling policy performs much better than that of the Eucalyptus, Open Nebula, Nimbus IaaS cloud, etc. The tests illustrate that the speed of the IGA almost twice the traditional GA scheduling method in Grid environment and the utilization rate of resources always higher than the open-source IaaS cloud systems.","1949-131X;1949-1328","978-1-4244-7544-5978-1-4244-7543","10.1109/ChinaGrid.2010.37","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5563015","cloud computing;resource scheduling;genetic algorithm;grid computing;IaaS","Clouds;Processor scheduling;Cloud computing;Computational modeling;Open source software;Biological cells;Resource management","genetic algorithms;grid computing;Internet;scheduling;virtual machines","optimized resource scheduling algorithm;open-source cloud systems;Infrastructure as a Service;virtual machines;improved genetic algorithm;dividend policy;grid environment","","44","22","","","","","","IEEE","IEEE Conferences"
"Parallel implementation of the Finite-Difference Time-Domain method in Open Computing Language","T. P. Stefański; S. Benkler; N. Chavannes; N. Kuster","Integrated Systems Laboratory, Swiss Federal Institute of Technology, Gloriastrasse 35, 8092 Zurich, Switzerland; SPEAG Software R&D, Zeughausstrasse 43, 8004 Zurich, Switzerland; SPEAG Software R&D, Zeughausstrasse 43, 8004 Zurich, Switzerland; Foundation for Research on Information Technologies in Society, Zeughausstrasse 43, 8004 Zurich, Switzerland","2010 International Conference on Electromagnetics in Advanced Applications","","2010","","","557","560","In this paper we evaluate the usability and performance of Open Computing Language (OpenCL) targeted for implementation of the Finite-Difference Time-Domain (FDTD) method. The simulation speed was compared to implementations based on alternative techniques of parallel processor programming. Moreover, the portability of OpenCL FDTD code between modern computing architectures was assessed. The average speed of OpenCL FDTD simulations on a GPU was about 1.1 times lower than a comparable CUDA based solver for domains with sizes varying from 50<sup>3</sup>to 400<sup>3</sup>cells. Although OpenCL code dedicated to GPUs can be executed on multi-core CPUs, a direct porting does not provide satisfactory performance due to an application of architecture specific features in GPU code. Therefore, the OpenCL kernels of the developed FDTD code were optimized for multi-core CPUs. However, this improved OpenCL FDTD code was still about 1.5 to 2.5 times slower than the FDTD solver developed in the OpenMP parallel programming standard. The study concludes that, despite current performance drawbacks, the future potential of OpenCL is significant due to its flexibility and portability to various architectures.","","978-1-4244-7368-7978-1-4244-7366-3978-1-4244-7367","10.1109/ICEAA.2010.5653857","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5653857","","Finite difference methods;Graphics processing unit;Time domain analysis;Computational modeling;Benchmark testing;Runtime;Central Processing Unit","finite difference time-domain analysis;parallel programming;programming languages","finite difference time-domain method;open computing language;usability;parallel processor programming;portability;computing architecture;multicore CPU;GPU code;FDTD code;OpenMP parallel programming standard","","5","7","","","","","","IEEE","IEEE Conferences"
"Viral Systems Application for Knapsack Problem","D. Suryadi; E. K. Kartika","NA; NA","2011 Third International Conference on Computational Intelligence, Communication Systems and Networks","","2011","","","11","16","Knapsack problem is a problem that arises when determining what items should be included in a storage area, which has a maximum storage capacity. A metaheuristic algorithm, Viral Systems, is applied to overcome the weaknesses of optimization methods. There are 3 hypothetical cases, which the basic difference is the number of items involved. The cases are used for parameter testing. Based on the experimental results, no Viral Systems parameters influence the objective function value in Case 1. The interaction between LIT<sup>0</sup>, p<sub>r</sub>, and p<sub>lt</sub> influences the objective function value in Case 2. LNR<sup>0</sup>, p<sub>lt</sub>, the interaction between p<sub>r</sub> and p<sub>lt</sub>, and the interaction between LIT<sup>0</sup>, LNR<sup>0</sup>, and pi influence the objective function value in Case 3. This research shows that Viral Systems is found to be promising, since it successfully reaches optimal solution for the first two cases. Nevertheless, parameters selection should be investigated more thoroughly.","","978-1-4577-0975-3978-0-7695-4482","10.1109/CICSyN.2011.16","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6005667","Viral Systems;metaheuristic;knapsack problem;algorithm","Bioinformatics;Genomics;Computer aided software engineering;DNA;Organisms;Mathematical model;Equations","biology;integer programming;knapsack problems;storage","viral systems application;knapsack problem;maximum storage capacity;optimization methods;parameter testing;objective function value","","","7","","","","","","IEEE","IEEE Conferences"
"Feasibility of congestive heart failure telemanagement using a wii-based telecare platform","J. Finkelstein; J. Wood; E. Cha; A. Orlov; C. Dennison","Chronic Disease Informatics Program, Johns Hopkins University, USA; Chronic Disease Informatics Program, Johns Hopkins University, USA; Chronic Disease Informatics Program, Johns Hopkins University, USA; Chronic Disease Informatics Program, Johns Hopkins University, USA; Chronic Disease Informatics Program, Johns Hopkins University, USA","2010 Annual International Conference of the IEEE Engineering in Medicine and Biology","","2010","","","2211","2214","A gaming platform has been used to implement a Home Automated Telemanagement (HAT) system for chronic disease management in the patient's home. The system questions patients with congestive heart failure (CHF) to monitor symptoms, weight changes, and quality of life while educating the patient on their disease. The system is designed to run on the Nintendo Wii videogame console using an active internet connection and the console's built in internet browser. It questions the patient daily on their condition, monitors their weight, and provides the patient with instant feedback on their condition in the form of a 3 zone CHF action plan. The system is designed to be as simple as possible, making it usable by patients with no prior computer or videogame experience. This telemanagement system has been successfully designed and implemented to optimize the care of patients with CHF.","1094-687X;1558-4615","978-1-4244-4123-5978-1-4244-4124","10.1109/IEMBS.2010.5627087","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5627087","","Diseases;Heart;Internet;Built-in self-test;Monitoring;Computers","cardiology;computer games;Internet;medical computing;medical disorders;multimedia communication;patient monitoring;telemedicine","congestive heart failure telemanagement;Wii-based telecare platform;gaming platform;home automated telemanagement;chronic disease management;patient monitoring;Nintendo Wii videogame console;active Internet connection;Internet browser","Body Weight;Chronic Disease;Computer Communication Networks;Disease Management;Equipment Design;Feasibility Studies;Heart Failure;Heart Failure;Humans;Internet;Patient Education as Topic;Quality of Life;Software;Telemedicine;Telemedicine;Video Games","9","21","","","","","","IEEE","IEEE Conferences"
"Optimized bit extraction of SVC exploiting linear error model","W. Zhang; J. Sun; J. Liu; Z. Guo","Inst. of Comput. Sci. &amp; Technol., Peking Univ. Beijing 100871, China; Inst. of Comput. Sci. &amp; Technol., Peking Univ. Beijing 100871, China; Inst. of Comput. Sci. &amp; Technol., Peking Univ. Beijing 100871, China; Inst. of Comput. Sci. &amp; Technol., Peking Univ. Beijing 100871, China","2012 IEEE International Symposium on Circuits and Systems","","2012","","","1887","1890","The Scalable Video Coding (SVC) extension of the H.264/AVC video coding standard supports fidelity or quality (SNR) scalability. The quality enhancement packets would be discarded in case of limited network capacity, which calls for an optimized bit extraction strategy. In this paper, we first analyze the linear feature in H.264/AVC video coding. A linear error model is also constructed using this feature in case of SVC quality scalability. Then based on the linear error model, the rate and distortion (R-D) impact of each quality enhancement packet over the whole sequence is obtained. Finally a new priority assigning algorithm is designed for a more efficient extraction, giving high rank to those with great R-D impacts. Extensive experiments are presented to demonstrate the accuracy of the linear error model and the validity of the priority assigning algorithm. Tests on the set of eight standard video sequences show the quality promotion under any bitrate constraint, and a fidelity gain up to 0.4 dB PSNR is achieved by the proposed strategy, compared to the JSVM reference software with Quality Layer information.","2158-1525;0271-4302;0271-4302","978-1-4673-0219-7978-1-4673-0218-0978-1-4673-0217","10.1109/ISCAS.2012.6271639","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6271639","","Vectors;Video coding;PSNR;Static VAr compensators;Scalability;Video sequences;Decoding","","","","3","7","","","","","","IEEE","IEEE Conferences"
"Prioritizing demand response programs from reliability aspect","M. Nikzad; M. Bashirvand; B. Mozafari; A. M. Ranjbar","Department of Electrical Engineering, Islamshahr Branch, Islamic Azad University, Tehran, Iran; Department of Electrical and Computer Engineering, Science and Research Branch, Islamic Azad University, Tehran, Iran; Department of Electrical and Computer Engineering, Science and Research Branch, Islamic Azad University, Tehran, Iran; Department of Electrical Engineering, Sharif University of Technology, Tehran, Iran","2012 11th International Conference on Environment and Electrical Engineering","","2012","","","229","234","In this paper, the impact of demand response programs (DRPs) on reliability improvement of the restructured power systems is quantified. In this regard, the demand response (DR) model which treats consistently the main characteristics of the demand curve is developed for modeling. In proposed model, some penalties for customers in case of no responding to load reduction and incentives for customers who respond to reducing their loads are considered. In order to make analytical evaluation of the reliability, a mixed integer DCOPF is proposed by which load curtailments and generation re-dispatches for each contingency state are determined. Both transmission and generation failures are considered in contingency enumeration. The proposed technique is modeled in the GAMS software and solved using CPLEX. Reliability indices for generation-side, transmission network and whole system are calculated using this technique. Different DRPs based on the DR model are implemented over the IEEE RTS 24-bus test system, and reliability indices for different parties are calculated. Afterward, using proposed performance index, the priority of the considered programs is determined from view point of different market participants.","","978-1-4577-1829-8978-1-4577-1830-4978-1-4577-1828","10.1109/EEEIC.2012.6221578","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6221578","demand response programs;analytical reliability evaluation techniques;mixed integer linear programming;expected energy not supplied;expected interruption cost","Reliability;Electricity;Power system reliability;Mathematical model;Load modeling;Load management;Elasticity","demand side management;load dispatching;power system reliability","demand response programs;reliability aspect;restructured power systems;main characteristics;load curtailments;generation re-dispatches;GAMS software;CPLEX;performance index","","2","13","","","","","","IEEE","IEEE Conferences"
"Efficiently solving quantified bit-vector formulas","C. M. Wintersteiger; Y. Hamadi; L. de Moura","ETH Zurich, Zurich, Switzerland; Microsoft Research, Cambridge, UK, LIX &#x00C9;cole Polytechnique, Palaiseau, France; Microsoft Research, Redmond, USA","Formal Methods in Computer Aided Design","","2010","","","239","246","In recent years, bit-precise reasoning has gained importance in hardware and software verification. Of renewed interest is the use of symbolic reasoning for synthesising loop invariants, ranking functions, or whole program fragments and hardware circuits. Solvers for the quantifier-free fragment of bit-vector logic exist and often rely on SAT solvers for efficiency. However, many techniques require quantifiers in bit-vector formulas to avoid an exponential blow-up during construction. Solvers for quantified formulas usually flatten the input to obtain a quantified Boolean formula, losing much of the word-level information in the formula. We present a new approach based on a set of effective word-level simplifications that are traditionally employed in automated theorem proving, heuristic quantifier instantiation methods used in SMT solvers, and model finding techniques based on skeletons/templates. Experimental results on two different types of benchmarks indicate that our method outperforms the traditional flattening approach by multiple orders of magnitude of runtime.","","978-0-9835678-0-6978-1-4577-0734","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770955","","Density estimation robust algorithm;Hardware;Mathematical model;Cognition;Encoding;Benchmark testing;Software","Boolean functions;computability;formal verification;theorem proving","quantified bit-vector formulas;bit-precise reasoning;hardware and software verification;symbolic reasoning;synthesising loop invariants;ranking functions;program fragments;hardware circuits;quantifier-free fragment;bit-vector logic;SAT solvers;exponential blow-up;quantified formulas;quantified Boolean formula;word-level information;word-level simplifications;automated theorem proving;heuristic quantifier instantiation methods;SMT solvers;model finding techniques;skeletons;flattening approach","","1","29","","","","","","IEEE","IEEE Conferences"
"Branch and data herding: Reducing control and memory divergence for error-tolerant GPU applications","J. Sartori; R. Kumar","University of Illinois at Urbana-Champaign, USA; University of Illinois at Urbana-Champaign, USA","2012 21st International Conference on Parallel Architectures and Compilation Techniques (PACT)","","2012","","","427","428","Control and memory divergence between threads in the same execution bundle, or warp, can significantly throttle the performance of GPU applications. We exploit the observation that many GPU applications exhibit error tolerance to propose branch and data herding. Branch herding eliminates control divergence by forcing all threads in a warp to take the same control path. Data herding eliminates memory divergence by forcing each thread in a warp to load from the same memory block. To safely and efficiently support branch and data herding, we propose a static analysis and compiler framework to prevent exceptions when control and data errors are introduced, a profiling framework that aims to maximize performance while maintaining acceptable output quality, and hardware optimizations to improve the performance benefits of exploiting error tolerance through branch and data herding. Our software implementation of branch herding on NVIDIA GeForce GTX 480 improves performance by up to 34% (13%, on average) for a suite of NVIDIA CUDA SDK and Parboil [7] benchmarks. Our hardware implementation of branch herding improves performance by up to 55% (30%, on average). Data herding improves performance by up to 32% (25%, on average). Observed output quality degradation is minimal for several applications that exhibit error tolerance, especially for visual computing applications. For a more detailed exposition of this work, see [6].","","978-1-4503-1182-3978-1-5090-6609","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7842957","GPGPU;Error Tolerance;High Performance;Control Divergence;Memory Divergence","Degradation;Instruction sets;Benchmark testing;Bandwidth;Safety;Graphics processing units;Hardware","graphics processing units;parallel processing;program compilers;program diagnostics","branch herding;data herding;control divergence reduction;memory divergence reduction;error-tolerant GPU applications;static analysis;compiler framework;profiling framework;hardware optimizations;software implementation;NVIDIA GeForce GTX 480;NVIDIA CUDA SDK;Parboil benchmarks;visual computing applications","","3","9","","","","","","IEEE","IEEE Conferences"
"Implementation and Test of Appearance-Based Vision Algorithms Using High-Level Synthesis in FPGA","E. Ortiz-Lopez; M. Ibarra-Manzano; J. A. Andrade-Lucio; J. G. A. Cervantes; O. G. Ibarra-Manzano","Electron. Dept., DICIS Univ. de Guanajuato, Salamanca, Mexico; Electron. Dept., DICIS Univ. de Guanajuato, Salamanca, Mexico; Electron. Dept., DICIS Univ. de Guanajuato, Salamanca, Mexico; Electron. Dept., DICIS Univ. de Guanajuato, Salamanca, Mexico; Electron. Dept., DICIS Univ. de Guanajuato, Salamanca, Mexico","2011 IEEE Electronics, Robotics and Automotive Mechanics Conference","","2011","","","143","148","This article presents an architecture to detect objects from images based on color and texture features. This architecture is simplified and efficient as a result of the optimization of Adequacy of Sum and Difference of Histograms(ASDH) for embedded systems. Our architecture was prototyped using LabVIEW FPGA which is a practical tool to develop high-level synthesis. We take advantage of LabVIEW FPGA to do rapid prototyping and implement the architecture and to make a general comparison among this architecture implemented with Hardware Description Language (HDL) and LabVIEW FPGA, this lets us analyze if the use of high level synthesis improve the systems performance. The use of high level synthesis give us an interesting option to improve in digital design made it the time of prototyping more shortly, efficiently and flexible for example in applications for vision systems.","","978-1-4577-1879","10.1109/CERMA.2011.30","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6125820","FPGA;High Level Synthesis;Adaptive Boosting algoritm;Color and Texture Features","Field programmable gate arrays;Image color analysis;Computer architecture;Algorithm design and analysis;Cameras;Hardware;Hardware design languages","field programmable gate arrays;hardware description languages;image colour analysis;image texture;object detection;software prototyping","appearance-based vision algorithms;high-level synthesis;object detection;image color features;image texture features;optimization;Adequacy of Sum and Difference of Histograms;ASDH;embedded systems;LabVIEW FPGA;rapid prototyping;hardware description language;HDL;digital design;vision systems","","2","8","","","","","","IEEE","IEEE Conferences"
"Parallel instruction set extension identification","D. Shapiro; M. Montcalm; M. Bolic","School of Information Technology and Engineering, Computer Architecture Research Group, University of Ottawa; School of Information Technology and Engineering, Computer Architecture Research Group, University of Ottawa; School of Information Technology and Engineering, Computer Architecture Research Group, University of Ottawa","2010 IEEE 26-th Convention of Electrical and Electronics Engineers in Israel","","2010","","","000535","000539","Modern embedded processors are often customized to accelerate native code. However, the design space exploration of hardware/software trade-offs is often time-intensive. To explore the design space of a processor's instruction set, simulations are utilized. Instruction set extension identification is usually performed by analyzing the basic blocks of an application in a linear fashion. We present an instruction set extension identification pass implemented in the COINS compiler which can simultaneously enumerate candidate instruction set extensions in multiple basic blocks. Using benchmarks such as FFT and Dijkstra we show a compiler execution improvement of time up to 53.7% compared to the traditional sequential approach.","","978-1-4244-8682-3978-1-4244-8681-6978-1-4244-8680","10.1109/EEEI.2010.5662163","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5662163","","Instruction sets;Hardware;Benchmark testing;Optimization;Computer architecture","hardware-software codesign;instruction sets;multiprocessing systems;program compilers","parallel instruction set extension identification;embedded processors;design space exploration;hardware-software design;COINS compiler","","1","22","","","","","","IEEE","IEEE Conferences"
"BenchNN: On the broad potential application scope of hardware neural network accelerators","T. Chen; Y. Chen; M. Duranton; Q. Guo; A. Hashmi; M. Lipasti; A. Nere; S. Qiu; M. Sebag; O. Temam","ICT, China; ICT, China; CEA LIST, France; IBM Research, China; University of Wisconsin, USA; University of Wisconsin, USA; University of Wisconsin, USA; USTC, China; LRI, CNRS, France; INRIA, France","2012 IEEE International Symposium on Workload Characterization (IISWC)","","2012","","","36","45","Recent technology trends have indicated that, although device sizes will continue to scale as they have in the past, supply voltage scaling has ended. As a result, future chips can no longer rely on simply increasing the operational core count to improve performance without surpassing a reasonable power budget. Alternatively, allocating die area towards accelerators targeting an application, or an application domain, appears quite promising, and this paper makes an argument for a neural network hardware accelerator. After being hyped in the 1990s, then fading away for almost two decades, there is a surge of interest in hardware neural networks because of their energy and fault-tolerance properties. At the same time, the emergence of high-performance applications like Recognition, Mining, and Synthesis (RMS) suggest that the potential application scope of a hardware neural network accelerator would be broad. In this paper, we want to highlight that a hardware neural network accelerator is indeed compatible with many of the emerging high-performance workloads, currently accepted as benchmarks for high-performance micro-architectures. For that purpose, we develop and evaluate software neural network implementations of 5 (out of 12) RMS applications from the PARSEC Benchmark Suite. Our results show that neural network implementations can achieve competitive results, with respect to application-specific quality metrics, on these 5 RMS applications.","","978-1-4673-4532-3978-1-4673-4531","10.1109/IISWC.2012.6402898","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6402898","neural networks;benchmark;accelerator;PARSEC","Neurons;Artificial neural networks;Benchmark testing;Hardware;Optimization;Wires","neural chips;neural net architecture;power aware computing;software fault tolerance","BenchNN;hardware neural network accelerators;device sizes;die area allocation;application domain;energy properties;fault-tolerance properties;recognition-mining-and-synthesis;RMS applications;high-performance workloads;high-performance microarchitectures;software neural network implementations;PARSEC Benchmark Suite;voltage scaling;application-specific quality metrics","","44","32","","","","","","IEEE","IEEE Conferences"
"Exploring Dynamic Program Behavior with Frames and Phases","D. César; G. Araújo; E. Borin","NA; NA; NA","2012 13th Symposium on Computer Systems","","2012","","","118","125","The kind and amount of hardware resources demanded for the efficient execution of different programs are not the same. In fact, even the same program may have different requirements at different moments during its execution. However, current computers hardware is not designed to adapt itself during the execution of programs. The continuous need for energy efficient computation and the ever-decreasing size of transistors will enable the design and manufacturing of smart processors capable of adapting itself to meet the needs of the executing software. In this work, we investigate how program phase analysis and dynamic code optimization can be combined to achieve these goals. We first propose and evaluate a technique to perform online program phase detection based on the execution of frames built by the replay framework and then we show how the phases information can be used to improve the effectiveness of the replay framework.","","978-1-4673-4468-5978-0-7695-4847","10.1109/WSCAD-SSC.2012.30","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6391772","program phases;dynamic optimization;dynamic reconfiguration","Hardware;Vectors;Benchmark testing;Optimization;Graphics;Dynamic scheduling;Educational institutions","program diagnostics","dynamic program behavior;program frame;hardware resource;program exeuction;energy efficient computation;smart processor;dynamic code optimization;program phase detection;replay framework","","","24","","","","","","IEEE","IEEE Conferences"
"Three-phase squirrel-cage induction motor modal analyses. Theoretical and experimental aspects","I. C. Rosca; M. Filip; E. Helerea","Transilvania University of Brasov, 29, Eroilor Str., Brasov, Romania; Mircea Cristea High School, 5, Turnului Str., Brasov, Romania; Transilvania University of Brasov, 29, Eroilor Str., Brasov, Romania","2012 13th International Conference on Optimization of Electrical and Electronic Equipment (OPTIM)","","2012","","","606","611","The vibrations developed during the work represent one of the main mechanical problems of the electric motors. Their frequencies and levels can cause large damages of different electrical motors parts. This is the reason why even in the design phase it is useful to be known the values of the natural frequencies of the parts. This can be made in two different ways: using modern capabilities of finite element method (FEM) or doing experimental modal analysis. The study developed in the present paper refers to the modal analysis of a three-phase squirrel cage induction of 1.5 kW, for both, for its elements and, for motor and the electric motor as an assembly. The FEM modal analysis was applied using ABAQUS software for assembly of the 7 elements of the motor. The 3D model was developed, mechanical connections being of screw type. The theoretical model was validated by experiments, with impact hammer test method. The validation confirms the assumptions made for the 3D model and the designed model can be use, as example, for other FEM based modal analysis. The natural frequencies obtained both by FEM and experimental method does not overlap on the working frequencies.","1842-0133;1842-0133;1842-0133","978-1-4673-1653-8978-1-4673-1650-7978-1-4673-1652","10.1109/OPTIM.2012.6231965","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6231965","","Finite element methods;Electric motors;Rotors;Assembly;Stators;Modal analysis","finite element analysis;machine theory;modal analysis;squirrel cage motors;vibrations","three-phase squirrel-cage induction motor modal analyses;vibrations;electric motors;finite element method;experimental modal analysis;FEM modal analysis;ABAQUS software;3D model;hammer test method;FEM based modal analysis;power 1.5 kW","","6","9","","","","","","IEEE","IEEE Conferences"
"Optimized harmonic estimation in bus transfer systems from polluted grid to DG unit using wavelet transform and radial basis functions neural networks","H. R. Baghaee; A. Mazloomzadeh; M. Mirsalim; G. B. Gharehpetian","Electrical Engineering Department at Amirkabir University of Technology, Hafez Ave, Tehran-Iran; ECE Department, College of Engineering &amp; Computing, Florida International University, Miami, Florida, 33174, USA; Electrical Engineering Department at Amirkabir University of Technology, Hafez Ave, Tehran-Iran; Electrical Engineering Department at Amirkabir University of Technology, Hafez Ave, Tehran-Iran","2012 Second Iranian Conference on Renewable Energy and Distributed Generation","","2012","","","45","50","Providing clean, continuous and reliable power to many industrial customers has become a necessity. Where possible, sensitive customers are supplied by two low-voltage utility feeders one of which is used as backup. Bus Transfer is able to provide the sensitive loads with a fast transfer to the backup feeder when the main one fails. This paper presents a new method to optimize harmonic estimation in bus transfer systems that use thyristor-based static transfer switch. The method uses filter bank property of Discrete Wavelet Packet Transform to measure the effective values of harmonics which are compatible with IEC standards, and implements them in power quality indices for operation of bus transfer systems. Implementation of wavelet packet transform has the benefit of inter-harmonic recognition, low spectral leakage, and the use of inappropriate length of a sampling window. The proposed method decomposes the voltage and current waveforms into uniform frequency bands corresponding to harmonics, and uses an optimized method to reduce the spectral leakage due to imperfect frequency response of the applied wavelet filter bank. As a case study, by using PSCAD software linked with MATLAB, the method is implemented and tested on a harmonic polluted network. The simulation results show the high accuracy of the proposed method.","2325-9272;2325-9280","978-1-4673-0665-2978-1-4673-0663-8978-1-4673-0664","10.1109/ICREDG.2012.6190466","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6190466","Harmonics;Bus Transfer;Selective Gating Strategy;Power Quality;Wavelet;WPT","Harmonic analysis;Power harmonic filters;Training;Filter banks;Vectors;Transforms","channel bank filters;discrete wavelet transforms;distributed power generation;IEC standards;neural nets;power engineering computing;power harmonic filters;power system measurement;radial basis function networks;thyristor applications","optimized harmonic estimation;bus transfer systems;polluted grid;distributed generation unit;wavelet transform;radial basis functions neural networks;filter bank;discrete wavelet packet transform;harmonic effective value;IEC standard;harmonic polluted network;thyristor-based static transfer switch;power quality indices;PSCAD software;MATLAB","","","11","","","","","","IEEE","IEEE Conferences"
"A unit selection text-to-speech synthesis system optimized for use with screen readers","A. Chalamandaris; S. Karabetsos; P. Tsiakoulis; S. Raptis","Institute for Language and Speech Processing, Athena Research Centre; Institute for Language and Speech Processing; Institute for Language and Speech Processing; Voice and Sound Technology, Department at the Institute for Language and Speech Processing","IEEE Transactions on Consumer Electronics","","2010","56","3","1890","1897","Currently, unit-selection text-to-speech technology is the common approach for near-natural speech synthesis systems. Such systems provide an important aid for blind or partially-sighted people, when combined with screen reading software. However, although the overall quality of the synthetic speech achieved by such systems can be quite high, this fact alone does not guarantee a high level of user satisfaction. Many issues have to be coped with in order to fulfill users' expectations when integrating such systems with screen reading tools aiming to assist blind users. This work describes the design and the implementation approaches for the efficient integration of this technology into screen reading environments. In particular, the issues of natural language processing, speed optimization, multilingual design and overall quality optimization are mainly addressed in this paper. In order to evaluate the resulting system, we carried out subjective assessment tests where expert users provided feedback about performance, quality and overall experience.","0098-3063;1558-4127","","10.1109/TCE.2010.5606343","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5606343","Speech Synthesis, Unit Selection, Text-to- Speech, Screen Reader, Assistive Technology","Speech;Databases;Digital signal processing;Speech synthesis;Cost function","handicapped aids;speech synthesis","text-to-speech synthesis system;screen readers;near-natural speech synthesis systems;screen reading software;user satisfaction;natural language processing;speed optimization;multilingual design","","13","26","","","","","","IEEE","IEEE Journals & Magazines"
"A Hybrid Evolution Algorithm for VLSI Floorplanning","J. Chen; J. Chen","NA; NA","2010 International Conference on Computational Intelligence and Software Engineering","","2010","","","1","4","The floorplanning is a critical phase in very large-scale integrated-circuit(VLSI) physical design. It determines the topology of layout, and it aims to arrange a set of rectangular modules on a chip so as to optimize the chip area, wirelength, etc. This problem is known to be NP-hard, and has received much attention in recent years. B*-tree representation is adopted in this paper. Based on the concept of evolutionary algorithm and simulated annealing, a hybrid evolutionary algorithm(ESA) is proposed. It is effective to explore solution space and locate the optimal solution. The effectiveness of our method is demonstrated on several cases of MCNC benchmarks.","","978-1-4244-5391-7978-1-4244-5392","10.1109/CISE.2010.5676951","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676951","","Algorithm design and analysis;Simulated annealing;Very large scale integration;Integrated circuit modeling;Benchmark testing;Space exploration;Computational modeling","evolutionary computation;integrated circuit layout;network topology;simulated annealing;trees (mathematics);VLSI","hybrid evolution algorithm;VLSI floorplanning;very large-scale integrated-circuit;topology;B*-tree representation;evolutionary algorithm;simulated annealing","","","16","","","","","","IEEE","IEEE Conferences"
"Strategy and contract design in supply chain with incomplete information about quality","W. Ma; Y. Zhou; Y. Hu","School of Economics and, Management, Tongji University, Shanghai, China; School of Economics and, Management, Tongji University, Shanghai, China; School of Economics and, Management, Tongji University, Shanghai, China","2010 IEEE International Conference on Software Engineering and Service Sciences","","2010","","","667","670","This paper considers that in a supply chain consisting of the manufacturer and supplier, how would the manufacturer choose the controlling strategy, sharing the external losses or taking them back all by himself, to optimize his own profit and maximize the supply chain's total revenue under incomplete information compared with a benchmark of the solutions under complete information. If the external loss is shared with the supplier, the contract is effective to induce the supplier to positively improve his own level of quality prevention. And the most important of all, the overall effectiveness of the supply chain is improved at the same time.","2327-0586;2327-0594","978-1-4244-6055-7978-1-4244-6054-0978-1-4244-6053","10.1109/ICSESS.2010.5552256","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5552256","contract design;incomplete information;supply chain's quality management","Contracts;Supply chains;Quality control;Economics;Games;Benchmark testing","contracts;quality management;supply chain management","contract design;supply chain quality management;supply chain total revenue;quality prevention","","","9","","","","","","IEEE","IEEE Conferences"
"Reduced Support Vector Machine Based on Margin Vectors","B. Kong; H. Wang","NA; NA","2010 International Conference on Computational Intelligence and Software Engineering","","2010","","","1","4","Reduced Support Vector Machine (RSVM) was proposed as an alternate of the standard SVM. Motivated by resolving the difficulty on handling large data sets using SVM, it pre-extracts a subset of data as `support vectors' and solves a smaller optimization problem. But it selects `support vectors' randomly from the training set, and this will affect the result. A new method called reduced support vector machine based on margin vectors is presented in this paper, some margin vectors were extracted as `support vectors' via center distance ratio, then were applied in the RSVM . The new method can be used to unbalanced data and reduce the effects of outliers. So the new method improves the ability of RSVM to classify and the training speed of SVM greatly.","","978-1-4244-5391-7978-1-4244-5392","10.1109/CISE.2010.5677026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5677026","","Training;Noise;Accuracy;Support vector machine classification;Databases;Testing","support vector machines","reduced support vector machine;margin vectors;center distance ratio","","2","5","","","","","","IEEE","IEEE Conferences"
"Shared engine model for automotive engine calibration platform development in JCUG","A. Ohata","Toyota Motor Corporation","Proceedings of SICE Annual Conference 2010","","2010","","","2491","2496","JCUG (Japan Calibration User Group) was established to initiate the development of the efficient engine calibration environment in April 2009. Engine calibration means the final coordination mainly by adjusting and properly determining parameters of engine control. Due to the strong demands from CO<sub>2</sub> reduction, environment protection and vehicle safety, the number of the control parameters has been rapidly increasing. Thus, the automotive industry needs the innovation of their calibration process ranging over test facilities, measurement devices, data acquisition systems, engineering software tools and the tool chain. JCUG provides their engine model to collaborators to make the communication among relevant researchers and engineers efficient.","","978-4-907764-36-4978-1-4244-7642-8978-4-907764-35","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5602416","automotive engine Calibration;calibration;modeling;calibration platform;engine model;JCUG","Engines;Calibration;Data models;Atmospheric modeling;Fuels;Automotive engineering;Optimization","automobile industry;calibration;control engineering computing;impact testing;internal combustion engines;production engineering computing;road vehicles","Shared Engine Model;Automotive Engine Calibration;JCUG;engine control;automotive industry;Japan Calibration User Group","","","4","","","","","","IEEE","IEEE Conferences"
"LOFT: Redundant Synchronization Event Removal for Data Race Detection","Y. Cai; W. K. Chan","NA; NA","2011 IEEE 22nd International Symposium on Software Reliability Engineering","","2011","","","160","169","Many happens-before based techniques for multithreaded programs implement vector clocks to track incrementally the causal relations among the synchronization operations acting on threads and locks. In these detectors, every such operation results in a vector-based assignment to a vector clock, even though the assigned value is the same as the value of the vector clock right before the assignment. The cost of such vector-based operations however grows with the number of threads and the amount of such operations. It is unclear to what extent redundant assignments can be removed. Whether two consecutive assignments to the same vector clock of a thread result in the same content critically depends on the operations on the locks occurred in between these assignments. In this paper, we systematically explore the said insight and quantify a sufficient condition that can soundly remove such operations without affecting the precision of such tracking. We applied our approach on Fast Track to formulate LOFT. We evaluate LOFT using the PARSEC benchmarking suite. The result shows that, on average, LOFT removes 58.0% of all such operations incurred by Fast Track, and runs 16.2% faster than the latter in tracking the causal relations among these operations.","2332-6549;1071-9458;1071-9458","978-1-4577-2060-4978-0-7695-4568","10.1109/ISSRE.2011.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6132964","data race detection;redundant operation optimization","Vectors;Clocks;Instruction sets;Detectors;Heuristic algorithms;Synchronization;Algorithm design and analysis","benchmark testing;multiprocessing systems;multi-threading;synchronisation","LOFT;redundant synchronization event removal;data race detection;multithreaded program;synchronization operation;vector-based assignment;vector clock;vector-based operation;PARSEC benchmarking suite;fast track;causal relations tracking","","8","24","","","","","","IEEE","IEEE Conferences"
"System identification on rotation motion pattern of trimaran unmanned surface vessel","Jian Cui; Pengyu Wang; Songlin Yang; Jingpin Yuan; Nan Zhao; Wei Wu","School of Naval Architecture and Ocean Engineering, Jiangsu University of Science and Technology, Zhenjiang, China; School of Naval Architecture and Ocean Engineering, Jiangsu University of Science and Technology, Zhenjiang, China; School of Naval Architecture and Ocean Engineering, Jiangsu University of Science and Technology, Zhenjiang, China; School of Naval Architecture and Ocean Engineering, Jiangsu University of Science and Technology, Zhenjiang, China; School of Naval Architecture and Ocean Engineering, Jiangsu University of Science and Technology, Zhenjiang, China; School of Naval Architecture and Ocean Engineering, Jiangsu University of Science and Technology, Zhenjiang, China","2010 IEEE International Conference on Progress in Informatics and Computing","","2010","1","","250","253","This article studied on the rotation motion of trimaran unmanned surface vessel. The experiment has been based on the trimaran unmanned surface vessel model. We selected the Genetic algorithm as the optimization method first, then identified the test data of trimaran unmanned surface vessel rotation motion by the program assisted with ISIGHT-FD software. Having compared the identified results with the test data and analyzed the error, we got the identified result matched the test result well from a set of compared data plots. The trimaran unmanned surface vessel model and its control system have important value for research and have good reference value for practical applications of unmanned surface vessels.","","978-1-4244-6789-1978-1-4244-6788-4978-1-4244-6787","10.1109/PIC.2010.5687456","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5687456","trimaran unmanned surface vessel;identification system;genetic algorithm;rotation test","Genetics;Trajectory;Planning;Global Positioning System;Legged locomotion;Engines","boats;control engineering computing;genetic algorithms;identification;remotely operated vehicles","system identification;rotation motion pattern;trimaran unmanned surface vessel;genetic algorithm;ISIGHT system FD software","","1","10","","","","","","IEEE","IEEE Conferences"
"Development of a cost effective bird's eye view parking assistance system","B. Thomas; R. Chithambaran; Y. Picard; C. Cougnard","Business Unit Automotive, NXP Semiconductors, Bangalore, India; Business Unit Automotive, NXP Semiconductors, Bangalore, India; Central R&amp;D, NXP Semiconductors, Caen, France; Central R&amp;D, NXP Semiconductors, Caen, France","2011 IEEE Recent Advances in Intelligent Computational Systems","","2011","","","461","466","Development of a cost effective parking assistance system including the features of the system and the implementation challenges are described. The details of the system including, system architecture, software architecture, features developed and performance optimization approaches are described. Detailed overview of the system architecture of the low cost prototype system including the key components used and the mapping of the functionality onto target processor are provided. A series of road tests were conducted for parking scenarios and confirmed the usability of the system in enhancing driving comfort. Finally, the results of the parking assistance system including the output image for multiple use case scenarios are shown.","","978-1-4244-9477-4978-1-4244-9478-1978-1-4244-9475-0978-1-4244-9476","10.1109/RAICS.2011.6069355","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6069355","fish eye models;projection;transformation;bird's eye view;system architecture","Birds;Cameras;Vehicles;Marine animals;Image color analysis;Computer architecture;Streaming media","computer vision;software architecture;traffic information systems","cost effective bird eye view parking assistance system development;system architecture;software architecture;features develop;performance optimization approach;target processor;road tests;driving comfort enhancement","","","23","","","","","","IEEE","IEEE Conferences"
"Taming the component timing: A CBD methodology for real-time embedded systems","M. G. Dixit; P. Dasgupta; S. Ramesh","India Science Lab, GM R&D, India; Indian Institute of Technology, Kharagpur, India; India Science Lab, GM R&D, India","2010 Design, Automation & Test in Europe Conference & Exhibition (DATE 2010)","","2010","","","1649","1652","The growing trend towards using component based design approach in embedded system development requires addressing newer system engineering challenges. These systems are usually time critical and require timing guarantees from components. The articulation of a desirable response bounds for the components is often ad-hoc and happens late in development. In this work, we present a formal methods based methodology for an early stage design space exploration. We focus on real-time response of a component as a basis for exploration and allow the developer model it using constant values or parameters. To quantify the parameters, we propose a novel constraint synthesis technique to correlate response times of interacting components. Finally, for system integration, we introduce a new notion of timing layout to specify time-budgeting for each component. The selection of a suitable layout can be made based on system optimization criteria. We have demonstrated our methodology on an automotive Adaptive Cruise Control feature.","1558-1101;1530-1591;1530-1591","978-3-9810801-6-2978-1-4244-7054-9978-3-9810801-6","10.1109/DATE.2010.5457077","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5457077","","Timing;Real time systems;Embedded system;Design engineering;Systems engineering and theory;Space exploration;Delay;Automotive engineering;Programmable control;Adaptive control","embedded systems;formal specification;object-oriented programming;software engineering","component timing;CBD methodology;real-time embedded system;component based design;embedded system development;system engineering;formal methods based methodology;constraint synthesis technique;system optimization criteria;automotive adaptive cruise control feature","","2","14","","","","","","IEEE","IEEE Conferences"
"Predicting the priority of a reported bug using machine learning techniques and cross project validation","M. Sharma; P. Bedi; K. K. Chaturvedi; V. B. Singh","Department of Computer Science, University of Delhi, India; Department of Computer Science, University of Delhi, India; Department of Computer Science, University of Delhi, India; Delhi College of Arts &amp; Commerce, University of Delhi, India","2012 12th International Conference on Intelligent Systems Design and Applications (ISDA)","","2012","","","539","545","In bug repositories, we receive a large number of bug reports on daily basis. Managing such a large repository is a challenging job. Priority of a bug tells that how important and urgent it is for us to fix. Priority of a bug can be classified into 5 levels from PI to P5 where PI is the highest and P5 is the lowest priority. Correct prioritization of bugs helps in bug fix scheduling/assignment and resource allocation. Failure of this will result in delay of resolving important bugs. This requires a bug prediction system which can predict the priority of a newly reported bug. Cross project validation is also an important concern in empirical software engineering where we train classifier on one project and test it for prediction on other projects. In the available literature, we found very few papers for bug priority prediction and none of them dealt with cross project validation. In this paper, we have evaluated the performance of different machine learning techniques namely Support Vector Machine (SVM), Naive Bayes (NB), K-Nearest Neighbors (KNN) and Neural Network (NNet) in predicting the priority of the newly coming reports on the basis of different performance measures. We performed cross project validation for 76 cases of five data sets of open office and eclipse projects. The accuracy of different machine learning techniques in predicting the priority of a reported bug within and across project is found above 70% except Naive Bayes technique.","2164-7143;2164-7143;2164-7151","978-1-4673-5119-5978-1-4673-5117-1978-1-4673-5118","10.1109/ISDA.2012.6416595","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6416595","Bug repositories;Bug priority;Triager;Classifiers;10-fold Cross Validation;SVM;KNN;Naive Bayes;Neural Net","Intelligent systems;Hafnium compounds;Decision support systems","belief networks;learning (artificial intelligence);neural nets;pattern classification;program debugging;program testing;project management;resource allocation;support vector machines","reported bug priority prediction;machine learning techniques;cross project validation;bug repositories;bug priority classification;bug prioritization;resource allocation;bug fix scheduling;bug prediction system;empirical software engineering;support vector machine;SVM;Naive Bayes classifier;k-nearest neighbor classifier;neural network;NNet;NB;KNN;open office projects;eclipse projects","","13","25","","","","","","IEEE","IEEE Conferences"
"Self-Organizing Map based operating regime estimation for state based control of wastewater treatment plants","P. Kern; C. Wolf; M. Bongards; T. D. Oyetoyan; S. McLoone","Institute for Automation & Industrial IT, Cologne University of Applied Sciences, Germany; Institute for Automation & Industrial IT, Cologne University of Applied Sciences, Germany; Institute for Automation & Industrial IT, Cologne University of Applied Sciences, Germany; Software Engineering Group, Department of Computer and Information Science (IDI), Norwegian University of Science and Technology, Trondheim, Norway; Department of Electronic Engineering, National University of Ireland Maynooth, Ireland","2011 International Conference of Soft Computing and Pattern Recognition (SoCPaR)","","2011","","","390","395","An optimal control of wastewater treatment plants (WWTP) has to account for changes in the bio-chemical state of the bioreactors. As many process variables of a WWTP are not measurable online, the development of an efficient control strategy is one of the greatest challenges in the optimization of WWTP operation. This paper presents an approach, which combines the use of Self-Organizing Maps (SOM) and a clustering algorithm to identify operational patterns in WWTP process data. These patterns provide a basis for the optimization of controller set points that are well suited for the previously identified operation regimes of the plant. The optimization is performed using Genetic Algorithms. This approach was developed, tested and validated on a simulation model based on the Activated Sludge Model No.1 (ASM1). The results of this state-based control indicate that the presented methodology is a promising and useful control strategy that is definitely able to address the distinctive energy and effluent limit challenges faced by WWTP operators.","","978-1-4577-1196-1978-1-4577-1195-4978-1-4577-1194","10.1109/SoCPaR.2011.6089275","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6089275","Wastewater Treatment;State based Control;Self Organizing Maps;Clustering;Optimization;Genetic Algorithm","Biological system modeling;Bioreactors;Vectors;Optimization;Couplings;Computational modeling;Clustering algorithms","bioreactors;control engineering computing;estimation theory;genetic algorithms;optimal control;process control;self-organising feature maps;sludge treatment;wastewater treatment","self-organizing map;operating regime estimation;state based control;wastewater treatment plants;optimal control;biochemical state;bioreactors;process variables;control strategy;WWTP operation;SOM;clustering algorithm;operational patterns;WWTP process data;controller set points;genetic algorithms;simulation model;activated sludge model No.1;ASM1;state-based control;distinctive energy;WWTP operators","","1","13","","","","","","IEEE","IEEE Conferences"
"A measurement system of cone meter differential pressure based on LabVIEW","F. Li; F. Dong; F. Zhang","Tianjin Key Laboratory of Process Measurement and Control, School of Electrical Engineering and Automation, Tianjin University, 300072, China; Tianjin Key Laboratory of Process Measurement and Control, School of Electrical Engineering and Automation, Tianjin University, 300072, China; Tianjin Key Laboratory of Process Measurement and Control, School of Electrical Engineering and Automation, Tianjin University, 300072, China","Proceedings of the 30th Chinese Control Conference","","2011","","","5815","5819","V type cone throttling device as new differential pressure flow sensor have been used in two-phase flow measurement. As a new instrument, virtual instrument combines computer resources, equipment monitoring and control hardware, data analysis and processing software, and graphical user interface software, which has been widely used in the actual test. In this research, the differential pressure signal generated by the V type cone throttling device is tested, The functions such as the real-time data acquisition, signal filtering, discharge coefficient and flow rate calculation, the real-time analysis and display are realized by programming with LabVIEW software, and finished the test by simulation experiment. Experimental results show that virtual instruments in V type cone flow measurement system can be convenient to monitored on-line experiment. It has played an important role to improving the reliability of experimental data and experimental efficiency, and optimizing the experimental design.","2161-2927;1934-1768;1934-1768","978-988-17255-9-2978-1-4577-0677","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6000595","Cone Flowmeter;Virtual Instruments;Differential Pressure Signal;Real-Time Analysis","Fluid flow measurement;Sea measurements;Instruments;Pressure measurement;Software;Real time systems;Signal to noise ratio","flow measurement;measurement systems;pressure measurement;shapes (structures);virtual instrumentation","measurement system;cone meter differential pressure;Lab VIEW;cone throttling device;flow sensor;flow measurement;virtual instrument;computer resources;equipment monitoring;control hardware;data analysis;processing software;graphical user interface;differential pressure signal;data acquisition;signal filtering;discharge coefficient;flow rate calculation;real-time analysis","","","9","","","","","","IEEE","IEEE Conferences"
"Precise calling context encoding","W. N. Sumner; Y. Zheng; D. Weeratunge; X. Zhang","Purdue University; Purdue University; Purdue University; Purdue University","2010 ACM/IEEE 32nd International Conference on Software Engineering","","2010","1","","525","534","Calling contexts are very important for a wide range of applications such as profiling, debugging, and event logging. Most applications perform expensive stack walking to recover contexts. The resulting contexts are often explicitly represented as a sequence of call sites and hence bulky. We propose a technique to encode the current calling context of any point during an execution. In particular, an acyclic call path is encoded into one number through only integer additions. Recursive call paths are divided into acyclic subsequences and encoded independently. We leverage stack depth in a safe way to optimize encoding: if a calling context can be safely and uniquely identified by its stack depth, we do not perform encoding. We propose an algorithm to seamlessly fuse encoding and stack depth based identification. The algorithm is safe because different contexts are guaranteed to have different IDs. It also ensures contexts can be faithfully decoded. Our experiments show that our technique incurs negligible overhead (1.89% on average). For most medium-sized programs, it can encode all contexts with just one number. For large programs, we are able to encode most calling contexts to a few numbers.","1558-1225;0270-5257","978-1-60558-719","10.1145/1806799.1806875","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062056","calling context;dynamic context sensitivity;profiling","Context;Encoding;Instruments;Decoding;Runtime;Testing;Legged locomotion","program debugging;program testing","precise calling context encoding;profiling;debugging;event logging;expensive stack walking;acyclic call path;integer addition;recursive call path;acyclic subsequence;stack depth based identification;medium-sized program","","3","23","","","","","","IEEE","IEEE Conferences"
"Safari: Function-level power analysis using automatic instrumentation","S. Wang; Youhuizi Li; W. Shi; Lingjun Fan; A. Agrawal","Department of Computer Science, Wayne State University, Detroit, Michigan, USA; Department of Computer Science, Wayne State University, Detroit, Michigan, USA; Department of Computer Science, Wayne State University, Detroit, Michigan, USA; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Intel Corp., USA","2012 International Conference on Energy Aware Computing","","2012","","","1","6","Resolving excessive power dissipation of modern computer systems has become a substantial challenge. However, few research projects have targeted on application power analysis or application-aware power management, which becomes a rising factor in energy efficient system design. In this paper, we describe and implement an application function (subroutine call) level profiler, Safari. It can be used to generate power profiles of each function in an automatic manner. The experiment results using NPB parallel benchmark suite show that Safari is able to collect function level run-time information with overhead (16% on average) comparable to gprof. The power profiling results can be used for code optimization, power-aware scheduling, or even computing resource billing for future research.","2381-0947","978-1-4673-5328-1978-1-4673-5326-7978-1-4673-5327","10.1109/ICEAC.2012.6471014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6471014","","Power dissipation;Instruments;Benchmark testing;Software;Data models;Computers;Switches","benchmark testing;computerised instrumentation;energy conservation;parallel processing;power aware computing;processor scheduling","Safari;function-level power analysis;automatic instrumentation;power dissipation;computer systems;application power analysis;application-aware power management;energy efficient system design;application function level profiler;NPB parallel benchmark suite;function level run-time information;gprof;power profiling;code optimization;power-aware scheduling;computing resource billing","","1","15","","","","","","IEEE","IEEE Conferences"
"Design techniques for increasing performance and resource utilization of reconfigurable soft CPUs","A. Wold; D. Koch; J. Torresen","Department of Informatics, University of Oslo, Norway; Department of Informatics, University of Oslo, Norway; Department of Informatics, University of Oslo, Norway","2012 IEEE 15th International Symposium on Design and Diagnostics of Electronic Circuits & Systems (DDECS)","","2012","","","50","55","Reconfigurable hardware allows application specific customization of soft microprocessors. Techniques such as removing unused instructions, software emulation of instructions, custom instruction set extensions, and run-time reconfigurable instructions have been suggested. However, the techniques have largely been studied separately from each other. The contribution of this paper is a classification method enabling integration of these techniques. This allows for generating an application specific microprocessor based system from a given program. The generated microprocessor is optimized with respect to performance per area. The improvement of our methodology is demonstrated for the CoreBench benchmark. The benefit of combining the removal of unused instructions (ISA subsetting) with software emulation of rarely used instructions is shown to increase performance while at the same time reducing resource requirements. Improvement in both area and performance is accomplished thorough simplifying the design allowing an increase in clock frequency for the synthesized soft CPU. Optimizing only by using custom instructions allowed a 12% increase in performance, but also increased resource usage by 6%. Software emulation combined with ISA subsetting allowed area savings of 7%, but only improved performance by 3%. By combining custom instructions, software emulation and ISA subsetting, we achieved an performance improvement of 15% while at the same time reducing resource requirements.","","978-1-4673-1188-5978-1-4673-1187-8978-1-4673-1186","10.1109/DDECS.2012.6219024","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6219024","ISA (Instruction set architecture) subsetting;Field Programmable Gate Array (FPGA);Instruction Set Extension (ISE);custom instruction;reconfigurable instruction;application specific microprocessor","Hardware;Emulation;Field programmable gate arrays;Kernel;Microprocessors;Benchmark testing","benchmark testing;instruction sets;multiprocessing systems;reconfigurable architectures","design technique;resource utilization;reconfigurable soft CPU;reconfigurable hardware;application specific customization;soft microprocessor;unused instruction removal;software emulation;custom instruction set extension;run-time reconfigurable instruction;classification method;application specific microprocessor based system;CoreBench benchmark;ISA subsetting;clock frequency;soft CPU synthesis;performance improvement;resource requirement reduction;instruction set architecture","","3","22","","","","","","IEEE","IEEE Conferences"
"Research on dynamic characteristics of the assembled beams","XiaoYichuan; WuLiangsheng; ZhouDashuai; WangZelin; ChuChengchun","College Of Mechanical Engineering And Applied, Electronics Technology, BeiJing University Of Technology, China; College Of Mechanical Engineering And Applied, Electronics Technology, BeiJing University Of Technology, China; College Of Mechanical Engineering And Applied, Electronics Technology, BeiJing University Of Technology, China; College Of Mechanical Engineering And Applied, Electronics Technology, BeiJing University Of Technology, China; College Of Mechanical Engineering And Applied, Electronics Technology, BeiJing University Of Technology, China","2011 International Conference on Electric Information and Control Engineering","","2011","","","3502","3505","The paper introduces a method combining the calculating result of Ansys which is a emulating software of finite element theory with the dynamic test to obtain the characteristics of the assembled beams, this method have high reliability because the two ways can validate each other, so it is very useful for designing and optimizing the assembled structures.","","978-1-4244-8039-5978-1-4244-8036-4978-1-4244-8038","10.1109/ICEICE.2011.5778296","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5778296","joint;the assembled beams;finite element analysis;excitation experiment","Analytical models;Vibrations;Finite element methods;Joints;Mechanical engineering;Materials;Powders","beams (structures);dynamic testing;finite element analysis;reliability","dynamic characteristics;assembled beams;ANSYS;finite element theory;dynamic test;reliability;assembled structures","","","","","","","","","IEEE","IEEE Conferences"
"An empirical validation of FindBugs issues related to defects","A. Vetro; M. Morisio; M. Torchiano","Politecnico di Torino; Politecnico di Torino; Politecnico di Torino","15th Annual Conference on Evaluation & Assessment in Software Engineering (EASE 2011)","","2011","","","144","153","Background: Effective use of bug finding tools promise to speed up the process of source code verification and to move a portion of discovered defects from testing to coding phase. However, many problems related to their usage, especially the large number of false positives, could easily hinder the potential benefits of such tools. Aims: Assess the percentage and type of issues of a popular bugfinding tool (FindBugs) that are actual defects. Method: We analyzed 301 Java Projects developed at a university with FindBugs, collecting the issues signalled on the source code. Afterwards, we checked the precision of issues with information on changes, we ranked and validated them using both manual inspection and validation with tests failures. Results: We observed that a limited set of issues have high precision and conversely we identified those issues characterized by low precision. We compared findings first with our previous experiment and then to related work: results are consistent with both of them. Conclusions: Since our and other empirical studies demonstrated that few issues are related to real defects with high precision, developers could enable only them (or prioritize), reducing the information overload of FindBugs and having the possibility to discover defects earlier. Furthermore, the technique presented in the paper can be adopted to other tools on a code base with tests to find issues with high precision that can be checked on code in production to find defects earlier.","","978-1-84919-509","10.1049/ic.2011.0018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6083173","","","data flow analysis;Java;program debugging;program verification;software tools","FindBugs issues;bug finding tools;source code verification;coding phase;Java projects","","6","","","","","","","IET","IET Conferences"
"Symbolic design space exploration for multi-mode reconfigurable systems","S. Wildermann; F. Reimann; D. Ziener; J. Teich","University of Erlangen-Nuremberg, Germany; University of Erlangen-Nuremberg, Germany; University of Erlangen-Nuremberg, Germany; University of Erlangen-Nuremberg, Germany","2011 Proceedings of the Ninth IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS)","","2011","","","129","138","In today's complex embedded systems not all applications are running all the time, but depend on the operational mode. By incorporating knowledge about the temporal behavior of such multi-mode systems, it is possible to share hardware by means of partial reconfiguration, and thus, reduce costs and improve performance. In this paper, we specify the temporal behavior of the functionality by applying known models based on state machines. In addition, we introduce an architectural model that allows to express the characteristics of nowadays partially reconfigurable architectures, focusing on FPGAs. We develop a symbolic encoding of this novel system specification, which allows to perform a unified system synthesis for allocation, binding, placement of partially reconfigurable modules, and routing the on-chip communication. The proposed encoding enables the use of sophisticated optimization techniques, coupling a SAT solver with a Multi-objective Evolutionary Algorithm. The proposed methodology is highly applicable for building multi-mode systems on advanced reconfigurable technology. We demonstrate this by experiments on test-cases from the image processing domain applying state-of-the-art technology. The results show the superiority of the presented approach in terms of run-time and quality of the found solutions compared to existing system synthesis approaches.","","978-1-4503-0715-4978-1-4503-0715-4978-1-4503-0712","10.1145/2039370.2039393","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062281","","Hardware;Resource management;Routing;Computer architecture;Tiles;System-on-a-chip;Switching circuits","encoding;evolutionary computation;field programmable gate arrays;finite state machines;optimisation;reconfigurable architectures;symbol manipulation","symbolic design space exploration;multimode reconfigurable system;operational mode;temporal behavior;partially reconfigurable architecture;state machine based model;FPGA;symbolic encoding;unified system synthesis;on-chip communication;optimization technique;SAT solver;multobjective evolutionary algorithm;image processing domain","","7","21","","","","","","IEEE","IEEE Conferences"
"Analysis and validation of IEC 61131-3 applications using a MDE approach","M. Marcos; E. Estévez; N. Iriondo; D. Orive","ETSI Bilbao, Basque Country University, Alameda de Urquijo s/n, 48013, Spain; ETSI Bilbao, Basque Country University, Alameda de Urquijo s/n, 48013, Spain; ETSI Bilbao, Basque Country University, Alameda de Urquijo s/n, 48013, Spain; ETSI Bilbao, Basque Country University, Alameda de Urquijo s/n, 48013, Spain","2010 IEEE 15th Conference on Emerging Technologies & Factory Automation (ETFA 2010)","","2010","","","1","8","Model-centric development is one of the hottest topics discussed in the software engineering field today. Combination of Model Driven Development and Model Driven Architecture can be extremely powerful for supporting the development cycle of the application. The use of these software methodologies within the Industrial Automation field allows meeting the requirement of current automation applications, such as flexibility, reuse, optimization and time and cost reduction. This paper focuses on the validation phase of a previously proposed Model-based Integrated Development Environment for Industrial Process Measurement and Control Systems (IPMCS). In particular, the validation tests for the industrial control system being designed are automatically generated from the application model. The tests are executed by the control co-simulation tool that closes the loop between the control model, running in the PLC environment, and the process model running in a simulation environment.","1946-0759;1946-0740;1946-0740","978-1-4244-6850-8978-1-4244-6848-5978-1-4244-6849","10.1109/ETFA.2010.5641238","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5641238","","Unified modeling language;Process control;Software;Automation;XML;Control engineering","IEC standards;measurement systems;process control;production engineering computing;programmable controllers;software architecture","IEC 61131-3 application;model-centric development;software engineering;model driven development;model driven architecture;industrial automation field;time reduction;cost reduction;model-based integrated development environment;industrial process measurement system;industrial process control system;control co-simulation tool;PLC environment;simulation environment","","10","31","","","","","","IEEE","IEEE Conferences"
"A novel, ferrofluid-cooled transformer. electromagnetic field and heat transfer by numerical simulation","A. M. Morega; M. Morega; L. Pîslaru-Dănescu; V. Stoica; F. Nouraş; F. D. Stoian","University POLITEHNICA of Bucharest, Faculty of Electrical Engineering, ROMANIA; University POLITEHNICA of Bucharest, Faculty of Electrical Engineering, ROMANIA; Microelectromechanical Department, National Institute for Electrical Engineering, ICPE-CA, Bucharest, ROMANIA; Microelectromechanical Department, National Institute for Electrical Engineering, ICPE-CA, Bucharest, ROMANIA; Research Development and Testing National Institute for Electrical Engineering, ICMET, Craiova, ROMANIA; University POLITEHNICA of Timişoara, Faculty of Mechanical Engineering, ROMANIA","2010 12th International Conference on Optimization of Electrical and Electronic Equipment","","2010","","","140","146","A ferrofluid-cooled low power, single-phased electric transformer was designed and prototyped with the aim of investigating the performance that such an apparatus may exhibit. The nanometric, colloidal, super-paramagnetic fluid used as coolant has specific electric, magnetic, and thermal properties, and presents an overall better stability and capacity to withstanding electromagnetic and thermal stress. This paper addresses also the electromagnetic and heat transfer processes that occur. First, the physical, mathematical, and numerical models are introduced. Numerical simulation results suggest that the magnetization body forces may add to the thermal, buoyancy body forces in providing for better heat transfer. To outline this, several numerical models that may conveniently be treated numerically within the current hardware and software limits, while still providing for satisfactory accuracy were developed. The results may be utilized also in the design phase of the transformer.","1842-0133;1842-0133","978-1-4244-7020-4978-1-4244-7019-8978-1-4244-7018","10.1109/OPTIM.2010.5510425","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5510425","","Electromagnetic fields;Electromagnetic heating;Heat transfer;Numerical simulation;Thermal stresses;Electromagnetic forces;Numerical models;Prototypes;Magnetic liquids;Coolants","heat transfer;magnetic fluids;magnetisation;numerical analysis;power transformers","ferrofluid-cooled transformer;electromagnetic field;heat transfer;single-phased electric transformer;paramagnetic fluid;magnetization body forces","","4","15","","","","","","IEEE","IEEE Conferences"
"Design and demonstration of a very high data rate multimedia HF communication system","I. Icart; J. -. Bernier; R. Elmostadi; R. Koch","Thales Communications and Security, France; Thales Belgium S.A., Belgium; Thales Communications and Security, France; Fraunhofer Institute IIS, Germany","12th IET International Conference on Ionospheric Radio Systems and Techniques (IRST 2012)","","2012","","","1","5","This paper presents and demonstrates a system for very high data rate (about 80 kbps) multimedia transmissions over HF channels. Optimizations are proposed at various levels of the OSI/ISO protocol stack, to enable efficient use of standard multimedia and data IPv6 applications in HF, in a way that is completely transparent for the end user. A demonstration test-bed was built, in order to evaluate the performance of the proposed system, through various scenarios derived from military requirements and operational needs. Experimental results obtained during laboratory tests and on-air demonstrations campaigns are reported in the final part of the present paper.","","978-1-84919-623","10.1049/cp.2012.0380","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6251387","Wideband HF;JPIP;IP;software defined radio;DRM","","multimedia communication;protocols;wireless channels","very high data rate multimedia HF communication system;HF channels;OSI-ISO protocol stack;data IPv6 applications;demonstration test-bed;on-air demonstrations campaigns;laboratory tests","","1","","","","","","","IET","IET Conferences"
"Self-adaptive differential evolution algorithm for economic dispatch with transmission losses consideration","T. Sum-Im","Department of Electrical Engineering, Faculty of Engineering, Srinakharinwirot University, Nakhon Nayok, Thailand","TENCON 2010 - 2010 IEEE Region 10 Conference","","2010","","","90","95","In this paper, a self-adaptive differential evolution algorithm (SaDEA) is proposed for solving conventional economic dispatch (ED) problem with transmission losses consideration. The purpose of ED problem is to minimize the total fuel cost of thermal power plants associated with the technical operation and economical constraints. The software development has been performed within the mathematical programming environment of MATLAB in this work. The efficiency of the proposed methodology is initially demonstrated via the analysis of IEEE 30-bus test case. A detailed comparative study among Lambda iteration, conventional genetic algorithm (CGA), tabu search/simulated annealing (TS/SA), ant colony search algorithm (ACSA) and the proposed method is presented. From the experimental results, the proposed method has achieved solutions with good accuracy, stable convergence characteristics, simple implementation and satisfactory computational time.","2159-3450;2159-3442","978-1-4244-6890-4978-1-4244-6889-8978-1-4244-6888","10.1109/TENCON.2010.5685839","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5685839","Differential Evolution Algorithm;Economic Dispatch;Power System Optimization;Artificial Intelligence","Optimization;Propagation losses;Generators;Production;Power generation;Convergence;Fuels","costing;genetic algorithms;iterative methods;power generation dispatch;power generation economics;power transmission economics;search problems;simulated annealing;thermal power stations","self-adaptive differential evolution algorithm;economic dispatch;transmission loss consideration;SaDEA;ED problem;thermal power plants;technical operation;economical constraints;fuel cost;software development;mathematical programming environment;Matlab;IEEE 30-bus test case;Lambda iteration;conventional genetic algorithm;tabu search-simulated annealing;ant colony search algorithm;ACSA","","6","25","","","","","","IEEE","IEEE Conferences"
"The design of general development model based on energy information management system","Ma Haitao; You Wen; Wang Huan","College of Electrical&amp;Electronic Engineering, Changchun University of Technology, 130012, China; College of Electrical&amp;Electronic Engineering, Changchun University of Technology, 130012, China; Anshan Iron and Steel Mining Group, 114005, China","2010 International Conference on Computer, Mechatronics, Control and Electronic Engineering","","2010","3","","369","371","The system start with 6 opening of dagushan mineral separation factory NO 08 substations. the high and low voltage electrical equipment-large and medium capacity load of hyperbaric chamber and the electromagnetic station are looked as the main control object, setting up a reasonable charge collection point, using the current / voltage sensors and digital power meter as the main testing device, using RS-485 communication method, based on the principle of the same communication protocol, real-time access to electricity data from the scene collector, the collector transmit data to the sub-station control by using OPC technology,with optical fiber and factory management network, the information is focused to the control center station, followed to process by upper management, analysis and optimization software, and realize electricity consumption of the device to monitor, statistics, output and energy consumption analysis, process optimization and to realize energy efficiency.","2159-6026;2159-6034","978-1-4244-7958-0978-1-4244-7957-3978-1-4244-7955","10.1109/CMCE.2010.5610290","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5610290","energy information management;mineral separation factory;model;save energy","Loss measurement;Servers","energy consumption;energy management systems;factory automation;mineral processing industry;optimisation;peripheral interfaces;substations","energy information management system;substations;hyperbaric chamber;electromagnetic station;RS-485 communication method;communication protocol;optical fiber;factory management network;electricity consumption;mineral separation;energy consumption;optimization;energy efficiency;mineral separation factory;control center station","","","4","","","","","","IEEE","IEEE Conferences"
"NBTI-aware data allocation strategies for scratchpad memory based embedded systems","C. Ferri; D. Papagiannopoulou; R. I. Bahar; A. Calimera","School of Engineering, Brown University Providence, RI 02912; School of Engineering, Brown University Providence, RI 02912; School of Engineering, Brown University Providence, RI 02912; Dipartimento di Automatica e Informatica, Politecnico di Torino, Torino, Italy 10129","2011 12th Latin American Test Workshop (LATW)","","2011","","","1","6","While performance and power continue to be important metrics for embedded systems, as CMOS technologies continue to shrink, new metrics such as variability and reliability have emerged as limiting factors in the design of modern embedded systems. In particular, the reliability impact of pMOS negative bias temperature instability (NBTI) has become a serious concern. Recent works have shown how conventional leakage optimization techniques can help mitigate NBTI-induced aging effects on cache memories. In this paper we focus specifically on scratchpad memory (SPM) and present novel software approaches as a means of alleviating the NBTI-induced aging effects. In particular, we demonstrate how intelligent software directed data allocation strategies can extend the lifetime of partitioned SPMs by means of distributing the idleness across the memory sub-banks.","2373-0862","978-1-4577-1490-0978-1-4577-1489-4978-1-4577-1488","10.1109/LATW.2011.5985932","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5985932","","Aging;Degradation;Resource management;Reliability;Embedded systems;Libraries","CMOS memory circuits;embedded systems;integrated circuit reliability;SRAM chips","NBTI-aware data allocation strategy;scratchpad memory;embedded system;CMOS technology;reliability;pMOS negative bias temperature instability;pMOS NBTI;leakage optimization technique;NBTI-induced aging effect;cache memory;SPM;software directed data allocation strategy;memory subbank","","13","20","","","","","","IEEE","IEEE Conferences"
"Project-driven teaching model for web database course","Jieru Lu; Nigang Sun; Suhong Wang","School of Information Science &amp; Engineering, Changzhou University, China; School of Information Science &amp; Engineering, Changzhou University, China; Department of Neuroscience, The Third Affiliated Hospital of Soochow University, Changzhou, China","2012 International Conference on Computer Science and Information Processing (CSIP)","","2012","","","163","166","According to the experience of developing the faculty management system and assessment on setting of institutions, processes of business, requirements of users, a model of faculty management information system is designed based on theories of software engineering and database technology. Using Struts2, Spring and Hibernate framework technology of J2EE, this system is flexible, easy to expand and maintain. The performance test shows that after optimization of database, the processing efficiency of system is improved.","","978-1-4673-1411-4978-1-4673-1410-7978-1-4673-1409","10.1109/CSIP.2012.6308820","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6308820","J2EE;System Architecture;Faculty management information system","Lead;Collaboration","computer aided instruction;database management systems;educational courses;Internet;management information systems;software engineering;teaching","project-driven teaching model;Web database course;business processes;institutions setting;faculty management information system;software engineering;database technology;Struts2 framework technology;Hibernate framework technology;Spring framework technology;J2EE","","","6","","","","","","IEEE","IEEE Conferences"
"Eyecharts: Constructive benchmarking of gate sizing heuristics","P. Gupta; A. B. Kahng; A. Kasibhatla; P. Sharma","University of California, Los Angeles; University of California, San Diego; University of California, Los Angeles; Freescale Semiconductor","Design Automation Conference","","2010","","","597","602","Discrete gate sizing is one of the most commonly used, flexible, and powerful techniques for digital circuit optimization. The underlying problem has been proven to be NP-hard. Several (suboptimal) gate sizing heuristics have been proposed over the past two decades, but research has suffered from the lack of any systematic way of assessing the quality of the proposed algorithms. We develop a method to generate benchmark circuits (called eyecharts) of arbitrary size along with a method to compute their optimal solutions using dynamic programming. We evaluate the suboptimalities of some popular gate sizing algorithms. Eyecharts help diagnose the weaknesses of existing gate sizing algorithms, enable systematic and quantitative comparison of sizing algorithms, and catalyze further gate sizing research. Our results show that common sizing methods (including commercial tools) can be suboptimal by as much as 54% (V<sub>t</sub>-assignment), 46% (gate sizing) and 49% (gate-length biasing) for realistic libraries and circuit topologies.","0738-100X;0738-100X","978-1-4503-0002-5978-1-4244-6677-1978-1-4503-0002","10.1145/1837274.1837421","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5523536","Gate sizing;benchmarking;dynamic programming","Circuit topology;Dynamic programming;Algorithm design and analysis;Logic;Software libraries;Very large scale integration;Threshold voltage;Delay;Permission;Benchmark testing","circuit complexity;circuit optimisation","eyecharts;constructive benchmarking;discrete gate sizing;digital circuit optimization;NP-hard problem;suboptimal gate sizing heuristics;dynamic programming","","6","27","","","","","","IEEE","IEEE Conferences"
"Approximately Orchestrated Routing and Transportation Analyzer: Large-scale traffic simulation for autonomous vehicles","D. Carlino; M. Depinet; P. Khandelwal; P. Stone","Department of Computer Science, The University of Texas at Austin, 78712, USA; Department of Computer Science, The University of Texas at Austin, 78712, USA; Department of Computer Science, The University of Texas at Austin, 78712, USA; Department of Computer Science, The University of Texas at Austin, 78712, USA","2012 15th International IEEE Conference on Intelligent Transportation Systems","","2012","","","334","339","Autonomous vehicles have seen great advancements in recent years, and such vehicles are now closer than ever to being commercially available. The advent of driverless cars provides opportunities for optimizing traffic in ways not possible before. This paper introduces an open source multiagent microscopic traffic simulator called AORTA, which stands for Approximately Orchestrated Routing and Transportation Analyzer, designed for optimizing autonomous traffic at a city-wide scale. AORTA creates scale simulations of the real world by generating maps using publicly available road data from OpenStreetMap (OSM). This allows simulations to be set up through AORTA for a desired region anywhere in the world in a matter of minutes. AORTA allows for traffic optimization by creating intelligent behaviors for individual driver agents and intersection policies to be followed by these agents. These behaviors and policies define how agents interact with one another, control when they cross intersections, and route agents to their destination. This paper demonstrates a simple application using AORTA through an experiment testing intersection policies at a city-wide scale.","2153-0017;2153-0009;2153-0009","978-1-4673-3063-3978-1-4673-3064-0978-1-4673-3062","10.1109/ITSC.2012.6338701","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6338701","","Roads;Vehicles;Mobile robots;Generators;Humans;Delay;Data models","digital simulation;multi-agent systems;optimisation;public domain software;road traffic;traffic engineering computing","large-scale traffic simulation;autonomous vehicles;driverless cars;open source multiagent microscopic traffic simulator;AORTA;approximately orchestrated routing and transportation analyzer;autonomous traffic optimization;intersection policies;OSM;OpenStreetMap;city-wide scale","","6","22","","","","","","IEEE","IEEE Conferences"
"Statistical fault localization using execution sequence","Zunwen You; Zengchang Qin; Zheng Zheng","School of Automation Science and Electrical Engineering (ASEE), Beihang University, China; School of Automation Science and Electrical Engineering (ASEE), Beihang University, China; School of Automation Science and Electrical Engineering (ASEE), Beihang University, China","2012 International Conference on Machine Learning and Cybernetics","","2012","3","","899","905","Fault localization is one of the most expensive and time consuming jobs in program debugging. Many approaches were proposed in order to locate faults effectively and efficiently. In this paper, we proposed a novel statistical approach by exploiting the statistical behavior of two sequentially connected predicates in the execution. If the predicates are regarded as the vertices of a graph, then the edges of the graph represent the transition of two sequential predicates in the execution trace of the program. The label of each edge is the frequency of each transition. For each edge, we apply hypothesis testing to evaluate the difference between edge evaluation bias in the passed runs and that in the failed runs. The edges are ranked according to the fault relevance score obtained from the hypothesis testing. The experimental results on Siemens suite show that the our proposed predicate-based fault localization method outperforms other well-used statistical fault localization techniques.","2160-1348;2160-133X;2160-133X","978-1-4673-1487-9978-1-4673-1484-8978-1-4673-1486","10.1109/ICMLC.2012.6359473","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6359473","Statistical fault localization;Predicate;Hypothesis testing;Execution sequence","Abstracts","fault diagnosis;graph theory;program debugging;software fault tolerance;statistical analysis","execution sequence;program debugging;statistical approach;statistical behavior;sequentially connected predicates;graph vertices;sequential predicates;execution trace;hypothesis testing;edge evaluation bias;fault relevance score;Siemens suite;predicate-based fault localization method;statistical fault localization techniques","","","14","","","","","","IEEE","IEEE Conferences"
"Digital microfluidic biochips: Recent research and emerging challenges","T. Ho; K. Chakrabarty; P. Pop","Dept. of Computer Science and Information Engineering, National Cheng Kung Univ., Tainan, Taiwan; Department of Electrical and Computer Engineering, Duke University, Durham, NC, USA; DTU Informatics, Technical University of Denmark Kgs. Lyngby, Denmark","2011 Proceedings of the Ninth IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS)","","2011","","","335","343","Microfluidic biochips are replacing the conventional biochemical analyzers, and are able to integrate on-chip all the basic functions for biochemical analysis. The “digital” microfluidic biochips (DM-FBs) are manipulating liquids not as a continuous flow, but as discrete droplets on a two-dimensional array of electrodes. Basic mi-crofluidic operations, such as mixing and dilution, are performed on the array, by routing the corresponding droplets on a series of electrodes. The challenges facing biochips are similar to those faced by microelectronics some decades ago. To meet the challenges of increasing design complexity, computer-aided-design (CAD) tools are being developed for DMFBs. This paper provides an overview of DMFBs and describes emerging CAD tools for the automated synthesis and optimization of DMFB designs, from fluidic-level synthesis and chip-level design to testing. Design automations are expected to alleviate the burden of manual optimization of bioassays, time-consuming chip designs, and costly testing and maintenance procedures. With the assistance of CAD tools, users can concentrate on the development and abstraction of nanoscale bioassays while leaving chip optimization and implementation details to CAD tools.","","978-1-4503-0715-4978-1-4503-0715-4978-1-4503-0712","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062306","Microfluidics;biochips;design automation","Electrodes;Routing;Design automation;Arrays;Pins;Wires;Mixers","biochemistry;bioMEMS;computer aided analysis;electrodes;lab-on-a-chip;microfluidics","digital microfluidic biochips;biochemical analyzers;integrate on-chip;biochemical analysis;continuous flow;discrete droplets;two-dimensional array;electrodes;mixing;dilution;microelectronics;computer-aided-design tools;design complexity;automated synthesis;fluidic-level synthesis;chip-level design;manual optimization;bioassays;time-consuming chip designs;nanoscale bioassays;chip optimization","","","50","","","","","","IEEE","IEEE Conferences"
"Early Prediction of Hardware Complexity in HLL-to-HDL Translation","A. Cilardo; P. Durante; C. Lofiego; A. Mazzeo","NA; NA; NA; NA","2010 International Conference on Field Programmable Logic and Applications","","2010","","","483","488","Early prediction of hardware complexity is essential in driving hardware/software partitioning and the automatic generation of HDL descriptions from high-level code. In fact, early prediction helps estimate the “hardware cost” of a given high-level code segment before the actual synthesis, dramatically reducing the time required for an exhaustive exploration of different design choices. Clearly, this early estimation is inherently influenced by the specific toolchain for HLL-to-HDL translation. As a consequence, suitable early prediction metrics should be studied and carefully selected for each given toolchain. In this paper, we propose a general framework for the systematic study of such metrics. Unlike some previous works, the proposed framework is not specific to a given toolchain as it lets designers plug their own synthesis tool and characterize its behaviour in order to identify the most effective metrics to be used during the design space exploration. The framework is developed on top of the LLVM compiler infrastructure along with the R statistical package used to perform regression analysis. For a specific HLL-to-HDL compiler chosen for tests, we collected extensive experimental results on a large base of benchmarks, which show interesting accuracy improvements over some related work previously presented and confirm the effectiveness of the framework in deriving a characterization of the underlying hardware compiler.","1946-1488;1946-147X;1946-1488","978-1-4244-7843-9978-1-4244-7842-2978-0-7695-4179","10.1109/FPL.2010.97","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5694298","","Measurement;Hardware;Estimation;Software;Benchmark testing;Complexity theory;Optimization","hardware description languages;high level languages;program compilers;regression analysis","hardware complexity;HLL-to-HDL translation;HDL description;high level code segment;synthesis tool;space exploration;LLVM compiler infrastructure;R statistical package;regression analysis;hardware compiler;hardware-software partitioning;early estimation;early prediction metrics","","11","17","","","","","","IEEE","IEEE Conferences"
"On fast coding tree block and mode decision for high-Efficiency Video Coding (HEVC)","H. L. Tan; F. Liu; Y. H. Tan; C. Yeo","Signal Processing Department, Institute for Infocomm Research, 1 Fusionopolis Way, #21-01 Connexis, South Tower, Singapore 138632; School of Electrical & Electronic Engineering, Nanyang Technological University, Singapore; Signal Processing Department, Institute for Infocomm Research, 1 Fusionopolis Way, #21-01 Connexis, South Tower, Singapore 138632; Signal Processing Department, Institute for Infocomm Research, 1 Fusionopolis Way, #21-01 Connexis, South Tower, Singapore 138632","2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2012","","","825","828","In the current HEVC test model (HM), a quad-tree based coding tree block (CTB) representation is used to signal mode, partition, prediction and residual information. The large number of combinations of quad-tree partitions and modes to be tested during rate-distortion optimization (RDO) results in a high encoding complexity. In this paper, we investigate and compare a variety of algorithms for fast CTB and mode decision. Experimental results from HM4-based implementations show that different strategies can provide a range of complexity-performance trade-offs. In particular, our proposed CU Depth Pruning algorithm can reduce encoding time by about 10% with only 0.1% coding loss, while a combination of our proposed Early Partition Decision and an early CU termination approach can reduce encoding time by about 40% with about 1% coding loss.","2379-190X;1520-6149;1520-6149","978-1-4673-0046-9978-1-4673-0045-2978-1-4673-0044","10.1109/ICASSP.2012.6288011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6288011","High Efficiency Video Coding (HEVC);Coding Tree Block (CTB);Fast Mode Decision","Encoding;Video coding;Partitioning algorithms;Software;Optimization;Complexity theory;Indexes","quadtrees;video coding","coding tree block;fast mode decision;high-efficiency video coding;HEVC test model;CTB representation;quad-tree partition;rate-distortion optimization;RDO;encoding complexity;HM4-based implementation;CU Depth Pruning algorithm;encoding time;early partition decision;early CU termination approach","","26","12","","","","","","IEEE","IEEE Conferences"
"Linear Time Memory Consistency Verification","W. Hu; Y. Chen; T. Chen; C. Qian; L. Li","Institute of Computing Technology, Chinese Academy of Sciences, Beijing; Institute of Computing Technology, Chinese Academy of Sciences, Beijing; Institute of Computing Technology, Chinese Academy of Sciences, Beijing; Institute of Computing Technology, Chinese Academy of Sciences, Beijing; Institute of Computing Technology, Chinese Academy of Sciences, Beijing","IEEE Transactions on Computers","","2012","61","4","502","516","Verifying the execution of a parallel program against a given memory consistency model (memory consistency verification) is a crucial problem in the functional validation of Chip Multiprocessor (CMP). In the absence of additional information, the above problem is known to be NP-hard. By adopting the pending period information, this paper proposes the first linear-time software-based approach to memory consistency verification. Our approach relies on a novel technique called reusable cycle checking, which reuses the previous order information when repeatedly checking cycle at different frontiers. In the context of pending period information, this technique significantly reduces the overall computational costs required by cycle checking, enabling linear-time (in the number of memory operations) memory consistency verification for any given multicore system with a constant number of processors. From a practical perspective, an industrial memory consistency verification tool, named XCHECK, has been developed based on our approach. XCHECK is capable of working with neither test program constraint nor dedicated hardware support in postsilicon verifications of many multiprocessor systems. Experimental results show that XCHECK is 3-10 times faster than a state-of-art software-based approach. XCHECK has been integrated into the verification platforms for an industrial multicore processor Godson-3B, and found several bugs of the design.","0018-9340;1557-9956;2326-3814","","10.1109/TC.2011.41","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5710891","Memory consistency;physical time order;verification;global clock;pending period;reusable cycle checking.","Program processors;Complexity theory;Clocks;Hardware;Multiprocessing systems;Memory management","formal verification;microprocessor chips;multiprocessing systems;optimisation;parallel memories;software reusability","memory consistency verification;parallel program;memory consistency model;chip multiprocessor;NP-hard problem;linear-time software based approach;reusable cycle checking;XCHECK;industrial multicore processor","","9","51","","","","","","IEEE","IEEE Journals & Magazines"
"A generalized control-flow-aware pattern recognition algorithm for behavioral synthesis","J. Cong; H. Huang; W. Jiang","Department of Computer Science, University of California, Los Angeles, USA; Department of Computer Science, University of California, Los Angeles, USA; Department of Computer Science, University of California, Los Angeles, USA","2010 Design, Automation & Test in Europe Conference & Exhibition (DATE 2010)","","2010","","","1255","1260","Pattern recognition has many applications in design automation. A generalized pattern recognition algorithm is presented in this paper which can efficiently extract similar patterns in programs. Compared to previous pattern-based techniques, our approach overcomes their limitation in handling control-flow-aware patterns, and leads to more opportunities for optimization. Our algorithm uses a feature-based filtering approach for fast pruning, and an elegant graph similarity metric called the generalized edit distance for measuring variations in CDFGs. Furthermore, our pattern recognition algorithm is applied to solve the area optimization problem in behavioral synthesis. Our experimental results show up to a 40% area reduction on a set of real-world benchmarks with a moderate 9% latency overhead, compared to synthesis results without pattern extractions; and up to a 30% area reduction, compared to the results using only data-flow patterns.","1558-1101;1530-1591;1530-1591","978-3-9810801-6-2978-1-4244-7054-9978-3-9810801-6","10.1109/DATE.2010.5456999","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5456999","Behavioral Synthesis;control flow;pattern;feature","Pattern recognition;Circuit synthesis;Automatic control;Flow graphs;Clustering algorithms;Data mining;Filtering;Pattern matching;Computer science;Application software","data flow graphs;filtering theory;pattern recognition","generalized control-flow-aware pattern recognition algorithm;behavioral synthesis;design automation;pattern-based techniques;feature-based filtering approach;elegant graph similarity metric;generalized edit distance;optimization problem;data-flow patterns","","5","18","","","","","","IEEE","IEEE Conferences"
"The Significance of CMP Cache Sharing on Contemporary Multithreaded Applications","E. Z. Zhang; Y. Jiang; X. Shen","The College of William and Mary, Williamsburg; The College of William and Mary, Williamsburg; The College of William and Mary, Williamsburg","IEEE Transactions on Parallel and Distributed Systems","","2012","23","2","367","374","Cache sharing on modern Chip Multiprocessors (CMPs) reduces communication latency among corunning threads, and also causes interthread cache contention. Most previous studies on the influence of cache sharing have concentrated on the design or management of shared cache. The observed influence is often constrained by the reliance on simulators, the use of out-of-date benchmarks, or the limited coverage of deciding factors. This paper describes a systematic measurement of the influence with most of the potentially important factors covered. The measurement shows some surprising results. Contrary to commonly perceived importance of cache sharing, neither positive nor negative effects from the cache sharing are significant for most of the program executions in the PARSEC benchmark suite, regardless of the types of parallelism, input data sets, architectures, numbers of threads, and assignments of threads to cores. After a detailed analysis, we find that the main reason is the mismatch between the software design (and compilation) of multithreaded applications and CMP architectures. By performing source code transformations on the programs in a cache-sharing-aware manner, we observe up to 53 percent performance increase when the threads are placed on cores appropriately, confirming the software-hardware mismatch as a main reason for the observed insignificance of the influence from cache sharing, and indicating the important role of cache-sharing-aware transformations-a topic only sporadically studied so far-for exerting the power of shared cache.","1045-9219;1558-2183;2161-9883","","10.1109/TPDS.2011.130","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6095485","Shared cache;thread scheduling;parallel program optimizations;chip multiprocessors.","Instruction sets;Message systems;Benchmark testing;Libraries;Arrays;Systematics","cache storage;multiprocessing systems;multi-threading","CMP cache sharing;chip multiprocessor;multithreaded application;interthread cache contention;shared cache design;shared cache management;program execution;software-hardware mismatch;source code transformation;software design;parallelism","","6","34","","","","","","IEEE","IEEE Journals & Magazines"
"Fault Localization in Constraint Programs","N. Lazaar; A. Gotlieb; Y. Lebbah","NA; NA; NA","2010 22nd IEEE International Conference on Tools with Artificial Intelligence","","2010","1","","61","67","Constraint programs such as those written in high level modeling languages (e.g., OPL, ZINC, or COMET) must be thoroughly verified before being used in applications. Detecting and localizing faults is therefore of great importance to lower the cost of the development of these constraint programs. In a previous work, we introduced a testing framework called CPTEST enabling automated test case generation for detecting non-conformities. In this paper, we enhance this framework to introduce automatic fault localization in constraint programs. Our approach is based on constraint relaxation to identify the constraint that is responsible of a given fault. CPTEST is henceforth able to automatically localize faults in optimized OPL programs. We provide empirical evidence of the effectiveness of this approach on classical benchmark problems, namely Golomb rulers, n-queens, social golfer and car sequencing.","2375-0197;1082-3409","978-1-4244-8817","10.1109/ICTAI.2010.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5670017","Fault localization;Testing software;Constraint programs","Testing;Debugging;Atmospheric modeling;Approximation algorithms;Fault diagnosis;Couplings;Fault detection","constraint handling;fault location;fault tolerant computing;high level languages","fault localization;constraint programs;fault detection;CPTEST;OPL programs;Golomb rulers;n-queens;classical benchmark problems;social golfer;car sequencing","","3","16","","","","","","IEEE","IEEE Conferences"
"On the interplay of loop caching, code compression, and cache configuration","M. Rawlins; A. Gordon-Ross","Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611, USA; Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611, USA","16th Asia and South Pacific Design Automation Conference (ASP-DAC 2011)","","2011","","","243","248","Even though much previous work explores varying instruction cache optimization techniques individually, little work explores the combined effects of these techniques (i.e., do they complement or obviate each other). In this paper we explore the interaction of three optimizations: loop caching, cache tuning, and code compression. Results show that loop caching increases energy savings by as much as 26% compared to cache tuning alone and reduces decompression energy by as much as 73%.","2153-697X;2153-6961;2153-6961","978-1-4244-7516-2978-1-4244-7515-5978-1-4244-7514","10.1109/ASPDAC.2011.5722191","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5722191","","Tuning;Benchmark testing;Optimization;Energy consumption;Runtime;Software;Hardware","cache storage","loop caching;code compression;cache configuration;cache optimization techniques;cache tuning;decompression energy","","5","18","","","","","","IEEE","IEEE Conferences"
"Detecting Solid-State Disk Geometry for Write Pattern Optimization","C. Kuo; J. Hsieh; L. Chang","NA; NA; NA","2011 IEEE 17th International Conference on Embedded and Real-Time Computing Systems and Applications","","2011","2","","89","94","Solid-state disks use flash memory as their storage medium, and adopt a firmware layer that makes data mapping and wear leveling transparent to the hosts. Even though solid-state disks emulate a collection of logical sectors, the I/O delays of accessing all these logical sectors are not uniform because the management of flash memory is subject to many physical constraints of flash memory. This work proposes a collection of black-box tests can detect the geometry inside of a solid-state disk. The host system software can arrange data in the logical disk space according to the detected geometry information to match the host write pattern with the device characteristic for reducing the flash management overhead in solid-state disks.","2325-1301;2325-1271","978-1-4577-1118","10.1109/RTCSA.2011.76","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6029897","flash memory;solid-state disk;storage systems","Ash;Geometry;Time factors;Throughput;Switches;Computer architecture;Software","firmware;flash memories;geometry;storage management","write pattern optimization;flash memory;firmware;data mapping;wear leveling;logical disk space;geometry information;flash management;solid-state disk geometry detection","","","6","","","","","","IEEE","IEEE Conferences"
"Extension of 1-wire measuring system SenSys","J. Dudak; G. Gaspar; G. Michalconok","Inset s.r.o., Lucemburská 1170/7, Praha 3, Czech Republic; Slovak University of Technology in Bratislava, Faculty of Materials Science and Technology, Paulínska 16, Trnava, Slovak Republic; Slovak University of Technology in Bratislava, Faculty of Materials Science and Technology, Paulínska 16, Trnava, Slovak Republic","Proceedings of 15th International Conference MECHATRONIKA","","2012","","","1","4","Measuring systems in industrial environments often require a more types of sensors to refine the quality of environment. Distributed measurement system SenSys is designed to measure 1-wire sensors, but the current offer of sensors despite their undeniable advantages is too narrow. Due to the cost optimization of the necessary cabling of other types of sensors, it is appropriate to use or extend the existing 1-wire network. Extending the measurement system SenSys using solutions based on integrated circuit DS2438 manufactured by Maxim IC retain their benefits of parasitic power connection of sensors in the network with a unique identifier. An example other than measuring temperature is the relative air humidity measurement carried out in office spaces, production or test areas where this is an important parameter. In addition to the hardware extension, it is necessary to include the changes in software for measurement and presentation of the measured data.","","978-80-01-04987-7978-1-4673-0979-0978-80-01-04985","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6415032","1-wire;temperature;relative air humidity;SenSys","Temperature measurement;Temperature sensors;Humidity;Humidity measurement;Voltage measurement;Time measurement","humidity measurement;optimisation","1-wire measuring system;SenSys;industrial environments;quality of environment;distributed measurement system;cost optimization;parasitic power connection;air humidity measurement","","1","5","","","","","","IEEE","IEEE Conferences"
"Towards an Efficient Context-Aware System: Problems and Suggestions to Reduce Energy Consumption in Mobile Devices","J. F. M. Bernal; L. Ardito; M. Morisio; P. Falcarin","NA; NA; NA; NA","2010 Ninth International Conference on Mobile Business and 2010 Ninth Global Mobility Roundtable (ICMB-GMR)","","2010","","","510","514","Looking for optimizing the battery consumption is an open issue, and we think it is feasible if we analyze the battery consumption behavior of a typical context-aware application to reduce context-aware operations at runtime. This analysis is based on different context sensors configurations. Actually existing context-aware approaches are mainly based on collecting and sending context data to external components, without taking into account how expensive are these operations in terms of energy consumption. As a first result of our work in progress, we are proposing a way for reducing the context data publishing. We have designed a testing battery consumption architecture supported by Nokia Energy Profiler tool to verify consumption in different scenarios.","","978-1-4244-7424-0978-1-4244-7423-3978-0-7695-4084","10.1109/ICMB-GMR.2010.34","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5494822","context-awareness;energy consumption;mobile device","Energy consumption;Batteries;Runtime;Testing;Automation;Information analysis;Publishing;System performance;Software performance;Application software","energy consumption;mobile computing;power aware computing","context-aware system;energy consumption reduction;mobile devices;context data publishing;testing battery consumption architecture;context sensors configurations;Nokia energy profiler tool","","6","16","","","","","","IEEE","IEEE Conferences"
"Evaluation of FDTD modelling as a tool for predicting the response of UHF partial discharge sensors","A. M. Ishak; M. D. Judd; W. H. Siew; P. C. Baker","University of Strathclyde, 204 George St., Glasgow G1 1XW, UK; University of Strathclyde, 204 George St., Glasgow G1 1XW, UK; University of Strathclyde, 204 George St., Glasgow G1 1XW, UK; University of Strathclyde, 204 George St., Glasgow G1 1XW, UK","2012 IEEE International Symposium on Electrical Insulation","","2012","","","502","506","Ultra high frequency (UHF) partial discharge sensors are important tools for condition monitoring and fault location in high voltage equipment. There are many designs of UHF sensors which can detect electromagnetic waves that radiate from partial discharge sources. The general types of UHF PD sensors are disc, monopole, probe, spiral, and conical types with each type of sensor having different characteristics and applications. Computational modelling of UHF PD sensors using Finite-difference time-domain (FDTD) simulation can simplify the process of sensor design and optimisation, reducing the development cost for repeated testing (in order to select the best materials and designs for the sensors), and giving greater insight into how the mechanical design and mounting will influence frequency response. This paper reports on an investigation into the application of FDTD methods in modelling and calibrating UHF PD sensors. This paper focuses on the disc-type sensor where the sensor has been modelled in software and the predicted responses are compared with experimental measurements. Results indicate that the FDTD method can accurately predict the output voltages and frequency responses of disc-type sensors. FDTD simulation can reduce reliance upon costly experimental sensor prototypes, leading to quicker assessment of design concepts, improved capabilities and reduced development costs.","1089-084X;1089-084X","978-1-4673-0487-0978-1-4673-0488-7978-1-4673-0486","10.1109/ELINSL.2012.6251520","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6251520","calibration;FDTD simulation;partial discharge measurement;UHF sensors","Sensors;Finite difference methods;Partial discharges;Computational modeling;Time domain analysis;Sensitivity;Couplers","condition monitoring;electric sensing devices;electromagnetic waves;fault location;finite difference time-domain analysis;frequency response;optimisation;partial discharge measurement","UHF partial discharge sensors;FDTD modelling;finite-difference time-domain;response prediction;condition monitoring;fault location;high voltage equipment;electromagnetic waves;optimisation;disc-type sensor;frequency response","","5","17","","","","","","IEEE","IEEE Conferences"
"Correctness Verification of Generalized Algebraic Deadlock Avoidance Policies Through Mathematical Programming","S. Reveliotis; E. Roszkowska; J. Y. Choi","NA; NA; NA","IEEE Transactions on Automation Science and Engineering","","2010","7","2","240","248","Generalized algebraic deadlock avoidance policies (DAPs) for sequential resource allocation systems (RASs) have recently been proposed as an interesting extension of the class of algebraic DAPs, that maintains the analytical representation and computational simplicity of the latter, while it guarantees completeness with respect to the maximally permissive DAP. The authors' original work that introduced these policies also provided a design methodology for them, but this methodology is limited by the fact that it necessitates the deployment of the entire state space of the considered RAS. Hence, this paper seeks the development of an alternative computational tool that can support the synthesis of correct generalized algebraic DAPs, while controlling the underlying computational complexity. More specifically, the presented correctness verification test possesses the convenient form of a mixed integer programming (MIP) formulation that employs a number of variables and constraints polynomially related to the size of the underlying RAS, and it can be readily solved through canned optimization software. Furthermore, since generalized algebraic DAPs do not admit a convenient representation in the Petri net modeling framework, an additional contribution of the presented results is that they effect the migration of the relevant past insights and developments with respect to simpler DAP classes, from the representational framework of Petri nets to that of the Deterministic Finite-State Automata.","1545-5955;1558-3783","","10.1109/TASE.2009.2022985","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5196689","Deadlock avoidance;discrete-event systems;mathematical programming;resource allocation systems (RASs);supervisory control","System recovery;Mathematical programming;Digital audio players;Resource management;Design methodology;State-space methods;Computational complexity;Software testing;Linear programming;Polynomials","computational complexity;finite state machines;integer programming;operating systems (computers);Petri nets;resource allocation","correctness verification;generalized algebraic deadlock avoidance policies;mathematical programming;resource allocation systems;computational tool;computational complexity;mixed integer programming formulation;optimization software;Petri net modeling framework;deterministic finite state automata","","5","24","","","","","","IEEE","IEEE Journals & Magazines"
"Parallel processing for Fingerprint feature extraction","G. Indrawan; B. Sitohang; S. Akbar","Data &amp; Software Engineering Research Division, School of Electrical Engineering and Informatics, ITB, Jl. Ganesha No. 10 Bandung, West Java, Indonesia; Data &amp; Software Engineering Research Division, School of Electrical Engineering and Informatics, ITB, Jl. Ganesha No. 10 Bandung, West Java, Indonesia; Data &amp; Software Engineering Research Division, School of Electrical Engineering and Informatics, ITB, Jl. Ganesha No. 10 Bandung, West Java, Indonesia","Proceedings of the 2011 International Conference on Electrical Engineering and Informatics","","2011","","","1","6","Real time fingerprint identification based on original non-alphanumeric content matching shouldn't compromise with its speed factor. Real production-quality implementation usually equipped with specific machine architecture to optimize this factor. On this research, we focus on achieving higher speed on common machine with respect to fingerprint accuracy factor. We explore the possibility to optimize execution time of fingerprint minutiae-based feature extraction using parallel process. Hypothetically, in general using parallel process will be faster than using sequential process. Taking advantage of multicore processor technology, optimization of execution time of fingerprint feature extraction is conducted using parallel process by processor cores. We have identified the possibility to implement parallel process on 29 different routines that are executed once or more. We compare result of average execution time of a fingerprint feature extraction using parallel and sequential process on Set B of Database 1 of Fingerprint Verification Contest 2000, 2002, and 2004. The experiment confirms the hypothesis and gives consistent result, i.e. faster about 57%, 55%, and 60% for each tested database.","2155-6830;2155-6822;2155-6822","978-1-4577-0752-0978-1-4577-0753-7978-1-4577-0751","10.1109/ICEEI.2011.6021606","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6021606","extraction;feature;fingerprint;identification;minutiae","Fingerprint recognition;Histograms;Parallel processing;Feature extraction;Smoothing methods;Instruction sets;Bifurcation","feature extraction;fingerprint identification;multiprocessing systems;parallel processing","parallel processing;fingerprint identification;nonalphanumeric content matching;production-quality implementation;fingerprint accuracy factor;fingerprint minutiae-based feature extraction;multicore processor technology;processor cores;sequential process","","2","8","","","","","","IEEE","IEEE Conferences"
"Selection of optimal texture descriptors for retrieving ultrasound medical images","A. S. M. Sohail; P. Bhattacharya; S. P. Mudur; S. Krishnamurthy","Dept. of Computer Science and Software Engineering, Concordia University, Montreal, Canada; Dept. of Computer Science, University of Cincinnati, Ohio, USA; Dept. of Computer Science and Software Engineering, Concordia University, Montreal, Canada; Dept. of Obstetrics and Gynecology, Royal Victoria Hospital, Montreal, Canada","2011 IEEE International Symposium on Biomedical Imaging: From Nano to Macro","","2011","","","10","16","Although feature selection has been proven to be very effective in machine learning and pattern classification applications, it has not been widely practiced in the area of image annotation and retrieval. This paper presents a method of selecting a near optimal to optimal subset of statistical texture descriptors in efficient representation and retrieval of ultrasound medical images. An objective function combining the concept of between-class distance and within-class divergence among the training dataset has been proposed as the evaluation criteria of optimality. Searching for the selection of optimal subset of image descriptors has been performed using Multi-Objective Genetic Algorithm (MOGA). The proposed feature selection based approach of image annotation and retrieval has been tested using a database of 679 ultrasound ovarian images and satisfactory retrieval performance has been achieved. Besides, performance of ultrasound medical image retrieval with and without applying feature selection based image annotation technique has also been compared.","1945-8452;1945-7928;1945-7928","978-1-4244-4127-3978-1-4244-4128","10.1109/ISBI.2011.5872343","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5872343","texture descriptors;feature selection;medical image retrieval;multi-objective optimization","Biological cells;Biomedical imaging;Ultrasonic imaging;Image retrieval;Entropy;Feature extraction;Genetic algorithms","biomedical ultrasonics;data analysis;feature extraction;genetic algorithms;image classification;image representation;image retrieval;image texture;medical image processing;statistical analysis","ultrasound medical image retrieval;feature selection;machine learning;pattern classification;image annotation;statistical texture descriptors;image representation;training dataset;multiobjective genetic algorithm;ultrasound ovarian images","","2","13","","","","","","IEEE","IEEE Conferences"
"Outsourced XML Database: Query Assurance Optimization","A. Clarke; E. Pardede","NA; NA","2010 24th IEEE International Conference on Advanced Information Networking and Applications","","2010","","","1181","1188","The area of XML database outsourcing, whereby the data owner enlists an external service provider to manage the storage and retrieval of their database, has been of increasing interest in recent years due to the relatively inexpensive nature of hardware/bandwidth, compared to the higher expense of in-house expert staff/software. As such it has become increasingly practical to use outsourced database solutions. However, as the service provider may not be fully trusted, XML database outsourcing introduces several security concerns that are new or more complex than those encountered in traditional database implementations. These include: data confidentiality, privacy, secure auditing, query assurance and secure and efficient storage. Of particular importance due to its relevance to most outsourced database models is query assurance - ensuring the database responds correctly to queries. In this paper, we propose the use of temporary time stamps and hash granularity to increase the efficiency of query assurance. This approach is tested against real datasets of varying type and size. Further, we consider how best to create time stamps and the issues associated with expiring versus distributed time stamp models.","2332-5658;1550-445X;1550-445X","978-1-4244-6696-2978-1-4244-6695-5978-0-7695-4018","10.1109/AINA.2010.130","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5474846","Outsourced Database;XML Database;Query Assurance","XML;Databases;Outsourcing;Information retrieval;Hardware;Bandwidth;Data security;Data privacy;Secure storage;Testing","data privacy;database management systems;outsourcing;query processing;storage management;XML","outsourced XML database;query assurance optimization;XML database outsourcing;external service provider;storage management;information retrieval;outsourced database solutions;data confidentiality;data privacy;secure auditing;temporary time stamps;hash granularity;distributed time stamp models","","2","16","","","","","","IEEE","IEEE Conferences"
"ATHENa - Automated Tool for Hardware EvaluatioN: Toward Fair and Comprehensive Benchmarking of Cryptographic Hardware Using FPGAs","K. Gaj; J. Kaps; V. Amirineni; M. Rogawski; E. Homsirikamol; B. Y. Brewster","NA; NA; NA; NA; NA; NA","2010 International Conference on Field Programmable Logic and Applications","","2010","","","414","421","A fair comparison of functionally equivalent digital system designs targeting FPGAs is a challenging and time consuming task. The results of the comparison depend on the inherent properties of competing algorithms, as well as on selected hardware architectures, implementation techniques, FPGA families, languages and tools. In this paper, we introduce an open-source environment, called ATHENa for fair, comprehensive, automated, and collaborative hardware benchmarking of algorithms belonging to the same class. As our first goal, we select the benchmarking of algorithms belonging to the area of cryptography. Algorithms from this area have been shown to achieve significant speed-ups and security gains compared to software when implemented in FPGAs. The capabilities of our environment are demonstrated using three examples: two different hardware architectures of the current cryptographic hash function standard, SHA-256, and one architecture of a candidate for the new standard, Fugue. All source codes, testbenches, and configuration files necessary to repeat experiments described in this paper are made available through the project web site.","1946-1488;1946-147X;1946-1488","978-1-4244-7843-9978-1-4244-7842-2978-0-7695-4179","10.1109/FPL.2010.86","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5694286","open-source;performance evaluation;benchmark tool","Field programmable gate arrays;Hardware;Optimization;Cryptography;Algorithm design and analysis;Performance evaluation;Clocks","benchmark testing;cryptography;field programmable gate arrays;public domain software","cryptographic hardware;FPGA;hardware architectures;open-source environment;hardware benchmarking;hash function standard;SHA-256","","22","11","","","","","","IEEE","IEEE Conferences"
"Efficient software-based online phase classification","A. Sembrant; D. Eklov; E. Hagersten","Uppsala University, Department of Information Technology, P.O. Box 337, SE-751 05, Sweden; Uppsala University, Department of Information Technology, P.O. Box 337, SE-751 05, Sweden; Uppsala University, Department of Information Technology, P.O. Box 337, SE-751 05, Sweden","2011 IEEE International Symposium on Workload Characterization (IISWC)","","2011","","","104","115","Many programs exhibit execution phases with time-varying behavior. Phase detection has been used extensively to find short and representative simulation points, used to quickly get representative simulation results for long-running applications. Several proposals for hardware-assisted phase detection have also been proposed to guide various forms of optimizations and hardware configurations. This paper explores the feasibility of low overhead phase detection at runtime based entirely on existing features found in modern processors. If successful, such a technology would be useful for cache management, frequency adjustments, runtime scheduling and profiling techniques. The paper evaluates several existing and new alternatives for efficient runtime data collection and online phase detection. ScarPhase (Sample-based Classification and Analysis for Runtime Phases), a new online phase detection library, is presented. It makes extensive usage of the new hardware counter features, introduces a new phase classification heuristic and suggests a way to dynamically adjust the sample rate. ScarPhase exhibits runtime overhead below 2%.","","978-1-4577-2064-2978-1-4577-2063-5978-1-4577-2062","10.1109/IISWC.2011.6114207","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6114207","","Vectors;Radiation detectors;Runtime;Hardware;Phase detection;Benchmark testing;Support vector machine classification","program diagnostics;sampling methods","software-based online phase classification;time-varying behavior;hardware-assisted phase detection;low overhead phase detection;runtime data collection;ScarPhase;sample-based classification;sample-based analysis;runtime phases;online phase detection library","","13","32","","","","","","IEEE","IEEE Conferences"
"Towards Synthesizing Realistic Workload Traces for Studying the Hadoop Ecosystem","G. Wang; A. R. Butt; H. Monti; K. Gupta","NA; NA; NA; NA","2011 IEEE 19th Annual International Symposium on Modelling, Analysis, and Simulation of Computer and Telecommunication Systems","","2011","","","400","408","Designing cloud computing setups is a challenging task. It involves understanding the impact of a plethora of parameters ranging from cluster configuration, partitioning, networking characteristics, and the targeted applications' behavior. The design space, and the scale of the clusters, make it cumbersome and error-prone to test different cluster configurations using real setups. Thus, the community is increasingly relying on simulations and models of cloud setups to infer system behavior and the impact of design choices. The accuracy of the results from such approaches depends on the accuracy and realistic nature of the workload traces employed. Unfortunately, few cloud workload traces are available (in the public domain). In this paper, we present the key steps towards analyzing the traces that have been made public, e.g., from Google, and inferring lessons that can be used to design realistic cloud workloads as well as enable thorough quantitative studies of Hadoop design. Moreover, we leverage the lessons learned from the traces to undertake two case studies: (i) Evaluating Hadoop job schedulers, and (ii) Quantifying the impact of shared storage on Hadoop system performance.","1526-7539;2375-0227","978-1-4577-0468","10.1109/MASCOTS.2011.59","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6005384","Cloud computing;Performance analysis;Design optimization;Software performance modeling","Analytical models;Computational modeling;Google;Biological system modeling;Visualization;Memory management;Color","cloud computing;software performance evaluation","realistic workload traces;Hadoop ecosystem;cloud computing;cluster configuration;networking characteristics;Google;cloud workloads;Hadoop design;Hadoop job schedulers;shared storage;Hadoop system performance","","15","11","","","","","","IEEE","IEEE Conferences"
"Resilient workflows for cooperative design","T. Nguyên; L. Trifan; J. Désidéri","INRIA, 655, avenue de l'Europe, Montbonnot, 38334 Saint-Ismier, France; INRIA, 655, avenue de l'Europe, Montbonnot, 38334 Saint-Ismier, France; INRIA, 655, avenue de l'Europe, Montbonnot, 38334 Saint-Ismier, France","Proceedings of the 2011 15th International Conference on Computer Supported Cooperative Work in Design (CSCWD)","","2011","","","69","75","This paper describes an approach to extend process modeling for engineering design applications with fault-tolerance and resilience capabilities. It is based on the requirements for application-level error handling, which is a requirement for petascale and exascale scientific computing. This complements the traditional fault-tolerance management features provided by the existing hardware and distributed systems. These are often based on data and operations duplication and migration, and on checkpoint-restart procedures. We show how they can be optimized for high-performance infrastructures. This approach is applied on a prototype tested against industrial testcases for optimization of engineering design artifacts.his electronic document is a “live” template. The various components of your paper [title, text, heads, etc.] are already defined on the style sheet, as illustrated by the portions given in this document.","","978-1-4577-0387-4978-1-4577-0386-7978-1-4577-0385","10.1109/CSCWD.2011.5960057","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5960057","Workflows;fault-tolerance;resilience;distributed systems;process modeling;high-performance computing;engineering design","Software;Resilience;Hardware;Fault tolerance;Computational modeling;Fault tolerant systems;Optimization","checkpointing;design engineering;natural sciences computing;software fault tolerance","resilient workflows;cooperative design;process modeling;engineering design applications;application level error handling;exascale scientific computing;petascale scientific computing;fault tolerance management features;checkpoint restart procedures;high performance infrastructures;engineering design artifacts;electronic document","","","27","","","","","","IEEE","IEEE Conferences"
"Characterizing and predicting which bugs get fixed: an empirical study of Microsoft Windows","P. J. Guo; T. Zimmermann; N. Nagappan; B. Murphy","Stanford University; Microsoft Research; Microsoft Research; Microsoft Research","2010 ACM/IEEE 32nd International Conference on Software Engineering","","2010","1","","495","504","We performed an empirical study to characterize factors that affect which bugs get fixed in Windows Vista and Windows 7, focusing on factors related to bug report edits and relationships between people involved in handling the bug. We found that bugs reported by people with better reputations were more likely to get fixed, as were bugs handled by people on the same team and working in geographical proximity. We reinforce these quantitative results with survey feedback from 358 Microsoft employees who were involved in Windows bugs. Survey respondents also mentioned additional qualitative influences on bug fixing, such as the importance of seniority and interpersonal skills of the bug reporter. Informed by these findings, we built a statistical model to predict the probability that a new bug will be fixed (the first known one, to the best of our knowledge). We trained it on Windows Vista bugs and got a precision of 68% and recall of 64% when predicting Windows 7 bug fixes. Engineers could use such a model to prioritize bugs during triage, to estimate developer workloads, and to decide which bugs should be closed or migrated to future product versions.","1558-1225;0270-5257","978-1-60558-719","10.1145/1806799.1806871","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062117","","Computer bugs;Buildings;Databases;Open source software;Testing;Measurement","operating systems (computers);program debugging;statistical analysis","Windows Vista;Windows 7;Windows bugs;bug fixing;statistical model;probability","","49","42","","","","","","IEEE","IEEE Conferences"
"VAPRES: A Virtual Architecture for Partially Reconfigurable Embedded Systems","A. Jara-Berrocal; A. Gordon-Ross","NSF Center for High-Performance Reconfigurable Computing (CHREC), Department of Electrical and Computer Engineering, University of Florida, Gainesville, 32611, USA; NSF Center for High-Performance Reconfigurable Computing (CHREC), Department of Electrical and Computer Engineering, University of Florida, Gainesville, 32611, USA","2010 Design, Automation & Test in Europe Conference & Exhibition (DATE 2010)","","2010","","","837","842","Due to the runtime flexibility offered by field programmable gate arrays (FPGAs), FPGAs are popular devices for stream processing systems, since many stream processing applications require runtime adaptability (i.e. throughput, data transformations, etc.). FPGAs can offer this adaptability through runtime assembly of stream processing systems that are decomposed into hardware modules. Runtime hardware module assembly consists of dynamic hardware module replacement and hardware module communication reconfiguration. In this paper, we architect a flexible base embedded system amenable to runtime assembly of stream processing systems using custom communication architecture with dynamic streaming channel establishment between hardware modules. We present a hardware module swapping methodology that replaces hardware modules without stream processing interruption. Finally, we formulate two design flows, system and application construction, to provide system and application designer assistance.","1558-1101;1530-1591;1530-1591","978-3-9810801-6-2978-1-4244-7054-9978-3-9810801-6","10.1109/DATE.2010.5456934","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5456934","","Embedded system;Field programmable gate arrays;Hardware;Runtime;Assembly systems;Microprocessors;Embedded software;Design optimization;Computer architecture;Embedded computing","embedded systems;field programmable gate arrays;multiprocessing systems;reconfigurable architectures","VAPRES;virtual architecture;partially reconfigurable embedded systems;field programmable gate arrays;stream processing systems;hardware module swapping methodology","","7","12","","","","","","IEEE","IEEE Conferences"
"CAWDOR: Compiler Assisted Worm Defense","J. Yuan; R. Johnson","NA; NA","2012 IEEE 12th International Working Conference on Source Code Analysis and Manipulation","","2012","","","54","63","This paper explores how much the source code analysis can assist worm defense system. Previously-proposed worm defense systems have used disparate mechanisms to detect worms, analyze exploits, verify alerts, and apply mitigations. Furthermore, previous systems have not offered predictability, i.e. it is not possible to verify, in advance, that the defense system will never generate a mitigation that breaks the program. This paper describes a program transformation technique that makes collaborative worm defense systems easy to build, predictable and fast-responsive. Our transformation provides a single building block that can be used to perform worm detection, exploit analysis, alert verification, and mitigation application. In fact, our transformation makes most of these tasks trivial. Furthermore, software vendors and users can test, in advance, that the defense system will very unlikely apply a mitigation that breaks their software. Mitigations are vulnerability-specific not exploit-specific. Finally, our system can respond extremely quickly to a new worm. The exploit analysis becomes trivial so sentinel hosts can issue an alert the instant they detect a worm. We have implemented a prototype of our system based on the Jones and Kelly program transformation for memory safety. During normal operation, our system incurs only 5% overhead. We take advantage of static analysis to develop several optimizations and make the Jones and Kelly approach to memory safety efficient and practical.","","978-0-7695-4783-1978-1-4673-2398","10.1109/SCAM.2012.30","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6392102","","Grippers;Instruments;Solids;Optimization;Safety;Registers;Collaboration","computer viruses;formal verification;optimisation;program compilers;program diagnostics;source coding","source code analysis;compiler assisted worm defense;worm detection;program transformation technique;collaborative worm defense systems;exploit analysis;alert verification;vulnerability specific mitigation;memory safety;static analysis;optimization;Jones and Kelly approach","","2","26","","","","","","IEEE","IEEE Conferences"
"A Scalable, Lightweight WebOS Application Framework","D. A. Ostrowski","NA","2012 IEEE First International Conference on Internet Operating Systems","","2012","","","5","8","Frequently, as applications scale, they are considered in the context of a web OS-based architecture. In support of this goal, we present a lightweight framework designed as a middleware application. This architecture is highly influenced by hypermedia-based techniques, leveraging metadata in the context of HTML5. Our framework relies on a novel incorporation of a number of open source technologies including node.js and couchDB to support priorities of fast-prototyping, scalability and maintainability. Initial experiments have demonstrated that our approach performs effectively among the dynamics of our environment.","","978-1-4673-5092-1978-0-7695-4936","10.1109/ICIOS.2012.9","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6424523","Web OS;Internet Operating System;Hyperdata API;metadata","Computer architecture;Testing;Servers;Databases;Conferences;Internet;Security","hypermedia markup languages;Internet;meta data;middleware;network operating systems;software prototyping","lightweight WebOS application framework;Web OS-based architecture;middleware application;hypermedia-based techniques;metadata leveraging;HTML5;open source technologies;node.js;couchDB","","","43","","","","","","IEEE","IEEE Conferences"
"As-If Infinitely Ranged Integer Model","R. B. Dannenberg; W. Dormann; D. Keaton; R. C. Seacord; D. Svoboda; A. Volkovitsky; T. Wilson; T. Plum","NA; NA; NA; NA; NA; NA; NA; NA","2010 IEEE 21st International Symposium on Software Reliability Engineering","","2010","","","91","100","Integers represent a growing and underestimated source of vulnerabilities in C and C++ programs. This paper presents the As-if Infinitely Ranged (AIR) Integer model for eliminating vulnerabilities resulting from integer overflow, truncation, and unanticipated wrapping. The AIR Integer model either produces a value equivalent to that obtained using infinitely ranged integers or results in a runtime-constraint violation. With the exception of wrapping (which is optional), this model can be implemented by a C99-conforming compiler and used by the programmer with little or no change to existing source code. Fuzz testing of libraries that have been compiled using a prototype AIR integer compiler has been effective in discovering vulnerabilities in software with low false positive and false negative rates. Furthermore, the runtime overhead of the AIR Integer model is low enough that typical applications can enable it in deployed systems for additional runtime protection.","1071-9458;1071-9458;2332-6549","978-1-4244-9056-1978-0-7695-4255","10.1109/ISSRE.2010.29","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635126","security;programming languages;empirical study","Atmospheric modeling;Wrapping;Charge carrier processes;Optimization;Semantics;Runtime;Program processors","C++ language;program compilers;program diagnostics;security of data","as-if infinitely ranged integer model;C program vulnerability;C++ program vulnerability;integer overflow;truncation;unanticipated wrapping;runtime-constraint violation;C99-conforming compiler;fuzzy testing;runtime overhead;runtime protection","","13","14","","","","","","IEEE","IEEE Conferences"
"Investigation and evaluation of optimal registration for medical CT images","Y. Jin; G. Ma","School of Computer Science and Technology, Harbin Engineering University, School of Computer Science and Technology, Heilongjiang University, Harbin, China; School of Computer Science and Technology, Harbin Engineering University, Harbin, China","2010 3rd International Congress on Image and Signal Processing","","2010","6","","2794","2797","Due to its capability of maximizing the embedded information from a large amount of medical images captured from different imaging modalities and acquired from different scanning intervals, medical image registration plays an essential role in clinical applications including diagnosis, radiotherapy planning, assessment of patient's response to the treatment. An intensive research on registration methodologies has been conducted during the last three decades. However, by far there is no general software solution to clinically acceptable registration. To sort out more appropriate registration for clinical CT images, in this paper, the-state-of-the-art registration methods were tested and compared. In particular, the multiresolution deformable registration was investigated for accommodating a wider range of deformations. Our experimental results on the clinical CT datasets demonstrated the superior performance of the B-spline deformable registration by multiresolution scheme.","","978-1-4244-6516-3978-1-4244-6513-2978-1-4244-6515","10.1109/CISP.2010.5647407","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5647407","medical image;deformable registration;mutual information;optimization;multiresolution","Biomedical imaging;Computed tomography;Spline;Optimization;Spatial resolution;Image registration","computerised tomography;image registration;image resolution;medical image processing;patient treatment;splines (mathematics)","medical CT images;medical image registration;medical diagnosis;medical radiotherapy planning;patient treatment response assessment;multiresolution deformable image registration;B-spline deformable registration","","1","14","","","","","","IEEE","IEEE Conferences"
"Fast head-shoulder detection on mobile phones","J. Wang; H. Ma; A. Ming","Beijing Key Laboratory of Intelligent Telecommunications Software and Multimedia, Beijing University of Posts and Telecommunications, Beijing 100876, China; Beijing Key Laboratory of Intelligent Telecommunications Software and Multimedia, Beijing University of Posts and Telecommunications, Beijing 100876, China; Beijing Key Laboratory of Intelligent Telecommunications Software and Multimedia, Beijing University of Posts and Telecommunications, Beijing 100876, China","2011 IEEE International Conference on Consumer Electronics (ICCE)","","2011","","","205","206","Numerous digital cameras and modern phones have a face detection module, which is used to automatically focus (AF) and optimize exposure (AE). But the face detection will fail when person doesn't face the camera or the part of the face is occluded. In order to avoid such problems, we propose a fast head-shoulder detector, which uses Variable-size block Histograms of Orientated Gradients (VHOG) descriptors. AdaBoost-based feature selection algorithm and integral image representation are used to speed up the algorithm. The tests reveal that the method shows very good results and works efficiently in spite of the low computational power and memory available in mobile devices.","2158-4001;2158-3994;2158-3994;2158-3994","978-1-4244-8712-7978-1-4244-8711-0978-1-4244-8709-7978-1-4244-8710","10.1109/ICCE.2011.5722542","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5722542","","Histograms;Face detection;Mobile handsets;Cameras;Face;Detectors;Support vector machines","cameras;computer graphics;face recognition;feature extraction;gradient methods;image representation;mobile handsets;object detection","head-shoulder detection;mobile phone;digital camera;face detection;automatic focusing;occlusion;variable-size block histograms of orientated gradients;VHOG descriptor;AdaBoost-based feature selection algorithm;image representation","","1","4","","","","","","IEEE","IEEE Conferences"
"Reuse-aware modulo scheduling for stream processors","L. Wang; J. Xue; X. Yang","National Laboratory for Parallel and Distributed Processing, School of Computer, NUDT, China; Programming Languages & Compilers Group, School of Computer Science and Engineering, UNSW, Australia; National Laboratory for Parallel and Distributed Processing, School of Computer, NUDT, China","2010 Design, Automation & Test in Europe Conference & Exhibition (DATE 2010)","","2010","","","1112","1117","This paper presents reuse-aware modulo scheduling to maximizing stream reuse and improving concurrency for stream-level loops running on stream processors. The novelty lies in the development of a new representation for an unrolled and software-pipelined stream-level loop using a set of reuse equations, resulting in simultaneous optimization of two performance objectives for the loop, reuse and concurrency, in a unified framework. We have implemented this work in the compiler developed for our 64-bit FT64 stream processor. Our experimental results obtained on FT64 and by simulation using nine representative stream applications demonstrate the effectiveness of the proposed approach.","1558-1101;1530-1591;1530-1591","978-3-9810801-6-2978-1-4244-7054-9978-3-9810801-6","10.1109/DATE.2010.5456975","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5456975","","Processor scheduling;Concurrent computing;Streaming media;Kernel;Delay;Equations;Pipeline processing;Laboratories;Program processors;Distributed processing","media streaming;multiprocessing programs;multiprocessing systems;scheduling","reuse aware modulo scheduling;stream processor;stream reuse maximization;concurrency improvement;stream level loop;software pipelined stream level loop;simultaneous optimization;64-bit FT64 stream processor","","3","13","","","","","","IEEE","IEEE Conferences"
"A highly parallel design of image surface layout recovering on GPGPU","Guan-Ru Li; Bo-Cheng Charles Lai","Department of Electronics Engineering, National Chiao Tung University, Hsinchu, Taiwan; Department of Electronics Engineering, National Chiao Tung University, Hsinchu, Taiwan","Proceedings of Technical Program of 2012 VLSI Design, Automation and Test","","2012","","","1","4","Surface layout recovering helps computers understand the intricate information in an image by assigning local segments to different geometric classes. It greatly reduces the complexity of the following-up image processing and is widely used in various computer vision applications. However, the algorithm walks through every image pixel and imposes intensive computation requirement. Through comprehensive analysis on the execution behavior, this paper identifies significant parallelism inherent in the algorithm. With careful concerns on both multi-threaded software and parallel hardware, the optimized parallel design on a modern GPGPU has reached an average of 10.7X performance enhancement.","","978-1-4577-2081-9978-1-4577-2080-2978-1-4577-2079","10.1109/VLSI-DAT.2012.6212628","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6212628","","Image segmentation;Layout;Parallel processing;Algorithm design and analysis;Surface treatment;Instruction sets;Hardware","computer vision;graphics processing units;multi-threading","image surface layout recovery;image information;image processing;computer vision applications;image pixel;multithreaded software;optimized parallel design;general purpose graphic processing units;GPGPU","","","21","","","","","","IEEE","IEEE Conferences"
"Printing optimization and thermal sintering study of silver nano-ink for InkJet printing of solar electrodes using an industrial printhead","R. Shankar; A. Amert; J. J. Kellar; K. W. Whites","Nanoscience and Engineering Program, South Dakota School of Mines and Technology, Rapid City, SD 57701; Department of Electrical and Computer Engineering, South Dakota School of Mines and Technology, Rapid City, SD 57701; Ddepartment of Materials and Metallurgical Engineering, South Dakota School of Mines and Technology, Rapid City, SD 57701; Department of Electrical and Computer Engineering, South Dakota School of Mines and Technology, Rapid City, SD 57701","2012 IEEE Nanotechnology Materials and Devices Conference (NMDC2012)","","2012","","","15","20","We present a successful inkjet printing capability for solar electrodes with line widths below 100 μm. A Theologically tailored conductive silver nano-ink was created to meet the rheological requirement of industrial printheads (XAAR 1001). The jetting optimization for the printhead was done using a desktop R&D inkjet printer from PiXDRO (OTB Solar). Printer-integrated software allowed the optimization of droplet formation and droplet stability during the jetting process. Typical printing parameters such as greyscale, DPI variation, substrate heating, etc., were optimized to achieve high print quality. However, initial printing results showed poor line definition and inhomogeneous film thickness. To overcome this, we developed a multiple pass printing process that results in excellent line definition. We also performed statistical analysis to correlate the droplet size with printed feature size. The minimum feature size of -15 μm was achieved by jetting ~6 pi droplets. After printing, the films were sintered thermally to achieve metallization. Detailed TGA study showed that complete metallization was achieved at temperature 400°C. Adhesion tests performed using an ASTM standard tape test on printed pattern showed less than 5% of delamination or flaking. Finally, test prototypes of solar front electrodes were printed successfully on ITO and silicon substrates.","","978-1-4673-2871-5978-1-4673-2871-5978-1-4673-2870","10.1109/NMDC.2012.6527602","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6527602","","","electrodes;ink jet printing;silver;sintering","printing optimization;thermal sintering;silver;nanoink;solar electrodes;industrial printhead;inkjet printing capability;metallization;temperature 400 degC","","2","13","","","","","","IEEE","IEEE Conferences"
"Analysis of instruction-level vulnerability to dynamic voltage and temperature variations","A. Rahimi; L. Benini; R. K. Gupta","Department of Computer Science and Engineering, University of California, San Diego, USA; Dipartimento di Elettronica, Informatica e Sistemistica, Università di Bologna, Italy; Department of Computer Science and Engineering, University of California, San Diego, USA","2012 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2012","","","1102","1105","Variation in performance and power across manufactured parts and their operating conditions is an accepted reality in aggressive CMOS processes. This paper considers challenges and opportunities in identifying this variation and methods to combat it for improved computing systems. We introduce the notion of instruction-level vulnerability (ILV) to expose variation and its effects to the software stack for use in architectural/compiler optimizations. To compute ILV, we quantify the effect of voltage and temperature variations on the performance and power of a 32-bit, RISC, in-order processor in 65 nm TSMC technology at the level of individual instructions. Results show 3.4 ns (68FO4) delay variation and 26.7x power variation among instructions, and across extreme corners. Our analysis shows that ILV is not uniform across the instruction set. In fact, ILV data partitions instructions into three equivalence classes. Based on this classification, we show how a low-overhead robustness enhancement techniques can be used to enhance performance by a factor of 1.1x-5.5x.","1558-1101;1530-1591;1530-1591","978-1-4577-2145-8978-3-9810801-8","10.1109/DATE.2012.6176659","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6176659","","Clocks;Pipelines;Power system dynamics;Temperature sensors;Delay;Temperature distribution","CMOS integrated circuits;delays;equivalence classes","instruction-level vulnerability analysis;dynamic voltage;temperature variation;power across manufactured part;aggressive CMOS processing;computing system;ILV analysis;software stack effect;architectural-compiler optimization;RISC;in-order processor;TSMC technology;delay variation;power variation;instruction set;ILV data partition instruction;low-overhead robustness enhancement technique;equivalence class;word length 32 bit;size 65 nm;time 3.4 ns","","23","16","","","","","","IEEE","IEEE Conferences"
"Computer assisted prosthetic surgery, based on deformable surfaces and statistical modeling","A. I. Mitrea; A. Prodan; P. Mitrea; F. Gavrilaş; V. Oprea; O. M. Gurzău; R. Cîmpean; M. Mesaroş","Technical Univ. of Cluj Napoca; Univ. of Medicine and Pharmacy "Iuliu Hatieganu" Cluj Napoca; Technical Univ. of Cluj Napoca; Univ. of Medicine and Pharmacy "Iuliu Hatieganu" Cluj Napoca; Univ. of Medicine and Pharmacy "Iuliu Hatieganu" Cluj Napoca; Technical Univ. of Cluj Napoca; Univ. of Medicine and Pharmacy "Iuliu Hatieganu" Cluj Napoca; Univ. of Medicine and Pharmacy "Iuliu Hatieganu" Cluj Napoca","2010 IEEE International Conference on Automation, Quality and Testing, Robotics (AQTR)","","2010","2","","1","6","This paper is devoted to investigate numerical optimization algorithms of the theory of deformable surfaces and to emphasize some of their computerized applications in medical image analysis and prosthetic surgery. After defining the 3D variational deformable model, both in static and dynamic form, an algorithm for finding the optimal deformable surface is presented, together with estimations for its approximation error and a condition for its convergence. Finally, a statistical modeling and a corresponding algorithm applied in surgery are presented.","","978-1-4244-6725-9978-1-4244-6724-2978-1-4244-6723","10.1109/AQTR.2010.5520788","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5520788","","Prosthetics;Surgery;Deformable models;Computer applications;Application software;Biomedical imaging;Image analysis;Approximation algorithms;Heuristic algorithms;Approximation error","medical image processing;prosthetics;statistical analysis;surgery","computer assisted prosthetic surgery;deformable surface;statistical modeling;numerical optimization algorithm;medical image analysis;3D variational deformable model;approximation error","","","8","","","","","","IEEE","IEEE Conferences"
"The Simulation and Analysis of the Electromagnetic Shielding Effectiveness of PoE-Hub with Redundancy by FEM","F. Wei; T. Shi-hua; Z. Ya-chao; Z. Jun","NA; NA; NA; NA","2010 International Conference on Electrical and Control Engineering","","2010","","","2436","2438","This paper is to introduce the design and optimization the PoE Hub with redundancy based on the principle of EMC; through the simulation and analysis by FEM to the effectiveness of the hub of the electromagnetic shielding to enhance anti-interference performance of the hub according to the optimization to the problems discussed in the simulation. Having the aid of EMC simulation software, the analysis problem being found during the test of hub is also get the effective solution. The method of the analysis testing and problem-solving will be the guidance and reference to the design and research of the network equipment based on the principle of EMC in the future.","","978-1-4244-6881-2978-1-4244-6880-5978-0-7695-4031","10.1109/iCECE.2010.602","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5630042","EMC;PoE Hub;FEM","Finite element methods;Electromagnetic compatibility;Analytical models;Redundancy;Computational modeling;Ethernet networks;Electromagnetic shielding","electromagnetic compatibility;electromagnetic shielding;finite element analysis","electromagnetic shielding effectiveness;PoE-Hub design;FEM simulation;EMC simulation software;hub testing;analysis testing","","","","","","","","","IEEE","IEEE Conferences"
"An approach for quickly assessing mobile user objective system (MUOS) frequency assignment configurations","C. M. Dean; T. S. Derek; M. M. Robert; A. M. Richard","RAM Laboratories, Inc. San Diego, CA; RAM Laboratories, Inc. San Diego, CA; RAM Laboratories, Inc. San Diego, CA; RAM Laboratories, Inc. San Diego, CA","2010 - MILCOM 2010 MILITARY COMMUNICATIONS CONFERENCE","","2010","","","686","690","Current plans for the MUOS satellite communication system require military units and other users of MUOS communication resources to submit one or more satellite access requests (SARs) to a “chop-chain” approval process. The approval process can take considerable time, and the submitter cannot know whether the SAR will be successful until after the process completes. The problem compounded by the fact that reassignment of satellite beam carrier (SBC) frequencies to better suit new SARs is requires massive computation via the MUOS Performance Model (MPM) algorithms that can take over a day to run. To address these problems, this work demonstrates a dual-component software tool or planner. The first component is the SAR evaluator that evaluates a SAR's likelihood of success according to the same criteria that are considered when submitting a SAR over the SIPRNET for approval. It also shows a graphical assessment over time of the SAR's projected impact to the availability of various MUOS resources. The second component is the optimizer that rapidly determines SBC frequency assignments that minimize multi-access interference (MAI) on one or more MOUS satellites, given the newly specified SARs and currently committed traffic. The optimizer can aid the SAR submission process and can calculate “what-if scenarios” for SAR evaluators who must determine regional or global bandwidth allocations. The investigation includes benchmarks on proof-of-concept algorithms to demonstrate the capability of the optimizer to determine the carrier-frequency assignments in a few seconds when operating on portable devices such as an ordinary laptop as well as a PDA having limited computational bandwidth.","2155-7586;2155-7578;2155-7578","978-1-4244-8180-4978-1-4244-8178-1978-1-4244-8179","10.1109/MILCOM.2010.5680392","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5680392","SATCOM;MUOS;Multi-Access;MAI Minimization;frequency assignment optimization;Uplink;Downlink","Optimization;Satellites;Personal digital assistants;Benchmark testing;Downlink;Computational modeling;Bandwidth","frequency allocation;multi-access systems;satellite communication","mobile user objective system;frequency assignment configurations;satellite communication system;satellite access requests;satellite beam carrier;multi-access interference","","","2","","","","","","IEEE","IEEE Conferences"
"Using filesystem virtualization to avoid metadata bottlenecks","E. Artiaga; T. Cortes","Barcelona Supercomputing Center (BSC-CNS), Jordi Girona 31, E-08034 Spain; Barcelona Supercomputing Center (BSC-CNS), and Dept. of Computer Architecture, Technical University of Catalonia (UPC), Jordi Girona 1-3, E-08034 Spain","2010 Design, Automation & Test in Europe Conference & Exhibition (DATE 2010)","","2010","","","562","567","Parallel file systems are very sensitive to adverse conditions, and the lack of synergy between such file systems and some of the applications running on them has a negative impact on the overall system performance. Our observations indicate that the increased pressure on metadata management is one of the relevant causes of performance drops. This paper proposes a virtualization layer above the native file system that, transparently to the user, reorganizes the underlying directory tree, mitigating bottlenecks by taking advantage of the native file system optimizations and limiting the effects of potentially harmful application behavior. We developed COFS (COmposite File System) as a proof-of-concept virtual layer to evaluate the feasibility of the proposal.","1558-1101;1530-1591;1530-1591","978-3-9810801-6-2978-1-4244-7054-9978-3-9810801-6","10.1109/DATE.2010.5457144","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5457144","","File systems;Proposals;Application virtualization;System performance;Information management;Concurrent computing;File servers;Computer architecture;Application software;Production","file organisation;parallel processing","filesystem virtualization;metadata bottlenecks;parallel file systems;metadata management;native file system optimization;composite file system","","3","11","","","","","","IEEE","IEEE Conferences"
"Investigation of transient faults on JOP processor","S. Bahramnejad; H. R. Zarandi; M. Shojaei","Department of Computer Engineering and Information Technology, Amirkabir University of Technology; Department of Computer Engineering and Information Technology, Amirkabir University of Technology; Department of Computer Engineering, University of Isfahan","2010 IEEE International Conference on Automation, Quality and Testing, Robotics (AQTR)","","2010","2","","1","6","This paper investigates the effect and propagation of transient faults on JOP (Java Optimized Processor). JOP processor is intended for applications of embedded real-time systems and the primary implementation technology is in an FPGA. The investigation is based on 4350 transient faults which are injected in the processor using simulation-based fault injection method. The effect and propagation of the faults on different parts of this processor are observed and evaluated. Based on the experimental results, bytecode cache and bytecode memory are two sensible parts in the processor, since about 100 percent of injected faults have affected them. The results show that between 81% and 85% of injected faults cause processor failure and between 12% and 14% of non-failing faults are overwritten.","","978-1-4244-6725-9978-1-4244-6724-2978-1-4244-6723","10.1109/AQTR.2010.5520800","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5520800","","Java;Real time systems;Aerospace safety;Single event upset;Microprocessors;Application software;Aerospace industry;Logic devices;Registers;Information technology","embedded systems;fault tolerant computing;field programmable gate arrays;Java;microprocessor chips","JOP processor;transient faults;Java optimized processor;embedded real-time systems;FPGA;simulation-based fault injection method;bytecode cache;bytecode memory","","","15","","","","","","IEEE","IEEE Conferences"
"Design and simulation of dynamic slot allocation protocol for TDMA on tactical Internet","S. Yin","School of Information Engineering, Chang An University, Xi'An, China","2011 IEEE 3rd International Conference on Communication Software and Networks","","2011","","","439","443","Time slot allocation is one of the most crucial problems in the tactical internet of TDMA (Time Division Multiple Access) protocol since the improvement of the network latency and throughput depends on the efficient management of time slot resource. This paper presents a new TDMA based on MAC protocol. The protocol is not only throughput large but also delay sensitive, and it is introduced for the tactical internet of PRNET (Packet Radio Network) topology, which can dynamically allocate time slots according to the number of nodes and the node traffic load. The inter-cluster data forwarding mechanism utilized for cluster head is adopted too. Computer modeling and simulation of the new protocol are carried out by using OPNET Modeler. The simulation results are compared with those of a counterpart which uses the fixed slot allocation TDMA protocol. The test show the protocol is useful for the design of TDMA protocol optimization.","","978-1-61284-486-2978-1-61284-485-5978-1-61284-484","10.1109/ICCSN.2011.6014306","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014306","MAC;OPNET;PRNET;TDMA","Time division multiple access;Algorithm design and analysis;Optimization;Internet;Delay","access protocols;Internet;packet radio networks;telecommunication network topology;telecommunication traffic;time division multiple access","dynamic slot allocation protocol simulation;tactical Internet;time division multiple access protocol;network latency;time slot resource management;MAC protocol;PRNET topology;packet radio network topology;node traffic load;intercluster data forwarding mechanism;cluster head;computer modeling;OPNET modeler;fixed slot allocation TDMA protocol optimization","","","13","","","","","","IEEE","IEEE Conferences"
"Benchmarking MapReduce Implementations for Application Usage Scenarios","Z. Fadika; E. Dede; M. Govindaraju; L. Ramakrishnan","NA; NA; NA; NA","2011 IEEE/ACM 12th International Conference on Grid Computing","","2011","","","90","97","The MapReduce paradigm provides a scalable model for large scale data-intensive computing and associated fault-tolerance. With data production increasing daily due to ever growing application needs, scientific endeavors, and consumption, the MapReduce model and its implementations need to be further evaluated, improved, and strengthened. Several MapReduce frameworks with various degrees of conformance to the key tenets of the model are available today, each, optimized for specific features. HPC application and middleware developers must thus understand the complex dependencies between MapReduce features and their application. We present a standard benchmark suite for quantifying, comparing, and contrasting the performance of MapReduce platforms under a wide range of representative use cases. We report the performance of three different MapReduce implementations on the benchmarks, and draw conclusions about their current performance characteristics. The three platforms we chose for evaluation are the widely used Apache Hadoop implementation, Twister, which has been discussed in the literature, and LEMO-MR, our own implementation. The performance analysis we perform also throws light on the available design decisions for future implementations, and allows Grid researchers to choose the MapReduce implementation that best suits their application's needs.","2152-1093;2152-1085","978-0-7695-4572-1978-1-4577-1904","10.1109/Grid.2011.21","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6076503","MapReduce;Benchmarking;Hadoop;LEMO-MR","Fault tolerance;Fault tolerant systems;Random access memory;Memory management;Benchmark testing;Data processing;Linux","benchmark testing;fault tolerant computing;grid computing;middleware;software performance evaluation","benchmarking MapReduce implementations;application usage scenarios;MapReduce paradigm;scalable model;large scale data-intensive computing;associated fault-tolerance;data production;HPC application;middleware developers;complex dependency;MapReduce features;standard benchmark suite;MapReduce platforms;representative use cases;current performance characteristics;Apache Hadoop implementation;Twister;LEMO-MR;performance analysis;design decisions;grid researchers","","8","25","","","","","","IEEE","IEEE Conferences"
"Accelerating search and recognition workloads with SSE 4.2 string and text processing instructions","G. Shi; M. Li; M. Lipasti","University of Wisconsin-Madison; University of Wisconsin-Madison; University of Wisconsin-Madison","(IEEE ISPASS) IEEE International Symposium on Performance Analysis of Systems and Software","","2011","","","145","153","Today's information is increasing rapidly, doubling every three years. Consequently, the search and recognition stages in computer applications will consume a growing portion of the total CPU time. The SSE 4.2 instruction set, first implemented in Intel's Core i7, provides string and text processing instructions (STTNI) that utilize SIMD operations for processing character data. Though originally conceived for accelerating string, text, and XML processing, the powerful new capabilities of these instructions are useful outside of these domains, and it is worth revisiting the search and recognition stages of numerous applications to utilize STTNI to improve performance. In this paper, we explored the feasibility and potential benefit of using STTNI to improve the CPU and memory performance of search-and-recognition applications. We optimized four benchmark applications - cache simulation, B+tree search algorithm, template matching, Basic Local Alignment Search Tool (BLAST) - with STTNI, and the new applications outperform their respective original implementations by a factor of 1.4× to 13×.","","978-1-61284-368-1978-1-61284-367","10.1109/ISPASS.2011.5762731","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5762731","","Strontium;Optimization;Registers;Benchmark testing;Acceleration;XML;Text processing","instruction sets;multiprocessing systems;parallel processing;search problems;string matching;text analysis;trees (mathematics);word processing;XML","SSE 4.2 string;search workload;recognition workload;SSE 4.2 instruction set;Intel Core i7;text processing instructions;SIMD operations;character data processing;XML processing;STTNI;CPU performance;memory performance;cache simulation;B+tree search algorithm;template matching;basic local alignment search tool","","2","18","","","","","","IEEE","IEEE Conferences"
"Behavior Aware Data Locality for Caches","G. Jia; X. Li; C. Wang; X. Zhou; Z. Zhu","NA; NA; NA; NA; NA","2012 IEEE 18th International Conference on Parallel and Distributed Systems","","2012","","","514","521","Optimizing cache performance through improving data locality has been receiving a lot of attention. However, none of the existing approaches can combine each task's behavior to optimize data locality for caches. We present a behavior aware data locality (BADL) to optimize cache performance in this paper. The key idea is to add each task's behavior when allocating memory, which can take advantage of each task's different locality to optimize cache performance. There are five main contributions: 1. to our best knowledge, this is the first attempt to improve cache performance through combining task behavior, 2. BADL detailed analyzes low performance derived from internal of the cache line, which is more fine-grained than the current state-of-the-art fine-grained in hardware angle, 3. BADL optimizes the cache performance through improving internal of cache line efficiency, 4. we implement BADL both in single-threaded application and multi-threaded applications scenarios, 5. BADL can be combined to most of the cache optimizing researches. The experiment results show our proposed BADL can improve 18.6% performance on average in single-threaded application situation and improve 20.8% performance on average in multi-threaded application situation.","1521-9097;1521-9097","978-1-4673-4565-1978-0-7695-4903","10.1109/ICPADS.2012.76","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6414451","Cache performance;task behavior;data locality;cache line;fine-grained;multi-threaded;single-threaded","Instruction sets;Time frequency analysis;Hardware;Benchmark testing;Electronics packaging;Distributed databases","cache storage;data handling;multi-threading;software performance evaluation;storage allocation","behavior aware data locality;cache performance optimization;BADL;task behavior;memory allocation;hardware angle;internal cache line efficiency improvement;single-threaded application situation;multithreaded application situation","","1","28","","","","","","IEEE","IEEE Conferences"
"A Ubiquitous Sensor Network for Domestic Energy Monitoring and Energy Consumption Optimization","A. Gupta; P. Murarka","NA; NA","2012 IEEE International Conference on Green Computing and Communications","","2012","","","58","65","The current energy dilemma facing the world today expatriates the need for smart grid development, effective home power management &amp; realizing automated smart load distribution systems. In this paper, we introduce you to ""Slug"" (Smart Plug) sensor network. The ""Slug"" system is designed for use in occupational and domestic environments as a device that is able to identify the domestic appliance based on the behavior of the current patterns consumed by the appliance. The ""Slug"" system acts as both a sensor network and the individual node in the network acts as a functioning power strip. The nodes communicate with each other using the existing Power Line architecture of the environment. The use of Power Lines Communication (PLC) makes the system a seamless part of the environment. The issues related to the excessive noise and potential loss of data packets in PLC is solved by using the flood transmission protocol. We believe that the seamless integration of sensor network in its environment is important for being successful in the realm of successful ubiquitous computing. In this paper we discuss the hardware and software architectures of the ""Slug"" system which look at certain test appliances and explain by what we mean by current consumption patterns and current profiles of appliances. We try our best to come with a possible hypothesis of this behavior and explain the use of classifier algorithm which needs zero training but only a dynamically upgradeable database, thus creating a need of a cloud database connected to all the home servers. The system implementations and description of protocols developed for the appliance controls will also be explained in the paper. Finally we present the need for Flood transmission protocols in PLC and an encryption scheme to deal with security threats in the system.","","978-1-4673-5146-1978-0-7695-4865","10.1109/GreenCom.2012.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6468295","Ubiquitous networks;Sensor networks;Power Line Communications;Smart meter","Home appliances;Plugs;Servers;Current measurement;Power measurement;Monitoring;Object recognition","carrier transmission on power lines;cloud computing;domestic appliances;energy management systems;power system measurement;sensors;smart power grids;ubiquitous computing","ubiquitous sensor network;domestic energy monitoring;energy consumption optimization;energy dilemma;smart grid development;home power management;automated smart load distribution systems;Slug sensor network;smart plug sensor network;occupational environments;domestic environments;domestic appliance;functioning power strip;power line architecture;power lines communication;data packets;ubiquitous computing;hardware architecture;software architecture;current consumption patterns;current profiles;classifier algorithm;cloud database;home servers;appliance controls;flood transmission protocols;encryption scheme;security threats","","2","11","","","","","","IEEE","IEEE Conferences"
"Swarm intelligence-based algorithm for dropped packets modeling","A. H. Yassin; H. H. Hussien","Faculty of Engineering, University of Alexandria, Egypt Alexandria, Egypt; Department Of Information Technology, IGSR, University of Alexandria, Egypt Alexandria, Egypt","2012 8th International Conference on Informatics and Systems (INFOS)","","2012","","","NW-55","NW-59","An enhanced predication approach for the network dropped packets problem is introduced. This work along with test results shows the possibility of guessing when the dropped packet occurs and also the source address for it. Since artificial neural networks (ANNs) algorithms are able to model nonlinear relations between different data sets, a proposed ANN based on particle swarm optimization training algorithm (PSO) is proposed. This global optimization algorithm is applied to the proposed ANN to avoid the local minima problem in the gradient descent-training algorithm and to achieve acceptable solution. The Particle swarm optimization technique is used in this work to optimize the performance of radial basis function artificial neural network (RBF-ANN). The data used in training and testing is the data collected by a particular network simulator (NS-2) program which is utilized to simulate the data for the neural network. This RBF-ANN model has been verified by comparing ANN simulated and test data. The presented results are obtained through the use of MATLAB 8.5 software from Math works.","","978-977-403-506-7978-1-4673-0828-1978-977-403-506","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6236564","Network;Neural Network;Radial Basis Function;NS-2;Dropped packets;optimization;particle swarm optimization","Portable document format;IEEE Xplore","","","","","13","","","","","","IEEE","IEEE Conferences"
"Rate-distortion optimized transform for intra-frame coding","X. Zhao; L. Zhang; S. Ma; W. Gao","Key Lab of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, Beijing 100190, China; Institute of Digital Media, Peking University, Beijing 100871, China; Institute of Digital Media, Peking University, Beijing 100871, China; Institute of Digital Media, Peking University, Beijing 100871, China","2010 IEEE International Conference on Acoustics, Speech and Signal Processing","","2010","","","1414","1417","In this paper, a novel algorithm is proposed for intra-frame coding, named as rate-distortion optimized transform (RDOT). Unlike existing intra-frame coding schemes where the transform matrices are either fixed or mode dependent, in the proposed algorithm, transform is implemented with multiple candidate transform matrices. With this flexibility, for coding each residual block, the encoder is endowed with the power to select the optimal transform matrix in terms of rate-distortion tradeoff. The proposed algorithm has been implemented in the latest ITU-T VCEG-KTA software. Experimental results show that, over a wide range of test set, the proposed method achieves average 0.43dB coding gain compared with the recent Mode-Dependent Directional Transform (MDDT). The improvement is more significant at high bit-rates, and up to 1dB coding gain can be achieved.","2379-190X;1520-6149;1520-6149","978-1-4244-4295-9978-1-4244-4296","10.1109/ICASSP.2010.5495468","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5495468","Video coding;Transforms;Karhunen-Loeve transforms","Transforms;Encoding;Image coding;IP networks;Training;Decoding;Gain","","","","9","6","","","","","","IEEE","IEEE Conferences"
"BlobSeer: Bringing high throughput under heavy concurrency to Hadoop Map-Reduce applications","B. Nicolae; D. Moise; G. Antoniu; L. Bougé; M. Dorier","University of Rennes 1, IRISA, Rennes, France; INRIA, IRISA, Rennes, France; INRIA, IRISA, Rennes, France; ENS Cachan, Brittany, IRISA, Rennes, France; ENS Cachan, Brittany, IRISA, Rennes, France","2010 IEEE International Symposium on Parallel & Distributed Processing (IPDPS)","","2010","","","1","11","Hadoop is a software framework supporting the Map-Reduce programming model. It relies on the Hadoop Distributed File System (HDFS) as its primary storage system. The efficiency of HDFS is crucial for the performance of Map-Reduce applications. We substitute the original HDFS layer of Hadoop with a new, concurrency-optimized data storage layer based on the BlobSeer data management service. Thereby, the efficiency of Hadoop is significantly improved for data-intensive Map-Reduce applications, which naturally exhibit a high degree of data access concurrency. Moreover, BlobSeer's features (built-in versioning, its support for concurrent append operations) open the possibility for Hadoop to further extend its functionalities. We report on extensive experiments conducted on the Grid'5000 testbed. The results illustrate the benefits of our approach over the original HDFS-based implementation of Hadoop.","1530-2075;1530-2075","978-1-4244-6443-2978-1-4244-6442-5978-1-4244-6441","10.1109/IPDPS.2010.5470433","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5470433","Large-scale distributed computing;Data-intensive;Map-Reduce-based application;Distributed file systems;High Throughput;Heavy access concurrency;Hadoop;BlobSeer","Throughput;Concurrent computing;File systems;Application software;Memory;Distributed computing;Data processing;File servers;Testing;Parallel programming","concurrency control;distributed processing;storage management","heavy concurrency;Hadoop Map-Reduce application;software framework;Map-Reduce programming model;Hadoop distributed file system;primary storage system;concurrency-optimized data storage layer;BlobSeer data management service;data access concurrency","","22","17","","","","","","IEEE","IEEE Conferences"
"Minimum Loss Network Reconfiguration Using Mixed-Integer Convex Programming","R. A. Jabr; R. Singh; B. C. Pal","Department of Electrical & Computer Engineering, American University of Beirut, Riad El-Solh/Beirut, Lebanon; Electrical and Electronic Engineering Department, Imperial College, London, U.K.; Electrical and Electronic Engineering Department, Imperial College, London, U.K.","IEEE Transactions on Power Systems","","2012","27","2","1106","1115","This paper proposes a mixed-integer conic programming formulation for the minimum loss distribution network reconfiguration problem. This formulation has two features: first, it employs a convex representation of the network model which is based on the conic quadratic format of the power flow equations and second, it optimizes the exact value of the network losses. The use of a convex model in terms of the continuous variables is particularly important because it ensures that an optimal solution obtained by a branch-and-cut algorithm for mixed-integer conic programming is global. In addition, good quality solutions with a relaxed optimality gap can be very efficiently obtained. A polyhedral approximation which is amenable to solution via more widely available mixed-integer linear programming software is also presented. Numerical results on practical test networks including distributed generation show that mixed-integer convex optimization is an effective tool for network reconfiguration.","0885-8950;1558-0679","","10.1109/TPWRS.2011.2180406","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140618","Nonlinear programming;optimization methods;power distribution control","Mathematical model;Equations;Programming;Substations;Switches;Approximation methods;Optimization","approximation theory;convex programming;distributed power generation;distribution networks;integer programming;load flow","mixed-integer convex programming;mixed-integer conic programming formulation;minimum loss distribution network reconfiguration problem;network convex representation model;conic quadratic format;power flow equation;branch-and-cut algorithm;relaxed optimality gap;polyhedral approximation;mixed-integer linear programming software;distributed generation","","146","34","","","","","","IEEE","IEEE Journals & Magazines"
"Experimental study of dual-ring CMUT array optimization for forward-looking IVUS","C. Tekes; J. Zahorian; G. Gurun; S. Satir; M. Hochman; T. Xu; M. W. Rashid; L. Degertekin; M. Karaman","George W. Woodruff School of Mechanical Engineering, Georgia Institute of Technology, Atlanta, USA; George W. Woodruff School of Mechanical Engineering, Georgia Institute of Technology, Atlanta, USA; George W. Woodruff School of Mechanical Engineering, Georgia Institute of Technology, Atlanta, USA; George W. Woodruff School of Mechanical Engineering, Georgia Institute of Technology, Atlanta, USA; George W. Woodruff School of Mechanical Engineering, Georgia Institute of Technology, Atlanta, USA; George W. Woodruff School of Mechanical Engineering, Georgia Institute of Technology, Atlanta, USA; George W. Woodruff School of Mechanical Engineering, Georgia Institute of Technology, Atlanta, USA; George W. Woodruff School of Mechanical Engineering, Georgia Institute of Technology, Atlanta, USA; Electronics Engineering, Isik University, Istanbul, Turkey","2011 IEEE International Ultrasonics Symposium","","2011","","","1127","1130","Forward-looking (FL) catheters have guiding and volumetric imaging capacities which are highly desirable for IVUS applications. Large channel and firing counts have to be reduced to enable 3-D real-time imaging and simplify front-end electronics. Recently, we have proposed an optimization procedure for dual ring FL arrays which is based on finding an optimal coarray set using the simulated annealing algorithm. The presented algorithm is based on finding a predefined number of optimal firing set which results in elimination of redundant spatial frequencies in the coarray. In this study, we present the experimental demonstration of the proposed method with fabricated single chip CMUT on CMOS system based FL dual ring arrays. The dual ring CMUT arrays were monolithically fabricated on top of CMOS chips which have 25-V pulsers and low-noise transimpedance amplifiers for each transmit and receive array elements. The fabricated CMUT arrays have 56 transmit and 48 receive elements operating at 12 MHz with a 1.4 mm outer diameter. To test the imaging performance of the optimal reduced set, we obtained a 512-element coarray set from the full 2688-element set. In the experiment, we used a phantom of 100-μm aluminium wires immersed in oil tank. We have reconstructed both 2-D PSFs and B-scan images of wire targets. Experimental results demonstrate that the simulated annealing based optimal firing set achieves acceptable lateral and contrast resolution performances with 1/5 of the full set.","1948-5727;1051-0117;1948-5719","978-1-4577-1252-4978-1-4577-1253-1978-1-4577-1251","10.1109/ULTSYM.2011.0277","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6293604","Intravascular ultrasound;CMUT;Simulated Annealing;Forward-Looking;co-array","Arrays;Imaging;Wires;Firing;Simulated annealing;Signal to noise ratio;Image resolution","biomedical electronics;biomedical ultrasonics;CMOS integrated circuits;image reconstruction;medical image processing;micromachining;phantoms;simulated annealing;ultrasonic transducers","dual-ring cmut array optimization;forward-looking IVUS application;custom software;forward-looking catheters;volumetric imaging capacities;firing counts;3D real-time imaging;front-end electronics;optimal coarray set;simulated annealing algorithm;optimal firing set;redundant spatial frequencies;fabricated single chip CMUT;CMOS system based FL dual ring arrays;low-noise transimpedance amplifiers;receive array elements;transmit receive array elements;phantom;aluminium wire immersion;oil tank;B-scan image reconstruction;simulated annealing based optimal firing set;voltage 25 V;frequency 12 MHz;size 1.4 mm;size 100 mum","","1","10","","","","","","IEEE","IEEE Conferences"
"Estimating Android applications' CPU energy usage via bytecode profiling","S. Hao; D. Li; W. G. J. Halfond; R. Govindan","Computer Science Department, University of Southern California; Computer Science Department, University of Southern California; Computer Science Department, University of Southern California; Computer Science Department, University of Southern California","2012 First International Workshop on Green and Sustainable Software (GREENS)","","2012","","","1","7","Optimizing the energy efficiency of mobile applications can greatly increase user satisfaction. However, developers lack easily applied tools for estimating the energy consumption of their applications. This paper proposes a new approach, eCalc, that is lightweight in terms of its developer requirements and provides code-level estimates of energy consumption. The approach achieves this using estimation techniques based on program analysis of the mobile application. In evaluation, eCalc is able to estimate energy consumption within 9.5% of the ground truth for a set of mobile applications. Additionally, eCalc provides useful and meaningful feedback to the developer that helps to characterize energy consumption of the application.","","978-1-4673-1832-7978-1-4673-1833","10.1109/GREENS.2012.6224263","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224263","Android apps;bytecode profiling;eCalc;energy estimation","Energy consumption;Software;Accuracy;Benchmark testing;Hardware;Monitoring;Energy measurement","estimation theory;mobile computing;power aware computing","android applications CPU energy usage estimation;bytecode profiling;energy efficiency;mobile applications;user satisfaction;energy consumption;code level estimation","","26","22","","","","","","IEEE","IEEE Conferences"
"Copper wire bonding challenges and solutions of small outline packages","N. Lin; C. Tan; Y. Pan","ON Semiconductor, Lot 122, Senawang Industrial Estate, 70450 Seremban, N.Sembilan, Malaysia; ON Semiconductor, Lot 122, Senawang Industrial Estate, 70450 Seremban, N.Sembilan, Malaysia; ON Semiconductor, Lot 122, Senawang Industrial Estate, 70450 Seremban, N.Sembilan, Malaysia","2010 12th Electronics Packaging Technology Conference","","2010","","","603","607","Moving towards next level of major cost saving in small outline packages, conversion from gold wire to copper wire became necessary action. With normal production expectations of fast bonding and good quality performance on flimsy leadframe, copper wire challenges of small outline packages were incredible. Furthermore, situation became worse when new copper wire process was found requiring additional bonding time and mechanism due to harder material properties. In order to enjoy full saving of material cost, there was no loss expected on yield, quality, throughput, and machine efficiency. Under these stringent requirements, development of copper wire bonding process requires full considerations and optimization of all possible root causes, with targeted performance matching gold wire bonding process. By utilizing Six Sigma DMAIC methodology, appropriate characterization and optimization were performed on major elements, material, machine and bonding method. For wafer technology that come with thin top metal, bonding optimization was important in ensuring additional stress from Cu wire process would not cause any metal peeling. In order to achieve that, new bonding method was introduced by flattening bonded ball and then applying gentle ultrasonic vibration. This approach was later discovered to be one of the most important factors in ensuring successful reliability tests. When established process ran on more wire bonders, copper wire bonding was found very susceptible to leadframe clamping condition. Any slight unit floating caused positional type non-sticking, and most of time it happened regardless to any parameter settings. In order to resolve this issue on machines with multiple bonding sites, a new machine characterization methodology was established. This method focused and emphasized on variations of machine parts which are in contact with the leadframe. The most important tool created was the new quantifiable measuring sub-systems on machine parts, absolutely different from conventional type of product buyoff. On the machine itself, bonding mechanism was analyzed through new established external instruments, especially for some older generation machines that never provide such utilities in the operating software. Other than that, comprehensive study was also performed on the material factors. From the study of capillary, certain design and surface roughness provided significant advantage in the wire bond responses. As for the wire properties, study showed that wire coupling with optimum electrical firing parameters and air cushion could provide robust Free Air Ball formation, free from oxidation risk. In the overall combination of optimum settings and conditions, copper wire bonding showed significantly improved performance, whereby cycle time and reliability was not different from previous gold wire process. The project success was more precious when older machines were beneficial from the study, and therefore copper wire conversion did not require any capital investment. This project has enabled substantial amount of cost reduction possible for small outline packages, with zero impact to capacity or customer quality risk.","","978-1-4244-8562-8978-1-4244-8560-4978-1-4244-8561","10.1109/EPTC.2010.5702709","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5702709","","Wire;Copper;Bonding;Optimization;Fluid flow","copper;lead bonding;six sigma (quality);wafer level packaging","copper wire bonding;small outline packages;flimsy leadframe;Six Sigma DMAIC methodology;wafer technology;bonding optimization;ultrasonic vibration;leadframe clamping condition;wire coupling;electrical firing parameters;air cushion;free air ball formation;cost reduction;quality risk","","2","3","","","","","","IEEE","IEEE Conferences"
"School that has a vision","V. Mitický","Tatranská akadémia, n.o. Ul.29.augusta 4812, 058 01 Poprad, Slovakia","2012 IEEE 10th International Conference on Emerging eLearning Technologies and Applications (ICETA)","","2012","","","275","278","The article is devoted to presentation of the concept and results of the Private secondary professional school in Poprad that is actually realization of the vision of two high school teachers on how education might look like at high school. Based on the inclusion of Private secondary professional school into the school system and the subsequent approval of the experimental verification of branches of study the founder began to build a school, which is based on two main priorities - high quality educational process and modern hardware and software. The school where students have their own laptops, use electronic books along with an educational portal, do test and homework via the internet and where parents can check the results by means of the electronic register of students' grades. The school which offers their students new perspective branches of study tailor-made according to the current and future requirements of the market, i.e. without the school subjects that are not relevant anymore, and with an ongoing innovative curriculum; the school with enthusiastic highly qualified teachers using new methods and forms of teaching including modern technique. The school that partners with the university sector to ensure the follow-up study of the graduates provides video conferencing for specialized subjects and conversational courses with schools abroad. Private secondary professional school, Ul.29.augusta 4812 in Poprad, officially began on 01.09.2008, the founder of which is Tatranská Akadémia.","","978-1-4673-5122-5978-1-4673-5120-1978-1-4673-5121","10.1109/ICETA.2012.6418322","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6418322","","Educational institutions;Graphics;Computer networks;Software;Portable computers;Motion pictures","computer aided instruction;educational courses;educational institutions;electronic publishing;Internet;portals;teaching;teleconferencing;video communication","private secondary professional school;Poprad;high school teachers;school system;high quality educational process;modern hardware;modern software;electronic books;educational portal;Internet;student grade electronic register;teaching;video conferencing;conversational course;Tatranská Akadémia","","","3","","","","","","IEEE","IEEE Conferences"
"Development of interactive tutorial tool for simulation and identification of electrical machines and transformers","R. Wamkeue; A. Lalami","Université du Québec en Abitibi-Témiscamingue, Département des sciences appliquées, 445, Boul. de l'Université, J9X 5E4, Rouyn-Noranda, Canada; Université du Québec en Abitibi-Témiscamingue, Département des sciences appliquées, 445, Boul. de l'Université, J9X 5E4, Rouyn-Noranda, Canada","2012 6th IEEE International Conference on E-Learning in Industrial Electronics (ICELIE)","","2012","","","40","46","The paper focuses on the development of a Matlab based interactive and tutorial tool for simulation and parameters identification of electrical machines, transformers and several other dynamic systems. The proposed software allows predicting the steady-state and dynamic performances of three-phase induction and synchronous machines, DC machines in both motor and generator modes, three-phase transformers and several other dynamic systems. A given machine under study is formatted in state space models. This allows performing various standard and non-standard tests. For linear and nonlinear deterministic machine models, linear and nonlinear deterministic predictors (Euler method and fourth order Runge-Kutta) are used, while the classical linear Kalman Filter (LKF) and Unscented Kalman Filter (UKF) are applied for the state estimation of linear and nonlinear stochastic machine models respectively. The availability of several optimization approaches for parameters identification experiences offers to users a great flexibility and opportunity to compare their robustness.","","978-1-4673-4756-3978-1-4673-4754-9978-1-4673-4755","10.1109/ICELIE.2012.6471145","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6471145","Educational software tool;models;UKF and UKF state estimators;optimization;identification and validation","MATLAB;Robustness;Mathematical model;Predictive models;Rotors;DC machines","asynchronous machines;computer aided instruction;electric machine analysis computing;interactive systems;Kalman filters;linear synchronous motors;power engineering education;power system state estimation;power transformers;Runge-Kutta methods;state-space methods;stochastic processes","interactive tutorial tool development;electrical machine;transformer;Matlab;dynamic systems;steady-state performance;dynamic performance;induction machine;synchronous machine;DC machine;generator mode;motor mode;state space model;nonlinear deterministic machine model;linear machine model;linear deterministic predictor;state estimation;linear stochastic machine model;nonlinear stochastic machine model;optimization;linear Kalman filter;unscented Kalman filter;UKF;LKF;Runge-Kutta method;Euler method;nonlinear deterministic predictor","","","15","","","","","","IEEE","IEEE Conferences"
"Contribution of the numerical simulation tools in the high voltage circuit-breaker conception","G. Marquezin; L. Yi; Z. Deng; P. Robin-Jouan; J. Rodriguez","ALSTOM Grid, China Technology Center, No.500 Jiangyue Road, Minhang District, Shanghai, PRC 201114, China; ALSTOM Grid, China Technology Center, No.500 Jiangyue Road, Minhang District, Shanghai, PRC 201114, China; ALSTOM Grid, China Technology Center, No.500 Jiangyue Road, Minhang District, Shanghai, PRC 201114, China; ALSTOM Grid High Voltage Switchgear Research Center, 130 rue L&#x00E9;on BLUM 69611 VILLEURBANNE Cedex, France; ALSTOM Grid High Voltage Switchgear Research Center, 130 rue L&#x00E9;on BLUM 69611 VILLEURBANNE Cedex, France","2011 1st International Conference on Electric Power Equipment - Switching Technology","","2011","","","370","374","The development of modern simulation tools in all the physics and the important increase of the computer capabilities allow engineers to use numerical simulation tools for high voltage circuit-breaker developments. First dimensioning and optimization are possible and lead to a reduction of the tests. With appropriate hypothesis, today commercial softwares are perfectly relevant, typically for the dielectric, thermal and mechanical calculations. The electromagnetic, fluid and breaking process simulations are more complex since precise phenomena have to be considered and can be partially defined. Particular criteria can be used to predict the success or failure of the real test. To be more and more fitted to tests, these tools are still being developed and particular validation steps are required. The paper presents some details about these tools and their applications.","","978-1-4577-1272-2978-1-4577-1273-9978-1-4577-1271","10.1109/ICEPE-ST.2011.6123010","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6123010","","Integrated circuit modeling;Solid modeling;Computational modeling;Dielectrics;Numerical models;Atmospheric modeling;Electromagnetics","circuit breakers;circuit optimisation;failure analysis;high-voltage techniques;numerical analysis;power engineering computing","numerical simulation tool;high voltage circuit-breaker conception;computer capability;commercial software;dielectric calculation;thermal calculation;mechanical calculation;breaking process simulation;fluid process simulation;electromagnetic process simulation;real test failure","","","5","","","","","","IEEE","IEEE Conferences"
"Assessing MapReduce for Internet Computing: A Comparison of Hadoop and BitDew-MapReduce","L. Lu; H. Jin; X. Shi; G. Fedak","NA; NA; NA; NA","2012 ACM/IEEE 13th International Conference on Grid Computing","","2012","","","76","84","MapReduce is emerging as an important programming model for data-intensive application. Adapting this model to desktop grid would allow taking advantage of the vast amount of computing power and distributed storage to execute new range of application able to process enormous amount of data. In 2010, we have presented the first implementation of MapReduce dedicated to Internet Desktop Grid based on the BitDew middleware. In this paper, we present new optimizations to BitDew-MapReduce (BitDew-MR): aggressive task backup, intermediate result backup, task re-execution mitigation and network failure hiding. We propose a new experimental framework which emulates key fundamental aspects of Internet Desktop Grid. Using the framework, we compare BitDew-MR and the open-source Hadoop middleware on Grid5000. Our experimental results show that 1) BitDew-MR successfully passes all the stress-tests of the framework while Hadoop is unable to work in typical wide-area network topology which includes PC hidden behind firewall and NAT; 2) BitDew-MR outperforms Hadoop performances on several aspects: scalability, fairness, resilience to node failures, and network disconnections.","2152-1093;2152-1085","978-1-4673-2901","10.1109/Grid.2012.31","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6319157","desktop grid computing;MapReduce;data-intensive application;cloud computing","Peer to peer computing;Runtime;Internet;Dynamic scheduling;Distributed databases;Heart beat;Data processing","grid computing;Internet;middleware;optimisation;public domain software","Internet computing;BitDew-MapReduce;data-intensive application;desktop grid;Internet desktop grid;BitDew middleware;aggressive task backup;optimizations;intermediate result backup;task reexecution mitigation;network failure hiding;BitDew-MR;open-source Hadoop middleware;Grid5000;network disconnections","","7","28","","","","","","IEEE","IEEE Conferences"
"A 3-layer Dynamic CAPTCHA Implementation","J. Cui; W. Zhang; Y. Peng; Y. Liang; B. Xiao; J. Mei; D. Zhang; X. Wang","NA; NA; NA; NA; NA; NA; NA; NA","2010 Second International Workshop on Education Technology and Computer Science","","2010","1","","23","26","In order to avoid tremendous attack from malicious computer programs, CAPTCHA (Completely Automated Public Turing test to tell Computers and Human Apart) mechanism has been introduced to distinguish humans and computers. Due to the fast development of pattern recognition and artificial intelligence technology, there are increasing safety loopholes concerning traditional 2D static CAPTCHAs, resulting in that certain malicious computer programs could launch serious program attack through breaking such CAPTCHAs. In this article, we proposal a practical and safe 3-layer dynamic CAPTCHA, originally bonding the biological vision theory with the single-frame zero-knowledge theory, ensuring it not only extremely hard to recognize each single frame, but easy to identify for humans as well. It also makes full use of disadvantages of computers in recognizing numerous moving objects from a complicated background, making it still very difficult for computer programs to break even using several frames. To verify its security, an analysis is further carried out. Moreover, the 3-layer structure makes the design of CAPTCHA more distinct, taking on high expansibility as well as plenty of room for sustainable optimization.","","978-1-4244-6389-3978-1-4244-6388","10.1109/ETCS.2010.575","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5460117","CAPTCHA;3-layer;dynamic;single-frame zero-knowledge theory;biological vision theory;moving objects recognition","Humans;Biology computing;Voting;Optical character recognition software;Computer science;Postal services;Automatic testing;Computer science education;Educational programs;Educational technology","artificial intelligence;computer vision;human computer interaction;security of data;Turing machines","malicious computer programs;completely automated public Turing test;pattern recognition;artificial intelligence;safety loopholes;program attack;3-layer dynamic CAPTCHA;biological vision theory;single-frame zero-knowledge theory;security analysis;sustainable optimization","","4","12","","","","","","IEEE","IEEE Conferences"
"Design of Compact Dual-Mode Microstrip Filters","L. Athukorala; D. Budimir","Department of Electronic, Communication and Software Engineering, University of Westminster, London, United Kingdom; Department of Electronic, Communication and Software Engineering, University of Westminster, London, United Kingdom","IEEE Transactions on Microwave Theory and Techniques","","2010","58","11","2888","2895","This paper presents a novel filter design technique for the compact microstrip dual-mode filters. An equivalent circuit for the single dual-mode filter section is derived to show that a single unit behaves as a pair of coupled synchronously tuned single-mode resonators. The equivalent circuit was linked to the inverter-coupled bandpass prototype network to allow higher order filters to be realized. A complete design example (from design to realization) of a fourth-order Chebyshev bandpass filter is presented. It is shown that the dual-mode resonator may be employed to design cross-coupled filters with finite frequency zeros. Two filters are designed using optimized coupling matrix method, fabricated and tested. Experimental and simulation results are presented to validate the argument. Finally, it is shown that more compactness may be achieved with narrowband filters by employing folded resonators.","0018-9480;1557-9670","","10.1109/TMTT.2010.2079110","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5605278","Compact filters;dual-mode filters;microstrip filters;microstrip resonators","Couplings;Impedance;Integrated circuit modeling;Inverters;Filtering theory;Passband;Microstrip filters","band-pass filters;equivalent circuits;microstrip filters;resonators","compact dual-mode microstrip filters;filter design technique;equivalent circuit;single-mode resonators;inverter-coupled bandpass prototype network;higher order filters;fourth-order Chebyshev bandpass filter;dual-mode resonator;cross-coupled filters;finite frequency zeros;optimized coupling matrix","","21","15","","","","","","IEEE","IEEE Journals & Magazines"
"A decomposition strategy to solve the Short-Term Hydrothermal Scheduling based on Lagrangian Relaxation","F. Y. K. Takigawa; E. C. Finardi; E. L. da Silva","Universidade Federal de Santa Catarina, Trindade, CEP 88040-900, Florianópolis, Brazil; Universidade Federal de Santa Catarina, Trindade, CEP 88040-900, Florianópolis, Brazil; Universidade Federal de Santa Catarina, Trindade, CEP 88040-900, Florianópolis, Brazil","2010 IEEE/PES Transmission and Distribution Conference and Exposition: Latin America (T&D-LA)","","2010","","","681","688","The problem dealt here is called Short-Term Hydrothermal Scheduling - STHS. The objective of the STHS problem is to optimize the electricity production, considering a short-term planning horizon. In this problem, hydro and thermal plants must be coordinated in order to satisfy the electricity demand at the minimum cost and complying with reliability standards. Mathematically, the STHS is modeled as a deterministic, nonlinear, mixed-integer and large scale optimization problem. A Lagrangian Relaxation - LR scheme based on a variable splitting technique is proposed. By using it, the resulting separable dual problem is solved by a Bundle method. The purpose of this study is to analyze the decomposition strategy and the quality of the solution produced by the LR and the pseudo-primal point, which is calculated by active cuts of the Bundle method. A computational model is tested on a hydrothermal configuration, whose data were extracted from the Brazilian Hydrothermal power system.","","978-1-4577-0488-8978-1-4577-0488-8978-85-99916-03","10.1109/TDC-LA.2010.5762957","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5762957","Hydroelectric-thermal power generation;Power generation scheduling","Reservoirs;Thermal decomposition;Biological system modeling;Indexes;Software;Couplings","hydrothermal power systems;integer programming;nonlinear programming;power generation planning;power generation reliability;power generation scheduling","decomposition strategy;short term hydrothermal scheduling;Lagrangian relaxation scheme;electricity production;short-term planning horizon;electricity demand;deterministic optimization problem;nonlinear optimization problem;mixed-integer optimization problem;large scale optimization problem;variable splitting technique;bundle method;pseudoprimal point;Brazilian Hydrothermal power system","","","22","","","","","","IEEE","IEEE Conferences"
"Sentomist: Unveiling Transient Sensor Network Bugs via Symptom Mining","Y. Zhou; X. Chen; M. R. Lyu; J. Liu","NA; NA; NA; NA","2010 IEEE 30th International Conference on Distributed Computing Systems","","2010","","","784","794","Wireless Sensor Network (WSN) applications are typically event-driven. While the source codes of these applications may look simple, they are executed with a complicated concurrency model, which frequently introduces software bugs, in particular, transient bugs. Such buggy logics may only be triggered by some occasionally interleaved events that bear implicit dependency, but can lead to fatal system failures. Unfortunately, these deeply-hidden bugs or even their symptoms can hardly be identified by state-of-the-art debugging tools, and manual identification from massive running traces can be prohibitively expensive. In this paper, we present Sentomist (Sensor application anatomist), a novel tool for identifying potential transient bugs in WSN applications. The Sentomist design is based on a key observation that transient bugs make the behaviors of a WSN system deviate from the normal, and thus outliers (i.e., abnormal behaviors) are good indicators of potential bugs. Sentomist introduces the notion of event-handling interval to systematically anatomize the long-term execution history of an event-driven WSN system into groups of intervals. It then applies a customized outlier detection algorithm to quickly identify and rank abnormal intervals. This dramatically reduces the human efforts of inspection (otherwise, we have to manually check tremendous data samples, typically with brute force inspection) and thus greatly speeds up debugging. We have implemented Sentomist based on the concurrency model of TinyOS. We apply Sentomist to test a series of representative real-life WSN applications that contain transient bugs. These bugs, though caused by complicated interactions that can hardly be predicted during the programming stage, are successfully confined by Sentomist.","1063-6927;1063-6927","978-1-4244-7262-8978-1-4244-7261-1978-0-7695-4059","10.1109/ICDCS.2010.75","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5541625","Sensor Networks;Debugging;Testing;Embedded Systems","Computer bugs;Wireless sensor networks;Application software;Concurrent computing;Debugging;Inspection;Logic;History;Detection algorithms;Humans","concurrency control;data mining;operating systems (computers);security of data;telecommunication computing;telecommunication security;wireless sensor networks","wireless sensor network;transient bugs;symptom mining;complicated concurrency model;software bugs;buggy logics;Sentomist tool;sensor application anatomist tool;TinyOS model","","16","36","","","","","","IEEE","IEEE Conferences"
"Shared-Cache Simulation for Multi-core System with LRU2-MRU Collaborative Cache Replacement Algorithm","S. Ding; S. Lui; Y. Li","NA; NA; NA","2012 13th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing","","2012","","","127","131","The L2 shared cache is an important resource for multi-core system. The cache replacement algorithm of L2 shared cache is one of the key factors in judging whether the L2 shared cache of multi-core system is efficient. In this paper, we study shared-cache simulation for multi-core with the LRU2-MRU collaborative cache replacement algorithm. We propose a theoretical foundation for LRU2-MRU to show the property, test the stack distance of the LRU2-MRU algorithm. In addition, the simulation results demonstrate that the MPKI (misses per thousand instructions) of LRU2-MRU is lower than other cache replacement algorithm, and the miss ratio for shared-cache can be reduce through cache replacement algorithm optimization.","","978-1-4673-2120","10.1109/SNPD.2012.112","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6299268","Shared-cache;Miss ratio;Stack distance;Replacement algorithm;MPKI;LRU2;MRU","Collaboration;Algorithm design and analysis;Prediction algorithms;Testing;Multicore processing;Computational modeling;Data models","cache storage;multiprocessing systems","L2 shared-cache simulation;multicore system;LRU2-MRU collaborative cache replacement algorithm;stack distance;MPKI;misses per thousand instruction;miss ratio;cache replacement algorithm optimization","","","13","","","","","","IEEE","IEEE Conferences"
"LnQ: Building High Performance Dynamic Binary Translators with Existing Compiler Backends","C. Hsu; P. Liu; C. Wang; J. Wu; D. Hong; P. Yew; W. Hsu","NA; NA; NA; NA; NA; NA; NA","2011 International Conference on Parallel Processing","","2011","","","226","234","This paper presents an LLVM+QEMU (LnQ)framework for building high performance and retargetable binary translators with existing compiler modules. Dynamic binary translation is a just-in-time (JIT) compilation from binary code of guest ISA to binary code of host ISA. The quality of translated code is critical to the performance of a dynamic binary translator, which translates code between different IS As, so the translated code is often carefully hand-optimized. As a result, it takes tremendous implementation efforts for software engineers to port an existing dynamic binary translator to anew host ISA. The goal of LnQ framework is to enable the process of building high performance and retarget able dynamic binary translators with existing optimizers and code generation back ends. LnQ framework consists of a translation module and an emulation engine. We design the translation module based on LLVM compiler infrastructure, and use QEMU as our emulation engine. We implement an x86-to-x86 64 dynamic binary translator with our LnQ framework to show that the framework is retarget able, and conduct experiments on SPECCPU2006 benchmarks to show that the resulting binary translator has good performance. The experiment results indicate that the x86-to-x86 64 LnQ translator achieves an average speedup of 1.62X in integer benchmarks, and 3.02X in floating point benchmarks than QEMU.","0190-3918;0190-3918;2332-5690","978-1-4577-1336-1978-0-7695-4510","10.1109/ICPP.2011.57","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6047191","Dynamic Binary Translation;Optimization;LLVM;QEMU","Registers;Emulation;Engines;Optimization;Joining processes;Benchmark testing;Libraries","program compilers;program interpreters;virtual machines","dynamic binary translator;compiler backend;just-in-time compilation;guest ISA;host ISA;binary code;LnQ framework;code generation backend;QEMU emulation engine;LLVM compiler infrastructure;low level virtual machine","","2","28","","","","","","IEEE","IEEE Conferences"
"Voltage stability assessment: An approach with expanded Newton Raphson -Sydel","M. Eidiani; H. Zeynal; A. K. Zadeh; S. Mansoorzadeh; K. M. Nor","Department of electrical engineering, Bojnourd Branch, Islamic Azad University, Bojnourd, Iran; Centre of Electrical Energy Systems (CEES), Faculty of Electrical Engineering, Universiti Teknologi Malaysia (UTM), 81310 Skudai, Johor, Malaysia; Centre of Electrical Energy Systems (CEES), Faculty of Electrical Engineering, Universiti Teknologi Malaysia (UTM), 81310 Skudai, Johor, Malaysia; Faculty of mechanical engineering, Universiti Teknologi Malaysia (UTM), 81310 Skudai, Johor, Malaysia; Centre of Electrical Energy Systems (CEES), Faculty of Electrical Engineering, Universiti Teknologi Malaysia (UTM), 81310 Skudai, Johor, Malaysia","2011 5th International Power Engineering and Optimization Conference","","2011","","","31","35","Voltage instability is major cause of cascading failures in the current power systems. Static assessment of voltage stability has been used widely to draw the secure margin of voltage security for the system components. Among the many methods reported, Conventional Newton Raphson method is popular but it suffers from curse of singularity on its Jacobian matrix which precludes converging onto the solution. To overcome this problem, Continuation Power Flow (CPF) method was come up. CPF method is a very powerful method that can give the solution without having the singularity problem. The CPF method has then been improved using new CPF-GMRES method. In contrary, NRS (Newton - Raphson - Seydel) is old method but is fast and accurate. This paper expands existing NRS method which then demonstrated that it is more reliable and faster than CPF-GMRES and NRS. The algorithm tested on practical 350 bus network in IRAN (Khorasan region).","","978-1-4577-0354-6978-1-4577-0355-3978-1-4577-0353","10.1109/PEOCO.2011.5970424","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5970424","Voltage Stability;Static ATC;NRS;MATLAB;DIgSILENT Power Factory Software","Jacobian matrices;Load flow;Power system stability;Stability analysis;Thermal stability;Bifurcation","Jacobian matrices;load flow;Newton-Raphson method;power system faults;power system security;power system stability","voltage stability assessment;Newton Raphson-Sydel method;cascading failures;static assessment;voltage security;Jacobian matrix;continuation power flow method;350 bus network","","3","20","","","","","","IEEE","IEEE Conferences"
"Prototyping platform for performance evaluation of SHA-3 candidates","K. Kobayashi; J. Ikegami; M. Knežević; E. Xu Guo; S. Matsuo; Sinan Huang; L. Nazhandali; Ü. Kocabaş; Junfeng Fan; A. Satoh; I. Verbauwhede; K. Sakiyama; K. Ohta","The University of Electro-Communications, 1-5-1, Chofugaoka, Chofu, Tokyo 182-8585, Japan; The University of Electro-Communications, 1-5-1, Chofugaoka, Chofu, Tokyo 182-8585, Japan; Katholieke Universiteit Leuven, Kasteelpark Arenberg 10, B-3001 Heverlee, Belgium; Electrical and Computer Engineering Dept. Virginia Polytechnic Institute and State University Blacksburg, 24061, USA; National Institute of Information and Communications Technology, 4-2-1 Nukui-Kitamachi, Koganei, Tokyo 184-8795, Japan; Electrical and Computer Engineering Dept. Virginia Polytechnic Institute and State University Blacksburg, 24061, USA; Electrical and Computer Engineering Dept. Virginia Polytechnic Institute and State University Blacksburg, 24061, USA; Katholieke Universiteit Leuven, Kasteelpark Arenberg 10, B-3001 Heverlee, Belgium; Katholieke Universiteit Leuven, Kasteelpark Arenberg 10, B-3001 Heverlee, Belgium; Research Center for Information Security, National Institute of Advanced Industrial Science and Technology, 1-18-13, Sotokanda, Chiyoda, Tokyo 101-0021, Japan; Katholieke Universiteit Leuven, Kasteelpark Arenberg 10, B-3001 Heverlee, Belgium; The University of Electro-Communications, 1-5-1, Chofugaoka, Chofu, Tokyo 182-8585, Japan; The University of Electro-Communications, 1-5-1, Chofugaoka, Chofu, Tokyo 182-8585, Japan","2010 IEEE International Symposium on Hardware-Oriented Security and Trust (HOST)","","2010","","","60","63","The objective of the SHA-3 NIST competition is to select, from multiple competing candidates, a standard algorithm for cryptographic hashing. The selected winner must have adequate cryptographic properties and good implementation characteristics over a wide range of target platforms, including both software and hardware. Performance evaluation in hardware is particularly challenging because of the large design space, wide range of target technologies, and multitude of optimization criteria. We describe the efforts of three research groups to evaluate SHA-3 candidates using a common prototyping platform. Using a SASEBO-GII FPGA board as a starting point, we evaluate the performance of the 14 remaining SHA-3 candidates with respect to area, throughput, and power consumption. Our approach defines a standard testing harness for SHA-3 candidates, including the interface specifications for the SHA-3 module on the SASEBO testing board.","","978-1-4244-7812-5978-1-4244-7811-8978-1-4244-7810","10.1109/HST.2010.5513111","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5513111","hardware implementation;hardware evaluation;hash function;SHA-3","Prototypes;Software prototyping;Cryptography;Hardware;Space technology;Testing;NIST;Design optimization;Field programmable gate arrays;Throughput","cryptography;field programmable gate arrays;performance evaluation","performance evaluation;prototyping platform;SHA-3 Candidates;cryptographic hashing;SASEBO-GII FPGA board","","9","12","","","","","","IEEE","IEEE Conferences"
"Advanced computational methods for security constrained financial Transmission Rights","K. Kalsi; S. Elbert; M. Vlachopoulou; N. Zhou; Z. Huang","Pacific Northwest National Laboratory, Richland, WA 99354 USA; Pacific Northwest National Laboratory, Richland, WA 99354 USA; Pacific Northwest National Laboratory, Richland, WA 99354 USA; Pacific Northwest National Laboratory, Richland, WA 99354 USA; Pacific Northwest National Laboratory, Richland, WA 99354 USA","2012 IEEE Power and Energy Society General Meeting","","2012","","","1","8","Financial Transmission Rights (FTRs) are financial insurance tools to help power market participants reduce price risks associated with transmission congestion. FTRs are issued based on a process of solving a constrained optimization problem with the objective to maximize the FTR social welfare under power flow security constraints. Security constraints for different FTR categories (monthly, seasonal or annual) are usually coupled and the number of constraints increases exponentially with the number of categories. Commercial software for FTR calculation can only provide limited categories of FTRs due to the inherent computational challenges mentioned above. In this paper, first an innovative mathematical reformulation of the FTR problem is presented, which dramatically improves the computational efficiency of optimization problem. After having re-formulated the problem, a novel non-linear dynamic system (NDS) approach is proposed to solve the optimization problem. The new formulation and performance of the NDS solver is benchmarked against widely used linear programming (LP) solvers like CPLEX™. Tests are performed on both standard IEEE test systems and large-scale systems using data from the Western Electricity Coordinating Council (WECC). The NDS is demonstrated to be comparable, and in many cases outperforms the widely used CPLEX algorithms. The proposed formulation and NDS based solver are easily parallelizable, enabling further computational improvement.","1932-5517;1944-9925","978-1-4673-2729-9978-1-4673-2727-5978-1-4673-2728","10.1109/PESGM.2012.6345577","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6345577","Financial Transmission Rights;Linear programming;CPLEX;dynamic system","Load flow;Optimization;Linear programming;Standards;Transmission line matrix methods;Security;Convergence","load flow;power markets;power system security","advanced computational methods;security constrained financial transmission rights;financial insurance tools;power market;price risks;transmission congestion;power flow security constraints;commercial software;mathematical reformulation;computational efficiency;non-linear dynamic system;linear programming;standard IEEE test systems;large-scale systems;Western Electricity Coordinating Council;WECC","","3","36","","","","","","IEEE","IEEE Conferences"
"Implementation of the OBAMP overlay protocol for multicast delivery in OLSR wireless community networks","F. S. Proto; C. Pisa","University of Rome Tor Vergata; University of Rome Tor Vergata","2010 IEEE International Symposium on "A World of Wireless, Mobile and Multimedia Networks" (WoWMoM)","","2010","","","1","3","The Optimized Link State Routing (OLSR) protocol is widely employed in Wireless Community Networks (WCNs), where users are connected to a wireless backbone with high bandwidth links. In these networks users can access both the Internet and internal network services. However the routers lack multicast support and users cannot exploit the high capacity of wireless links to stream and receive multimedia services. We implemented an extension to the OLSR protocol to support the delivery of multicast traffic using the Overlay Borovka-based Ad-hoc Multicast Protocol (OBAMP). We present a demo of our implementation that is devised for the GNU/Linux operating system. The implementation has been tested both on standard PCs and on embedded devices running OpenWRT and it is distributed as a plugin within the olsr.org OLSR implementation. This protocol enhancement makes possible to create a multicast distribution tree among a subset of nodes, providing the mesh users with multicast community services.","","978-1-4244-7265-9978-1-4244-7264-2978-1-4244-7263","10.1109/WOWMOM.2010.5534941","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5534941","","Routing protocols;Wireless communication;Communities;IP networks;Mesh networks","Internet;Linux;multicast communication;multimedia communication;public domain software;radio links;routing protocols","OBAMP overlay protocol;multicast delivery;OLSR wireless community networks;Optimized Link State Routing protocol;WCN;high bandwidth links;Internet;internal network services;routers;multimedia services;multicast traffic;Overlay Borovka-based Ad-hoc Multicast Protocol;GNU;Linux operating system;embedded devices;OpenWRT;distributed","","","13","","","","","","IEEE","IEEE Conferences"
"Time-Aware Instrumentation of Embedded Software","S. Fischmeister; P. Lam","Department of ECE, University of Waterloo, Waterloo, Canada; Department of ECE, University of Waterloo, Waterloo, Canada","IEEE Transactions on Industrial Informatics","","2010","6","4","652","663","Software instrumentation is a key technique in many stages of the development process. It is particularly important for debugging embedded systems. Instrumented programs produce data traces which enable the developer to locate the origins of misbehaviors in the system under test. However, producing data traces incurs runtime overhead in the form of additional computation resources for capturing and copying the data. The instrumentation may therefore interfere with the system's timing and perturb its behavior. In this work, we propose an instrumentation technique for applications with temporal constraints, specifically targeting background/foreground or cyclic executive systems. Our framework permits reasoning about space and time and enables the composition of software instrumentations. In particular, we propose a definition for trace reliability, which enables us to instrument real-time applications which aggressively push their time budgets. Using the framework, we present a method with low perturbation by optimizing the number of insertion points and trace buffer size with respect to code size and time budgets. Finally, we apply the theory to two concrete case studies: we instrument the OpenEC firmware for the keyboard controller of the One Laptop Per Child project, as well as an implementation of a flash file system.","1551-3203;1941-0050","","10.1109/TII.2010.2068304","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5559440","Debugging;instrumentation;real-time systems;tracing","Reliability;Real time systems;Monitoring;Computational modeling;Debugging;Embedded systems;Runtime","embedded systems;firmware;program debugging","time-aware instrumentation;embedded software;embedded systems debugging;trace reliability;OpenEC firmware;keyboard controller;one laptop per child project;flash file system","","20","33","","","","","","IEEE","IEEE Journals & Magazines"
"Research on coating production scheduling based on improved ant colony algorithm","Chen Zhuo; Zhang Jie; Meng You-xin","College of Information Science &amp; Technology, Qingdao University of Science and Technology, China; College of Information Science &amp; Technology, Qingdao University of Science and Technology, China; College of Information Science &amp; Technology, Qingdao University of Science and Technology, China","2010 International Conference on Mechanic Automation and Control Engineering","","2010","","","2955","2958","The production of small and medium-sized coating enterprises has the characteristics of small batch, multi-type and higher flexibility, and its scheduling problem has great complexity, so traditional methods are hard to achieve the ideal effect. How to establish the reasonable production scheduling model, find out efficient scheduling algorithm, to raise the production efficiency and the utilization rate of resources, becomes a burning question. This paper considers the general constraints of production scheduling and the characteristic of coating production-the placed time of semi-finished can not be too long. The paper puts the placed time of semi-finished into probability selection formula, and uses the improved Ant Colony Algorithm to solve coating production scheduling problem. In simulation test, the machines have higher utilization rate, and it indicates that the algorithm is effective.","","978-1-4244-7739-5978-1-4244-7737-1978-1-4244-7738","10.1109/MACE.2010.5536497","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5536497","coating production scheduling;ant colony algorithm;the placed time of semi-finished;batch chemical process","Coatings;Production;Scheduling algorithm;Paper technology;Job shop scheduling;Processor scheduling;Application software;Computer aided manufacturing;Educational institutions;Information science","chemical industry;coatings;optimisation;production control;scheduling;small-to-medium enterprises","coating production scheduling problem;improved ant colony algorithm;medium-sized coating enterprises;small sized coating enterprises;probability selection formula","","","","","","","","","IEEE","IEEE Conferences"
"Application of genetic algorithm to the development of artificial intelligence module system","H. Wu; W. Hsiao; C. Lin; T. Cheng","Department of Construction Engineering, Chaoyang University of Technology, Taichung, Taiwan; Department of Construction Engineering, Chaoyang University of Technology, Taichung, Taiwan; Department of Civil Engineering, National Chung Hsing University, Taichung, Taiwan; Department of Construction Engineering, Chaoyang University of Technology, Taichung, Taiwan","2011 2nd International Conference on Intelligent Control and Information Processing","","2011","1","","290","294","Artificial intelligence is an extremely important aspect of modern technological progress. It represents humanity's long-time dream of elevating computers beyond the realm of simple machines for the calculation and processing of data. With the incorporation of artificial intelligence, computers are able to engage in behavior similar to human thought, helping humans perform necessary tasks of cognition, theorizing, and even judgment. Genetic algorithms (GA) are a branch of artificial intelligence and have been demonstrated to be fast and accurate in the pursuit of efficiency and accuracy. GA is often applied to optimization problems for single and multiple targets. Progress in computer hardware and software programming language has driven the development of artificial intelligence. All applications or research relevant to artificial intelligence algorithms require computer programs that meet individual constraints and individual objectives; in other words, each different problem has its own specialized computer program. Though software suites based on GA already exist (e.g., Evolver, SUGAL), these have limitations in practical usage and may be unable to fully realize the effectiveness of GA. As such, this study utilizes Microsoft Visual Basic to develop an open source standard module system based on GA, allowing users to create programs which meet problem constraints within a short period of time. When standard program modules are unable to meet these requirements, users can also customize the original source code according to particular demands. Research results demonstrate that standard module systems built on GA can help users to complete computer programs within a short period of time, and also increase the accuracy of programs. This study has already developed around 30 standard program modules. Testing has already been completed for all program modules, and the open source code for all program modules has been placed on the http://cost.ce.cyut.edu.tw/lab website for download.","","978-1-4577-0816-9978-1-4577-0813-8978-1-4577-0815","10.1109/ICICIP.2011.6008251","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6008251","","Genetic algorithms;Biological cells;Artificial intelligence;Computers;Genetics;Encoding;Accuracy","artificial intelligence;cognition;genetic algorithms;public domain software;Visual BASIC","genetic algorithm;artificial intelligence module system;optimization problems;computer hardware;software programming language;Microsoft Visual Basic;open source standard module system","","1","15","","","","","","IEEE","IEEE Conferences"
"A general purpose Ethernet based readout data acquisition system","B. Mindur; L. Jachymczyk","Faculty of Physics and Applied Computer Science, AGH University of Science and Technology, al. Mickiewicza 30, 30-059 Kraków, Poland; Faculty of Physics and Applied Computer Science, AGH University of Science and Technology, al. Mickiewicza 30, 30-059 Kraków, Poland","2011 IEEE Nuclear Science Symposium Conference Record","","2011","","","800","806","A flexible dedicated readout system is one of the most important part of any kind of dedicated detection system, especially for its testing phase as well as when the final system is ready for implementation. An obvious choice is to use a FPGA (apart from dedicated front-end electronics) as a first stage of data storage and processing element. Furthermore the FPGA has to prepare and transfer the incoming/processed data to the host PC. The implementation of the data exchange can be a problem, especially for small groups of developers, who have an option to buy a general solution with its limitations and a price, or to do time-consuming development of their own system practically from scratch. This paper presents a FPGA based general purpose readout solution which lies in between the two opposite approaches. Presented system uses a FPGA mezzanine board equipped with Ethernet Gigabit connection to PC. The FPGA FIFO based readout of a digital data stream is packed directly into the Ethernet frames and send to the destination PC using point-to point connection. The standard Ethernet frames are used in this design, additionally equipped with one byte carrying information on data type. When a high throughput is needed the data type is employed to prioritize them. This moderately simple but very powerful interface is relatively easy to be implemented in many applications [1]. The custom approach chosen for FPGA implementation causes a need to prepare dedicated software suite to process all incoming data in the PC side. The developed software package is called EPPRO (Ethernet Packet PROxy) since it exploits special Ether net frames for data exchange. The core part of EPPRO is a Linux kernel module, responsible for data reception/transmission and dispatching, taking into account their types to filter and prioritize the incoming packets. Overall performance of the whole system has been evaluated in respect to its throughput and reliability, presented test results confirm that all of the design goals have been fulfilled.","1082-3654;1082-3654","978-1-4673-0120-6978-1-4673-0118-3978-1-4673-0119","10.1109/NSSMIC.2011.6154542","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6154542","FPGA;Ethernet;network protocol;kernel driver;readout system","Field programmable gate arrays;Sockets;Indexes;Payloads;Clocks;Decoding;Random access memory","data acquisition;data communication;digital storage;electronic data interchange;field programmable gate arrays;flexible electronics;high energy physics instrumentation computing;Linux;local area networks;microcomputers;nuclear electronics;readout electronics;software packages","readout data acquisition system;detection system;FPGA;front-end electronics;data processing element;incoming data;data processing;data exchange;FPGA mezzanine board;ethernet gigabit connection;FPGA FIFO;digital data stream;PC;standard Ethernet frame;EPPRO software package;Linux kernel module;data reception;data transmission;data dispatching;incoming data packet","","5","8","","","","","","IEEE","IEEE Conferences"
"CHASE: An Autonomic Service Engine for Cloud Environments","M. Rak; A. Cuomo; U. Villano","NA; NA; NA","2011 IEEE 20th International Workshops on Enabling Technologies: Infrastructure for Collaborative Enterprises","","2011","","","116","121","The rapidly spreading cloud computing paradigm delegates to the network the provision of most resources, even those strictly linked to hardware as storage and CPU time. This approach enables the development of applications which may exploit a variable amount of resources in a flexible way, so as to satisfy the actual load of requests coming from the users. A side effect of such flexibility is that optimization has to be more focused on user-perceived performance indexes than on resource usage. This paper takes a step in this direction, presenting the design and development of CHASE, an autonomic engine designed to optimize the scheduling of virtual machines in a cloud environment. The paper illustrates the CHASE architecture and its application in two different contexts: in PerfCloud, an environment for IaaS provision based on cloud and grid integration, and inside Cloud@Home, a project whose objective is to build a cloud using volunteer-based resources. Some preliminary experimental results based on HPC applications are presented.","1524-4547;1524-4547","978-1-4577-0134-4978-0-7695-4410","10.1109/WETICE.2011.21","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5990035","Scheduling;Cloud Computing;Autonomic Computing;Grid Computing;Performance Prediction","Computer architecture;Unified modeling language;Benchmark testing;Virtual machining;Optimization;Engines;Computational modeling","cloud computing;grid computing;scheduling;software architecture;virtual machines","CHASE architecture;autonomic service engine;cloud computing paradigm;user-perceived performance indexes;resource usage;virtual machine scheduling;PerfCloud;IaaS provision;grid integration;cloud integration;Cloud@Home;volunteer-based resources","","6","25","","","","","","IEEE","IEEE Conferences"
"A multiobjective optimisation approach for the dynamic inference and refinement of agent-based model specifications","S. F. Adra; M. Kiran; P. McMinn; N. Walkinshaw","Microsoft, STC, London, UK; School of Computing, University of Leeds, Leeds, UK; Dept. of Computer Science, University of Sheffield, Sheffield, UK; Dept. of Computer Science, University of Leicester, Leicester, UK","2011 IEEE Congress of Evolutionary Computation (CEC)","","2011","","","2237","2244","Despite their increasing popularity, agent-based models are hard to test, and so far no established testing technique has been devised for this kind of software applications. Reverse engineering an agent-based model specification from model simulations can help establish a confidence level about the implemented model and in some cases reveal discrepancies between observed and normal or expected behaviour. In this study, a multiobjective optimisation technique based on a simple random search algorithm is deployed to dynamically infer and refine the specification of three agent-based models from their simulations. The multiobjective optimisation technique also incorporates a dynamic invariant detection technique which serves to guide the search towards uncovering new model behaviour that better captures the model specification. The Non-dominated Sorting Genetic Algorithm (NSGA-II) was also deployed to replace the random search algorithm, and the results from both approaches were compared. While both algorithms revealed good potential in capturing the model specifications, the pure exploratory nature of random search was found more suitable for the application at hand, compared to the balanced exploitation/exploration nature of genetic algorithms in general.","1941-0026;1089-778X","978-1-4244-7835-4978-1-4244-7834-7978-1-4244-7833","10.1109/CEC.2011.5949892","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5949892","","Rabbits;Predator prey systems;Computational modeling;Economics;Search problems;Skin;Biological system modeling","genetic algorithms;inference mechanisms;multi-agent systems;search problems","multiobjective optimisation approach;dynamic inference;agent based model specifications;reverse engineering;random search algorithm;dynamic invariant detection technique;nondominated sorting genetic algorithm","","1","19","","","","","","IEEE","IEEE Conferences"
"Wavelet based processing of physiological signals for purposes of embedded computing","S. Knežević; R. Stojanović; J. Kovačević; D. Karadaglić","Faculty of Electrical Engineering, University of Montenegro, Podgorica, Montenegro; Faculty of Electrical Engineering, University of Montenegro, Podgorica, Montenegro; Faculty of Electrical Engineering, University of Montenegro, Podgorica, Montenegro; School of Electrical and Electronic Engineering, University of Manchester, United Kingdom","2012 Mediterranean Conference on Embedded Computing (MECO)","","2012","","","42","45","The Wavelet Transform in its discrete form has been applied to a wide range of biomedical signals by now. Typically, its calculation is performed off-line and calculation systems suffer from limited autonomy, bulkiness and obtrusiveness. A surge in industrial, research and academic interest into telemedicine and medical embedded systems, has been noticed recently, where miniature, low-cost, autonomous and ultra-low-power devices play a major role. Such devices are usually based on microcontrollers, which in addition to other tasks need to perform signal processing, very often in real-time. This paper presents a methodology to perform wavelet transform on general purpose microcontrollers. By using its optimized versions the electrocardiogram and photoplethysmographic signals are processed in real time for the purposes of QRS complex extraction and denoising. After the theoretical considerations on wavelets and their optimization in integer arithmetic, the embedded hardware and software computation architectures are described. The following is the presentation of obtained results during intensive tests on real signals. The same approach can be applied with other signals where the embedded implementation of wavelets can be benefitial.","2377-5475","978-9940-9436-0-8978-1-4673-2366","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6268920","wavelet transform;microcontroller;QRS;denoising","Electrocardiography;Noise reduction;Real time systems;Discrete wavelet transforms;Microcontrollers","discrete wavelet transforms;electrocardiography;embedded systems;medical signal processing;microcontrollers;photoplethysmography;physiology;signal denoising","physiological signal processing;discrete wavelet transform;biomedical signals;general-purpose microcontrollers;optimized electrocardiogram signals;optimized photoplethysmographic signals;QRS complex extraction;QRS complex denoising;integer arithmetic;embedded hardware computation architectures;embedded software computation architectures;off-line calculation;telemedicine;medical embedded systems;power devices;PPG;ECG","","","9","","","","","","IEEE","IEEE Conferences"
"Research and implementation of iSCSI-based SAN static data encryption system","X. Xuedong; S. Jiyuan","School of Mechanical &amp;Electrical Engineering, Changchun institute of Technology, CCIT, Changchun, China; Technology Department, Changchun E-sun Software Co., ltd, Esunsoft, Changchun, China","Proceedings of 2012 2nd International Conference on Computer Science and Network Technology","","2012","","","257","260","Proposed and realized a SAN storage and encryption system, it is based on iSCSI technology to achieve a static data encryption. which realized a flexible security management strategy by multiple-key and multiple layer encryption system to project the physical resources through iSCSI protocol stack and virtualization technique. Besides, as it has optimized iSCSI's function through RC aggregation cache data pack and this system is proved to function well in simulation test. This system is as good as or better than other products in this line on the international market, by saving about 10% of the conversation expenditure.","","978-1-4673-2964-4978-1-4673-2963-7978-1-4673-2962","10.1109/ICCSNT.2012.6525933","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6525933","iSCSI;SAN;backend encryption;virtualization","","cache storage;computer network security;cryptographic protocols;storage area networks;virtualisation","iSCSI-based SAN static data encryption system;SAN storage and encryption system;iSCSI technology;flexible security management strategy;multiple-key encryption system;multiple layer encryption system;iSCSI protocol stack;virtualization technique;optimized iSCSI function;RC aggregation cache data pack;storage area network","","","10","","","","","","IEEE","IEEE Conferences"
"Compact Differential Evolution","E. Mininno; F. Neri; F. Cupertino; D. Naso","Department of Mathematical Information Technology, University of Jyväskylä, Jyväskylä, Finland; Department of Mathematical Information Technology, University of Jyväskylä, Jyväskylä, Finland; Department of Electrical and Electronic Engineering, Technical University of Bari, Bari, Italy; Department of Electrical and Electronic Engineering, Polytechnic Institute of Bari, Bari, Italy","IEEE Transactions on Evolutionary Computation","","2011","15","1","32","54","This paper proposes the compact differential evolution (cDE) algorithm. cDE, like other compact evolutionary algorithms, does not process a population of solutions but its statistic description which evolves similarly to all the evolutionary algorithms. In addition, cDE employs the mutation and crossover typical of differential evolution (DE) thus reproducing its search logic. Unlike other compact evolutionary algorithms, in cDE, the survivor selection scheme of DE can be straightforwardly encoded. One important feature of the proposed cDE algorithm is the capability of efficiently performing an optimization process despite a limited memory requirement. This fact makes the cDE algorithm suitable for hardware contexts characterized by small computational power such as micro-controllers and commercial robots. In addition, due to its nature cDE uses an implicit randomization of the offspring generation which corrects and improves the DE search logic. An extensive numerical setup has been implemented in order to prove the viability of cDE and test its performance with respect to other modern compact evolutionary algorithms and state-of-the-art population-based DE algorithms. Test results show that cDE outperforms on a regular basis its corresponding population-based DE variant. Experiments have been repeated for four different mutation schemes. In addition cDE outperforms other modern compact algorithms and displays a competitive performance with respect to state-of-the-art population-based algorithms employing a DE logic. Finally, the cDE is applied to a challenging experimental case study regarding the on-line training of a nonlinear neural-network-based controller for a precise positioning system subject to changes of payload. The main peculiarity of this control application is that the control software is not implemented into a computer connected to the control system but directly on the micro-controller. Both numerical results on the test functions and experimental results on the real-world problem are very promising and allow us to think that cDE and future developments can be an efficient option for optimization in hardware environments characterized by limited memory.","1089-778X;1089-778X;1941-0026","","10.1109/TEVC.2010.2058120","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5675671","Adaptive systems;compact genetic algorithms;differential evolution (DE);estimation distribution algorithms","Algorithm design and analysis;Optimization;Evolutionary computation;Training;Robots;Memory management;Hardware","evolutionary computation;formal logic;learning (artificial intelligence);microcontrollers;neurocontrollers;nonlinear control systems;position control;search problems","compact differential evolution;cDE algorithm;evolutionary algorithm;statistic description;search logic;optimization process;population based algorithm;neural network based training;nonlinear neural network based controller;positioning system;precise positioning system;control software;microcontroller system;limited memory","","129","63","","","","","","IEEE","IEEE Journals & Magazines"
"Intelligent mobile safety system to educational organization","L. Chen","Department of Information Management, Fortune Institute of Technology, No. 1-10, Nongchang Rd., Daliao Township, Kaohsiung County 83160, Kaohsiung, Taiwan (R.O.C.)","2010 International Conference on e-Business (ICE-B)","","2010","","","1","8","This study aims to develop safety system, and let the system become intelligent. We adopt the swarm intelligence and active Ultra-High Frequency RFID for safety system, and develop friendly human-computer-interface software for users use the personal digital assistants. We program the system and software with Extensible Markup Language (XML) and C sharp language. If the users begin to search, the kernel safety system automatically communicates with other RFID readers by agents, and the agents can search the closer camera for users. This study's result has successfully implemented to one educational organization, and it would be helpful for the paterfamiliases to hold all situations about their children at the educational organization. That will be great help in the grip of whole after-school remedial education, teaching and learning situation. We select 100 paterfamiliases to test this system and software. It is revealed that 93% of the paterfamilias are satisfied with the system (Strongly agree : 25/100; Agree : 68/100; Disagree : 5/100; Strongly disagree : 2/100). The software searching correctness is 95% (Strongly agree : 30/100; Agree : 65/100; Disagree : 3/100; Strongly disagree : 2/100).","","978-989-8425-17","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5740447","Swarm Intelligence;Active Ultra-High Frequency;RFID;Human-Computer-Interface;After-School Remedial Education","Radiofrequency identification;Generators;Safety;Software;XML;Particle swarm optimization;Mobile communication","C language;educational institutions;human computer interaction;intelligent tutoring systems;mobile computing;radiofrequency identification;teaching;XML","intelligent mobile safety system;educational organization;swarm intelligence;active ultrahigh frequency RFID;friendly human computer interface software;personal digital assistant;extensible markup language;C sharp language;teaching system;after school remedial education","","","33","","","","","","IEEE","IEEE Conferences"
"Multimodal Transport Distribution Network Design with Time Window","L. Tang; J. Huo","NA; NA","2011 International Conference on Management and Service Science","","2011","","","1","4","There are multiple choices of transport modes crossing in logistics process, namely, the existence of multimodal transport issues. The multimodal transportation is used in distributed supply chain environment, and multimodal transport network is deformed. Considerate of the production costs, transportation costs, earliness/tardiness penalty costs and sale price comprehensively, the maximize object optimization mode of distribution supply chain network profit is established. Lingo software is applied to a given example for testing, and it achieved good results.","","978-1-4244-6581-1978-1-4244-6579-8978-1-4244-6580","10.1109/ICMSS.2011.5998316","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5998316","","Supply chains;Production facilities;Roads;Rails;Optimization","costing;goods distribution;profitability;supply chains;transportation","multimodal transport distribution network design;time window;transport modes;logistics process;multimodal transport issues;distributed supply chain environment;production costs;transportation costs;penalty costs;sale price;Lingo software;distribution supply chain network profit","","","9","","","","","","IEEE","IEEE Conferences"
"Notice of Retraction<br>Methodology for Model-Based Calibration on 4jb1 High-Pressure Common-Rail Diesel Engine","L. Xuesong; W. Shangyong; Z. Youtong; X. Qinghui","NA; NA; NA; NA","2010 Second International Workshop on Education Technology and Computer Science","","2010","3","","103","106","Calibration of engine management systems requires considerable engineering resources during the development of modern engines. Common-rail diesel engine increases a greater level of system complexity and greatly increases test requirements to achieve successful calibrations. This study is about calibration of the 4JB1 high-pressure common-rail diesel engine based on a simulation model which built up in the GT-Power software. The model is verified by the comparison between simulation and experiment results on some operating points. The model can be applied to calibrate and optimize the injection parameters through the method of DoE (Design of Experiments), and get a set of injection parameters MAPs. From the experiment results, the simulation model can fulfill the calibration needs of the 4JB1 diesel engine. This method gets a set of pre-calibration MAPs off-line which is critical to reduce the development period and cost.;Notice of Retraction<br><br>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<br><br>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<br><br>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<br><br>","","978-1-4244-6389-3978-1-4244-6388","10.1109/ETCS.2010.546","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5460111","diesel engine;high-pressure common-rail;DoE;injection parameters MAP;calibration","Calibration;Diesel engines;Mathematical model;Object oriented modeling;Costs;Rails;Fuels;Chromium;Educational technology;Design optimization","calibration;design of experiments;diesel engines;digital simulation;mechanical engineering computing","model-based calibration;4jb1 high-pressure common-rail diesel engine;engine management systems calibration;GT-power software;design of experiments;injection parameters MAP","","","5","","","","","","IEEE","IEEE Conferences"
"Design space exploration of a 2-D DWT system architecture","I. Sameen; Yoong Choon Chang; Ng Mow Song; Bok-Min Goi; Chee Siong Lee","Faculty of Engineering, Multimedia University, Cyberjaya, Selangor, Malaysia; Faculty of Engineering, Multimedia University, Cyberjaya, Selangor, Malaysia; Faculty of Engineering and Science, Universiti Tunku Abdul Rahman, Kuala Lumpur, Malaysia; Faculty of Engineering and Science, Universiti Tunku Abdul Rahman, Kuala Lumpur, Malaysia; Penang Design Center, Intel Microelectronics Sdn. Bhd, Malaysia","2010 IEEE Conference on Cybernetics and Intelligent Systems","","2010","","","36","40","This paper proposes a programmable 2-D DWT system architecture designed for the JPEG-2000 standard. The proposed system architecture, derived from an iterative design space exploration process using Altera's C2H compiler, provides a significant performance acceleration of 2-D DWT when compared to an optimized 2-D DWT software implementation and is capable of real-time video processing performance up to 720p (1280 × 720) image resolutions when synthesized and tested on an Altera DE3 Stratix III FPGA board.","2326-8123;2326-8239","978-1-4244-6502-6978-1-4244-6499-9978-1-4244-6501","10.1109/ICCIS.2010.5518584","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5518584","Discrete Wavelet Transform (DWT);Field-Programmable Gate Array (FPGA);Design Space Exploration","Space exploration;Discrete wavelet transforms;Computer architecture;Process design;Life estimation;Design optimization;Optimizing compilers;Software performance;Real time systems;Image resolution","discrete wavelet transforms;electronic engineering computing;field programmable gate arrays;image coding;image resolution;logic design;program compilers;video signal processing","programmable 2D DWT system architecture;JPEG-2000 standard;iterative design space exploration process;Altera C2H compiler;real-time video processing;image resolution;Altera DE3 Stratix III FPGA board;discrete wavelet transform","","","7","","","","","","IEEE","IEEE Conferences"
"DSP-Driven Self-Tuning of RF Circuits for Process-Induced Performance Variability","D. Han; B. S. Kim; A. Chatterjee","NA; NA; NA","IEEE Transactions on Very Large Scale Integration (VLSI) Systems","","2010","18","2","305","314","In the deep-submicrometer design regime, RF circuits are expected to be increasingly susceptible to process variations, and thereby suffer from significant loss of parametric yield. To address this problem, a postmanufacture self-tuning technique that aims to compensate for multiparameter variations is presented. The proposed method incorporates a ¿response feature¿ detector and ¿hardware tuning knobs,¿ designed into the RF circuit. The RF device test response to a specially crafted diagnostic test stimulus is logged via the built-in detector and embedded analog-to-digital converter. Analysis and prediction of the optimal tuning knob control values for performance compensation is performed using software running on the baseband DSP processor. As a result, the RF circuit performance can be diagnosed and tuned with minimal assistance from external test equipment. Multiple RF performance parameters can be adjusted simultaneously under tuning knob control. The proposed concepts are illustrated for an RF low-noise amplifier (LNA) design and can be applied to other RF circuits as well. A simulation case study and hardware measurements on a fabricated 1.9-GHz LNAs show significant parametric yield enhancement (up to 58%) across the critical RF performance specifications of interest.","1063-8210;1557-9999","","10.1109/TVLSI.2008.2009454","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5153126","Parametric yield;process variability;RF circuits;self-tuning","Radio frequency;Circuit optimization;Detectors;Circuit testing;Tuning;Computer vision;Analog-digital conversion;Performance analysis;Optimal control;Software performance","circuit tuning;low noise amplifiers;mixed analogue-digital integrated circuits;network topology;signal processing;UHF amplifiers","DSP-driven self-tuning;RF circuits;process-induced performance variability;analog-to-digital converter;RF low-noise amplifier;response feature detector;hardware tuning knob;LNA;frequency 1.9 GHz","","52","30","","","","","","IEEE","IEEE Journals & Magazines"
"Implementation of a genetic algorithm-based decision making framework for opportunistic radio","S. Chantaraskul; K. Moessner","University of Surrey, Guildford GU2 7XH, UK; University of Surrey, Guildford GU2 7XH, UK","IET Communications","","2010","4","5","495","506","The cognitive radio (CR) is known as a radio that can reconfigure its transceiver parameters based on the environmental awareness. The opportunistic radio (OR) is considered in this work, with a narrower definition where the awareness is limited to the spectrum knowledge. The decision making framework is employed as a crucial entity to control the behaviour of the OR. The main purpose is to enable an efficient spectrum usage while avoiding the interference to other users. This study describes the proposed OR decision making framework including the flow of context information as an input process to the decision making engine, the context filtering and the reasoning mechanisms in which the decision optimisation is achieved using a genetic algorithm (GA)-based approach. The system stability of the GA-based reasoning engine is tested through simulations. Then, the experimental study is performed on a test platform for a practical proof of the concept. The test platform is based on the Ettus USRP (Universal Software Radio Peripheral) hardware and the GNU Radio open source software. Several tests were carried out to observe the OR capabilities of the proposed decision making framework. Test environment settings together with the observation results are provided in this study, covering the spectrum sensing and opportunistic channel allocation in the industrial, scientific and medical (ISM) band of 2.4 GHz.","1751-8628;1751-8636","","10.1049/iet-com.2009.0479","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5437521","","","cognitive radio;decision making;genetic algorithms;inference mechanisms;transceivers","genetic algorithm;opportunistic radio;cognitive radio;transceiver parameters;environmental awareness;spectrum knowledge;context information;decision making engine;reasoning mechanisms;context filtering;Ettus universal software radio peripheral hardware;GNU radio open source software","","4","","","","","","","IET","IET Journals & Magazines"
"New foundry models - accelerations in transformations of the semiconductor industry","M. Chian","GLOBAL FOUNDRIES, Germany","2012 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2012","","","2","2","Summary form only given. Moore's Law continues to deliver ever-more transistors on an integrated circuit, but discontinuities in the progress of technology mean that the future isn't simply an extrapolation of the past. For example, design cost and complexity constraints have recently caused the microprocessor industry to switch to multi-core architectures, even though these parallel machines present programming challenges that are far from solved. Moore's Law now translates into ever-more processors on a multi-, and soon many-core chip. The software challenge is compounded by the need for increasing fault-tolerance as near-atomic-scale variability and robustness problems bite harder. We look beyond this transitional phase to a future where the availability of processor resource is effectively unlimited and computations must be optimised for energy usage rather than load balancing, and we look to biology for examples of how such systems might work. Conventional concerns such as synchronisation and determinism are abandoned in favour of real-time operation and adapting around component failure with minimal loss of system efficacy.","1558-1101;1530-1591;1530-1591","978-1-4577-2145-8978-3-9810801-8","10.1109/DATE.2012.6176422","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6176422","","","","","","","","","","","","","IEEE","IEEE Conferences"
"IMS Threat and Attack Surface Analysis Using Common Vulnerability Scoring System","S. Petajasoja; H. Kortti; A. Takanen; J. Tirila","NA; NA; NA; NA","2011 IEEE 35th Annual Computer Software and Applications Conference Workshops","","2011","","","68","73","For the purposes of this study, IMS specifications and public sources were analyzed using the general attack surface analysis methodology. These findings were verified and augmented by active scanning and passive analysis of the available real-world IMS test setups that were investigated during the project. As various tests and security probes were performed against the test setups, the system behaviour was analyzed for previously undetermined interactions and transient attack surfaces. After the IMS attack vectors had been identified, the Common Vulnerability Scoring System version 2 (CVSSv2) Base Scores were used to prioritize the IMS attack surface interfaces. CVSS is an industry standard for classifying vulnerabilities. It must be noted however that the idea of applying CVSS scoring to an a priori comparison of vulnerability categories and potential attack surfaces is original research by the authors of this study.","","978-1-4577-0980-7978-0-7695-4459","10.1109/COMPSACW.2011.22","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032214","next generation networks;security;threat analysis;attack surface analysis","Measurement;Protocols;Availability;Authentication;Complexity theory;Surface treatment","computer network security;IP networks;multimedia systems","IMS threat;IMS attack surface analysis;IMS specifications;public sources;general attack surface analysis methodology;active scanning;passive analysis;IMS attack vectors;common vulnerability scoring system version 2 base scores;IMS attack surface interfaces;IP multimedia subsystem","","4","6","","","","","","IEEE","IEEE Conferences"
"Hooke and Jeeves algorithm for linear support vector machine","Y. Liu; S. Liu; M. Gu","Department of Mathematical Sciences, Xidian University, Xi'an 710071, P. R. China; School of Science, Henan University of Science &#x0026; Technology, Luoyang 471003, P. R. China; School of Science, Henan University of Science &#x0026; Technology, Luoyang 471003, P. R. China; PLA Unit 96251, Luoyang 471003, P. R. China","Journal of Systems Engineering and Electronics","","2010","21","1","138","141","Coordinate descent method is a unconstrained optimization technique. When it is applied to support vector machine (SVM), at each step the method updates one component of w by solving a one-variable sub-problem while fixing other components. All components of w update after one iteration. Then go to next iteration. Though the method converges and converges fast in the beginning, it converges slow for final convergence. To improve the speed of final convergence of coordinate descent method, Hooke and Jeeves algorithm which adds pattern search after every iteration in coordinate descent method was applied to SVM and a global Newton algorithm was used to solve one-variable sub-problems. We proved the convergence of the algorithm. Experimental results show Hooke and Jeeves' method does accelerate convergence specially for final convergence and achieves higher testing accuracy more quickly in classification.","1004-4132","","10.3969/j.issn.1004-4132.2010.01.022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6073064","support vector machine;classification;pattern search;Hooke and Jeeves;coordinate descent;global Newton algorithm","Support vector machines;Convergence;Training;Testing;Optimization;Accuracy;Educational institutions","","","","","","","","","","","BIAI","BIAI Journals & Magazines"
"Evolving a CUDA kernel from an nVidia template","W. B. Langdon; M. Harman","CREST centre, Department of Computer Science, King's College, London, Strand, London, WC2R 2LS, UK; CREST centre, Department of Computer Science, King's College, London, Strand, London, WC2R 2LS, UK","IEEE Congress on Evolutionary Computation","","2010","","","1","8","Rather than attempting to evolve a complete program from scratch we demonstrate genetic interface programming (GIP) by automatically generating a parallel CUDA kernel with identical functionality to existing highly optimised ancient sequential C code (gzip). Generic GPGPU nVidia kernel C++ code is converted into a BNF grammar. Strongly typed genetic programming uses the BNF to generate compilable and executable graphics card kernels. Their fitness is given by running the population on a GPU with randomised subsets of training data itself derived from gzip's SIR test suite. Back-to-back validation uses the original code as a test oracle.","1089-778X;1941-0026","978-1-4244-6911-6978-1-4244-6909-3978-1-4244-6910","10.1109/CEC.2010.5585922","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5585922","","Grammar;Kernel;Humans;Training data;Graphics processing unit;Training;Testing","C language;computer graphic equipment;genetic algorithms;operating system kernels;parallel processing;software engineering;user interfaces","CUDA kernel;nVidia template;genetic interface programming;parallel CUDA kernel;highly optimised ancient sequential C code;Generic GPGPU nVidia kernel C++ code;BNF grammar;executable graphics card kernels;gzip;compute unified device architecture","","22","17","","","","","","IEEE","IEEE Conferences"
"Research on Cylindricity Error Calculation Based on Improved GA","X. Qian; P. Lou","NA; NA","2010 International Conference of Information Science and Management Engineering","","2010","1","","181","184","An objective function is proposed in this paper to evaluate the minimum zone cylindricity error. The error model is optimized by Genetic Algorithm (GA). The mathematical model may work out the minimum zone solution of cylindricity error with arbitrary position in space, and there are no special requirements in choosing measurement points. A test has been given to prove that optimal approximation solution to cylinder axis's vectors and minimum zone cylindricity can be worked out by the objective function. The approach can also be extended for solving other form and position errors when cylinder axis is used as datum.","","978-1-4244-7670-1978-1-4244-7669","10.1109/ISME.2010.37","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5574035","Cylindricity Error Evaluation;Minimum Zone;Genetic Algorithm","Biological cells;Measurement uncertainty;Optimization;Software algorithms;Extraterrestrial measurements;Coordinate measuring machines;Search problems","error analysis;genetic algorithms;shapes (structures)","cylindricity error calculation;genetic algorithm;minimum zone cylindricity error","","","10","","","","","","IEEE","IEEE Conferences"
"Time Windows Based Dynamic Routing in Multi-AGV Systems","N. Smolic-Rocak; S. Bogdan; Z. Kovacic; T. Petrovic","NA; NA; NA; NA","IEEE Transactions on Automation Science and Engineering","","2010","7","1","151","155","This paper presents a dynamic routing method for supervisory control of multiple automated guided vehicles (AGVs) that are traveling within a layout of a given warehouse. In dynamic routing a calculated path particularly depends on the number of currently active AGVs' missions and their priorities. In order to solve the shortest path problem dynamically, the proposed routing method uses time windows in a vector form. For each mission requested by the supervisor, predefined candidate paths are checked if they are feasible. The feasibility of a particular path is evaluated by insertion of appropriate time windows and by performing the windows overlapping tests. The use of time windows makes the algorithm apt for other scheduling and routing problems. Presented simulation results demonstrate efficiency of the proposed dynamic routing. The proposed method has been successfully implemented in the industrial environment in a form of a multiple AGV control system.","1545-5955;1558-3783","","10.1109/TASE.2009.2016350","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4907246","Automated guided vehicles;dynamic routing;path feasibility;time windows","Routing;Vehicle dynamics;Supervisory control;Vehicles;Shortest path problem;Performance evaluation;Testing;Scheduling algorithm;Job shop scheduling;Electrical equipment industry","automatic guided vehicles;integrated software;path planning","time windows based dynamic routing;multi AGV systems;supervisory control;automated guided vehicles;shortest path problem;vector form;predefined candidate paths;windows overlapping tests;industrial environment","","39","29","","","","","","IEEE","IEEE Journals & Magazines"
"A new framework for large distribution system optimal planning in a competitive electricity market","S. Porkar; P. Poure; A. Abbaspour-Tehrani-fard; S. Saadate","Department of Electrical Engineering, Sharif University of Technology, Tehran, Iran; Laboratoire d'Instrumentation Electronique de Nancy, LIEN, EA 3440, Universit&#x00E9; Henri Poincar&#x00E9; de Nancy I, BP 239, 54506 Vandoeuvre les Nancy cedex France; Department of Electrical Engineering, Sharif University of Technology, Tehran, Iran; Groupe de Recherches en Electrotechnique et Electronique de Nancy, GREEN-UHP, UMR 7037, Universit&#x00E9; Henri Poincar&#x00E9; de Nancy I, BP 239, 54506 Vandoeuvre les Nancy cedex France","2010 IEEE International Energy Conference","","2010","","","1","6","Distribution systems management is becoming an increasingly complicated issue due to the introduction of new technologies, new energy trading strategies and new deregulated environment. In the new deregulated energy market and considering the incentives coming from the technical and economical fields, it is reasonable to consider Distributed Generation (DG) as a viable option to solve the lacking electric power supply problem. This paper presents a mathematical distribution system planning model considering three planning options to system expansion and to meet the load growth requirements with a reasonable price as well as the system power quality problems. DG is introduced as an attractive planning option with competition of voltage regulator devices and interruptible load. In mathematical model, the object function includes investment costs, which are evaluated as annualized total cost, plus total running cost as well as cost of curtailed loads and losses. This model identifies the optimal type, size and location of the planning options. This paper is also studied fluctuation of load and electricity market price versus time period and the effect of DG placement on system improvement. To solve the proposed mathematical planning model a new software package interfacing MATLAB and GAMS is developed. This package is enabling to solve large extent distribution system planning program visually and very fast. The proposed methodology is tested in the case of the well-known IEEE 30-bus test system.","","978-1-4244-9380-7978-1-4244-9378-4978-1-4244-9379","10.1109/ENERGYCON.2010.5771675","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5771675","Deregulation;Distributed Generation (DG);Distribution Company (DISCO);GAMS-MATLAB interface;Optimization","Planning;Mathematical model;Investments;Optimization;MATLAB;Load modeling;Reactive power","distributed power generation;investment;power distribution economics;power distribution planning;power markets","large distribution system optimal planning;competitive electricity market;distribution systems management;energy trading;distributed generation;load growth;object function;investment costs;annualized total cost;total running cost;MATLAB;GAMS;IEEE 30-bus test system","","","17","","","","","","IEEE","IEEE Conferences"
"Three-dimensional analysis on resistance tomography sensors and its application in inspection of milk flow","P. Wang; B. Guo; N. Li","Institute of Intelligent Control and Image Engineering, Xidian University, 710071, China; Institute of Intelligent Control and Image Engineering, Xidian University, 710071, China; Institute of Intelligent Control and Image Engineering, Xidian University, 710071, China","IEEE ICCA 2010","","2010","","","693","697","3D simulation of ERT sensor is established with multi-physical coupling software COMSOL and MATLAB7.0, analyze the limitation of ERT system 3D sensor field and deeply study the influence on the established fields by sizes of electrodes and frequency characteristics. Thus, design two kinds of sensors of pointed electrode array and rectangular-shaped electrode array and also carry out comparison test on frequency characteristics during the detection of milk flows.. The result shows that increasing width and height of electrodes is favorable for improving uniformity of sensitivity field in a radial and axial distributions. It also shows that increasing area of electrodes can reduce “Contact resistance” and have more stable frequency characteristic, which is good for increasing SNR of ERT system, thereby, providing evidence and convenience for optimization design of sensors and reconstruction of images.","1948-3457;1948-3449;1948-3449","978-1-4244-5195-1978-1-4244-5196","10.1109/ICCA.2010.5524069","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5524069","","Tomography;Inspection;Dairy products;Sensor phenomena and characterization;Electrodes;Sensor arrays;Sensor systems and applications;Frequency;Application software;Analytical models","computerised instrumentation;contact resistance;electric sensing devices;flow measurement;mathematics computing;tomography","three-dimensional analysis;electrical resistance tomography;milk flow inspection;3D simulation;ERT sensor;multiphysical coupling software;COMSOL;MATLAB7.0;3D sensor field;pointed electrode array;rectangular-shaped electrode array;radial distributions;axial distributions;contact resistance","","","9","","","","","","IEEE","IEEE Conferences"
"Wideband automatic level control circuit for wideband signal generator application","H. Cao; L. Tian; W. Hong","State Key Laboratory of Millimeter Waves, School of Information Science and Technology Southeast University, Nanjing, 210096, P.R. China; State Key Laboratory of Millimeter Waves, School of Information Science and Technology Southeast University, Nanjing, 210096, P.R. China; State Key Laboratory of Millimeter Waves, School of Information Science and Technology Southeast University, Nanjing, 210096, P.R. China","2012 International Conference on Microwave and Millimeter Wave Technology (ICMMT)","","2012","3","","1","4","This paper presents the design and implementation of the output power control circuit for a signal generator from 3GHz to 6GHz, which is achieved with automatic level control (ALC) technique using voltage variable attenuator. The performance of the ALC loop is simulated and optimized by using commercial EDA software. The digital control system consisting of PC, MCU and DAC is designed to calibrate the output power of ALC by calibration routine. The testing results indicate that the ALC loop circuit possesses good performance within the operating frequency range from 3GHz to 6GHz, the input dynamic range as large as 28dB and the output power variation lower than 0.1dB. The measured results show that the developed ALC loop circuit is good enough to be applied into the output power control for signal generator in microwave frequency band.","","978-1-4673-2185-3978-1-4673-2184","10.1109/ICMMT.2012.6230224","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6230224","Automatic level control;flatness;large dynamic range;signal generator;wideband","Power generation;Power amplifiers;Voltage control;Signal generators;Attenuators;Software;Dynamic range","attenuators;digital-analogue conversion;signal generators","wideband automatic level control circuit;ALC loop circuit;wideband signal generator;voltage variable attenuator;EDA software;PC;MCU;digital-to-analog converters;DAC;output power variation;output power control;frequency 3 GHz to 6 GHz","","2","11","","","","","","IEEE","IEEE Conferences"
"Research on the low-power of air-conditioner remote control system terminal device based on Zigbee technology","X. Yang; S. Wang; X. Lian; X. Zhang; Z. Duan","Beijing Technology and Business University, Beijing, China; Beijing Technology and Business University, Beijing, China; Beijing Technology and Business University, Beijing, China; Beijing Technology and Business University, Beijing, China; Beijing Technology and Business University, Beijing, China","2012 3rd International Conference on System Science, Engineering Design and Manufacturing Informatization","","2012","2","","184","187","In the field of intelligent home, when using Zigbee technology to network the management of air-conditioner control device, it is very necessary to reduce the power consumption of battery-powered devices in the network. Through the research of Battery-powered equipment composed by the CC2530 and MSP430, we provide a practical way to extend the working hours of the equipment, to reduce the power consumption and to save the overall cost of the system from both software and hardware devices optimized. Test results show that, after low-power processing the terminal equipment, the working time of the two batteries, whose capacity are 800mAh 1.5V, grow from 11 days to about 280 days.","","978-1-4673-0915-8978-1-4673-0914","10.1109/ICSSEM.2012.6340839","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6340839","MSP430;Low-power technology;Zigbee technology;CC2530","Power demand;Zigbee;Control systems;Software;Hardware;Servers","air conditioning;home automation;intelligent control;telecontrol;Zigbee","air-conditioner remote control system terminal device;Zigbee technology;intelligent home;air-conditioner control device management;power consumption reduction;battery-powered device;battery-powered equipment;CC2530;MSP430;software device;hardware device;terminal equipment;current 800 mA;voltage 1.5 V","","","7","","","","","","IEEE","IEEE Conferences"
"2.45 GHz perfluator heating module for industrial infiltration processes","S. Stanculovic; L. E. Feher","Karlsruhe Institute of Technology, Germany; Karlsruhe Institute of Technology, Germany","2010 IEEE MTT-S International Microwave Symposium","","2010","","","1","1","A microwave (MW) perfluator heating module is a device for industrial resin infiltration processes. This MW system consists of a waveguide section, cylindrical holder of a Teflon hose for fluid injection, a MW source at 2.45 GHz and a control unit. It has been designed using full electromagnetic 3D numerical software tool CST Microwave Studio to provide an efficient energy transfer from the MW source to the liquid and to support an appropriate electromagnetic field structure for supperoior infiltration. This system has been optimized by experimental investigations for industrial production. In measurements with a vector network analyzer the system geometry has been adjusted to ensure the lowest reflections. Several prototypes have been successfully verified in standard as well as in sophisticated flow processes under industrial conditions. These high power tests have shown that the MW perfluator module is a flexible stand-alone unit that can be connected to existing product lines.","0149-645X;0149-645X;0149-645X","978-1-4244-6058-8978-1-4244-6056-4978-1-4244-7732-6978-1-4244-6057","10.1109/MWSYM.2010.5518290","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5518290","","Electrical equipment industry;Microwave devices;Electromagnetic heating;Electromagnetic fields;Resins;Electromagnetic waveguides;Hoses;Control systems;Software tools;Energy exchange","electromagnetic fields;geometry;microwave devices;process heating;resins","microwave perfluator heating module;industrial resin infiltration processes;Teflon hose;fluid injection;electromagnetic 3D numerical software tool;CST Microwave Studio;energy transfer;electromagnetic field structure;vector network analyzer;system geometry;frequency 2.45 GHz","","","","","","","","","IEEE","IEEE Conferences"
"An AVS VLD architecture based on HW/SW partitioning","Wei Liu; Xuejie Wang","School of Information and Electrical Engineering, Zhejiang University City College, Hangzhou, China; School of Information and Electrical Engineering, Zhejiang University City College, Hangzhou, China","Proceedings 2011 International Conference on Transportation, Mechanical, and Electrical Engineering (TMEE)","","2011","","","1453","1456","In this paper, an AVS variable length code decoder (VLD) architecture based on software/hardware partitioning is proposed. Fixed Length Code, unsigned or signed Exp-Golomb Code and context-based adaptive 2D-VLC (CA-2D-VLC) Code can be correctly decoded. As a result of reasonable design and optimization for nineteen 2D-VLC tables, a new code table design method is proposed and a reduction of more than 30% in hardware cycle number consumption is achieved. In order to ensure the rationality and correctness of the whole system, a verifier based on RM52J is presented as well. Tested by ninety-two streams, this design is proved to reach the requirement of AVS video decoding.","","978-1-4577-1701-7978-1-4577-1700-0978-1-4577-1699","10.1109/TMEE.2011.6199481","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6199481","AVS;HW/SW partition;variable length code;CA-2D-VLC","Decoding;Hardware;Syntactics;Software;Computer architecture;Encoding;Registers","adaptive codes;audio coding;decoding;variable length codes;video coding","AVS VLD architecture;audio video coding standard;variable length code decoder;HW-SW partitioning;software-hardware partitioning;fixed length code;unsigned Exp-Golomb code;signed Exp-Golomb code;context-based adaptive 2D-VLC code;CA-2D-VLC code;code table design method;hardware cycle number consumption;RM52J","","","6","","","","","","IEEE","IEEE Conferences"
"Mobility Effects on the Performance of Mobile Ad hoc Networks","M. Ikeda; M. Hiyama; L. Barolli; F. Xhafa; A. Durresi","NA; NA; NA; NA; NA","2010 International Conference on Complex, Intelligent and Software Intensive Systems","","2010","","","230","237","In this paper, we present the implementation and analysis of our implemented testbed considering the Optimized Link State Routing (OLSR) protocol for Mobile Ad hoc Networks (MANET). We investigate the effect of mobility and topology changing in the throughput of MANETs. We study the impact of best-effort traffic for Mesh Topology. Experimental time is 150 seconds. In this work, we consider 6 experimental models and we assess the performance of our testbed in terms of throughput, round trip time and packet loss. From our experiments, we found that the OLSR protocol has a good performance when the source node is moving. However, the performance is not good when the three relay nodes are moving.","","978-1-4244-5918-6978-1-4244-5917-9978-0-7695-3967","10.1109/CISIS.2010.90","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5447431","MANET;Mobility Effects;Topology Change;Testbed;Performance Evaluation","Mobile ad hoc networks;Routing protocols;Throughput;Intelligent networks;Performance analysis;System testing;Network topology;Telecommunication traffic;Traffic control;Performance loss","ad hoc networks;mobile radio;routing protocols;telecommunication network topology;telecommunication traffic","mobility effects;mobile ad hoc networks;optimized link state routing protocol;best-effort traffic;mesh topology","","1","22","","","","","","IEEE","IEEE Conferences"
"Provisioning Norm: An Asymmetric Quality Measure for SaaS Resource Allocation","M. B. Reynolds; K. M. Hopkinson; M. E. Oxley; B. E. Mullins","NA; NA; NA; NA","2011 IEEE International Conference on Services Computing","","2011","","","112","119","Large scale, multi-dimensional resource provisioning for Software as a Service (SaaS) presents a significant challenge. Analytical calculation of the quality of a configuration is necessary for effectively assigning new services to servers and reorganizing assigned services. This work describes the Provisioning Norm which meets this critical need. The Provisioning Norm (an asymmetric norm) analytically calculates the quality of a configuration, the placement of services on server nodes. The Provisioning Norm partially orders all possible configurations for a set of nodes and services from best to worst by numerically biasing over-provisioned configurations relative to under-provisioned configurations. This work proves that the parameter to the Provisioning Norm function has a value which partitions the partially ordered configurations into over-provisioned configurations and under-provisioned configurations. The application of the Provisioning Norm in a testing environment demonstrates a correlation between the analytical quality and the empirical performance.","","978-1-4577-0863-3978-0-7695-4462","10.1109/SCC.2011.16","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6009251","web services;quality;asymmetric norm;placement","Servers;Peer to peer computing;Virtual machining;Bandwidth;Databases;Optimization;Resource management","cloud computing;resource allocation;software quality","asymmetric quality measure;SaaS resource allocation;software as a service;provisioning norm;server nodes;partially ordered configurations;over provisioned configurations;under provisioned configurations","","4","18","","","","","","IEEE","IEEE Conferences"
"Synchronized-TSP as a model for multilocus genetic consensus mapping","D. Mester; Y. Ronin; D. Minkov; R. Belotzerkovsky; Z. Frenkel; A. Korol","Institute of Evolution, Department of Evolutionary and Environmental Biology, University of Haifa, Israel; Institute of Evolution, Department of Evolutionary and Environmental Biology, University of Haifa, Israel; Institute of Evolution, Department of Evolutionary and Environmental Biology, University of Haifa, Israel; Institute of Evolution, Department of Evolutionary and Environmental Biology, University of Haifa, Israel; Institute of Evolution, Department of Evolutionary and Environmental Biology, University of Haifa, Israel; Institute of Evolution, Department of Evolutionary and Environmental Biology, University of Haifa, Israel","2010 Sixth International Conference on Natural Computation","","2010","8","","4310","4317","Numerous mapping projects conducted on different organisms have generated an abundance of mapping data. Consequently, many multilocus maps were constructed using diverse mapping populations and marker sets for the same species. The quality of maps varied broadly between populations, marker sets, and applied software. There might be some inconsistencies between different versions of the maps for the same organism, calling for the integration of mapping information and building of consensus maps. The problem of multilocus consensus genetic mapping (MCGM) is even more challenging, compared to multilocus mapping based on one data set, due to several complications: differences in recombination rate and distribution along chromosomes, and different subsets of markers used by different labs. We developed an approach to solve MCGM problems, by searching multilocus orders with the maximum number of shared markers yielding maps with minimum total length. The approach is based on re-analysis of raw data and is implemented in a two-phase algorithm. In Phase 1, for each data set, multilocus ordering is performed combined with iterative re-sampling to evaluate the stability of marker orders. In this phase, the ordering problem is reduced to the well known Traveling Salesperson Problem (TSP). In Phase 2, consensus mapping is conducted by reducing the problem to a specific version of TSP that can be referred to as synchronized TSP. The optimal consensus order of shared markers is defined by the minimal total length of non-conflicting maps of the chromosome. This criterion includes various modifications that take into account the variation in the quality of the original data (e.g., population size, marker quality, etc.). We use our powerful Guided Evolution Strategy algorithm for discrete optimization of constrained problems that was adapted to solve MCGM problems. The developed approach was tested on a wide range of simulated data.","2157-9555;2157-9563","978-1-4244-5961-2978-1-4244-5958-2978-1-4244-5959","10.1109/ICNC.2010.5583617","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5583617","genetic mapping;consensus mapping;shared markers;guided evolution strategy algorithm;discrete optimization;synchronized-TSP","Optimization;Biological cells;Algorithm design and analysis;Genetics;Synchronization;Buildings;Stability criteria","biology computing;data analysis;DNA;evolutionary computation;genetics;iterative methods;travelling salesman problems","synchronized-TSP;multilocus genetic consensus mapping;diverse mapping population;map quality;mapping information;shared markers;multilocus ordering;iterative resampling;ordering problem;traveling salesperson problem;chromosome nonconflicting maps;population size;marker quality;guided evolution strategy algorithm;discrete optimization;constrained problems","","","20","","","","","","IEEE","IEEE Conferences"
"A branch and bound algorithm for the sequential ordering problem","M. Karan; N. Skorin-Kapov","Dept. of Telecommunications, FER, University of Zagreb, Zagreb, Croatia; Dept. of Telecommunications, FER, University of Zagreb, Zagreb, Croatia","2011 Proceedings of the 34th International Convention MIPRO","","2011","","","452","457","The Sequential Ordering Problem (SOP) is the problem of finding the shortest hamiltonian path in a graph while satisfying given precedence constraints regarding the order in which the nodes are visited. This classical optimization problem has many real-world applications, particularly in production planning, scheduling and transportation. We propose a branch and bound-based approach which can be used to solve medium instances (up to 30 nodes) of the problem to optimality in reasonable time. It does not require any specialized software for solving Mixed Integer Linear programs (MILP) and performs well particularly for heavily-constrained instances. The performance of the algorithm is tested on benchmark problems found in the TSPLIB online library.","","978-953-233-059-5978-1-4577-0996-8978-953-233-067","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5967099","Sequential ordering problem;Branch and bound;Optimization","Optimization;Algorithm design and analysis;Memory management;Vehicles;Complexity theory;Random access memory;Libraries","graph theory;integer programming;linear programming;order processing;production planning;scheduling;transportation;tree searching","branch and bound algorithm;sequential ordering problem;shortest hamiltonian path;graph theory;optimization problem;production planning;production scheduling;transportation;mixed integer linear program;MILP;TSPLIB online library","","","7","","","","","","IEEE","IEEE Conferences"
"Sizing and locating distributed generations for losses minimization and voltage stability improvement","K. Mahmoud; M. Abdel-Akher; A. A. Ahmed","APEARC, Department of Electrical Engineering, Aswan Faculty of Engineering, South Valley University, 81542 ASWAN, Egypt; APEARC, Department of Electrical Engineering, Aswan Faculty of Engineering, South Valley University, 81542 ASWAN, Egypt; APEARC, Department of Electrical Engineering, Aswan Faculty of Engineering, South Valley University, 81542 ASWAN, Egypt","2010 IEEE International Conference on Power and Energy","","2010","","","600","604","The paper presents analysis of distribution system connected with distributed generations. The study addresses aspects related to optimal sizing and location of DG units for losses minimization and voltage stability improvements. Many cases have investigated to highlight the relationship between the optimum size and location for losses minimization and the optimum size and location for stability improvements. The student version of the AMPL software is used in the proposed study. The objective function is formulated with full consideration of both quality and inequality constraints. On the other hand, the stability index criterion is used for calculating the best location and size for system stability improvements. The 90 bus test system from the literature is used for the different studied cases. The results show that calculating minimum system losses is not necessary to achieve coherence improvement for the voltage stability problem.","","978-1-4244-8946-6978-1-4244-8947-3978-1-4244-8945","10.1109/PECON.2010.5697652","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5697652","Distributed Generations;DG size;Optimization technique;Stability index","Stability criteria;Power system stability;Indexes;Minimization;Distributed power generation;Joining processes","distributed power generation;fault diagnosis;power system stability","distributed generations location;distributed generations sizing;losses minimization;voltage stability improvement;AMPL software;quality constraints;inequality constraints;stability index criterion","","6","7","","","","","","IEEE","IEEE Conferences"
"Seamless SIP multimedia session transfer on IPv6 network via device switching","S. Hossain; S. H. S. Ariffin; N. Fisal; N. S. A. Hassan; L. A. Latiff; C. K. Neng","Telematic Research Group (TRG), Faculty of Electrical Engineering, Universiti Teknologi, Malaysia, Skudai, Johor, Malaysia; Telematic Research Group (TRG), Faculty of Electrical Engineering, Universiti Teknologi, Malaysia, Skudai, Johor, Malaysia; UTM SPACE, International Campus, Jalan Semarak, 54100 Kuala Lumpur, Malaysia; UTM SPACE, International Campus, Jalan Semarak, 54100 Kuala Lumpur, Malaysia; UTM SPACE, International Campus, Jalan Semarak, 54100 Kuala Lumpur, Malaysia; MIMOS Berhad, Technology Park Malaysia, Kuala Lumpur, Malaysia","2011 Fourth International Conference on Modeling, Simulation and Applied Optimization","","2011","","","1","7","Due to significant popularity of Location-based Services and Multimedia communication over mobile devices, there are many researches has been conducted to extend the features of location tracking and make it cost-effective to users. It becomes necessary for the users to have seamless communication with automated switching of software applications. This paper focuses on the performance of indoor location tracking system on IPv6 Network Island with multiple real time applications that has location assisted transfer for mobile users. RSSI (Received Signal Strength Indicator) mechanism has been used to locate the moving nodes. The developed location tracking server is having dynamic and centralized MySQL database management system. SIP (Session Initial Protocols) user agent has been used to deploy intercommunicating of multimedia data for instance; video and audio conference, text messaging among the moving nodes and user can transfer the seamlessly transfer the session to their nearest mobile node which will be determined by the Location Server. This paper is going to discuss about the seamless performance of SIP during the session transference. The developed project is cost-effective and precisely conducive for the industries or any indoor organization. The prototype of the project has been successfully developed and has been tested as well. The results show the seamless connectivity of the multimedia application during device switching.","","978-1-4577-0005-7978-1-4577-0003-3978-1-4577-0004","10.1109/ICMSAO.2011.5775485","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5775485","","Mobile communication;Servers;Mathematical model;Equations;Multimedia communication;Peer to peer computing;Protocols","indoor radio;IP networks;mobile handsets;multimedia communication;signalling protocols;SQL","seamless SIP multimedia session transfer;device switching;multimedia communication;location-based services;mobile devices;automated switching;software applications;indoor location tracking system;IPv6 network island;RSSI;received signal strength indicator;centralized MySQL database management system;session initial protocols;video conference;audio conference;text messaging;mobile node;indoor organization","","1","12","","","","","","IEEE","IEEE Conferences"
"Current controller considering harmonics compensation for grid connected converter in DPGS applications","L. Barote; C. Marinescu; R. Teodorescu","Transilvania University/Electrical Engineering and Computer Science, Brasov, Romania; Transilvania University/Electrical Engineering and Computer Science, Brasov, Romania; Aalborg University/Institute of Energy Technology, Denmark","2012 13th International Conference on Optimization of Electrical and Electronic Equipment (OPTIM)","","2012","","","899","905","This paper deals with the design and implementation of PR current control method in the &#x03B1;&#x03B2; stationary reference frame for the grid side converter in Distributed Power Generation Systems (DPGS) applications. The goals of this paper are to implement a control technique for the grid side inverter including a LC filter, a compensation technique for low-order harmonics and to examine the grid current harmonic content with and without harmonic compensation. A comparative study in terms of current harmonic distortion between two different values of PR proportional gain running in steady state condition is made. The analyzed structure was simulated with Simulink software, then implemented and tested in laboratory using a dSPACE setup. The parameters of the system can be set according to the used experimental setup to validate the simulation results.","1842-0133;1842-0133;1842-0133","978-1-4673-1653-8978-1-4673-1650-7978-1-4673-1652","10.1109/OPTIM.2012.6231827","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6231827","","Harmonic analysis;Power harmonic filters;Inverters;Reactive power;Current control;Voltage control;Current measurement","distributed power generation;electric current control;filters;invertors;power convertors;power grids","current controller;harmonics compensation;grid connected converter;DPGS applications;distributed power generation systems;PR current control method;αβ stationary reference frame;control technique;grid side inverter;LC filter;compensation technique;low-order harmonics;grid current harmonic content;PR proportional gain;steady state condition;Simulink software;dSPACE setup","","2","23","","","","","","IEEE","IEEE Conferences"
"Comparative studies on load frequency control for islanded distribution network connected with mini hydro","J. Laghari; H. Mokhlis; A. B. H. A. Bakar; H. Mohammad","Department of Electrical Engineering, Faculty of Engineering, University of Malaya, Malaysia; Department of Electrical Engineering, Faculty of Engineering, University of Malaya, Malaysia; Department of Electrical Engineering, Faculty of Engineering, University of Malaya, Malaysia; Faculty of Electrical Engineering, University of Teknologi Mara, Shah Alam, Malaysia","2011 5th International Power Engineering and Optimization Conference","","2011","","","211","216","IEEE working group on prime movers recommended different governors for hydro power plants. This paper presents the comparative studies on all of these governors for mini hydro power plants (MHPP) operating in parallel and supplying power to distribution network. The purpose of this study is to find their suitability in controlling the frequency of the system for large load variations in an islanded distribution network. The mini hydro power plant with its distribution network is modeled in PSCAD software. The mechanical hydraulic governor, electro hydraulic PID governor, electro-hydraulic PI governor and Enhanced governor are evaluated one by one to find the best response in controlling frequency when disturbance occurs. The load variations from 10% to 50% both addition and reduction are tested for every governor. This study will assists in selecting a particular governor for isolated mode, interconnected mode as well as for islanding mode of operation in distributed generation.","","978-1-4577-0354-6978-1-4577-0355-3978-1-4577-0353","10.1109/PEOCO.2011.5970388","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5970388","Distribution network;Frequency response;Governor Controller;Islanded network","Logic gates;Hydraulic turbines;Servomotors;Frequency control;Valves;Load modeling","distribution networks;hydroelectric power stations;load regulation","load frequency control;islanded distribution network;mini hydro power plants;PSCAD software;mechanical hydraulic governor;electro hydraulic PID governor;electro-hydraulic PI governor;enhanced governor","","1","21","","","","","","IEEE","IEEE Conferences"
"OpenQoS: An OpenFlow controller design for multimedia delivery with end-to-end Quality of Service over Software-Defined Networks","H. E. Egilmez; S. T. Dane; K. T. Bagci; A. M. Tekalp","Koc University, Istanbul, Turkey; Koc University, Istanbul, Turkey; Koc University, Istanbul, Turkey; Koc University, Istanbul, Turkey","Proceedings of The 2012 Asia Pacific Signal and Information Processing Association Annual Summit and Conference","","2012","","","1","8","OpenFlow is a Software Defined Networking (SDN) paradigm that decouples control and data forwarding layers of routing. In this paper, we propose OpenQoS, which is a novel OpenFlow controller design for multimedia delivery with end-to-end Quality of Service (QoS) support. Our approach is based on QoS routing where the routes of multimedia traffic are optimized dynamically to fulfill the required QoS. We measure performance of OpenQoS over a real test network and compare it with the performance of the current state-of-the-art, HTTP-based multi-bitrate adaptive streaming. Our experimental results show that OpenQoS can guarantee seamless video delivery with little or no video artifacts experienced by the end-users. Moreover, unlike current QoS architectures, in OpenQoS the guaranteed service is handled without having adverse effects on other types of traffic in the network.","","978-0-6157-0050-2978-1-4673-4863","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6411795","","Quality of service;Routing;Multimedia communication;Streaming media;Delay;Internet;Computer architecture","hypermedia;media streaming;quality of service;telecommunication network routing;telecommunication traffic;transport protocols","OpenQoS;OpenFlow controller design;multimedia delivery;software-defined network;data forwarding layer;end-to-end quality of service support;QoS routing;multimedia traffic;HTTP-based multibitrate adaptive streaming;seamless video delivery","","10","25","","","","","","IEEE","IEEE Conferences"
"An enhanced workflow management for Utility Management Systems","S. Vukmirovic; A. Erdeljan; F. Kulic; S. Luković","Faculty of Technical Sciences, University of Novi Sad, Serbia; Faculty of Technical Sciences, University of Novi Sad, Serbia; Faculty of Technical Sciences, University of Novi Sad, Serbia; Faculty of Informatics - ALaRI, University of Lugano, Switzerland","International Congress on Ultra Modern Telecommunications and Control Systems","","2010","","","429","436","The emerging computational grid infrastructure consists of widely distributed heterogeneous resources, which makes mapping of increasingly complex applications a very challenging task. Utility Management Systems (UMS) manage large number of workflows with high resource requirements and thereby optimization of resource utilization has to be adapted. In this work we propose the architecture that implements a novel concept for dynamical execution of a scheduling algorithm using near real-time feedback from the execution monitoring process. An Artificial Neural Network (ANN) was trained for workflow scheduling. In the case study, we first perform experiments with same number of workflows and then introduce two additional in the system observing its' behavior with and without proposed improvements. Performance tests show that significant improvements of overall execution time can be achieved by introducing adaptive Artificial Neural Network.","2157-023X;2157-0221;2157-0221","978-1-4244-7286-4978-1-4244-7285-7978-1-4244-7284","10.1109/ICUMT.2010.5676601","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676601","","Computer architecture;Artificial neural networks;Databases;Monitoring;Actuators;Optimization;Schedules","grid computing;learning (artificial intelligence);neural nets;scheduling;workflow management software","enhanced workflow management;utility management systems;computational grid infrastructure;distributed heterogeneous resources;resource utilization optimization;real-time feedback;ANN;workflow scheduling algorithm;adaptive artificial neural network","","","16","","","","","","IEEE","IEEE Conferences"
"Fast Start-up for Spartan-6 FPGAs using Dynamic Partial Reconfiguration","J. Meyer; J. Noguera; M. Hübner; L. Braun; O. Sander; R. M. Gil; R. Stewart; J. Becker","Institute for Information Processing Technology, Karlsruhe Institute of Technology, Karlsruhe, Germany; Xilinx Inc., Ireland; Institute for Information Processing Technology, Karlsruhe Institute of Technology, Karlsruhe, Germany; Institute for Information Processing Technology, Karlsruhe Institute of Technology, Karlsruhe, Germany; Institute for Information Processing Technology, Karlsruhe Institute of Technology, Karlsruhe, Germany; Departamento de Electrónica, Universidad de Alcalá, Madrid, Spain; Xilinx Inc., Ireland; Institute for Information Processing Technology, Karlsruhe Institute of Technology, Karlsruhe, Germany","2011 Design, Automation & Test in Europe","","2011","","","1","6","This paper introduces the first available tool flow for Dynamic Partial Reconfiguration on the Spartan-6 family. In addition, the paper proposes a new configuration method called Fast Start-up targeting modern FPGA architectures, where the FPGA is configured in two-steps, instead of using a single (monolithic) full device configuration. In this novel approach, only the timing-critical modules are loaded at power-up using the first high-priority bitstream, while the non-timing critical modules are loaded afterwards. This two-step or prioritized FPGA start-up is used in order to meet the extremely tight startup timing specifications found in many modern applications, like PCI-express or automotive applications. Finally, the developed tool flow and methods for Fast Start-up have been used and tested to implement a CAN-based automotive ECU on a Spartan-6 evaluation board (i.e., SP605). By using this novel approach, it was possible to decrease the initial bitstream size and hence, achieve a configuration time speed-up of up to 4.5×, when compared to a standard configuration solution.","1558-1101;1530-1591;1530-1591","978-3-9810801-8-6978-1-61284-208-0978-3-9810801-7","10.1109/DATE.2011.5763244","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5763244","","Field programmable gate arrays;Vehicle dynamics;Clocks;Timing;Control systems;Software","field programmable gate arrays;monolithic integrated circuits","fast start-up;Spartan-6 FPGA;dynamic partial reconfiguration;tool flow;configuration method;monolithic full device configuration;timing-critical modules;high-priority bitstream","","6","13","","","","","","IEEE","IEEE Conferences"
"Evolving a Mario agent using cuckoo search and softmax heuristics","E. R. Speed","Square Enix Research Center, Square Enix Co., Ltd., Tokyo, Japan","2010 2nd International IEEE Consumer Electronics Society's Games Innovations Conference","","2010","","","1","7","This paper presents a method for evolving an agent which can successfully play a level of Super Mario Brothers as implemented on the MarioAI Benchmark. The Mario search space is extremely large, making finding reasonable solutions intractable for ordinary agents. The recently introduced evolutionary algorithm, cuckoo search is especially well suited toward searching such large spaces when it employs the use of Lévy flights. Unfortunately, these Lévy flights cannot be applied to non numerical problems such as Mario. We present a modification of the algorithm which uses the Lévy distribution to effect appropriate change in a much wider set of problems, including Mario. To further optimize the search of Mario's problem space, a softmax heuristic is presented to focus on areas with likely solutions.","2166-6741;2166-675X","978-1-4244-7178-2978-1-4244-7179","10.1109/ICEGIC.2010.5716893","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5716893","Super Mario Bros;cuckoo search;Le´vy flights;softmax;evolutionary algorithm","Artificial intelligence;Games;Evolutionary computation;Optimization;Space exploration;Benchmark testing;Radio frequency","artificial intelligence;computer games;evolutionary computation;search problems;software agents;statistical distributions","Mario agent;cuckoo search;softmax heuristics;Super Mario Brothers;MarioAI Benchmark;Mario search space;evolutionary algorithm;Lévy distribution","","7","8","","","","","","IEEE","IEEE Conferences"
"Performance Evaluation of Intel and Portland Compilers Using Intel Westmere Processor","M. Al-Mulhem; R. Al-Shaikh","NA; NA","2011 Second International Conference on Intelligent Systems, Modelling and Simulation","","2011","","","261","266","In recent years, we have witnessed a growing interest in optimizing the parallel and distributed computing solutions using scaled-out hardware designs and scalable parallel programming paradigms. This interest is driven by the fact that the microchip technology is gradually reaching its physical limitations in terms of heat dissipation and power consumption. Therefore and as an extension to Moore's law, recent trends in high performance and grid computing have shown that future increases in performance can only be reached through increases in systems scale using a larger number of components, supported by scalable parallel programming models. In this paper, we evaluate the performance of two commonly used parallel compilers, Intel and Portland's PGI, using a state-of-the-art Intel West mere-based HPC cluster. The performance evaluation is based on two sets of experiments, once evaluating the compilers' performance using an MPI-based code, and another using OpenMP. Our results show that, for scientific applications that are matrices-dependant, the MPI and OpenMP features of the Intel compiler supersede PGI when using the defined HPC cluster.","2166-0662;2166-0670","978-1-4244-9809","10.1109/ISMS.2011.47","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5730357","HPC;Intel compiler;PGI compiler;Infiniband","Optimization;Switches;Instruction sets;Benchmark testing;Programming;Servers;Schedules","application program interfaces;grid computing;message passing;microprocessor chips;parallel programming;program compilers;software performance evaluation","performance evaluation;Intel compiler supersede PGI;Portland compilers;Intel Westmere processor;parallel computing;distributed computing;parallel programming models;microchip technology;heat dissipation;power consumption;Moore law;grid computing;parallel compilers;MPI-based code;OpenMP","","","14","","","","","","IEEE","IEEE Conferences"
"Throughput driven transformations of Synchronous Data Flows for mapping to heterogeneous MPSoCs","A. Stulova; R. Leupers; G. Ascheid","Institute for Communication Technologies and Embedded Systems, RWTH Aachen University, Germany; Institute for Communication Technologies and Embedded Systems, RWTH Aachen University, Germany; Institute for Communication Technologies and Embedded Systems, RWTH Aachen University, Germany","2012 International Conference on Embedded Computer Systems (SAMOS)","","2012","","","144","151","Due to energy efficiency requirements of modern embedded systems, chip vendors are inclined towards multicore architectures with different types of processing engines and non-uniform interconnect fabrics. At the same time multiple applications are intended to run concurrently on the devices with such heterogeneous architectures. This rapid growth in the complexity of the hardware and its use cases imposes new challenges on the software development tools. To overcome this complexity, model of computation based approaches are becoming increasingly promising. Synchronous Data Flow (SDF) is a popular specification formalism for streaming applications with inherently concurrent nature. However, the parallelism expressed in the original representation is often not sufficient to maximally exploit the potential of multicore platforms. In this paper we present a holistic methodology for improving the throughput of streaming applications while mapping them onto heterogeneous architectures. The approach uses transformations that adapt the parallelism in SDF according to available platform resources. We use a genetic algorithm to explore SDF instances with the objective of maximizing throughput on a target platform. Our model supports architecture heterogeneity and multi-application scenarios. The experiments indicate that our approach outperforms other techniques for exploiting parallelism on a single application in most of the test cases and enables concurrent applications optimization.","","978-1-4673-2297-3978-1-4673-2295-9978-1-4673-2296","10.1109/SAMOS.2012.6404168","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6404168","","Parallel processing;Program processors;Computer architecture;Delay;Throughput;Merging;Production","concurrency control;data flow computing;embedded systems;genetic algorithms;multiprocessing systems;parallel architectures;software engineering;system-on-chip","throughput driven transformations;synchronous data flow;heterogeneous MPSoC;energy efficiency requirements;embedded systems;multicore architectures;processing engines;nonuniform interconnect fabrics;heterogeneous architecture;software development tools;computation based approach;specification formalism;streaming applications;holistic methodology;platform resources;genetic algorithm;SDF instances;target platform;throughput maximization;architecture heterogeneity;multiapplication scenario;concurrent application optimization;multiprocessor system-on-chip architecture","","9","22","","","","","","IEEE","IEEE Conferences"
"Micro-architectural characterization of desktop cloud workloads","T. Jiang; R. Hou; L. Zhang; K. Zhang; L. Chen; M. Chen; N. Sun","State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, China","2012 IEEE International Symposium on Workload Characterization (IISWC)","","2012","","","131","140","Desktop cloud replaces traditional desktop computers with completely virtualized systems from the cloud. It is becoming one of the fastest growing segments in the cloud computing market. However, as far as we know, there is little work done to understand the behavior of desktop cloud. On one hand, desktop cloud workloads are different from conventional data center workloads in that they are rich with interactive operations. Desktop cloud workloads are different from traditional non-virtualized desktop workloads in that they have an extra layer of software stack - hypervisor. On the other hand, desktop cloud servers are mostly built with conventional commodity processors. While such processors are well optimized for traditional desktops and high performance computing workloads, their effectiveness for desktop cloud workloads remains to be studied. As an attempt to shed some lights on the effectiveness of conventional general-purpose processors on desktop cloud workloads, we have studied the behavior of desktop cloud workloads and compared it with that of SPEC CPU2006, TPC-C, PARSEC, and CloudSuite. We evaluate a Xen-based virtualization platform. The performance results reveal that desktop cloud workloads have significantly different characteristics with SPEC CPU2006, TPC-C and PARSEC, but they perform similarly with data center scale-out benchmarks from CloudSuite. In particular, desktop cloud workloads have high instruction cache miss rate (12.7% on average), high percentage of kernel instructions (23%, on average), and low IPC (0.36 on average). And they have much higher TLB miss rates and lower utilization of off-chip memory bandwidth than traditional benchmarks. Our experimental numbers indicate that the effectiveness of existing commodity processors is quite low for desktop cloud workloads. In this paper, we provide some preliminary discussions on some potential architectural and micro-architectural enhancements. We hope that the performance numbers presented in this paper will give some insights to the designers of desktop cloud systems.","","978-1-4673-4532-3978-1-4673-4531","10.1109/IISWC.2012.6402917","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6402917","","Benchmark testing;Servers;Program processors;Hardware;Radiation detectors;Virtual machine monitors","cache storage;cloud computing;software performance evaluation;virtual machines;virtualisation","desktop cloud workloads;microarchitectural characterization;virtualized systems;cloud computing market;interactive operations;software stack;hypervisor;desktop cloud servers;Xen-based virtualization platform;instruction cache miss rate;kernel instructions;IPC;TLB miss rates;off-chip memory bandwidth;commodity processors","","3","37","","","","","","IEEE","IEEE Conferences"
"Automatic Generation of Stream Descriptors for Streaming Architectures","L. Gao; D. Zaretsky; G. Mittal; D. Schonfeld; P. Banerjee","NA; NA; NA; NA; NA","2010 39th International Conference on Parallel Processing","","2010","","","307","312","We describe a novel approach for automatically generating streaming architectures from software programs. While existing systems require user-defined stream models, our method automatically identifies producer-consumer streaming relationships and translates them into streaming architectures. Data streams between producer-consumer kernels are represented using a combination of stream descriptors and CFGs, which are categorized into four stream types. A bridge module is generated based on the stream type in the streaming architecture to facilitate data streaming between each producer-consumer pair. Several optimizations are also developed to improve throughput and parallelism. We demonstrate our results on a FPGA based platform. The automatically generated streaming architectures show 1.5-3x speedups over the non-streaming designs by employing spatial and temporal data independence to increase parallelism.","0190-3918;0190-3918;2332-5690","978-1-4244-7913-9978-0-7695-4156","10.1109/ICPP.2010.38","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5599175","Parallel processing;stream descriptor;stream architecture;FPGA","Kernel;Computer architecture;Bridges;Streaming media;Random access memory;Benchmark testing;Throughput","field programmable gate arrays;software architecture","stream descriptors;automatic generation streaming architectures;software programs;user-defined stream models;producer-consumer streaming relationships;producer-consumer kernels;bridge module;data streaming;FPGA based platform;nonstreaming designs;temporal data independence","","1","16","","","","","","IEEE","IEEE Conferences"
"The quality of multiple VoIP calls in an encrypted wireless network","M. Ivanovici; Ş. Savu","Department of Electronics and Computers, Transilvania University, Brasşv, România; Siemens Program and System Engineering, Brasşv, România","2010 12th International Conference on Optimization of Electrical and Electronic Equipment","","2010","","","1052","1055","We quantified the user-perceived quality for a VoIP application running in an encrypted wireless network and we experimentally determined the maximum number of parallel VoIP calls that can be achieved, at the best quality of the speech signal. We studied the behaviour of the application for four encryption mechanisms and for each of them we measured the bandwidth waste due to encryption. We used the ITU-T PESQ score to objectively assess the quality of the speech signal. In this paper we present the test setup and the tools we used for our experiments, as well as our experimental results and conclusions.","1842-0133;1842-0133","978-1-4244-7020-4978-1-4244-7019-8978-1-4244-7018","10.1109/OPTIM.2010.5510404","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5510404","","Cryptography;Wireless networks;Wireless LAN;Internet telephony;Space technology;Speech analysis;Computer networks;Application software;Bandwidth;Costs","cryptography;Internet telephony;radio networks","multiple VoIP call;encrypted wireless network;parallel VoIP call;speech signal;bandwidth waste;ITU-T PESQ","","","20","","","","","","IEEE","IEEE Conferences"
"Improving throughput via slowdowns","M. Goldstein; O. Shehory; R. Tzoref-Brill; S. Ur","IBM Haifa Research Lab, Haifa University Campus, Mount Carmel, Haifa, Israel; IBM Haifa Research Lab, Haifa University Campus, Mount Carmel, Haifa, Israel; IBM Haifa Research Lab, Haifa University Campus, Mount Carmel, Haifa, Israel; IBM Haifa Research Lab, Haifa University Campus, Mount Carmel, Haifa, Israel","2010 ACM/IEEE 32nd International Conference on Software Engineering","","2010","2","","11","20","Many service-oriented systems are not well equipped to guarantee that service time is optimized. We have specifically examined two industrial systems which implement service-oriented architectures in real, field environments. We discovered that both were not engineered to properly address surges in service request rate. In the absence of an integral solution, it is difficult and costly to (re-) engineer such a solution in the field. The challenge faced by this study was to deliver a low cost solution, without re-engineering the target systems. This paper introduces such a generic solution. The solution slows-down some components to deliver improvement in request service time. It was implemented, tested, and successfully applied to two industrial systems with no need to modify their logic or architecture. Experiments with those systems exhibited significant improvement in performance. These results have validated our solution and its industrial applicability across systems and environments.","1558-1225;0270-5257","978-1-60558-719","10.1145/1810295.1810298","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062134","automated performance management;self-managing systems","Monitoring;Runtime;Servers;Throughput;Computer architecture;Noise;Actuators","object-oriented programming;service-oriented architecture;software reliability","service-oriented systems;industrial systems;service-oriented architectures;real environments;field environments;service request rate;integral solution;systems re-engineering","","","18","","","","","","IEEE","IEEE Conferences"
"Reinforcement learning algorithm for industrial robot programming by demonstration","M. Stoica; F. Sisak; A. D. Morosan","&#x201C;Transilvania&#x201D; University of Brasov, Automatics Department, Brasov, Romania; &#x201C;Transilvania&#x201D; University of Brasov, Automatics Department, Brasov, Romania; &#x201C;Transilvania&#x201D; University of Brasov, Automatics Department, Brasov, Romania","2012 13th International Conference on Optimization of Electrical and Electronic Equipment (OPTIM)","","2012","","","1517","1524","Programming by demonstration represent a significant subject in the field of robotics and it is developing more and more in the direction of robots for services and humanoid robots. Programming by demonstration is much less researched, when we talk about industrial robots. One of the reasons is that an industrial robot has to act in a precise and certain manner. However, extending research regarding programming by demonstration in industrial robots area, could lead to development of intelligent systems, where the industrial robot could be programmed in an easier way. In this paper we proposed an algorithm based on reinforcement learning and we developing, implementing and testing this algorithm which can offer flexibility in intelligent systems. Initially, we have focused our research on the creation of a reasoning algorithm based on artificial neural networks, but the results of this algorithm weren't satisfying, so we have switched our focus towards proposed algorithm. The results of this algorithm is that the robot will be capable to learn from its mistakes and he will know how to act in unknown situation; this will be possible because the robot will get marks for each possible action and he will updates its behavior.","1842-0133;1842-0133;1842-0133","978-1-4673-1653-8978-1-4673-1650-7978-1-4673-1652","10.1109/OPTIM.2012.6231926","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6231926","","Programming;Service robots;Robot kinematics;Software;Classification algorithms;Learning","automatic programming;humanoid robots;industrial robots;knowledge based systems;learning (artificial intelligence);robot programming;service robots","reinforcement learning algorithm;industrial robot;programming by demonstration;service robot;humanoid robot;intelligent system","","","19","","","","","","IEEE","IEEE Conferences"
"Notice of Retraction<br>QSAR study of selective HIV integrase inhibitors based on Boosting regression","Changhong Zhou; Yuntao Zhang; Zhengjun Cheng","Institute of Applied Chemistry, China West Normal University, Nanchong Sichuan 637002, China; Institute of Applied Chemistry, China West Normal University, Nanchong Sichuan 637002, China; Institute of Applied Chemistry, China West Normal University, Nanchong Sichuan 637002, China","2010 International Conference on Computer Application and System Modeling (ICCASM 2010)","","2010","3","","V3-251","V3-255","The QSAR study of 67 selective HIV integrase inhibitors is based on Boosting regression combined with MLR. Seven descriptors have been calculated by E-Dragon software as independent variables. The IC50 values of compounds are used as dependent variables. Seven models have been constructed by MLR and their variables are selected by particle swarm optimization. By testing the generalization of each model, the values of the internal and external validation of the RDF, Topological and 3D-MoRSE descriptor model are more than 0.5, so the Boosting-MLR model has been constructed by these descriptors. The best Boosting-MLR model of RDF descriptor improved the correlation coefficients (R2) prediction to 0.827, up from 0.794 when only MLR was applied. By the model validation, Boosting-MLR has shown the stability generalization.;Notice of Retraction<br><br>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<br><br>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<br><br>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<br><br>","2161-9069;2161-9077","978-1-4244-7237-6978-1-4244-7235","10.1109/ICCASM.2010.5620016","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5620016","quantitative structure-activity relationship;Boosting regression;HIV integrase inhibitors","Predictive models;Human immunodeficiency virus;Resource description framework;Computational modeling;TV;Biological system modeling;Solid modeling","diseases;medical computing;particle swarm optimisation","QSAR study;selective HIV integrase inhibitor;Boosting regression;e-dragon software;particle swarm optimization;3D-MoRSE descriptor model;correlation coefficient prediction;stability generalization","","","21","","","","","","IEEE","IEEE Conferences"
"On-chip memory space partitioning for chip multiprocessors using polyhedral algebra","O. Ozturk; M. Kandemir; M. J. Irwin","Department of Computer Engineering, Bilkent University; Computer Engineering Department, The Pennsylvania State University; Computer Engineering Department, The Pennsylvania State University","IET Computers & Digital Techniques","","2010","4","6","484","498","One of the most important issues in designing a chip multiprocessor is to decide its on-chip memory organisation. While it is possible to design an application-specific memory architecture, this may not necessarily be the best option, in particular when storage demands of individual processors and/or their data sharing patterns can change from one point in execution to another for the same application. Here, two problems are formulated. First, we show how a polyhedral method can be used to design, for array-based data-intensive embedded applications, an application-specific hybrid memory architecture that has both shared and private components. We evaluate the resulting memory configurations using a set of benchmarks and compare them to pure private and pure shared memory on-chip multiprocessor architectures. The second approach proposed consider dynamic configuration of software-managed on-chip memory space to adapt to the runtime variations in data storage demand and interprocessor sharing patterns. The proposed framework is fully implemented using an optimising compiler, a polyhedral tool, and a memory partitioner (based on integer linear programming), and is tested using a suite of eight data-intensive embedded applications.","1751-8601;1751-861X","","10.1049/iet-cdt.2009.0089","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5621949","","","algebra;integer programming;linear programming;microprocessor chips;multiprocessing systems;optimising compilers;storage management","on-chip memory space partitioning;chip multiprocessors;polyhedral algebra;on-chip memory organisation;application-specific memory architecture;data sharing patterns;array-based embedded application;software-managed on-chip memory space;data storage demand;interprocessor sharing patterns;optimising compiler;polyhedral tool;integer linear programming","","","","","","","","","IET","IET Journals & Magazines"
"A Security Evaluation Method Based on STRIDE Model for Web Service","L. Jiang; H. Chen; F. Deng","NA; NA; NA","2010 2nd International Workshop on Intelligent Systems and Applications","","2010","","","1","5","Web service is a distributed computing model which has characteristics of loose coupling, self-description and strong self-government, how to evaluate the degree of Web service security is a challenging problem. On the basis of analyzing the threat which Web service facing, a security evaluation method based on STRIDE model for Web service is proposed. According to its own features of Web service and threat classification method of STRIDE model, this paper designed a WS-Security Evaluation Model, it's provide a valuable way to help user to create the threat modeling and evaluating the safety degree of Web service security. With the case study of SOA system in a certain enterprise, experimental results show that it provides a valuable reference to check out security vulnerabilities of Web service and optimize the system's security design.","","978-1-4244-5874-5978-1-4244-5872","10.1109/IWISA.2010.5473445","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5473445","","Web services;Information security;Testing;Service oriented architecture;Computer security;Distributed computing;Safety;Local area networks;Educational institutions;Application software","pattern classification;security of data;software architecture;Web services","security evaluation method;STRIDE model;Web service security;distributed computing model;threat classification method;WS-security evaluation model;SOA system;system security design;service oriented architecture system","","5","14","","","","","","IEEE","IEEE Conferences"
"On genetic algorithm methodology for robust system design","A. M. Shamsieva; V. U. Arkov","Ufa State Aviation Technical University/Department of Automated Control and Management Systems, Russia; Ufa State Aviation Technical University/Department of Automated Control and Management Systems, Russia","2011 IEEE 12th International Symposium on Computational Intelligence and Informatics (CINTI)","","2011","","","421","426","Optimization of the trading strategy with application of the genetic algorithm methodology is discussed. The trading strategy design in the context of robustness criteria is analyzed. The trading system optimization is performed using technical analysis and genetic optimization software.","","978-1-4577-0045-3978-1-4577-0044-6978-1-4577-0043","10.1109/CINTI.2011.6108542","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6108542","","Optimization;Robustness;Algorithm design and analysis;Smoothing methods;Genetic algorithms;Training;Testing","genetic algorithms;investment","genetic algorithm methodology;robust system design;trading strategy;robustness criteria;trading system optimization;technical analysis;portfolio investment","","","15","","","","","","IEEE","IEEE Conferences"
"Synthesis of multi-level pipelines for programmable logic devices","I. Damaj","Division of Sciences and Engineering American University of Kuwait P.O. Box 3323, Safat, Kuwait 13034","2010 17th International Conference on Telecommunications","","2010","","","973","980","Recently, hardware and software engineers have been showing considerable attention to high-level parallelization and hardware synthesis methodologies. State-of-the-art approaches have benefited from the emergence of modern high-density Field Programmable Gate Arrays. In this paper, we explore the effectiveness of a formal methodology in the design of pipelined versions of a matrix multiplication algorithm. The suggested methodology adopts a functional programming notation for specifying algorithms and for reasoning about them. The parallel behavior of the specification is then derived and mapped onto hardware. Several pipelined implementations are developed with different performance characteristics. The refined designs are tested under Agility's RC-1000 reconfigurable computer with its 2 million gates Virtex-E FPGA. Performance analysis and evaluation of the proposed implementations are presented in comparison with an Intel Core 2 DUO processor.","","978-1-4244-5246-0978-1-4244-5247","10.1109/ICTEL.2010.5478861","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5478861","","Pipelines;Programmable logic devices;Hardware;Field programmable gate arrays;Programmable logic arrays;Design methodology;Algorithm design and analysis;Functional programming;Testing;Performance analysis","functional programming;logic programming;matrix algebra;optimisation;pipeline processing","multilevel pipelines synthesis;programmable logic devices;software engineers;hardware engineers;high level parallelization;hardware synthesis methodologies;field programmable gate arrays;matrix multiplication algorithm;functional programming notation;pipelined implementations;Intel Core 2 DUO processor","","","14","","","","","","IEEE","IEEE Conferences"
"Environmental integrated closed loop logistics model: An artificial bee colony approach","V. K. Vishwa; F. T. S. Chan; N. Mishra; V. Kumar","Department of Industrial and Systems Engineering, The Hong Kong Polytechnic University, Hong Kong; Department of Industrial and Systems, Engineering, The Hong Kong Polytechnic University, Hong Kong; School of Computer Science, University of Nottingham, United, Kingdom, NG8 IBB; DCU Business School, Dublin City, University, Dublin, Ireland","2010 8th International Conference on Supply Chain Management and Information","","2010","","","1","7","Economical and Ecological consciousness have led to incorporate close loop logistics in modern manufacturing arena. The success of the close loop supply logistics mainly depends on good retrieval of the components still have significant amount of value added in the returned products. Thus, the proper management of the returned products is one of the key elements for enterprises. This paper illustrates the complexities involved in resolving a remanufacturing problem and formulates a mathematical model in which the return rate is a function of environmental factor. Since, such model belongs to a class on NP hard problems; an Artificial Bee Colony (ABC) algorithm has been presented and tested over an illustrative example. The algorithm adequately explores the entire search space to achieve superior performance on the underlying problem. Further, the results obtained by ABC have been analyzed with particle swarm optimization where ABC was seen to significantly outperform the rest.","","978-962-367-696-0978-962-367-696-0978-962-367-697","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5681670","Remanufacturing;Artificial Intelligent techniques","Environmental factors;Mathematical model;Electronic mail;Indexes;Supply chains;Optical character recognition software","closed loop systems;environmental factors;logistics;particle swarm optimisation;recycling;search problems;supply chains","environmental integrated closed loop logistic model;remanufacturing problem;mathematical model;environmental factor;NP hard problem;artificial bee colony algorithm;search space;particle swarm optimization","","","15","","","","","","IEEE","IEEE Conferences"
"Dynamic difficulty adjustment of game AI for video game Dead-End","X. Yu; S. He; Y. Gao; J. Yang; L. Sha; Y. Zhang; Z. Ai","International School, Beijing University of Posts and Telecommunications, Beijing, China; School of Software Engineering, Beijing University of Posts and Telecommunications, Beijing, China; International School, Beijing University of Posts and Telecommunications, Beijing, China; International School, Beijing University of Posts and Telecommunications, Beijing, China; International School, Beijing University of Posts and Telecommunications, Beijing, China; International School, Beijing University of Posts and Telecommunications, Beijing, China; International School, Beijing University of Posts and Telecommunications, Beijing, China","The 3rd International Conference on Information Sciences and Interaction Sciences","","2010","","","583","587","To create a satisfactory game opponent is to optimize player's experience through creation of an dynamic balanced game, which means that win-rate of players is adjusted according to their ability. The most commonly used approach for generating satisfactory game opponent is Dynamic Difficulty Adjustment (DDA), which is to dynamically adjust challenge level of the opponent according to the player's skill level. However, DDA currently used is relatively simple and implementing DDA by adjusting opponent's intelligence is still challenging. In this paper, we propose to use Artificial Neural Network(ANN) to implement DDA and unsupervised learning methodologies to improve the performance of ANN. ANN-controlled Non-Player Characters (NPC) can make ""wise"" decision based on collected attributes of all the characters in the game. Different ANNs can provide different win-rates for different player strategies, which can achieve the dynamic balance we expected and enhance the user experience of games.","","978-1-4244-7386-1978-1-4244-7384","10.1109/ICICIS.2010.5534761","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5534761","ANN;DDA;clustering;player strategy","Games;Artificial intelligence;Testing;Artificial neural networks;Dogs;Helium;Software engineering;Unsupervised learning;Humans;Clustering algorithms","artificial intelligence;computer games;neural nets;unsupervised learning","dynamic difficulty adjustment;game artificial intelligence;video game dead-end;satisfactory game opponent;artificial neural network;unsupervised learning methodology;nonplayer characters","","","9","","","","","","IEEE","IEEE Conferences"
"VC-Migration: Live Migration of Virtual Clusters in the Cloud","K. Ye; X. Jiang; R. Ma; F. Yan","NA; NA; NA; NA","2012 ACM/IEEE 13th International Conference on Grid Computing","","2012","","","209","218","Live migration of virtual machines (VM) has recently become a key ingredient behind the management activities of cloud computing system to achieve the goals of load balancing, energy saving, failure recovery, and system maintenance. However, to our knowledge, most of the previous live VM migration techniques concentrated on the migration of a single VM which means these techniques are insufficient when the whole virtual cluster or multiple virtual clusters need to be migrated. This paper investigates various live migration strategies for virtual clusters (VC). We first describe a framework VC-Migration to control the migration of virtual clusters. Then we perform a series of experiments to study the performance and overheads of different migration strategies for virtual clusters, including concurrent migration, mutual migration, homogeneous VC migration, and heterogeneous VC migration. After that, we present several optimization principles to improve the migration performance of virtual clusters. The HPCC benchmark is selected to represent the virtual cluster workloads, and the metrics such as downtime, total migration time, and workload performance are measured. Experimental results reveal some new discoveries which are useful to the future development of new migration mechanisms and algorithms to optimize the migration of virtual clusters.","2152-1093;2152-1085","978-1-4673-2901","10.1109/Grid.2012.27","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6319172","virtual machine;virtual cluster;live migration;performance;cloud computing","Virtual machining;Benchmark testing;Scalability;Monitoring;Cloud computing;Parallel processing","cloud computing;concurrency control;resource allocation;software performance evaluation;system recovery;virtual machines","live migration;virtual clusters;virtual machines;cloud computing system;load balancing;energy saving;failure recovery;system maintenance;VM migration techniques;single VM migration;VC-migration framework;migration overheads;concurrent migration;mutual migration;homogeneous VC migration;heterogeneous VC migration;migration performance improvement;HPCC benchmark;virtual cluster workload representation;virtual cluster migration optimization","","19","21","","","","","","IEEE","IEEE Conferences"
"Exploring FPGA Routing Architecture Stochastically","M. Lin; J. Wawrzynek; A. E. Gamal","Department of Electrical Engineering and Computer Science, University of California, Berkeley, CA, USA; University of California, Berkeley, CA, USA; School of Engineering, Stanford Faculty, Stanford, CA, USA","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","","2010","29","10","1509","1522","This paper proposes a systematic strategy to efficiently explore the design space of field-programmable gate array (FPGA) routing architectures. The key idea is to use stochastic methods to quickly locate near-optimal solutions in designing FPGA routing architectures without exhaustively enumerating all design points. The main objective of this paper is not as much about the specific numerical results obtained, as it is to show the applicability and effectiveness of the proposed optimization approach. To demonstrate the utility of the proposed stochastic approach, we developed the tool for optimizing routing architecture (TORCH) software based on the versatile place and route tool. Given FPGA architecture parameters and a set of benchmark designs, TORCH simultaneously optimizes the routing channel segmentation and switch box patterns using the performance metric of average interconnect power-delay product estimated from placed and routed benchmark designs. Special techniques - such as incremental routing, infrequent placement, multi-modal move selection, and parallelized metric evaluation - are developed to reduce the overall run time and improve the quality of results. Our experimental results have shown that the stochastic design strategy is quite effective in co-optimizing both routing channel segmentation and switch patterns. With the optimized routing architecture, relative to the performance of our chosen architecture baseline, TORCH can achieve average improvements of 24% and 15% in delay and power consumption for the 20 largest Microelectronics Center of North Carolina benchmark designs, and 27% and 21% for the eight benchmark designs synthesized with the Altera Quartus II University Interface Program tool. Additionally, we found that the average segment length in an FPGA routing channel should decrease with technology scaling. Finally, we demonstrate the versatility of TORCH by illustrating how TORCH can be used to optimize other aspects of the routing architecture in an FPGA.","0278-0070;1937-4151","","10.1109/TCAD.2010.2061530","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5580223","Design exploration;FPGA;routing architecture;stochastic","Switches;Routing;Field programmable gate arrays;Delay;Computer architecture;Benchmark testing","field programmable gate arrays;network routing;stochastic processes","FPGA routing architecture;field-programmable gate array;stochastic methods;tool for optimizing routing architecture software;versatile place;route tool;routing channel segmentation;switch box patterns;average interconnect power-delay product;incremental routing;infrequent placement;multimodal move selection;parallelized metric evaluation;Altera Quartus II University interface program tool","","3","37","","","","","","IEEE","IEEE Journals & Magazines"
"Mining Data Chain Graph for Fault Localization","B. Yang; J. Wu; C. Liu","NA; NA; NA","2012 IEEE 36th Annual Computer Software and Applications Conference Workshops","","2012","","","464","469","Fault localization is a challenging task in domain specific data mining. Most existing works focus on call graph that can find bugs which are associated with control flow. However, there are a lot of bugs related to data flow. In this paper, we presented a data dependency graph in fault localization. The approach at first analyzes the execution of the test suites dynamically, then derives the data dependency graph which reflects data flow traces of any test case. Frequency subgraphs generated which are based on the analysis of these data dependency graph. At last ranking the variables that in those graphs and get the suspicious variables. We have conducted a case study use this approach. The preliminary result shows that our approach is feasible and effective.","","978-1-4673-2714-5978-0-7695-4758","10.1109/COMPSACW.2012.88","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6341620","","Data mining;Data models;Software;Instruments;Computer bugs;Debugging;Context","data flow graphs;data mining;fault tolerant computing;program debugging","data chain graph;fault localization;domain specific data mining;call graph;data dependency graph;data flow trace;frequency subgraph","","","34","","","","","","IEEE","IEEE Conferences"
"Parallel Reservoir Computing Using Optical Amplifiers","K. Vandoorne; J. Dambre; D. Verstraeten; B. Schrauwen; P. Bienstman","Photonics Research Group, Department of Information Technology, Ghent University-Interuniversity Microelectronics Center, Ghent, Belgium; Computer Systems Laboratory, Electronics and Information Systems Department, Ghent University, Ghent, Belgium; Computer Systems Laboratory, Electronics and Information Systems Department, Ghent University, Ghent, Belgium; Computer Systems Laboratory, Electronics and Information Systems Department, Ghent University, Ghent, Belgium; Photonics Research Group, Department of Information Technology, Ghent University-Interuniversity Microelectronics Center, Ghent, Belgium","IEEE Transactions on Neural Networks","","2011","22","9","1469","1481","Reservoir computing (RC), a computational paradigm inspired on neural systems, has become increasingly popular in recent years for solving a variety of complex recognition and classification problems. Thus far, most implementations have been software-based, limiting their speed and power efficiency. Integrated photonics offers the potential for a fast, power efficient and massively parallel hardware implementation. We have previously proposed a network of coupled semiconductor optical amplifiers as an interesting test case for such a hardware implementation. In this paper, we investigate the important design parameters and the consequences of process variations through simulations. We use an isolated word recognition task with babble noise to evaluate the performance of the photonic reservoirs with respect to traditional software reservoir implementations, which are based on leaky hyperbolic tangent functions. Our results show that the use of coherent light in a well-tuned reservoir architecture offers significant performance benefits. The most important design parameters are the delay and the phase shift in the system's physical connections. With optimized values for these parameters, coherent semiconductor optical amplifier (SOA) reservoirs can achieve better results than traditional simulated reservoirs. We also show that process variations hardly degrade the performance, but amplifier noise can be detrimental. This effect must therefore be taken into account when designing SOA-based RC implementations.","1045-9227;1941-0093","","10.1109/TNN.2011.2161771","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5966352","Integrated optics;optical neural networks;photonic reservoir computing;semiconductor optical amplifiers;speech recognition","Reservoirs;Photonics;Neurons;Semiconductor optical amplifiers;Topology;Speech recognition;Network topology","learning (artificial intelligence);optical neural nets;parallel processing;pattern classification;recurrent neural nets;semiconductor optical amplifiers","parallel reservoir computing;coupled semiconductor optical amplifiers;neural systems;complex recognition problems;complex classification problems;photonic reservoirs;babble noise;performance evaluation;software reservoir;simulated reservoirs;amplifier noise;recurrent neural networks;optical neural networks;leaky hyperbolic tangent functions;isolated word recognition task","Amplifiers, Electronic;Computer Simulation;Humans;Neural Networks (Computer);Noise;Optical Devices;Pattern Recognition, Physiological;Semiconductors;Spectrum Analysis;Speech Recognition Software","56","28","","","","","","IEEE","IEEE Journals & Magazines"
"Design Tools for Digital Microfluidic Biochips: Toward Functional Diversification and More Than Moore","K. Chakrabarty; R. B. Fair; J. Zeng","Department of Electrical and Computer Engineering, Duke University, Durham, NC, USA; Department of Electrical and Computer Engineering, Duke University, Durham, NC, USA; Hewlett-Packard Laboratories, Hewlett-Packard Company, Palo Alto, CA, USA","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","","2010","29","7","1001","1017","Microfluidics-based biochips enable the precise control of nanoliter volumes of biochemical samples and reagents. They combine electronics with biology, and they integrate various bioassay operations, such as sample preparation, analysis, separation, and detection. Compared to conventional laboratory procedures, which are cumbersome and expensive, miniaturized biochips offer the advantages of higher sensitivity, lower cost due to smaller sample and reagent volumes, system integration, and less likelihood of human error. This paper first describes the droplet-based “digital” microfluidic technology platform and emerging applications. The physical principles underlying droplet actuation are next described. Finally, the paper presents computer-aided design tools for simulation, synthesis and chip optimization. These tools target modeling and simulation, scheduling, module placement, droplet routing, pin-constrained chip design, and testing.","0278-0070;1937-4151","","10.1109/TCAD.2010.2049153","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5487469","Clinical diagnostics;lab-on-chip;simulation;synthesis;testing","Microfluidics;Computational modeling;Nanobioscience;Laboratories;Costs;Humans;Computer errors;Application software;Design automation;Computer simulation","circuit CAD;integrated circuit design;lab-on-a-chip;microfluidics","digital microfluidic biochip design tool;droplet-based digital microfluidic technology platform;droplet actuation;chip optimization;computer-aided design tools;droplet routing;pin-constrained chip design","","37","81","","","","","","IEEE","IEEE Journals & Magazines"
"Identification of the weakest buses in unbalanced multiphase smart grids with Plug-in Electric Vehicle charging stations","P. Juanuwattanakul; M. A. S. Masoum","Department of Electrical and Computer Engineering, Curtin University, Perth, WA, Australia; Department of Electrical and Computer Engineering, Curtin University, Perth, WA, Australia","2011 IEEE PES Innovative Smart Grid Technologies","","2011","","","1","5","Charging stations will be increasing in popularity as they can promote and support the increasing number of Plug-in Electric Vehicles (PEVs) entering into smart grid to reduce emissions and fuel consumptions. However, the locations, relatively large ratings and unpredictable charging characteristics of these stations can have a significant impact on the performance of smart grid. This paper proposes identification of the weakest buses over 24 hours in order to study and compensate the detrimental impacts of PEV charging stations on voltage profiles and voltage stability of smart grid. Assuming a smart grid communication infrastructure, the positive sequence voltage ranking index V/V<sub>o</sub> is utilized in hourly bases to identify the weakest buses in unbalanced multiphase distribution networks without/with PEV charging stations. Simulation results are performed and compared for an unbalanced multiphase 13 node test feeder with different locations of PEV charging stations using DIgSILENT PowerFactory software.","","978-1-4577-0875-6978-1-4577-0873-2978-1-4577-0874","10.1109/ISGT-Asia.2011.6167155","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6167155","weakest bus;plug-in electric vehicles;charging stations and smart grid","Power system stability;Smart grids;Stability criteria;Indexes;Electric vehicles;Educational institutions","distribution networks;electric vehicles;smart power grids","weakest buses identification;unbalanced multiphase smart grid communication infrastructure;plug-in electric vehicle charging station;fuel consumption;unpredictable charging characteristics;PEV charging station;voltage stability;positive sequence voltage ranking index;unbalanced multiphase distribution network;unbalanced multiphase node test feeder;DIgSILENT PowerFactory software","","4","15","","","","","","IEEE","IEEE Conferences"
"Simulation and sensitivity analysis on temperature field of heat transmission in single underground thermal well","Shi Yan; Tian Hailong; Lei Hongwu; T. Xu","College of Environment and Resources, JinLin University, Changchun, China; College of Environment and Resources, JinLin University, Changchun, China; College of Environment and Resources, JinLin University, Changchun, China; College of Environment and Resources, JinLin University, Changchun, China","Proceedings 2011 International Conference on Transportation, Mechanical, and Electrical Engineering (TMEE)","","2011","","","1386","1389","Based on TOUGHREACT simulator, the mathematical model is established for representing the process of unsteady-state heat transfer between underground heat exchanger and soil water-rock-air in the infinite area. Heat transfer characteristics and distribution of transient temperature field of heat exchange wells is simulated for cooling in summer. In addition, from the statistics theory, using the statistical significance test module of SPSS software, the significance and correlation between the changes of thermo-physical parameters and temperature field of the soil are analyzed by using nonparametric Sperman rank correlation coefficient method. The practical application and design can be guided by the paper.","","978-1-4577-1701-7978-1-4577-1700-0978-1-4577-1699","10.1109/TMEE.2011.6199465","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6199465","Heat transfer;Numerical simulation;Sensitivity analysis;component;formatting","Heat transfer;Soil;Mathematical model;Correlation;Heat pumps;Water heating","cooling;geophysical fluid dynamics;geophysical techniques;ground source heat pumps;heat exchangers;sensitivity analysis;terrestrial heat","simulation;sensitivity analysis;temperature field;heat transmission;single underground thermal well;TOUGHREACT simulator;mathematical model;unsteady-state heat transfer;underground heat exchanger;soil water-rock-air;infinite area;heat exchange wells;summer cooling;statistics theory;statistical significance test module;SPSS software;thermo-physical parameters;nonparametric Sperman rank correlation coefficient method","","","7","","","","","","IEEE","IEEE Conferences"
"Efficient OpenMP data mapping for multicore platforms with vertically stacked memory","A. Marongiu; M. Ruggiero; L. Benini","DEIS - University of Bologna, Viale Risorgimento, 2 - 40136 - Italy; DEIS - University of Bologna, Viale Risorgimento, 2 - 40136 - Italy; DEIS - University of Bologna, Viale Risorgimento, 2 - 40136 - Italy","2010 Design, Automation & Test in Europe Conference & Exhibition (DATE 2010)","","2010","","","105","110","Emerging TSV-based 3D integration technologies have shown great promise to overcome scalability limitations in 2D designs by stacking multiple memory dies on top of a many-core die. Application software developers need programming models and tools to fully exploit the potential of vertically stacked memory. In this work, we focus on efficient data mapping for SPMD parallel applications on an explicitly managed 3D-stacked memory hierarchy, which requires placement of data across multiple vertical memory stacks to be carefully optimized. We propose a programming framework with compiler support that enables array partitioning. Partitions are mapped to the 3D-stacked memory on top of the processor that mostly accesses it to take advantage of the lower latencies of vertical interconnect and for minimizing high-latency traffic on the horizontal plane.","1558-1101;1530-1591;1530-1591","978-3-9810801-6-2978-1-4244-7054-9978-3-9810801-6","10.1109/DATE.2010.5457227","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5457227","","Multicore processing;Application software;Memory management;Delay;Random access memory;Scalability;Stacking;Embedded computing;Network-on-a-chip;Program processors","data handling;multiprocessing systems;parallel processing;storage management","OpenMP data mapping;multicore platform;vertically stacked memory;TSV based 3D integration technology;SPMD parallel application;3D stacked memory hierarchy;multiple vertical memory stack;programming framework","","4","22","","","","","","IEEE","IEEE Conferences"
"Google's C/C++ toolchain for smart handheld devices","Doug Kwan; Jing Yu; B. Janakiraman","Google Inc., 1600 Amphitheatre Parkway, Mountain View, CA, USA; Google Inc., 1600 Amphitheatre Parkway, Mountain View, CA, USA; Google Inc., 1600 Amphitheatre Parkway, Mountain View, CA, USA","Proceedings of Technical Program of 2012 VLSI Technology, System and Application","","2012","","","1","4","Smart handheld devices are ubiquitous today and software plays an important role on them. Therefore a compiler and related tools can improve devices by generating efficient, compact and secure code. In this paper, we share our experience of applying various compilation techniques at Google to improve software running on smart handheld devices, using our mobile platforms as examples. At Google we use the GNU toolchain for generating code on different platforms and for conducting compiler research and development. We have developed new techniques, added features and functionality in the GNU tools. Some of these results are now used for smart handheld devices.","1524-766X;1930-8868","978-1-4577-2084-0978-1-4577-2083-3978-1-4577-2082","10.1109/VLSI-TSA.2012.6210142","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6210142","","Optimization;Google;Handheld computers;Benchmark testing;Software;Mobile communication;Performance evaluation","","","","","29","","","","","","IEEE","IEEE Conferences"
"Effect of Common Rail System on Vehicle Engine Combustion Performance","W. Ping; J. Chunjun; T. Bin; S. Xigeng","NA; NA; NA; NA","2010 International Conference on Optoelectronics and Image Processing","","2010","1","","464","467","A test bench is set up based on a CA6106 Direct Injection (DI) diesel engine equipped with ECD-U2. MAP calibration is accomplished using the online calibration software, combustion analysis software and data acquisition software. The precision, feasibility and reliability of software design are verified by the tests. The experiments indicate that Common Rail (CR) injection system has a great potential to decrease the emission and improve the performance of DI diesel engine. The experiments on function matching of CA6DE diesel engine are conducted. The structural parameters of injector, the structural parameters and swirl ratio of combustion chamber are improved and optimized. Using the high pressure CR electronically controlled system and corresponding measurement instruments, the fuel consumption and exhaust parameters are measured. The results show that a higher injection pressure, smaller injector core diameter, as well as a lower swirl ratio, a proper combustion chamber shape, and a delay in injection timing, can improve the mixing and combustion of fuel-air mixture, reduce harmful exhaust, and improve the overall performance of economy, power and exhaust.","","978-1-4244-8683","10.1109/ICOIP.2010.280","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5663173","Diesel Engine;Common Rail;Combustion Performance;Pilot Injection","Image processing","automotive engineering;calibration;combustion;data acquisition;diesel engines;mechanical engineering computing","common rail injection system;vehicle engine combustion performance;CA6106 direct injection diesel engine;MAP calibration;online calibration software;combustion analysis software;data acquisition software","","1","8","","","","","","IEEE","IEEE Conferences"
"Specific Absorption Calculation for Pilot Exposed to High Power Microwave","J. Huang; Y. Hu; H. Zhang; Y. Jin","NA; NA; NA; NA","2010 International Conference on Computational Intelligence and Software Engineering","","2010","","","1","4","A simplified parallel finite difference time domain algorithm is brought out based on 1-ranked Debye equation by nonlinear levenberg-marquardt least-squares curve fitting method. A safety threshold with respect to spatial average and spatial peak specific absorption (SA) is proposed, a shielded cockpit glass construction is designed, and specific absorption calculation for pilot in an aircraft is carried out on 3D model with a scale of 1:1, which is electrically huge and very complex in geometry and electromagnetic parameters. The simulation tests show that the shielded cockpit is protective effective in the range of 0.3-6GHz by avoiding resonance frequency of human body and speedup ratio of the presented approach can compare with that of the conventional method.","","978-1-4244-5391-7978-1-4244-5392","10.1109/CISE.2010.5676717","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676717","","Time domain analysis;Finite difference methods;Humans;Safety;Equations;Mathematical model","biological effects of microwaves;curve fitting;finite difference time-domain analysis;least squares approximations","specific absorption calculation;high power microwave;parallel finite difference time domain algorithm;Debye equation;nonlinear Levenberg-Marquardt least-squares curve fitting method;spatial peak specific absorption","","","12","","","","","","IEEE","IEEE Conferences"
"Catalogue personolization using distributed learning automata and Markov chain probability model","M. Bazarganigilani","Charles Sturt University, Faculty of Business, Melbourne, Australia","2010 2nd International Conference on Software Technology and Engineering","","2010","2","","V2-82","V2-85","The huge amount of information of the web has caused problems for selecting their item. The only solution for this problem is to score the items and then sorting them according to their ranks. we filtered them in a catalogue and perform our tests with the different sizes of catalogues.","","978-1-4244-8666-3978-1-4244-8667","10.1109/ICSTE.2010.5608767","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5608767","component;Customer Requirement Classification;Distributed Learning Automata;Markov Chain Model;Catalogue Personlization","Computational modeling;Learning automata","cataloguing;electronic commerce;information filtering;Internet;learning automata;Markov processes;personal information systems;probability;sorting","catalogue personalization;distributed learning automata;Markov chain probability model;Web information;item score;item selection;sorting;information filtering","","","8","","","","","","IEEE","IEEE Conferences"
"Simulating LTE Cellular Systems: An Open-Source Framework","G. Piro; L. A. Grieco; G. Boggia; F. Capozzi; P. Camarda","&#x201C;DEE&#x2014;Dipartimento di Elettrotecnica ed Elettronica&#x201D;, Politecnico di Bari, Bari, Italy; &#x201C;DEE&#x2014;Dipartimento di Elettrotecnica ed Elettronica&#x201D;, Politecnico di Bari , Bari, Italy; &#x201C;DEE&#x2014;Dipartimento di Elettrotecnica ed Elettronica&#x201D;, Politecnico di Bari , Bari, Italy; &#x201C;DEE&#x2014;Dipartimento di Elettrotecnica ed Elettronica&#x201D;, Politecnico di Bari , Bari, Italy; &#x201C;DEE&#x2014;Dipartimento di Elettrotecnica ed Elettronica&#x201D;, Politecnico di Bari , Bari, Italy","IEEE Transactions on Vehicular Technology","","2011","60","2","498","513","Long-term evolution (LTE) represents an emerging and promising technology for providing broadband ubiquitous Internet access. For this reason, several research groups are trying to optimize its performance. Unfortunately, at present, to the best of our knowledge, no open-source simulation platforms, which the scientific community can use to evaluate the performance of the entire LTE system, are freely available. The lack of a common reference simulator does not help the work of researchers and poses limitations on the comparison of results claimed by different research groups. To bridge this gap, herein, the open-source framework LTE-Sim is presented to provide a complete performance verification of LTE networks. LTE-Sim has been conceived to simulate uplink and downlink scheduling strategies in multicell/multiuser environments, taking into account user mobility, radio resource optimization, frequency reuse techniques, the adaptive modulation and coding module, and other aspects that are very relevant to the industrial and scientific communities. The effectiveness of the proposed simulator has been tested and verified considering 1) the software scalability test, which analyzes both memory and simulation time requirements; and 2) the performance evaluation of a realistic LTE network providing a comparison among well-known scheduling strategies.","0018-9545;1939-9359","","10.1109/TVT.2010.2091660","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5634134","Long-term evolution (LTE);modeling;performance evaluation;simulation","Unified modeling language;Object oriented modeling;Downlink;Adaptation model;OFDM;Media Access Protocol","adaptive codes;adaptive modulation;cellular radio;frequency response;Internet;Long Term Evolution;mobile computing","LTE cellular systems;long-term evolution;broadband ubiquitous Internet access;open-source framework LTE-Sim;uplink scheduling strategies;downlink scheduling strategies;multicell-multiuser environments;user mobility;radio resource optimization;frequency reuse techniques;adaptive modulation module;adaptive coding module;software scalability test;memory requirements;simulation time requirements","","338","44","","","","","","IEEE","IEEE Journals & Magazines"
"Shuffled Frog Leaping Algorithm for solving profit based unit commitment problem","K. Selvakumar; R. S. Vignesh; R. Vijayabalan","Department Of EEE, KSR College of Technology, Tiruchengode-637215., Tamil Nadu, India; Department Of ECE, KSR College of Technology, Tiruchengode-637215., Tamil Nadu, India; Department Of EEE, KSR College of Technology, Tiruchengode-637215., Tamil Nadu, India","2012 International Conference on Computing, Communication and Applications","","2012","","","1","6","In this paper, Shuffled Frog Leaping Algorithm (SFLA) is employed to solve the profit based unit commitment problem (PBUCP) under deregulated environment, to maximize the GENeration Companies (GENCO) profit. This paper presents a new approach of GENCOs profit based unit commitment using SFLA technique in a day ahead competitive electricity markets. Profit-based UC formulation considers the softer demand constraint and allocates fixed and transitional costs to the scheduled hours. Deregulation is unbundling of vertically integrated power system into generation (GENCOs), transmission (TRANSCOs) and distribution companies (DISCOs). Deregulation in power sector increases the efficiency of electricity production and distribution, offer lower prices, higher quality, secure, more reliable product and also it has the advantage that customers are allowed to choose their suppliers. This technique achieved great challenges for the power industry and thus an individual human can take their own decision by choosing the reliable continuous supply of power from the electricity markets at an affordable price. Under restructured system, GENeration COmpanies schedules their generators with the objective of maximizing their profit. This proposed algorithm is for a small unit test system with 10 units 24 hour data and the simulations are carried out to show the performance of proposed methodology using MATLAB software. It is observed from the simulation results that the proposed algorithm provides maximum profit with less computational time compared to existing techniques.","2325-6001;2325-601X","978-1-4673-0273-9978-1-4673-0270-8978-1-4673-0272","10.1109/ICCCA.2012.6179234","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6179234","Deregulation;Profit Based Unit Commitment (PBUC);Shuffled Frog Leaping Algorithm (SFLA)","Power systems;Companies;Optimization;Electricity supply industry;Simulation;Silicon;Schedules","power distribution economics;power generation economics;power markets;power transmission economics;profitability","shuffled frog leaping algorithm;profit based unit commitment problem;deregulated environment;generation companies profit;GENCOs profit;SFLA technique;day ahead competitive electricity market;demand constraint;fixed cost;transitional cost;vertically integrated power system;transmission company;distribution company;power sector deregulation;Matlab software","","1","12","","","","","","IEEE","IEEE Conferences"
"Research on the electric field distribution of multilayer dielectric under AC-DC Composite Voltage","Jianwei Cheng; Zongren Peng; Peng Liu","State Key Laboratory of Electrical Insulation and Power Equipment, Xi'an Jiaotong University, 710049, China; State Key Laboratory of Electrical Insulation and Power Equipment, Xi'an Jiaotong University, 710049, China; State Key Laboratory of Electrical Insulation and Power Equipment, Xi'an Jiaotong University, 710049, China","2012 IEEE 10th International Conference on the Properties and Applications of Dielectric Materials","","2012","","","1","4","Converter transformer, converter valve and bushing are the most important equipments in the valve hall of converter station. At normal working condition, these equipments will bear AC-DC composite voltage and current; at polarity reversal condition, they must endure a higher transient DC bias voltage, which will bring out serious electric field distortion of the multilayer dielectric in devices. In the type test process of converter transformer and bushing, the AC superposed DC voltage and transient DC bias voltage are also included, so researching the electric field distribution of multilayer dielectric in AC-DC composite voltage has great significance for the design of converter transformer, converter valve and bushing. In this paper, a double-layered dielectric model was put forward to analyse the law of electric field distribution under different voltage forms by using field-circuit coupled method, and the simulation results of the model under AC superposed DC and polarity reversal voltage were given via a finite element method (FEM) analysis software ANSYS. The simulation calculation method was proved to be correct in compared with the theoretical analysis result. Based on these studies, the electric field distribution of one ±800 kV converter transformer bushing outlet terminal under AC superposed DC and polarity reversal voltage were calculated and discussed. The above research results provide an important reference for the design of the converter transformer, converter valve and bushing.","2160-9241;2160-9225;2160-9225","978-1-4673-2851-7978-1-4673-2852-4978-1-4673-2850","10.1109/ICPADM.2012.6318972","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6318972","multilayer dielectric;field-circuit coupled method;polarity reversal;FEM;electirc field distribution","Insulators;Oil insulation;Optimization","AC-DC power convertors;bushings;electric fields;finite element analysis;multilayers;power transformers","electric field distribution;multilayer dielectric model;AC-DC composite voltage;converter valve design;polarity reversal condition;transient DC bias voltage;AC superposed DC voltage;double-layered dielectric model;field-circuit coupled method;polarity reversal voltage;finite element method;FEM analysis;ANSYS software;simulation calculation method;converter transformer bushing outlet terminal design;voltage 800 kV","","1","3","","","","","","IEEE","IEEE Conferences"
"Evaluating the potential of graphics processors for high performance embedded computing","S. Mu; C. Wang; M. Liu; D. Li; M. Zhu; X. Chen; X. Xie; Y. Deng","Singhua University; Singhua University; BeiHang University; BeiHang University; Singhua University; Chinese Academy of Sciences; Singhua University; Singhua University","2011 Design, Automation & Test in Europe","","2011","","","1","6","Today's high performance embedded computing applications are posing significant challenges for processing throughout. Traditionally, such applications have been realized on application specific integrated circuits (ASICs) and/or digital signal processors (DSP). However, ASICs' advantage in performance and power often could not justify the fast increasing fabrication cost, while current DSP offers a limited processing throughput that is usually lower than 100GFLOPS. On the other hand, current multi-core processors, especially graphics processing units (GPUs), deliver very high computing throughput, and at the same time maintain high flexibility and programmability. It is thus appealing to study the potential of GPUs for high performance embedded computing. In this work, we perform a comprehensive performance evaluation on GPUs with the high performance embedded computing (HPEC) benchmark suite, which consist a broad range of signal processing benchmarks with an emphasis on radar processing applications. We develop efficient GPU implementations that could outperform previous results for all the benchmarks. In addition, a systematic instruction level analysis for the GPU implementations is conducted with a GPU micro-architecture simulator. The results provide key insights on optimizing GPU hardware and software. Meanwhile, we also compared the performance and power efficiency between GPU and DSP with the HPEC benchmarks. The comparison reveals that the major hurdle for GPU's applications in embedded computing is its relatively low power efficiency.","1558-1101;1530-1591;1530-1591","978-3-9810801-8-6978-1-61284-208-0978-3-9810801-7","10.1109/DATE.2011.5763120","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5763120","GPU;Multi-core;HPEC benchmark;DSP;parallel computing;Fermi;GFLOPS","Graphics processing unit;Benchmark testing;Digital signal processing;Instruction sets;Parallel processing;Throughput;Kernel","computer graphic equipment;coprocessors;embedded systems;multiprocessing systems;radar signal processing","graphics processors;high performance embedded computing;application specific integrated circuits;digital signal processors;multicore processors;radar processing application","","5","29","","","","","","IEEE","IEEE Conferences"
"Mediation of Service Overhead in Service-Oriented Grid Architectures","P. Östberg; E. Elmroth","NA; NA","2011 IEEE/ACM 12th International Conference on Grid Computing","","2011","","","9","18","Grid computing applications and infrastructures build heavily on Service-Oriented Computing development methodology and are often realized as Service-Oriented Architectures. The Grid Job Management Framework (GJMF) is a flexible Grid infrastructure and application support tool that offers a range of abstractive and platform independent interfaces for middleware-agnostic Grid job submission, monitoring, and control. In this paper we use the GJMF as a test bed for characterization of Grid Service-Oriented Architecture overhead, and evaluate the efficiency of a set of design patterns for overhead mediation mechanisms featured in the framework.","2152-1093;2152-1085","978-0-7695-4572-1978-1-4577-1904","10.1109/Grid.2011.11","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6076493","Grid computing;Grid job management;Grid ecosystem;Performance Analysis;Service-Oriented Architecture","Throughput;Web services;Optimization;Computer architecture;Performance evaluation;Time measurement","grid computing;middleware;service-oriented architecture;software engineering","service overhead;service-oriented grid architectures;grid computing applications;grid infrastructures;service-oriented computing development methodology;service-oriented architectures;grid job management framework;GJMF;flexible grid infrastructure;application support tool;abstractive independent interfaces;platform independent interfaces;middleware-agnostic grid job submission;test bed;grid service-oriented architecture overhead;design patterns;overhead mediation mechanisms","","2","13","","","","","","IEEE","IEEE Conferences"
"An FPGA-based prototyping method for verification, characterization and optimization of LDPC error correction systems","P. Sakellariou; I. Tsatsaragkos; N. Kanistras; A. Mahdi; V. Paliouras","Electrical and Computer Engineering Department, University of Patras, 26500, Greece; Electrical and Computer Engineering Department, University of Patras, 26500, Greece; Electrical and Computer Engineering Department, University of Patras, 26500, Greece; Electrical and Computer Engineering Department, University of Patras, 26500, Greece; Electrical and Computer Engineering Department, University of Patras, 26500, Greece","2012 International Conference on Embedded Computer Systems (SAMOS)","","2012","","","286","293","This paper introduces a methodology for forward error correction (FEC) architectures prototyping, oriented to system verification and characterization. A complete design flow is described, which satisfies the requirement for error-free hardware design and acceleration of FEC simulations. FPGA devices give the designer the ability to observe rare events, due to tremendous speed-up of FEC operations. A Matlab-based system assists the investigation of the impact of very rare decoding failure events on the FEC system performance and the finding of solutions which aim to parameters optimization and BER performance improvement of LDPC codes in the error floor region. Furthermore, the development of an embedded system, which offers remote access to the system under test and verification process automation, is explored. The presented here prototyping approach exploits the high-processing speed of FPGA-based emulators and the observability and usability of software-based models.","","978-1-4673-2297-3978-1-4673-2295-9978-1-4673-2296","10.1109/SAMOS.2012.6404188","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6404188","LDPC;FPGA emulation;embedded systems;verification;code characterization;error correcting performance","Parity check codes;Decoding;Field programmable gate arrays;MATLAB;Hardware;Forward error correction;Embedded systems","electronic engineering computing;embedded systems;error correction codes;field programmable gate arrays;formal verification;parity check codes","FPGA-based prototyping;LDPC error correction systems;forward error correction;FEC architectures prototyping;system verification;system characterization;error-free hardware design;Matlab-based system;LDPC codes;embedded system;software-based models","","2","35","","","","","","IEEE","IEEE Conferences"
"Bridge deck survey with high resolution Ground Penetrating Radar","A. Simi; G. Manacorda; A. Benedetto","Engineering Department, Georadar Division, IDS - Ingegneria Dei Sistemi, Pisa, Italy; Engineering Department, Georadar Division, IDS - Ingegneria Dei Sistemi, Pisa, Italy; Sciences of Civil Engineering Department, Roma 3 University, Roma, Italy","2012 14th International Conference on Ground Penetrating Radar (GPR)","","2012","","","489","495","Ground Penetrating Radar applications for structure surveying started to grow in the 1980s; amongst these, initial civil engineering applications included condition assessment of highway pavements and their foundations, with applications to structural concrete focusing on inspection of bridge decks. There are many factors that can cause or contribute to the damage of the top layer of concrete in bridge decks including the corrosion of steel rebar, freeze and thaw cycles, traffic loading, initial damage resulting from poor design and/or construction, and inadequate maintenance. When applied to the analysis of bridge decks, GPR can be successfully used for detecting internal corrosion of steel reinforcement within the concrete deck, which can be an indicator of poor quality overlay bonding or delamination at the rebar level. Therefore, this equipment has the ability to gain information about the condition of bridge decks in a more rapid and less costly fashion than coring and will perhaps yield a more reliable assessment than current geotechnical procedures. However, this application requires suitably designed equipment; for instance, optimization of antenna orientation to take advantage of signal polarization is an important feature for successfully locating reinforcing bars in a time-depth slice. Novel equipment has recently been developed to enable the nondestructive analysis of bridge decks; the IDS RIS Hi-Bright runs two arrays of high frequency sensors featuring a rapid, but very dense data collection, thus dramatically increasing the resolution of the GPR survey. Antenna dipoles in these arrays are deployed to collect two data sets with orthogonal antenna orientations, one with the electric field parallel to the scanning direction (VV), the other perpendicular to it (HH); in this way, the equipment is capable of collecting 16 profiles, 10 cm spaced in a single swath, thus collecting an incredible amount of information. Dedicated data analysis software provides a 2-D tomography of the underground layers and a 3-D view of the surveyed volume. Main output include the determination of pavement and concrete thickness, the detection of moist areas as well as concrete damage and the location of rebars and ducts within the concrete slab.","","978-1-4673-2663-6978-1-4673-2662-9978-1-4673-2661","10.1109/ICGPR.2012.6254915","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6254915","GPR;bridge;corrosion;delamination","Ground penetrating radar;Conferences","bridges (structures);concrete;condition monitoring;corrosion;dipole antennas;ducts;electric fields;electromagnetic wave polarisation;foundations;fracture;freezing;geotechnical engineering;ground penetrating radar;inspection;maintenance engineering;melting;nondestructive testing;radar antennas;radar resolution;rebar;slabs;steel;structural engineering;surveying","bridge deck;high resolution ground penetrating radar;structure surveying;civil engineering application;condition assessment;highway pavement;foundation;structural concrete;inspection;corrosion detection;steel rebar;freeze cycle;thaw cycle;traffic loading;maintenance;GPR;steel reinforcement;concrete deck;rebar level;reliable assessment;geotechnical procedure;optimization;signal polarization;reinforcing bar;nondestructive analysis;IDS RIS Hi-Bright;high frequency sensor;antenna dipole;orthogonal antenna orientation;electric field;scanning direction;data analysis software;2D tomography;underground layer;3D view;concrete thickness;moist area detection;concrete damage;ducts;concrete slab;distance 10 cm","","5","11","","","","","","IEEE","IEEE Conferences"
"A Multi-Scenario Model for Mid-Long Term Hydro-Thermal Optimal Scheduling","X. Ge; L. Zhang; J. Shu; N. Fu","NA; NA; NA; NA","2012 Asia-Pacific Power and Energy Engineering Conference","","2012","","","1","4","This paper has proposed a multi-scenario model of hydro-thermal optimal operation. On the premise of reliable supply of power and energy assurance, the objective of minimizing energy consumption is designed. Hydro-thermal optimal scheduling is formulated by the optimization of generation, maintenance plans and the water level. According to different scenarios of runoff, scenario tree model is established, while ensuring that optimization variables of all the forecast scenarios can satisfy the various constraints. For the complexity of the established model, based on integer algebraic model techniques, auxiliary state variables and associated constraints are introduced, and then the model is converted to a linear mixed integer programming problem. With the help of advanced GAMS model software, simulation system is established. Finally, using CPLEX solver, the scheme of hydro-thermal plants optimal operation is formatted. By the test and analysis of the practical examples including 2 thermal power units, 6 hydropower units, 17 maintenance programs, results demonstrate the rationality of the model and the availability of the method.","2157-4847;2157-4839;2157-4839","978-1-4577-0547-2978-1-4577-0545-8978-1-4577-0546","10.1109/APPEEC.2012.6306995","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6306995","","Maintenance engineering;Hydroelectric power generation;Optimal scheduling;Reservoirs;Power systems","algebra;digital simulation;hydrothermal power systems;integer programming;linear programming;minimisation;power engineering computing;power generation reliability;power generation scheduling","multiscenario model;midlong term hydrothermal optimal scheduling;power reliable supply;energy assurance;energy consumption minimization;water level;scenario tree model;optimization variables;forecast scenarios;integer algebraic model techniques;auxiliary state variables;linear mixed integer programming problem;advanced GAMS model software;CPLEX solver;hydrothermal plant optimal operation;thermal power units;hydropower units;maintenance programs","","1","10","","","","","","IEEE","IEEE Conferences"
"Patus for convenient high-performance stencils: Evaluation in earthquake simulations","M. Christen; O. Schenk; Y. Cui","NA; NA; NA","SC '12: Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis","","2012","","","1","10","PATUS is a code generation and auto-tuning framework for stencil computations targeting modern multi and many-core processors. The goals of the framework are productivity and portability for achieving high performance on the target platform. Its stencil specification language allows the programmer to express the computation in a concise way independently of hardware architecture-specific details. Thus, it increases the programmer productivity by removing the need for manual low-level tuning. We illustrate the impact of the stencil code generation in seismic applications, for which both weak and strong scaling are important. We evaluate the performance by focusing on a scalable discretization of the wave equation and testing complex simulation types of the AWP-ODC code to aim at excellent parallel efficiency, preparing for petascale 3-D earthquake calculations.","2167-4337;2167-4329;2167-4329","978-1-4673-0806-9978-1-4673-0805-2978-1-4673-0804","10.1109/SC.2012.95","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6468453","","Computer architecture;Hardware;Computational modeling;Propagation;Earthquakes;Laplace equations;Optimization","earthquake engineering;geophysics computing;parallel processing;program compilers;software portability;specification languages;wave equations","earthquake simulation;petascale 3-D earthquake calculations;parallel efficiency;AWP-ODC code;complex simulation testing;scalable wave equation discretization;seismic applications;stencil code generation;hardware architecture;stencil specification language;high performance stencil computations;portability;productivity;autotuning framework;many-core processors;multicore processors;code generation;PATUS","","11","28","","","","","","IEEE","IEEE Conferences"
"TPM-SIM: A framework for performance evaluation of Trusted Platform Modules","J. Schmitz; J. Loew; J. Elwell; D. Ponomarev; N. Abu-Ghazaleh","Department of Computer Science, State University of New York at Binghamton, USA; Department of Computer Science, State University of New York at Binghamton, USA; Department of Computer Science, State University of New York at Binghamton, USA; Department of Computer Science, State University of New York at Binghamton, USA; Department of Computer Science, State University of New York at Binghamton, USA","2011 48th ACM/EDAC/IEEE Design Automation Conference (DAC)","","2011","","","236","241","This paper presents a simulation toolset for estimating the impact of Trusted Platform Modules (TPMs) on the performance of applications that use TPM services, especially in multi-core environments. The proposed toolset, consisting of an integrated CPU/TPM simulator and a set of micro-benchmarks that exercise the major TPM services, can be used to analyze and optimize the performance of TPM-based systems and the TPM itself. In this paper, we consider two such optimizations: (1) exploiting multiple TPMs; and (2) reordering requests within the software stack to minimize queueing delays. Our studies indicate that both techniques result in significant performance improvement, especially as the number of concurrent applications using the TPM increases.","85-644924;0738-100x;0738-100x","978-1-4503-0636-2978-1-4503-0636-2978-1-4503-0636","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5981940","Trusted Platform Module;Performance Evaluation","Benchmark testing;Software;Delay;Seals;Multicore processing;Generators","security of data","TPM-SIM;performance evaluation;trusted platform modules;simulation toolset;multicore environment;integrated CPU/TPM simulator;micro-benchmarks;reordering requests;software stack;queueing delays minimization","","","16","","","","","","IEEE","IEEE Conferences"
"A Research on I.C. Engine Misfire Fault Diagnosis Based on Rough Sets Theory and Neural Network","Y. Wu; B. Kuang; H. Li; H. Gong","NA; NA; NA; NA","2010 International Conference on Intelligent Computation Technology and Automation","","2010","1","","318","323","A method for diagnosis of misfire fault in internal combustion engine based on exhaust density of HC, CO<sub>2</sub>, O<sub>2</sub> and the engine's work parameters are presented in this paper. Rough sets theory is used to simplify attribute parameter reflecting exhaust emission and conditions of internal combustion engine and in which unnecessary properties are eliminated. The engine's work parameters, exhaust emission with misfire fault and without fault are tested by the experimentation of CA6100 engine. A diagnosis model which describing the relationship between the misfire degree and the internal combustion engine's exhaust emission and work parameters is established based on rough sets theory and RBF neural network. The model reduces the sample size, optimizes the neural network, increase the diagnosis correctness. The model is also trained by test data and MATLAB software. The model has been used to diagnosis internal combustion engine misfire fault, the result illustrates that this diagnosis model is suitable. This system can reduce input node number and overcome some shortcomings, such as neural network scale is too large and the rate of classification is slow.","","978-1-4244-7280-2978-1-4244-7279","10.1109/ICICTA.2010.110","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5523314","internal combustion engine;misfire;rough sets;fault diagnosis;information fusion","Fault diagnosis;Rough sets;Neural networks;Internal combustion engines;Fires;Mathematical model;Electric breakdown;Testing;Pollution;Data mining","carbon compounds;fault diagnosis;internal combustion engines;mechanical engineering computing;radial basis function networks;rough set theory","I.C. engine misfire fault diagnosis method;rough sets theory;internal combustion engine;CA6100 engine;RBF neural network;MATLAB software;test data;CO<sub>2</sub>","","2","9","","","","","","IEEE","IEEE Conferences"
"Direct importance estimation with probabilistic principal component analyzers","M. Yamada; M. Sugiyama; G. Wichern","Colorado State University, Japan; Department of Computer Science, Tokyo Institute of Technology & JST, Japan; Arts, Media, and Engineering, Arizona State University, USA","2010 IEEE International Conference on Acoustics, Speech and Signal Processing","","2010","","","1962","1965","The importance estimation problem (estimating the ratio of two probability density functions) has recently gathered a great deal of attention for use in various applications, e.g., outlier detection and covariate shift adaptation. In this paper, we propose a new importance estimation method using mixtures of probabilistic principal component analyzers (PPCAs). Our method employs the framework of the Kullback-Leibler importance estimation procedure (KLIEP) using using linear or kernel models. The proposed approach entitled PPCA mixture KLIEP (PM-KLIEP) can improve importance estimation accuracy with correlated and rank-deficient data. Through experiments, we show the validity of the proposed approach.","2379-190X;1520-6149;1520-6149","978-1-4244-4295-9978-1-4244-4296","10.1109/ICASSP.2010.5495290","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5495290","Importance;KLIEP;Probabilistic PCA;EM algorithm","Principal component analysis;Testing;State estimation;Probability density function;Kernel;Computer science;Art;Application software;Data processing;Supervised learning","principal component analysis;probability","direct importance estimation;probabilistic principal component analyzers;estimation problem;probability density functions;Kullback-Leibler importance estimation procedure","","","9","","","","","","IEEE","IEEE Conferences"
"Optimal allocation of capacitors in radial distribution systems with distributed generation","J. F. Franco; M. J. Rider; M. Lavorato; R. Romero","Faculdade de Engenharia de Ilha Solteira, UNESP - Universidade Estadual Paulista, Departamento de Engenharia Elétrica, Ilha Solteira - SP, Brazil; Faculdade de Engenharia de Ilha Solteira, UNESP - Universidade Estadual Paulista, Departamento de Engenharia Elétrica, Ilha Solteira - SP, Brazil; Faculdade de Engenharia de Ilha Solteira, UNESP - Universidade Estadual Paulista, Departamento de Engenharia Elétrica, Ilha Solteira - SP, Brazil; Faculdade de Engenharia de Ilha Solteira, UNESP - Universidade Estadual Paulista, Departamento de Engenharia Elétrica, Ilha Solteira - SP, Brazil","2011 IEEE PES CONFERENCE ON INNOVATIVE SMART GRID TECHNOLOGIES LATIN AMERICA (ISGT LA)","","2011","","","1","6","This paper presents a mixed-integer linear programming approach to solving the optimal fixed/switched capacitors allocation (OCA) problem in radial distribution systems with distributed generation. The use of a mixed-integer linear formulation guarantees convergence to optimality using existing optimization software. The results of one test system and one real distribution system are presented in order to show the accuracy as well as the efficiency of the proposed solution technique.","","978-1-4577-1801-4978-1-4577-1802-1978-1-4577-1800","10.1109/ISGT-LA.2011.6083188","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6083188","Distribution system optimization;optimal allocation of capacitors;mixed-integer linear programming;CPLEX;AMPL","Capacitors;Switches;Reactive power;Generators;Resource management;Mathematical model;Investments","distributed power generation;integer programming;linear programming;power capacitors","optimal capacitor allocation;radial distribution systems;distributed generation;mixed-integer linear programming;optimization software","","","24","","","","","","IEEE","IEEE Conferences"
"Towards cyber operations - The new role of academic cyber security research and education","J. Kallberg; B. Thuraisingham","CySREC, Erik Jonsson School of Engineering and Computer Science, The University of Texas at Dallas, Richardson, 75083-0688, USA; CySREC, Erik Jonsson School of Engineering and Computer Science, The University of Texas at Dallas, Richardson, 75083-0688, USA","2012 IEEE International Conference on Intelligence and Security Informatics","","2012","","","132","134","The shift towards cyber operations represents a shift not only for the defense establishments worldwide but also cyber security research and education. Traditionally cyber security research and education has been founded on information assurance, expressed in underlying subfields such as forensics, network security, and penetration testing. Cyber security research and education is connected to the homeland security agencies and defense through funding, mutual interest in the outcome of the research, and the potential job market for graduates. The future of cyber security is both defensive information assurance measures and active defense driven information operations that jointly and coordinately are launched, in the pursuit of a cohesive and decisive execution of the national cyber defense strategy. The cohesive cyber defense requires universities to optimize their campus wide resources to fuse knowledge, intellectual capacity, and practical skills in an unprecedented way in cyber security. The future will require cyber defense research teams to address not only computer science, electrical engineering, software and hardware security, but also political theory, institutional theory, behavioral science, deterrence theory, ethics, international law, international relations, and additional social sciences. This paper is the result of an ocular survey of the U.S. 48 academic CAE-R research centers, evaluating the collective group of research centers' ability to adapt to the shift towards cyber operations, and the challenges therein.","","978-1-4673-2104-4978-1-4673-2105","10.1109/ISI.2012.6284146","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6284146","cyber operations;cyberdefense;information assurance;center of academic excellence;CAE-R;defense;cyberwar;cyber education;information operations","Educational institutions;Computer security;Computer science;Computer aided engineering;International relations","computer science education;educational institutions;further education;industrial property;national security;security of data","cyber operations;academic cyber security research;academic cyber security education;information assurance;forensics;network security;penetration testing;homeland security agencies;national cyber defense strategy;knowledge fusion;intellectual capacity;computer science;electrical engineering;software security;hardware security;political theory;institutional theory;behavioral science;deterrence theory;international law;international relations;social sciences;ethics","","5","3","","","","","","IEEE","IEEE Conferences"
"Heuristic approaches for optimizing the performance of rule-based classifiers","D. Azar; H. Harmanani","Department of Computer Science and Mathematics, Lebanese American University, Byblos, Lebanon 1401 2010; Department of Computer Science and Mathematics, Lebanese American University, Byblos, Lebanon 1401 2010","2011 IEEE International Conference on Information Reuse & Integration","","2011","","","25","31","Rule-based classifiers are supervised learning techniques that are extensively used in various domains. This type of classifiers is popular because of its nature which makes it modular and easy to interpret and also because of its ability to provide the classification label as well as the reason behind it. Rule-based classifiers suffer from a degradation of their accuracy when they are used on new data. In this paper, we present an approach that optimizes the performance of the rule-based classifiers on the testing set. The approach is implemented using five different heuristics. We compare the behavior on different data sets that are extracted from different domains. Favorable results are reported.","","978-1-4577-0966-1978-1-4577-0964-7978-1-4577-0965","10.1109/IRI.2011.6009515","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6009515","","Software systems;Genetic algorithms;Object oriented modeling;Biological cells;Encoding;Predictive models;Accuracy","data analysis;knowledge based systems;learning (artificial intelligence)","rule-based classifiers;supervised learning techniques;heuristic approaches;data sets","","","25","","","","","","IEEE","IEEE Conferences"
"Design and implementation of multi-touch system using FTIR technique for optimization of finger touch detection","Ahsanullah; A. K. B. Mahmood; S. Sulaiman","Department of Computer and Information Sciences, Universiti Teknologi PETRONAS, Bandar Seri Iskandar, 31750 Tronoh, Perak, Malaysia; Department of Computer and Information Sciences, Universiti Teknologi PETRONAS, Bandar Seri Iskandar, 31750 Tronoh, Perak, Malaysia; Department of Computer and Information Sciences, Universiti Teknologi PETRONAS, Bandar Seri Iskandar, 31750 Tronoh, Perak, Malaysia","2010 International Symposium on Information Technology","","2010","1","","1","7","In recent years, multi-touch screens are emerging and seem to be ubiquitous in near future. The intuitive surfaces provide collaborative workspace for users to perform natural style of multi-touch interaction simultaneously. Users can access the interactive elements such as WIMP (window, icon, menus and pointing) using their bare fingers. At the same time multi-touch screens have ambiguity in multiple finger touch detection. Users acquire the accuracy in the fingers touch detection during the selection of interactive elements. In order to optimize the multiple finger touch detection, we propose the system and its architecture using the Frustrated Total Internal Reflection (FTIR) sensing technique. The system surface is improved by superimposing the transparent layers on acrylic surface. In addition, we modify the architecture of multi-touch surface by configuring the infrared light emitting diodes against the clean edges of acrylic surface. In order to test the system we conduct the experiment by involving the subjects and investigate the quality of fingertip blobs. The proposed system may increase the image quality of fingertip blobs which may help in achieving the fidelity for sensing the multiple fingers on interactive surface.","2155-899X;2155-8973;2155-897","978-1-4244-6718-1978-1-4244-6715-0978-1-4244-6717","10.1109/ITSIM.2010.5561308","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5561308","multi-touch surface;multi-touch;finger touch;fingertip blobs","Fingers;Cameras;Surface treatment;Sensors;Computer architecture;Software;Hardware","Fourier transform spectra;infrared spectra;light emitting diodes;touch sensitive screens","FTIR technique;finger touch detection;multitouch screens;collaborative workspace;WIMP;window icon menus and pointing;interactive elements;frustrated total internal reflection;image quality","","3","21","","","","","","IEEE","IEEE Conferences"
"Dynamic active power control with improved undead-band droop for HVDC grids","T. K. Vrana; L. Zeni; O. B. Fosso","NTNU, Norway; DTU, Denmark; NTNU, Norway","10th IET International Conference on AC and DC Power Transmission (ACDC 2012)","","2012","","","1","6","The earlier developed control method using a piecewise linear droop curve, with different droop values for the different segments, has now been optimised for dynamic performance. Non-linearities at the junctions of two linear droop sections have been adressed. Also non-linearity of power based DC voltage control has been adressed. Dynamic instability due to high control gains has been treated and a new improved control structure has been proposed. The concepts have been validated with RMS simulation with the DIgSILENT PowerFactory software on the CIGRE B4 DC grid test system.","","978-1-84919-700","10.1049/cp.2012.1975","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6521274","HVDC;Power Converter;Active Power Control;Droop Control;Meshed DC Grid","","control nonlinearities;HVDC power transmission;piecewise linear techniques;power control;power grids;power system stability;power transmission control","dynamic active power control;undead-band droop;HVDC grids;piecewise linear droop curve;droop values;dynamic performance;linear droop sections;power-based DC voltage control nonlinearity;dynamic instability;high control gains;control structure;RMS simulation;DIgSILENT PowerFactory software;CIGRE B4 DC grid test system","","7","","","","","","","IET","IET Conferences"
"Parameter learning in lookahead online algorithms for data acknowledgment","T. Németh; B. Gyekiczki; C. Imreh","Institute of Informatics, University of Szeged, Hungary; Institute of Informatics, University of Szeged, Hungary; Institute of Informatics, University of Szeged, Hungary","3rd IEEE International Symposium on Logistics and Industrial Informatics","","2011","","","195","198","In the communication aspect of a computer network, data is sent by packets. If the communication channel is not completely safe, then the arrival of the packets must be acknowledged. In the data acknowledgment problem the goal is to determine the time of sending acknowledgments. Here we study lookahead online algorithms, where at time t the algorithm knows the arrival time of the packets arriving till time t+c. We present a new algorithm which is based on the idea of learning the optimal value of a parameter. The efficiency of the algorithms is investigated by testing them experimentally, and it is demonstrated that the new parameter learning algorithm performs significantly better than the original one.","2156-8790;2156-8804","978-1-4577-1841-0978-1-4577-1842-7978-1-4577-1840","10.1109/LINDI.2011.6031146","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6031146","","Algorithm design and analysis;Heuristic algorithms;Optimized production technology;Delay;Learning systems;Educational institutions;Software algorithms","computer network security;data handling;parameter estimation;telecommunication channels","lookahead online algorithms;computer network;communication channel;data acknowledgment problem;packets arrival time;parameter optimal value learning algorithm","","","12","","","","","","IEEE","IEEE Conferences"
"Design and verification of simulation models of passive houses","P. Novák; R. Šindelář","Christian Doppler Laboratory for Software Engineering Integration for Flexible Automation Systems, Vienna University of Technology, AT-1040 Vienna, Austria; Christian Doppler Laboratory for Software Engineering Integration for Flexible Automation Systems, Vienna University of Technology, AT-1040 Vienna, Austria","Proceedings of 2012 IEEE 17th International Conference on Emerging Technologies & Factory Automation (ETFA 2012)","","2012","","","1","4","Passive houses are featured with very low energy consumption for heating and operation, which is reached not only by excellent insulation, but also by proper control algorithms. Optimization of passive houses must be done both by civil and control engineers; furthermore, these disciplines must collaborate. For the optimization, simulation models are very useful test-beds. This paper presents the Building Simulation Library (Bldsimlib) and its mathematical-physical background. The universal library blocks simulate temperature, carbon dioxide concentration, relative humidity, and air pressure. The paper compares preliminary really measured data from an experimental passive house and simulation results. Finally, an ontology-based methodology for semi-automated design of simulation models is proposed and evaluated.","1946-0740;1946-0740;1946-0759","978-1-4673-4737-2978-1-4673-4735-8978-1-4673-4736","10.1109/ETFA.2012.6489697","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6489697","","","energy consumption;heating;home automation","simulation models;passive houses;low energy consumption;heating;proper control algorithms;civil engineers;control engineers;Building Simulation Library;Bldsimlib;mathematical-physical background;universal library blocks;carbon dioxide concentration;relative humidity;air pressure;ontology-based methodology","","3","14","","","","","","IEEE","IEEE Conferences"
"Cost excessive paths in cloud based services","K. Buell; J. Collofello","School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, USA; School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, USA","2012 IEEE 13th International Conference on Information Reuse & Integration (IRI)","","2012","","","324","331","The pay-as-you-go economic model of cloud computing leads naturally to an earn-as-you-go profit model for many cloud based services. These applications can benefit from low level analyses for cost optimization and verification. Testing cloud applications to ensure they meet monetary cost objectives has not been well explored in the current literature. We present a static analysis approach for determining which control flow paths in cloud applications can exceed a cost threshold. We build on tools used in Worst Case Execution Time analysis that provide a tight bound on processing time, and we implement provisions for adding bandwidth, storage, and service costs. Our approach determines the magnitude of cost excess for nodes in an application's call graph so that cloud developers can better understand where to focus their efforts to lower costs (or deem some excesses acceptable based on business case analysis).","","978-1-4673-2284-3978-1-4673-2282-9978-1-4673-2283","10.1109/IRI.2012.6303027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6303027","","Bandwidth;Cloud computing;Computational modeling;Business;Weight measurement;Real time systems","cloud computing;data flow graphs;program diagnostics;program verification;software cost estimation","cost excessive path;cloud based service;pay-as-you-go economic model;cloud computing;earn-as-you-go profit model;cost optimization;verification;cloud application testing;monetary cost objective;static analysis approach;control flow path;cost threshold;worst case execution time analysis;processing time;bandwidth cost;storage cost;service cost;application call graph;cloud developer;business case analysis","","1","20","","","","","","IEEE","IEEE Conferences"
"System#: High-level synthesis of physical simulations for FPGA-based real-time execution","C. Köllner; N. Adler; K. D. Müller-Glaser","Embedded Systems and Sensors Engineering, FZI Research Center for Information Technology, Haid-und-Neu-Str. 10-14, 76131 Karlsruhe, Germany; Embedded Systems and Sensors Engineering, FZI Research Center for Information Technology, Haid-und-Neu-Str. 10-14, 76131 Karlsruhe, Germany; Embedded Systems and Sensors Engineering, FZI Research Center for Information Technology, Haid-und-Neu-Str. 10-14, 76131 Karlsruhe, Germany","22nd International Conference on Field Programmable Logic and Applications (FPL)","","2012","","","731","734","FPGA-accelerated real-time simulations of physical phenomena gain importance in the area of Hardware-in-the-Loop tests for electrified powertrain components. Especially the challenging timing requirements demand dedicated hardware solutions. We propose System#, which is a highly modular, flexible and extensible open source environment for synthesizing physical computations to FPGAs. We demonstrate its capabilities by using an existing model of a squirrel-cage induction machine. We oppose several design variants, using fixed point and floating point arithmetic, and various settings for schedule length and pipelining depth of arithmetic operators. We conclude that the synthesized designs are on the par with a manually optimized implementation. Moreover, our approach scales well by allowing a time/area tradeoff over a wide range.","1946-147X;1946-1488","978-1-4673-2256-0978-1-4673-2257-7978-1-4673-2255","10.1109/FPL.2012.6339164","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6339164","","Field programmable gate arrays;Computational modeling;Mathematical model;Resource management;Algorithm design and analysis;IP networks;Hardware","digital simulation;field programmable gate arrays;fixed point arithmetic;floating point arithmetic;high level synthesis;mechanical engineering computing;pipeline arithmetic;power transmission (mechanical);public domain software;real-time systems;squirrel cage motors;vehicle dynamics","System#;high-level synthesis;physical simulations;FPGA-based real-time execution;FPGA-accelerated real-time simulations;hardware-in-the-Loop tests;electrified powertrain components;timing requirements;flexible open source environment;physical computations;squirrel-cage induction machine;fixed point arithmetic;floating point arithmetic;design variants;schedule length;pipelining depth;arithmetic operators;manually optimized implementation;HiL simulations","","","13","","","","","","IEEE","IEEE Conferences"
"Incorporating Large-Scale Distant Wind Farms in Probabilistic Transmission Expansion Planning—Part II: Case Studies","M. Moeini-Aghtaie; A. Abbaspour; M. Fotuhi-Firuzabad","Center of Excellence in Power System Management and Control, Department of Electrical Engineering, Sharif University of Technology, Tehran, Iran; Center of Excellence in Power System Management and Control, Department of Electrical Engineering, Sharif University of Technology, Tehran, Iran; Center of Excellence in Power System Management and Control, Department of Electrical Engineering, Sharif University of Technology, Tehran, Iran","IEEE Transactions on Power Systems","","2012","27","3","1594","1601","This paper is the second part of a two-paper set which comprehensively sets forth an innovative approach in transmission grid reinforcement studies in the presence of wind energy. Part I thoroughly defined the theory and algorithms. Here, to trace the feasibility of the proposed algorithm, three different case studies are implemented on the 24-Bus IEEE Reliability Test System (IEEE-RTS). The optimal solutions in Pareto fronts of different cases are reached, analyzed, and the final solution (optimal plan) of each case is obtained using the fuzzy decision making method. Moreover, in order to analyze the effects of variations in the large-scale wind farm generation on the transmission expansion planning (TEP) studies, the methodology is applied to the Iran 400-kV transmission grid. Two different generation expansion strategies are considered to investigate the impacts of various renewable energy policies on the TEP results. The wind energy-imposed costs of these two strategies are addressed, discussed, and compared to introduce some recommendations for wind integration policies.","0885-8950;1558-0679","","10.1109/TPWRS.2011.2182364","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6151863","IEEE-RTS;Non-Dominated Sorting Genetic Algorithm II (NSGA II);Point Estimation Method (PEM);transmission expansion planning (TEP);wind farm","Wind farms;Investments;Wind speed;Wind energy;Planning;Power transmission lines;Computer aided software engineering","decision making;fuzzy set theory;government policies;IEEE standards;Pareto optimisation;power generation planning;power generation reliability;power grids;power transmission planning;power transmission reliability;probability;renewable energy sources;wind power plants","large-scale distant wind farm generation;probabilistic transmission expansion planning;transmission grid reinforcement study;24-bus IEEE reliability test system;24-bus IEEE-RTS;Pareto front optimal solution;fuzzy decision making method;probabilistic TEP;Iran transmission grid;generation expansion strategy;renewable energy policy;wind energy-imposed cost;wind integration policy;voltage 400 kV","","32","8","","","","","","IEEE","IEEE Journals & Magazines"
"High speed robust image registration and localization using optimized algorithm and its performances evaluation","M. An; Z. Jiang; D. Zhao","School of Astronautics, Beihang University, Beijing 100191, P. R. China; School of Astronautics, Beihang University, Beijing 100191, P. R. China; School of Astronautics, Beihang University, Beijing 100191, P. R. China","Journal of Systems Engineering and Electronics","","2010","21","3","520","526","Local invariant algorithm applied in downward-looking image registration, usually computes the camera's pose relative to visual landmarks. Generally, there are three requirements in the process of image registration when using these approaches. First, the algorithm is apt to be influenced by illumination. Second, algorithm should have less computational complexity. Third, the depth information of images needs to be estimated without other sensors. This paper investigates a famous local invariant feature named speeded up robust feature (SURF), and proposes a highspeed and robust image registration and localization algorithm based on it. With supports from feature tracking and pose estimation methods, the proposed algorithm can compute camera poses under different conditions of scale, viewpoint and rotation so as to precisely localize object's position. At last, the study makes registration experiment by scale invariant feature transform (SIFT), SURF and the proposed algorithm, and designs a method to evaluate their performances. Furthermore, this study makes object retrieval test on remote sensing video. For there is big deformation on remote sensing frames, the registration algorithm absorbs the Kanade-Lucas-Tomasi (KLT) 3-D coplanar calibration feature tracker methods, which can localize interesting targets precisely and efficiently. The experimental results prove that the proposed method has a higher localization speed and lower localization error rate than traditional visual simultaneous localization and mapping (vSLAM) in a period of time.","1004-4132","","10.3969/j.issn.1004-4132.2010.03.026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6073147","local invariant features;speeded up robust feature (SURF);Harris corner;Kanada-Lucas-Tomasi (KLT) transform;Coplanar camera calibration algorithm;landmarks","Feature extraction;Cameras;Robustness;Image registration;Remote sensing;Lighting;Transmission line matrix methods","","","","1","","","","","","","BIAI","BIAI Journals & Magazines"
"Correlation based local measurement of small CTE for high temperature power electronics packaging","J. Hammacher; M. Dost; B. Seiler; L. Scheiter; E. Noack; S. Rzepka; B. Michel","Chemnitzer Werkstoffmechanik GmbH, Chemnitz, Germany; Chemnitzer Werkstoffmechanik GmbH, Chemnitz, Germany; Chemnitzer Werkstoffmechanik GmbH, Chemnitz, Germany; Chemnitzer Werkstoffmechanik GmbH, Chemnitz, Germany; Micro Materials Center, Fraunhofer ENAS, Chemnitz, Germany; Micro Materials Center, Fraunhofer ENAS, Chemnitz, Germany; Micro Materials Center, Fraunhofer ENAS, Chemnitz, Germany","2011 Semiconductor Conference Dresden","","2011","","","1","4","The image based displacement analysis called Digital Image Correlation (DIC) can be used as an alternative to classical TMA and other bulk methods to measure CTE-values of electronic packaging materials. By combining the operation of the thermally optimized commercial equipment EQUINOX from the company OMI and the correlation software VEDDAC of CWM GmbH Chemnitz, it was possible to develop a method to acquire anisotropic CTE values of specimens locally and with an accuracy of +/- 0.5*10<sup>-6</sup> /K over the entire value scale.","","978-1-4577-0430-7978-1-4577-0431-4978-1-4577-0429","10.1109/SCD.2011.6068734","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6068734","digital image correlation;CTE;non-destructive testing;reliability","Temperature measurement;Correlation;Materials;Displacement measurement;Measurement uncertainty;Optical variables measurement;Reliability","correlation methods;electronic engineering computing;electronics packaging;image processing;power electronics","correlation based local measurement;small CTE;high temperature power electronics packaging;image based displacement analysis;digital image correlation;DIC;thermally optimized commercial equipment;EQUINOX;correlation software;VEDDAC;CWM GmbH Chemnitz","","","3","","","","","","IEEE","IEEE Conferences"
"Run&#150;time Control of Subthreshold Current using Double&#150;Gate Device Technology","P. Beckett; H. Rudolph","NA; NA","2010 Fifth IEEE International Symposium on Electronic Design, Test & Applications","","2010","","","245","249","The close link between power and performance represents a significant hurdle to the design of portable embedded systems. As supply voltage falls, thereby saving dynamic power, the impact of threshold voltage on delay becomes greater so that it will become increasingly difficult to set a fixed threshold voltage that optimizes both performance and static power. This paper proposes a method around these conflicting requirements by allowing threshold voltages to be dynamically adjusted at run time via back-gate control on thin-body double-gate Silicon on Insulator (TBDGSOI) transistors. Using a multi-level simulation approach, with SPICE and VHDL-AMS, and using a small RISC microprocessor architecture as an example, it is shown that reductions in subthreshold leakage of between 6X and 11X can be achieved using this run-time control technique under a number of small software workloads.","","978-1-4244-6026-7978-1-4244-6025-0978-0-7695-3978","10.1109/DELTA.2010.28","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5438682","low power computer architecture;double gate;SOI;VHDL-AMS;modeling","Subthreshold current;Threshold voltage;Embedded system;Delay;Voltage control;Silicon on insulator technology;SPICE;Reduced instruction set computing;Microprocessors;Computer architecture","hardware description languages;silicon-on-insulator;SPICE;transistors","run-time control;subthreshold current;double-gate device technology;back-gate control;thin-body double-gate silicon on insulator transistors;SPICE;VHDL-AMS;RISC microprocessor architecture;run-time control technique","","","13","","","","","","IEEE","IEEE Conferences"
"An on-line approach for ANFIS modelling and control of a flexible manoeuvring system","M. Omar; M. Mohamad; M. A. Zaidan; M. O. Tokhi","Department of Automatic Control and Systems Engineering, The University of Sheffield, United Kingdom; Department of Automatic Control and Systems Engineering, The University of Sheffield, United Kingdom; Department of Automatic Control and Systems Engineering, The University of Sheffield, United Kingdom; Department of Automatic Control and Systems Engineering, The University of Sheffield, United Kingdom","2011 IEEE International Conference on Mechatronics","","2011","","","324","329","This paper presents an on-line nonlinear dynamic modelling and control approach based on adaptive neuro-fuzzy inference system (ANFIS) for a twin rotor multi-input multi-output system (TRMS), in the vertical plane motion. The TRMS can be considered as a flexible aerodynamic test rig that resembles the behaviour of a helicopter in hovering mode. The TRMS and similar manoeuvring systems are often subjected to random disturbances arising from various sources such as driving motors and external environmental sources. For such highly nonlinear systems with varying operating conditions, adaptive control approaches are suitable tools to cope with plant uncertainties. A model inverse control of the TRMS is developed using on-line ANFIS learning algorithm. The consequent and antecedent parameters of a first order Takagi-Sugeno fuzzy inference system are optimised on-line using recursive least squares and gradient descent algorithms, respectively. The optimal initialization of the ANFIS parameters is achieved through an off-line training process. The developed strategy is compared to other control laws in terms of tracking performance and disturbance rejection. The obtained simulation results demonstrate the efficiency of the on-line control scheme.","","978-1-61284-985-0978-1-61284-982-9978-1-61284-984","10.1109/ICMECH.2011.5971304","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5971304","ANFIS;flexible systems;on-line learning;set-point tracking;twin rotor","Transmission line measurements;Adaptation models;Propulsion;Switches;Software","adaptive systems;fuzzy reasoning;gradient methods;helicopters;learning (artificial intelligence);least squares approximations;MIMO systems;modelling;nonlinear dynamical systems","on-line approach;ANFIS modelling;flexible manoeuvring system;online nonlinear dynamic modelling;adaptive neuro-fuzzy inference system;twin rotor multiinput multioutput system;TRMS;vertical plane motion;flexible aerodynamic test rig;helicopter;hovering mode;inverse control;online ANFIS learning algorithm;first order Takagi-Sugeno fuzzy inference system;recursive least squares;gradient descent algorithms","","3","8","","","","","","IEEE","IEEE Conferences"
"Development of Brake-by-Wire Hardware-in-Loop Simulation Bench","J. Liu; Y. Fu; Z. Wang; J. Chen","NA; NA; NA; NA","2012 Second International Conference on Intelligent System Design and Engineering Application","","2012","","","1413","1417","This paper mainly describes a PC-based Brake-By-Wire Hardware-In-Loop-simulation Bench which has high efficiency and applicability. System architecture, hardware configuration, and software model of this bench are presented in details. The electric braking controllers collect, process and transfer all kinds of information, and generate simultaneous control codes according to the control algorithm, then drive electromechanical actuator and braking parts. For improving the safety and reliability, redundancy is adopted to realize reconstruction when the simulation system occurs faults. Experiments show this simulation bench is a practical tool for the design, test and parameter optimization for electric braking system.","","978-1-4577-2120-5978-0-7695-4608","10.1109/ISdea.2012.734","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6173474","brake-by-wire (BBW);hardware-in-the-loop simulation (HILS);redundancy;electro-mechanical actuator (EMA)","Atmospheric modeling;Aircraft;Wheels;Mathematical model;Computational modeling;Sensors;Synchronous motors","aerospace computing;aerospace safety;aircraft;brakes;braking;control engineering computing;electromechanical actuators;mechanical engineering computing;redundancy;wires","PC-based brake-by-wire hardware-in-loop simulation;system architecture;hardware configuration;software model;electric braking controller;control code;control algorithm;electromechanical actuator;safety;reliability;redundancy;simulation system;parameter optimization;electric braking system","","1","5","","","","","","IEEE","IEEE Conferences"
"Experimental and Numerical Investigation of Flux Density Distribution in the Individual Packets of a 100 kVA Transformer Core","F. Marketos; D. Marnay; T. Ngnegueu","ALSTOM GRID, Power Transformers Product Line, TICC,, Massy,, France; NA; NA","IEEE Transactions on Magnetics","","2012","48","4","1677","1680","The accurate measurement and calculation of transformer no-load losses is of high importance to transformer users and manufacturers alike. There are various ways of calculating no-load losses of transformers based on empirical formulas and experimentally obtained coefficients but these tend to mostly apply to known and tested core designs. The most important factors that contribute to the no-load loss of transformer cores are the grade of steel used, the core and corner geometry and stacking configuration. This paper presents re- sults and analysis for a 100 kVA, 3-phase model transformer core, consisting of 11 packets of different width laminations, over a range of flux densities up to 1.8 T. The results show the average magnetic flux desnity across the core cross section as well as the magnetic flux density of the individual packets. Using this information in combination with the magnetising current the no-load loss can be derived for the whole core as well as for the individual packets. The paper also shows numerical calculations of the excitation current required to magnetise the core. The FEA software used was developed in ALSTOM and allows the calculation of flux density distribution as well as the determination of the magnetising current necessary to magnetise the core at different flux density levels. It is believed that the combination of measurements and FE modelling will enable the calculation of local power loss thus allowing for optimisation of the various core parameters.","0018-9464;1941-0069","","10.1109/TMAG.2011.2173667","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6172357","Magnetic flux density;magnetic losses;magnetic cores;transformer cores","Transformer cores;Loss measurement;Magnetic flux;Current measurement;Magnetic domains;Density measurement","eddy current losses;finite element analysis;laminations;magnetic flux;parameter estimation;transformer cores","flux density distribution;transformer no-load loss measurement;transformer no-load loss calculation;core designs;core geometry;corner geometry;stacking configuration;three-phase model transformer core;laminations;average magnetic flux desnity;excitation current;numerical calculations;FEA software;magnetising current determination;local power loss calculation;core parameter optimisation","","9","5","","","","","","IEEE","IEEE Journals & Magazines"
"The forecasting method for the furnace bottom temperature and carbon content of submerged arc furnace based on improved BP neural network","Sun Ying; Yang Hongxia","Department of Electrical and Electronic Engineering, Changchun University of Technology, China; Department of Electrical and Electronic Engineering, Changchun University of Technology, China","2010 International Conference on Computer, Mechatronics, Control and Electronic Engineering","","2010","3","","238","240","In this paper, the forecasting method for the furnace bottom temperature and carbon content in smelting process is proposed to aim at the shortcomings of great energy and low productivity from control strategy, through the analysis of submerged arc furnace smelting process. The method provides a theoretical basis for dynamical control of submerged arc furnace smelting process. The forecasting model is established base on improved BP neural network, and 10 major factors affect the furnace bottom temperature and carbon content are selected to be as input variables. Variable step-size learning algorithm is used to achieve the purpose of global optimization and fast convergence. Data information from the regular and consecutive smelting process of the 801 furnace in 8th branch of Sinosteel Jilin Ferroalloy SCNB are selected to be as training samples and test samples, MATLAB simulation software is used to verify the accuracy and usefulness of the model.","2159-6026;2159-6034","978-1-4244-7958-0978-1-4244-7957-3978-1-4244-7955","10.1109/CMCE.2010.5610345","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5610345","Improved BP neural network;forecasting model;MATLAB","Mathematical model;MATLAB","arc furnaces;backpropagation;forecasting theory;neural nets;smelting","forecasting method;furnace bottom temperature;carbon content;submerged arc furnace;improved BP neural network;smelting process;variable step-size learning algorithm;global optimization;fast convergence;MATLAB simulation software","","","8","","","","","","IEEE","IEEE Conferences"
"Optimal size and location of distributed generation unit for voltage stability enhancement","M. Abdel-Akher; A. A. Ali; A. M. Eid; H. El-Kishky","Department of Electrical Engineering, Aswan Faculty of Engineering, South Valley University, 81542, Egypt; Department of Electrical Engineering, Aswan Faculty of Engineering, South Valley University, 81542, Egypt; Department of Electrical Engineering, Aswan Faculty of Engineering, South Valley University, 81542, Egypt; Department of Electrical Engineering, University of Texas at Tyler, 3900 University Blvd., 75799, USA","2011 IEEE Energy Conversion Congress and Exposition","","2011","","","104","108","This paper presents the analysis of a distribution system connected with distributed generation (DG) units. The developed technique is based on the steady state voltage stability index. The weakest branches in the distribution system which are more likely go to the instability region are selected for DG allocation. Two optimization methods are utilized to find out the size of the DG corresponding to minimum losses or minimum stability index. The Newton-Raphson load-flow is used to find the steady-state solution of the studied distribution system. The AMPL software package is utilized for evaluating the size of the DG units. The developed methods are tested using a 90-bus distribution system with a variety of case studies.","2329-3721;2329-3748","978-1-4577-0541-0978-1-4577-0542-7978-1-4577-0540","10.1109/ECCE.2011.6063755","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6063755","","Stability criteria;Indexes;Power system stability;Distributed power generation;Resource management;Steady-state","distributed power generation;distribution networks;load flow;power system stability","optimal size;distributed generation unit;voltage stability enhancement;DG units;steady state voltage stability index;optimization methods;Newton-Raphson load-flow;AMPL software package;90-bus distribution system","","5","14","","","","","","IEEE","IEEE Conferences"
"Development of Brake-By-Wire Hardware-In-Loop Simulation Bench","J. Liu; Y. Fu; Z. Wang; J. Chen","NA; NA; NA; NA","2012 International Conference on Computer Distributed Control and Intelligent Environmental Monitoring","","2012","","","236","240","This paper mainly describes a PC-based Brake-By-Wire Hardware-In-Loop-simulation Bench which has high efficiency and applicability. System architecture, hardware configuration, and software model of this bench are presented in details. The electric braking controllers collect, process and transfer all kinds of information, and generate simultaneous control codes according to the control algorithm, then drive electromechanical actuator and braking parts. For improving the safety and reliability, redundancy is adopted to realize reconstruction when the simulation system occurs faults. Experiments show this simulation bench is a practical tool for the design, test and parameter optimization for electric braking system.","","978-1-4673-0458-0978-0-7695-4639","10.1109/CDCIEM.2012.62","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6178558","brake-by-wire (BBW);hardware-in-the-loop simulation (HILS);redundancy;electro-mechanical actuator (EMA)","Aircraft;Atmospheric modeling;Wheels;Mathematical model;Computational modeling;Sensors;Control systems","aerospace components;aerospace computing;aerospace safety;brakes;braking;control engineering computing;electromechanical actuators;mechanical engineering computing","PC-based brake-by-wire hardware-in-loop-simulation bench;system architecture;hardware configuration;software model;electric braking controllers;simultaneous control codes;control algorithm;electromechanical actuator;braking parts;safety improvement;reliability improvement;electric braking system;aircraft braking system;parameter optimization","","","5","","","","","","IEEE","IEEE Conferences"
"Crafting a usable microkernel, processor, and I/O system with strict and provable information flow security","M. Tiwari; J. K. Oberg; X. Li; J. Valamehr; T. Levin; B. Hardekopf; R. Kastner; F. T. Chong; T. Sherwood","Dept. of Computer Science, University of California, Santa Barbara, USA; Dept. of Computer Science, University of California, Santa Barbara, USA; Dept. of Computer Science, University of California, Santa Barbara, USA; Dept. of Computer Science, University of California, Santa Barbara, USA; Naval Postgraduate School, Monterey, CA, USA; Dept. of Computer Science, University of California, Santa Barbara, USA; Dept. of Computer Science and Engineering, University of California, San Diego, USA; Dept. of Computer Science, University of California, Santa Barbara, USA; Dept. of Computer Science, University of California, Santa Barbara, USA","2011 38th Annual International Symposium on Computer Architecture (ISCA)","","2011","","","189","199","High assurance systems used in avionics, medical implants, and cryptographic devices often rely on a small trusted base of hardware and software to manage the rest of the system. Crafting the core of such a system in a way that achieves flexibility, security, and performance requires a careful balancing act. Simple static primitives with hard partitions of space and time are easier to analyze formally, but strict approaches to the problem at the hardware level have been extremely restrictive, failing to allow even the simplest of dynamic behaviors to be expressed. Our approach to this problem is to construct a minimal but configurable architectural skeleton. This skeleton couples a critical slice of the low level hardware implementation with a microkernel in a way that allows information flow properties of the entire construction to be statically verified all the way down to its gate-level implementation. This strict structure is then made usable by a runtime system that delivers more traditional services (e.g. communication interfaces and long-living contexts) in a way that is decoupled from the information flow properties of the skeleton. To test the viability of this approach we design, test, and statically verify the information-flow security of a hardware/software system complete with support for unbounded operation, inter-process communication, pipelined operation, and I/O with traditional devices. The resulting system is provably sound even when adversaries are allowed to execute arbitrary code on the machine, yet is flexible enough to allow caching, pipelining, and other common case optimizations.","1063-6897;1063-6897","978-1-4503-0472-6978-1-4503-0472-6978-1-4503-0472","10.1145/2000064.2000087","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6307757","High Assurance Systems;Gate Level Information Flow Tracking;Non-interference","Security;Kernel;Logic gates;Hardware;Skeleton;Registers","cache storage;hardware-software codesign;pipeline processing;security of data","microkernel;processor;I-O system;information flow security;high assurance systems;static primitives;dynamic behaviors;architectural skeleton;gate-level implementation;runtime system;skeleton information flow properties;hardware-software system;unbounded operation;interprocess communication;pipelined operation;arbitrary code execution;caching;pipelining","","21","34","","","","","","IEEE","IEEE Conferences"
"Optimal size and location of distributed generations using Differential Evolution (DE)","I. Hussain; A. K. Roy","Electrical Engg. Dept, Royal Group of Institutions, Guwahati, Assam, India; Electrical Engg. Dept National Institute of Technology, (NIT), Silchar, Assam, India","2012 2nd National Conference on Computational Intelligence and Signal Processing (CISP)","","2012","","","57","61","To improve the overall efficiency of power system, the performance of distribution system must be improved. This paper presents a new methodology using Differential Evolution (DE) for the placement of DG units in electrical distribution systems to reduce the power losses and to improve the voltage profile. Unlike the conventional evolutionary algorithms that depend on predefined probability distribution function for mutation process, differential evolution uses the differences of randomly sampled pairs of objective vectors for its mutation process. The Due to the increasing interest on renewable sources in recent times, the studies on integration of distributed generation to the power grid have rapidly increased. The distributed generation (DG) sources are added to the network mainly to reduce the power losses by supplying a net amount of power. In order to minimize the line losses of power systems, it is equally important to define the size and location of local generation. The suggested method is programmed under MATLAB software and is tested on IEEE 33-bus test system and the results are presented. The method is found to be effective and applicable for practical network.","","978-1-4577-0720-9978-1-4577-0719","10.1109/NCCISP.2012.6189708","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6189708","Distributed Generation;Power Loss Reduction;Differential Evolution","Vectors;Optimization;Load flow;Minimization;Evolutionary computation;Distributed power generation;IEEE Press","distributed power generation;distribution networks;evolutionary computation","optimal size;differential evolution;power system;DG units;electrical distribution systems;power losses;voltage profile;conventional evolutionary algorithms;predefined probability distribution function;mutation process;randomly sampled pairs;objective vectors;renewable sources;distributed generation sources;line losses;local generation;MATLAB software;IEEE 33-bus test system","","5","10","","","","","","IEEE","IEEE Conferences"
"Study on depth migration with Arbitrarily Wide-Angle Wave Equation","Q. Sun; Q. Du","College of Computer and Communication Engineering, China University of Petroleum, Dongying, China; College of Geo-Resources and Information, China University of Petroleum, Dongying, China","2011 IEEE 3rd International Conference on Communication Software and Networks","","2011","","","230","233","Based on the research of Arbitrarily Wide-Angle Wave Equation (AWWEs), we present a new migration method implemented using finite difference scheme in frequency-space domain. The result of depth migration imaging test illustrates the advantages of the new method. In contrast with conventional wide-angle wave equations, the second order AWWEs can be adapted to steep dips up to 85° by optimization of the parameters. Wave field extrapolation method is realized on finite difference method, so the proposed method can adapt to vary in velocity. Compared with the realization in the time-space domain, it has advantage in computational efficiency and easy imaging.","","978-1-61284-486-2978-1-61284-485-5978-1-61284-484","10.1109/ICCSN.2011.6014429","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014429","depth migration;finite difference;AWWEs;frequency-space domain","Stability analysis;Gold","geophysical image processing;geophysical techniques;seismic waves;seismology;wave equations","depth migration method;arbitrarily wide angle wave equation;finite difference scheme;frequency-space domain;depth migration imaging;second order AWWE;wave field extrapolation method","","","11","","","","","","IEEE","IEEE Conferences"
"A demand-pull model of support system for spare parts: Based on system dynamics","W. Wang","School of Economics and Management, Weifang University, Weifang, China","2011 IEEE 3rd International Conference on Communication Software and Networks","","2011","","","98","101","The logistics of spare parts shows the characteristics of dynamics. We establish a support system based on demand-pull model. This paper analyses the feedback control mechanism of the system and the mathematical principles of the dynamics of the supply-demand model of spare parts based on system dynamics. Then we build a system state model, analyzing the dynamic actions of the support system. Our study shows that: (1) Excessive logistics delay may affect the stability of the system; (2) The stability and performance of the support system of spare parts based on the ""demand-pull"" model are closely related to the supply chain management system and technical conditions, also depend on the interactions of multiple factors. Then we get the value threshold of the key parameters to make the system stable, and test the impacts of logistics delay on the stability of the system. Finally, through system dynamics simulation, we optimize the system.","","978-1-61284-486-2978-1-61284-485-5978-1-61284-484","10.1109/ICCSN.2011.6014397","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014397","spare parts;system dynamics;organizational level;support system","Analytical models;Oscillators","logistics;maintenance engineering;supply chain management","demand-pull model;spare parts support system;spare parts logistics;feedback control mechanism;supply-demand model;system dynamics;supply chain management system","","","5","","","","","","IEEE","IEEE Conferences"
"Real-time Estimation of Road Friction Coefficient for the Electric Vehicle","L. Cheng; W. Gang; C. Wan-Ke; Z. Feng-Jun","NA; NA; NA; NA","2012 Third World Congress on Software Engineering","","2012","","","172","175","The friction coefficient of road is the primary factor of the traction control system. In this paper, a real-time estimation of road coefficient based on the distributed driven electric vehicle is developed. The simplified quarter car model and the Burckhardt tire model are selected and optimized. The algorithm based on recursive least square with forgetting factor is used for road estimation. The test data under a wide range of road conditions are analyzed. The results show that the algorithm is able to estimate two surfaces: asphalt and ice road. The proposed approach has several advantages such as accurate, effective, short cycle and low cost. Also, it has the ability to provide with reliable information for vehicle active safety control.","","978-1-4673-4546","10.1109/WCSE.2012.41","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6394946","electric vehicle£»distributed driven£» estimation of road coefficient£»Recursive least squares algorithm","Roads;Vehicles;Tires;Adhesives;Ice;Estimation;Friction","electric vehicles;friction;safety;vehicle dynamics","real-time estimation;road friction coefficient;primary factor;traction control system;distributed driven electric vehicle;road estimation;vehicle active safety control","","1","10","","","","","","IEEE","IEEE Conferences"
"Implement and application for Virtual Assembly Platform based on PC","M. Wu; J. Qian","School of Mechanical Engineering, Jiangnan University, Wuxi, China; School of Mechanical Engineering, Jiangnan University, Wuxi, China","2010 IEEE International Conference on Software Engineering and Service Sciences","","2010","","","282","285","With the application of the VR, more and more products are designed and manufactured globally in a distributed, collaborative and virtual environment. The development of Virtual Assembly Platform (VAP) based on PC is discussed in this paper. The virtual assembly simulation scenic framework is set up, and the Level of Detail technology is adopted to optimize the simulation scene. An example of product assembly and test is provided to illustrate the effectiveness of the VAP system. The simulation platform can help shorten the design cycle, improve design quality and reduce the assembly cost.","2327-0586;2327-0594","978-1-4244-6055-7978-1-4244-6054-0978-1-4244-6053","10.1109/ICSESS.2010.5552433","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5552433","Virtual assembly;Assembly process modeling;Virtual reality;Virtual Assembly Platform (VAP)","Assembly;Solid modeling;Object oriented modeling;Virtual reality;Unified modeling language;Laser modes;Planning","assembling;CAD;virtual manufacturing;virtual reality","virtual assembly platform;PC;level of detail technology;virtual reality;product assembly","","","10","","","","","","IEEE","IEEE Conferences"
"AML: A Novel Page Replacement Algorithm for Solid State Disks","H. Zhu; H. Dai; Y. Yan","NA; NA; NA","2010 International Conference on Computational Intelligence and Software Engineering","","2010","","","1","6","Traditional page replacement algorithms such as LRU have been optimized in various ways on the assumption of magnetic disks based storage system. The proposed algorithms for solid state disks based storage system, such as CFLRU and APRA, show better performance than LRU by some tests and experiments. However, these algorithms have several deficiencies, evicting less frequent clean page is too slow, the grain of implementation is coarse, and experiments are not very comprehensive. We suggest a novel algorithm called AML which means Adaptively Mixed List. AML prefers to evict the cold clean page and changes the window size adaptively according to the history of access patterns. We conduct an overall trace-driven simulation. The runtime of AML is reduced at most by 18% compared to APRA, and on average by 14% compared to CFLRU.","","978-1-4244-5391-7978-1-4244-5392","10.1109/CISE.2010.5676718","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676718","","Flash memory;Runtime;Delay;Performance evaluation;History;Adaptation model;Solids","flash memories;magnetic disc storage;paged storage","AML;page replacement algorithm;solid state disks;magnetic disks based storage system;CFLRU;APRA;adaptively mixed list;window size;access patterns;trace-driven simulation","","","12","","","","","","IEEE","IEEE Conferences"
"Patrol Cars Deployment Analysis Based on Modified Greedy Algorithm","J. Zhang; B. Zhao; S. Chen; H. Peng","NA; NA; NA; NA","2010 International Conference on Computational Intelligence and Software Engineering","","2010","","","1","4","The deployment problem of police patrol cars is more complicated than ordinary set cover problem due to the motility of vehicles and the multiple objectives, such as cost, response capability, deterrent capability and so on. This paper puts forward a new model for the solution of the patrol car deployment problem, which can be used to optimize the number of police cars and determine the patrol path. Road discretization treatment is introduced, simplifying the calculations and reducing the solution difficulty. Two criteria are set up in terms of response capability and deterrent capability for the judgment of a patrol scheme. A modified greedy algorithm is proposed to solve the multi-objective model. As the test show, the modified greedy algorithm can stably produce reliable and cost-effective deployment scheme, which is helpful to find an equilibrium point between patrol cost and patrol effect and improve the level of public security.","","978-1-4244-5391-7978-1-4244-5392","10.1109/CISE.2010.5677115","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5677115","","Roads;Greedy algorithms;Heuristic algorithms;Mathematical model;Approximation algorithms;Security;Algorithm design and analysis","automobiles;public administration;road traffic;traffic engineering computing","police patrol car deployment analysis;greedy algorithm;patrol path;road discretization treatment;public security;set cover problem","","1","6","","","","","","IEEE","IEEE Conferences"
"Lower bounds for single machine subproblems occurring in weighted tardiness oriented shifting bottleneck procedures","R. Braune; G. Zäpfel; M. Affenzeller","Institute for Production and Logistics Management, Johannes Kepler University, Linz, Austria; Institute for Production and Logistics Management, Johannes Kepler University, Linz, Austria; Department of Software Engineering, Upper Austria University of Applied Sciences, Hagenberg, Austria","2011 IEEE Symposium on Computational Intelligence in Scheduling (SCIS)","","2011","","","25","32","In this paper, we propose lower bounds for single machine scheduling problems which occur during a run of a shifting bottleneck procedure for total weighted tardiness job shops. The specific structure of this kind of problem and its objective function in particular prevent an immediate transfer or an adaption of existing lower bounds from “conventional” single machine problems with tardiness related objectives. Hence it has been necessary to develop bounding approaches which are to some extent conceptually new. Potential application scenarios range from exact subproblem solution methods or machine prioritization criteria in a shifting bottleneck procedure to branch-and-bound algorithms for job shops with total weighted tardiness objective. In order to provide a significant evaluation of the proposed lower bounds regarding their effectiveness and efficiency, we tested them based on problem instances which actually have been generated in a shifting bottleneck procedure applied to benchmark job shop problems.","","978-1-61284-196-0978-1-61284-195-3978-1-61284-194","10.1109/SCIS.2011.5976548","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5976548","","Schedules;Indexes;Single machine scheduling;Cost function;Processor scheduling;Equations;Complexity theory","integer programming;job shop scheduling;single machine scheduling;tree searching","weighted tardiness oriented shifting bottleneck procedure;single machine scheduling problem;job shops;tardiness related objective;bounding approach;machine prioritization;branch-and-bound algorithm;integer programming","","1","30","","","","","","IEEE","IEEE Conferences"
"Optimal placement of power quality monitors in distribution systems using the topological monitor reach area","A. A. Ibrahim; A. Mohamed; H. Shareef","Department of Electrical, Electronic &amp; Systems Engineering, Faculty of Engineering and Built Environment, Universiti Kebangsaan Malaysia, 43600 Bangi, Selangor, Malaysia; Department of Electrical, Electronic &amp; Systems Engineering, Faculty of Engineering and Built Environment, Universiti Kebangsaan Malaysia, 43600 Bangi, Selangor, Malaysia; Department of Electrical, Electronic &amp; Systems Engineering, Faculty of Engineering and Built Environment, Universiti Kebangsaan Malaysia, 43600 Bangi, Selangor, Malaysia","2011 IEEE International Electric Machines & Drives Conference (IEMDC)","","2011","","","394","399","Installation of power quality monitors (PQMs) at all buses in a power distribution network is uneconomical and it needs to be minimized. This paper presents a PQM positioning technique which finds the optimal number and location of PQMs in power distribution systems for voltage sag detection. The IEEE 34-node and 69 bus test systems were modeled in DIgSILENT software to obtain the topological monitor reach area matrix for various types of faults. Then, an optimization problem is formulated and solved using genetic algorithm to find the minimum number of PQMs in the distribution system which can guarantee the observability of the whole system. Finally, the sag severity index has been used to find the best location to install monitors in the distribution system.","","978-1-4577-0061-3978-1-4577-0060-6978-1-4577-0059","10.1109/IEMDC.2011.5994627","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5994627","power quality monitor (PQM);distribution system;voltage sag detection;topological monitor reach area (TMRA);sag severity index (SSI)","Monitoring;Voltage fluctuations;Voltage measurement;Power quality;Optimization;Genetic algorithms","distribution networks;power supply quality","optimal placement;power quality monitoring;power distribution network;PQM positioning technique;IEEE 34-node;bus test system;genetic algorithm;optimization;voltage sag detection","","4","12","","","","","","IEEE","IEEE Conferences"
"High dynamic range acquisition and shape recognition","S. Ivascu; C. Toma; A. Gräser","Stimme der Hoffnung - Adventist Media Center, Alsbach-Hãhnlein, Germany; Faculty of Electronics and Telecommunications, “Politehnica” University, Timisoara, Romania; Institute of Automation, University of Bremen, Germany","2010 9th International Symposium on Electronics and Telecommunications","","2010","","","357","360","The paper presents different approaches in improving the digital image acquisition with focus on shape recognition. The study is part of a larger project focused on image pre-processing optimization in order to obtain a higher efficiency of automated features recognition. The final target would be to use the results in the field of number plate recognition in road traffic. Structured in two main sections, the work started with several small projects targeted at improving the image acquisition in the process of welding observation. Emphasis is being set on presenting an introduction to the state of the art in shape recognition, and the ultimate intention is the one of improving such techniques. We intend to apply afterwards the results on the practical application mentioned before, namely the number plates recognition in road traffic. Noticeable results have been obtained in the area of improving the high dynamic range at image acquisition. Several techniques have been tested in order to obtain a better dynamic range in the field of welding observation.","","978-1-4244-8460-7978-1-4244-8457-7978-1-4244-8459","10.1109/ISETC.2010.5679352","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5679352","image processing;high dynamic range;shape recognition;automatic number plate recognition","Cameras;Welding;Dynamic range;Licenses;Optical character recognition software;Shape;Roads","image recognition;shape recognition;traffic engineering computing","digital image acquisition;shape recognition;optimization;features recognition;number plate recognition;road traffic","","","16","","","","","","IEEE","IEEE Conferences"
"ENOSYS FP7 EU project: An integrated modeling and synthesis flow for embedded systems design","E. Brosse; I. R. Quadri; A. Sadovykh; F. Ieromnimon; D. Kritharidis; R. Catrou; M. Sarlotte","Softeam, France; Softeam, France; Softeam, France; Intracom Telecom, Greece; Intracom Telecom, Greece; Thales Communication and Security, France; Thales Communication and Security, France","7th International Workshop on Reconfigurable and Communication-Centric Systems-on-Chip (ReCoSoC)","","2012","","","1","5","The ENOSYS project, funded by the EC, aims to shorten the time-to-market of high-performance SoCs by providing design and tool flows for the design and the implementation of embedded systems by seamless integration of high-level system specifications, software code generation, hardware synthesis, code optimization and design space exploration. The objective here is to automatically generate code, for implementation in execution platforms such as FPGAs, from validated high level designs incorporating initial end user requirements, system functional and non-functional description, platform modelling. This paper describes the design flow developed in the context of the ENOSYS project and the results of a first industrial evaluation through two industrial test cases.","","978-1-4673-2572-1978-1-4673-2570-7978-1-4673-2571","10.1109/ReCoSoC.2012.6322880","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6322880","Real-Time Embedded Systems Modeling;UML;MARTE;Synthesis;Design space exploration","Unified modeling language;Hardware;Optimization;Embedded systems;Space exploration;Transform coding","embedded systems;field programmable gate arrays;logic design;program compilers;system-on-chip","ENOSYS FP7 EU project;integrated modeling;synthesis flow;embedded systems design;time-to-market;system-on-chip;SoC;high-level system;software code generation;hardware synthesis;code optimization;design space exploration;FPGA","","","6","","","","","","IEEE","IEEE Conferences"
"Solis'Curse - A Cultural Heritage Game Using Voice Interaction with a Virtual Agent","J. N. Neto; R. Silva; J. P. Neto; J. M. Pereira; J. Fernandes","NA; NA; NA; NA; NA","2011 Third International Conference on Games and Virtual Worlds for Serious Applications","","2011","","","164","167","This paper presents a game with the aim of developing persistent knowledge inside a Cultural Heritage exhibition. The authors developed a game called Solis Curse, that was used as a way of testing the knowledge acquired during the museum visit. Similar to an entertainment quiz, the game presents a set of questions with progressional levels of difficulty. Through the analysis of the responses and some others qualitative and quantitative metrics such, as the difficulty level and time, the players score is calculated and compared with a global ranking ladder. Beside putting to test the visitors knowledge, the game also offers the possibility of speech interaction with a embodied conversational agent that steers the game and helps the user to successfully complete the task at hand. Based on interdisciplinary multimedia technologies and cultural contents, Solis Curse supports an entertainment platform for a younger audience, which may be regarded as a mean to convey cultural heritage contents, conducted by the motto of learning by playing. The results of this research point out the benefits of having game as an aid to the learning process and also shows the impact of multi-modal interaction on the user immersion.","","978-1-4577-0316-4978-0-7695-4419","10.1109/VS-GAMES.2011.31","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5962081","","Games;Speech recognition;Cultural differences;Speech;Three dimensional displays;Engines;Facial animation","computer games;history;software agents;virtual reality","Solis curse;cultural heritage game;voice interaction;virtual agent;cultural heritage exhibition;entertainment quiz;quantitative metrics;qualitative metrics;global ranking ladder;visitors knowledge;multimedia technologies;cultural contents;entertainment platform;younger audience","","3","13","","","","","","IEEE","IEEE Conferences"
"FLEXOR: User friendly wireless sensor network development and deployment","A. Förster; K. Garg; D. Puccinelli; S. Giordano","Networking Laboratory, University of Applied Sciences of Southern Switzerland; Networking Laboratory, University of Applied Sciences of Southern Switzerland; Networking Laboratory, University of Applied Sciences of Southern Switzerland; Networking Laboratory, University of Applied Sciences of Southern Switzerland","2012 IEEE International Symposium on a World of Wireless, Mobile and Multimedia Networks (WoWMoM)","","2012","","","1","9","Wireless sensor networks (WSNs) penetrated the market mainly as solutions for specific application scenarios. However, this strong specialization limits WSNs reuse both in terms of development as well as in terms of technical results: Every new application scenario requires a new design, development and validation, as well as management skills. This is frustrating for any WSNs user, developer or manager. To reverse this tendency and thus improve the quality of experience and user-friendliness in WSNs, we designed FLEXOR, a sustainable software architecture optimized to support the implementation, rapid prototyping, evaluation, and testing of wireless sensor network applications, that is platform independent and user-friendly. FLEXOR is designed to accommodate many different applications and services for wireless sensor networks and foster code re-usability and cross-platform component re-usability. FLEXOR offers high modularity, well defined interfaces, remote node management functionality as well as run-time module exchange. Finally, the introduction of a unifying way for WSNs development opens to a higher homogeneity and thus to more easy comparison among different solutions. We present here an analysis of FLEXOR from these new angles and show how effective it is for several purposes and in particular for non-experts and in education.","","978-1-4673-1239-4978-1-4673-1238-7978-1-4673-1237","10.1109/WoWMoM.2012.6263698","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6263698","QoE;User-friendliness;Wireless Sensor Networks;Architecture;Sustainable reuse;Mobility","Wireless sensor networks;Protocols;Computer architecture;Programming;Hardware;Routing;Software architecture","sensor placement;telecommunication computing;wireless sensor networks","FLEXOR;user-friendly wireless sensor network development;management skills;quality of experience;sustainable software architecture;rapid prototyping;WSN testing;WSN evaluation;foster code reusability;cross-platform component reusability;remote node management functionality;run-time module exchange","","4","21","","","","","","IEEE","IEEE Conferences"
"A GPU based implementation of Center-Surround Distribution Distance for feature extraction and matching","A. Rathi; M. DeBole; W. Ge; R. T. Collins; N. Vijaykrishnan","The Department of Computer Science and Engineering, The Pennsylvania State University, University Park, 16802, USA; The Department of Computer Science and Engineering, The Pennsylvania State University, University Park, 16802, USA; The Department of Computer Science and Engineering, The Pennsylvania State University, University Park, 16802, USA; The Department of Computer Science and Engineering, The Pennsylvania State University, University Park, 16802, USA; The Department of Computer Science and Engineering, The Pennsylvania State University, University Park, 16802, USA","2010 Design, Automation & Test in Europe Conference & Exhibition (DATE 2010)","","2010","","","172","177","The release of general purpose GPU programming environments has garnered universal access to computing performance that was once only available to super-computers. The availability of such computational power has fostered the creation and re-deployment of algorithms, new and old, creating entirely new classes of applications. In this paper, a GPU implementation of the Center-Surround Distribution Distance (CSDD) algorithm for detecting features within images and video is presented. While an optimized CPU implementation requires anywhere from several seconds to tens of minutes to perform analysis of an image, the GPU based approach has the potential to improve upon this by up to 28X, with no loss in accuracy.","1558-1101;1530-1591;1530-1591","978-3-9810801-6-2978-1-4244-7054-9978-3-9810801-6","10.1109/DATE.2010.5457215","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5457215","","Feature extraction;Filtering;Computer vision;Application software;Central Processing Unit;Programming environments;Distributed computing;Power engineering computing;Image processing;Computer architecture","coprocessors;feature extraction","center-surround distribution distance algorithm;feature extraction;general purpose GPU programming environments;supercomputers;computational power;graphical processing unit","","","12","","","","","","IEEE","IEEE Conferences"
"MedlinePlus Mobile: Consumer Health Information On-the-Go","W. Ma; B. Goldstein; Y. Ma; D. Sun; S. Burgess","US National Library of Medicine; US National Library of Medicine; US National Library of Medicine; US National Library of Medicine; US National Library of Medicine","IT Professional","","2012","14","3","44","49","MedlinePlus Mobile provides a core set of the authoritative health information found on MedlinePlus.gov, optimized for mobile devices. The authors detail the technical approach taken to create this website, from the system architecture through development and testing.","1520-9202;1941-045X","","10.1109/MITP.2012.20","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6143912","emerging technologies;computer systems organization;applicative (functional) programming;programming techniques;software/software engineering;affective computing;information technology;health IT;MedlinePlus Mobile;mobile computing","Mobile communication;Mobile handsets;Medical services;Internet;Biomedical computing;Information systems;Medical information systems","health care;medical computing;mobile computing","MedlinePlus mobile;consumer health information on-the-go;authoritative health information;mobile devices","","1","1","","","","","","IEEE","IEEE Journals & Magazines"
"High temperature proton exchange membrane fuel cell hybrid power system monitoring platform designing","Bo Zhang; Zhixing Cao; Xiuliang Li; Hongye Su; Xuelan Chen","State Key Laboratory of Industrial Control Technology, Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, 310027, China; State Key Laboratory of Industrial Control Technology, Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, 310027, China; State Key Laboratory of Industrial Control Technology, Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, 310027, China; State Key Laboratory of Industrial Control Technology, Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, 310027, China; State Key Laboratory of Industrial Control Technology, Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, 310027, China","International Conference on Automatic Control and Artificial Intelligence (ACAI 2012)","","2012","","","1129","1132","The design of 5kw high temperature proton exchange membrane fuel cell (HTPEMFC) hybrid power system monitoring platform is presented in this paper. The purpose of the system is to overcome the problems of low temperature proton exchange membrane fuel cell (LTPEMFC), such as poor water management and requirement of high-purity hydrogen. The present computerized real-time monitoring system is based on NI's PXIe hardware system and Labview software. The platform has two working modes - test mode and simulation mode. In the test mode, it provides the conditions for studying the problems of fuel cell hybrid power system, such as system performance, modelling and optimization, by completely monitoring all the working parameters in real-time. And in the simulation mode, besides the problems mentioned above, those with feeding control under changing loads, stability of output power and optimization of energy efficiency, can also be studied with multi-energy management system.","","978-1-84919-537","10.1049/cp.2012.1177","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6492784","High Temperature Proton Exchange Membrane Fuel Cell (HTPEMFC);Monitoring;System optimization;Multi-energy management","","","","","","","","","","","","IET","IET Conferences"
"Electric vehicle platform for drivability analysis","B. Eller; J. F. Hetet; S. André; G. Hennequet","Energy Synthesis and Storage Group of Renault SA, 78228 Guyancourt, France; Ecole Centrale de Nantes, Laboratoire de Mécanique des Fluides - UMR CNRS 6598, 44321 Nantes Cedex 3, France; Energy Synthesis and Storage Group of Renault SA, 78228 Guyancourt, France; Energy Synthesis and Storage Group of Renault SA, 78228 Guyancourt, France","IEEE ICCA 2010","","2010","","","2251","2257","In order to lead properly an electric vehicle project, it is very important to assess and optimize the driver perception during manoeuvres such as tip-in and tip-out. This aspect of vehicle customer perception is called “drivability”. To meet these objectives, a co-simulation platform has been developed between models under LMS-AMESim<sup>®</sup>and Matlab/Simulink<sup>®</sup>. The first model represents the physical characteristics of the vehicle with LMS-AMESim<sup>®</sup>and takes into account stiffness of the driveline, motor block behaviour onto its mounting blocks, suspensions, tires and vehicle dynamics. The second model computes the torque command of the electric motor for drivability manoeuvres as well as for driving cycles with Matlab/Simulink<sup>®</sup>. This paper shows how this platform can help integration teams to tune software and hardware parameters from the early steps of the project's design. Concerning the software, any torque setpoint strategy can be integrated in order to filter the shocks amplitude during tip-in. The impact of an RST filter on the vehicle response is studied; moreover, this study shows that the vehicle range can be added to classical optimization criteria such as time response and overshoot. For the hardware side of this platform, the very good correlation of the model's acceleration with track tests gives a high level of trust in the sensitivity study of most influents parameters on the drivability rating, which is assessed by some objectives criteria on vehicle acceleration.","1948-3457;1948-3449;1948-3449","978-1-4244-5195-1978-1-4244-5196","10.1109/ICCA.2010.5524381","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5524381","","Electric vehicles;Mathematical model;Vehicle driving;Torque;Hardware;Filters;Life estimation;Suspensions;Tires;Vehicle dynamics","electric motors;electric vehicles;optimisation;road vehicles;vehicle dynamics","electric vehicle;vehicle customer perception;drivability analysis;LMS-AMESim;Matlab;Simulink;electric motor torque command;torque setpoint strategy;RST filter;optimization criteria;vehicle acceleration","","4","9","","","","","","IEEE","IEEE Conferences"
"Exploring Distributional Discrepancy for Multidimensional Point Set Retrieval","J. Shao; H. T. Shen; Z. Huang; X. Zhou","Department of Computer Science and Software Engineering, The University of Melbourne, Parkville, Australia; School of Information Technology and Electrical Engineering, The University of Queensland, St Lucia, Australia; School of Information Technology and Electrical Engineering, The University of Queensland, St Lucia, Australia; School of Information Technology and Electrical Engineering, The University of Queensland, St Lucia, Australia","IEEE Transactions on Multimedia","","2011","13","1","71","81","How to effectively and efficiently assess similarity is a long-standing and challenging research problem in various multimedia applications. For ranked retrieval in a collection of objects based on series of multivariate observations (e.g., searching similar video clips to a query example), satisfactory performance cannot be achieved by using many conventional similarity measures that aggregate element-to-element comparison results. Some correlation information among the individual elements has also been investigated to characterize each set of multidimensional points for comparison, but with an unwarranted assumption that the underlying data distribution has a particular parametric form. Motivated by these concerns, measuring the similarity of multidimensional point sets is approached from a novel collective perspective in this paper, by evaluating the probability that they are consistent with a same distribution. We propose to make use of nonparametric hypothesis tests in statistics to compute the distributional discrepancy of samples for assessing the degree of similarity between two ensembles of points. While our proposal is mainly presented in the context of video similarity search, it enjoys great flexibility and is extensible to other applications where multidimensional point set representations are involved, such as motion capture retrieval.","1520-9210;1941-0077","","10.1109/TMM.2010.2085424","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5597949","Distributional discrepancy;hypothesis tests;multi dimensional point set;nonparametric;similarity measures","Principal component analysis;Australia;Proposals;Correlation;Materials;Electrical engineering;Scattering","motion estimation;multimedia computing;set theory;video retrieval","exploring distributional discrepancy;multidimensional point set retrieval;multimedia applications;multivariate observations;video clips;element-to-element comparison;correlation information;unwarranted assumption;data distribution;video similarity search;motion capture retrieval","","2","36","","","","","","IEEE","IEEE Journals & Magazines"
"Learning equivariant structured output SVM regressors","A. Vedaldi; M. Blaschko; A. Zisserman","Department of Engineering Science, University of Oxford, UK; Department of Engineering Science, University of Oxford, UK; Department of Engineering Science, University of Oxford, UK","2011 International Conference on Computer Vision","","2011","","","959","966","Equivariance and invariance are often desired properties of a computer vision system. However, currently available strategies generally rely on virtual sampling, leaving open the question of how many samples are necessary, on the use of invariant feature representations, which can mistakenly discard information relevant to the vision task, or on the use of latent variable models, which result in non-convex training and expensive inference at test time. We propose here a generalization of structured output SVM regressors that can incorporate equivariance and invariance into a convex training procedure, enabling the incorporation of large families of transformations, while maintaining optimality and tractability. Importantly, test time inference does not require the estimation of latent variables, resulting in highly efficient objective functions. This results in a natural formulation for treating equivariance and invariance that is easily implemented as an adaptation of off-the-shelf optimization software, obviating the need for ad hoc sampling strategies. Theoretical results relating to vicinal risk, and experiments on challenging aerial car and pedestrian detection tasks show the effectiveness of the proposed solution.","2380-7504;1550-5499;1550-5499","978-1-4577-1102-2978-1-4577-1101-5978-1-4577-1100","10.1109/ICCV.2011.6126339","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6126339","","Support vector machines;Kernel;Detectors;Training;Object detection;Feature extraction;Context","computer vision;regression analysis;support vector machines","equivariant structured output SVM regressor;computer vision system;virtual sampling;invariant feature representation;nonconvex training;expensive inference;convex training procedure;optimality;tractability;test time inference;equivariance;invariance;off-the-shelf optimization software;ad hoc sampling strategy;aerial car;pedestrian detection task","","14","33","","","","","","IEEE","IEEE Conferences"
"Object localization using RFID","K. Chawla; G. Robins; L. Zhang","Department of Computer Science, University of Virginia, Charlottesville, Virginia, 22904, USA; Department of Computer Science, University of Virginia, Charlottesville, Virginia, 22904, USA; Department of Computer Science, University of Virginia, Charlottesville, Virginia, 22904, USA","IEEE 5th International Symposium on Wireless Pervasive Computing 2010","","2010","","","301","306","Object localization is a key primitive in pervasive computing environments, where numerous applications depend on the rapid and accurate position estimation of objects. We present a general RFID-based localization framework that reliably determines the positions of objects with unprecedented accuracy and speed. This is achieved by varying the power levels of the RFID readers, calibrated against reference tags of known sensitivity. Our implementation and experiments are able to localize objects to an accuracy of 15 cm within a few seconds, and this compares favorably with previous techniques. We also suggest several practical optimizations for further enhancing the speed and accuracy of the method.","","978-1-4244-6858-4978-1-4244-6855-3978-1-4244-6856-0978-1-4244-6857","10.1109/ISWPC.2010.5483750","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5483750","RFID;localization;positioning algorithms","Radiofrequency identification;Pervasive computing;Working environment noise;Testing;Heuristic algorithms;Computer science;Application software;Optimization methods;Heart;Radiofrequency interference","radiofrequency identification;ubiquitous computing","object localization;general RFID-based localization framework;pervasive computing environments;radio frequency identification technology","","14","16","","","","","","IEEE","IEEE Conferences"
"Optimizing Warehouse Forklift Dispatching Using a Sensor Network and Stochastic Learning","R. M. Estanjini; Y. Lin; K. Li; D. Guo; I. C. Paschalidis","Center for Information &amp; Systems Eng., Dept. of Electrical &amp; Computer Eng., and Division of Systems Eng., Boston University, Boston; Center for Information &amp; Systems Eng., Dept. of Electrical &amp; Computer Eng., and Division of Systems Eng., Boston University, Boston; Center for Information &amp; Systems Eng., Dept. of Electrical &amp; Computer Eng., and Division of Systems Eng., Boston University, Boston; Center for Information &amp; Systems Eng., Dept. of Electrical &amp; Computer Eng., and Division of Systems Eng., Boston University, Boston; Center for Information &amp; Systems Eng., Dept. of Electrical &amp; Computer Eng., and Division of Systems Eng., Boston University, Boston","IEEE Transactions on Industrial Informatics","","2011","7","3","476","486","The authors report on a successful deployment of an inexpensive mobile wireless sensor network in a commercial warehouse served by a fleet of forklifts. The aim is to improve forklift dispatching and reduce the costs associated with the delays of loading/unloading delivery trucks. To that end, an integrated system including both hardware and software is constructed. First, the forklifts are instrumented with sensor nodes that collect an array of information, including the forklifts' physical location, usage time, bumping/collision history, and battery status. The hardware's capability is enhanced with a theoretically sound hypothesis testing technique to capture the rather elusive location information, and the collection of the data is done in an efficient event-driven manner. The information acquired combined with inventory information is fed into a sophisticated actor-critic type stochastic learning method to generate dispatching recommendations. Because noise is inevitable in such wireless sensor networks, the performance of the algorithm is investigated under different noise levels. In combining wireless sensing with state-of-the-art decision theory, this work extends beyond the standard use of wireless sensor networks as monitoring devices.","1551-3203;1941-0050","","10.1109/TII.2011.2158834","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5959999","Decision support systems;dispatching;event driven sensing;industrial mornitoring;intelligent systems;statistical learning;wireless sensor networks","Batteries;Servers;Dispatching;Monitoring;Wireless sensor networks;Markov processes;Engines","computerised instrumentation;dispatching;fork lift trucks;learning (artificial intelligence);mobile computing;production engineering computing;stochastic processes;warehouse automation;wireless sensor networks","warehouse forklift dispatching;mobile wireless sensor network;actor-critic type stochastic learning method;delivery truck loading;delivery truck unloading;forklifts physical location;usage time;bumping history;collision history;battery status;sound hypothesis testing technique;dispatching recommendations","","10","48","","","","","","IEEE","IEEE Journals & Magazines"
"Complexity Modeling of the Motion Compensation Process of the H.264/AVC Video Coding Standard","M. Semsarzadeh; M. J. Langroodi; M. R. Hashemi; S. Shirmohammadi","NA; NA; NA; NA","2012 IEEE International Conference on Multimedia and Expo","","2012","","","925","930","With recent advances in computing and communication technologies, ubiquitous access to high quality multimedia content such as high definition video using smart phones, Net books, or tablets is a fact of our daily life. However, power is still a major concern for any mobile device, and requires optimization of power consumption using a power model for each multimedia application, such as a video decoder. In this paper, a generic decoding complexity model for the motion compensation (MC) process, which constitutes up to 25% of the computational complexity and hence power consumption of an H.264/AVC decoder, has been proposed. For the model to remain independent from a specific implementation or platform, it has been developed by analysing the MC algorithm as described in the standard. Simulation results indicate that the proposed model estimates MC complexity with an average accuracy of 95.63%, for a wide range of test sequences using both JM and x.264 software implementations of H.264/AVC. For a dedicated hardware implementation of the MC module the modeling accuracy is around 89.61%, according to our simulation results. It should be noted that in addition to power consumption control, the proposed model can be used for designing a receiver-aware H.264/AVC encoder, where the complexity constraints of the receiver side are taken into account during compression.","1945-788X;1945-7871;1945-7871","978-1-4673-1659-0978-0-7695-4711","10.1109/ICME.2012.91","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6298521","Decoder complexity modeling;H.264/AVC decoding;motion compensation","Complexity theory;Decoding;Interpolation;Computational modeling;Video coding;Estimation error;Software","computational complexity;data compression;motion compensation;power consumption;telecommunication power supplies;video coding","motion compensation process;H.264-AVC video coding standard;computing technology;communication technology;ubiquitous access;high-quality multimedia content;high-definition video;smart phones;net books;tablets;mobile device;power consumption optimization;power model;video decoder;generic decoding complexity model;MC process;computational complexity;H.264-AVC decoder;MC complexity estimation;JM implementation;x.264 software implementation;power consumption control;receiver-aware H.264-AVC encoder;data compression","","1","15","","","","","","IEEE","IEEE Conferences"
"Implementation and performance analysis of advanced IT services based on open source JAIN SLEE","M. Femminella; R. Francescangeli; E. Maccherani; L. Monacelli","Dipartimento di Ingegneria Informatica ed Elettronica - DIEI, Universit&#x00E0; degli Studi di Perugia, Perugia, Italy; Dipartimento di Ingegneria Informatica ed Elettronica - DIEI, Universit&#x00E0; degli Studi di Perugia, Perugia, Italy; Dipartimento di Ingegneria Informatica ed Elettronica - DIEI, Universit&#x00E0; degli Studi di Perugia, Perugia, Italy; Dipartimento di Ingegneria Informatica ed Elettronica - DIEI, Universit&#x00E0; degli Studi di Perugia, Perugia, Italy","2011 IEEE 36th Conference on Local Computer Networks","","2011","","","746","753","The standard framework proposed by the JAIN SLEE specifications is emerging as a viable alternative for the development of advanced telecom services. The Mobicents JSLEE platform is the only open source implementation of those specifications. Its tuning is not trivial since it relies on several Java technologies implemented in multiple abstract layers. In this paper, we present the implementation of two realistic benchmark services and the relevant test campaign for the performance analysis of the Mobicents JAIN SLEE platform, with the aim of optimizing the achievable system throughput, for using it in a carrier-grade environment.","0742-1303;0742-1303","978-1-61284-928-7978-1-61284-926-3978-1-61284-927","10.1109/LCN.2011.6115544","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6115544","JAIN SLEE;SIP;Java;Transactions;MGCP;scalability analysis","Databases;Servers;Java;Protocols;Media;Containers;Telecommunications","formal specification;Java;public domain software;software performance evaluation;telecommunication services","performance analysis;advanced IT services;open source JAIN SLEE;JAIN SLEE specifications;advanced telecom services;Mobicents JSLEE platform;open source implementation;Java technologies;multiple abstract layers;realistic benchmark services;Mobicents JAIN SLEE platform;carrier-grade environment;service logic execution environment","","2","27","","","","","","IEEE","IEEE Conferences"
"A method proposed for training an artificial neural network used for industrial robot programming by demonstration","M. Stoica; G. A. Calangiu; F. Sisak; I. Sarkany","&#x201C;Transilvania&#x201D; University of Brasov, Automatics Department, Romania; &#x201C;Transilvania&#x201D; University of Brasov, Automatics Department, Romania; &#x201C;Transilvania&#x201D; University of Brasov, Automatics Department, Romania; &#x201C;Transilvania&#x201D; University of Brasov, Automatics Department, Romania","2010 12th International Conference on Optimization of Electrical and Electronic Equipment","","2010","","","831","836","Robot programming by demonstration has become a central topic in the field of robotics. Artificial neural networks play an important role in this type of robot programming. Artificial neural networks have a great disadvantage: the network must to be trained with a huge number of data in order to achieve good results. In our case (industrial robot programming by demonstration), it is necessary to train the neural network in one single step, when the robot is trained with some data. In this paper we propose a method for artificial neural network training, which works in these conditions. The main idea of this method is to train the artificial neural network with all of the data, before the current training step. At a certain step the network is already trained a huge number of times. A software application was designed for testing the method. This software application implements the training method on a unidirectional multi-layer neural network, using back propagation error algorithm. The results obtained using the software application are also presented.","1842-0133;1842-0133","978-1-4244-7020-4978-1-4244-7019-8978-1-4244-7018","10.1109/OPTIM.2010.5510463","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5510463","","Industrial training;Artificial neural networks;Robot programming;Mobile robots;Robot sensing systems;Neural networks;Intelligent robots;Welding;Motion planning;Motion control","backpropagation;industrial robots;neural nets;robot programming","artificial neural network training;industrial robot programming;back propagation error algorithm;robot programming by demonstration","","4","9","","","","","","IEEE","IEEE Conferences"
"Reliability cost allocation of protective devices using binary programming","F. Darabi; A. T. Shooshtari; E. Babaei; S. Darabi","Dept. Elec. Eng. Sharif, University of Technology, Tehran, Iran; Department of Electrical Engineering, Science and Research Branch, Islamic Azad University, Ahwaz, Iran; Dept. Elec. Eng. Saveh Brach, Islamic Azad University, Iran; Dept. Elec. Eng. K.N. Toosi, University of Technology, Tehran, Iran","2011 IEEE Symposium on Business, Engineering and Industrial Applications (ISBEIA)","","2011","","","469","474","The problem of location of protective devices is important in distribution networks so the various methods were considered for installation of fuse, recloser, disconnecting switch and breaker. In these methods, a set of protection, economic and reliability aspects are considered. Because of various goals, the optimal approach from some intelligent methods such as genetic algorithm does not get the best approach. At this condition, using mathematic methods is reasonable because they satisfy the best and the optimal approach. Binary programming method is one of these mathematic optimization methods that its speed is high. The problem of location of protective devices is naturally binary in distribution networks. This specification causes using binary programming method for optimal location of protective devices. In this analysis, the binary programming method is implemented for optimal location of protective devices installation in distribution networks. The optimization objective function includes the switching procedure cost and protective devices lifetime cost. Furthermore, the SAIFI and SAIDI analyses beside optimal approaches are calculated. The programmer with reliability consideration easily can choose the optimal possible approach. The proposed method make easy to obtain the best approach with protection consideration and etc. To analyze proposed method, a test network is considered. At a distribution network, the breaker is located in the beginning of the network so the simulation result can be extended to obtain the optimal location for recloser, fuse and disconnecting switch. The proposed method is obtained from Visual Studio C# .NET software.","","978-1-4577-1549-5978-1-4577-1548-8978-1-4577-1547","10.1109/ISBEIA.2011.6088860","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6088860","recloser;fuse;disconnecting switch;binary programming;Visual Studio C#","Switches;Reliability;Indexes;Programming;Fuses;Power system reliability;Visualization","circuit breakers;electric fuses;genetic algorithms;power distribution economics;power distribution protection;power distribution reliability","reliability cost allocation;protective device location;distribution network;fuse installation;recloser installation;disconnecting switch installation;disconnecting breaker installation;genetic algorithm;binary programming method;mathematic optimization method;optimization objective function;switching procedure cost;protective device lifetime cost;SAIDI analyses;SAIFI analyses;Visual Studio C# .NET software","","1","8","","","","","","IEEE","IEEE Conferences"
"Optimizing and Post Processing of a Smart Beamformer for Obstacle Retrieval","A. Lay-Ekuakille; P. Vergallo; D. Saracino; A. Trotta","Dipartimento d'Ingegneria dell'Innovazione, University of Salento, Lecce, Italy; Dipartimento d'Ingegneria dell'Innovazione, University of Salento, Lecce, Italy; Dipartimento d'Ingegneria dell'Innovazione, University of Salento, Lecce, Italy; Dipartimento di Elettrotecnica ed Elettronica, Polytechinc of Bari, Bari, Italy","IEEE Sensors Journal","","2012","12","5","1294","1299","Beamforming is one of the most interesting techniques used to know distance systems in order to detect punctual, widespread obstacles. If correctly associated to DOA (Difference of Arrival), it can allow the description of obstacle shape. Distance ranging, for mobile and fixed systems, namely cars, vehicles, vessels and airplanes, that is a key issue for demands of nowadays. Distance between cars and from obstacles can be established and measured using laser and ultrasound. Cloudy and foggy conditions are very important requirements for testing distance ranging facilities. If based on acoustic waves, they can be easily integrated by sophisticated on-board software in order to perform new features. This research presents interesting aspects of defining new requirements for an acoustic scanning capable of reconstructing fixed obstacle features by targeting them using a special array of sensors. The term “acoustic scanning” is intended here as an aspect of sound ranging and reproduction regarding spatial locations of the obstacle, that is spatial shaping. The paper illustrates first an experimental system from which it is possible to derive parameters for setting spatial shaping of scenarios and after a clear identification of DOAs.","1530-437X;1558-1748;2379-9153","","10.1109/JSEN.2011.2169782","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6029957","Beamforming;ultrasonic obstacle detection;DOA;distance measurement;array of sensors","Sensor arrays;Shape;Multiple signal classification;Array signal processing;Noise","acoustic signal processing;array signal processing;collision avoidance;distance measurement;ultrasonic applications","smart beamformer optimization;smart beamformer post processing;obstacle retrieval;widespread obstacle;DOA;difference of arrival;obstacle shape;distance ranging;acoustic wave;acoustic scanning;sound ranging","","19","21","","","","","","IEEE","IEEE Journals & Magazines"
"Computational and Experimental Study of a Coaxial Pipe Flow Generator","S. Wang; R. Doblado","NA; NA","2012 International Symposium on Computer, Consumer and Control","","2012","","","435","439","This study presents the research and develop of a Coaxial Pipe Flow Generator. Modern engineering tools such as Computational Fluid Dynamics (CFD) software and Rapid Prototyping (RP) technology were utilized to facilitate the numerical and experimental studies. The CFD numerical simulations consisted of three-dimensional steady state analyses. The simulations were performed to optimize blade shapes within the geometrical range. Based on the results from CAD simulations, different types of impellers were fabricated to test different turbine parameters. Rotational speed of the turbine and electrical voltages were measured for each turbine design. The study leads to several important findings for better pipe flow generator design.","","978-1-4673-0767-3978-0-7695-4655","10.1109/IS3C.2012.116","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6228339","Pipe Flow Generator;Impeller;CFD;RP;Flow Metering","Blades;Turbines;Generators;Coils;Torque;Stators;Fluids","AC machines;blades;computational fluid dynamics;flow measurement;hydraulic turbines;impellers;pipe flow;rapid prototyping (industrial);testing","coaxial pipe flow generator;computational fluid dynamics;rapid prototyping technology;blade shapes;turbine parameter","","1","13","","","","","","IEEE","IEEE Conferences"
"Quality benchmarking relational databases and Lucene in the TREC4 adhoc task environment","A. Arslan; O. Yilmazel","Anadolu University, Computer Engineering Department, Eskisehir, Turkey; Anadolu University, Computer Engineering Department, Eskisehir, Turkey","Proceedings of the International Multiconference on Computer Science and Information Technology","","2010","","","365","372","The present work covers a comparison of the text retrieval qualities of open source relational databases and Lucene, which is a full text search engine library, over English documents. TREC-4 adhoc task is completed to compare both search effectiveness and search efficiency. Two relational database management systems and four different well-known English stemming algorithms have been tried. It has been found that language specific preprocessing improves retrieval quality for all systems. The results of the English text retrieval experiments by using Lucene are at par with top six results presented at TREC-4 automatic adhoc. Although open source relational databases integrated full text retrieval technology, their relevancy ranking mechanisms are not as good as Lucene's.","2157-5533;2157-5525;2157-5525","978-83-60810-27-9978-1-4244-6432-6978-83-60810-27","10.1109/IMCSIT.2010.5679643","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5679643","","Relational databases;Indexes;Libraries;Natural languages;Search engines","benchmark testing;database management systems;information retrieval;public domain software;relational databases;search engines;text analysis","quality benchmarking relational database;TREC4 adhoc task environment;text retrieval quality;open source relational databases;Lucene;search engine library;english document;relational database management system;english stemming algorithm;language specific preprocessing;retrieval quality;english text retrieval","","","20","","","","","","IEEE","IEEE Conferences"
"Task scheduling for multi-electro-magnetic detection satellite with a combined algorithm","J. Zhu; L. Zhang; D. Qiu; H. Li","Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha 410073, P. R. China; Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha 410073, P. R. China; Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha 410073, P. R. China; Northern Electronic Equipment Institute, Beijing 100191, P. R. China","Journal of Systems Engineering and Electronics","","2012","23","1","88","98","Task scheduling for electro-magnetic detection satellite is a typical combinatorial optimization problem. The count of constraints that need to be taken into account is of large scale. An algorithm combined integer programming with constraint programming is presented. This algorithm is deployed in this problem through two steps. The first step is to decompose the original problem into master and sub-problem using the logic-based Benders decomposition; then a circus combines master and sub-problem solving process together, and the connection between them is general Benders cut. This hybrid algorithm is tested by a set of derived experiments. The result is compared with corresponding outcomes generated by the strength Pareto evolutionary algorithm and the pure constraint programming solver — GECODE, which is an open source software. These tests and comparisons yield promising effect.","1004-4132","","10.1109/JSEE.2012.00012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6161700","task scheduling;combined algorithm;logic-based;Benders decomposition;combinatorial optimization;constraint programming (CP)","Electromagnetic measurements;Mathematical model;Job shop scheduling;Algorithm design and analysis;Constraint optimization;Combinatorial mathematics","","","","","","","","","","","BIAI","BIAI Journals & Magazines"
"Multiway Spectral Clustering with Out-of-Sample Extensions through Weighted Kernel PCA","C. Alzate; J. A. K. Suykens","Katholieke Universiteit Leuven, Leuven; Katholieke Universiteit Leuven, Leuven","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2010","32","2","335","347","A new formulation for multiway spectral clustering is proposed. This method corresponds to a weighted kernel principal component analysis (PCA) approach based on primal-dual least-squares support vector machine (LS-SVM) formulations. The formulation allows the extension to out-of-sample points. In this way, the proposed clustering model can be trained, validated, and tested. The clustering information is contained on the eigendecomposition of a modified similarity matrix derived from the data. This eigenvalue problem corresponds to the dual solution of a primal optimization problem formulated in a high-dimensional feature space. A model selection criterion called the balanced line fit (BLF) is also proposed. This criterion is based on the out-of-sample extension and exploits the structure of the eigenvectors and the corresponding projections when the clusters are well formed. The BLF criterion can be used to obtain clustering parameters in a learning framework. Experimental results with difficult toy problems and image segmentation show improved performance in terms of generalization to new samples and computation times.","0162-8828;2160-9292;1939-3539","","10.1109/TPAMI.2008.292","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4711055","Spectral clustering;kernel principal component analysis;out-of-sample extensions;model selection.;Spectral Clustering;Kernel Principal Component Analysis;Out-of-Sample Extension;Model Selection","Kernel;Principal component analysis;Eigenvalues and eigenfunctions;Clustering algorithms;Support vector machines;Testing;Image segmentation;Clustering methods;Couplings;Application software","eigenvalues and eigenfunctions;least mean squares methods;matrix algebra;pattern clustering;principal component analysis;support vector machines","multiway spectral clustering;out-of-sample extension;weighted kernel PCA;principal component analysis;primal-dual least-squares support vector machine;LS-SVM;eigendecomposition;eigenvalue problem;primal optimization problem;balanced line fit;similarity matrix","","116","43","","","","","","IEEE","IEEE Journals & Magazines"
"Management and optimal distribution of large student numbers","W. Gora; G. Lach; J. Lübbe; O. Pfeiffer; E. Zorn; S. Jeschke","Center for Multimedia in Education and Research, Technische Universität Berlin, Berlin, Germany; Center for Multimedia in Education and Research, Technische Universität Berlin, Berlin, Germany; Center for Multimedia in Education and Research, Technische Universität Berlin, Berlin, Germany; Center for Multimedia in Education and Research, Technische Universität Berlin, Berlin, Germany; Center for Multimedia in Education and Research, Technische Universität Berlin, Berlin, Germany; Faculty of Mechanical Engineering, RWTH Aachen University, Aachen, Germany","IEEE EDUCON 2010 Conference","","2010","","","1891","1896","In principle, timetabling problems appear at every school and university. However, the degree of difficulty increases dramatically with an increasing number of students and courses for which the scheduling shall be carried out. From the mathematical point of view this is a “hard” problem, since the runtime on a computer cannot be estimated by a simple law (i.e. by a polynomial law) by the number of parameters. These kinds of problems are called “NP hard”. There are three important versions of the timetabling problem at universities, known as the university timetabling problem, i.e. curriculum-based course timetabling, post-enrollment timetabling and examination timetabling. All specified problems are important for room management at universities, for the realization of courses that can be studied according to curricula, and for the satisfaction of students and teachers. These problems are related to the optimization of room management and personnel costs (e.g. by a uniform distribution of students). Thus, the solution to these problems is related to the optimization of “real” costs, a more and more important economic factor at (German) universities. Introduction of the two-tiered Bachelor and Master courses has raised awareness for these problems at German universities: due to the multitude of new courses the timetables which have been established and stood the test of time cannot be used any longer. Moreover classes tend to be more structured and have strong similarities to classical school situations; attendance is compulsory and dependencies between modules determine the feasibility of the curricula. This feasibility is also evaluated while accrediting new study courses. Since 2003, we have been using using an algorithm that has been realized by members of our team for the solution of the post-enrollment-based course timetabling problem at the Technische Universität Berlin. For classes with more than 2000 enrolled students, organization by itself is a challenge; problems may include splitting of those classes into several separate lectures, arranging the associated tutorials into small groups of students, allocating adequate rooms, and scheduling examinations. Moreover, homework and exams need to be administered, whereby, depending on the field of study, very different rules are to be obeyed. This especially pertains to the Department of Mathematics because it offers most of the compulsory classes in mathematics for all fields of study held at Technische Universität Berlin. These are the biggest classes at the university and are to be attended by the majority of students. Thus, the Moses (Mobile Services for Students)-Account has been being developed and used since 2004. This web-based software allows students to enroll in tutorials with a list of preferences for given dates. A special algorithm, providing a globally optimized (with respect to the students' wishes and resources available) solution, processes all registrations.","2165-9559;2165-9567","978-1-4244-6571-2978-1-4244-6568-2978-1-4244-6569-9978-1-4244-6570","10.1109/EDUCON.2010.5492433","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5492433","university timetabling;academic administration;integer programming;NP-completeness","Educational institutions;Mathematics;Cost function;Linear algebra;Engineering management;Mechanical engineering;Processor scheduling;Runtime;Polynomials;Personnel","computational complexity;educational administrative data processing;educational courses;educational institutions;optimisation","university timetabling;curriculum-based course timetabling;post-enrollment timetabling;examination timetabling;room management;cost optimization;bachelor courses;Master course;German university;NP hard problem","","3","39","","","","","","IEEE","IEEE Conferences"
"Application of KLIMA/G-POD algorithm to CO<inf>2</inf>retrieval from IASI/METOP-A observations and comparison with GOSAT/TANSO-FTS products","L. M. Laurenza; U. Cortesi; S. Del Bianco; M. Gai","Istituto di Fisica Applicata “Nello Carrara” del Consiglio Nazionale delle Ricerche (IFAC-CNR), via Madonna del Piano 10, 50019 Sesto Fiorentino (Florence), Italy; Istituto di Fisica Applicata “Nello Carrara” del Consiglio Nazionale delle Ricerche (IFAC-CNR), via Madonna del Piano 10, 50019 Sesto Fiorentino (Florence), Italy; Istituto di Fisica Applicata “Nello Carrara” del Consiglio Nazionale delle Ricerche (IFAC-CNR), via Madonna del Piano 10, 50019 Sesto Fiorentino (Florence), Italy; Istituto di Fisica Applicata “Nello Carrara” del Consiglio Nazionale delle Ricerche (IFAC-CNR), via Madonna del Piano 10, 50019 Sesto Fiorentino (Florence), Italy","2012 IEEE International Geoscience and Remote Sensing Symposium","","2012","","","3635","3637","The ESA research project “Application of KLIMA algorithm to CO<sub>2</sub>retrieval from IASI/METOP-A observations and comparison with GOSAT/TANSO-FTS products” aims at the develop of a dedicated software, based on the KLIMA inversion algorithm (originally proposed by IFAC-CNR for the ESA Earth Explorer Mission), optimally suited for CO<sub>2</sub>retrieval and integrated into the ESA GRID-based operational environment G-POD (Grid Processing On-Demand) to processing Level-1 data acquired by the IASI instrument onboard the METOP-A satellite and to perform a comparison and cross-validation of GOSAT TANSO-FTS Level-2 data. Optimized versions of the KLIMA-IASI code have been investigated, aiming at developing a non-operation retrieval code with capabilities that meet the requirements of cross validation with GOSAT TANSO-FTS products and with adequate features for the integration on the G-POD system. The KLIMA algorithm consists of two distinct modules: the Forward Model and the Retrieval Model, optimized for inverse processing of IASI measurements. The Forward Model is a line-by-line radiative transfer model, with capability to simulate wideband spectral radiances acquired by IASI. The adopted spectroscopic database is HITRAN 2008. Dedicated spectroscopic database and line shape are implemented for CO<sub>2</sub>, to take into account the line-mixing effect (Niro et al., 2005). The Retrieval Model uses a constrained Nonlinear Least Square Fit approach and the cost function to be minimized takes into account the a priori information (Optimal Estimation Method) and the Marquardt parameter. To reduce the impact of systematic uncertainties, this module implements a multi-target retrieval (more than one species is simultaneously retrieved) and a complete variance-covariance matrix (VCM), including both the measurement errors and the errors in the estimate of FM parameters. For the performance of the retrieval a target accuracy of 0.3% (1 ppmv out of 370 ppmv) on regional scales (1000 × 1000 km) at monthly intervals, which is consistent with the requirements of the GOSAT mission for CO<sub>2</sub>products, was assumed as reference value. The required maximum program size was set to one Gbyte and the running time was limited with the aim of processing one orbit of IASI data in one day when using G-POD computing resources. The KLIMA-IASI ARM algorithm has been successfully integrated in the ESA G-POD for Earth Observation Application and now the code is available to all interested users for bulk processing of IASI data. ESA G-POD is a GRID based-operational environment, accessible using a dedicated Web portal (http://gpod.eo.esa.int/). It hosts more than 350 local computing nodes, more than 500 TB of EO data coming from ESA and non-ESA satellites. The full capacity of G-POD computing resources, that can be made available for KLIMA-IASI bulk processing, are currently under evaluation; moreover the upload of the full set of IASI data acquired in one year (march 2010 - march 2011) shall be completed (3 months are available at the moment). After the procurement of a consolidated version of the KLIMA-IASI/G- POD retrieval code it was possible to start the processing of IASI spectra and for comparison and cross-validation of KLIMA-IASI CO<sub>2</sub>products with GOSAT/TANSO-FTS operational products. Using the KLIMA inversion code integrated into the ESA G-POD, it was possible to perform an extensive inter-comparison and cross-validation of a selected set of IASI measurements collocated with GOSAT TANSO-FTS observations. The KLIMA-IASI code was used for the retrieval of CO<sub>2</sub>total column from a series of test cases for comparison with GOSAT TANSO-FTS SWIR L2 data. Retrieved values from measurements acquired by the two instruments in February 2011 in two selected areas between 25-45°N over Nord America and Asia are shown in figures 1-2, along with maps of the footprints of IASI (red crosses) and GOSAT (green crosses) Field of Views.","2153-7003;2153-6996;2153-6996","978-1-4673-1159-5978-1-4673-1160-1978-1-4673-1158","10.1109/IGARSS.2012.6350629","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6350629","","Extraterrestrial measurements;Air pollution;Atmospheric measurements;Databases;Geophysical measurements;Software algorithms;Software","atmospheric composition;atmospheric techniques;carbon compounds;covariance matrices;data analysis;geophysics computing;grid computing;inverse problems;portals;radiative transfer;spectral analysis","KLIMA/G-POD algorithm;CO2retrieval;IASI/METOP-A observation;GOSAT/TANSO-FTS product;ESA research project;KLIMA inversion algorithm;IFAC-CNR;ESA Earth Explorer Mission;ESA GRID-based operational environment;grid processing on-demand;Level-1 data processing;IASI instrument;METOP-A satellite;GOSAT TANSO-FTS Level-2 data;nonoperation retrieval code;forward model;retrieval model;inverse processing;line-by-line radiative transfer model;wideband spectral radiance;spectroscopic database;HITRAN 2008;line shape;line-mixing effect;constrained nonlinear least square fit approach;cost function minimization;optimal estimation method;Marquardt parameter;systematic uncertainty;multitarget retrieval;variance-covariance matrix;VCM;measurement error;GOSAT mission;G-POD computing resource;KLIMA-IASI ARM algorithm;ESA G-POD;Earth observation application;Web portal;computing node;AD 2011 02;North America;Asia;CO2","","","7","","","","","","IEEE","IEEE Conferences"
"Research and implementation of resource management system based on Xen virtual machine","Gang Ning; Yongqing Sun","School of Software, Shanghai Jiaotong University, #800 Rd Dongchuan, China; Network Security Center, The Third Institute of Ministry of Public Security, #339 Rd Bisheng, Shanghai, China","Proceedings of 2011 International Conference on Computer Science and Network Technology","","2011","3","","1371","1374","With the development of computer technology, there is gradually abundant resource available for computer systems. Virtualization technology provides a viable solution for effective management and rational allocation of system resources. Xen virtual machine is an excellent open source virtual machine, so attracting widespread attention, with broad application prospects. However, the traditional resource management of Xen virtual machine focuses on sharing processor resources fairly, while ignoring the effect of the virtual machines with the different priorities. This would cause the practicality and the performance issues in using virtual machine. This paper proposes a resource management system model based on Xen virtual machine. The model monitors guest domain and analyzes runtime information for automating resource allocation. It mainly take into account the virtual machines with different priorities. Through a series of comparative tests, the results verify that this model can enhance practicality.","","978-1-4577-1587-7978-1-4577-1586-0978-1-4577-1585","10.1109/ICCSNT.2011.6182220","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6182220","Xen;virtual technology;resource management","Prototypes;Monitoring;Analytical models;Educational institutions","resource allocation;virtual machines;virtualisation","resource management system;Xen virtual machine;virtualization technology;system resource rational allocation;open source virtual machine;processor resource sharing;guest domain","","2","8","","","","","","IEEE","IEEE Conferences"
"Overlay spectrum reuse in a multicarrier broadcast network: Single receiver analysis","A. Rico-Alvarino; C. Mosquera; F. Pérez-González","Signal Theory and Communications Department, Universidade de Vigo, 36310 Vigo, Spain; Signal Theory and Communications Department, Universidade de Vigo, 36310 Vigo, Spain; Signal Theory and Communications Department, Universidade de Vigo, 36310 Vigo, Spain","2012 IEEE 13th International Workshop on Signal Processing Advances in Wireless Communications (SPAWC)","","2012","","","209","213","The overlay cognitive radio paradigm presents a framework where a secondary user exploits the knowledge of the primary user's message to improve spectrum utilization. A multicarrier broadcast network is one of the scenarios where this knowledge is possible: the secondary user could join a single frequency network and, therefore, gain access to the primary message. However, if the primary signal is received with a strong line of sight component, its relaying from the secondary transmitter does not suffice to ensure the primary user quality of service. In this paper we study the scenario where a secondary transmitter maximizes its own transmission rate, keeping the quality of a primary receiver over a given threshold. The analytical results, based on bit error rate bounds, are verified by means of software simulations and hardware tests.","1948-3252;1948-3244;1948-3244","978-1-4673-0971-4978-1-4673-0970-7978-1-4673-0969","10.1109/SPAWC.2012.6292890","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6292890","","Optimized production technology;Bit error rate;Conferences;Signal processing;Wireless communication;MATLAB;Viterbi algorithm","broadcast communication;cognitive radio;quality of service;radio receivers;radio transmitters","overlay spectrum reuse;multicarrier broadcast network;single receiver analysis;cognitive radio paradigm;multicarrier broadcast net- work;single frequency network;secondary transmitter;primary user quality of service;transmission rate;hardware tests;software simulations","","3","10","","","","","","IEEE","IEEE Conferences"
"Towards Optimal Resource Provisioning for Running MapReduce Programs in Public Clouds","F. Tian; K. Chen","NA; NA","2011 IEEE 4th International Conference on Cloud Computing","","2011","","","155","162","Running MapReduce programs in the public cloud introduces the important problem: how to optimize resource provisioning to minimize the financial charge for a specific job? In this paper, we study the whole process of MapReduce processing and build up a cost function that explicitly models the relationship between the amount of input data, the available system resources (Map and Reduce slots), and the complexity of the Reduce function for the target MapReduce job. The model parameters can be learned from test runs with a small number of nodes. Based on this cost model, we can solve a number of decision problems, such as the optimal amount of resources that can minimize the financial cost with a time deadline or minimize the time under certain financial budget. Experimental results show that this cost model performs well on tested MapReduce programs.","2159-6190;2159-6182;2159-6182","978-1-4577-0836-7978-0-7695-4460","10.1109/CLOUD.2011.14","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6008705","MapReduce;Cloud Computing;Resource Provisioning;Performance Modeling","Complexity theory;Cloud computing;Optimization;Data models;Analytical models;Mathematical model;Programming","cloud computing;software cost estimation","optimal resource provisioning;MapReduce programs;public clouds;reduce function complexity;financial cost minimization","","63","21","","","","","","IEEE","IEEE Conferences"
"High Performance Memory Requests Scheduling Technique for Multicore Processors","W. El-Reedy; A. A. El-Moursy; H. A. H. Fahmy","NA; NA; NA","2012 IEEE 14th International Conference on High Performance Computing and Communication & 2012 IEEE 9th International Conference on Embedded Software and Systems","","2012","","","127","134","In modern computer systems, long memory latency is one of the main bottlenecks micro-architects are facing for leveraging the system performance especially for memory-intensive applications. This emphasises the importance of the memory access scheduling to efficiently utilize memory bandwidth. Moreover, in recent micro-processors, multithread and multicore is turned to be the default choice for their design. This resulted in more contention on memory. Hence, the effect of memory access scheduling schemes is more critical to the overall performance boost. Although memory access scheduling techniques have been recently proposed for performance improvement, most of them have overlooked the fairness among the running applications. Achieving both high-throughput and fairness simultaneously is challenging. In this paper, we focus on the basic idea of memory requests scheduling, which includes how to assign priorities to threads, what request should be served first, and how to achieve fairness among the running applications for multicore microprocessors. We propose two new memory access scheduling techniques FLRMR, and FIQMR. Compared to recently proposed techniques, on average, FLRMR achieves 8.64% speedup relative to LREQ algorithm, and FIQMR achieves 11.34% speedup relative to IQ-based algorithm. FLRMR outperforms the best of the other techniques by 8.1% in 8-cores workloads. Moreover, FLRMR improves fairness over LREQ by 77.2% on average.","","978-1-4673-2164-8978-0-7695-4749","10.1109/HPCC.2012.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6332168","Computer architecture;Memory management;Multicore processing","Instruction sets;Benchmark testing;Multicore processing;Throughput;Scheduling;Scheduling algorithms","coprocessors;multiprocessing systems;multi-threading;processor scheduling;storage management","high performance memory requests scheduling;multicore microprocessor;memory latency;microarchitect;memory intensive application;memory access scheduling;memory bandwidth;multithread processor;throughput;priority assignment;FLRMR;FIQMR","","3","18","","","","","","IEEE","IEEE Conferences"
"Hardware-in-the-loop validation of energy managed home thermal zone","R. Missaoui; G. Warkozek; S. Ploix; S. Bacha; V. Debusschere","G2LAB Grenoble Electrical Engineering laboratory, France; G2LAB Grenoble Electrical Engineering laboratory, France; G-SCOP Grenoble Sciences of Conception, Optimization and ProductionUniversity of Grenoble, France; G2LAB Grenoble Electrical Engineering laboratory, France; G2LAB Grenoble Electrical Engineering laboratory, France","2012 IEEE International Conference on Industrial Technology","","2012","","","421","425","The goal of this paper is to propose a real time solution to validate energy smart-home strategies that support Demand Side Management done by direct control. The validation concerns the appliances under control. An Energy Management System globally controls the appliances of a house, satisfying the subscription power limits. A hardware-in-the-loop test bench, based on a real-time simulator called RT_LAB, is proposed to exemplify the validation mean: it is used to assess the performances of an energy management strategy for a thermal zone based on the control of a heater. Physical devices are introduced into the simulated system to check whether they can support the control strategies.","","978-1-4673-0342-2978-1-4673-0340-8978-1-4673-0341","10.1109/ICIT.2012.6209974","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6209974","Energy management system (EMS);hardware-in-the-loop simulation;application building and heating control","Heating;Buildings;IP networks;Subscriptions;Application software","building management systems;energy management systems;home automation","hardware-in-the-loop validation;energy managed home thermal zone;energy smart-home strategies;demand side management;energy management system;RT_LAB","","2","23","","","","","","IEEE","IEEE Conferences"
"A solution for implementing resilience in open source Cloud platforms","G. Suciu; C. Cernat; G. Todoran; V. Suciu; V. Poenaru; T. Militaru; S. Halunga","POLITEHNICA University of Bucharest, Faculty of Electronics, Telecommunications and Information Technology, Bucharest, Romania; POLITEHNICA University of Bucharest, Faculty of Electronics, Telecommunications and Information Technology, Bucharest, Romania; POLITEHNICA University of Bucharest, Faculty of Electronics, Telecommunications and Information Technology, Bucharest, Romania; POLITEHNICA University of Bucharest, Faculty of Electronics, Telecommunications and Information Technology, Bucharest, Romania; POLITEHNICA University of Bucharest, Faculty of Electronics, Telecommunications and Information Technology, Bucharest, Romania; POLITEHNICA University of Bucharest, Faculty of Electronics, Telecommunications and Information Technology, Bucharest, Romania; POLITEHNICA University of Bucharest, Faculty of Electronics, Telecommunications and Information Technology, Bucharest, Romania","2012 9th International Conference on Communications (COMM)","","2012","","","335","338","SlapOS is the first open source operating system for Distributed Cloud Computing. SlapOS is based on a grid computing daemon - called slapgrid - which is capable of installing any software on a PC and instantiate any number of processes of potentially infinite duration of any installed software. Slapgrid daemon receives requests from a central scheduler - the SlapOS Master - which collects back accounting information from each process. SlapOS Master follows an Enterprise Resource Planning (ERP) model to handle at the same time process allocation optimization and billing. Recent research on Cloud Computing has focused on the implementation of Service Level Agreements (SLA) and operation of large Data Centers. However, in case of Force Majeure such as natural disaster, strike, terrorism, unpreventable accident, etc., Service Level Agreements (SLA) no longer apply. Rather than centralizing Cloud Computing resources in large data centers, Distributed Cloud Computing resources are aggregated from a grid of standard PCs hosted in homes, offices and small data centers.","","978-1-4577-0058-3978-1-4577-0057-6978-1-4577-0056","10.1109/ICComm.2012.6262565","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6262565","Cloud;Security;Resilience;Cloud Computing;Cloud Communications;ERP;SLA;IaaS;PaaS;SaaS;CaaS","Cloud computing;Resilience;Computer architecture;Testing;Servers;Scalability","cloud computing;enterprise resource planning;grid computing;operating systems (computers);public domain software","open source cloud platforms;SlapOS Master;open source operating system;distributed cloud computing;grid computing daemon;Slapgrid daemon;enterprise resource planning model;ERP model;service level agreements;data centers;Force Majeure","","9","15","","","","","","IEEE","IEEE Conferences"
"Fast Morphological Image Processing Open-Source Extensions for GPU Processing With CUDA","M. J. Thurley; V. Danell","Department of Computer Science, Electrical & Space Engineering, Luleå University of Technology, Luleå, Sweden; Department of Computer Science, Electrical & Space Engineering, Luleå University of Technology, Luleå, Sweden","IEEE Journal of Selected Topics in Signal Processing","","2012","6","7","849","855","GPU architectures offer a significant opportunity for faster morphological image processing, and the NVIDIA CUDA architecture offers a relatively inexpensive and powerful framework for performing these operations. However, the generic morphological erosion and dilation operation in the CUDA NPP library is relatively naive, and performance scales expensively with increasing structuring element size. The objective of this work is to produce a freely available GPU capability for morphological operations so that fast GPU processing can be readily available to those in the morphological image processing community. Open-source extensions to CUDA (hereafter referred to as LTU-CUDA) have been produced for erosion and dilation using a number of structuring elements for both 8 bit and 32 bit images. Support for 32 bit image data is a specific objective of the work in order to facilitate fast processing of image data from 3D range sensors with high depth precision. Furthermore, the implementation specifically allows scalability of image size and structuring element size for processing of large image sets. Images up to 4096 by 4096 pixels with 32 bit precision were tested. This scalability has been achieved by forgoing the use of shared memory in CUDA multiprocessors. The vHGW algorithm for erosion and dilation independent of structuring element size has been implemented for horizontal, vertical, and 45 degree line structuring elements with significant performance improvements over NPP. However, memory handling limitations hinder performance in the vertical line case providing results not independent of structuring element size and posing an interesting challenge for further optimisation. This performance limitation is mitigated for larger structuring elements using an optimised transpose function, which is not default in NPP, and applying the horizontal structuring element. LTU-CUDA is an ongoing project and the code is freely available at https://github.com/VictorD/LTU-CUDA.","1932-4553;1941-0484","","10.1109/JSTSP.2012.2204857","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6218162","Morphological image processing;erosion;dilation;GPU;NVIDIA;CUDA","Graphics processing unit;Application software;Image processing;Message systems;Libraries;Signal processing algorithms;Timing","graphics processing units;image processing;image sensors;parallel architectures;public domain software;shared memory systems","open-source extensions;GPU processing;GPU architectures;NVIDIA CUDA architecture;generic morphological erosion;dilation operation;CUDA NPP library;LTU-CUDA;3D range sensors;shared memory;CUDA multiprocessors;vHGW algorithm;structuring element size;transpose function;horizontal structuring element;fast morphological image processing","","19","20","","","","","","IEEE","IEEE Journals & Magazines"
"A DSP based SVC IP STB using open SVC decoder","F. Pescador; D. Samper; M. J. Garrido; E. Juarez; M. Blestel","Grupo de Dise&#x00F1;o Electr&#x00F3;nico y Microelectr&#x00F3;nico (GDEM), Universidad Polit&#x00E9;cnica de Madrid., Madrid, Spain; Grupo de Dise&#x00F1;o Electr&#x00F3;nico y Microelectr&#x00F3;nico (GDEM), Universidad Polit&#x00E9;cnica de Madrid., Madrid, Spain; Grupo de Dise&#x00F1;o Electr&#x00F3;nico y Microelectr&#x00F3;nico (GDEM), Universidad Polit&#x00E9;cnica de Madrid., Madrid, Spain; Grupo de Dise&#x00F1;o Electr&#x00F3;nico y Microelectr&#x00F3;nico (GDEM), Universidad Polit&#x00E9;cnica de Madrid., Madrid, Spain; IETR/Image Group Lab, UMR CNRS 6164/INSA, Rennes, France","IEEE International Symposium on Consumer Electronics (ISCE 2010)","","2010","","","1","6","In this paper, a implementation of a DSP-based IP set-top box (IP-STB) to decode CIF sequences compliant with the new Scalable Video Coding standard (14496-10 Amd 3) using Open SVC Decoder (OSD) is presented. The OSD software, designed for the PC environment, has been integrated into a previously developed IP-STB prototype. About 15 CIF frames per second can be decoded with the IP-STB.","0747-668X;2159-1423","978-1-4244-6673-3978-1-4244-6671-9978-1-4244-6672","10.1109/ISCE.2010.5523708","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5523708","Set-top box;Scalable Video Coding;Open SVC Decoder;DSP","Static VAr compensators;Digital signal processing;US Department of Transportation;Decoding;Video coding;Video sequences;Optimization methods;Testing;IEC standards;ISO standards","digital signal processing chips;sequences;video coding","DSP based SVC IP STB;DSP-based IP set-top box;open SVC decoder;CIF sequences;scalable video coding;OSD software","","6","26","","","","","","IEEE","IEEE Conferences"
"Determining the Optimal Window Length for Pattern Recognition-Based Myoelectric Control: Balancing the Competing Effects of Classification Error and Controller Delay","L. H. Smith; L. J. Hargrove; B. A. Lock; T. A. Kuiken","Feinberg School of Medicine, Northwestern University, Chicago, IL, USA; Center for Bionic Medicine, Rehabilitation Institute of Chicago, Chicago, IL, USA; Center for Bionic Medicine, Rehabilitation Institute of Chicago, Chicago, IL, USA; Center for Bionic Medicine, Rehabilitation Institute of Chicago, Chicago, IL, USA","IEEE Transactions on Neural Systems and Rehabilitation Engineering","","2011","19","2","186","192","Pattern recognition-based control of myoelectric prostheses has shown great promise in research environments, but has not been optimized for use in a clinical setting. To explore the relationship between classification error, controller delay, and real-time controllability, 13 able-bodied subjects were trained to operate a virtual upper-limb prosthesis using pattern recognition of electromyogram (EMG) signals. Classification error and controller delay were varied by training different classifiers with a variety of analysis window lengths ranging from 50 to 550 ms and either two or four EMG input channels. Offline analysis showed that classification error decreased with longer window lengths ( <i>p</i> &lt;; 0.01). Real-time controllability was evaluated with the target achievement control (TAC) test, which prompted users to maneuver the virtual prosthesis into various target postures. The results indicated that user performance improved with lower classification error (<i>p</i> &lt;; 0.01 ) and was reduced with longer controller delay ( <i>p</i> &lt;; 0.01), as determined by the window length. Therefore, both of these effects should be considered when choosing a window length; it may be beneficial to increase the window length if this results in a reduced classification error, despite the corresponding increase in controller delay. For the system employed in this study, the optimal window length was found to be between 150 and 250 ms , which is within acceptable controller delays for conventional multistate amplitude controllers.","1534-4320;1558-0210","","10.1109/TNSRE.2010.2100828","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676233","Controller delay;myoelectric control;pattern recognition;prosthesis;surface electromyography","Delay;Pattern recognition;Electromyography;Classification algorithms;Real time systems;Prosthetics;Controllability","electromyography;medical signal processing;neurophysiology;pattern classification;prosthetics;real-time systems","optimal window length;pattern recognition;myoelectric control;competing effects;classification error;controller delay;myoelectric prosthesis;virtual upper-limb prosthesis;electromyogram signals;EMG;target achievement control testing;conventional multistate amplitude controllers","Algorithms;Artificial Limbs;Computer Simulation;Data Interpretation, Statistical;Electrodes;Electromyography;Electrophysiological Phenomena;Humans;Linear Models;Muscle, Skeletal;Pattern Recognition, Visual;Prosthesis Design;Software;User-Computer Interface","125","26","","","","","","IEEE","IEEE Journals & Magazines"
"Concurrent Enterprising as a Knowledge reservoir to bridge the gap between engineering and science","A. M. Stanescu; A. Cornescu; C. Vintila; M. A. Moisescu; I. S. Sacala","University “Politehnica” Bucharest, Automatic Control & Computers Science, 313 Splaiul Independentei, Romania; Sc Edata Srl, Bucuresti, Bucharest Business Park, 12 D Menuetului, Romania; Sc Edata Srl, Bucuresti, Bucharest Business Park, 12 D Menuetului, Romania; University “Politehnica” Bucharest, Automatic Control & Computers Science, 313 Splaiul Independentei, Romania; University “Politehnica” Bucharest, Automatic Control & Computers Science, 313 Splaiul Independentei, Romania","2010 IEEE International Technology Management Conference (ICE)","","2010","","","1","8","Concurrent Engineering (CE) is an innovative non-sequentially methodology to design, manufacture, assembly, test, maintain and servicing non-sequentially complex products (like aircrafts, cars, buildings, a.s.o.). It has become a grand challenge for engineering since 1982. The Product Life Management (PLM) is nowadays one of the most powerful software tool case. The mission to be accomplished next decade is to drive the Enlarged European Union economy from “economic recovery (2010) towards sustainability, innovation and competitiveness on Global Turbulent Markets” [FIA Valencia, 2010]. The first part of the paper is concerned with an ambitious purpose: to capture some milestones (both scientific and business oriented ones) which fuelled the Knowledge reservoir of our Virtual Professional Community (VPC) within the European Society of Concurrent Engineering (ESoCE). The second part is focused on defining the dimensions of Concurrent Enterprising, which sets up adaptable enterprise architecture through its mechanism based on continuous improvement loops, in spite of the potential turbulences coming from the external environment. Furthermore, we analyse the adaptability of the model, enabled through continuous execution of the mechanisms, ultimately aimed at enhancing enterprise optimization and effective management of change.","","978-1-62748-686","10.1109/ICE.2010.7476993","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7476993","Concurrent enterprising;improvement loop;ecosystem;enterprise adaptability","","concurrent engineering;continuous improvement;globalisation;innovation management;maintenance engineering;management of change;product design;product life cycle management;sustainable development","concurrent enterprising;knowledge reservoir;CE;innovative nonsequentially methodology;nonsequentially complex product design;nonsequentially complex product manufacture;nonsequentially complex product assembly;non-sequentially complex products test;nonsequentially complex product servicing;product life management;PLM;software tool;enlarged European union;global turbulent market;European society of concurrent engineering;ESoCE;virtual professional community;VPC;continuous improvement loop;enterprise optimization enhancement;change management","","","11","","","","","","IEEE","IEEE Conferences"
"Rate Distortion Data Hiding of Motion Vector Competition Information in Chroma and Luma Samples for Video Compression","J. Thiesse; J. Jung; M. Antonini","I3S/CNRS Laboratory, University of Nice-Sophia Antipolis, Nice, France; Orange Labs, Issy Les Moulineaux, France; I3S/CNRS Laboratory, University of Nice-Sophia Antipolis, Sophia Antipolis, France","IEEE Transactions on Circuits and Systems for Video Technology","","2011","21","6","729","741","New standardization activities have been recently launched by the JCT-VC experts group in order to challenge the current video compression standard H.264/AVC. Several improvements of this standard, previously integrated in the JM key technical area software, are already known and gathered in the high efficiency video coding test model. In particular, competition-based motion vector prediction has proved its efficiency. However, the targeted 50% bitrate saving for equivalent quality is not yet achieved. In this context, this paper proposes to reduce the signaling information resulting from this motion vector competition, by using data hiding techniques. As data hiding and video compression traditionally have contradictory goals, an advanced study of data hiding schemes is first performed. Then, an original way of using data hiding for video compression is proposed. The main idea of this paper is to hide the competition index into appropriately selected chroma and luma transform coefficients. To minimize the prediction errors, the transform coefficients modification is performed via a rate-distortion optimization. The proposed scheme is evaluated on several low and high resolution sequences. Objective improvements (up to 2.40% bitrate saving) and subjective assessment of the chroma loss are reported.","1051-8215;1558-2205","","10.1109/TCSVT.2011.2130330","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5734824","Chroma and luma samples;competition index signalization;data hiding;H.264/AVC;video coding","Indexes;Transforms;Bit rate;Video compression;Encoding;Automatic voltage control;Rate-distortion","data compression;data encapsulation;rate distortion theory;video coding","rate distortion data hiding;motion vector competition information;video compression;JCT-VC experts group;H.264/AVC;JM key technical area software;video coding test model;competition index;chroma transform coefficients;luma transform coefficients;rate-distortion optimization;chroma loss","","4","24","","","","","","IEEE","IEEE Journals & Magazines"
"Use of embedded intelligence in tactical grids for energy surety and fuel conservation","D. D. Massie; P. S. Curtiss; M. A. Miller","IPERC, Fort Montgomery, NY; IPERC, Fort Montgomery, NY; IPERC, Fort Montgomery, NY","2012 IEEE Power and Energy Society General Meeting","","2012","","","1","5","This paper describes a system for creating an energy-sharing infrastructure, effectively creating redundant sources of energy supply and significantly reducing the logistical burdens associated with providing power. An intelligent power management and power grid system has been developed and tested. This system optimizes performance and efficiency through local and system-level autonomous controls. The grid system was based on existing military, trailer-mounted, mobile power equipment. A reduction in fuel consumption of 36 percent was observed. In addition, prioritized load shedding was demonstrated as a means to prevent the generators from being overloaded.","1932-5517;1944-9925","978-1-4673-2729-9978-1-4673-2727-5978-1-4673-2728","10.1109/PESGM.2012.6345122","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6345122","Energy conservation;smart grid;intelligent control;power optimization","Generators;Monitoring;Computers;Heating;Fuels;Software;Cooling","control engineering computing;electric generators;embedded systems;graphical user interfaces;intelligent control;load shedding;military equipment;power apparatus;power engineering computing;power grids;power system management","embedded intelligence;tactical grids;energy surety;fuel conservation;energy-sharing infrastructure;energy supply sources;intelligent power management;power grid system;system-level autonomous controls;mobile power equipment;trailer-mounted eqipment;military equipment;load shedding;generators;fuel consumption","","1","","","","","","","IEEE","IEEE Conferences"
"A simplified subjective video quality assessment method based on signal detection theory","M. de-Frutos-López; A. B. Mejía-Ocaña; S. Sanz-Rodríguez; C. Peláez-Moreno; F. Díaz-de-María; Z. Pizlo","Department of Signal Theory and Communications, University Carlos III Madrid, Spain; Department of Signal Theory and Communications, University Carlos III Madrid, Spain; Department of Signal Theory and Communications, University Carlos III Madrid, Spain; Department of Signal Theory and Communications, University Carlos III Madrid, Spain; Department of Signal Theory and Communications, University Carlos III Madrid, Spain; Department of Psychological Sciences, School of Electrical and Computer Engineering, Purdue University, USA","2012 Picture Coding Symposium","","2012","","","237","240","A simplified protocol and associated metrics based on Signal Detection Theory (SDT) for subjective Video Quality Assessment (VQA) is proposed with the aim of filling the gap existing between the lack of discrimination abilities of objective Quality Estimates (specially when perceptually motivated processing methods are involved) and the costly normative subjective quality tests. The proposed protocol employs a reduced number of assessors and provides a quality ranking of the methods being evaluated. It is intended for providing the rapid experimental turn around necessary for developing algorithms. We have validated our proposal by corroborating with our test a well-known result for the video coding community: the quality benefits of including an in-loop deblocking filter. A software interface to design and administrate the test is also made publicly available.","","978-1-4577-2049-9978-1-4577-2047-5978-1-4577-2048","10.1109/PCS.2012.6213336","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6213336","Video Quality Assessment;Subjective Quality;Signal Detection Theory;Statistical Decission Theory;Pair Comparison","Bit rate;Quality assessment;Signal detection;Humans;Encoding;Video coding","filtering theory;protocols;signal detection;video coding","subjective video quality assessment;signal detection theory;protocol;objective quality estimation;quality ranking;video coding community;in-loop deblocking filter;software interface","","","17","","","","","","IEEE","IEEE Conferences"
"Fault-tolerant middleware based on multistream pipeline for private storage services","J. L. Gonzalez; V. Sosa-Sosa; B. Bergua; L. M. Sanchez; J. Carretero","Cd. Valles Institute of Technology, Information Technology, Laboratory Center of Research CINVESTAV Ciudad, Victoria, Mexico; Cd. Valles Institute of Technology, Information Technology, Laboratory Center of Research CINVESTAV Ciudad, Victoria, Mexico; Computer Architecture and Technology Area, Computer Science. Department UNIVERSIDAD CARLOS III, Madrid, Spain; Computer Architecture and Technology Area, Computer Science. Department UNIVERSIDAD CARLOS III, Madrid, Spain; Computer Architecture and Technology Area, Computer Science. Department UNIVERSIDAD CARLOS III, Madrid, Spain","2012 International Conference for Internet Technology and Secured Transactions","","2012","","","548","555","This paper presents a fault-tolerant middleware for private storage services based on a client-server model. A client-side API split files into redundant chunks, which are encoded/decoded, anonymized, and distributed to different storage locations in parallel by using multistream pipeline. A chunk placement engine on the server-side API receives/delivers chunks from/to the client side API. In this model, the user preserves her files in-house and achieves data availability anytime from anywhere even in site failure scenarios while the organizations preserves the control of their storage resources. The experimental evaluation reveals the middleware can compensate the overhead produced on the client side by taking advantage of multiple cores commonly found in user's devices. The users observe that the middleware produces similar performance to the private and public file hosting systems tested in the evaluation. The organizations observe a significant reduction of work in their servers. In site disaster scenarios, the middleware yields even better performance than fault-tolerant online distributed solutions tested in the evaluation.","","978-1-908320-08-7978-1-4673-5325","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6470871","Optimization;Storage Services;Privacy;Fault tolerance;Virtualization","Pipelines;Organizations;Servers;Libraries;Sockets;Measurement","middleware;multiprocessing systems;pipeline processing;software fault tolerance;storage management","fault-tolerant middleware;multistream pipeline;private storage services;client-server model;client-side API;chunk placement engine;server-side API;data availability;site failure scenarios;storage resources;multiple cores;public file hosting systems;private file hosting systems;in site disaster scenarios;fault-tolerant online distributed solutions","","","39","","","","","","IEEE","IEEE Conferences"
"Design and construction of a parallel hybrid prototype","V. Di Giacomo; L. Martellucci; S. Sgreccia; F. M. F. Mascioli","DIET Department - Sustainable Mobility Research Center-POMOS, Italy; DIET Department - Sustainable Mobility Research Center-POMOS, Italy; DIET Department - Sustainable Mobility Research Center-POMOS, Italy; DIET Department - Sustainable Mobility Research Center-POMOS, Italy","2012 IEEE International Symposium on Industrial Electronics","","2012","","","1720","1725","The project “Bizzarrini P538 EcoTarga” has been developed by the DIET department of “Sapienza” University of Rome. The project consisted in the design and construction of a replica of a historic Italyn car, updated with modern parallel hybrid-type plug-in technologies, and a Lithium-polymer battery storage system. The research aimed at undertaking practical experimentation on parallel hybrid vehicles, with a special focus on the problems related to the construction and optimization of hybrid propulsion systems. For these reasons special focus are dedicated to describe the layout of the prototype and some technical data of the devices used in the project, and the measurement campaign achieved on the tested storate system. Finally the hybrid parallel control strategy was obtained, in first, with a simulation model by coupling the simulation software vehicle dynamics GT-Drive with the Matlab/Simulink environment, to optimize control strategy of the prototype. and then used to implement the control system in to CompactRio CPU, to manage the load requests to the motor and engine.","2163-5145;2163-5137;2163-5137","978-1-4673-0158-9978-1-4673-0159-6978-1-4673-0157","10.1109/ISIE.2012.6237350","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6237350","Batteries;Energy storage;permanent magnets motor;road vehicle contro;road vehicle electric propulsion;simulation;software engineering;traction motors","Batteries;Engines;Vehicles;Discharges (electric);Prototypes;Electric motors;Voltage measurement","battery powered vehicles;electric propulsion;hybrid electric vehicles","parallel hybrid prototype construction;parallel hybrid prototype design;Bizzarrini P538 EcoTarga;DIET department of Sapienza University of Rome;historic Italian car;parallel hybrid-type plug-in technologies;hybrid propulsion systems;measurement campaign;hybrid parallel control strategy;simulation software vehicle dynamics GT-Drive;Matlab/Simulink environment;control strategy;control system;CompactRio CPU;engine;lithium-polymer battery storage system;Li","","","7","","","","","","IEEE","IEEE Conferences"
"Efficient Online RTL Debugging Methodology for Logic Emulation Systems","S. Banerjee; T. Gupta","NA; NA","2012 25th International Conference on VLSI Design","","2012","","","298","303","The offline debugging model provided by logic emulation systems has some specific disadvantages. Since analysis of signal traces and bug fixing is decoupled from emulation run, validation of a potential fix requires a costly iteration through design recompilation and mapping process, followed by fresh emulation run. This slows down overall verification process. This paper presents an online debugging methodology to achieve rapid verification closure with capability to execute the design back and forward for debug. On encountering an error, the design under test (DUT) can be reverse executed step-by-step to locate source of the error. A two pass emulation technique is used to generate checkpoints and traces needed to support reverse execution. Easy and efficient reverse execution based debug is supported using an innovative technique called optimized design slicing, which allows debug along a meaningful design portion likely to cause the error being investigated. Once the source of error is located, potential bug fixes can be evaluated online by forcing a set of signals to desired values, without going through the design recompilation process and restarting emulation from time 0. Benchmarks on several customer designs have shown that the methodology enhances verification performance significantly.","2380-6923;1063-9667;1063-9667","978-1-4673-0438-2978-0-7695-4638","10.1109/VLSID.2012.87","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6167768","Emulation;Debugging","Emulation;Clocks;Debugging;Software;Force;Logic gates;Hardware design languages","integrated circuit modelling;iterative methods;logic circuits","online RTL debugging methodology;logic emulation systems;costly iteration through design recompilation;mapping process;online debugging methodology;design under test;DUT;two pass emulation technique;optimized design slicing","","3","10","","","","","","IEEE","IEEE Conferences"
"Real-time execution models for co-simulation","C. Faure; N. Pernet","IFP, 1-4 Avenue de Bois Pr&#x00E9;au, 92852 Rueil-Malmaison, FRANCE; IFP, 1-4 Avenue de Bois Pr&#x00E9;au, 92852 Rueil-Malmaison, FRANCE","ACS/IEEE International Conference on Computer Systems and Applications - AICCSA 2010","","2010","","","1","8","In order to reduce development cycle, powertrain control and optimization is increasingly based on system modeling and several steps of simulation are needed. As an intermediate step, real-time simulation aim to prepare further test bed validations embedding the actual engine. This article discusses the challenge to find a real-time execution model offering the determinism needed to maintain the numerical results observed in offline simulation and still managing to meet real-time constraints. We first propose a conservative real-time execution model, totally deterministic but pessimistic in term of computational resources management. Then, since we need better use of these resources in order to achieve real-time simulation of representative complex models, we present different alternatives and discuss the advantages and drawbacks of each of them.","2161-5322;2161-5330","978-1-4244-7717-3978-1-4244-7716-6978-1-4244-7715","10.1109/AICCSA.2010.5586968","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5586968","","Real time systems;Computational modeling;Numerical models;Engines;Load modeling;Computer architecture;Software","digital simulation;mechanical engineering computing;power transmission (mechanical)","real-time execution models;co-simulation;development cycle reduction;powertrain control;optimization;system modeling;test bed validations;engine;real-time constraints;computational resources management","","","14","","","","","","IEEE","IEEE Conferences"
"Compilation of stream programs onto scratchpad memory based embedded multicore processors through retiming","W. Che; K. Chatha","Faculty of Computer Science and Engineering, Arizona State University, Tempe, 85287, USA; Faculty of Computer Science and Engineering, Arizona State University, Tempe, 85287, USA","2011 48th ACM/EDAC/IEEE Design Automation Conference (DAC)","","2011","","","122","127","The prevalence of stream applications in signal processing, multi-media, and network processing domains has resulted in a new trend of programming and architecture design. Several languages and multicore architectures have been developed to support streaming applications. In many of these multicore architectures scratchpad memories (SPM) have substituted caches due to their lower power consumption. Performance optimization on SPM based architectures requires the programmer/compiler to efficiently manage the limited local memory. Our paper addresses the problem of compilation of stream programs onto multicore architectures that incorporate SPMs. We propose a retiming technique that maximizes the throughput under a memory constraint with a user-specified number of software pipeline stages. Trade-offs between double buffering and code overlay are explored intensively in our technique to achieve the best performance. The efficiency of our technique was evaluated by compiling several stream applications for the IBM Cell BE and comparing their results against existing approaches.","85-644924;0738-100x;0738-100x","978-1-4503-0636-2978-1-4503-0636-2978-1-4503-0636","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5981708","Stream;Compiler;Retiming;Scratchpad Memory;Embedded;Multicore;Processors","Pipelines;Program processors;Multicore processing;Registers;Schedules;Benchmark testing","memory architecture;multiprocessing systems;program compilers;random-access storage","stream program compilation;scratchpad memories;embedded multicore processors;signal processing;multimedia domains;network processing domains;multicore architectures;power consumption;performance optimization;SPM based architectures;memory constraint;software pipeline stages;code overlay;IBM Cell BE","","","16","","","","","","IEEE","IEEE Conferences"
"Performance Investigation of a MANET Testbed in Outdoor Stairs Environment for Different Scenarios","M. Hiyama; E. Kulla; T. Oda; M. Ikeda; L. Barolli; M. Takizawa","NA; NA; NA; NA; NA; NA","2012 Sixth International Conference on Complex, Intelligent, and Software Intensive Systems","","2012","","","284","289","The absence of any fixed infrastructure and mobility of Mobile Ad-hoc Networks (MANETs) make them attractive for usage in time-critical applications and collaborative computing. Recently MANETs are also used in real-time communications such as audio phone calls. In this paper, we investigate the performance of an outdoor stairs MANET test bed for different scenarios considering throughput and packet loss. We analyze the behavior of Optimized Link State Routing (OLSR) protocol. We design and implement two experimental scenarios: static scenario and moving scenario and draw conclusions regarding mobile communications with different number of hops.","","978-1-4673-1233-2978-0-7695-4687","10.1109/CISIS.2012.127","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6245598","Experimental Results;MANET Testbed;Outdoor Stairs;OLSR Protocol","Throughput;Mobile ad hoc networks;Routing protocols;Measurement;Routing","mobile ad hoc networks;routing protocols;telecommunication network routing","performance investigation;MANET testbed;outdoor stairs environment;mobile ad-hoc networks mobility;mobile ad-hoc networks infrastructure;collaborative computing;optimized link state routing protocols;OLSR protocol;mobile communications","","","20","","","","","","IEEE","IEEE Conferences"
"Evaluation of a MANET Testbed in Indoor Stairs Environment Considering OLSR Protocol","E. Kulla; M. Ikeda; M. Hiyama; L. Barolli","NA; NA; NA; NA","2011 International Conference on Complex, Intelligent, and Software Intensive Systems","","2011","","","160","167","In this paper, we deal with evaluation of our MANET test bed in indoor stairs scenarios. We analyse the behavior of Optimized Link State Routing (OLSR), considering two metrics: throughput and delay. We implement two experimental scenarios: static and shifting. We draw conclusions regarding the comparison between two scenarios and between communications with different number of hops. From our experiments, we showed that for both scenarios, in one-hop and two-hop communications, OLSR shows a good performance. As the number of hops increases to three or more hops, the communication shows a worse performance. The performance decreases further when the nodes are mobile, i.e in shifting scenario, which shows that for three hops the communication becomes difficult.","","978-1-61284-709-2978-0-7695-4373","10.1109/CISIS.2011.32","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5989010","MANET;Stairs Environment;OLSR;Testbed","Throughput;Floors;Mobile ad hoc networks;Measurement;Routing;Routing protocols","indoor communication;mobile ad hoc networks;routing protocols","MANET testbed;indoor stairs environment;OLSR protocol;optimized link state routing","","","25","","","","","","IEEE","IEEE Conferences"
"A New Approach Based on a Least Square Method for Real-Time Estimation of Cantilever Array Deflections with an FPGA","R. Couturier; S. Domas; G. Goavec-Merou; M. Favre; M. Lenczner; A. Meister","NA; NA; NA; NA; NA; NA","2012 Second Workshop on Design, Control and Software Implementation for Distributed MEMS","","2012","","","30","37","Atomic force microscopes (AFM) provide high resolution images of surfaces. In this paper, we focus our attention on an inter ferometry method for deflection estimation of cantilever arrays in quasi-static regime. In its original form, spline interpolation was used to determine interference fringe phase, and thus the deflections. Computations were performed on a PC. Here, we propose a new complete solution with a least square based algorithm and an optimized FPGA implementation. Simulations and real tests showed very good results and open perspective for real-time estimation and control of cantilever arrays in the dynamic regime.","","978-1-4673-1203-5978-0-7695-4679","10.1109/dMEMS.2012.16","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6195431","FPGA;cantilever arrays;interferometry","Field programmable gate arrays;Arrays;Cameras;Optical interferometry;Real time systems;Laser beams;Spline","atomic force microscopy;cantilevers;field programmable gate arrays;interferometry;least squares approximations;splines (mathematics)","least square method;real-time estimation;cantilever array deflections;atomic force microscopes;AFM;high resolution images;inter ferometry method;deflection estimation;spline interpolation;optimized FPGA implementation","","1","7","","","","","","IEEE","IEEE Conferences"
"Genetic Algorithm for Arabic Word Sense Disambiguation","M. E. B. Menai; W. Alsaeedan","NA; NA","2012 13th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing","","2012","","","195","200","Word sense disambiguation (WSD) problem asks to assign a meaning to a word according to a context in which it occurs. Many solutions exist for WSD in natural languages, such as English, but research work on Arabic WSD (AWSD) remains limited. AWSD is a more exigent task because Arabic has an intrinsic complexity in its writing structure and ambiguity, such as syntactic, semantic, and anaphoric ambiguity levels. Genetic algorithms (GAs) can be effective to solve this problem since they have been successfully used for many NP-hard optimization problems. In this paper, we propose a new approach to solve AWSD problem based on a GA. We describe a prototype of AWSD system in which we test the performance of our algorithm by carrying out experiments on Arabic sample text, and compare it with a naïve Bayes classifier for AWSD. We show the benefit of the proposed approach and its advantage over naïve Bayes classifier.","","978-1-4673-2120","10.1109/SNPD.2012.38","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6299280","Genetic algorithms;natural language processing;word sense disambiguation;Arabic language","Genetic algorithms;Semantics;Prototypes;Sociology;Statistics;USA Councils;Natural language processing","Bayes methods;computational complexity;genetic algorithms;natural language processing;pattern classification","genetic algorithm;Arabic word sense disambiguation;natural languages;writing structure;syntactic ambiguity;semantic ambiguity;anaphoric ambiguity;NP-hard optimization;naïve Bayes classifier","","2","19","","","","","","IEEE","IEEE Conferences"
"Grey decision of optimal simultaneous mapping and clustering to improve FPGA performance","J. Wu; Y. Fan; S. Wang","Department of Electronic Engineering, De Lin Institute of Technology, Tu-Cheng City, Taipei County 236, Taiwan, ROC; Department of Computer Science and Information, Engineering National Taitung University, Taitung, Taiwan, ROC; Department of Electronic, Engineering, Ming Chi University of Technology Taipei, Taiwan, ROC","2011 IEEE 3rd International Conference on Communication Software and Networks","","2011","","","413","417","This work studies how the architectural parameters of LUT-based field programmable gate arrays (FPGAs) are related to the LUT cluster size N and input number k A novel algorithm is proposed to combine grey decision-making approach for solving the problem of FPGA performance. Experimental results demonstrate that the algorithm improves the DAO map+T-VPack delay by 7.27% and reduces the SMAC total of CLB number by 22.15% on average. Furthermore, our proposed can get optimizes performance by appropriate selection of pairs of LUT cluster size N and input number k to construct the FPGA architecture with inequality demandable weight in area, energy, and delay.","","978-1-61284-486-2978-1-61284-485-5978-1-61284-484","10.1109/ICCSN.2011.6014925","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014925","FPGAs (Field programmable gate arrays);LUT cluster size;grey decision-making","Silicon;Benchmark testing;Delay;Radio frequency;Computer numerical control","decision making;field programmable gate arrays;grey systems;table lookup","optimal simultaneous mapping;field programmable gate arrays;lookup tables;grey decision-making;DAO map+T-VPack delay;CLB number","","1","18","","","","","","IEEE","IEEE Conferences"
"Green Metrics for Energy-aware IT Systems","A. Kipp; T. Jiang; M. Fugini","NA; NA; NA","2011 International Conference on Complex, Intelligent, and Software Intensive Systems","","2011","","","241","248","This paper presents a novel approach to characterise applications with respect to their energy consumption by using a set of energy-related metrics, called Green Metrics. These indicators are based on energy consumption measurements, such as indexes of computing resource usage, of environmental impact, and even of development costs required to (re)design an application in order to optimise its energy consumption footprint, or of organizational factors related to application management. Our approach is framed in the GAMES (Green Active Management of Energy in IT Service centres) EU Project1 about green IT. In this paper, we define four clusters of Green Metrics enabling to feature an application in terms of the energy it consumes at run time. Such metrics are the basis for measuring the ""greenness"" of an application and to detect where it consumes and wastes energy. Hints are provided to improve applications design and execution. We show within an application scenario how monitoring and evaluation of the Green Metrics helps to improve energy efficiency.","","978-1-61284-709-2978-0-7695-4373","10.1109/CISIS.2011.42","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5988982","energy efficiency measurement;energy-aware IT systems;performance measurement;green metrics","Green products;Energy consumption;Energy measurement;Benchmark testing;Energy efficiency;Indexes","energy consumption;organisational aspects;power aware computing","green metrics;energy aware IT systems;computing resource usage;energy consumption footprint;organizational factors;GAMES","","6","28","","","","","","IEEE","IEEE Conferences"
"Implementation of the CA-CFAR algorithm for pulsed-Doppler radar on a GPU architecture","C. J. Venter; H. Grobler; K. A. AlMalki","Defence, Peace, Safety and Security, Council for Scientific and Industrial Research, South Africa; Department of Electrical, Electronic and Computer Engineering, University of Pretoria, South Africa; Electronics, Communications and Photonics, King Abdulaziz City for Science and Technology, Kingdom of Saudi Arabia","2011 IEEE Jordan Conference on Applied Electrical Engineering and Computing Technologies (AEECT)","","2011","","","1","6","The Cell-Averaging Constant False-Alarm Rate (CA-CFAR) algorithm was implemented and optimized in software on the NVIDIA Tesla C1060 GPU architecture for application in pulsed-Doppler radar signal processors. A systematic approach was followed to gradually explore opportunities for parallel execution and optimization by implementing the algorithm first in MATLAB (CPU), followed by native C (CPU) and finally NVIDIA CUDA (GPU) environments. Three techniques for implementing the CA-CFAR in software were identified and implemented, namely a naïıve technique, sliding window technique and a new variant which employs the Summed-Area Table (SAT) algorithm. The naïıve technique performed best on the GPU architecture. The SAT technique shows potential, especially for cases where very large CFAR windows are required. However, the results do not justify using the GPU architecture instead of the CPU architecture for this application when data transfer to and from the GPU is taken into consideration.","","978-1-4577-1084-1978-1-4577-1083-4978-1-4577-1082","10.1109/AEECT.2011.6132514","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6132514","CFAR;GPU;Doppler;radar;signal processing","Graphics processing unit;Computer architecture;Doppler effect;Microprocessors;Benchmark testing;Optimization;Throughput","Doppler radar;graphics processing units;mathematics computing;radar signal processing","CA-CFAR algorithm;cell-averaging constant false-alarm rate;NVIDIA Tesla C1060 GPU architecture;pulsed-Doppler radar signal processors;Matlab (CPU);summed-area table algorithm;SAT algorithm","","3","13","","","","","","IEEE","IEEE Conferences"
"Using Laguerre functions to improve efficiency of multi-parametric predictive control","G. Valencia-Palomo; J. A. Rossiter","Department of Automatic Control and Systems Eng., University of Sheffield, South Yorkshire, UK. S1 3JD; Department of Automatic Control and Systems Eng., University of Sheffield, South Yorkshire, UK. S1 3JD","Proceedings of the 2010 American Control Conference","","2010","","","4731","4736","Multi-parametric quadratic programming (mp-QP) is an alternative means of implementing conventional predictive control algorithms whereby one transfers much of the computational load to offline calculations. However, coding and implementation of this solution may be more burdensome than simply solving the original QP. This paper shows how Laguerre functions can be used in conjunction with mp-QP to achieve a large decrease in both the online computations and data storage requirements while increasing the feasible region of the optimization problem. Extensive simulation results are given to back this claim.","2378-5861;0743-1619;0743-1619","978-1-4244-7427-1978-1-4244-7426-4978-1-4244-7425","10.1109/ACC.2010.5531098","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5531098","Predictive control;multi-parametric quadratic programming;Laguerre functions","Predictive control;Quadratic programming;Memory;Testing;Sampling methods;Application software;Trajectory;Performance analysis;Prediction algorithms;Computational modeling","predictive control;quadratic programming","Laguerre functions;multiparametric predictive control;multiparametric quadratic programming;computational load;offline calculations;data storage requirements;optimization problem","","7","22","","","","","","IEEE","IEEE Conferences"
"Exploiting the forgiving nature of applications for scalable parallel execution","J. Mengte; A. Raghunathan; S. Chakradhar; S. Byna","NEC Laboratories America, Princeton, NJ; NEC Laboratories America, Princeton, NJ; NEC Laboratories America, Princeton, NJ; NEC Laboratories America, Princeton, NJ","2010 IEEE International Symposium on Parallel & Distributed Processing (IPDPS)","","2010","","","1","12","It is widely believed that most Recognition and Mining (RM) workloads can easily take advantage of parallel computing platforms because these workloads are dataparallel. Contrary to this popular belief, we present RM workloads for which conventional parallel implementations scale poorly on multi-core platforms. We identify off-chip memory transfers and overheads in the parallel runtime library as the primary bottlenecks that limit speedups to be well below the ideal linear speedup expected for data-parallel workloads. To achieve improved parallel scalability, we identify and exploit several interesting properties of RM workloads - sparsity of model updates, low spatial locality among model updates, presence of insignificant computations, and the inherently self-healing nature of these algorithms in the presence of errors. We leverage these domain-specific characteristics to improve parallel scalability in two major ways. First, we utilize data dependency relaxation to simultaneously execute multiple training iterations in parallel, thereby increasing the granularity of the parallel tasks and significantly lowering the run-time overheads of fine-grained threading. Second, we strategically drop selected computations that are insignificant to the accuracy of the final result, but account for a disproportionately large amount of off-chip (memory and coherence) traffic. Through the application of the proposed techniques, we show that much higher speedups are possible on multi-core platforms for two important RM applications - document search using semantic indexing, and eye detection in images using generalized learning vector quantization. On an 8-core platform, we achieve application speedups of 5.5X and 7.3X compared to sequential implementations. Compared to conventional parallel implementations of these applications using Intel's TBB, the proposed techniques result in 4.3X and 4.9X improvements. Although the optimized parallel implementations are not numerically equivalent to the sequential implementations, the output quality is shown to be comparable (and within the margin of variation produced by processing the input data in a different order). We also explore error mitigation techniques that can be used to ensure that the accuracy of results is not compromised.","1530-2075;1530-2075","978-1-4244-6443-2978-1-4244-6442-5978-1-4244-6441","10.1109/IPDPS.2010.5470469","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5470469","Best-effort computing;Parallel computing;Parallel programming;Recognition;Mining;Multi-core;Dependency relaxation","Parallel processing;Concurrent computing;Scalability;Application software;Vector quantization;Iterative algorithms;National electric code;Laboratories;Computer science;Testing","parallel processing;parallel programming","scalable parallel execution;recognition workload;mining workload;parallel computing platform;multi-core platform;off-chip memory transfer;parallel runtime library;data-parallel workload;parallel scalability;data dependency relaxation;document search;semantic indexing;eye detection;generalized learning vector quantization;optimized parallel implementation","","7","32","","","","","","IEEE","IEEE Conferences"
"CT-index: Fingerprint-based graph indexing combining cycles and trees","K. Klein; N. Kriege; P. Mutzel","Department of Computer Science, Technische Universit&#x00E4;t Dortmund, Germany; Department of Computer Science, Technische Universit&#x00E4;t Dortmund, Germany; Department of Computer Science, Technische Universit&#x00E4;t Dortmund, Germany","2011 IEEE 27th International Conference on Data Engineering","","2011","","","1115","1126","Efficient subgraph queries in large databases are a time-critical task in many application areas as e.g. biology or chemistry, where biological networks or chemical compounds are modeled as graphs. The NP-completeness of the underlying subgraph isomorphism problem renders an exact subgraph test for each database graph infeasible. Therefore efficient methods have to be found that avoid most of these tests but still allow to identify all graphs containing the query pattern. We propose a new approach based on the filter-verification paradigm, using a new hash-key fingerprint technique with a combination of tree and cycle features for filtering and a new subgraph isomorphism test for verification. Our approach is able to cope with edge and vertex labels and also allows to use wild card patterns for the search. We present an experimental comparison of our approach with state-of-the-art methods using a benchmark set of both real world and generated graph instances that shows its practicability. Our approach is implemented as part of the Scaffold Hunter software, a tool for the visual analysis of chemical compound databases.","2375-026X;1063-6382;1063-6382","978-1-4244-8960-2978-1-4244-8959-6978-1-4244-8958","10.1109/ICDE.2011.5767909","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5767909","","Feature extraction;Encoding;Indexing;Chemical compounds;Biology","data visualisation;database indexing;fingerprint identification;graph theory;optimisation;query processing;trees (mathematics);very large databases","CT-Index;fingerprint-based graph indexing;trees;subgraph queries;large databases;time-critical task;NP-completeness;subgraph isomorphism problem;filter-verification paradigm;hash-key fingerprint technique;cycle features;vertex labels;edge labels;wild card patterns;Scaffold Hunter software;visual analysis","","5","29","","","","","","IEEE","IEEE Conferences"
"Development and analysis of genetic algorithms: Sudoku case study","A. Milton; C. Ortega-Sanchez","Department of Electrical and Computer Engineering, Curtin University, Australia; Department of Electrical and Computer Engineering, Curtin University, Australia","TENCON 2012 IEEE Region 10 Conference","","2012","","","1","6","This paper discusses the design and subsequent analysis of software implementing a configurable genetic algorithm. The genetic algorithm is primarily targeted towards the solving of Sudoku puzzles. Sudoku is regarded as an ideal test-bed for algorithm development due to the fact that it is a constrained optimisation problem that belongs to the NP-complete class of computational problems. The aim of this paper is to outline the various features currently implemented in the software, and to present preliminary results of an analysis of various aspects of the underlying genetic algorithm.","2159-3450;2159-3442;2159-3442","978-1-4673-4824-9978-1-4673-4823-2978-1-4673-4822","10.1109/TENCON.2012.6412205","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6412205","","Genetic algorithms;Encoding;Sociology;Statistics;Software;Algorithm design and analysis;Software algorithms","computational complexity;game theory;genetic algorithms","genetic algorithm analysis;genetic algorithm development;Sudoku case study;Sudoku puzzles;NP-complete problem","","1","15","","","","","","IEEE","IEEE Conferences"
"Acquisition & integration activities of ground support hardware items in terms of system engineering process","A. Pahsa; G. Alat; P. Kaya; B. Baykal","HAVELSAN, Ankara, Turkey; HAVELSAN, Ankara, Turkey; HAVELSAN, Ankara, Turkey; HAVELSAN, Ankara, Turkey","29th Digital Avionics Systems Conference","","2010","","","6.D.6-1","6.D.6-9","In this study, acquisition and integration activities of ground support hardware items in terms of system engineering processes and optimizations at HAVELSAN A.Ş Peace Eagle project is explained. The Peace Eagle (PE) Airborne Early Warning & Control (AEW&C) System is an airborne surveillance system being developed for the Republic of Turkey. The AEW&C System provides surveillance, air defense, fleet support and force coordination in defense of sovereignty and national interests as an integral element of the Turkish Air Force (TurAF) as customer. The PE AEW&C System comprises an Airborne Segment (ABS) 737 series flight platform and a Ground Support Segment (GSS). These segments are formed of subsystems with hardware and software configuration items to perform the PE AEW&C System functions. In this project HAVELSAN participates in acquisition of PE AEW&C as the sole in-country subcontractor of BOEING for the Mission Computing Segment (MCS) and Ground Support Segment (GSS) and all the engineering processes throughout the program, starting from the system analysis till the end of system test and evaluation. HAVELSAN also contributes to GSS software Turkish unique design, development, integration and testing, and Integrated Logistics Support (ILS) for the GSS throughout the service life of the system.","2155-7209;2155-7195;2155-7195","978-1-4244-6618-4978-1-4244-6616-0978-1-4244-6617","10.1109/DASC.2010.5655455","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5655455","","Systems engineering and theory;Software;Hardware;Humans;Computer architecture;Aircraft;Training","aerospace components;defence industry;ground support systems;military aircraft;surveillance;systems engineering","ground support hardware item;system engineering;peace eagle airborne early warning and control system;airborne surveillance system;Republic of Turkey;AEW&C system;air defense;fleet support;Turkish Air Force;TurAF;Airborne Segment 737;ground support segment;HAVELSAN;BOEING;mission computing segment;MCS;GSS software;integrated logistics support;ILS","","","9","","","","","","IEEE","IEEE Conferences"
"Customizable FPGA IP Core Implementation of a General-Purpose Genetic Algorithm Engine","P. R. Fernando; S. Katkoori; D. Keymeulen; R. Zebulum; A. Stoica","Department of Computer Science and Engineering, University of South Florida, Tampa, FL, USA; Department of Computer Science and Engineering, University of South Florida, Tampa, FL, USA; Jet Propulsion Laboratory, Pasadena, CA, USA; Jet Propulsion Laboratory, Pasadena, CA, USA; Jet Propulsion Laboratory, Pasadena, CA, USA","IEEE Transactions on Evolutionary Computation","","2010","14","1","133","149","Hardware implementation of genetic algorithms (GAs) is gaining importance because of their proven effectiveness as optimization engines for real-time applications (e.g., evolvable hardware). Earlier hardware implementations suffer from major drawbacks such as absence of GA parameter programmability, rigid predefined system architecture, and lack of support for multiple fitness functions. In this paper, we report the design of an IP core that implements a general-purpose GA engine that addresses these problems. Specifically, the proposed GA IP core can be customized in terms of the population size, number of generations, crossover and mutation rates, random number generator seed, and the fitness function. It has been successfully synthesized and verified on a Xilinx Virtex II Pro Field programmable gate arrays device (xc2vp30-7ff896) with only 13% logic slice utilization, 1% block memory utilization for GA memory, and a clock speed of 50 MHz. The GA core has been used as a search engine for real-time adaptive healing but can be tailored to any given application by interfacing with the appropriate application-specific fitness evaluation module as well as the required storage memory and by programming the values of the desired GA parameters. The core is soft in nature i.e., a gate-level netlist is provided which can be readily integrated with the user's system. The performance of the GA core was tested using standard optimization test functions. In the hardware experiments, the proposed core either found the globally optimum solution or found a solution that was within 3.7% of the value of the globally optimal solution. The experimental test setup including the GA core achieved a speedup of around 5.16× over an analogous software implementation.","1089-778X;1089-778X;1941-0026","","10.1109/TEVC.2009.2025032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5299091","Evolvable hardware;field programmable gate arrays;genetic algorithm;IP core","Field programmable gate arrays;Genetic algorithms;Engines;Hardware;Random number generation;Programmable logic arrays;Testing;Genetic mutations;Logic devices;Clocks","field programmable gate arrays;genetic algorithms;real-time systems","customizable FPGA IP core;general-purpose genetic algorithm engine;hardware implementation;optimization engines;real-time application;GA parameter programmability;system architecture;multiple fitness functions;Xilinx Virtex II Pro;field programmable gate arrays device;real-time adaptive healing;application-specific fitness evaluation","","42","39","","","","","","IEEE","IEEE Journals & Magazines"
"Research on bleaching process for wheat straw","B. Liu; F. Liu; F. Wang; X. Zhu","Academy of Fine Arts, Northeast Normal University Jilin 130117, China; Academy of Fine Arts, Northeast Normal University, Jilin 130117, China; Material Science and Engineering College, Northeast Forestry University, Harbin 150040, China; Material Science and Engineering College, Northeast Forestry University, Harbin 150040, China","Proceedings of 2012 International Conference on Biobase Material Science and Engineering","","2012","","","32","35","The bleaching process of wheat straw was studied. NaHSO<sub>3</sub>as bleaching main agent, selected concentration, temperature, ultrasonic frequency, time mainly on the factor orthogonal experiment, borrow SPSS13.0 software for statistical analysis, the impact factor on the process factors were optimized by the range analysis and variance analysis, determine suitable wheat straw bleaching optimum process parameters and confirmatory testing. The results showed that: NaHSO<sub>3</sub>significant bleaching of wheat straw; The optimum process parameters: concentration 3 %, temperature 40 °C, frequency 45 KHz, time 60min.","","978-1-4673-2383-3978-1-4673-2382-6978-1-4673-2381","10.1109/BMSE.2012.6466174","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6466174","Wheat straw;Bleaching Process;Whiteness","Bleaching;Color;Materials;Time frequency analysis;Temperature;Educational institutions;Surface treatment","agricultural products;bleaching (materials processing);materials testing;statistical analysis","bleaching agent;ultrasonic frequency;bleaching concentration;bleaching temperature;bleaching time;factor orthogonal experiment;SPSS13.0 software;statistical analysis;process factors;variance analysis;range analysis;wheat straw bleaching optimum process parameters;temperature 40 degC;frequency 45 kHz;time 60 min","","","10","","","","","","IEEE","IEEE Conferences"
"Automated Reply to Students' Queries in E-Learning Environment Using Web-BOT","M. Farhan; I. M. Munwar; M. Aslam; A. M. M. Enriquez; A. Farooq; S. Tanveer; A. P. Mejia","NA; NA; NA; NA; NA; NA; NA","2012 11th Mexican International Conference on Artificial Intelligence","","2012","","","63","65","Electronic Learning (e-Learning) is used to educate people in these days. Using e-Learning, a number of world ranking universities are starting different courses for high school level to degree level and even at post graduate level through distance learning. This paper describes the best-known different machine learning techniques to boost up the e-Learning education standard and model. Comprehensively supervised and unsupervised techniques are described here for the e-Learning paradigm to auto reply of students' questions. Web bot is incorporated for the learning of students who are taking courses by remote mode. Due to the number of students enrolled in a particular course, student teacher interaction is a major challenge. Thus the solution is to train the Web based bot and make it available for the students' interaction 24/7. Main drawback of e-Learning environment is not frequent replies of student queries which we are going to cover by using web bot. The key demand is to deal with learning techniques but not fully automated. Training of the machine is performed on training data and validation is performed on test data set. Proposed idea of using the web bot in e-Learning is helpful to increase the learning curve of the students.","","978-1-4673-4731","10.1109/MICAI.2012.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6387216","Web bot;machine learning;artificial neural networks;supervised learning and support vector machine","Electronic learning;Training;Educational institutions;Machine learning;Training data","computer aided instruction;distance learning;Internet;learning (artificial intelligence);software agents","student query;e-learning environment;Web-BOT;electronic learning;distance learning;machine learning technique;unsupervised technique;student question reply;remote mode;student-teacher interaction;Web based bot training;student interaction;machine training;student learning curve","","","","","","","","","IEEE","IEEE Conferences"
"Implementation of vector-space online document retrieval system using open source technology","N. Jamil; N. A. Jamaludin; N. A. Rahman; N. Sabari","Department of Computer Science, Faculty of Computer and Mathematical Sciences, Shah Alam, Selangor, Malaysia; Department of Computer Science, Faculty of Computer and Mathematical Sciences, Shah Alam, Selangor, Malaysia; Department of Computer Science, Faculty of Computer and Mathematical Sciences, Shah Alam, Selangor, Malaysia; Department of Computer Science, Faculty of Computer and Mathematical Sciences, Shah Alam, Selangor, Malaysia","2011 IEEE Conference on Open Systems","","2011","","","395","399","Vector-space model is one of the most popular information retrieval models and it has been successfully implemented in retrieving many textual document collections. However, not many vector-space implementations employed open source technology. This paper discusses the integration of Oracle 10g Express edition database and Java language in realizing the development of an online document retrieval system for an Archive and Museum Unit in a public university. The retrieval system is tested on thirty publication documents and natural language queries entered managed to retrieve and ranked the documents successfully.","","978-1-61284-931-7978-1-61284-931-7978-1-61284-930","10.1109/ICOS.2011.6079228","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6079228","vector-space model;document retrieval;open source technology;digital archive information system","Databases;Adaptation models;Information retrieval;Computational modeling;Mathematical model;Vectors;Materials","document handling;electronic publishing;information retrieval;Java;public domain software","vector space online document retrieval system implementation;open source technology;information retrieval;textual document collections;vector space implementations;Oracle 10g Express edition database;Java language;publication documents;natural language queries","","","12","","","","","","IEEE","IEEE Conferences"
"FREM: A Fast Restart Mechanism for General Checkpoint/Restart","Y. Li; Z. Lan","Google, Inc; Illinois Institute of Technology, Chicago","IEEE Transactions on Computers","","2011","60","5","639","652","As failure rate keeps on increasing in large systems, applications running atop restart more frequently than ever. Existing research on checkpoint/restart mainly focuses on optimizing checkpoint operation, without paying much attention to the restart operation. As a result, application restart latency maybe substantial, which greatly threatens system dependability and performance. To attack the restart latency problem, in this paper, we present FREM, a fast restart mechanism for general checkpoint/restart protocols. By dynamically tracking the process data accesses after each checkpoint, FREM masks restart latency by overlapping application recovery with the retrieval of its checkpoint image. We have implemented FREM as a prototype system and tested it under Linux environments. Extensive experiments with real applications demonstrate that it can effectively reduce restart latency by over 50 percent on average, as compared to the conventional restart mechanisms.","0018-9340;1557-9956;2326-3814","","10.1109/TC.2010.129","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5483295","Fast restart;operating system;Linux;fault tolerance;high performance computing.","Protocols;Checkpointing;Optimization;Hardware;Kernel;Computed tomography;Complexity theory","checkpointing;fault tolerant computing;Linux;software fault tolerance","fast restart mechanism;checkpoint operation;restart operation;application restart latency;system dependability;system performance;checkpoint-restart protocols;Linux;fault tolerance technique;failure rate","","7","43","","","","","","IEEE","IEEE Journals & Magazines"
"Improving independence in the community for stroke survivors: The role of biomechanics visualisation in ankle-foot orthosis tuning","B. Carse; R. Bowers; B. Meadows; P. Rowe","Bioengineering, Department, University of Strathclyde, Glasgow, UK; National Centre for Prosthetics and Orthotics, University of Strathclyde, Glasgow, UK; WestMARC, Southern General Hospital, Glasgow, UK; Bioengineering, Department, University of Strathclyde, Glasgow, UK","2011 5th International Conference on Pervasive Computing Technologies for Healthcare (PervasiveHealth) and Workshops","","2011","","","399","403","One of the key priorities for stroke survivors in their rehabilitation process is regaining their ability to walk. Evidence has shown that provision of ankle-foot orthoses (AFOs) can have a positive impact on walking. This paper discusses the role of gait analysis in the provision of AFOs for stroke survivors. A discussion of the shortcomings of gait analysis techniques is included, with a description of how these might be overcome during the AFO tuning process through the ongoing development of data visualisation software. The design of a randomised controlled trial in conjunction with a series of qualitative measures is described, which will be used to test the efficacy of the visualisation software.","2153-1633;2153-1641","978-1-936968-15-2978-1-61284-767-2978-1-936968-14","10.4108/icst.pervasivehealth.2011.246128","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6038838","stroke;biomechanics;ankle-foot orthoses;gait analysis;visualisation;randomised controlled trial;rehabilitation","Data visualization;Tuning;Biomechanics;Three dimensional displays;Decision making;Software;Interviews","data visualisation;gait analysis;medical computing;orthotics;patient rehabilitation","stroke survivors;biomechanics visualisation;ankle-foot orthosis tuning;rehabilitation process;gait analysis techniques;AFO tuning process;data visualisation software","","","25","","","","","","IEEE","IEEE Conferences"
"A New Approach to Modeling Emotions and Their Use on a Decision-Making System for Artificial Agents","M. A. Salichs; M. Malfaz","Carlos III University, Leganes; Carlos III University, Leganes","IEEE Transactions on Affective Computing","","2012","3","1","56","68","In this paper, a new approach to the generation and the role of artificial emotions in the decision-making process of autonomous agents (physical and virtual) is presented. The proposed decision-making system is biologically inspired and it is based on drives, motivations, and emotions. The agent has certain needs or drives that must be within a certain range, and motivations are understood as what moves the agent to satisfy a drive. Considering that the well-being of the agent is a function of its drives, the goal of the agent is to optimize it. Currently, the implemented artificial emotions are happiness, sadness, and fear. The novelties of our approach are, on one hand, that the generation method and the role of each of the artificial emotions are not defined as a whole, as most authors do. Each artificial emotion is treated separately. On the other hand, in the proposed system it is not mandatory to predefine either the situations that must release any artificial emotion or the actions that must be executed in each case. Both the emotional releaser and the actions can be learned by the agent, as happens on some occasions in nature, based on its own experience. In order to test the decision-making process, it has been implemented on virtual agents (software entities) living in a simple virtual environment. The results presented in this paper correspond to the implementation of the decision-making system on an agent whose main goal is to learn from scratch how to behave in order to maximize its well-being by satisfying its drives or needs. The learning process, as shown by the experiments, produces very natural results. The usefulness of the artificial emotions in the decision-making system is proven by making the same experiments with and without artificial emotions, and then comparing the performance of the agent.","1949-3045;2371-9850","","10.1109/T-AFFC.2011.32","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6035666","Artificial emotions;decision-making system;motivations;autonomy;learning.","Decision making;Humans;Monitoring;Robot kinematics;Animals;Appraisal","decision making;intelligent robots;learning (artificial intelligence);software agents","emotion modeling;decision-making system;artificial agent;artificial emotion;autonomous agent;drive;motivation;happiness;sadness;fear;generation method;virtual agent;software entities;virtual environment;learning process;robot","","22","40","","","","","","IEEE","IEEE Journals & Magazines"
"Enhancing retrieval and novelty detection for arabic text using sentence level information pattern","E. AL-Shdaifat; M. N. Al-Kabi; E. Al-Shawakfa; A. H. Wahbeh","Software Engineering Dept., IT &amp; CS Faculty, The Hashemite University, 13115 Zarqa - Jordan; CIS Department, IT &amp; CS Faculty, Yarmouk University, 21163 Irbid - Jordan; CIS Department, IT &amp; CS Faculty, Yarmouk University, 21163 Irbid - Jordan; College of Business and Information System, Dakota State University, Madison, SD, USA","2012 International Conference on Computer, Information and Telecommunication Systems (CITS)","","2012","","","1","4","Novelty detection is already used in many Natural Processing Language (NLP) applications, such as information retrieval systems, Web search engines, text summarization, question answering systems...etc. This study aims to detect novel Arabic sentence level information patterns. The Length Adjusted (LA) model is based on sentence level information patterns is used, which depends on the sentence length. Test results show a significant improvement in the performance of novelty detection for Arabic texts in terms of precision at top ranks.","","978-1-4673-1550-0978-1-4673-1549-4978-1-4673-1548","10.1109/CITS.2012.6220389","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6220389","Novelty detection;information patterns;Information retrieval","Research and development;Redundancy;Educational institutions;Information filtering;Materials;Event detection","information retrieval;natural language processing;search engines;text analysis","enhancing retrieval;novelty detection;arabic text;sentence level information pattern;natural processing language;NLP;information retrieval systems;Web search engines;text summarization;question answering systems;Arabic sentence level information patterns;length adjusted model;LA","","","13","","","","","","IEEE","IEEE Conferences"
"Analysis and research of automotive trapezoid synchronous belt's fatigue life based on RecurDyn","Li Zhanguo; Jiangming; Li Jiaxing","Changchun University, Jilin Province, China; Changchun University of Science and Technology, Jilin Province, China; Changchun University of Science and Technology, Jilin Province, China","2010 International Conference on Computer, Mechatronics, Control and Electronic Engineering","","2010","2","","193","195","The dynamic simulation model of synchronous belt meshing transmission is established in dynamics software “RecurDyn” applying the theory of MFBD (Multi-Flexible Body Dynamics), which analyzed the contact force on the working surfaces and stress distribution during the meshing transmission. The Finite Element Analysis Method was used to make the synchronous belt model meshed in Femap and investigated the belt's teeth stress distribution after the model was imported into RecurDyn. A new design of synchronous belt's and pulley's tooth profiles for improving the transmission capacity of the automotive synchronous belt was proposed, in order to discover better materials to increase the fatigue life of the belt by optimization of the geometry of tooth profiles.","2159-6026;2159-6034","978-1-4244-7958-0978-1-4244-7957-3978-1-4244-7955","10.1109/CMCE.2010.5609713","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5609713","based on RecurDyn;Synchronous Belt;MFBD;Fatigue Life;Simulation","Pulleys","automotive components;belts;fatigue testing;mechanical contact;mesh generation;pulleys","automotive trapezoid synchronous belt;fatigue life;RecurDyn;dynamic simulation model;meshing transmission;multiflexible body dynamic;contact force;stress distribution;finite element analysis method;Femap;pulley tooth","","","3","","","","","","IEEE","IEEE Conferences"
"Enhancing the contingency performance of HELENSÄHKÖVERKKO OY 110 KV NETWORK by optimal installation of UPFC based on Genetics Algorithm","A. M. Othman; M. Lehtonen; M. M. El-Arini","Power Systems Lab., Electrical Engineering Dept., Helsinki University of Technology (TKK), Finland; Electrical Engineering Department, Helsinki University of Technology (TKK), Finland; Electronics Department and Vice-Dean of Faculty of Engineering, Zagazig University, Egypt","IEEE PES General Meeting","","2010","","","1","8","Unified Power Flow Controller (UPFC) has a promising effect on controlling the characteristics of the power system operation. During the contingency outage cases, the transmission lines are overloaded and the buses are subjected to voltage violations. Unified Power Flow Controller (UPFC) has both shunt and series controller effect inside its frame. This option gives the UPFC the power to control the voltage profile and the transmission lines flow simultaneously. In this paper, we use the Genetics Algorithm (GA) to find the optimal location and the optimal settings of UPFC to improve the performance of the power system at the contingency of the transmission line outage. The proposed technique will be tested on the IEEE 6-bus system to show the validity of the technique. This procedure is proposed for being applied on the Finnish network, Helsinki HELENSÄHKÖVERKKO OY 110 KV NETWORK. The simulations will be achieved until operating conditions of Year 2020.","1932-5517;1944-9925;1944-9925","978-1-4244-6551-4978-1-4244-6549-1978-1-4244-6550-7978-1-4244-8357","10.1109/PES.2010.5588096","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5588096","Contingency Analysis;Genetic Algorithm (GA);Optimal Setting;Overloading;Ranking Process;Unified Power Flow controller (UPFC)","Computer aided software engineering;Power transmission lines;Reactive power;Load flow;Mathematical model;Equations;Indexes","genetic algorithms;load flow control;power control;power transmission control;power transmission lines;voltage control","contingency performance;optimal installation;UPFC;genetic algorithm;unified power flow controller;power system operation;transmission line flow control;voltage violations;shunt controller effect;series controller effect;voltage profile control;optimal location;IEEE 6-bus system;Finnish network;Helsinki HELENSÄHKÖVERKKO OY network","","2","16","","","","","","IEEE","IEEE Conferences"
"A low voltage sensorless Switched Reluctance Motor drive using flux linkage method","T. Koblara; C. Sorandaru; S. Musuroi; M. Svoboda","Polytechnic University of Tirana, Faculty of Electrical Engineering, ALBANIA; “Politehnica” University of Timisoara, Faculty of Electrical and Power Engineering, ROMANIA; “Politehnica” University of Timisoara, Faculty of Electrical and Power Engineering, ROMANIA; “Politehnica” University of Timisoara, Faculty of Electrical and Power Engineering, ROMANIA","2010 12th International Conference on Optimization of Electrical and Electronic Equipment","","2010","","","665","672","The inherent vulnerability to mechanical failures, extra cost, and size associated with external position sensors such as optical encoders and Hall sensors has motivated many researchers to develop sensorless control techniques for SRM drives. In this paper a flux linkage method and dual layer controller is developed to estimate rotor position and speed of a low-voltage Switched Reluctance Motor (SRM) drive. The basic concept of this application is that of a sensorless speed closed loop with an inner current loop using flux linkage position estimation. The voltage drop on the power devices is more significant in case of low voltage then in case of the high voltage drive. This voltage drop needs to be considered in the algorithm. Simulations and real-time experimental results given in this paper shows that the proposed position estimation method can provide accurate and continuous position information over a wide range of speeds, even in low speed applications. To ensure a sure operation, a start up algorithm is also included. The proposed method was implemented and tested by using a digital signal processor 56F807EVM from Freescale Semiconductor Company and an 8/6 switched reluctance motor coupled with a brushless DC motor as load. The software has been developed in C language.","1842-0133;1842-0133","978-1-4244-7020-4978-1-4244-7019-8978-1-4244-7018","10.1109/OPTIM.2010.5510366","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5510366","","Low voltage;Reluctance motors;Couplings;Optical sensors;Mechanical sensors;Sensorless control;Reluctance machines;Signal processing algorithms;Costs;Rotors","brushless DC motors;reluctance motor drives;sensorless machine control","low voltage sensorless switched reluctance motor drive;flux linkage method;Hall sensors;optical encoders;external position sensors;rotor speed estimation;sensorless speed closed loop;flux linkage position estimation;digital signal processor 56F807EVM;Freescale Semiconductor Company;brushless DC motor;C language;sensorless control techniques","","7","13","","","","","","IEEE","IEEE Conferences"
"Genetic and hybrid shuffled frog leaping algorithms for solving a 2-stage model for a hub covering location network","M. Mohammadi; R. Tavakkoli-Moghaddam; H. Rostami","Department of Industrial Engineering, University of Tehran, Tehran, Iran; Department of Industrial Engineering, University of Tehran, Tehran, Iran; Department of Industrial Engineering, University of Tabriz, Tabriz, Iran","2011 Fourth International Conference on Modeling, Simulation and Applied Optimization","","2011","","","1","6","A hub location problem appears in a variety of applications, such as airline systems, cargo delivery systems and telecommunication network design. In this paper, we consider a central mine and a number of hubs (e.g., factories) connected to a number of nodes (e.g., shops or customers) in a network. First we design the hub network, and then schedule the raw materials transportation from the central mine to hubs (i.e., factories). In this case, we consider only one transportation system regarded as single machine scheduling. Furthermore, we use this hub network to solve the scheduling model. In this paper, we study the single allocation hub covering problem with capacity constraints, known as capacitated single allocation hub covering location problem (CSAHCLP), and then present a mixed-integer programming (MIP) model. In addition, we propose an efficient genetic algorithm and a hybrid shuffled frog leaping algorithm to solve the first and second stages of our presented model, respectively. A number of test problems are solved by the forgoing algorithms and the related results are compared with those results obtained by the Lingo software.","","978-1-4577-0005-7978-1-4577-0003-3978-1-4577-0004","10.1109/ICMSAO.2011.5775615","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5775615","Hub covering location problem;Network design;Single machine scheduling;Genetic algorithm;Shuffled frog leaping algorithm","Resource management;Algorithm design and analysis;Single machine scheduling;Transportation;Genetic algorithms;Mathematical model;Europe","facility location;genetic algorithms;integer programming;single machine scheduling;transportation","genetic algorithm;hybrid shuffled frog leaping algorithm;2-stage model;hub covering location network;hub location problem;airline systems;cargo delivery systems;telecommunication network design;raw materials transportation;transportation system;single machine scheduling;scheduling model;single allocation hub covering problem;capacity constraints;capacitated single allocation hub covering location problem;mixed-integer programming","","","35","","","","","","IEEE","IEEE Conferences"
"Forecasting High Frequency Financial Time Series Using Parallel FFN with CUDA and ZeroMQ","P. Arce; C. Maureira; R. Bonvallet; C. Fernández","NA; NA; NA; NA","2012 9th Asia-Pacific Symposium on Information and Telecommunication Technologies (APSITT)","","2012","","","1","5","Feed forward neural networks (FFNs) are powerful data-modelling tools that have been used in many fields of science. Specifically in financial applications, due to the number of factors affecting the market, models with a large quantity of input features, hidden and output neurons can be obtained. In financial problems, the response time is crucial and it is necessary to have faster applications. Most of the current applications have been implemented as non-parallel software running on serial processors. In this paper we present a parallel implementation of a FFN using GPU in order to reduce response time when a new data arrives. The problem can be conveniently represented by matrix operations implemented using the CUBLAS library. It provides highly optimized linear algebra routines that take advantage of the hardware features of the GPU. The algorithm was developed in C++ and CUDA and all the input features were received using the ZeroMQ library, which was also used to publish the output features. ZeroMQ is an abstraction over system sockets that allow chunks of data to be efficiently sent therefore minimizing the overhead and system calls. The CUDA implementation was tested on a compute server with an NVIDIA M2050 GPU and an Intel Xeon X5650 2.67GHz CPU, on neural networks of increasing sizes. After comparing against a straightforward 24-thread CPU implementation using MKL, experiments show that, while still slower for small FFNs, for 1000 × 1000 × 1000 networks the GPU already outperforms the CPU.","","978-4-88552-265-9978-1-4673-2434","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6379709","","Graphics processing units;Neurons;Neural networks;Libraries;Vectors;Central Processing Unit;Training","feedforward neural nets;financial data processing;linear algebra;parallel architectures;software libraries;time series","high frequency financial time series forecasting;parallel feed forward neural network;CUDA;data-modelling tool;response time reduction;matrix operation;CUBLAS library;linear algebra routine optimisation;C++;ZeroMQ library;NVIDIA M2050 GPU;Intel Xeon X5650 CPU;frequency 2.67 GHz","","1","25","","","","","","IEEE","IEEE Conferences"
"Optimal distributed generation allocation in distribution systems employing modified artificial bee colony algorithm to reduce losses and improve voltage profile","I. Hussain; A. Kumar Roy","Electrical Engg. Dept, Royal Group of Institutions (RGI), Guwahati, Assam, India; Electrical Engg. Dept National Institute of Technology (NIT), Silchar, Assam, India","IEEE-International Conference On Advances In Engineering, Science And Management (ICAESM -2012)","","2012","","","565","570","One of the modern and important techniques in electrical distribution systems is to solve the networks problem of service availability, high loss and low voltage stability by accommodating small scaled de-centralized generating units in these networks, which is known as distributed generation (DG).This paper presents a new methodology using a new population based meta heuristic approach namely Modified Artificial Bee Colony algorithm (ABC) for the placement of Distributed Generators (DG) in the radial distribution systems to reduce the real power losses and to improves supply quality and reliability, reduces green house effects, improves voltage profile in radial distribution system, reduces line loss and environment impact. The modification is in the neighbouring search of the artificial bee colony (ABC) algorithm. The suggested method is programmed under MATLAB software and is tested on IEEE 33-bus test system and the results are presented. The method is found to be effective and applicable for practical network.","","978-81-909042-2-3978-1-4673-0213-5INAVLID IS","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6216063","Distributed Generation;Power Loss Reduction;Artificial Bee Colony Algorithm","Heuristic algorithms;Reliability;Convergence","ant colony optimisation;distributed power generation;losses;power supply quality;stability","optimal distributed generation allocation;modified artificial bee colony algorithm;voltage profile improvement;loss reduction;low voltage stability;service availability;small scaled decentralized generating units;ABC;DG;supply quality improvement;radial distribution system;Matlab algorithm;IEEE 33-bus test system","","1","10","","","","","","IEEE","IEEE Conferences"
"On applying the method of “system of systems” in robustness analysis and autonomous control of dynamics-aware internet architecture","J. Liu; H. Nishimura; H. Umehara","Advanced ICT Research Institute, National Institute of Information and Communications Technology, Kobe, Japan; Graduate School of Applied, Informatics University of Hyogo, Kobe, Japan; Advanced ICT Research Institute, National Institute of Information and Communications Technology, Kobe, Japan","2012 IEEE International Systems Conference SysCon 2012","","2012","","","1","6","Owing to the advantages of the “system of systems” that has been widely used in enormous fields ranging from science to engineering, in this paper the method of the “system of systems” is applied to the analysis of the network architecture of the Internet and the corresponding controller of the Internet flow where the robustness is one of the most crucial factors for the performance analysis of the autonomous architecture of the Internet. In our research, the underlying network architecture of the internet is formalized by a model of a generalized networked control system (NCS) where the autonomous distributed information processing units are constrained by the end-to-end rule of the Internet. With the formalization, the factors of nonlinear controllers, communication channels, and communication protocols are interacted in the model based on the principles of the “system of systems”. Accordingly, the technology of “software systems engineering” is applied to design and test the new algorithm proposed by us for the Internet flow control, which is based on the nonlinear filtering mechanism of the TCP congestion control, to provide a new Internet flow control protocol. The higher stability of the nonlinear controller for the Internet flows is observed in the experiments compared with the linear ones. The robustness of the Internet flow control is achieved even under the unexpected dynamical environment that causes the uncertainty of bandwidth, delay, and loss of the communication. In order to optimize the design of the network architecture, the corresponding analysis of the complex behavior of the flow dynamics of the Internet has been carried out. The significance of the results shows that a new design principle of the dynamics-aware protocols for the future Internet can be innovated in terms of the designed mathematical model of the “system of systems”. From our practice, it can be inferred that the performance evaluation of the Internet flow control under the cross layers can be extended to reconfigurable overlay networks.","","978-1-4673-0750-5978-1-4673-0748-2978-1-4673-0749","10.1109/SysCon.2012.6189453","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6189453","application of the “system of systems”;nonlinear dynamics;signal processing for communications;Internet protocol;software engineering","Internet;Computer architecture;Modeling;Robustness;Protocols;Steady-state;Delay","computer network performance evaluation;control engineering computing;distributed parameter systems;Internet;networked control systems;nonlinear control systems;overlay networks;reconfigurable architectures;telecommunication channels;telecommunication congestion control;transport protocols","system-of-systems;robustness analysis;autonomous control;dynamics-aware Internet autonomous architecture;network architecture analysis;Internet flow control protocol;performance analysis;generalized networked control system;NCS;autonomous distributed information processing;end-to-end rule;nonlinear controllers;communication channels;communication protocols;software systems engineering;nonlinear filtering mechanism;TCP congestion control;bandwidth uncertainty;delay uncertainty;communication loss;flow dynamics;design principle;performance evaluation;reconfigurable overlay networks","","3","16","","","","","","IEEE","IEEE Conferences"
"Parallel Protein Docking Tool","I. Sović; N. Antulov-Fantulin; I. Čanadi; M. Piškorec; M. Šikić","Faculty of Electrical Engineering and Computing, University of Zagreb, Unska 3, 10000, Croatia; Faculty of Electrical Engineering and Computing, University of Zagreb, Unska 3, 10000, Croatia; Faculty of Electrical Engineering and Computing, University of Zagreb, Unska 3, 10000, Croatia; Faculty of Electrical Engineering and Computing, University of Zagreb, Unska 3, 10000, Croatia; Faculty of Electrical Engineering and Computing, University of Zagreb, Unska 3, 10000, Croatia","The 33rd International Convention MIPRO","","2010","","","1333","1338","Parallel Protein Docking Tool (PPDT) system is performing a shape based process of protein docking using spherical harmonics. The software is made from scratch in C++. Input data are two PDB (Protein Data Bank) files and desired docking parameters. The PDB files hold information about protein's atom coordinates in three dimensional space. Protein structures are transformed with spherical functions which allow fast rotation and translation of 3D shapes in space. By translation and rotation (with a given resolution of angles and distance) the surface complementarity is tested for each orientation of two proteins in 3D space. The orientations are then ranked according to the given scores. The output PDB file now holds orientations with best scores ready for visualization. As a part of the system a tool for visualization of molecules was developed. It's purpose is to provide a visualization of molecular structure in all phases of protein docking process. The visualizer also allows interactive translation and rotation of input proteins and evaluation of their complementarity in real time. Although docking tools that use spherical harmonics already exist, we implemented a parallel version using MPI (message passing interface). In this implementation any worker MPI process independently makes docking search on six dimensional subspace determined by parameters in its task. PPDT has high scalability, modularity, variable granularity of tasks and provides significant speedup comparing to a non-parallel version of software.","","978-9-5323-3050-2978-1-4244-7763-0978-953-233-058","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5533683","protein docking;spherical harmonics;PDB;MPI","Visualization;Protein engineering;Testing;Shape;Concurrent computing;Message passing;Scalability;Geometry;Electrostatics;Performance analysis","biology computing;C++ language;data visualisation;message passing;proteins","parallel protein docking tool system;shape based process;spherical harmonics;C++;protein data bank files;protein atom coordinates;three dimensional space;protein structures;3D shapes;3D space;PDB file;molecule visualization tool;interactive translation;parallel version;MPI;message passing interface;six dimensional subspace;task variable granularity;nonparallel software version","","","8","","","","","","IEEE","IEEE Conferences"
"Reinforcement Learning for Controlling a Coupled Tank System Based on the Scheduling of Different Controllers","A. A. R. Diniz; P. R. M. Pires; J. D. d. Melo; A. D. D. Neto; A. J. J. L. Filho; S. M. Kanazava","NA; NA; NA; NA; NA; NA","2010 Eleventh Brazilian Symposium on Neural Networks","","2010","","","212","216","Reinforcement Learning has been an approach successfully applied for solving several problems available in literature. It is usually employed for solving complex problems, such as the ones involving systems with incomplete knowledge, time variant systems, non-linear systems, etc., but it does not mean that it cannot be applied for solving simple problems. Therefore, this paper proposes an alternative application, where RL could be applied to switch among controllers with a fixed tuning, in a system with a known non-linear dynamics, aiming to optimize its time response. It was shown, after the online training and test of the RL agent that it could take advantage of the best characteristics of the available controllers to improve the response of the coupled tank system.","2375-0235;1522-4899;1522-4899","978-1-4244-8391-4978-0-7695-4210","10.1109/SBRN.2010.44","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5715239","reinforcement learning;Q-learning;intelligent control","Training;Switches;Learning;Time factors;Indexes;Electronic mail","control system synthesis;learning (artificial intelligence);nonlinear dynamical systems;self-adjusting systems;software agents;tanks (containers)","reinforcement learning;coupled tank system;controller scheduling;fixed tuning controller;nonlinear dynamics;time response;RL agent;control system design","","4","6","","","","","","IEEE","IEEE Conferences"
"The impact of memory and processor in determining the performance of programs","A. Dika; B. Prevalla","Faculty of Electrical &amp; Computer, Engineering, University of Pristina, Kosovo; Faculty of Information and Communication Technologies FON University, Macedonia","2010 IEEE 26-th Convention of Electrical and Electronics Engineers in Israel","","2010","","","000015","000018","Performance analysis of a program means executing a program on computers with different hardware resources and optimization of their running time. It is very useful because it provides information of how to distribute efforts and resources in order to ensure greater efficiency and to keep developers focused on the essential goals of the program. In this paper, the main attention on determining the performance of a program is focused in finding the impact of memory size and the speed of processor. To that end, as an example for analysing the performance of a program and for seeing the dependance of the execution time of a program we will study some examples of programs for sorting. Sorting programs are chosen to analyse because many scientist consider sorting as one of the most crucial problems in the study of algorithms and programs. The sorting algorithm are implemented in C++ and tested in 3 machines with different configurations and with different input values, to see the changes of running time depending on many circumstances. The results are presented in tabular and graphical format. Also they are analysed if the crucial factor for an algorithm to run faster or slower is the amount of CPU used or is the amount of RAM memory used.","","978-1-4244-8682-3978-1-4244-8681-6978-1-4244-8680","10.1109/EEEI.2010.5661956","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5661956","Time Complexity;Sorting Algorithms;Performance Analysis","Algorithm design and analysis;Sorting;Computers;Random access memory;Prediction algorithms;Analytical models;Complexity theory","C++ language;microprocessor chips;random-access storage;software performance evaluation;sorting","performance analysis;program performance;memory size;processor speed;sorting program;C++;RAM memory;CPU","","","7","","","","","","IEEE","IEEE Conferences"
"Run-time resolution of uncertainty","K. Welsh; P. Sawyer; N. Bencomo","School of Computing &amp; Communications, Lancaster University, Lancaster, UK; School of Computing &amp; Communications, Lancaster University, Lancaster, UK; INRIA Paris - Rocquencourt, 78153 Le Chesnay, France","2011 IEEE 19th International Requirements Engineering Conference","","2011","","","355","356","Requirements awareness should help optimize requirements satisfaction when factors that were uncertain at design time are resolved at runtime. We use the notion of claims to model assumptions that cannot be verified with confidence at design time. By monitoring claims at runtime, their veracity can be tested. If falsified, the effect of claim negation can be propagated to the system's goal model and an alternative means of goal realization selected automatically, allowing the dynamic adaptation of the system to the prevailing environmental context.","2332-6441;1090-705X;1090-705X","978-1-4577-0924-1978-1-4577-0921-0978-1-4577-0923","10.1109/RE.2011.6051673","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6051673","self adaptive systems;requirements models;goals","Adaptation models;Hazards;Cleaning;Cognition;Runtime;Monitoring;Uncertainty","formal specification;software engineering","runtime resolution;uncertainty;requirements awareness;requirement satisfaction;dynamic adaptation;environmental context","","4","4","","","","","","IEEE","IEEE Conferences"
"Application of lightweight MVC-like structure in PHP","Guanhua Wang","School of Computer Science &amp; Engineering, Southeast University, Nanjing, China","2011 International Conference on Business Management and Electronic Information","","2011","2","","74","77","This paper introduced a lightweight Model-View-Controller-liked (MVC-like) format of PHP programming that combined the traditional PHP programming format with the MVC architecture of PHP and set the advantages of both to optimize the programming of PHP. The idea to realize the lightweight MVC-liked format of PHP is adjustment based on the traditional PHP programming format which was added in the MVC architecture. And through using the abstracted simple document of classes to fulfill the corresponding model, view and controller function, this new format achieved combination of class package and function package with the subject. All the algorithms proposed in this paper were tested and verified to be feasible. Besides, the analysis and judgment of this new format's prospect was done in the end.","","978-1-61284-109-0978-1-61284-108-3978-1-61284-107","10.1109/ICBMEI.2011.5917846","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5917846","PHP;MVC;lightweight;OOP","Programming;HTML;Object oriented modeling;Computer architecture;Engines;Servers","object-oriented languages;object-oriented programming;software architecture","lightweight MVC-like structure application;lightweight model view controller-like format;PHP programming;function package;class package","","2","5","","","","","","IEEE","IEEE Conferences"
"Order-lot pegging heuristics for minimizing total tardiness in a semiconductor wafer fabrication facility","J. Kim; S. Lim; S. Shim; S. Choi","Department of Industrial and Management Engineering, University of Incheon, Korea; Department of Industrial and Management Engineering, Sungkyul University, Anyang, Korea; Department of Business Administration, Hanbat National University, Daejeon, Korea; Department of Business Administration, Kyonggi University, Seoul, Korea","2010 IEEE International Conference on Industrial Engineering and Engineering Management","","2010","","","1224","1229","We consider a problem of order-lot pegging in a semiconductor wafer fabrication facility. In the problem, we determine assignments of wafers in lots to orders and plans for input release of wafers into wafer fabrication facility with the objective of minimizing the total tardiness of orders over finite time horizon. The problem is formulated as a mixed integer linear program. To tackle industrial-sized problems, we develop six heuristic algorithms based on the earliest due date rule. The test results on randomly generated problems show that the suggested algorithms give better solutions than an optimization method of a commercial software package within a reasonable computation time.","2157-362X;2157-3611;2157-3611","978-1-4244-8503-1978-1-4244-8501-7978-1-4244-8502","10.1109/IEEM.2010.5674360","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5674360","Order-lot pegging;heuristic;semiconductor wafer fabrication","Heuristic algorithms;Fabrication;Software algorithms;Classification algorithms;Optimization;Job shop scheduling","integer programming;integrated circuit manufacture;linear programming;order processing;production management;semiconductor industry","order lot pegging heuristics;total tardiness;semiconductor wafer fabrication facility;finite time horizon;mixed integer linear program;industrial sized problem;heuristic algorithms;earliest due date rule","","1","10","","","","","","IEEE","IEEE Conferences"
"Dynamic Modeling and Simulation Analysis of the Cushioning System of the Impact Roller Based on ADAMS","Z. Jinning; Z. Hong; L. Jie","NA; NA; NA","2011 Third International Conference on Measuring Technology and Mechatronics Automation","","2011","1","","1125","1128","The mathematical model of cushioning system is established on the basis of analyzing the structure and work characteristics of the impact roller. The assembly model of the whole machine was developed in UG software utilizing the multi-body dynamics theory and imported into ADAMS that is the integrated mechanical system simulation software. The simulation model was created including tire and road model and was simulated dynamically in ADAMS under different working conditions. By simulation, the horizontal impact load is acquired which acted on the traction vehicle. The simulation results are compared with the testing data, which validates the correctness of the dynamic simulation model. It is simulated and analyzed how the model parameters influence the traction load. The results suggest that the dynamic model and road model can simulate working conditions of the impact roller and help engineering designers optimize the parameters.","2157-1473;2157-1481","978-1-4244-9010","10.1109/ICMTMA.2011.281","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5720984","Impact Roller;Cushioning System;ADAMS;Modeling;Dynamic Simulation","Load modeling;Wheels;Roads;Damping;Force;Mathematical model;Tires","impact (mechanical);mechanical engineering computing;rollers (machinery);vehicle dynamics;vibration isolation;virtual prototyping","cushioning system;impact roller;ADAMS;machine assembly model;UG software;multibody dynamics theory;integrated mechanical system simulation software;traction vehicle;compacting machine;impact load;virtual prototype","","","8","","","","","","IEEE","IEEE Conferences"
"Resolution-Level-Controlled WM Inverter for PMG-Based Wind Energy Conversion System","S. A. Saleh; R. Ahshan","Department of Electrical and Computer Engineering, University of New Brunswick, Fredericton, Canada; Engineering Technology Center, College of the North Atlantic, St. John's, Canada","IEEE Transactions on Industry Applications","","2012","48","2","750","763","This paper presents the development, implementation, and performance testing of a permanent-magnet generator (PMG)-based wind energy conversion system (WECS) for grid-connected applications. The grid-connected operation is constructed using the new three-phase resolution-level-controlled wavelet-modulated inverter. The output of the wind generator is fed to a switch-mode ac-dc converter, where the dc voltage is controlled to ensure extracting power from the wind under variable wind speed conditions in order to meet the grid active and reactive power demands. The resolution-level control of the grid-side inverter is designed as a current controller for adjusting active and reactive powers delivered to the grid. The proposed WECS is implemented both in software and hardware for performance testing on a direct-drive 6-kW laboratory PMG operated with variable speed. Test results demonstrate that an accurate control of the dc voltage on the generator side ensures adjusting the generator speed to extract power at each wind speed in order to meet the demand active and reactive power delivery to the grid. Also, test results show significant abilities of the resolution-level controller to initiate fast and accurate adjustments in the active and reactive powers delivered to the grid in order to follow any changes in their demand values under variable wind speed.","0093-9994;1939-9367","","10.1109/TIA.2012.2182750","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6122504","DC–AC power conversion;permanent-magnet generators (PMGs);proportional–integral (PI) control and digital signal processors;switch-mode ac–dc converters;wavelet transforms","Inverters;Wind speed;Wind turbines;Switches;Rotors;Generators;Optimized production technology","invertors;permanent magnet generators;wind power","three phase resolution level controlled wavelength modulated inverter;performance testing;permanent magnet generator based wind energy conversion system;grid connected applications;wind generator;switch mode ac dc converter;wind under variable wind speed conditions;grid active;reactive power demands;grid side inverter;current controller","","16","31","","","","","","IEEE","IEEE Journals & Magazines"
"Computer aided design and simulation analysis for matsune shovel of stubble harvester","Ma Liuxuan; Chu Xuhong; Li Xiaohai; Wang Junfa; Qiu Congyu","College of Mechanical Engineering, Jiamusi University, CHINA; College of Mechanical Engineering, Jiamusi University, CHINA; College of Mechanical Engineering, Jiamusi University, CHINA; College of Mechanical Engineering, Jiamusi University, CHINA; Research and Development department, Panasonic Motor Research and Development Center, Zhuhai, CHINA","2010 International Conference on Mechanic Automation and Control Engineering","","2010","","","381","383","At present, three-dimensional parametric CAD technology is used widely in the field of agricultural mechanical design. Based on the analysis of matsune shovel's structural features for the stubble harvester machine, matsune shovel solid model was drawn by using three-dimensional CAD software Pro/E. Matsune shovel's static analysis was done by finite element analysis software ANSYS Workbench, and static strain distribution had been known. The results of the analysis can not only provide a theoretical basis for optimizing the structure of matsune shovel and improving the performance, but also reduce the cumbersome design process of the prototype testing and amending, and improved the design efficiency.","","978-1-4244-7739-5978-1-4244-7737-1978-1-4244-7738","10.1109/MACE.2010.5535593","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5535593","CAD;analysis;matsune shovel;stubble","Computational modeling;Analytical models;Computer simulation;Design automation;Solid modeling;Finite element methods;Capacitive sensors;Performance analysis;Design optimization;Process design","CAD;structural engineering","computer aided design;simulation analysis;stubble harvester;CAD technology;agricultural mechanical design;matsune shovel structural feature;finite element analysis;ANSYS Workbench;static strain distribution","","1","7","","","","","","IEEE","IEEE Conferences"
"An algorithm for calculating the hypervolume contribution of a set","Xiuling Zhou; Ping Guo; C. L. P. Chen","Image Processing and Pattern Recognition Lab, Beijing Normal University, 100875, China; Image Processing and Pattern Recognition Lab, Beijing Normal University, 100875, China; The Faculty of Science and Technology, University of Macau, China","World Automation Congress 2012","","2012","","","439","443","The reliability model can be optimized with a multi-objective optimization algorithm, while hypervolume-based multi-objective evolutionary algorithms (MOEAs) have been shown to produce better results for multi-objective problem in practice. When hypervolume is used in some MOEAs as archiving strategy, diversity mechanism or selection criterion to guide the search, it is necessary to determine which subset contributes the least hypervolume contribution. Few algorithms have been designed for this purpose. In this paper a new algorithm based on HSO (hypervolume by slicing objective) is proposed for calculating the exclusive hypervolume contributions of each subset to the whole nondominated set directly for small dimension. The new algorithm is composed of two parts: the algorithm SHSO (set hypervolume contribution by slicing objective) and the algorithm SHSO*. SHSO is used to calculate the exclusive hypervolume contribution of a subset to the whole nondominated set. SHSO* is applied to select the subset which contributes the least hypervolume contribution by repeated application of SHSO. Compared with HSO adopted for calculating the exclusive hypervolume contribution, SHSO* outperforms HSO for all of the test fronts with small dimension.","2154-4824;2154-4824","978-1-889334-47-9978-1-4673-4497-5978-1-889335-47","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6321095","Reliability model;Multi-objective evolutionary algorithms;Hypervolume contribution","Optimization;Evolutionary computation;Vectors;Algorithm design and analysis;Software reliability;Measurement","","","","","25","","","","","","IEEE","IEEE Conferences"
"Math Spotting: Retrieving Math in Technical Documents Using Handwritten Query Images","R. Zanibbi; L. Yu","NA; NA","2011 International Conference on Document Analysis and Recognition","","2011","","","446","451","A method for locating mathematical expressions in document images without the use of optical character recognition is presented. An index of document regions is produced from recursive X-Y trees produced for each page in the corpus. Queries are provided as images of handwritten expressions, for which an X-Y tree is computed. During retrieval, the query is looked up in the document region index using features of its X-Y tree, producing a set of candidate regions. Candidate regions are ranked by the similarity of vertical pixel projections in their upper and lower halves with those of the query image, as computed using Dynamic Time Warping of the image columns. In an experiment, ten participants each wrote twenty queries from a 200-page corpus. On average, the top-10 retrieval candidates included a candidate covering 43.3% of the test query image (σ = 14.0), with the correct page being returned between 30.0% and 85.0% of the time across participants (μ = 63.2%, σ = 14.9%). When testing using the original query images, 90.0% of the queries were retrieved correctly.","2379-2140;1520-5363;1520-5363","978-1-4577-1350-7978-0-7695-4520","10.1109/ICDAR.2011.96","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065351","Mathematical Information Retrieval;Math Recognition;Keyword Spotting","Vegetation;Measurement;Training;Optical character recognition software;Indexing;Testing","document handling;handwriting recognition;image retrieval;mathematics computing","math spotting;math retrieval;technical documents;handwritten query images;mathematical expressions;document images;X-Y trees;vertical pixel projections;dynamic time warping","","8","28","","","","","","IEEE","IEEE Conferences"
"Context Adaptive Lagrange Multiplier (CALM) for Rate-Distortion Optimal Motion Estimation in Video Coding","J. Zhang; X. Yi; N. Ling; W. Shang","Department of Computer Engineering, Santa Clara University, Santa Clara, CA, USA; Department of Computer Engineering, Santa Clara University, Santa Clara, CA, USA; Department of Computer Engineering, Santa Clara University, Santa Clara, CA, USA; Department of Computer Engineering, Santa Clara University, Santa Clara, CA, USA","IEEE Transactions on Circuits and Systems for Video Technology","","2010","20","6","820","828","In this paper, we propose an efficient and practical algorithm to dynamically adapt the Lagrange multipliers for each macroblock based on the context of the neighboring or upper layer blocks to improve rate-distortion performance. Our method improves the accuracy for the detection of true motion vectors as well as the most efficient encoding modes for luma, which are used for deriving the motion vectors, and modes for chroma. Simulation results for H.264/advanced video coding video demonstrate that our method reduces bit rate significantly and achieves peak signal-to-noise ratio gain over those of the joint model (JM) software for all sequences tested, with negligible extra computational cost. The improvement is particularly significant for high motion high-resolution videos. This paper describes our work that led to our Joint Video Team adopted contribution (included in software JM 12.0 onward), collectively known as context adaptive Lagrange multiplier (CALM).","1051-8215;1558-2205","","10.1109/TCSVT.2010.2045915","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5433010","H264/advanced video coding (AVC);Lagrange multiplier;motion estimation;rate-distortion optimization;source coding;video coding;visual communications","Lagrangian functions;Rate-distortion;Motion estimation;Video coding;Heuristic algorithms;Motion detection;Encoding;Computational modeling;Bit rate;PSNR","data compression;motion estimation;video coding","context adaptive Lagrange multiplier;rate-distortion optimal motion estimation;H.264;advanced video coding video;peak signal-to-noise ratio;joint model software;high motion high-resolution videos","","18","30","","","","","","IEEE","IEEE Journals & Magazines"
"An image compositing solution at scale","K. Moreland; W. Kendall; T. Peterka; J. Huang","Sandia National Laboratories; University of Tennessee, Knoxville; Argonne National Laboratory; University of Tennessee, Knoxville","SC '11: Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis","","2011","","","1","10","The only proven method for performing distributed-memory parallel rendering at large scales, tens of thousands of nodes, is a class of algorithms called sort last. The fundamental operation of sort-last parallel rendering is an image composite, which combines a collection of images generated independently on each node into a single blended image. Over the years numerous image compositing algorithms have been proposed as well as several enhancements and rendering modes to these core algorithms. However, the testing of these image compositing algorithms has been with an arbitrary set of enhancements, if any are applied at all. In this paper we take a leading production-quality image compositing framework, IceT, and use it as a testing frame work for the leading image compositing algorithms of today. As we scale IceT to ever increasing job sizes, we consider the image compositing systems holistically, incorporate numerous optimizations, and discover several improvements to the process never considered before. We conclude by demonstrating our solution on 64K cores of the Intrepid Blue Gene/P at Argonne National Laboratories.","2167-4329;2167-4337","978-1-4503-0771-0978-1-4503-0771","10.1145/2063384.2063417","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6114465","Image compositing;Parallel scientific visualization","Rendering (computer graphics);Partitioning algorithms;Geometry;Image coding;Software algorithms;Encoding;Optimization","distributed memory systems;parallel processing;rendering (computer graphics)","image compositing solution;distributed-memory parallel rendering;sort-last parallel rendering;image composite;production-quality image compositing framework;IceT;image compositing algorithm;image compositing systems","","7","34","","","","","","IEEE","IEEE Conferences"
"An improved evaluation method for interactive genetic algorithms and its application in product design","Sun Yan; Wang Wanliang; L. Xiaojian","College of Business and Administration, Zhejiang University of Technology, No. 288 Liuhe Road, Hangzhou, China; School of Software, Zhejiang University of Technology, No. 288 Liuhe Road, Hangzhou, China; School of Art, Zhejiang University of Technology, No. 288 Liuhe Road, Hangzhou, China","2010 IEEE Fifth International Conference on Bio-Inspired Computing: Theories and Applications (BIC-TA)","","2010","","","840","843","Regarding the fatigue problem in the interactive genetic algorithms (IGA) during the user's evaluation process, the paper proposed a Boolean evaluation method, which replaces user's grading work with selection. The new method reduces user's labor considerably, and makes it possible to construct user's image model (by means of user feature curves) through their selections, from which user's sensitivity to genes can also be calculated in quantified way. User image helps to estimate individual's sufficiency. User sensitivity parameters help to arrange the searching order while layered optimizing method is adopted, and can also decide which dimensions of the searching space can be neglected according to the preset threshold value. The sufficiency value's deduction method from selection information is given, together with the general skills for the recognizing and handling of coupling genes. The algorithms are tested in a product styling design example.","","978-1-4244-6440-1978-1-4244-6437-1978-1-4244-6439","10.1109/BICTA.2010.5645234","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5645234","interactive algorithms;product design;evaluation","Silicon","genetic algorithms;product design","interactive genetic algorithms;Boolean evaluation method;user grading work;user image model;user sensitivity parameters;value deduction method;product styling design","","4","9","","","","","","IEEE","IEEE Conferences"
"Towards High-Performance Pattern Matching on Multi-Core Network Processing Platforms","Y. Qi; Z. Zhou; Y. Wu; Y. Xue; J. Li","NA; NA; NA; NA; NA","2010 IEEE Global Telecommunications Conference GLOBECOM 2010","","2010","","","1","5","With the continual growth of network speed and the increasing sophistication of network applications, keeping network operations efficient and secure becomes more challenging. Pattern matching is one of the key technologies for content-ware network processing, such as traffic classification, application identification and intrusion prevention. In this paper, we propose a hybrid pattern matching algorithm optimized for multi-core network processing platforms. As a system-level solution, our scheme focuses on both performance stability and hardware/software co-design. To verify the effectiveness of our design, the proposed algorithm is implemented on a state-of-art 16-MIPS-core network processing platform and evaluated with real-life data sets. Experimental results show that, when compared with the traditional Aho-Corasick algorithm, our hybrid solution saves 60~95% memory space while guarantees stable performance on large pattern sets and against adverse test traffic.","1930-529X;1930-529X","978-1-4244-5638-3978-1-4244-5636-9978-1-4244-5637","10.1109/GLOCOM.2010.5684120","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5684120","","Algorithm design and analysis;Doped fiber amplifiers;Pattern matching;Software algorithms;Data structures;Hardware;Software","multiprocessing systems;pattern matching","high-performance pattern matching;content-ware network processing;traffic classification;application identification;intrusion prevention;hybrid pattern matching algorithm;multicore network processing platforms","","2","9","","","","","","IEEE","IEEE Conferences"
"Development of power supply to isolated territories in Russia on the bases of microgrid concept","N. I. Voropai; K. V. Suslov; T. V. Sokolnikova; Z. A. Styczynski; P. Lombardi","Energy Systems Institute SB RAS, Irkutsk, Russia; Irkutsk State Technical University, Irkutsk, Russia; Irkutsk State Technical University, Irkutsk, Russia; Otto-von-Guericke University, Magdeburg, Germany; Otto-von-Guericke University, Magdeburg, Germany","2012 IEEE Power and Energy Society General Meeting","","2012","","","1","5","The paper analyses specific features of a system for power supply to isolated territories and populated settlements in Russia. The problems of development of isolated power supply systems are formulated in terms of current trends, technologies and software tools. The optimization problem of different generation units mix is presented. Test system study is discussed.","1932-5517;1944-9925","978-1-4673-2729-9978-1-4673-2727-5978-1-4673-2728","10.1109/PESGM.2012.6344612","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6344612","Small isolated territories;Technical and economic features;microgrid concept;Development optimization procedures","Electricity;Power generation;Renewable energy resources;Power supplies;Energy storage;Educational institutions","distributed power generation;power supplies to apparatus","Russia;microgrid concept;isolated power supply systems;software tools;generation units mix","","10","18","","","","","","IEEE","IEEE Conferences"
"Protocol for Dynamic Component-Property Attestation in Trusted Computing","Y. Jianhong; P. Xinguang","NA; NA","2010 Second International Conference on Networks Security, Wireless Communications and Trusted Computing","","2010","2","","369","372","This paper gives a way of implementing a high-speed and reliable data conversion between UART and WLAN. After introducing the overview of our system, this paper shows the hardware structure and firmware design in brief. Then we analyze the bottleneck of the data conversion and give a way of how to improve and optimize the firmware to complete a high speed and reliable data conversion in detail. We use DMA mechanism and data flow control to ensure the speed and reliability of data conversion. We add traffic self-adaptation feature to make the data conversion adapted to different data rate. We also show how to reduce the influence caused by the instability of wireless connection. Complete performance test will be taken at the end of this paper.","","978-1-4244-6598-9978-1-4244-6597-2978-0-7695-4011","10.1109/NSWCTC.2010.220","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5480881","TCG binary attestation;property-based;remote attestation;DCPA;trusted computing","Protocols;Protection;Computer networks;Software measurement;Educational institutions;Registers;Computer architecture;Computer applications;Application software;Hardware","cryptographic protocols","dynamic component property attestation protocol;common computing platforms;trusted computing platforms;binary attestation;remote computing platform;remote attestation","","","10","","","","","","IEEE","IEEE Conferences"
"Implementation of genetic algorithm for TSP based on FPGA","Z. Yan-cong; G. Jun-hua; D. Yong-feng; H. Huan-ping","Hebei University of Technology, Tianjin University of Commerce, Tianjin, CO 300130 China; Hebei University of Technology, Tianjin University of Commerce, Tianjin, CO 300130 China; Hebei University of Technology, Tianjin University of Commerce, Tianjin, CO 300130 China; Hebei University of Technology, Tianjin University of Commerce, Tianjin, CO 300130 China","2011 Chinese Control and Decision Conference (CCDC)","","2011","","","2226","2231","TSP is a typical combinatorial optimization problems and GA is an adaptive searching algorithm for global optimization with the natural parallelism. The running speed of GA's software implementation for TSP is too slow, so a new scheme for hardware implementation was put forward. The pipelining structure for GA was designed for facilitating hardware implementation, and other parallel mechanisms were also added to it, thus greatly enhanced the speed. The design and structure of genetic operators - selection, crossover and mutation, individual population and fitness memory were given. The overall design idea, the sub-module program, as well as simulation and experimental data were presented in details. For the memory design of distance matrix, the use of symmetric matrix of compressed storage ideas made the uses of memory cells saved 50%. In the whole design Altera's EP2C70F896C6 chip was used and Verilog was the program language. Testing results have shown the running speed under hardware implementation is faster 2 to 3 orders than the software implementation. Thus its application in practical engineering can be greatly promoted.","1948-9439;1948-9447","978-1-4244-8738-7978-1-4244-8737-0978-1-4244-8736","10.1109/CCDC.2011.5968577","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5968577","Genetic Algorithm;Competition Selection;Travel Salesman Problem;FPGA(Field Program Gate Array);Pipelining","Genetic algorithms;Hardware;Genetics;Clocks;Field programmable gate arrays;Cities and towns;Pipeline processing","field programmable gate arrays;genetic algorithms;travelling salesman problems","genetic algorithm;TSP;FPGA;combinatorial optimization problems;adaptive searching algorithm;global optimization;natural parallelism;memory design;distance matrix;symmetric matrix","","2","13","","","","","","IEEE","IEEE Conferences"
"A novel implementation of sequential output based parallel processing - orthogonal wavelet division multiplexing for DAS on SDR platform","C. Mahapatra; A. Ramakrishnan; T. Stouraitis; V. C. M. Leung","The University of British Columbia, Department of Electrical and Computer Engineering, Vancouver, Canada; The University of British Columbia, Department of Electrical and Computer Engineering, Vancouver, Canada; The University of British Columbia, Department of Electrical and Computer Engineering, Vancouver, Canada; The University of British Columbia, Department of Electrical and Computer Engineering, Vancouver, Canada","2012 19th IEEE International Conference on Electronics, Circuits, and Systems (ICECS 2012)","","2012","","","320","323","The distributed antenna system (DAS) concept promises to enhance the capacity and diversity of next-generation wireless communication networks, due to the inherently added micro and macro diversity. Today, DAS is widely employed in cutting edge cellular communication to cover dead spots. The complexity of Orthogonal Frequency Division Multiplexing (OFDM) makes it challenging to build efficient and cost effective DAS systems. Orthogonal Wavelet Division Multiplexing (OWDM) is a system that has been proposed as an alternative to OFDM. In this paper we present a novel Sequential Output based Parallel Processing (SBPP) architecture for DAS using OWDM scheme. The experimental evaluation was done on a virtual test bed that includes one central processor with multiple antenna nodes and multiple mobile stations. The novel architecture shows a new approach that could provide a more flexible replacement for OFDM with reduced Peak to Average Power Ratio (PAPR) and Inter-Carrier Interference (ICI), while still maintaining the same channel capacity characteristics as OFDM. The architecture also demonstrates an efficient resource optimization technique based on LTE-driven data rate requirements for multimedia applications.","","978-1-4673-1260-8978-1-4673-1261-5978-1-4673-1259","10.1109/ICECS.2012.6463737","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6463737","DAS;SBPP;OWDM;OFDM;LTE;PAPR;ICI","Computer architecture;Peak to average power ratio;Field programmable gate arrays;Wavelet transforms;Modulation;Finite impulse response filter","cellular radio;diversity reception;intercarrier interference;Long Term Evolution;microprocessor chips;multiplexing;parallel processing;software radio","software defined radio;multimedia application;LTE driven data;intercarrier interference;peak to average power ratio;multiple mobile station;multiple antenna node;central processor;OFDM alternative;cellular communication;wireless communication network;distributed antenna system;SDR platform;DAS platform;orthogonal wavelet division multiplexing;sequential output based parallel processing","","","9","","","","","","IEEE","IEEE Conferences"
"Lossless image coding by cellular neural networks with backward error propagation learning","K. Takizawa; S. Takenouchi; H. Aomori; T. Otake; M. Tanaka; I. Matsuda; S. Itoh","Department of Electrical Engineering, Tokyo University of Science, 2641 Yamazaki, Noda-shi, Chiba 278-8510 Japan; Department of Electrical Engineering, Tokyo University of Science, 2641 Yamazaki, Noda-shi, Chiba 278-8510 Japan; Department of Electrical Engineering, Tokyo University of Science, 2641 Yamazaki, Noda-shi, Chiba 278-8510 Japan; Department of Software Science, Tamagawa University, 6-1-1 Tamagawagakuen, Machida, Tokyo, 194-8610 Japan; Department of Information and Communication Sciences, Sophia University, 7-1 Kioi-cho, Chiyoda-ku, Tokyo 102-8554 Japan; Department of Electrical Engineering, Tokyo University of Science, 2641 Yamazaki, Noda-shi, Chiba 278-8510 Japan; Department of Electrical Engineering, Tokyo University of Science, 2641 Yamazaki, Noda-shi, Chiba 278-8510 Japan","The 2012 International Joint Conference on Neural Networks (IJCNN)","","2012","","","1","6","This paper proposes a novel hierarchical lossless image coding scheme using cellular neural network (CNN). The coding architecture of proposed method is composed of three steps: split, predict, and entropy coding. The coding performance of proposed method highly depends on that of CNN predictors. The resulting prediction errors are encoded by the adaptive arithmetic coder. To achieve the high coding efficiency, the type of space-variant CNN templates and their parameters are optimized to minimize the actual coding bits of prediction residuals by the minimum coding rate learning with backward error propagation. Experimental results in 21 kinds of standard grayscale test images show that the average coding rates of the proposed scheme is better than that of the conventional schemes.","2161-4407;2161-4393;2161-4393","978-1-4673-1490-9978-1-4673-1488-6978-1-4673-1489","10.1109/IJCNN.2012.6252404","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6252404","","Image coding;Standards;Context;Context modeling;Entropy coding;Prediction algorithms","cellular neural nets;image coding;learning (artificial intelligence)","cellular neural networks;backward error propagation learning;novel hierarchical lossless image coding scheme;CNN;entropy coding;predict coding;split coding;adaptive arithmetic coder;minimum coding rate learning;grayscale test images","","","8","","","","","","IEEE","IEEE Conferences"
"Neural network workflow scheduling for large scale Utility Management Systems","S. Vukmirović; A. Erdeljan; L. Imre; N. Nedić","Faculty of Technical Sciences, University of Novi Sad, Serbia; Faculty of Technical Sciences, University of Novi Sad, Serbia; Faculty of Technical Sciences, University of Novi Sad, Serbia; Faculty of Technical Sciences, University of Novi Sad, Serbia","2010 IEEE International Conference on Systems, Man and Cybernetics","","2010","","","2307","2311","Grid computing is the future computing paradigm for enterprise applications. It can be used for executing large scale workflow applications. This paper focuses on the workflow scheduling mechanism. Although there is much work on static scheduling approaches for workflow applications in parallel environments, little work has been done on a Grid environment for industrial systems. Utility Management Systems (UMS) are executing very large numbers of workflows with very high resource requirements. Unlike the grid approach for standard scientific workflows, UMS workflows have a different set of requirements and thereby optimization of resource usage has to be made in a different way. This paper proposes a novel scheduling architecture which dynamically executes a scheduling algorithm using near real-time feedback from the execution monitor. An Artificial Neural Network was used for workflow scheduling and performance tests show that significant improvement of overall execution time can be achieved by this soft-computing method.","1062-922X;1062-922X","978-1-4244-6588-0978-1-4244-6586-6978-1-4244-6587","10.1109/ICSMC.2010.5641990","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5641990","","Artificial neural networks;IEEE Computer Society;Performance evaluation;Synchronization;Sensors;Visualization","grid computing;neural nets;real-time systems;resource allocation;scheduling;uncertainty handling;utility programs;workflow management software","workflow scheduling;utility management system;grid computing;parallel environment;industrial system;resource usage optimization;real time feedback;artificial neural network;soft computing method","","","19","","","","","","IEEE","IEEE Conferences"
"A fault tolerance approach for distributed systems using monitoring based replication","A. Costan; C. Dobre; F. Pop; C. Leordeanu; V. Cristea","University Politehnica of Bucharest, Computer Science Department, 313, Splaiul Independentei, 060042, Romania; University Politehnica of Bucharest, Computer Science Department, 313, Splaiul Independentei, 060042, Romania; University Politehnica of Bucharest, Computer Science Department, 313, Splaiul Independentei, 060042, Romania; University Politehnica of Bucharest, Computer Science Department, 313, Splaiul Independentei, 060042, Romania; University Politehnica of Bucharest, Computer Science Department, 313, Splaiul Independentei, 060042, Romania","Proceedings of the 2010 IEEE 6th International Conference on Intelligent Computer Communication and Processing","","2010","","","451","458","High availability is a desired feature of a dependable distributed system. Replication is a well-known technique to achieve fault tolerance in distributed systems, thereby enhancing availability. We propose an approach relying on replication techniques and based on monitoring information to be applied in distributed systems for fault tolerance. Our approach uses both active and passive strategies to implement an optimistic replication protocol. Using a proxy to handle service calls and relying on service replication strategies, we effectively deal with the complexity and overhead issues. This paper presents an architecture for implementing the proxy based on monitoring data and the replication management. Experimentation and application testing using an implementation of the architecture is presented. The architecture is demonstrated to be a viable technique for increasing dependability in distributed systems.","","978-1-4244-8230-6978-1-4244-8228","10.1109/ICCP.2010.5606398","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5606398","","Monitoring;Fault tolerance;Fault tolerant systems;Security;Computer architecture;Containers;Distributed databases","distributed processing;software fault tolerance;system monitoring","fault tolerance;distributed systems;monitoring based replication;dependable distributed system;replication techniques;monitoring information;optimistic replication protocol;service calls;service replication;replication management","","4","10","","","","","","IEEE","IEEE Conferences"
"Fault detection for high availability RAID system","Liu Zhiming; Sha Jichang; Yang Xiaohua; Wan Yaping","Institute of Computer Applications, University of South China, Hengyang, China; College of Information System and Management, National University of Defense Technology, Changsha, China; Institute of Computer Applications, University of South China, Hengyang, China; Institute of Computer Applications, University of South China, Hengyang, China","The 6th International Conference on Networked Computing and Advanced Information Management","","2010","","","27","32","Designing storage systems to provide high availability in the face of failures needs the use of various data protection techniques, such as dual-controller RAID. The failure of controller may cause data inconsistencies of RAID storage system. Heartbeat is used to detect controllers whether survival. So, the heartbeat cycle's impact on the high availability of a dual-controller hot-standby system has become the key of current research. To address the problem of fixed setting heartbeat in building high availability system currently, an adaptive heartbeat fault detection model of dual controller, which can adjust heartbeat cycle based on the frequency of data read-write request, is designed to improve the high availability of dual-controller RAID storage system. Additionally, this heartbeat mechanism can be used for other applications in distributed settings such as detecting node failures, performance monitoring, and query optimization. Based on this model, the high availability stochastic Petri net model of fault detection was established and used to evaluate the effect of the availability. In addition, we define a AHA (Adaptive Heart Ability) parameter to scale the ability of system heartbeat cycle to adapt to the environment which is changing. The results show that, relatively speaking with fixed configuration, the design is valid and effective, and can enhance dual controller RAID system high availability.","","978-89-88678-26-8978-1-4244-7671","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5572373","component;High availability;Adaptive;Dual controller;Heartbeat","Random access memory;Heating;Biomedical monitoring;Monitoring;Benchmark testing;Meteorology","fault tolerant computing;memory architecture;Petri nets;RAID;software fault tolerance;stochastic processes;system recovery","fault detection;storage system design;data protection technique;data inconsistencies;heartbeat cycle;dual-controller hot-standby system;adaptive heartbeat fault detection model;data read-write request;dual-controller RAID storage system;high availability stochastic Petri net model;adaptive heart ability parameter;AHA parameter","","","19","","","","","","IEEE","IEEE Conferences"
"Auto-tuning of Cloud-Based In-Memory Transactional Data Grids via Machine Learning","P. Di Sanzo; D. Rughetti; B. Ciciani; F. Quaglia","NA; NA; NA; NA","2012 Second Symposium on Network Cloud Computing and Applications","","2012","","","9","16","In-memory transactional data grids have revealed extremely suited for cloud based environments, given that they well fit elasticity requirements imposed by the pay-as-you-go cost model. Particularly, the non-reliance on stable storage devices simplifies dynamic resize of these platforms, which typically only involves setting up (or shutting down) some data-cache instance. On the other hand, defining the well suited amount of cache servers to be deployed, and the degree of replication of slices of data, in order to optimize reliability/availability and performance tradeoffs, is far from being a trivial task. As a example, scaling up/down the size of the underlying infrastructure might give rise to scarcely predictable secondary effects on the side of the synchronization protocol adopted to guarantee data consistency while supporting transactional accesses. In this paper we investigate on the usage of machine learning approaches with the aim at providing a means for automatically tuning the data grid configuration, which is achieved via dynamic selection of both the well suited amount of cache servers, and the well suited degree of replication of the data-objects. The final target is to determine configurations that are able to guarantee specific throughput or latency values (such as those established by some SLA), under some specific workload profile/intensity, while minimizing at the same time the cost for the cloud infrastructure. Our proposal has been integrated within an operating environment relying on the well known Infinispan data grid, namely a mainstream open source product by the Red Had JBoss division. Some experimental data are also provided supporting the effectiveness of our proposal, which have been achieved by deploying the data platform on top of Amazon EC2.","","978-1-4673-5581-0978-0-7695-4943","10.1109/NCCA.2012.20","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6472453","in-memory data platforms;transactional data platforms;cloud compitung","Servers;Machine learning;Benchmark testing;Throughput;Proposals;Neural networks;Time factors","cache storage;cloud computing;grid computing;learning (artificial intelligence);public domain software","cloud-based in-memory transactional data grids auto-tuning;machine learning;elasticity requirements;pay-as-you-go cost model;stable storage devices;cache servers;predictable secondary effects;data consistency;transactional accesses;workload profile-intensity;Infinispan data grid;mainstream open source product;Red Had JBoss division;Amazon EC2","","8","26","","","","","","IEEE","IEEE Conferences"
"On the Viability of Compression for Reducing the Overheads of Checkpoint/Restart-Based Fault Tolerance","D. Ibtesham; D. Arnold; P. G. Bridges; K. B. Ferreira; R. Brightwell","NA; NA; NA; NA; NA","2012 41st International Conference on Parallel Processing","","2012","","","148","157","The increasing size and complexity of high performance computing (HPC) systems have led to major concerns over fault frequencies and the mechanisms necessary to tolerate these faults. Previous studies have shown that state-of-the-field checkpoint/restart mechanisms will not scale sufficiently for future generation systems. Therefore, optimizations that reduce checkpoint overheads are necessary to keep checkpoint/restart mechanisms effective. In this work, we demonstrate that checkpoint data compression is a feasible mechanism for reducing checkpoint commit latencies and storage overheads. Leveraging a simple model for checkpoint compression viability, we show: (1) checkpoint data compression is feasible for many types of scientific applications expected to run on extreme scale systems, (2) checkpoint compression viability scales with checkpoint size, (3) user-level versus system-level checkpoints bears little impact on checkpoint compression viability, and (4) checkpoint compression viability scales with application process count. Lastly, we describe the impact that checkpoint compression might have on future generation extreme scale systems.","0190-3918;0190-3918;2332-5690","978-1-4673-2508-0978-0-7695-4796","10.1109/ICPP.2012.45","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6337576","Fault tolerance;Checkpoint Compression","Data compression;Checkpointing;Libraries;Benchmark testing;Compression algorithms;Mathematical model;Fault tolerance","checkpointing;data compression;software fault tolerance","checkpoint/restart-based fault tolerance;high performance computing;HPC system;fault frequency;checkpoint/restart mechanism;checkpoint overhead;checkpoint data compression;checkpoint commit latency;storage overhead;scientific application;checkpoint compression viability scale;checkpoint size;user-level checkpoints;system-level checkpoints","","17","44","","","","","","IEEE","IEEE Conferences"
"Bitstream Efficiency of Field Programmable One-Hot Arrays","M. G. Arnold; P. Vouzis; J. H. Cho","NA; NA; NA","2010 IEEE Computer Society Annual Symposium on VLSI","","2010","","","436","441","Field Programmable One-Hot Arrays (FPOHAs) have simple cells which are suitable to implement control-rich algorithms, where one-hot encoding is preferred. We present the cell design for the FPOHA and describe a modified open-source one-hot tool, known as Verilog Implicit To One-hot (VITO), to synthesize one-hot designs into FPOHA configurations without global optimization. We compare the bitstream sizes for FPOHAs and FPGAs using artificial benchmarks. In theory, optimal FPOHA layouts could have bitstream sizes half that of FPGAs. The observed FPOHA sizes synthesized from VITO may not be optimal, but are still often more efficient than FPGA sizes.","2159-3469;2159-3477","978-1-4244-7320-5978-1-4244-7321-2978-0-7695-4076","10.1109/ISVLSI.2010.117","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5572830","","Hardware design languages;Field programmable gate arrays;Logic gates;Layout;Benchmark testing;Adders;Routing","field programmable gate arrays;hardware description languages;programmable logic arrays;public domain software","bitstream efficiency;field programmable one hot arrays;control rich algorithm;open source one hot tool;Verilog implicit to one hot;one-hot design;FPGA","","","10","","","","","","IEEE","IEEE Conferences"
"Mixed integer linear programming models for scheduling the LED planting operation on PCBs","Jiaxiang Luo; Jiyin Liu","College of Automation Science and Engineering, South China University of Technology, Guangzhou, China; Business School and Economics, Loughborough University, Leicestershire, UK","The 17th International Conference on Automation and Computing","","2011","","","277","282","This paper deals with a scheduling problem arising in printed circuit board (PCB) assembly. In this problem, LED components are to be assembled in batches to specified positions on PCBs by a high speed assembly machine and the position sequence for component assembly needs to be optimized. Three different mixed integer linear programming (MILP) models are proposed for the problem. Problem instances based on real data are generated and used to test and compare the models. The models are solved using a commercial software package. The results show that the model with the minimum number of constraints can find the optimal solution in the shortest time.","","978-1-86218-098-7978-1-4673-0000","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6084941","Printed circuit board assembly;Component scheduling;MILP models","Assembly;Light emitting diodes;Mathematical model;Cutting tools;Magnetic heads;Optimization;Biological system modeling","integer programming;light emitting diodes;linear programming;printed circuit manufacture;scheduling","mixed integer linear programming models;LED planting operation;printed circuit board assembly;high speed assembly machine;position sequence;commercial software package","","","8","","","","","","IEEE","IEEE Conferences"
"Non-crypto Hardware Hash Functions for High Performance Networking ASICs","N. Hua; E. Norige; S. Kumar; B. Lynch","NA; NA; NA; NA","2011 ACM/IEEE Seventh Symposium on Architectures for Networking and Communications Systems","","2011","","","156","166","Hash functions are vital in networking. Hash-based algorithms are increasingly deployed in mission-critical, high speed network devices. These devices will need small, quick, hardware hash functions to keep up with Internet growth. There are many hardware hash functions used in this situation, foremost among them CRC-32. We develop parametrized methods for evaluating hash function output quality so as to better compare similar hash functions. We use these methods to explore the quality of candidate hash functions, including CRC-32, H<sub>3</sub> (with fixed seed), MD5 and others. We also propose optimized building blocks for hardware hash functions based on SP-networks. Given a size budget of 4K gates and only 1 cycle to compute the result, we demonstrate a 128 bit input, 64 bit output hash function built using this framework that ranks highly in our tests.","","978-1-4577-1454-2978-0-7695-4521","10.1109/ANCS.2011.32","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062728","Hash Function;Evaluation;Networking ASIC","Logic gates;Hardware;Software;Timing;Performance evaluation;Cryptography;Wires","application specific integrated circuits;cryptography;file organisation;Internet","networking ASIC;noncrypto hardware hash functions;hash-based algorithms;mission-critical;high speed network devices;Internet;CRC-32;optimized building blocks","","7","21","","","","","","IEEE","IEEE Conferences"
"Design for Motion Detection System Based on Embedded Linux","Z. Wei; L. Cai","NA; NA","2010 International Conference on Multimedia Technology","","2010","","","1","3","With the gradual maturity of image processing and embedded development technology, the conventional video detection system is improved towards the intelligence and the miniaturization. Embedded development board and embedded Linux OS offer enough support for this trend. This paper firstly introduces the construction of the hardware and software's platform, and then describes the system framework made of three main modules. As the hardcore of the whole system, classic object-detection algorithms are introduced. Pointing to shortcomings of the selected algorithm, the improved methods and the optimized idea are presented. Test results prove that our system achieves good effects finally.","","978-1-4244-7874-3978-1-4244-7871-2978-1-4244-7873","10.1109/ICMULT.2010.5631477","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5631477","","Motion detection;Noise;Pixel;Algorithm design and analysis;Streaming media;Hardware;Software","embedded systems;image motion analysis;Linux;object detection;video signal processing","motion detection system design;embedded Linux OS;image processing;embedded development technology;conventional video detection system;object-detection algorithms","","3","7","","","","","","IEEE","IEEE Conferences"
"Ultralow-Latency Hardware-in-the-Loop Platform for Rapid Validation of Power Electronics Designs","D. Majstorovic; I. Celanovic; N. D. Teslic; N. Celanovic; V. A. Katic","Faculty of Technical Sciences, University of Novi Sad, Novi Sad, Serbia; Massachusetts Institute of Technology, Boston, MA, USA; Faculty of Technical Sciences, University of Novi Sad, Novi Sad, Serbia; Faculty of Technical Sciences, University of Novi Sad, Novi Sad, Serbia; Faculty of Technical Sciences, University of Novi Sad, Novi Sad, Serbia","IEEE Transactions on Industrial Electronics","","2011","58","10","4708","4716","This paper introduces a unified approach to the validation of power-electronics (PE) control hardware, firmware, and software designs. It is based on a scalable application-specific ultralow-latency (ULL) digital processor core. The proposed ULL processor core simulates PE converters and systems comprising multiple power converters with a fixed 1-μs simulation time step and latency, regardless of the size of the system. Owing to its ULL, the proposed platform enables the fully automatic testing and validation of the complete PE design comprising component safe-operating-area validation, system protection, firmware, and software implementation as well as overall system performance optimization.","0278-0046;1557-9948","","10.1109/TIE.2011.2112318","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5762345","Automatic design verification;electronic design automation for power electronics (PE);field-programmable gate array (FPGA)-based ultralow-latency (ULL) processor;hardware in the loop (HIL);real-time digital simulator for PE and motor drives","Field programmable gate arrays;Computer architecture;Hardware;Computational modeling;Real time systems;Converters","firmware;power convertors;power electronics","ultralow-latency hardware-in-the-loop platform;rapid validation;power electronics design;power electronics control hardware;firmware;software design;scalable application-specific ultralow-latency digital processor core;ULL processor core;power electronics converter;automatic testing;system protection","","63","45","","","","","","IEEE","IEEE Journals & Magazines"
"Dynamic applications on reconfigurable systems: From UML model design to FPGAs implementation","J. Vidal; F. de Lamotte; G. Gogniat; J. Diguet; S. Guillet","Lab-STICC - European University of Brittany - UBS - CNRS, UMR 3192, Lorient, France; Lab-STICC - European University of Brittany - UBS - CNRS, UMR 3192, Lorient, France; Lab-STICC - European University of Brittany - UBS - CNRS, UMR 3192, Lorient, France; Lab-STICC - European University of Brittany - UBS - CNRS, UMR 3192, Lorient, France; Lab-STICC - European University of Brittany - UBS - CNRS, UMR 3192, Lorient, France","2011 Design, Automation & Test in Europe","","2011","","","1","4","In this paper we propose a design methodology to explore dynamic and partial reconfiguration (DPR) of modern FPGAs. We define a set of rules in order to model DPR by means of UML and design patterns. Our approach targets MPSoPC (Multiprocessor System on Programmable Chip) which allows: a) area optimization through partial reconfiguration without performance penalty and b) increased system flexibility through dynamic behavior modeling and implementation. In our case, area reduction is achieved by reconfiguring co-processors connected to embedded processors, and flexibility is achieved by permitting new behavior to be easily added to the system. Most of the system is automatically generated by means of MDE techniques. Our modeling approach allows designers to target dynamic reconfiguration without being experts of modern FPGAs. Such a methodology allows design time speed-up and a significant reduction of the gap between hardware and software modeling.","1558-1101;1530-1591;1530-1591","978-3-9810801-8-6978-1-61284-208-0978-3-9810801-7","10.1109/DATE.2011.5763315","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5763315","","Unified modeling language;Program processors;Field programmable gate arrays;Computational modeling;Object oriented modeling;Concrete;Context","field programmable gate arrays;integrated circuit design;microprocessor chips;multiprocessor interconnection networks;reconfigurable architectures","reconfigurable systems;UML model design;FPGA;multiprocessor system on programmable chip;MDE techniques;dynamic reconfiguration;partial reconfiguration","","3","12","","","","","","IEEE","IEEE Conferences"
"Evaluating MPI Implementations Using HPL on an Infiniband Nehalem Linux Cluster","M. Sindi","NA","2010 Seventh International Conference on Information Technology: New Generations","","2010","","","19","25","In conjunction with Moore's Law, computer speeds are expected to double approximately every two years, but with the current challenges that computer manufacturers are facing to double speeds of individual processors, due to various reasons, such as processor temperatures, multiprocessor architectures have become more popular nowadays. Eventually, this has led to an increased interest in standards for writing parallel applications. The Message Passing Interface (MPI) has become the de facto standard for writing parallel applications. Our growing needs for the latest high performance computing solutions in Saudi Aramco, the world's largest oil producing company, has given us the opportunity to evaluate three of the most commonly used MPI implementations, MVAPICH, Open MPI, and Intel MPI on Intel's latest Nehalem processor. In this paper, we describe our test bed environment along with the evaluations that we did using the High Performance Linpack (HPL) benchmark, which is the standard benchmark for ranking the world's Top 500 supercomputers. We also discuss our own developed Web tool that is used to suggest tuned input values for the HPL benchmark. We show the performance numbers in GFLOPS along with the run times and system efficiencies when running on 32, 64 and 128 Infiniband Nehalem Linux cluster nodes using the three implementations of MPI. We finally discuss our results in terms of performance and scalability and we share our interpretations and future work.","","978-1-4244-6271-1978-1-4244-6270-4978-0-7695-3984","10.1109/ITNG.2010.164","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5501463","Aramco;HPC;Linpack;Linux;MPI","Linux;Computer aided manufacturing;Benchmark testing;Writing;Moore's Law;Manufacturing processes;Temperature;Computer architecture;Application software;Message passing","application program interfaces;Linux;message passing;multiprocessing systems;parallel architectures","MPI;message passing interface;HPL;high performance Linpack benchmark;infiniband Nehalem Linux cluster;Moore's law;computer manufacturers;processor temperatures;multiprocessor architectures;high performance computing;MVAPICH;Open MPI;GFLOPS","","1","14","","","","","","IEEE","IEEE Conferences"
"Potential calculation and grading ring design for ceramic insulators in 1000 kV UHV substations","J. Li; Z. Peng; X. Yang","State Key Laboratory of Electrical Insulation and Power Equipment, Xi'an Jiaotong University; State Key Laboratory of Electrical Insulation and Power Equipment, Xi'an Jiaotong University; State Key Laboratory of Electrical Insulation and Power Equipment, Xi'an Jiaotong University","IEEE Transactions on Dielectrics and Electrical Insulation","","2012","19","2","723","732","The electromagnetic environment of insulators in substations is more complicated than that in transmission lines, this difference gets more prominent as the system voltage increases. Considering all the influencing factors, this paper presents the calculation results of the potential and electric field distribution of the ceramic insulator strings in 1000 kV ultra-high voltage (UHV) substations, using a three-dimensional (3-D) finiteelement method (FEM) software. Factors that affect the potential distribution such as grading ring, jumper, string structure, mounting location and drop conductors have been analyzed. The structural parameters of grading ring were optimized. A use of double ring was proposed for terminal tension insulators. A true-type test was carried out to verify the reliability of the FEM calculation method and results. The optimized grading ring configuration was successfully applied on the 1000 kV UHV pilot project in China and it was proved very effective by field observations with an ultraviolet camera.","1070-9878;1558-4135","","10.1109/TDEI.2012.6180268","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6180268","Ceramic insulators;ultra-high voltage (UHV);substation;potential calculation;finite element method (FEM);electric field distribution;grading ring design","Insulators;Electric potential;Finite element methods;Electric fields;Substations;Ceramics;Conductors","ceramic insulators;finite element analysis;substations","potential calculation;grading ring design;ceramic insulators;UHV substations;electromagnetic environment;transmission lines;electric field distribution;ultra-high voltage substations;three-dimensional finite element method software;terminal tension insulators;true-type test;FEM calculation method;China;ultraviolet camera;voltage 1000 kV","","15","15","","","","","","IEEE","IEEE Journals & Magazines"
"High-Performance Reconfigurable Hardware Architecture for Restricted Boltzmann Machines","D. L. Ly; P. Chow","Department of Mechanical and Aerospace Engineering, Cornell University, Ithaca, NY, USA; Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada","IEEE Transactions on Neural Networks","","2010","21","11","1780","1792","Despite the popularity and success of neural networks in research, the number of resulting commercial or industrial applications has been limited. A primary cause for this lack of adoption is that neural networks are usually implemented as software running on general-purpose processors. Hence, a hardware implementation that can exploit the inherent parallelism in neural networks is desired. This paper investigates how the restricted Boltzmann machine (RBM), which is a popular type of neural network, can be mapped to a high-performance hardware architecture on field-programmable gate array (FPGA) platforms. The proposed modular framework is designed to reduce the time complexity of the computations through heavily customized hardware engines. A method to partition large RBMs into smaller congruent components is also presented, allowing the distribution of one RBM across multiple FPGA resources. The framework is tested on a platform of four Xilinx Virtex II-Pro XC2VP70 FPGAs running at 100 MHz through a variety of different configurations. The maximum performance was obtained by instantiating an RBM of 256 256 nodes distributed across four FPGAs, which resulted in a computational speed of 3.13 billion connection-updates-per-second and a speedup of 145-fold over an optimized C program running on a 2.8-GHz Intel processor.","1045-9227;1941-0093","","10.1109/TNN.2010.2073481","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5580081","Boltzmann machines;computer architecture;field-programmable gate arrays;neural network hardware;parallel processing","Hardware;Computer architecture;Field programmable gate arrays;Program processors;Equations;Complexity theory;Artificial neural networks","Boltzmann machines;C language;computational complexity;field programmable gate arrays;reconfigurable architectures","high-performance reconfigurable hardware architecture;restricted Boltzmann machines;neural networks;general-purpose processors;field-programmable gate array platforms;time complexity;Xilinx Virtex II-Pro XC2VP70 FPGA;C program;Intel processor","Artificial Intelligence;Computer Simulation;Computers;Computers;Neural Networks (Computer);Programming Languages;Software Validation","31","21","","","","","","IEEE","IEEE Journals & Magazines"
"Application for voice transfer through Internet protocol","I. Marijašević; M. Janković; D. Žagar","Croatian Telecom, Zagreb, Croatia; Croatian Telecom, Zagreb, Croatia; University of Osijek, Faculty of Electrical Engineering, Kneza Trpimira 2b, HR-31000, CROATIA","The 33rd International Convention MIPRO","","2010","","","549","554","This text shows program solution for transferring voice data through Internet protocol. In order to transfer data between two or more terminals, specific steps are needed in HW and SW to get media information and adjust them for sending through computer network. Applications in the Java environment, analyzing the properties of the human voice, such as the frequency spectrum and frequency of recurrence sequences, and presents procedures digitizing speech. Described methods of creating the flow of data and displayed during the creation of a session in the application and the process of creating sockets for data and control channels. For satisfying quality, network must be able to transfer at certain speed and with minimum delay. Program code has to be optimized so that any terminal delays in performing programmed functions are down to their minimum level. The results of testing applications on the network using Ethereal software package are shown measurements.","","978-9-5323-3050-2978-1-4244-7763-0978-953-233-058","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5533455","JAVA;RTP;VoIP","Internet;Protocols;Application software;Speech analysis;Frequency;Computer networks;Java;Human voice;Sockets;Delay","Internet telephony;Java;speech processing","voice transfer;Internet protocol;media information;Java environment;frequency spectrum;speech digitation;data channels;control channels;terminal delays;Ethereal software package","","","7","","","","","","IEEE","IEEE Conferences"
"EROD: New collaborative design platform for developing energy efficient electric motors","C. Favi; M. Germani; M. Marconi; M. Mengoni; A. Tirabassi","Department of Mechanical Engineering, Universit&#x00E0; Politecnica delle Marche, Ancona, Italy; Department of Mechanical Engineering, Universit&#x00E0; Politecnica delle Marche, Ancona, Italy; Department of Mechanical Engineering, Universit&#x00E0; Politecnica delle Marche, Ancona, Italy; Department of Mechanical Engineering, Universit&#x00E0; Politecnica delle Marche, Ancona, Italy; Department of Mechanical Engineering, Universit&#x00E0; Politecnica delle Marche, Ancona, Italy","2011 IEEE International Electric Machines & Drives Conference (IEMDC)","","2011","","","59","64","In recent years International and Communitarian directives have focused their attention on the problem of energy consumption. In this context electric motors play a key role and designers must improve products considering this direction. This paper presents an innovative platform, called EROD (Energy Reduction Oriented Design), which consists of multiple software modules with different functionalities to support the whole design process of electric motors. The goal of EROD platform is to achieve energy efficient and sustainable electric motors and related devices. The platform facilitates knowledge and data sharing among design team members, it arranges the workflow activities and finally it promotes collaborative design sessions. All functionalities are implemented within the same web-based platform. This guarantees interoperability among different tools and leads up to significant reduction of development time due to the elimination of errors and iterations. The platform was tested to design five innovative electric motors for industrial and household applications. Results show that the new developed motors improve current solutions in terms of energy efficiency and environmental impact during use.","","978-1-4577-0061-3978-1-4577-0060-6978-1-4577-0059","10.1109/IEMDC.2011.5994878","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5994878","electric motors;collaborative design;optimization;environmental impact;efficiency","Electric motors;Brushless motors;Permanent magnet motors;Induction motors;Reluctance motors;Production","electric machines;groupware;power engineering computing;product design","EROD;collaborative design platform;energy efficient electric motor;energy reduction oriented design;multiple software module;design process;knowledge sharing;data sharing;design team member;environmental impact","","1","20","","","","","","IEEE","IEEE Conferences"
"Fast Bi-Directional Prediction Selection in H.264/MPEG-4 AVC Temporal Scalable Video Coding","H. Lin; H. Hang; W. Peng","MediaTek, Inc., Hsinchu, Taiwan; Department of Electronics Engineering, National Chiao-Tung University, Hsinchu, Taiwan; Department of Computer Science, National Chiao-Tung University, Hsinchu, Taiwan","IEEE Transactions on Image Processing","","2011","20","12","3508","3523","In this paper, we propose a fast algorithm that efficiently selects the temporal prediction type for the dyadic hierarchical-B prediction structure in the H.264/MPEG-4 temporal scalable video coding (SVC). We make use of the strong correlations in prediction type inheritance to eliminate the superfluous computations for the bi-directional (BI) prediction in the finer partitions, 16 × 8/8 × 16/8 × 8, by referring to the best temporal prediction type of 16 × 16. In addition, we carefully examine the relationship in motion bit-rate costs and distortions between the BI and the uni-directional temporal prediction types. As a result, we construct a set of adaptive thresholds to remove the unnecessary BI calculations. Moreover, for the block partitions smaller than 8 × 8, either the forward prediction (FW) or the backward prediction (BW) is skipped based upon the information of their 8 × 8 partitions. Hence, the proposed schemes can efficiently reduce the extensive computational burden in calculating the BI prediction. As compared to the JSVM 9.11 software, our method saves the encoding time from 48% to 67% for a large variety of test videos over a wide range of coding bit-rates and has only a minor coding performance loss.","1057-7149;1941-0042","","10.1109/TIP.2011.2156802","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5771590","Bi-directional prediction;bi-directionally predictive frame;encoder optimization;hierarchical prediction structure;H.264/MPEG-4 AVC scalable video coding;temporal scalability","Prediction methods;Algorithm design and analysis;MPEG 4 Standard;Complexity theory;Scalability;Bidirectional control;Video coding","data compression;media streaming;video coding","bidirectional prediction selection;H.264-MPEG-4 AVC temporal scalable video coding;hierarchical-B prediction structure;SVC;motion bit-rate costs;unidirectional temporal prediction;BI calculations;forward prediction;backward prediction;BW;FW;JSVM 9.11 software","","1","46","","","","","","IEEE","IEEE Journals & Magazines"
"Recent advances on the energy management of a Hybrid Electric vehicle","D. S. Efstathiou; A. K. Petrou; P. Spanoudakis; N. C. Tsourveloudis; K. P. Valavanis","Intelligent Systems and Robotics Lab, Technical University of Crete, Chania, Greece; Intelligent Systems and Robotics Lab, Technical University of Crete, Chania, Greece; Machine Tools Laboratory Technical University of Crete, Chania, Greece; Intelligent Systems and Robotics Lab, Technical University of Crete, Chania, Greece; Department of Electrical and Computer Engineering, University of Denver, CO 80208 USA","2012 20th Mediterranean Conference on Control & Automation (MED)","","2012","","","896","901","This paper presents new innovative subsystems of the ER11 prototype urban vehicle which is powered by hydrogen fuel cells and ultra-capacitors. The subsystems described here are: 1) the energy management system, which is responsible for the optimization of the fuel efficiency and the increased mileage of the vehicle, 2) the driver's monitoring and control panel and 3) the power transmission system, which offers the possibility of continuous change of gear. The powertrain of the test-bed vehicle consists of an electric motor, a fuel (H<sub>2</sub>) cell system, an ultra-capacitor bank and a DC/DC converter. Testing verified that the proposed energy management system is benefitted by the variable transmission of power leading to less fuel consumption.","","978-1-4673-2531-8978-1-4673-2530-1978-1-4673-2529","10.1109/MED.2012.6265752","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6265752","","Vehicles;Energy management;Fuels;Gears;Hydrogen;Fuel cells;Software","electric motors;energy management systems;fuel cell vehicles;gears;hybrid electric vehicles;innovation management;power transmission (mechanical);supercapacitors","hybrid electric vehicle;innovative subsystems;ER11 prototype urban vehicle;hydrogen fuel cells;energy management system;driver monitoring;control panel;gear;powertrain;test-bed vehicle;electric motor;fuel cell system;ultracapacitor bank;power variable transmission;fuel consumption","","3","16","","","","","","IEEE","IEEE Conferences"
"A study on the structural design and driving technology of decoupled vibratory feeding system","Liang Han; Weizhong Wu; Qian Zhang; Xiang Li","School of Mechanical Engineering, Southeast University, Jiangning District, Nanjing 211189, China; School of Mechanical Engineering, Southeast University, Jiangning District, Nanjing 211189, China; School of Mechanical Engineering, Southeast University, Jiangning District, Nanjing 211189, China; School of Mechanical Engineering, Southeast University, Jiangning District, Nanjing 211189, China","2012 19th International Conference on Mechatronics and Machine Vision in Practice (M2VIP)","","2012","","","253","258","This paper describes the development of decoupled vibratory feeding system (DVFS), and concentrates on its structural design and driving technology. In DVFS, vibration angle, frequency and waveform of the driving signals can be easily adjusted in software way, as well as the phase angle between vertical vibration and horizontal vibration. So DVFS has a great superiority. Up to now, two DVFS prototypes have been successfully developed, and the structure of decoupled vibratory feeder (DVF) becomes more and more compact. But its application is limited, because of complex structure, occupying much space and inconvenient installation and commissioning. So the structure should be optimized. The electromagnet and the drive circuit are the core of the drive system which should be simultaneously considered. Its external dimensions and drive performance should be optimized. Presently, some researches on the electromagnet driving technology have been made. One method is to receive analog signal from D/A output, then drive electromagnet through power amplifier. The other method actually is to use a switching circuit controlled by PWM technology. To test electromagnet performance, a test system has been designed.","","978-1-4673-1643-9978-0-473-20485","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6484597","Decoupled vibratory feeding system;Structural design;Test system;Electromagnet fabrication;Driving technology","Electromagnets;Vibrations;Wires;Coils;Silicon;Control systems;Resistance","design engineering;drives;materials handling equipment;structural engineering;vibrations","structural design;driving technology;decoupled vibratory feeding system;vibration angle;vertical vibration;horizontal vibration;DVFS prototypes;decoupled vibratory feeder;drive circuit;drive system;drive electromagnet;power amplifier","","","17","","","","","","IEEE","IEEE Conferences"
"Notice of Retraction<br>Designing and formulating organization performance evaluation model in AHP method based on EFQM criteria (case study)","S. Iranzadeh; F. Chakherlouy","Islamic Azad university,Tabriz Branch, Iran; Tabriz Business Training Center, Iran","2010 IEEE International Conference on Advanced Management Science(ICAMS 2010)","","2010","1","","606","609","Designing and formulating a comprehensive organization performance evaluation model based on European Foundation for Quality Management (EFQM) in AHP (Analytic Hierarchy Process) method is the main aim of the present research study. Evaluation is considered as one of the most important activities in each organization in a way that reformation of processes and procedures of doing activity without evaluation of results will be impossible. At the present research activity, AHP (Analytic Hierarchy Process) has been used as one of MADM (Multi-Attribute Decision Making) methods for the evaluation of performance of organizations through the application of EFQM (European Foundation for Quality Management) excellence model criteria. Also, Municipality of City of Tabriz has been selected as subjects for testing the presented model. In the same direction, seven districts of this municipality were selected as sample model. Necessary and required information were accumulated through questionnaire, interview and also taking advantage of data and library resources, details of which were analyzed and studied through the application of advanced Excel and Expert Choice 11.5 software package system. Eventually, various districts of this organization were evaluated and prioritized in terms of performance.;Notice of Retraction<br><br>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<br><br>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<br><br>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<br><br>","","978-1-4244-6932-1978-1-4244-6931-4978-1-4244-6930","10.1109/ICAMS.2010.5553098","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5553098","Evaluation of Performance;EFQM (European Foundation for Quality Management);Analytic Hierarchy Process (AHP)","Integrated circuits;Europe;Lead;Analytical models;Films","decision making;organisational aspects;quality management","organization performance evaluation model;European Foundation for quality management;analytic hierarchy process method;multiattribute decision making method;excellence model criteria;Tabriz;Excel;Expert Choice 11.5 software package system","","","8","","","","","","IEEE","IEEE Conferences"
"An Algorithm of Micromouse Maze Solving","J. Cai; X. Wan; M. Huo; J. Wu","NA; NA; NA; NA","2010 10th IEEE International Conference on Computer and Information Technology","","2010","","","1995","2000","This paper proposes a maze exploring algorithm named “Partition-central Algorithm”, which is used to find the shortest path in a micromouse competition maze. A standard 16*16 units maze is divided into 12 partitions in this algorithm. Depending on the absolute direction of the micromouse and the locations of each partition, exploring rules alter when the micromouse walks to optimize the maze exploring process. A simulation program is developed to verify the algorithm. The test result shows that Partition-central algorithm has higher average efficiency when compared with other algorithms.","","978-1-4244-7548-3978-1-4244-7547-6978-0-7695-4108","10.1109/CIT.2010.337","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5578409","Partition-central Algorithm;Micromouse;Maze;Simulator","Partitioning algorithms;Algorithm design and analysis;Legged locomotion;Chromium;Load modeling;Software algorithms;Software","microrobots;mobile robots;path planning","micromouse maze solving;maze exploring algorithm;partition-central algorithm;micromouse competition maze;simulation program;wheeled robot;embedded micro-control-chip","","5","12","","","","","","IEEE","IEEE Conferences"
"An FPGA-Based Accelerator for Analog VLSI Artificial Neural Network Emulation","B. van Liempd; D. Herrera; M. Figueroa","NA; NA; NA","2010 13th Euromicro Conference on Digital System Design: Architectures, Methods and Tools","","2010","","","771","778","Analog VLSI circuits are being used successfully to implement Artificial Neural Networks (ANNs). These analog circuits exhibit nonlinear transfer function characteristics and suffer from device mismatches, degrading network performance. Because of the high cost involved with analog VLSI production, it is beneficial to predict implementation performance during design. We present an FPGA-based accelerator for the emulation of large (500+ synapses, 10k+ test samples) single-neuron ANNs implemented in analog VLSI. We used hardware time-multiplexing to scale network size and maximize hardware usage. An on-chip CPU controls the data flow through various memory systems to allow for large test sequences. We show that Block-RAM availability is the main implementation bottleneck and that a trade-off arises between emulation speed and hardware resources. However, we can emulate large amounts of synapses on an FPGA with limited resources. We have obtained a speedup of 30.5 times with respect to an optimized software implementation on a desktop computer.","","978-1-4244-7839","10.1109/DSD.2010.20","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5615441","Artificial neural networks;analog VLSI emulation;FPGA-based accelerators;hardware time-multiplexing;embedded systems","Hardware;Neurons;Emulation;Field programmable gate arrays;Transfer functions;Process control;Artificial neural networks","field programmable gate arrays;neural chips;random-access storage;transfer functions;VLSI","FPGA-based accelerator;analog VLSI artificial neural network emulation;analog VLSI circuits;analog circuits;nonlinear transfer function characteristics;device mismatches;network performance degradation;analog VLSI production;implementation performance;single-neuron ANN;hardware time-multiplexing;scale network size;hardware usage;on-chip CPU;data flow;memory systems;block-RAM availability;emulation speed;hardware resources;optimized software implementation;desktop computer","","1","16","","","","","","IEEE","IEEE Conferences"
"Alpha-EM gives fast Hidden Markov Model estimation: Derivation and evaluation of alpha-HMM","Y. Matsuyama; R. Hayashi","Department of Computer Science and Engineering, Waseda University, Tokyo, Japan; Department of Computer Science and Engineering, Waseda University, Tokyo, Japan","The 2010 International Joint Conference on Neural Networks (IJCNN)","","2010","","","1","8","A fast learning algorithm for Hidden Markov Models is derived starting from convex divergence optimization. This method utilizes the alpha-logarithm as a surrogate function for the traditional logarithm to process the likelihood ratio. This enables the utilization of a stronger curvature than the logarithm. This paper's method includes the ordinary Baum-Welch re-estimation algorithm as a proper subset. The presented algorithm shows fast learning by utilizing time-shifted information during the progress of iterations. The computational complexity of this algorithm, which directly affects the CPU time, remains almost the same as the logarithmic one since only stored results are utilized for the speedup. Software implementation and speed are examined in the test data. The results showed that the presented method is creditable.","2161-4407;2161-4393;1098-7576","978-1-4244-6918-5978-1-4244-6916-1978-1-4244-6917","10.1109/IJCNN.2010.5596959","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5596959","","Equations;Hidden Markov models;Mathematical model;Markov processes;Software algorithms;Estimation;Convergence","computational complexity;convex programming;hidden Markov models","alpha-expectation-maximization algorithm;fast hidden Markov model estimation;alpha-hidden Markov model;fast learning algorithm;convex divergence optimization;alpha-logarithm;surrogate function;ordinary Baum-Welch re-estimation algorithm;computational complexity","","4","8","","","","","","IEEE","IEEE Conferences"
"Adding code generation to develop a simulation platform","C. Graichen; F. D'Amato","GE Global Research, Niskayuna, NY; GE Global Research, Niskayuna, NY","2011 IEEE Long Island Systems, Applications and Technology Conference","","2011","","","1","6","Efficient and accurate control technologies require extensive simulation capabilities to validate the control software and demonstrate the impact on the business and equipment. To create a platform for rapid development and simulation of complex dynamic models, the authors and their colleagues have designed an object-oriented architecture. A portion of the architecture framework is constructed using code generation based on XML component definitions. This paper describes the key aspects of the architecture of the control simulator platform and its code generation capabilities. The simulation platform consists of defining a collection of components represented by differential equations, the capability to select, configure and interconnect components, and the ability to solve the coupled set of equations. The code generation is custom-built, however, the generated code results in more consistency and improved reliability by eliminating error prone steps and allowing the simulation engineer to focus on component mathematical description. As the number of components that are generated increases, the investment in a custom-built code generation is quickly realized by significantly reducing the amount of time required to create or update the code template for each component. The paper also discusses the importance of handling updates as well as the initial creation of code files. The techniques leveraged within this project have been learned through the use of other code generation tools, including GUI development tools. In particular, care has been taken to minimize the accidental loss of manually introduced code and handle version updates of the code generator. Our team is using this framework to develop simulation tests for power plant optimizations and have plans to add custom code generation to other areas of the platform.","","978-1-4244-9877-2978-1-4244-9878","10.1109/LISAT.2011.5784231","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5784231","code generation;simulation;model-based engineering","Generators;Object oriented modeling;Unified modeling language;XML;Manuals;Software;Mathematical model","object-oriented methods;program compilers;XML","code generation;simulation platform;object-oriented architecture;XML;differential equations;component mathematical description;GUI development tools;power plant optimizations","","","11","","","","","","IEEE","IEEE Conferences"
"Low-frequency broadband Janus-Helmholtz underwater acoustic transducer","Y. Sang; Y. Lan","Science and Technology on Underwater Acoustic Laboratory, Harbin Engineering University, 150001, China; Science and Technology on Underwater Acoustic Laboratory, Harbin Engineering University, 150001, China","2011 Symposium on Piezoelectricity, Acoustic Waves and Device Applications (SPAWDA)","","2011","","","549","553","In this paper, using the finite element software ANSYS, the structural parameters of Janus-Helmholtz transducer were optimized to obtain excellent acoustic radiation properties in range of 2-5kHz. Based on optimization results produced a Janus-Helmholtz transducer test prototype, test results show that the simulation results and experimental results are basically consistent, verified Janus-Helmholtz transducer is an ideal low-frequency, broadband, high power, small size, deep water transducer.","","978-1-4673-1078-9978-1-4673-1075-8978-1-4673-1077","10.1109/SPAWDA.2011.6167310","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6167310","Janus-Helmholtz transducer;finite element;transmitting voltage response","Transducers;Cavity resonators;Fluids;Finite element methods;Resonant frequency;Broadband communication;Vibrations","acoustic transducers;finite element analysis;underwater equipment;underwater sound","low-frequency broadband Janus-Helmholtz underwater acoustic transducer;finite element software;structural parameters;acoustic radiation properties;high power small sized deep water transducer;frequency 2 kHz to 5 kHz","","","5","","","","","","IEEE","IEEE Conferences"
"Study on Wellsite Toxic Gas Leakage and Dispersion of High Temperature High Pressure Gas Wells with High Sulfur Content","Z. Huang; Q. Lin; B. Gao","NA; NA; NA","2010 4th International Conference on Bioinformatics and Biomedical Engineering","","2010","","","1","5","During the drilling, completion and testing of gas well with high temperature, high pressure and high sulfur content, toxic gas leakage is dangerous and may bring severe consequences. It's important for the safety of personnel and assets to ascertain the leaking position and research the leakage characteristics. Focusing on the wellsite-scale space, this paper discussed the key position of leakage for different well procedures, and calculated the dispersion of leaked gas with the software FLUENT based on three dimension methods of computational fluid dynamics. Characteristics of toxic gas dispersion is discussed, and influence of wind speed, toxic gas components, leakage flowrate, temperature and wellsite obstacles is considered, which would help to optimize the monitoring locations. New proposal was put forward in simulation of multi-component gas disperion on wellsite, as well as in explaination of toxic gas accumulation around obstacles.","2151-7622;2151-7614;2151-7614","978-1-4244-4712-1978-1-4244-4713","10.1109/ICBBE.2010.5516028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5516028","","Temperature;Drilling;Testing;Safety;Personnel;Computational fluid dynamics;Wind speed;Monitoring;Proposals;Computational modeling","computational fluid dynamics;drilling (geotechnical);hydrocarbon reservoirs;leak detection;occupational safety;toxicology","high temperature high pressure gas wells;sulfur content;software FLUENT;toxic gas dispersion;wellsite toxic gas leakage;wellsite toxic gas dispersion;computational fluid dynamics;drilling;multicomponent gas dispersion;personnel safety","","","14","","","","","","IEEE","IEEE Conferences"
"Hybrid of rough set theory and Artificial Immune Recognition System as a solution to decrease false alarm rate in intrusion detection system","F. N. M. Sabri; N. M. Norwawi; K. Seman","Faculty of Science and Technology, Universiti Sains, Islam Malaysia (USIM), Bandar Baru Nilai, Nilai, Negeri Sembilan, Malaysia; Faculty of Science and Technology, Universiti Sains, Islam Malaysia (USIM), Bandar Baru Nilai, Nilai, Negeri Sembilan, Malaysia; Faculty of Science and Technology, Universiti Sains, Islam Malaysia (USIM), Bandar Baru Nilai, Nilai, Negeri Sembilan, Malaysia","2011 7th International Conference on Information Assurance and Security (IAS)","","2011","","","134","138","Denial of Service (DoS) attacks is one of the security threats for computer systems and applications. It usually make use of software bugs to crash or freeze a service or network resource or bandwidth limits by making use of a flood attack to saturate all bandwidth. Predicting a potential DOS attacks would be very helpful for an IT departments or managements to optimize the security of intrusion detection system (IDS). Nowadays, false alarm rates and accuracy become the main subject to be addressed in measuring the effectiveness of IDS. Thus, the purpose of this work is to search the classifier that is capable to reduce the false alarm rates and increase the accuracy of the detection system. This study applied Artificial Immune System (AIS) in IDS. However, this study has been improved by using integration of rough set theory (RST) with Artificial Immune Recognition System 1 (AIRS1) algorithm, (Rough-AIRS1) to categorize the DoS samples. RST is expected to be able to reduce the redundant features from huge amount of data that is capable to increase the performance of the classification. Furthermore, AIS is an incremental learning approach that will minimize duplications of cases in a knowledge based. It will be efficient in terms of memory storage and searching for similarities in Intrusion Detection (IDS) attacks patterns. This study use NSL-KDD 20% train dataset to test the classifiers. Then, the performances are compared with single AIRS1 and J48 algorithm. Results from these experiments show that Rough-AIRS1 has lower number of false alarm rate compared to single AIRS but a little bit higher than J48. However, accuracy for this hybrid technique is slightly lower compared to others.","","978-1-4577-2155-7978-1-4577-2154-0978-1-4577-2153","10.1109/ISIAS.2011.6122808","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6122808","Intrusion detection system;accuracy;false alarm rate;rough set theory;artificial immune recognition system","Intrusion detection;Accuracy;Training;Testing;Immune system;Data mining","artificial immune systems;computer crime;learning (artificial intelligence);pattern classification;program debugging;rough set theory","rough set theory;artificial immune recognition system;false alarm rate;intrusion detection system;denial of service attack;security threat;software bug;flood attack;DOS attack;IT department;artificial immune system;Rough-AIRS 1;redundant feature;AIS;incremental learning approach;memory storage;NSL-KDD;J48 algorithm;AIRS1 algorithm","","1","15","","","","","","IEEE","IEEE Conferences"
"Effect of manufacturing procedure on the miniaturized flextensional transducers (Cymbals) and hydrophone array performance","C. Kannan; P. M. Rajeshwari; S. Jacob; A. Malarkkodi; R. Dhilsha; M. A. Atmanand","National Institute of Ocean Technology, Ministry of Earth Sciences, Velachery-Tambaram Main Road, Pallikaranai, Chennai-600100, Tamilnadu, India; National Institute of Ocean Technology, Ministry of Earth Sciences, Velachery-Tambaram Main Road, Pallikaranai, Chennai-600100, Tamilnadu, India; National Institute of Ocean Technology, Ministry of Earth Sciences, Velachery-Tambaram Main Road, Pallikaranai, Chennai-600100, Tamilnadu, India; National Institute of Ocean Technology, Ministry of Earth Sciences, Velachery-Tambaram Main Road, Pallikaranai, Chennai-600100, Tamilnadu, India; National Institute of Ocean Technology, Ministry of Earth Sciences, Velachery-Tambaram Main Road, Pallikaranai, Chennai-600100, Tamilnadu, India; National Institute of Ocean Technology, Ministry of Earth Sciences, Velachery-Tambaram Main Road, Pallikaranai, Chennai-600100, Tamilnadu, India","OCEANS 2011 IEEE - Spain","","2011","","","1","5","This paper summarizes the effect of manufacturing procedure on the performance of an element as well as an array of cymbal type transducers. A finite element software ATILA, is used to model the cymbal element and optimized the dimension and materials based on the results. A maximum Receiving Sensitivity (RS) of -203 dB re 1V/μPa @ 1m is obtained analytically for a single element with 13mm diameter and a cavity depth of 600 micron and maximum RS of -192 dB re 1V/μPa @ 1m is obtained analytically for a single element with 19mm diameter and a cavity depth of 320 micron. Several prototypes were fabricated and tested the performances in Acoustic Test Facility at NIOT, which is National Accreditation Board of Testing and Calibration of Laboratories (NABL) certified. The RS showed a maximum value of only-182 dB re 1V/μPa @ 1m even with a preamplifier having a gain of 20dB for a 13mm cymbal array (1×5). The experimental results are matching well with modelled results for single elements. But expected array gain as per the numerical calculations were not obtained when connected elements in an array. It has been also observed that the d<sub>33</sub>values of cymbal elements in an array should be identical to get the predicted results. Extreme care should be taken to avoid high pressure from acting upon the apex of the end caps during gluing and other stages. Special care in gluing, blanking and forming operations combined with optimum force/torque is necessary in getting good array performance.","","978-1-4577-0088-0978-1-4577-0086-6978-1-4577-0087","10.1109/Oceans-Spain.2011.6003458","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6003458","Cymbal hydrophone;Underwater acoustic transducer;Sensitivity;Array performance","Arrays;Transducers;Finite element methods;Cavity resonators;Sensitivity;Sonar equipment;Ceramics","acoustic transducers;hydrophones","manufacturing procedure;miniaturized flextensional transducers;hydrophone array;cymbal type transducers;finite element software;ATILA;maximum receiving sensitivity;gluing;blanking;forming operations","","2","7","","","","","","IEEE","IEEE Conferences"
"Power system study of engine waste heat recovery based on single screw expander","B. Liu; H. Zhang; H. Liang; Y. Chen; K. Yang","College of Environmental and Energy Engineering, Beijing University of Technology, China; College of Environmental and Energy Engineering, Beijing University of Technology, China; College of Environmental and Energy Engineering, Beijing University of Technology, China; College of Environmental and Energy Engineering, Beijing University of Technology, China; College of Environmental and Energy Engineering, Beijing University of Technology, China","2011 International Conference on Electric Information and Control Engineering","","2011","","","5199","5202","This paper introduces the principle of the engine waste heat recovery system. The organic Rankine cycle (ore) system is proposed to improve the efficiency of waste heat recovery. GT-power software is used to make a DI engine model of 493ZLQ3. The model simulates the exhaust temperature on the brake power curve and brake torque curve based on the optimized results, and put a comparison with the test. The results show that the error range of the simulation results is 5% compared with the test results. Therefore, the results of the simulation and analysis can afford valuable reference for calculating the entire power system. The whole power system can obtain the output net work of 2.67kW.","","978-1-4244-8039-5978-1-4244-8036-4978-1-4244-8038","10.1109/ICEICE.2011.5777171","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5777171","engine;waste heat recovery;simulation;exhaust temperature;generation system","Waste heat;Heat recovery;Power generation;Fluids;Power systems;Heat engines","exhaust systems;heat recovery;power engineering computing;waste recovery","power system study;engine waste heat recovery;single screw expander;organic Rankine cycle system;GT-power software;DI engine model;493ZLQ3;exhaust temperature;brake power curve;brake torque curve","","","","","","","","","IEEE","IEEE Conferences"
"Breaking the speed and scalability Barriers for Graph exploration on distributed-memory machines","F. Checconi; F. Petrini; J. Willcock; A. Lumsdaine; A. R. Choudhury; Y. Sabharwal","NA; NA; NA; NA; NA; NA","SC '12: Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis","","2012","","","1","12","In this paper, we describe the challenges involved in designing a family of highly-efficient Breadth-First Search (BFS) algorithms and in optimizing these algorithms on the latest two generations of Blue Gene machines, Blue Gene/P and Blue Gene/Q. With our recent winning Graph 500 submissions in November 2010, June 2011, and November 2011, we have achieved unprecedented scalability results in both space and size. On Blue Gene/P, we have been able to parallelize a scale 38 problem with 238 vertices and 242 edges on 131,072 processing cores. Using only four racks of an experimental configuration of Blue Gene/Q, we have achieved a processing rate of 254 billion edges per second on 65,536 processing cores. This paper describes the algorithmic design and the main classes of optimizations that we have used to achieve these results.","2167-4337;2167-4329;2167-4329","978-1-4673-0806-9978-1-4673-0805-2978-1-4673-0804","10.1109/SC.2012.25","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6468459","","Algorithm design and analysis;Program processors;Benchmark testing;Software algorithms;Communities;Prediction algorithms;Computer architecture","distributed memory systems;graph theory;search problems","blue gene/p;blue gene/q;blue gene machines;BFS;breadth first search algorithms;distributed memory machines;graph exploration","","36","50","","","","","","IEEE","IEEE Conferences"
"Hardware Pessimistic Run-Time Profiling for a Self-Reconfigurable Embedded Processor Architecture","S. O. Agwa; H. H. Ahmad; A. I. Saleh","NA; NA; NA","2010 International Conference on Reconfigurable Computing and FPGAs","","2010","","","162","167","Embedded processors are expected to immigrate towards self-reconfigurable architectures, and in the near future, the self-reconfiguration concept will be able to support revolutionary architectural innovations. The research described in this paper is a continuation of our previous work [1]. In [1], the advantage of the - so called - pessimistic run-time profiling approach was demonstrated and compared to preparation mode profiling and optimistic run-time profiling. Using pessimistic run-time profiling, a 36.09 % reduction in execution time was achieved compared to 23.02% in the optimistic run-time profiling case. These results were obtained using the “High Pass Grey-Scale Filter” benchmark as our case study [2]. Here, we extend the previous results to include a hardware implementation of pessimistic run-time profiling. Due to the parallel execution of run-time profiling and the running algorithm, execution time reduction was increased to 57.73% on the same benchmark. Total energy consumed by our hardware implementation was only 54 Pj, and some 1,371 gates were added to the design. These two figures represent approximately 1.4×10<sup>-5</sup>% and 1.828 % increase in energy consumption and gate count, respectively, as compared to the pessimistic-accelerated case and main core gate count. The hardware run-time profiling unit introduced here is based on the ""predetermined basic regions detection"" philosophy, and can operate at a maximum frequency of approximately 278 MHz for this particular case study. Profiling each critical region consumes less than 3.6 ns and executes in parallel with the decode stage of the main processor instruction pipeline. Hardware pessimistic run-time profiling is, thus, able to achieve the same speedup of the full acceleration case - in which no profiling is used - with only a marginal increase in area and energy consumption as compared to the non-accelerated case. All results were obtained using Ten silica [3] and Xilinx [4] Tools.","2325-6532","978-1-4244-9523-8978-0-7695-4314","10.1109/ReConFig.2010.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5695299","Reconfigurable;Architecture;Embedded;Processors;Acceleration;Profiling","Acceleration;Hardware;Monitoring;Clocks;Benchmark testing;Power demand;Software","embedded systems;low-power electronics;parallel processing;pipeline processing;power aware computing;reconfigurable architectures","self-reconfigurable embedded processor architecture;pessimistic run-time profiling;high pass grey scale filter;parallel execution;execution time reduction;energy consumption;gate count;predetermined basic regions detection;processor instruction pipeline;Tensilica tool;Xilinx tool","","","14","","","","","","IEEE","IEEE Conferences"
"MiyakoDori: A Memory Reusing Mechanism for Dynamic VM Consolidation","S. Akiyama; T. Hirofuchi; R. Takano; S. Honiden","NA; NA; NA; NA","2012 IEEE Fifth International Conference on Cloud Computing","","2012","","","606","613","In Infrastructure-as-a-Service datacenters, the placement of Virtual Machines (VMs) on physical hosts are dynamically optimized in response to resource utilization of the hosts. However, existing live migration techniques, used to move VMs between hosts, need to involve large data transfer and prevents dynamic consolidation systems from optimizing VM placements efficiently. In this paper, we propose a technique called “memory reusing” that reduces the amount of transferred memory of live migration. When a VM migrates to another host, the memory image of the VM is kept in the source host. When the VM migrates back to the original host later, the kept memory image will be “reused”, i.e. memory pages which are identical to the kept pages will not be transferred. We implemented a system named MiyakoDori that uses memory reusing in live migrations. Evaluations show that MiyakoDori significantly reduced the amount of transferred memory of live migrations and reduced 87% of unnecessary energy consumption when integrated with our dynamic VM consolidation system.","2159-6190;2159-6182;2159-6182","978-1-4673-2892-0978-0-7695-4755","10.1109/CLOUD.2012.56","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6253557","virtualization;live migration;IaaS cloud","Servers;Memory management;Benchmark testing;Energy consumption;Compounds;Computer aided software engineering;Educational institutions","cloud computing;computer centres;storage management;virtual machines","MiyakoDori;memory reusing mechanism;dynamic VM consolidation;infrastructure-as-a-service datacenters;virtual machines;resource utilization;live migration techniques;large data transfer;dynamic consolidation systems","","11","25","","","","","","IEEE","IEEE Conferences"
"Almost Isometric Mesh Parameterization through Abstract Domains","N. Pietroni; M. Tarini; P. Cignoni","Instituto of Science e Tecnolgie dell'Informazione, CNR-National Research Council, Pisa; Instituto of Science e Tecnolgie dell'Informazione, CNR-National Research Council, Pisa and DICOM, Universit dell'Insubria, Varese; Instituto of Science e Tecnolgie dell'Informazione, CNR-National Research Council, Pisa","IEEE Transactions on Visualization and Computer Graphics","","2010","16","4","621","635","In this paper, we propose a robust, automatic technique to build a global hi-quality parameterization of a two-manifold triangular mesh. An adaptively chosen 2D domain of the parameterization is built as part of the process. The produced parameterization exhibits very low isometric distortion, because it is globally optimized to preserve both areas and angles. The domain is a collection of equilateral triangular 2D regions enriched with explicit adjacency relationships (it is abstract in the sense that no 3D embedding is necessary). It is tailored to minimize isometric distortion, resulting in excellent parameterization qualities, even when meshes with complex shape and topology are mapped into domains composed of a small number of large continuous regions. Moreover, this domain is, in turn, remapped into a collection of 2D square regions, unlocking many advantages found in quad-based domains (e.g., ease of packing). The technique is tested on a variety of cases, including challenging ones, and compares very favorably with known approaches. An open-source implementation is made available.","1077-2626;1941-0506;2160-9306","","10.1109/TVCG.2009.96","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5204086","Modeling;surface parameterization.","Mesh generation;Robustness;Shape;Topology;Testing;Open source software;Distortion measurement;Interpolation;Geometry;Design methodology","computer graphics;distortion;mesh generation","isometric mesh parameterization;two-manifold triangular mesh;isometric distortion;equilateral triangular 2D regions;topology mapping;2D square regions;quad-based domains","Algorithms;Computer Graphics;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional","35","39","","","","","","IEEE","IEEE Journals & Magazines"
"A novel RFID bus information system based on data stream cleaning framework","Lu Ye","School of Information and ElectronicEngineering, ZheJiang University of Science and Technology, Hangzhou 310023, China","2010 3rd IEEE International Conference on Ubi-Media Computing","","2010","","","139","143","In this paper, researches on new RFID general framework based on J2EE and data cleaning are proposed in bus park information integration system. We use middle ware and J2EE to lower the coupling; use new technology to improve testing, use data stream cleaning method that optimizes the overall accuracy adjusted cleaning costs. A web application system is described and a multi-level distributive application model is provided, thus helping to maintain the consistency of management of the bus park database as well as to conduct the dynamic update and maintenance of data. At the same time, a high-efficiency RFID data stream cleaning framework is employed to ensure reliability of data in the system. It is proved by the experiment that this system has a promising application prospect.","","978-1-4244-6709-9978-1-4244-6708-2978-1-4244-6707","10.1109/UMEDIA.2010.5544483","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5544483","","Radiofrequency identification;Information systems;Cleaning;Wireless sensor networks;Logistics;Software maintenance;Radio frequency;Data analysis;Feedback;Testing","data mining;media streaming;middleware;radiofrequency identification;system buses","RFID bus information system;data stream cleaning framework;bus park information integration system;distributive application model;middleware","","2","5","","","","","","IEEE","IEEE Conferences"
"Highly scalable parallel sorting","E. Solomonik; L. V. Kalé","Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL 61801, USA; Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL 61801, USA","2010 IEEE International Symposium on Parallel & Distributed Processing (IPDPS)","","2010","","","1","12","Sorting is a commonly used process with a wide breadth of applications in the high performance computing field. Early research in parallel processing has provided us with comprehensive analysis and theory for parallel sorting algorithms. However, modern supercomputers have advanced rapidly in size and changed significantly in architecture, forcing new adaptations to these algorithms. To fully utilize the potential of highly parallel machines, tens of thousands of processors are used. Efficiently scaling parallel sorting on machines of this magnitude is inhibited by the communication-intensive problem of migrating large amounts of data between processors. The challenge is to design a highly scalable sorting algorithm that uses minimal communication, maximizes overlap between computation and communication, and uses memory efficiently. This paper presents a scalable extension of the Histogram Sorting method, making fundamental modifications to the original algorithm in order to minimize message contention and exploit overlap. We implement Histogram Sort, Sample Sort, and Radix Sort in Charm++ and compare their performance. The choice of algorithm as well as the importance of the optimizations is validated by performance tests on two predominant modern supercomputer architectures: XT4 at ORNL (Jaguar) and Blue Gene/P at ANL (Intrepid).","1530-2075;1530-2075","978-1-4244-6443-2978-1-4244-6442-5978-1-4244-6441","10.1109/IPDPS.2010.5470406","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5470406","","Sorting;Supercomputers;Histograms;Algorithm design and analysis;Testing;Computer science;Application software;Merging;High performance computing;Parallel processing","parallel machines;parallel programming;sorting","highly scalable parallel sorting;high performance computing;parallel processing;parallel machine;processor;communication-intensive problem;histogram sorting;message contention;sample sort;radix sort;supercomputer architecture","","15","21","","","","","","IEEE","IEEE Conferences"
"Voltage stability enhancement for unbalanced multiphase distribution networks","P. Juanuwattanakul; M. A. S. Masoum","Department of Electrical and Computer Engineering, Curtin University, Perth, WA, Australia; Department of Electrical and Computer Engineering, Curtin University, Perth, WA, Australia","2011 IEEE Power and Energy Society General Meeting","","2011","","","1","6","Voltage instability problems have become important issues in unbalanced multiphase distribution networks. This paper expands the well-known voltage index V/V<sub>0</sub> and defines an improved positive sequence voltage index of V<sub>collapse</sub>/V<sub>no-load</sub> to identify the weakest single-, two- and three-phase buses in unbalanced multiphase distribution networks. First, the ranking index is validated based on grid losses and PV curves without and with compensation devices. Then, the index is utilized to place single-phase shunt capacitors, three-phase DG without and with SVC devices at the most appropriate buses of the IEEE unbalanced multiphase 13 node test feeder using the DIgSILENT PowerFactory software. Finally, simulation results are presented to show the application of the proposed approach in improving voltage stability and increasing the maximum loading factor under unbalanced loading and/or network conditions.","1932-5517;1944-9925","978-1-4577-1002-5978-1-4577-1000-1978-1-4577-1001","10.1109/PES.2011.6039044","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6039044","Voltage stability;weakest bus;maximum loading factor;unbalanced distribution network","Indexes;Power system stability;Stability criteria;Loading;Capacitors;Voltage control","distributed power generation;distribution networks;power capacitors;power engineering computing;power system stability","voltage stability enhancement;unbalanced multiphase distribution networks;voltage instability problems;positive sequence voltage index;IEEE unbalanced multiphase 13 node test feeder;DIgSILENT PowerFactory software;three-phase DG;single-phase shunt capacitors;compensation devices","","11","20","","","","","","IEEE","IEEE Conferences"
"Embedded Unit for Point-of-Care Impedance Based Biosensor Readout","J. Broeders; D. Croux; A. Weustenraed; T. Cleij; P. Wagner; W. De Ceuninck; W. Vanaken; S. Duchateau; R. Thoelen","NA; NA; NA; NA; NA; NA; NA; NA; NA","2012 IEEE 14th International Conference on High Performance Computing and Communication & 2012 IEEE 9th International Conference on Embedded Software and Systems","","2012","","","1571","1577","Bridging the gap between state of the art consumer electronics and bio-analytical setups designed for lab environments, an embedded, miniaturised, stand-alone measurement device is developed. The presented impedance analyser incorporates most of the data communication features present in current-generation smartphones and is specifically optimised for low-frequency impedance based biosensor readout. The compact unit operates fully stand-alone with a touchscreen. Its functionality was tested and verified. In this work the unit is tested by using a biomimetic sensing device for the detection of L-nicotine. The combination between the handheld, embedded design of this specialised measurement equipment and a sensor layout fine-tuned for specific applications could mean significant advances in point-of-care systems.","","978-1-4673-2164-8978-0-7695-4749","10.1109/HPCC.2012.229","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6332001","impedance;embedded systems;biosensors;stand-alone","Impedance;Biosensors;Impedance measurement;Frequency measurement;Biomedical measurements;Resistors","biomedical equipment;biomedical measurement;biomimetics;biosensors;consumer electronics;data communication;medical computing;organic compounds;smart phones;user interfaces","point-of-care impedance based biosensor readout;state of the art consumer electronics;bioanalytical setups;lab environments;stand-alone measurement device;impedance analysis;data communication features;current-generation smartphones;low-frequency impedance based biosensor readout;touchscreen;biomimetic sensing device;L-nicotine;embedded design;specialised measurement equipment;sensor layout fine-tune;point-of-care systems","","1","34","","","","","","IEEE","IEEE Conferences"
"A single camera stereographic imaging system for tracking upper limb motions","C. Eastwood-Sutherland; T. J. Gale","School of Engineering, University of Tasmania, Churchill Avenue, Sandy Bay, Hobart, Australia; School of Engineering, University of Tasmania, Churchill Avenue, Sandy Bay, Hobart, Australia","2010 Annual International Conference of the IEEE Engineering in Medicine and Biology","","2010","","","3958","3961","A single camera stereographic imaging system was designed, built and tested for the intended application of tracking upper limb motion. The system consisted of a camera, quad-mirror system and image acquisition and processing software. This paper describes the design methods used and the testing of the system. Design methods included geometry optimisation and testing consisted of proof-of-concept trials. Preliminary results show that the concept is sound.","1094-687X;1558-4615","978-1-4244-4123-5978-1-4244-4124","10.1109/IEMBS.2010.5627697","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5627697","","Mirrors;Cameras;Table lookup;Tracking;Geometry;Accuracy","biomechanics;cameras;image motion analysis;medical image processing;mirrors;optical design techniques;stereo image processing","single camera stereographic imaging system;quad-mirror system;image acquisition;image processing;geometry optimisation;upper limb motion","Arm;Humans;Movement;Photography","3","4","","","","","","IEEE","IEEE Conferences"
"A Comparative Assessment of Conventional and Machine-Learning-Based Scoring Functions in Predicting Binding Affinities of Protein-Ligand Complexes","H. M. Ashtawy; N. R. Mahapatra","NA; NA","2011 IEEE International Conference on Bioinformatics and Biomedicine","","2011","","","627","630","Accurately predicting the binding affinities of large sets of protein-ligand complexes is a key challenge in computational biomolecular science, with applications in drug discovery, chemical biology, and structural biology. Since a scoring function (SF) is used to score, rank, and identify drug leads, the fidelity with which it predicts the affinity of a ligand candidate for a protein's binding site has a significant bearing on the accuracy of virtual screening. Despite intense efforts in developing conventional SFs, which are either force-field based, knowledge-based, or empirical, their limited predictive power has been a major roadblock toward cost-effective drug discovery. Therefore, in this work, we explore a range of novel SFs employing different machine-learning (ML) approaches in conjunction with physicochemical features characterizing protein-ligand complexes. We assess the scoring accuracies of these new ML-based SFs as well as those of conventional SFs in the context of the 2007 PDBbind benchmark dataset on both diverse and protein-family-specific test sets. We find that the best performing ML-based SF has a Pearson's correlation coefficient of 0.771 between predicted and measured binding affinities compared to 0.644 achieved by a state-of-the-art conventional SF.","","978-1-4577-1799","10.1109/BIBM.2011.128","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6120516","Machine learning;protein-ligand binding affinity;scoring function;scoring power;structure-based drug design","Proteins;Correlation;Radio frequency;Training;Software;Support vector machines;Drugs","drugs;learning (artificial intelligence);medical computing;proteins","comparative assessment;machine-learning-based scoring functions;binding affinity prediction;protein-ligand complex;computational biomolecular science;drug discovery;chemical biology;structural biology;virtual screening;force-field based SF;knowledge-based SF;empirical SF;protein-family-specific test sets","","2","11","","","","","","IEEE","IEEE Conferences"
"Evaluating and Enhancing Xen-Based Virtual Routers to Support Real-Time Applications","M. Bourguiba; K. Haddadou; G. Pujolle","NA; NA; NA","2010 7th IEEE Consumer Communications and Networking Conference","","2010","","","1","5","Router virtualization seems as the obvious next step to system virtualization and the key to easily deploy and manage next generation overlay virtual networks. In this paper, we investigate the viability of virtual routers on a Xen-based system. We first evaluate the system throughput when achieving forwarding in the virtual routers. Then, we consider the context where virtual routers are dedicated to flows of different types and propose a mechanism to guarantee the required throughput and latency to real time applications while maintaining an optimal aggregated system throughput. We achieved this through both configuring the Xen Credit scheduler and establishing priorities between packets in the driver domain before switching them to the target virtual router.","2331-9852;2331-9860","978-1-4244-5175-3978-1-4244-5176","10.1109/CCNC.2010.5421620","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5421620","","Throughput;Laboratories;Hardware;Computer architecture;Real time systems;Application virtualization;Delay;Protocols;Testing;Software performance","network servers;virtual machines;virtual private networks","Xen-based virtual routers;router virtualization;system virtualization;overlay virtual networks;Xen credit scheduler","","","7","","","","","","IEEE","IEEE Conferences"
"Toward dependability benchmarking of partitioning operating systems","R. Barbosa; J. Karlsson; Q. Yu; X. Mao","Department of Informatics Engineering, University of Coimbra, 3030-290, Portugal; Department of Computer Science and Engineering, Chalmers University of Technology, SE-412 96 Gothenburg, Sweden; Department of Computer Science and Engineering, Chalmers University of Technology, SE-412 96 Gothenburg, Sweden; Department of Computer Science and Engineering, Chalmers University of Technology, SE-412 96 Gothenburg, Sweden","2011 IEEE/IFIP 41st International Conference on Dependable Systems & Networks (DSN)","","2011","","","422","429","This paper describes a dependability benchmark intended to evaluate partitioning operating systems. The benchmark includes both hardware and software faultloads and measures the spatial as well as the temporal isolation among tasks, provided by a given real-time operating system. To validate the benchmark, a prototype implementation is carried out and three targets are benchmarked according to the specified process. The results substantiate that the proposed benchmark is able to compare and rank the targets in an objective way, and that it provides the ability to identify aspects of the target systems that need improvement.","2158-3927;1530-0889;1530-0889","978-1-4244-9233-6978-1-4244-9232-9978-1-4244-9231","10.1109/DSN.2011.5958255","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5958255","dependability benchmarking;fault injection;partitioning;operating systems;fault tolerance","Benchmark testing;Operating systems;Hardware;Registers;Real time systems;Measurement","fault tolerant computing;operating systems (computers)","dependability benchmarking;partitioning operating systems;hardware faultloads;software faultloads","","3","18","","","","","","IEEE","IEEE Conferences"
"Dual polarized microstrip antenna for monopulse application","S. Chakrabarti","SAMEER Kolkata Centre, Plot-L2, Block-GP, Sector-V, Salt Lake Electronics Complex, Kolkata 700091, India","2011 IEEE Applied Electromagnetics Conference (AEMC)","","2011","","","1","4","A simple technique to develop a monopulse antenna has been presented. A dual feed dual linearly polarized aperture coupled antenna at L band has been developed as the basic part of the overall antenna. Two 4×2 arrays of aperture coupled square resonant patch antenna have been designed, fabricated and tested. The basic antenna parameters are optimized using (MoM) based commercial simulation software IE3D. A planar comparator has also been simulated and optimized using IE3D. The two antenna outputs are suitably combined through the comparator to get the sum and difference pattern necessary for direction finding. Measured results are also presented.","","978-1-4577-1099-5978-1-4577-1098-8978-1-4577-1097","10.1109/AEMC.2011.6256844","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6256844","","Antenna measurements;Antenna arrays;Microstrip antennas;Microstrip;Feeds;Aperture coupled antennas","antenna feeds;antenna radiation patterns;aperture antennas;directive antennas;electromagnetic wave polarisation;method of moments;microstrip antenna arrays;planar antenna arrays","dual polarized microstrip antenna;monopulse application;monopulse antenna;dual feed dual linearly polarized aperture coupled antenna;L band;aperture coupled square resonant patch antenna array;antenna design;antenna parameter;MoM;commercial simulation software IE3D;planar comparator;antenna output;direction finding;sum pattern;difference pattern","","","8","","","","","","IEEE","IEEE Conferences"
"Numerical Simulation of the Thermal Convection in the Earth's Outer Core","C. Yang; Y. Zhang; L. Li","NA; NA; NA","2010 IEEE 12th International Conference on High Performance Computing and Communications (HPCC)","","2010","","","552","555","Large-scale simulation of the thermal convection in the Earth's outer core is studied. Starting from a legacy parallel code using Aztec and MPI, two optimized codes have been developed based on the PETSc software package. The first version gains several times acceleration with the help of the block-Jacobi preconditioners and the well-optimized libraries provided in PETSc. The second version, aiming at better parallel scalability, is developed based on the ideas of domain decomposition method for multi-physical problems. Test results employing thousands of processor cores on three supercomputers, i.e., an IBM Blue Gene/L, a Dawning 5000A and a Lenovo DeepComp 7000, are provided.","","978-1-4244-8335-8978-0-7695-4214","10.1109/HPCC.2010.70","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5581487","Earth's outer core;thermal convection;multi-physics;restricted additive Schwarz method;parallel scalability;parallel efficiency;flops","Earth;Linear systems;Supercomputers;Mathematical model;Scalability;Equations;Magnetic cores","convection;Earth core;geophysics computing","thermal convection;Earth outer core;legacy parallel code;Aztec code;MPI code;PETSc software package;block-Jacobi preconditioner","","","7","","","","","","IEEE","IEEE Conferences"
"Transition Thresholds for Binarization of Historical Documents","M. A. Ramirez-Ortegon; R. Rojas","NA; NA","2010 20th International Conference on Pattern Recognition","","2010","","","2362","2365","This paper extends the transition method for binarization based on transition pixels, a generalization of edge pixels. This method originally computes transition thresholds using the quantile thresholding algorithm, that has a critical parameter. We achieved an automatic version of the transition method by computing the transition thresholds with the Rosin's algorithm. We experimentally tested four variants of the transition method combining the density and cumulative distribution functions of transition values, with gray-intensity thresholds based on the normal and lognormal density functions. The results of our experiments show that these unsupervised methods yields superior binarization compared with top-ranked algorithms.","1051-4651;1051-4651","978-1-4244-7541-4978-1-4244-7542-1978-0-7695-4109","10.1109/ICPR.2010.578","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5597002","binarization;transition;method;documents;historical","Pixel;Histograms;Optical character recognition software;Pattern recognition;Approximation methods;Algorithm design and analysis;Density functional theory","document image processing","transition thresholds;binarization;historical documents;transition pixels;edge pixels;quantile thresholding;Rosin algorithm;cumulative distribution function;gray-intensity thresholds;lognormal density functions","","","10","","","","","","IEEE","IEEE Conferences"
"Investigation of substrate integrated waveguide in LTCC technology for mm-wave applications","A. Bunea; M. Lahti; D. Neculoiu; A. Stefanescu; T. Vaha-Heikkila","IMT-Bucharest, 32B, Erou Iancu Nicolae street, R-077190, Bucharest, Romania; VTT Technical Research Centre, Finland; IMT-Bucharest, 32B, Erou Iancu Nicolae street, R-077190, Bucharest, Romania; IMT-Bucharest, 32B, Erou Iancu Nicolae street, R-077190, Bucharest, Romania; VTT Technical Research Centre, Finland","Asia-Pacific Microwave Conference 2011","","2011","","","395","398","This paper presents the electromagnetic modeling and design of substrate integrated waveguides (SIW) up to W band (75 - 110 GHz) for Low-Temperature Co-fired Ceramics (LTCC) technology. The commercial software package CST Microwave Studio was used to optimize a coplanar waveguide (CPW) to SIW transition. Two test structures with two CPW to SIW transitions were designed, fabricated and measured up to 110 GHz. The comparison between simulated and measured data for the test structures shows a good agreement. The effective permittivity was extracted from both simulated and measured data and show good agreement.","2165-4727;2165-4743","978-0-85825-974-4978-1-4577-2034-5978-0-85825-974","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6173769","Substrate Integrated Waveguide;Low-Temperature Co-fired Ceramics;millimeter wave components","Electromagnetic waveguides;Transmission line measurements;Coplanar waveguides;Permittivity;Substrates;Dielectrics;Conductors","ceramics;coplanar waveguides;firing (materials);millimetre wave circuits;permittivity;substrate integrated waveguides","substrate integrated waveguide;LTCC technology;low-temperature co-fired ceramics;mm-wave applications;electromagnetic modeling;W band;commercial software package;CST Microwave Studio;coplanar waveguide;permittivity;frequency 75 GHz to 110 GHz","","2","7","","","","","","IEEE","IEEE Conferences"
"A novel thermal conductivity meter for thermal interface materials in optoelectronic device","P. Liao; Z. K. Hua; Y. C. Liao; J. H. Zhang","Key Laboratory of Advanced Display and System Applications (Shanghai University), Ministry of Education, School of Mechatronics Engineering and Automation, 200072, China; Key Laboratory of Advanced Display and System Applications (Shanghai University), Ministry of Education, School of Mechatronics Engineering and Automation, 200072, China; Key Laboratory of Advanced Display and System Applications (Shanghai University), Ministry of Education, School of Mechatronics Engineering and Automation, 200072, China; Key Laboratory of Advanced Display and System Applications (Shanghai University), Ministry of Education, School of Mechatronics Engineering and Automation, 200072, China","2010 11th International Conference on Electronic Packaging Technology & High Density Packaging","","2010","","","889","892","Nowadays, the light emitting diode (LED) is one of the most valuable light resources. However, it is well known that the optical output power is degraded with the heat integration. Hence, thermal management and evaluation system is required. Thermal conductivity is regarded as one of the most important properties that directly influence the thermal performance for thermal interface materials (TIMs). Different methods have been used to measure the thermal conductivity of TIM. Some testers with satisfied accuracy have also been discussed in papers. However, they are generally of high cost and complex structure design. In this paper, a novel thermal conductivity tester for optoelectronic materials with low cost was designed and built based on one-dimension heat transfer theory and the processing technology. Details of the measurement methodology and the apparatus development were introduced. The tester was designed based on ASTM D5470 which is specified to the thermal conductivity measurement of TIM. According to modular design principle, the apparatus can be divided into five parts, including frame, loading, heating and cooling units and in-situ thickness measurement. In determination of the external loads on the test samples, a force transducer was located in the loading module, which would improve the accuracy and repeatability during the tests. A circulating water cooling system and a flexible heating source were utilized to provide temperature gradient. Moreover, an in situ thickness meter was used for the specimen thickness measurement and monitoring, consequently, the error in thickness estimation could be minimized. On the other hand, by using finite element method, the structures of heating and cooling modules were optimized to achieve uniform temperature distribution and the simulated results matched well with the experiment. Thermistors were used for experimental comparison and characterization. A data acquisition system was developed via Labview software for data fitting and result analysis.","","978-1-4244-8142-2978-1-4244-8140-8978-1-4244-8141","10.1109/ICEPT.2010.5582397","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5582397","thermal interface materials;LED;thermal conductivity;thermal conductivity tester;steady state;ASTM D5470","Thermal conductivity;Heating;Conductivity;Temperature measurement;Thermal resistance;Bars;Materials","finite element analysis;light emitting diodes;thermal conductivity;thermal management (packaging);thermistors","thermal conductivity meter;thermal interface materials;optoelectronic device;light emitting diode;optical output power;heat integration;thermal management;1D heat transfer theory;ASTM D5470;in-situ thickness measurement;force transducer;circulating water cooling system;temperature gradient;thickness estimation;finite element method;thermistors;data acquisition system;Labview software","","1","8","","","","","","IEEE","IEEE Conferences"
"System Dynamics Modeling and Simulation of China's SO2 Emission Trading Policy","X. Liao; X. Li; B. Zhang","NA; NA; NA","2011 International Conference on Computer Distributed Control and Intelligent Environmental Monitoring","","2011","","","280","283","Based on the theory and method of system dynamics, we establish a China's SO<sub>2</sub> emission trading policy model which includes SO<sub>2</sub> pollution subsystem, policy implementation subsystem and policy effect of evaluation subsystem, and test the model from historical data and parameter sensitivity. And then, using Vensim software, annual SO<sub>2</sub> emissions and policy effect of SO<sub>2</sub> emission trading are simulated running from 2000 to 2020. The testing results indicate that the model has high precision and accuracy, and simulation results show that the contribution of trading policy to controlling SO<sub>2</sub> emission total amount will increase steadily form 2011, This conclusions will provide decision-making bases for designing and optimizing SO<sub>2</sub> pollution control policies in China.","","978-1-61284-278-3978-0-7695-4350","10.1109/CDCIEM.2011.150","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5747817","SO2 pollution;emission trading policy;system dynamics modeling;Simulation","Monitoring;Computers;Distributed control","decision making;pollution control","system dynamics modeling;China SO<sub>2</sub> emission trading policy;policy implementation subsystem;parameter sensitivity;Vensim software;decision making;SO<sub>2</sub> pollution control policy;SO<sub>2</sub>","","","7","","","","","","IEEE","IEEE Conferences"
"Evaluating an Interactive-Predictive Paradigm on Handwriting Transcription: A Case Study and Lessons Learned","L. A. Leiva; V. Romero; A. H. Toselli; E. Vidal","NA; NA; NA; NA","2011 IEEE 35th Annual Computer Software and Applications Conference","","2011","","","610","617","Transcribing handwritten text is a laborious task which currently is carried out manually. As the accuracy of automatic handwritten text recognizers improves, post-editing the output of these recognizers could be foreseen as a possible alternative. Alas, the state-of-the-art technology is not suitable to perform this kind of work, since current approaches are not accurate enough and the process is usually both inefficient and uncomfortable for the user. As alternative, an interactive-predictive paradigm has gained recently an increasing popularity, mainly due to promising empirical results that estimate considerable reductions of user effort. In order to assess whether these empirical results can lead indeed to actual benefits, we developed a working prototype and conducted a field study remotely. Thirteen regular computer users tested two different transcription engines through the above-mentioned prototype. We observed that the interactive-predictive version allowed to transcribe better (less errors and fewer iterations to achieve a high-quality output) in comparison to the manual engine. Additionally, participants ranked higher such an interactive-predictive system in a usability questionnaire. We describe the evaluation methodology and discuss our preliminary results. While acknowledging the known limitations of our experimentation, we conclude that the interactive-predictive paradigm is an efficient approach for transcribing handwritten text.","0730-3157;0730-3157;0730-3157","978-0-7695-4439-7978-1-4577-0544-1978-0-7695-4439","10.1109/COMPSAC.2011.85","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032407","Handwriting Recognition;Interactive Transcription;field study","Manuals;Engines;Humans;Handwriting recognition;Training;Prototypes;Time measurement","handwriting recognition;interactive systems;text analysis","automatic handwritten text recognizer;interactive-predictive paradigm;handwritten text transcription engines","","","15","","","","","","IEEE","IEEE Conferences"
"Asarum subspecies identification with pattern recognition techniques","S. Zhang; Y. Song; Y. Zhao; P. Jia; M. Shang; S. Cai","Dept. of Computer Science and Technology, Tsinghua University, Beijing, China; Dept. of Computer Science and Technology, Tsinghua University, Beijing, China; Dept. of Computer Science and Technology, Tsinghua University, Beijing, China; Dept. of Computer Science and Technology, Tsinghua University, Beijing, China; School of Pharmaceutical Sciences, Peking University, Beijing, China; School of Pharmaceutical Sciences, Peking University, Beijing, China","2011 IEEE 3rd International Conference on Communication Software and Networks","","2011","","","34","38","Chinese medicine pharmacologists examine the chemical features of Chinese medicine materials for identification of the subspecies. In this paper, three different types of chemical data, namely main constituent content, inorganic element content and HPLC fingerprint data of 54 asarum samples are tested and analyzed. Some types of data with strong connection with the sample subspecies classification are firstly filtered out with Principle Component Analysis and separability measure. Chemical features of these data types are then ranked with concern of the correlation with the sample subspecies using SVM RFE. At last, the effect of the filtered out chemical features on the sample subspecies classification are verified using leave-one-out strategy.","","978-1-61284-486-2978-1-61284-485-5978-1-61284-484","10.1109/ICCSN.2011.6014382","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014382","Subspecies Identification;Principle Component Analysis;Separability;SVM RFE","Materials;Fingerprint recognition","medicine;microorganisms;pattern recognition;pharmaceutical technology;principal component analysis;support vector machines","Asarum subspecies identification;pattern recognition techniques;Chinese medicine pharmacologists;chemical features;Chinese medicine materials;chemical data;main constituent content;inorganic element content;HPLC fingerprint data;sample subspecies classification;principle component analysis;separability measure;SVM RFE;leave-one-out strategy","","","10","","","","","","IEEE","IEEE Conferences"
"A Real-Time NetFlow-based Intrusion Detection System with Improved BBNN and High-Frequency Field Programmable Gate Arrays","Q. A. Tran; F. Jiang; J. Hu","NA; NA; NA","2012 IEEE 11th International Conference on Trust, Security and Privacy in Computing and Communications","","2012","","","201","208","Future large-scale complex computing environments present challenges to the real-time intrusion detection systems (IDSs). In this paper, we design a prototype with hybrid software-enabled detection engine on the basis of our improved block-based neural network (BBNN), and integrate it with a high-frequency FPGA board to form a real-time intrusion detection system. The established prototype can seamlessly feed the large-scale NetFlow data obtained from Cisco routers directly into the improved BBNN based IDS. The corresponding BBNN structure and parameter settings have been improved and experimentally tested. Experimental performance comparisons have been conducted against four major schemes of Support Vector Machine (SVM) and Naive Bayes algorithm. The results show that the improved BBNN outperforms other algorithms with respect to the classification and detection performances. The false alarm rate is successfully reduced as low as 5.14% while the genuine detection rate 99.92% is still maintained.","2324-898X;2324-9013","978-1-4673-2172-3978-0-7695-4745","10.1109/TrustCom.2012.51","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6295976","intrusion detection systems (IDSs);artificial neural network (ANN);field programmable gate arrays (FPGA);network security","Field programmable gate arrays;Real time systems;Optimization;Artificial neural networks;Feature extraction;Intrusion detection","Bayes methods;field programmable gate arrays;neural nets;pattern classification;protocols;support vector machines;telecommunication computing;telecommunication network routing;telecommunication security","real-time NetFlow-based intrusion detection system;BBNN;high-frequency field programmable gate arrays;large-scale complex computing environments;IDS;hybrid software-enabled detection engine;block-based neural network;high-frequency FPGA board;Cisco routers;support vector machine;SVM;naive Bayes algorithm;classification performances;detection performances","","12","24","","","","","","IEEE","IEEE Conferences"
"Theoretical and technological limitations of power scaling in network devices","R. Bolla; R. Bruschi; A. Carrega; F. Davoli","Department of Communications, Computer, and Systems Science (DIST), University of Genoa, Via Opera Pia 13, 16145 Genova, Italy; National Inter-University Consortium for, Telecommunications (CNIT), Research Unit of Genoa, Via Opera Pia 13, 16145 Genova, Italy; Department of Communications, Computer, and Systems Science (DIST), University of Genoa, Via Opera Pia 13, 16145 Genova, Italy; Department of Communications, Computer, and Systems Science (DIST), University of Genoa, Via Opera Pia 13, 16145 Genova, Italy","2010 Australasian Telecommunication Networks and Applications Conference","","2010","","","37","42","The largest part of routers and switches, today deployed in production networks, has very limited energy saving capabilities, and substantially requires the same amount of energy both when working at full speed or when being idle. In order to dynamically adapt such energy requirements to the real device work load, current approaches foster the introduction of low power idle and power scaling primitives in entire devices, internal components and network interfaces. Starting from these considerations, we focus on power scaling, and we propose an analysis of the theoretical and technological limitations in adopting such kind of mechanisms. Thus, our contribution is twofold. On one hand, we performed several tests to identify the technological limitations in a software router based on off-the-shelf hardware, which already includes such capabilities. The results achieved show that the power scaling allows a linear trade-off between consumption and network performance, but the time to switch between two power states may cause a non negligible service interruption. On the other hand, regarding the theoretical limitations, we consider the trade-off between the benefit in dynamically adapting the power states within short time-scales and the overhead needed to choose and select the new power state.","","978-1-4244-8172-9978-1-4244-8173-6978-1-4244-8171","10.1109/ATNAC.2010.5680253","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5680253","Green Networking;Power Management;Dynamic Power Scaling","Time frequency analysis;Power demand;Optimization;Program processors;Hardware;Multicore processing","network interfaces;performance evaluation;telecommunication network routing;telecommunication switching","network devices;routers;switches;production networks;energy saving capability;device work load;low power idle;power scaling primitives;internal components;network interfaces;technological limitations;software router;off-the-shelf hardware;linear trade-off;network performance;non negligible service interruption;theoretical limitations;short time-scales","","3","15","","","","","","IEEE","IEEE Conferences"
"A lightweight speculative and predicative scheme for hardware execution","R. Nane; V. Sima; K. Bertels","Computer Engineering Lab Delft University of Technology; Computer Engineering Lab Delft University of Technology; Computer Engineering Lab Delft University of Technology","2012 International Conference on Reconfigurable Computing and FPGAs","","2012","","","1","6","If-conversion is a known software technique to speedup applications containing conditional expressions and targeting processors with predication support. However, the success of this scheme is highly dependent on the structure of the if-statements, i.e., if they are balanced or unbalanced, as well as on the path taken. Therefore, the predication scheme does not always provide a better execution time than the conventional jump scheme. In this paper, we present an algorithm that leverages the benefits of both jump and predication schemes adapted for hardware execution. The results show that performance degradation is not possible anymore for the unbalanced if-statements as well as a speedup for all test cases between 4% and 21%.","2325-6532","978-1-4673-2921-7978-1-4673-2919-4978-1-4673-2920","10.1109/ReConFig.2012.6416721","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6416721","","Hardware;Engines;Program processors;Optimization;Degradation;Computer architecture;Delay","microprocessor chips","lightweight speculative;predicative scheme;hardware execution;software technique;performance degradation","","1","7","","","","","","IEEE","IEEE Conferences"
"SmartGrid role in reducing electrical losses — The InovGrid experience","R. Prata; H. Craveiro; C. A. Santos; E. Quaresma","EDP - Distribuição, Porto, Portugal; EDP - Distribuição, Lisboa, Portugal; EDP - Distribuição Lisboa, Portugal; EDP - Distribuição, Lisboa, Portugal","11th International Conference on Electrical Power Quality and Utilisation","","2011","","","1","6","EDP - Distribuição is implementing InovGrid project, a smartgrids initiative, in Évora municipality. This infrastructure will provide data concerning MV and LV operational status, including load diagrams of the LV feeders and consumption points. InovGrid data presented an opportunity to develop a new built-in module to DPlan, the network analysis software used by EDP-Distribuição. This module allows DPlan to simulate LV grids with greater precision, given the knowledge of power consumption diagrams on each home provided by the infrastructure. Furthermore, EDP-Distribuição developed a methodology aimed at reducing line losses on the LV grid using the data provided by InovGrid. A preliminary test performed on eight LV grids demonstrated a potential line loss reduction of 10% using the developed analysis methodology.","2150-6655;2150-6647;2150-6647","978-1-4673-0378-1978-1-4673-0379","10.1109/EPQU.2011.6128900","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6128900","InovGrid;SmartGrids;Jolue's Losses;LV grids;DPlan","Substations;Loss measurement;Load flow;Voltage measurement;Load modeling;Optimization;Probabilistic logic","power consumption;power engineering computing;smart power grids","SmartGrid;electrical loss reduction;InovGrid experience;Evora municipality;LV operational status;MV operational status;load diagram;LV feeder;network analysis software;EDP-Distribuicao;DPlan module;power consumption diagram;line loss reduction","","","4","","","","","","IEEE","IEEE Conferences"
"Self-learning state-of-available-power prediction for lithium-ion batteries in electrical vehicles","C. Fleischer; W. Waag; Ziou Bai; D. U. Sauer","Electrochemical Energy Conversion and Storage Systems Group, Institute for Power Electronics and Electrical Drives (ISEA), RWTH Aachen University, Germany; Electrochemical Energy Conversion and Storage Systems Group, Institute for Power Electronics and Electrical Drives (ISEA), RWTH Aachen University, Germany; Electrochemical Energy Conversion and Storage Systems Group, Institute for Power Electronics and Electrical Drives (ISEA), RWTH Aachen University, Germany; Electrochemical Energy Conversion and Storage Systems Group, Institute for Power Electronics and Electrical Drives (ISEA), RWTH Aachen University, Germany","2012 IEEE Vehicle Power and Propulsion Conference","","2012","","","370","375","This paper describes an overall system for state-of-available-power (SoAP) prediction for a lithium-ion battery pack. The essential part of this method is based on an adaptive network architecture which utilizes both fuzzy model (FIS) and artificial neural network (ANN) into the framework of adaptive neuro-fuzzy inference system (ANFIS). While battery aging proceeds, the system is capable of delivering accurate power prediction for the typical temperature range. Due to design property of ANN, the network parameters are adapted on-line to the current states (state of charge (SoC), state of health (SoH), temperature). SoC is required as an input parameter to SoAP module and high accuracy is crucial for a reliable self-learning capability. Therefore, a reasonable way to determine the battery state variables is proposed applying a combination of several partly different algorithms. Among others SoC boundary estimation methods, robust extended Kalman filter (REKF) for recalibration of amp hour counters was implemented. ANFIS then achieves the SoAP estimation by means of time forward voltage prognosis (TFVP) before a power pulse. The tradeoff between computational cost of batch-learning and accuracy during on-line adaptation was optimized resulting in a real-time system. The verification was performed on a software-in-the-loop test bench setup using a 53 Ah lithium-ion cell.","1938-8756","978-1-4673-0954-7978-1-4673-0953-0978-1-4673-0952","10.1109/VPPC.2012.6422670","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6422670","","Equations;Q measurement;System-on-a-chip","ageing;battery powered vehicles;calibration;estimation theory;fuzzy neural nets;fuzzy reasoning;Kalman filters;learning (artificial intelligence);lithium;modules;nonlinear filters;power engineering computing;secondary cells","self-learning state-of-available-power prediction;lithium-ion battery;electrical vehicle;SoAP;adaptive network architecture;fuzzy model;FIS;artificial neural network;ANN;adaptive neurofuzzy inference system;ANFIS;battery aging;state of charge;SoC;state of health;SoH;boundary estimation method;robust extended Kalman filter;REKF;amp recalibration;time forward voltage prognosis;TFVP;power pulse;software-in-the-Ioop test bench setup;Li","","5","16","","","","","","IEEE","IEEE Conferences"
"Improving automatic segmentation of tissue-targeted nanoparticles on echographic images","A. Greco; F. Conversano; R. Franchini; S. Casciaro; L. Massoptier; A. Lay-Ekuakille","Institute of Clinical Physiology, National Council of Research, Lecce, Italy; Institute of Clinical Physiology, National Council of Research, Lecce, Italy; Institute of Clinical Physiology, National Council of Research, Lecce, Italy; Institute of Clinical Physiology, National Council of Research, Lecce, Italy; University of New South Wales, Sidney, Australia; Department of Innovation Engineering, University of Salento, Lecce, Italy","2011 IEEE International Symposium on Medical Measurements and Applications","","2011","","","488","491","In recent years, intensive investigations have been undertaken to develop nanoparticle-based cancer targeting agents for various imaging modalities, including ultrasound. Thus, diagnostic paradigms are needed to correctly detect the presence of nanoparticles (NPs) in the anatomic districts. Furthermore, it would be desirable to have algorithms for the automatic recognition of areas where NPs are localized. In this work an experimental optimization of an algorithm for automatic segmentation of nanoparticle-containing tissues is presented and is based on time-frequency processing of the radiofrequency (RF) signals derived from conventional echographic acquisitions. The employed prototypal software (RULES, Radiofrequency Ultrasonic Local Estimator, developed by ELEN SpA, Florence, Italy) correlates spectral parameters to the mechanical and physical properties of the object examined. The effectiveness of the algorithm was evaluated for different configurations of the spectral parameters and tested for different NP size (330 and 660 nm). Accuracy of the algorithm has been quantified through two parameters: sensitivity and specificity. Specifically, the possibility to improve selective identification of NPs disperse in tissue-mimicking layer was investigated. Through subsequent refinement, the most promising results were obtained with algorithm parameter configuration for 330-nm nanoparticles. In particular, it was found that an increase in sensitivity up to 13.9% (from 63% to 76.9%) is achievable by accepting a decrease of 1.5% in specificity (from 99.6 % to 98.1%).","","978-1-4244-9338-8978-1-4244-9336-4978-1-4244-9337","10.1109/MeMeA.2011.5966745","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5966745","Ultrasound;tissue targeting;nanoparticles;automatic segmentation;sensitivity;specifity","Nanoparticles;Sensitivity;Radio frequency;Silicon compounds;Tumors;Ultrasonic imaging;Imaging","biomedical ultrasonics;cancer;image segmentation;medical image processing;nanobiotechnology;nanoparticles","automatic segmentation;tissue-targeted nanoparticle;echographic images;cancer targeting agent;ultrasound imaging;time-frequency processing;RULES software;Radiofrequency Ultrasonic Local Estimator software;sensitivity;specificity","","8","14","","","","","","IEEE","IEEE Conferences"
"A Unified Control Strategy for Electronically Interfaced Distributed Energy Resources","M. B. Delghavi; A. Yazdani","University of Western Ontario, London, Canada; Ryerson University, Toronto, Canada","IEEE Transactions on Power Delivery","","2012","27","2","803","812","This paper proposes a unified control strategy that enables islanded and grid-connected operations of three-phase electronically interfaced distributed energy resources (DERs), with no need for knowing the prevailing mode of operation or switching between two corresponding control architectures. The proposed strategy benefits from both active feedback compensation and the droop method. It allows the employment of the same power circuit and control architecture for the islanded operation as those established and optimized for grid-connected power-electronic converter systems. The proposed strategy can be directly adopted for dispatchable systems (e.g., battery energy storage systems) or, alternatively, it can be embedded in a nested control loop for non-dispatchable systems. This paper presents the mathematical model on which the proposed strategy is based. Further, the effectiveness of the proposed strategy is demonstrated through time-domain simulation of a two-unit test microgrid in the PSCAD/EMTDC software environment.","0885-8977;1937-4208","","10.1109/TPWRD.2011.2181430","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6152189","Current-mode control;distributed energy resource (DER);droop;electronically interfaced;frequency regulation;grid-connected mode;islanded mode;microgrid","Density estimation robust algorithm;Voltage control;Frequency control;Transfer functions;Steady-state;Phase locked loops","distributed power generation;electric current control;energy resources;load dispatching;power convertors;power system control;power system simulation","unified control strategy;electronically interfaced distributed energy resources;islanded operations;grid-connected operations;active feedback compensation;droop method;power-electronic converter systems;nested control loop;non-dispatchable systems;mathematical model;time-domain simulation;two-unit test microgrid;PSCAD/EMTDC software environment","","38","21","","","","","","IEEE","IEEE Journals & Magazines"
"Design and Analysis on Direct Anonymous Attestation for Security Chip","Y. Qin; Q. Wu; X. Chu","NA; NA; NA","2011 2nd International Symposium on Intelligence Information Processing and Trusted Computing","","2011","","","13","18","Direct Anonymous Attestation (DAA) is an anonymous protocol designed for TPM/TCM or other embedding devices. Recently, DAA schemes based on the pairing continues to advance rapidly, but it has a certain problems to put the scheme into application. This paper focuses on the practicability and feasibility for the design of the pairing DAA protocol and system. Our first contribution in this paper is presenting a pairing less DAA scheme for TCM, only 2 pairings required in the whole protocol, Furthermore no pairing computation required inside TCM. The second contribution is to design and implement a test platform prototype for analyzing and evaluating the pairing DAA scheme. We analyze the influence factors like elliptic curve selection, preprocessing and so on. The experiment results show that the trade-off between the security strength and the performance must be taken into the comprehensive consideration for pairing DAA design.","","978-1-4577-1130-5978-0-7695-4498","10.1109/IPTC.2011.11","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6103525","Trusted Computing;Direct Anonymous Attestation;Trust Cryptographic Module;Elliptic Curve Cryptography;Pairing","Protocols;Cryptography;Elliptic curves;Algorithm design and analysis;Optimization;Software","cryptographic protocols;public key cryptography","direct anonymous attestation;security chip;DAA protocol;TPM;TCM;elliptic curve selection","","","17","","","","","","IEEE","IEEE Conferences"
"Homogeneity and distortion-based intra mode decision architecture for H.264/AVC","G. Corrêa; C. Diniz; S. Bampi; D. Palomino; R. Porto; L. Agostini","Informatics Institute (INF) - Federal University of Rio Grande do Sul, Porto Alegre, Brazil; Informatics Institute (INF) - Federal University of Rio Grande do Sul, Porto Alegre, Brazil; Informatics Institute (INF) - Federal University of Rio Grande do Sul, Porto Alegre, Brazil; Federal University of Pelotas, Brazil; Federal University of Pelotas, Brazil; Federal University of Pelotas, Brazil","2010 17th IEEE International Conference on Electronics, Circuits and Systems","","2010","","","591","594","In order to achieve the best coding performance, the H.264/AVC encoder must choose the best coding mode and the best block size in terms of bit-rate and distortion. The H.264/AVC reference software applies the Rate-Distortion Optimization (RDO) technique, which makes the encoding process a complex task in applications which require real-time operation. This paper presents a fast intra decision process and its architecture, where the mode and the block size decisions are performed based on the block distortion and homogeneity. The performed tests show that the method achieves PSNR results similar to the RDO technique and a low bit-rate increase. On the other hand, the gains in terms of complexity are near to 148 times when compared to RDO method. Also, the implemented architecture is capable of processing 1080p videos in real time.","","978-1-4244-8157-6978-1-4244-8155-2978-1-4244-8156","10.1109/ICECS.2010.5724581","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5724581","","Automatic voltage control;Logic gates;PSNR","codecs;distortion;video coding","homogeneity-based intra mode decision architecture;distortion-based intra mode decision architecture;H.264/AVC encoder;H.264/AVC reference software;rate-distortion optimization technique;RDO technique;encoding process;block size decisions;video coding","","5","10","","","","","","IEEE","IEEE Conferences"
"Sample Adaptive Offset in the HEVC Standard","C. Fu; E. Alshina; A. Alshin; Y. Huang; C. Chen; C. Tsai; C. Hsu; S. Lei; J. Park; W. Han","MediaTek, Hsinchu, Taiwan; Digital Media and Communication Research and Development Center, Samsung Electronics, Suwon, Korea; Digital Media and Communication Research and Development Center, Samsung Electronics, Suwon, Korea; MediaTek, Hsinchu, Taiwan; MediaTek, Hsinchu, Taiwan; MediaTek, Hsinchu, Taiwan; MediaTek, Hsinchu, Taiwan; MediaTek, Hsinchu, Taiwan; Digital Media and Communication Research and Development Center, Samsung Electronics, Suwon, Korea; Gachon University, Seongnam, Korea","IEEE Transactions on Circuits and Systems for Video Technology","","2012","22","12","1755","1764","This paper provides a technical overview of a newly added in-loop filtering technique, sample adaptive offset (SAO), in High Efficiency Video Coding (HEVC). The key idea of SAO is to reduce sample distortion by first classifying reconstructed samples into different categories, obtaining an offset for each category, and then adding the offset to each sample of the category. The offset of each category is properly calculated at the encoder and explicitly signaled to the decoder for reducing sample distortion effectively, while the classification of each sample is performed at both the encoder and the decoder for saving side information significantly. To achieve low latency of only one coding tree unit (CTU), a CTU-based syntax design is specified to adapt SAO parameters for each CTU. A CTU-based optimization algorithm can be used to derive SAO parameters of each CTU, and the SAO parameters of the CTU are inter leaved into the slice data. It is reported that SAO achieves on average 3.5% BD-rate reduction and up to 23.5% BD-rate reduction with less than 1% encoding time increase and about 2.5% decoding time increase under common test conditions of HEVC reference software version 8.0.","1051-8215;1558-2205","","10.1109/TCSVT.2012.2221529","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6324411","Advanced video coding (AVC);band offset;edge offset;H.264;High Efficiency Video Coding (HEVC);Joint Collaborative Team on Video Coding (JCT-VC);sample adaptive offset (SAO);video standard","Encoding;Video coding;Filtering;Decoding;Standards","distortion;filtering theory;image classification;video coding","sample adaptive offset;SAO;high efficiency video coding;HEVC standard;in-loop filtering technique;sample distortion reduction;sample classification;coding tree unit;CTU based optimization;syntax design;BD-rate reduction;HEVC reference software version 8.0;edge offset;band offset","","141","34","","","","","","IEEE","IEEE Journals & Magazines"
"Development of hybrid city bus's driving cycle","Shiqi Ou; Yafu Zhou; Jing Lian; Pu Jia; Baoyu Tian","School of Automotive Engineering, Faculty of Vehicle Engineering and Mechanics, State Key Laboratory of Structural Analysis for Industrial Equipment, Dalian University of Technology, 116024, China; School of Automotive Engineering, Faculty of Vehicle Engineering and Mechanics, State Key Laboratory of Structural Analysis for Industrial Equipment, Dalian University of Technology, 116024, China; School of Automotive Engineering, Faculty of Vehicle Engineering and Mechanics, State Key Laboratory of Structural Analysis for Industrial Equipment, Dalian University of Technology, 116024, China; School of Automotive Engineering, Faculty of Vehicle Engineering and Mechanics, State Key Laboratory of Structural Analysis for Industrial Equipment, Dalian University of Technology, 116024, China; School of Automotive Engineering, Faculty of Vehicle Engineering and Mechanics, State Key Laboratory of Structural Analysis for Industrial Equipment, Dalian University of Technology, 116024, China","2011 International Conference on Electric Information and Control Engineering","","2011","","","2112","2116","We in this study adopted ""Electric Vehicle Information Terminals and Remote Monitoring System"" which is researched and developed independently to collect vehicle's data. We mainly used MATLAB software to statistically analyze the monitored hybrid buses' actual driving data on the road with micro-trip analysis method, and constructed the comprehensive driving cycle of hybrid bus for this city. The driving cycle construction method and procedure were further perfected in this study. Then the driving cycle was simulate calculated and compared with road driving conditions in the engine and motor's operation performance and fuel consumption in order to verify the accuracy of the constructed driving cycle and widely applicability of the construction method. The driving cycle and relevant results can directly affect the selection of hybrid electric vehicles (HEVs) components and the control strategy, and be the basis of testing the hybrid electric vehicle's driving system and motor system optimization matching, furthermore provide evaluation standard for the hybrid city bus's fuel consumption.","","978-1-4244-8039-5978-1-4244-8036-4978-1-4244-8038","10.1109/ICEICE.2011.5777149","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5777149","data collection;hybrid city bus;driving cycle;simulated cacullation","Vehicles;Cities and towns;Acceleration;Fuels;Engines;Roads;Monitoring","automobiles;hybrid electric vehicles;internal combustion engines;mathematics computing;statistical analysis","hybrid city bus driving cycle;electric vehicle information terminal;remote monitoring system;MATLAB software;statistical analysis;microtrip analysis method;engine operation performance;motor operation performance;fuel consumption;hybrid electric vehicle;control strategy;motor system optimization matching","","4","5","","","","","","IEEE","IEEE Conferences"
"Design, simulation and implementation of a Full Bridge Series-Parallel Resonant DC-DC converter using ANN controller","M. J. M. Imanieh; Z. Malekjamshidi","Department of Electrical Eng, Islamic Azad, University-Fasa branch, Fasa, Iran; Department of Electrical Eng, Islamic Azad, University-Marvdasht branch, Marvdasht, Iran","2011 International Siberian Conference on Control and Communications (SIBCON)","","2011","","","97","103","A new method of control for high-voltage Full Bridge Series-Parallel Resonant (FBSPR) DC-DC converter with capacitive output filter, using Artificial Neural Networks (ANN) is proposed in this paper. The output voltage regulation obtained via high switching frequency and Soft switching operation (ZCS and ZVS technologies) to decrease the losses and optimize the efficiency of converter. In the following sections, a Small-Signal Model of FBSPR converter on base of first harmonic analysis and the generalized averaging method is derived. Then the obtained model is used to simulate the dynamic behavior of real converter using Matlab software. It was also used to obtain ideal control signals which are the desired ANN inputs and outputs and were saved as a training data set. The data set is then used to train the ANN to mimic the behavior of the ideal controller. In fact the ANN controller is trained according to the small signal model of converter and the ideal operating points. To compare the performances of simulated and practical ANN controller, a prototype is designed and implemented. The prototype is tested for step changes in both output load and reference voltage at steady state and under transient conditions. Comparison between experimental and simulations show a very good agreement and the reliability of ANN based controllers.","","978-1-4577-1070-4978-1-4577-1069-8978-1-4577-1068","10.1109/SIBCON.2011.6072604","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6072604","Full Bridge;Series-Parallel Resonant Converter;Artificial Neural Networks;ANN;ZVS;ZCS","Mathematical model;Artificial neural networks;Equations;Steady-state;Zero voltage switching;Resonant frequency;Bridge circuits","DC-DC power convertors;neurocontrollers;power system reliability;resonant power convertors;set theory;switching convertors;voltage control;zero current switching;zero voltage switching","high voltage full bridge series-parallel resonant DC-DC converter optimization;voltage regulation;high switching frequency;soft switching operation;FBSPR converter;harmonic analysis;averaging method;Matlab software;control signal;data set training;small signal model;output load;transient condition;ANN based controller reliability","","","9","","","","","","IEEE","IEEE Conferences"
"Static simulation: A method for power and energy estimation in embedded microprocessors","O. Acevedo-Patiño; M. Jiménez; A. J. Cruz-Ayoroa","Electrical and Computer Engineering Department, University of Puerto Rico, Mayagüez Campus, 00681-5000, Spain; Electrical and Computer Engineering Department, University of Puerto Rico, Mayagüez Campus, 00681-5000, Spain; Electrical and Computer Engineering Department, University of Puerto Rico, Mayagüez Campus, 00681-5000, Spain","2010 53rd IEEE International Midwest Symposium on Circuits and Systems","","2010","","","41","44","Current methodologies for software-level power and energy estimation use a microprocessor's power model combined with specialized tools that profile the program under study. These tools commonly rely on real-time program execution or simulations to gather the information needed, a process that usually requires a full set of real run-time data. This work proposes the use of static code simulation as an alternative to analyze and predict the program's behavior. This, in combination with a microprocessor's power model, allows to estimate power and energy with only a small amount of run-time data. Furthermore, the low execution time of the proposed method allows for its use as in iterative power optimizers. We present results obtained for a set of representative benchmark programs applied ran on a PowerPC 603e microprocessor. Power and energy estimates with mean absolute errors below 7% and 15%, respectively, are reported for the analyzed test cases.","1558-3899;1548-3746;1548-3746","978-1-4244-7773-9978-1-4244-7771-5978-1-4244-7772","10.1109/MWSCAS.2010.5548556","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5548556","","Microprocessors;Computational modeling;Runtime;Energy consumption;Instruments;Predictive models;Power system modeling;Power generation;Costs;Current measurement","embedded systems;microprocessor chips","power estimation;energy estimation;embedded microprocessor;software-level power;power model;static code simulation;iterative power optimizer;representative benchmark programs;PowerPC 603e microprocessor","","2","19","","","","","","IEEE","IEEE Conferences"
"IP-cores design for the kNN classifier","E. S. Manolakos; I. Stamoulias","Department of Informatics and Telecommunications, University of Athens Panepistimioupolis, Ilisia, 15784, Athens, Greece; Department of Informatics and Telecommunications, University of Athens Panepistimioupolis, Ilisia, 15784, Athens, Greece","Proceedings of 2010 IEEE International Symposium on Circuits and Systems","","2010","","","4133","4136","We present the systematic design of two linear array IP cores for the k-nearest neighbor (k-NN) benchmark classifier. The need for real-time classification of data vectors with possibly thousands of features (dimensions) motivates the implementation of this widely used algorithm in hardware in order to achieve very high performance by exploiting block pipelining and parallel processing. The two linear array architectures that we designed have been described as soft IP cores in fully parameterizable VHDL that can be used to synthesize effortlessly different k-NN parallel architectures for any desirable combination of the problem size parameters. They have been evaluated for a large variety of parameter combinations and Xilinx FPGAs. It is shown that they can be used to solve efficiently very large size k-NN classification problems, even with thousands of training vectors or vector dimensions, using a single, moderate size FPGA device. Furthermore the FPGA implementations exceed by a factor of two the performance of optimized NVIDIA CUDA API software implementations for the powerful GeForce 8800GTX GPU.","0271-4302;2158-1525","978-1-4244-5308-5978-1-4244-5309","10.1109/ISCAS.2010.5537602","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5537602","","Field programmable gate arrays;Testing;Hardware;Vectors;Pipeline processing;Parallel architectures;Nearest neighbor searches;Parallel processing;Error analysis;Computer vision","field programmable gate arrays;hardware description languages;logic design;parallel architectures;parallel processing;pattern classification;pipeline processing","kNN classifier;k-nearest neighbor benchmark classifier;linear array IP core;data vector;block pipelining;parallel processing;VHDL;kNN parallel architecture;Xilinx FPGA;NVIDIA CUDA API software;GeForce 8800GTX GPU","","8","11","","","","","","IEEE","IEEE Conferences"
"A new mobile phone system architecture for the navigational travelling blind","C. So-In; S. Arch-Int; C. Phaudphut; K. Rujirakul; N. Weeramongkonlert","Department of Computer Science, KhonKaen University, 40002, TH; Department of Computer Science, KhonKaen University, 40002, TH; Department of Computer Science, KhonKaen University, 40002, TH; Department of Computer Science, KhonKaen University, 40002, TH; Department of Computer Science, KhonKaen University, 40002, TH","2012 Ninth International Conference on Computer Science and Software Engineering (JCSSE)","","2012","","","54","59","This paper introduces a new mobile phone system architecture to aid the blind in travelling. The architecture has four main units: voice/speech recognition, Global Positioning System (GPS) and map services, ultrasonic sensor, and image processing service. These four subsystems function simultaneously within a single mobile device using web service interaction. The paper describes the instrument and component of the key design as well as implementation decisions. The feasibility of the architecture has been tested with Windows Phone 7 (HTC HD7), and cooperated with the real blind. Additionally, we have evaluated the performance of Hough Transform for straight line detection - the main component intensively using the processing power - and then we have optimized the Hough Transform to reduce the complexity leading to lower average response time.","","978-1-4673-1921-8978-1-4673-1920-1978-1-4673-1919","10.1109/JCSSE.2012.6261925","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6261925","Blind;Global Positioning System;GPS;Hough Transform;Image Processing;Mapping Service;Mobile Phone System Architecture;Navigation System;Sonar;Traveling Aid;Voice and Speech Recognition;Windows Phone 7;WP7","Global Positioning System;Mobile handsets;Acoustics;Sonar;Image processing;Speech recognition;Transforms","cartography;Global Positioning System;handicapped aids;Hough transforms;mobile handsets;object detection;speech recognition;Web services","mobile phone system architecture;navigational travelling blind;voice-speech recognition;Global Positioning System;GPS;map services;ultrasonic sensor;image processing service;Web service interaction;Windows Phone 7;Hough transform;straight line detection","","2","12","","","","","","IEEE","IEEE Conferences"
"Adaptive Mechanism for P2P Video Streaming Using SVC and MDC","F. de Asis López Fuentes","NA","2010 International Conference on Complex, Intelligent and Software Intensive Systems","","2010","","","457","462","In this paper we propose and evaluate a combined SVC-MDC (Scalable Video Coding & Multiple Description Video Coding) video coding scheme for P2P video multicast. The proposed scheme is based on a full cooperation established between the peer sites, which contribute their upload capacity during video distribution. The source site splits the video content into many small blocks and assigns each block to a single peer for redistribution. Our solution is implemented in a fully meshed P2P network in which peers are connected to each other via UDP links. The video content is encoded using SVC. We present a flow control mechanism that allows us to dynamically optimize the overall throughput and to automatic adjust the video quality for each peer. Thus, peers with different upload capacity receive different video quality. We also combine SVC with MDC to alleviate the packet loss problem. We have implemented and tested this approach in the PlanetLab infrastructure. The obtained results show that our solution achieves good performance and remarkable video quality in the presence of packet loss.","","978-1-4244-5918-6978-1-4244-5917-9978-0-7695-3967","10.1109/CISIS.2010.111","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5447463","peer-to-peer networks;video streaming;scalable video coding;multiple description video coding","Streaming media;Static VAr compensators;Video coding;Throughput;Video sharing;Delay;Bandwidth;Peer to peer computing;Web and internet services;Competitive intelligence","peer-to-peer computing;telecommunication congestion control;transport protocols;video coding;video streaming","P2P video streaming;scalable video coding;multiple description video coding;UDP links;flow control mechanism;packet loss problem;PlanetLab infrastructure","","1","17","","","","","","IEEE","IEEE Conferences"
"Design, simulation and modelling for nanopackaging applications: current capabilities and future requirements","C. Bailey","Computational Mechanics and Reliability Group, University of Greenwich, London, UK","2012 12th IEEE International Conference on Nanotechnology (IEEE-NANO)","","2012","","","1","1","Summary form only given. The electronics packaging industry is undergoing dramatic changes due to the market need for increased product functionality, miniaturisation and reliability. According to the ITRS roadmap future higher value systems will combine the latest advances in System on Chip (More-Moore) with the diversification provided by System in Package (More-Than-Moore). System in Package and Wafer Level Packaging, with the introduction of nanotechnology (e.g. Carbon Nanotubes), provides the ability to integrate components with different functions (Digital, Analogue, RF, Optical, MEMS, etc) resulting in greater product functionality all in a smaller space. These trends are placing severe demands on the packaging design engineer in terms of optimising electrical, thermal and reliability performance. Design for X (DfX; X=Manufacturability, Packaging, Test, Reliability, etc) methodologies and associated software tools play a very important part in product design and development. This presentation will discuss the latest trends and developments in design, modelling and simulation for nanopackaging applications. The use of Multi-Physics/scale modelling tools will be discussed and examples provided demonstrating the use of molecular dyanimics and finite element modelling and the coupling techniques used to link results at the atomistic scale to the mico and maco scales.","1944-9399;1944-9399","978-1-4673-2200-3978-1-4673-2198-3978-1-4673-2199","10.1109/NANO.2012.6322238","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6322238","","Abstracts","finite element analysis;integrated circuit design;integrated circuit reliability;nanoelectronics;product design;system-in-package;system-on-chip;wafer level packaging","nanopackaging design;electronics packaging industry;reliability;ITRS roadmap;system-on-chip;more-than-Moore;system-in-package;wafer level packaging;nanotechnology;carbon nanotubes;packaging design engineer;software tools;product design;multiphysics-scale modelling tools;molecular dynamics;finite element modelling;coupling techniques","","","","","","","","","IEEE","IEEE Conferences"
"A Dynamic Core Grouping Approach to Improve Raw Architecture Many-core Processor Performance","Z. Wan","NA","2011 Sixth International Symposium on Parallel Computing in Electrical Engineering","","2011","","","31","35","The ongoing move of hardware platforms to many-core processor challenges the traditional software design methodology. It is critical to develop new programming paradigms and efficient ways to port legacy applications. This paper analyzed a typical packet processing application and also the cache hierarchy and behavior of Raw architecture many-core processor. It presented an easy to implement run-time dynamic core grouping approach to improve the system performance. This approach reduced the cache swap latency by grouping neighbor cores attached to the mesh network. It optimized the scale of group by experimental data got beforehand. The test results showed this approach can improve the Deep Packet Inspection (DPI) system performance around 10% with very minor code change.","","978-1-4577-0078-1978-0-7695-4397","10.1109/PARELEC.2011.30","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770397","Many-core Processor;Core Grouping;Packet Processing;Deep Packet Inspection","Throughput;Hardware;Tiles;Computer architecture;System performance;Mesh networks;Inspection","multiprocessing systems;parallel architectures;performance evaluation","raw architecture many-core processor performance;hardware platforms;software design methodology;programming paradigms;legacy applications;packet processing application;cache hierarchy;run-time dynamic core grouping approach;deep packet inspection system","","","7","","","","","","IEEE","IEEE Conferences"
"Fatigue life analysis of a new semi-trailer frame based on FEA method","Wang Feng; Wu Yong-hai","Faculty of Traffic Engineering, Huaiyin Institute of Technology, Huai'an, China; Faculty of Traffic Engineering, Huaiyin Institute of Technology, Huai'an, China","2010 IEEE 11th International Conference on Computer-Aided Industrial Design & Conceptual Design 1","","2010","2","","1295","1298","In allusion to the frame of a certain semi-trailer, through adopting ANSYS software, its stress response, modal response and random vibration response were calculated with FEA method under various load cases. The finite element fatigue analysis was carried out in the form of load spectrum, as well as fatigue life and the stress strain variation laws of the vehicle frame under dynamic loads have been obtained, which provide the valuable reference for the vehicle frame structure improvement and optimization. The analysis method adopted in this paper have significance to shorten product design cycle, save cost of testing and producing, reduce the development cost and guarantee the security, stability and reliability of the product.","","978-1-4244-7974-0978-1-4244-7973-3978-1-4244-7972","10.1109/CAIDCD.2010.5681971","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5681971","Frame;Fatigue life;FEA;ANSYS","Welding;Electronic mail;Stress;Fatigue;Tires","automobile industry;automotive components;fatigue;finite element analysis;mechanical engineering computing;modal analysis;product design;product development;reliability;stress analysis;stress-strain relations","fatigue life analysis;FEA method;ANSYS software;stress response;modal response;random vibration response;finite element fatigue analysis;stress strain variation laws;vehicle frame structure improvement;product design cycle;development cost reduction;product reliability;product stability;product security;semitrailer frame","","","6","","","","","","IEEE","IEEE Conferences"
"An Adaptive Performance Modeling Approach to Performance Profiling of Multi-service Web Applications","X. Huang; W. Wang; W. Zhang; J. Wei; T. Huang","NA; NA; NA; NA; NA","2011 IEEE 35th Annual Computer Software and Applications Conference","","2011","","","4","13","The performance of multi-service applications are known to be determined mainly by the interactions between workload and behaviors of the application. The change of workload can lead to dynamic service demands on system resources, and even cause dynamic bottleneck switches between services inside the application. In this paper, to profiling large-applications' behaviors, and help to locate the bottleneck and optimize their capacities, we focus on modeling their behavior according to the workload. Although this topic has been well studied at testing stage, building such a model under live workload remains a challenge, because the workload and application behaviors are time-varying. To tackle this problem, we propose an adaptive approach to build and rebuild performance model according to log files. Both the user behaviors and their corresponding internal service relations are modeled, and the CPU time consumed by each service is also obtained through Kalman filter, which can ""absorb"" some level of noise in real-world data. Our model can explain the behaviors of both the whole application and the individual services, and provide valuable information for capacity planning and bottleneck detection. At last, our work is evaluated with TPC-W bench mark, whose results can demonstrate the effectiveness of our approach.","0730-3157;0730-3157;0730-3157","978-0-7695-4439-7978-1-4577-0544-1978-0-7695-4439","10.1109/COMPSAC.2011.10","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032318","web applications;capacity planning;bottleneck detection;performance modeling;Kalman filter","Servers;Monitoring;Kalman filters;Adaptation models;Capacity planning;Analytical models;Databases","Kalman filters;Web services","adaptive performance modeling approach;performance profiling;multiservice Web application;dynamic service demands;system resources;dynamic bottleneck switches;log files;internal service relation;Kalman filter;capacity planning;bottleneck detection","","1","33","","","","","","IEEE","IEEE Conferences"
"Cache contention and application performance prediction for multi-core systems","Chi Xu; Xi Chen; R. P. Dick; Z. M. Mao","ECE Department, Univeristy of Minnesota, Minneapolis, 55455, USA; EECS Department, University of Michigan, Ann Arbor, 48109, USA; EECS Department, University of Michigan, Ann Arbor, 48109, USA; EECS Department, University of Michigan, Ann Arbor, 48109, USA","2010 IEEE International Symposium on Performance Analysis of Systems & Software (ISPASS)","","2010","","","76","86","The ongoing move to chip multiprocessors (CMPs) permits greater sharing of last-level cache by processor cores but this sharing aggravates the cache contention problem, potentially undermining performance improvements. Accurately modeling the impact of inter-process cache contention on performance and power consumption is required for optimized process assignment. However, techniques based on exhaustive consideration of process-to-processor mappings and cycle-accurate simulation are inefficient or intractable for CMPs, which often permit a large number of potential assignments. This paper proposes CAMP, a fast and accurate shared cache aware performance model for multi-core processors. CAMP estimates the performance degradation due to cache contention of processes running on CMPs. It uses reuse distance histograms, cache access frequencies, and the relationship between the throughput and cache miss rate of each process to predict its effective cache size when running concurrently and sharing cache with other processes, allowing instruction throughput estimation.We also provide an automated way to obtain process-dependent characteristics, such as reuse distance histograms, without offline simulation, operating system (OS) modification, or additional hardware. We tested the accuracy of CAMP using 55 different combinations of 10 SPEC CPU2000 benchmarks on a dual-core CMP machine. The average throughput prediction error was 1.57%.","","978-1-4244-6024-3978-1-4244-6023","10.1109/ISPASS.2010.5452065","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5452065","","Throughput;Histograms;Frequency estimation;Power system modeling;Energy consumption;Multicore processing;Degradation;Predictive models;Operating systems;Hardware","cache storage;multiprocessing systems;performance evaluation","cache contention;performance prediction;multi-core systems;chip multiprocessors;last-level cache;reuse distance histograms;cache access frequencies;cache miss rate;operating system modification;CAMP;SPEC CPU2000;cache aware performance model;least-recently-used","","13","19","","","","","","IEEE","IEEE Conferences"
"High Speed Dual-Layer Blu-Ray Recordable Disc Without Silver/Silver Alloy Semi-Reflective Layer","Y. Kuo; Y. Hung; C. Li; Y. Huang; K. Li","R&D Department,, CMC Magnetics Corporation,, Tao-Yuan, Taiwan; R&D Department, CMC Magnetics Corporation, Tao-Yuan, Taiwan; R&D Department, CMC Magnetics Corporation, Tao-Yuan, Taiwan; R&D Department, CMC Magnetics Corporation, Tao-Yuan, Taiwan; R&D Department, CMC Magnetics Corporation, Tao-Yuan, Taiwan","IEEE Transactions on Magnetics","","2011","47","3","568","571","In this paper, a design of dual-layer Blu-ray recordable (DL BD-R) disc without silver or silver alloy semi-reflective layer is introduced. Five potential dielectric reflector materials, including the AlN, BN, Nb<sub>2</sub>O<sub>5</sub>, TiO<sub>2</sub>and ZnS-SiO<sub>2</sub>are evaluated and tested to compensate the reflectivity of layer 1 after removing the metallic semi-reflective layer. With the help of optical simulation software, reflectivity in both layers could be optimized by fine tuning thickness of each material and the recording performance was examined in each condition. Furthermore, a simplified layer 1 stack was proposed and evaluated. The disc with only Nb<sub>2</sub>O<sub>5</sub>layer as semi-reflector in layer 1 can achieve the desired performance of 6× recording speed and provide the benefit of better mass production from the simplified practical film structure point of view.","0018-9464;1941-0069","","10.1109/TMAG.2010.2100811","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5721866","Dual-layer Blu-ray recordable (DL BD-R) disc;non-silver semi-reflective layer;optical simulation;sputtering","Silver;Dielectrics;Materials;Simulation;Heating;Jitter","aluminium compounds;boron compounds;dielectric thin films;niobium compounds;optical disc storage;silicon compounds;sputtered coatings;titanium compounds;zinc compounds","high speed dual-layer blu-ray recordable disc;silver-silver alloy semi-reflective layer;potential dielectric reflector materials;DL BD-R disc;metallic semireflective layer;optical simulation software;mass production;film structure;Nb2O5;AlN;BN;TiO2;ZnS-SiO2","","","5","","","","","","IEEE","IEEE Journals & Magazines"
"The design of duplexer with TE<inf>01δ</inf>mode dielectric resonator","M. Xu; H. Li; Z. Wang","FiberHome Technologies Institute, W.R.I., Wuhan, China; FiberHome Tech. Deve. Dept., W.R.I., Wuhan, China; HongXin Site Solutions Dept., W.R.I., Wuhan, China","Proceedings of 2012 5th Global Symposium on Millimeter-Waves","","2012","","","231","235","Compared to the traditional metal cavity device, duplexer with TE<sub>01δ</sub>mode dielectric resonator (DR) has more advantages such as smaller size, lower insertion loss (IL), better temperature stability of resonant frequency and so on. Based on microwave theory, this paper proposes a new type of duplexer which partly consists of TE<sub>01δ</sub>mode DR. It takes full advantage of the performance of different materials to make it conform as closely as possible to the signal transmission characteristics of the different frequency bands and to optimize the quality of the transmission channel. The design of TE<sub>01δ</sub>mode DR is presented at first of this paper. Then it gets the equivalent circuit of duplexer by formula deduction. The duplexer is simulated by using HFSS and CoupleFil software and it comes to conclusion that the test and the simulation results are consistent at last. In the fields of engineering of application, this design program can improve the performance and reduce the size of duplexer.","","978-1-4673-1305-6978-1-4673-1302-5978-1-4673-1304","10.1109/GSMM.2012.6314043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6314043","HFSS;TE01δMode;Duplexer;Ddielectric Resonator","Band pass filters;Dielectrics;Filtering theory;Cavity resonators;Resonator filters;Prototypes;Microwave filters","dielectric resonators;equivalent circuits;multiplexing equipment","duplexer;TE01δmode dielectric resonator;metal cavity device;insertion loss;temperature stability;microwave theory;signal transmission characteristics;transmission channel;equivalent circuit;HFSS simulation;CoupleFil software","","","10","","","","","","IEEE","IEEE Conferences"
"An 8-12GHz capacitive power sensor based on MEMS cantilever beam","Z. Yi; X. Liao; Z. Zhu","Key Laboratory of MEMS of the Ministry of Education, Southeast University, Nanjing, 210096, China; Key Laboratory of MEMS of the Ministry of Education, Southeast University, Nanjing, 210096, China; Key Laboratory of MEMS of the Ministry of Education, Southeast University, Nanjing, 210096, China","SENSORS, 2011 IEEE","","2011","","","1958","1961","In this paper, an 8-12GHz capacitive power sensor based on MEMS cantilever beam is proposed. When the displacement of the cantilever beam is far less than the initial height of the air gap, the capacitance between the measuring electrode and the cantilever beam has a linear dependence on the input power of the RF signal. The novelty of the presented design provides not only negligible disturbance of the input signal but also an improved sensitivity because of small elastic coefficient of the cantilever beam. Impedance compensating technology by modifying the slot width of the CPW transmission line is adopted and the validity is verified by the simulation of HFSS software. The presented power sensor has been designed, optimized and fabricated using GaAs MMIC process. The measured return loss is less than -25dB and the insertion loss is around 0.1dB at 8-12GHz. DC voltage measurement and RF power measurement have been performed to test against the validity of the design. A sensitivity of 6aF·mW<sup>-1</sup>is obtained which is limited by the thickness of the cantilever beam, the initial height of the air gap and the area of the measuring electrode.","1930-0395;1930-0395","978-1-4244-9289-3978-1-4244-9290-9978-1-4244-9288","10.1109/ICSENS.2011.6127229","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6127229","","Structural beams;Transmission line measurements;Loss measurement;Power measurement;Micromechanical devices;Capacitance;Voltage measurement","cantilevers;capacitive sensors;coplanar transmission lines;coplanar waveguides;gallium arsenide;III-V semiconductors;microelectrodes;microsensors;microwave detectors;MMIC;power measurement;slot lines;voltage measurement","capacitive power sensor;MEMS cantilever beam;cantilever beam displacement;air gap height;electrode measurement;RF signal;cantilever beam elastic coefficient;impedance compensating technology;CPW transmission line;HFSS software simulation;MMIC process;return loss;insertion loss;DC voltage measurement;RF power measurement;frequency 8 GHz to 12 GHz;GaAs","","4","6","","","","","","IEEE","IEEE Conferences"
"Research of chirped dispersive delay line based on the dispersion characteristics of microstrip lines","H. Zhou; B. Wang; S. Ding","Institute of Applied Physics, University of Electronic Science and Technology of China, Chengdu, 610054, China; Institute of Applied Physics, University of Electronic Science and Technology of China, Chengdu, 610054, China; Institute of Applied Physics, University of Electronic Science and Technology of China, Chengdu, 610054, China","The 2012 International Workshop on Microwave and Millimeter Wave Circuits and System Technology","","2012","","","1","5","Based on the dispersion characteristics of microstrip lines and theory of the coupled transmission line, this paper presents a dispersive delay line which works in the transmission mode and could be used in analog signal processing. This paper theoretically and experimentally confirms that desired group delay responses could be achieved by tuning the lengths of cascaded C-sections in the microstrip structure. The optimized structure is simulated by the electromagnetic simulation software CST Microwave Studio, and a satisfied chirped group-delay is achieved ranging from 0.2 GHz to 3.85 GHz. The experimental prototype is a coplanar design integrated on a microstrip substrate with a shielded aluminum lid above it. The dispersion of the cascaded C-sections has been verified by prototype test and its time-domain analysis.","","978-1-4673-1895-2978-1-4673-1893-8978-1-4673-1894","10.1109/MMWCST.2012.6238177","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6238177","Analog signal processing;chirped delay;C-sections;dispersion;microstrip lines;time-domain analysis;transmission mode","Dispersion;Microstrip;Chirp;Delay;Delay lines;Aluminum;Arrays","microstrip lines;time-domain analysis;transmission line theory","chirped dispersive delay line;dispersion characteristic;microstrip lines;coupled transmission line;transmission mode;analog signal processing;group delay response;cascaded C-section;microstrip structure;electromagnetic simulation software;CST Microwave Studio;satisfied chirped group-delay;coplanar design;microstrip substrate;time-domain analysis;frequency 0.2 GHz to 3.85 GHz","","1","10","","","","","","IEEE","IEEE Conferences"
"Design, Simulation and implementation of an adaptive controller on base of artificial neural networks for a resonant DC-DC converter","M. Jafari; Z. Malekjamshidi","Department of Electrical Engineering, Fasa branch, Islamic Azad University, Iran; Department of Electrical Engineering, Marvdasht branch, Islamic Azad University, Iran","2011 IEEE Ninth International Conference on Power Electronics and Drive Systems","","2011","","","1043","1046","A new method of control for high-voltage Full Bridge Series-Parallel Resonant (FBSPR) DC-DC converter with capacitive output filter, using Artificial Neural Networks (ANN) is proposed in this paper. The output voltage regulation obtained via high switching frequency and Soft switching operation (ZCS and ZVS technologies) to decrease the losses and optimize the efficiency of converter. In the following sections, The small signal model of converter is used to simulate the dynamic behavior of real converter using Matlab software. It was also used to obtain ideal control signals which are the desired ANN inputs and outputs and were saved as a training data set. The data set is then used to train the ANN to mimic the behavior of the ideal controller. In fact the ANN controller is trained according to the small signal model of converter and the ideal operating points. To compare the performances of simulated and practical ANN controller, a prototype is designed and implemented and is tested for step changes in both output load and reference voltage. Comparison between experimental and simulations show a very good agreement and the reliability of ANN based controllers.","2164-5264;2164-5256;2164-5256","978-1-4577-0001-9978-1-61284-999-7978-1-4577-0000","10.1109/PEDS.2011.6147388","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6147388","Full Bridge;Series-Parallel Resonant Converter;Artificial Neural Networks;ANN;ZVS;ZCS","Artificial neural networks;Mathematical model;Voltage control;Resonant frequency;Zero voltage switching;Equations;Zero current switching","adaptive control;bridge circuits;DC-DC power convertors;neurocontrollers;reliability;resonant power convertors;switching convertors;zero current switching;zero voltage switching","adaptive controller;artificial neural networks;capacitive output filter;output voltage regulation;zero current switching;zero voltage switching;small signal model;dynamic behavior;Matlab software;reliability","","1","9","","","","","","IEEE","IEEE Conferences"
"A novel modeling method of virtual lunar surface based on lunar surface roughness","Xiaolan Wang; Rongben Wang","Intelligent Vehicle Group, Transportation and Traffic, College Jilin University, Changchun, China; Intelligent Vehicle Group, Transportation and Traffic, College Jilin University, Changchun, China","2011 International Conference on Remote Sensing, Environment and Transportation Engineering","","2011","","","17","20","Simulation technology is an efficient approach for lunar rover designing, analyzing, optimizing. Constructing virtual lunar surface is a base of such researching works. A novel modeling method of virtual lunar surface based on lunar surface roughness was proposed. In this method, a mathematical formula of the lunar surface displacement power spectrum was compiled based on the National Aeronautics and Space Administration (NASA) research. The lunar surface roughness in the frequency domain model was obtained using the harmonic superposition method. The three-dimensional coordinates of the virtual lunar surface was obtained, and the lunar surface entities was built with reverse engineering software. The simulation test in ADAMS suggested the virtual lunar surface can meet the need of lunar rover performance simulation on uneven terrain.","","978-1-4244-9171-1978-1-4244-9172-8978-1-4244-9170","10.1109/RSETE.2011.5964075","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5964075","lunar surface modeling;displacement power spectrum;lunar surface roughness;simulation","Moon;Surface roughness;Rough surfaces;Surface topography;Surface reconstruction;Solid modeling;Surface treatment","lunar surface;Moon;planetary rovers","virtual lunar surface;lunar surface roughness;lunar rover;National Aeronautics and Space Administration research;harmonic superposition method;engineering software;ADAMS;Moon","","","10","","","","","","IEEE","IEEE Conferences"
"SlapOS: A Multi-Purpose Distributed Cloud Operating System Based on an ERP Billing Model","J. Smets-Solanes; C. Cérin; R. Courteaud","NA; NA; NA","2011 IEEE International Conference on Services Computing","","2011","","","765","766","SlapOS is an open source grid operating system for distributed cloud computing based on the moto everything is a process. SlapOS combines grid computing and Enterprise Resource Modeling (ERP) to provide Infrastructure as a Service (IaaS), Platform as a Service (PaaS) and Software as a Service (SaaS) through a simple, unified API which one can learn in a matter of minutes. Thanks to its unified approach and modular architecture, SlapOS has been used as a research test bed to benchmark NoSQL databases and optimize process allocation over intercontinental Cloud. SlapOS opens new perspectives for research in the area of resilience and security on the Cloud.","","978-1-4577-0863-3978-0-7695-4462","10.1109/SCC.2011.97","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6009348","Service Architecture;Enterprise Level Transformation;Business Process Management;Integration;Business Grid;Cloud Computing","Servers;Cloud computing;Business;Computer architecture;Computational modeling;Operating systems","cloud computing;distributed processing;enterprise resource planning;financial data processing;invoicing;operating systems (computers)","SlapOS;multipurpose distributed cloud operating system;ERP billing model;open source grid operating system;moto everything;grid computing;enterprise resource modeling;infrastructure as a service;platform as a service;software as a service;modular architecture;benchmark NoSQL databases;intercontinental cloud","","15","2","","","","","","IEEE","IEEE Conferences"
"Mutual coupling reduction between closely-packed MIMO PIFA arrays","Qian Li; A. P. Feresidis","Wireless Communications Research Group, Department of Electronic and Electrical Engineering, Loughborough University, LE11 3TU, UK; Wireless Communications Research Group, Department of Electronic and Electrical Engineering, Loughborough University, LE11 3TU, UK","Proceedings of the 5th European Conference on Antennas and Propagation (EUCAP)","","2011","","","301","304","A novel solution for reducing the mutual coupling between two closely packed Planar inverted-F antennas (PIFAs) positioned on a compact ground plane is presented. A single miniaturised convoluted slit with optimised dimensions is introduced between the two PIFAs. The presence of the slit leads to a significant reduction of the mutual coupling in the antennas' operation frequency band. The proposed slit has a small footprint and is positioned close to the edge of the ground plane thereby occupying very little space. The configurations of the PIFAs are studied and results obtained in 3D electromagnetic simulation software (CST Microwave studio) are presented. A prototype has been fabricated and tested in order to validate the simulation results.","2164-3342","978-88-8202-074-3978-1-4577-0250-1978-88-8202-074","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5782215","","Mutual coupling;Antenna arrays;Resonant frequency;MIMO;Antenna measurements;Prototypes","antenna arrays;MIMO communication;planar inverted-F antennas","mutual coupling reduction;closely-packed MIMO PIFA arrays;planar inverted-F antennas;3D electromagnetic simulation software","","1","6","","","","","","IEEE","IEEE Conferences"
"FPGA Implementation of M-PSK Modulators for Satellite Communication","S. Sharma; S. Kulkarn; V. Pujari; V. M.; P. Lakshminarsimhan","NA; NA; NA; NA; NA","2010 International Conference on Advances in Recent Technologies in Communication and Computing","","2010","","","136","139","In a satellite, there exist various communication subsystem like Telecommand, Telemetry and Payload data transfer. Depending on the requirements of a particular system, different modulation techniques are being used. In general, M-PSK modulation techniques are preferable as they are power and bandwidth efficient. The paper presents the digital implementation QPSK &amp; 8-PSK modulators for satellite communication. The modulators are realized using the concept of Direct Digital Frequency Synthesis (DDFS). The use of look up table and COrdinate Rotation Digital Computer (CORDIC) algorithm is explained in details with their comparison on the basis of spectral purity and hardware requirements. The paper describes different methods of implementations of QPSK and 8-PSK modulator with comparison in terms of hardware requirement. The design of modulators are first developed, simulated and optimized on the software tool and then coded in VHDL for hardware realization. The designs are tested on Xilinx platform.","","978-1-4244-8093-7978-0-7695-4201","10.1109/ARTCom.2010.71","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5656913","DDFS;Digital modulator;FPGA;Satellite communication","Hardware;Satellites;Frequency modulation;Binary phase shift keying","digital arithmetic;direct digital synthesis;field programmable gate arrays;hardware description languages;modulators;quadrature phase shift keying;satellite communication;table lookup","FPGA;M-PSK modulator;satellite communication;communication subsystem;telecommand;telemetry;payload data transfer;modulation technique;QPSK modulator;8-PSK modulator;direct digital frequency synthesis;look up table;cordinate rotation digital computer;CORDIC algorithm;software tool;VHDL;Xilinx platform","","3","6","","","","","","IEEE","IEEE Conferences"
"Numerical Simulation and Experimental Study of Coagulating Sedimentation after Addition Chemicals in the Radiant Sedimentation Tank","G. Jin; Q. Hu; W. Wu; B. Zhang","NA; NA; NA; NA","2011 Asia-Pacific Power and Energy Engineering Conference","","2011","","","1","4","Based on the principle of flocculation kinetics, and integrated the structure of radiant sedimentation tank (RST) and the elementary theory of particles sedimentation, hydraulic agitation is taken to realize the coagulating sedimentation after addition chemicals. Meanwhile, CFD software Fluent is used to research with numerical method the operation characteristics of conventional RST. On this basis, the physics model of a novel RST which can realize coagulating sedimentation is built with feasibility study using numerical simulation to determine the installation area of newly added reaction-cover, swirl-baffle and the position of dosing system. At the same time, the laboratory dynamic test model is established according to similarity principle and the optimized structure scheme of the coagulating sedimentation after addition chemicals is confirmed. In this scheme, the relation curve of dosage and turbidity of original water from the Yellow River with different sediment concentration are achieved, and included the corresponding optimal coagulant dose. The investigation provides an effective theoretical basis for practical engineering application.","2157-4847;2157-4839;2157-4839","978-1-4244-6255-1978-1-4244-6253-7978-1-4244-6254","10.1109/APPEEC.2011.5749100","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5749100","","Numerical models;Numerical simulation;Chemicals;Rivers;Coagulation;Mathematical model;Storage tanks","computational fluid dynamics;flocculation;numerical analysis;reaction kinetics;sedimentation;turbidity","coagulating sedimentation;numerical simulation;addition chemicals;radiant sedimentation tank;flocculation kinetics;particles sedimentation;hydraulic agitation;CFD software Fluent;reaction-cover;swirl-baffle;turbidity;Yellow River","","","7","","","","","","IEEE","IEEE Conferences"
"A realization of a P300 based Brain-Computer Interface system","B. Erdogan; N. G. Gençer","Elektrik ve Elektronik Mühendisligi Bölümü, Orta Dogu Teknik Üniversitesi; Elektrik ve Elektronik Mühendisligi Bölümü, Orta Dogu Teknik Üniversitesi","2010 15th National Biomedical Engineering Meeting","","2010","","","1","4","Brain-Computer Interface is an alternative communication system between human and outside world which enables paralyzed and locked-in patients (like Amyotrophic lateral sclerosis - ALS) to communicate with their environment or control some electronic devices like computer using only their brain activity. Over the last two decades, numerous studies have been performed on this title and researchers proposed various applications and methodologies related to BCI research. In this study, a design and implementation of a P300 based BCI is realized. The hardware of the system consists of a 10 channel Electroencephalography (EEG) device which has been developed in our laboratory for BCI research. As the first application of this system, the so called “P300 Speller” of Farwell and Donchin has been chosen. Several statistical signal processing techniques and operational optimizations have been applied to improve the speed-accuracy performance of this spelling application. According to the online experiment results performed to test the practicality of this system, two out of five healthy participants were able to operate the system using only two trial repetitions for the perfect prediction of the target characters (6 seconds). The average and maximum bit rates of the system were measured to be 10.4bits/min and 31.14bits/min respectively. Regarding these results, the developed system has superior performance as compared to most of the P300 based BCI systems in the literature.","","978-1-4244-6382-4978-1-4244-6380-0978-1-4244-6381","10.1109/BIYOMUT.2010.5479781","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5479781","Brain-Computer Interface;BCI;P300;EEG","Brain computer interfaces;Electroencephalography;Humans;Communication system control;Control systems;Computer interfaces;Application software;Hardware;Laboratories;Signal processing","brain-computer interfaces;electroencephalography;handicapped aids;medical signal processing","P300 based brain-computer interface system;electroencephalography device;P300 Speller;statistical signal processing techniques;operational optimizations;speed-accuracy performance;spelling application;online experiment","","","18","","","","","","IEEE","IEEE Conferences"
"A low-power cell-based-design multi-port register file in 65nm CMOS technology","J. Uhlig; S. Höppner; G. Ellguth; R. Schüffny","Department of Electrical Engineering, Technische Universität Dresden, Dresden, Germany; Department of Electrical Engineering, Technische Universität Dresden, Dresden, Germany; Department of Electrical Engineering, Technische Universität Dresden, Dresden, Germany; Department of Electrical Engineering, Technische Universität Dresden, Dresden, Germany","Proceedings of 2010 IEEE International Symposium on Circuits and Systems","","2010","","","313","316","This paper presents the design of a register file with 4 write and 6 read ports for an SDR multiprocessor in 65nm CMOS technology. A cell-based design (CBD) methodology is employed in which the circuit is partitioned into complex sub-cells, optimized on transistor level and layout. Each cell is completely characterized concerning timing and power for seamless integration into a semi-custom design flow. The CBD implementation shows 30% savings of power and 40% of area compared to a conventional semi-custom solution. The average power is 2.7mW from 1.0V supply and 300MHz operating frequency which is superior to previously published designs.","0271-4302;2158-1525","978-1-4244-5308-5978-1-4244-5309","10.1109/ISCAS.2010.5537838","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5537838","register file;cell-based design;low-power","CMOS technology;Radio frequency;Latches;Timing;Clocks;Software libraries;Logic testing;Paper technology;Registers;Circuits","CMOS integrated circuits;integrated circuit design;low-power electronics","low-power cell-based-design;multiport register file;CMOS technology;SDR multiprocessor;size 65 nm;power 2.7 mW;voltage 1.0 V;frequency 300 MHz","","","12","","","","","","IEEE","IEEE Conferences"
"Multiple instruction sets architecture (MISA)","H. Karaki; H. Akkary","Department of Electrical and Computer Engineering, American University of Beirut, FEA, Beirut, Lebanon; Department of Electrical and Computer Engineering, American University of Beirut, FEA, Beirut, Lebanon","2011 International Conference on Energy Aware Computing","","2011","","","1","6","In the computer hardware industry, there are currently two highly successful instruction set architectures (ISAs): the CISC X86 ISA which is an established standard architecture in the personal computer and server markets, and the RISC ARM ISA which is currently used in many ultra-mobile computing devices, such as smart-phones and tablets. Platforms that run one standard ISA cannot run the other ISA application binaries without recompiling the source code. We are investigating the technical feasibility of designing an energy-efficient multiple instruction sets architecture (MISA) processor that can run both X86 and ARM binaries. We propose an approach in which special decoders interpret the binary instructions of the running ISA and translates them to a native target machine ISA that executes within the processor pipeline. We discuss the completed initial stage of our work involving the design of XAM, an X86 hardware binary interpreter for a MISA processor that runs native ARM instructions, and describe our design in detail. We present performance and energy simulation results of our MISA processor design for a set of synthetic benchmarks including Dhrystone 2.1, measured using the ARM SimpleScalar microarchitecture and power simulator. We also discuss design issues of an ARM to X86 hardware interpreter we are currently developing. We expect the completed X86-to-ARM design and the current ARM-to-X86 work to lay a foundation for designing a well optimized processor having a new native ISA that can run efficiently both X86 and ARM binaries, using direct hardware interpretation.","2381-0947","978-1-4673-0465-8978-1-4673-0466-5978-1-4673-0464","10.1109/ICEAC.2011.6136696","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6136696","","Registers;Hardware;Computer architecture;Software;Benchmark testing;Decoding;Algorithms","instruction sets;microcomputers;mobile handsets;reduced instruction set computing","multiple instruction sets architecture;computer hardware industry;CISC X86 ISA;server markets;personal computer markets;RISC ARM ISA;smart phones;tablets;ultra mobile computing devices;XAM;X86 hardware binary interpreter;MISA processor;Dhrystone2.I;ARM SimpleScalar microarchitecture;power simulator;X86-to-ARM design","","1","14","","","","","","IEEE","IEEE Conferences"
"Adapting the Bloom filter to multithreaded environments","I. Voras; M. Žagar","University of Zagreb Faculty of electrical engineering and computing, Unska 3, Croatia; University of Zagreb Faculty of electrical engineering and computing, Unska 3, Croatia","Melecon 2010 - 2010 15th IEEE Mediterranean Electrotechnical Conference","","2010","","","1488","1493","Many classical algorithms like the Bloom filter were envisioned and created at a time when computers were the size of rooms and multithreading was not yet even explored theoretically. The landscape of modern mainstream computer systems today is heavily dominated with multi-core CPU-s but the effort to make full use of such systems is still ongoing. The Bloom filter has proven itself useful both as a core algorithm and as a supportive or optimizing addition to other data classification algorithms. This work explores adaptations to the basic Bloom filter algorithm for use in multithreaded applications on contemporary SMP systems and the consequences of such adaptations to its overall efficiency.","2158-8473;2158-8481","978-1-4244-5795-3978-1-4244-5793-9978-1-4244-5794","10.1109/MELCON.2010.5476244","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5476244","multithreading;multipricessing;Bloom filter;data structure;algorithms","Filters;Testing;Adaptive arrays;Multithreading;Application software;Classification algorithms;Performance analysis;Algorithm design and analysis;Data structures;Databases","data structures;multiprocessing systems;multi-threading","Bloom filter;multithreaded environments;mainstream computer systems;multicore CPU;SMP systems","","1","20","","","","","","IEEE","IEEE Conferences"
"GVT algorithms and discrete event dynamics on 129K+ processor cores","K. S. Perumalla; A. J. Park; V. Tipparaju","Oak Ridge National Laboratory, Oak Ridge, Tennessee, USA; Oak Ridge National Laboratory, Oak Ridge, Tennessee, USA; Oak Ridge National Laboratory, Oak Ridge, Tennessee, USA","2011 18th International Conference on High Performance Computing","","2011","","","1","11","Parallel discrete event simulation (PDES) represents a class of codes that are challenging to scale to large number of processors due to tight global timestamp-ordering and fine-grained event execution. One of the critical factors in scaling PDES is the efficiency of the underlying global virtual time (GVT) algorithm needed for correctness of parallel execution and speed of progress. Although many GVT algorithms have been proposed previously, few have been proposed for scalable asynchronous execution and none customized to exploit one-sided communication. Moreover, the detailed performance effects of actual GVT algorithm implementations on large platforms are unknown. Here, three major GVT algorithms intended for scalable execution on high-performance systems are studied: (1) a synchronous GVT algorithm that affords ease of implementation, (2) an asynchronous GVT algorithm that is more complex to implement but can relieve blocking latencies, and (3) a variant of the asynchronous GVT algorithm, proposed and studied for the first time here, to exploit one-sided communication in extant supercomputing platforms. Performance results are presented of implementations of these algorithms on up to 129,024 cores of a Cray XT5 system, exercised on a range of parameters: optimistic and conservative synchronization, fine-to medium-grained event computation, synthetic and non-synthetic applications, and different lookahead values. Performance to the tune of tens of billions of events executed per second are registered, exceeding the speeds of any known PDES engine, and showing asynchronous GVT algorithms to outperform state-of-the-art synchronous GVT algorithms. Detailed PDES-specific runtime metrics are presented to further the understanding of tightly-coupled discrete event dynamics on massively parallel platforms.","1094-7256","978-1-4577-1950-9978-1-4577-1951-6978-1-4577-1949","10.1109/HiPC.2011.6152725","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6152725","Parallel Discrete Event Simulation;Time Warp;Global Virtual Time;One-sided Communication;Asynchrony","Engines;Heuristic algorithms;Portals;Benchmark testing;Computational modeling;Runtime;Software algorithms","coprocessors;discrete event simulation;parallel machines;parallel programming","parallel discrete event simulation;core processor;PDES;global virtual time;parallel execution;scalable asynchronous execution;synchronous GVT algorithm;asynchronous GVT algorithm;supercomputing;Cray XT5 system;one-sided communication","","1","15","","","","","","IEEE","IEEE Conferences"
"Discovering and understanding performance bottlenecks in transactional applications","F. Zyulkyarov; S. Stipic; T. Harris; O. S. Unsal; A. Cristal; I. Hur; M. Valero","BSC-Microsoft Research Centre, USA; BSC-Microsoft Research Centre, USA; Microsoft Research, USA; BSC-Microsoft Research Centre, USA; BSC-Microsoft Research Centre, USA; BSC-Microsoft Research Centre, USA; BSC-Microsoft Research Centre, USA","2010 19th International Conference on Parallel Architectures and Compilation Techniques (PACT)","","2010","","","285","294","Many researchers have developed applications using transactional memory (TM) with the purpose of benchmarking different implementations, and studying whether or not TM is easy to use. However, comparatively little has been done to provide general-purpose tools for profiling and tuning programs which use transactions. In this paper we introduce a series of profiling techniques for TM applications that provide in-depth and comprehensive information about the wasted work caused by aborting transactions. We explore three directions: (i) techniques to identify multiple potential conflicts from a single program run, (ii) techniques to identify the data structures involved in conflicts by using a symbolic path through the heap, rather than a machine address, and (iii) visualization techniques to summarize how threads spend their time and which of their transactions conflict most frequently. To examine the effectiveness of the profiling techniques, we provide a series of illustrations from the STAMP TM benchmark suite and from the synthetic WormBench workload. We show how to use our profiling techniques to optimize the performance of the Bayes, Labyrinth and Intruder applications. We discuss the design and implementation of our techniques in the Bartok-STM system. We process data offline or during garbage collection, where possible, in order to minimize the probe effect introduced by profiling.","","978-1-4503-0178-7978-1-5090-5032","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7851528","Performance;Measurement","Data structures;Probes;C# languages;Benchmark testing;Program processors;Scalability;Software measurement","concurrency control;program diagnostics;program visualisation","probe effect minimization;garbage collection;Bayes applications;Labyrinth applications;intruder applications;synthetic WormBench workload;STAMP TM benchmark suite;visualization techniques;symbolic path;data structures;profiling techniques;transactional memory","","","29","","","","","","IEEE","IEEE Conferences"
"A Framework for Predicting Query Response Time","R. Singhal","NA","2012 IEEE 14th International Conference on High Performance Computing and Communication & 2012 IEEE 9th International Conference on Embedded Software and Systems","","2012","","","1137","1141","In a typical OLTP environment, emphasis has been given on promising Service level Agreements (SLAs) for perceived query elapsed response time; the SQL queries are tested on the small size of database which may be a fraction of the production database. As time progresses the database grows and the earlier optimized queries may not hold SLA anymore. Once the application is deployed, it becomes difficult to modify the application or alter the production system. In this paper, we have discussed a framework for predicting the SQL query response time with growth of the database while being transparent to the production hardware, storage subsystem and DB Server. We have discussed various factors which can impact query response time and are also affected by the increase in the data size. We have presented a theoretical model for predicting the elapsed response time of SQL queries and also discussed a case study of Oracle lOg for implementing the proposed framework.","","978-1-4673-2164-8978-0-7695-4749","10.1109/HPCC.2012.167","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6332302","SQL;Query;Response Time;Parsing;Data Acess;Large Size Database;Execution;Fetching","Conferences;High performance computing","query processing;SQL;storage management;Web services","query response time prediction;OLTP;service level agreement;SQL query;production database;SLA;storage subsystem;production hardware;DB server","","","17","","","","","","IEEE","IEEE Conferences"
"Analysis of nursing-care freestyle japanese text classification using ga-based term selection","M. Nii; T. Yamaguchi; Y. Takahashi; R. Sakashita; A. Uchinuno","WPI Immunology Frontier Research Center, Osaka University, Japan; Graduate School of Engineering, University of Hyogo, 2167 Shosha, Himeji, Hyogo, Japan; Graduate School of Engineering, University of Hyogo, 2167 Shosha, Himeji, Hyogo, Japan; College of Nursing Art amp; Science, University of Hyogo, 13-71 Kitaouji-cho, Akashi, Japan; College of Nursing Art amp; Science, University of Hyogo, 13-71 Kitaouji-cho, Akashi, Japan","2010 World Automation Congress","","2010","","","1","6","In this paper, classification performance of a term selection based on GA is analyzed. In the term selection based on GA, two objectives which are maximizing correctly classified texts and minimizing selected terms are optimized. An objective function based on the classification per-formance of the SVM with 10-fold cross validation is used for evaluating each individual in GA. Therefore, GA-based term selection is performed aiming at the improvement in classification per-formance on testing text sets. This causes the performance deterioration over unseen texts in actual use by GA-based term selection because terms are deleted excessively even when such terms have important role for the classification. In this paper, relation between the terms deleted by the term se-lection based on GA and the terms which appears in unseen texts is clarified by numerical simulation results.","2154-4824;2154-4824","978-1-889335-42-1978-1-4244-9673","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5665338","Nursing-care texts;term selection;support vector machine;genetic algorithm","Gallium;Support vector machines;Pain;Text categorization;Classification algorithms;Software","genetic algorithms;medical administrative data processing;numerical analysis;patient care;support vector machines;text analysis","nursing-care freestyle Japanese text classification;GA-based term selection;SVM;10-fold cross validation;numerical simulation;support vector machine;genetic algorithm","","7","","","","","","","IEEE","IEEE Conferences"
"Research on Workload Adaptation Architecture for DBMS","S. Qian; S. Wang","NA; NA","2010 International Symposium on Intelligence Information Processing and Trusted Computing","","2010","","","382","385","As the visits to databases in the network application become more and more, the DBMS, which is the core of network calculation, attracts increasing attention. The research on the self-calculating, which mainly includes self-configuring, self-restoring, self-optimizing and self-protecting, handles the internal resources management of the DBMS. But it can't handle the management of external workload. Different workloads have different commercial values. The workload which has great commercial value is entrusted great importance. Its performance should be fulfilled firstly. This paper handles this difficult problem by researching the workload adaptation technology. On the base of research, this paper gave workload adaptation architecture for DBMS and finished the testing.","","978-1-4244-8148-4978-0-7695-4196","10.1109/IPTC.2010.83","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5663268","workload adaptation;architecture;theoretical research","Predictive models;Process control;Adaptation model;Classification algorithms;Monitoring;Databases;Software algorithms","database management systems;information resources","workload adaptation architecture;DBMS;databases;network application;internal resources management","","1","8","","","","","","IEEE","IEEE Conferences"
"SINDI-WALKS: Workbench for PLOT-based Technological Information Extraction and Management","S. Choi; H. Chun; C. Jeong; S. Song; H. Jung","NA; NA; NA; NA; NA","2012 IEEE International Conference on Green Computing and Communications","","2012","","","712","716","This paper introduces a workbench called SINDI-WALKS capable of extracting inherent valuable information from scientific literatures such as articles and patents. The system identifies PLOTs, Person, Location, Organization and Terms and extracts the semantic relations between them. Also, the workbench provides two test beds for monitoring the run-time behavior of the core engines, SINDI-CORE (PLOT recognition engine) and SINDI-LINK (relation extraction engine). As an important auxiliary tool, an intuitive annotation system is additionally furnished to construct training or evaluation collections for the engines above. Various administrative functions are also provided to visually manage the extracted semantic triples in an efficient manner. The proposed toolsets in this paper will facilitate the performance optimization of the information extraction as well as the efficient management of the extracted semantic triples.","","978-1-4673-5146-1978-0-7695-4865","10.1109/GreenCom.2012.116","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6468396","Information Extraction;Scientific Intelligence;Text Mining;Relation Extraction;Knowledge Engineering","Engines;Semantics;Software;Text recognition;Dictionaries;Text mining","data mining;information management;knowledge acquisition;text analysis","SINDI-WALKS workbench;PLOT-based technological information;information extraction;information management;patent;article;person-location-organization-term;semantic relation;SINDI-CORE engine;SINDI-LINK engine;PLOT recognition engine;relation extraction engine;intuitive annotation system;administrative function;semantic triple;text mining method","","1","18","","","","","","IEEE","IEEE Conferences"
"Enabling Grid Computing over IPv6 within a Campus Network","J. Chen; Y. Zou; Z. Liu; Q. Wu","NA; NA; NA; NA","2011 Fourth International Symposium on Parallel Architectures, Algorithms and Programming","","2011","","","285","288","The biggest problem to be solved in grid computing is data transmission and communication efficiency and stability. For computing grid composed of small local area network(LAN), this problem can be solved by properly building LAN with the use of IPv4 network or high-speed computing network such as Infiniband. However, when the computing resources are distributed in different physical regions, data transmission and communication will be affected by various factors such as time period and load. On the one hand, the stability of transmission cannot be guaranteed; On the other hand, the speed of cross-regional data transmission is not optimistic. As the next generation of Internet protocol, IPv6 is at a transitional stage, and the load is far less than the IPv4-based network. So, it has a better stability and higher bandwidth. With the help of IPv6 network, cross-regional computing grids can be constructed to connect the computing resources in different physical regions in order to deal with some complex computing problems. This paper studies and tests the feasibility of transplanting MPI application (which is more commonly used in distributed computing) into IPv6 network for grid computing, and presents its implementation process in detail.","2168-3034;2168-3042","978-1-4577-1808","10.1109/PAAP.2011.71","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6128518","grid computing;IPv6;MPI","Educational institutions;Protocols;Data communication;Grid computing;Software;Electronic mail;High performance computing","application program interfaces;grid computing;Internet;IP networks;local area networks;message passing;transport protocols","IPv6;campus network;data transmission;communication efficiency;communication stability;local area network;IPv4 network;high-speed computing network;Infiniband;time period;load;Internet protocol;cross-regional computing grids;MPI application","","","7","","","","","","IEEE","IEEE Conferences"
"An Intelligent Monitoring System of Medium-Frequency Induction Furnace Based on Fuzzy Control","Z. Zheng; W. Guocheng; L. Xiaowei","NA; NA; NA","2010 International Conference on Intelligent System Design and Engineering Application","","2010","1","","261","264","Aiming at the problem such as slowly-varying, pure-lag and low accuracy in the temperature control process of medium-frequency induction furnace, designed an intelligent monitoring system based on fuzzy control. The major hardware is composed of Program Logic Controller and its special expansion modules, the algorithm of fuzzy controller is completely realized via software programming. In order to improve the resolution rate of fuzzy subsets, selected a kind of optimized triangle-shape membership function which based on the golden section algorithm to instead the common symmetrical triangle-shape function. The paper detailedly introduced the working principle of system, implementation procedure, off-line simulations result and practiced applications effects. Simulations and applications show that the system has advantages of simple configuration, low cost and high reliability. As a result, the system reaches the ideal effect.","","978-1-4244-8333","10.1109/ISDEA.2010.258","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5743174","M-frequency furnace;temperature monitoring;PLC;fuzzy control;golden section algorithm","Furnaces;Fuzzy control;Temperature control;Temperature;Hardware;Algorithm design and analysis;Testing","computerised monitoring;control system synthesis;electric furnaces;fuzzy control;induction heating;metallurgical industries;programmable controllers;temperature control","intelligent monitoring system design;medium-frequency induction furnace;fuzzy control;temperature control process;program logic controller;triangle-shape membership function;golden section algorithm","","","14","","","","","","IEEE","IEEE Conferences"
"Performance evaluation of MPI and hybrid MPI+OpenMP programming paradigms on multi-core processors cluster","R. Sharma; P. Kanungo","Vikram University, Ujjain, INDIA; Patel College of Sc. &amp; Technology, Indore, India","2011 International Conference on Recent Trends in Information Systems","","2011","","","137","140","Processors design is now switched to multi-core architecture in a direction that minimizes energy consumption. A decade ago CPUs speed could not accelerate without extra ordinary cooling and consequently hit a clock speed barrier. The multi-core architecture is introduced to improve computing performance by providing hardware parallelism through more CPU cores, each having restrained clock speed. This has been a break through in High Performance Computing (HPC). While more processor cores rendered effective execution results, multi-core technology inaugurated an extra layer of complexity for programming issues. To exploit each core in a multi-core environment, application software should be optimized by using multithreading. Multi-core processors can even degrade the performance for single threaded application due to reduction in clock speed. In this paper we compare performance of multithreading fine-grained and course-grained computational problems further flavored as computation-intensive and data-intensive problems by using MPI and hybrid MPI+OpenMP approach and evaluate suitability of multi-core cluster depending on the nature of the problems.","","978-1-4577-0792-6978-1-4577-0790-2978-1-4577-0791","10.1109/ReTIS.2011.6146855","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6146855","high performance computing;MPI and OpenMP programming paradigms;multicore processors","Multicore processing;Message systems;Programming;Program processors;Parallel processing;Benchmark testing","application program interfaces;message passing;microprocessor chips;multiprocessing systems;multi-threading","MPI;OpenMP programming paradigm;multicore processor cluster;multicore architecture;high performance computing;HPC;multithreading","","2","8","","","","","","IEEE","IEEE Conferences"
"Programmable implementations of MIMO-OFDM detectors: Design, benchmarking and comparison","J. Janhunen; T. Pitkänen; O. Silvén; M. Juntti","Centre for Wireless Communications, University of Oulu, Finland; Department of Computer Systems, Tampere University of Technology, Finland; Computer Science and Engineering Laboratory, University of Oulu, Finland; Centre for Wireless Communications, University of Oulu, Finland","2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2012","","","1581","1584","Programmable MIMO-OFDM detector design, benchmarking and comparison of implementations have been considered in this paper. We emphasize the significance of co-optimizing the algorithm, software and hardware together in order to reach demanding energy, latency and area restrictions introduced in current standards. We compare energy consumption of the detection algorithms based on the theoretical complexities in function of signal-to-noise ratio. Applying co-optimizing we show how a carefully designed programmable architecture can achieve the 3G long term evolution (LTE) detection rate requirements with a reasonable energy consumption.","2379-190X;1520-6149;1520-6149","978-1-4673-0046-9978-1-4673-0045-2978-1-4673-0044","10.1109/ICASSP.2012.6288195","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6288195","","Detectors;Antennas;Hardware;Signal to noise ratio;Complexity theory;Algorithm design and analysis;Computer architecture","3G mobile communication;benchmark testing;design;Long Term Evolution;MIMO communication;OFDM modulation","programmable implementations;MIMO-OFDM detectors;design;benchmarking;signal-to-noise ratio;programmable architecture;3G Long Term Evolution;LTE","","1","11","","","","","","IEEE","IEEE Conferences"
"A distributed futures program trading platform based on middleware","Z. Jiang; Xujuan Chi; Guangan Shang","School of Computer and Information, Hefei University of Technology, Anhui Province, China; School of Computer and Information, Hefei University of Technology, Anhui Province, China; School of Computer and Information, Hefei University of Technology, Anhui Province, China","2012 IEEE International Conference on Computer Science and Automation Engineering","","2012","","","41","46","Futures traders increasingly use program trading systems for one or more stages of the trading process. In order to stay competitive, futures firms have to rely more on their program trading systems to increase their responsiveness to market conditions. First, an overview of how these systems work was introduced. Then, aiming at the traits of Chinese futures trading information system, a distributed architecture for middleware-based futures program trading platform was proposed. Based on above mentioned architecture, the conceptual design of the platform was given. The key platform technologies including the description language interpreter, main memory database, communication middleware, real-time publish/subscribe middleware were discussed. Finally, the trading platform system, named SmartPTrader, was developed and applied based on the proposed system architecture and implementation method. Results of performance analysis and simulation test show that the architecture and technologies have a better result and can effectively enhance the communication availability and reliability for the platform.","2327-0586;2327-0594","978-1-4673-2008-5978-1-4673-2007-8978-1-4673-2006","10.1109/ICSESS.2012.6269401","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6269401","platform;program trading;futures;distributed architecture;middleware","Middleware;Portfolios;Business;Standards;Optimization","electronic trading;information systems;middleware;program interpreters;stock markets","distributed program trading platform;market conditions;Chinese trading information system;description language interpreter;memory database;communication middleware;real-time publish/subscribe middleware;SmartPTrader;system distributed architecture;implementation method;performance analysis;simulation test;communication availability enhancement;communication reliability enhancement","","1","11","","","","","","IEEE","IEEE Conferences"
"Simulation for a shingled magnetic recording disk","S. Tan; W. Xi; Z. Y. Ching; C. Jin; C. T. Lim","Data Center Technologies Division, Data Storage Institute, A*STAR, Singapore; Data Center Technologies Division, Data Storage Institute, A*STAR, Singapore; Data Center Technologies Division, Data Storage Institute, A*STAR, Singapore; Data Center Technologies Division, Data Storage Institute, A*STAR, Singapore; Data Center Technologies Division, Data Storage Institute, A*STAR, Singapore","2012 Digest APMRC","","2012","","","1","7","Shingled Magnetic Recording (SMR) is a new technology based on partially overlapping, or shingling, adjacent tracks, which is capable of increasing the density of a disk drive up to 10Tbit/inch2. To write data on overlapping tracks may erase data being written previously. Therefore without any data translation layer to redirect updates to different locations to avoid unintended data erasures, the SMR disk cannot be used as a conventional disk to perform unrestricted reads/writes directly. Various data translation/mapping strategies have been proposed to allow the disk to perform unrestricted reads/writes. However these strategies have tradeoffs in terms of system performance and cost, such as disk space utilization. Nevertheless system performance varies for different applications and under different workload conditions. To help optimize the design of a data translation layer for a SMR disk, simulation software is required to analyze tradeoffs and to fine tune parameters for data translation/remapping strategies. We have designed and developed a simulation software for SMR disks. This software can analyze both system performance and disk space utilization. The SMR disk simulation uses the shingled disk profiles generated from the conventional disk profiles which have been experimentally validated. Evaluation studies show that the simulation software can accurately simulate SMR disk performance. This software will be a useful platform to design and test the Shingle Translation Layer (STL) for a SMR disk with different disk layouts and data management strategies, and garbage collection algorithms.","","978-9-8107-2056-8978-1-4673-4734","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6407526","Data Band Layout;Data Management;Shingled Recording;Shingled Disk","Magnetic recording;System performance;Algorithm design and analysis;Data models;Computational modeling","disc drives;disc storage;magnetic recording","shingled magnetic recording disk;shingling;disk drive;data translation layer;unintended data erasures;disk space utilization;data translation strategy;data remapping strategy;SMR disk simulation software;disk profiles;SMR disk performance;shingle translation layer;disk layouts;data management strategy;garbage collection algorithm","","2","7","","","","","","IEEE","IEEE Conferences"
"SyncChecker: Detecting Synchronization Errors between MPI Applications and Libraries","Z. Chen; X. Li; J. Chen; H. Zhong; F. Qin","NA; NA; NA; NA; NA","2012 IEEE 26th International Parallel and Distributed Processing Symposium","","2012","","","342","353","While improving the performance, nonblocking communication is prone to synchronization errors between MPI applications and the underlying MPI libraries. Such synchronization error occurs in the following way. After initiating nonblocking communication and performing overlapped computation, the MPI application reuses the message buffer before the MPI library completes the use of the same buffer, which may lead to sending out corrupted message data or reading undefined message data. This paper presents a new method called Sync Checker to detect synchronization errors in MPI nonblocking communication. To examine whether the use of message buffers is well synchronized between the MPI application and the MPI library, Sync Checker first tracks relevant memory accesses in the MPI application and corresponding message send/receive operations in the MPI library. Then it checks whether the correct execution order between the MPI application and the MPI library is enforced by the MPI completion check routines. If not, Sync Checker reports the error with diagnostic information. To reduce runtime overhead, we propose three dynamic optimizations. We have implemented a prototype of Sync Checker on Linux and evaluated it with seven bug cases, i.e., five introduced by the original developers and two injected, in four different MPI applications. Our experiments show that Sync Checker detects all the evaluated synchronization errors and provides helpful diagnostic information. Moreover, our experiments with seven NAS Parallel Benchmarks demonstrate that Sync Checker incurs moderate runtime overhead, 1.3-9.5 times with an average of 5.2 times, making it suitable for software testing.","1530-2075;1530-2075","978-1-4673-0975-2978-0-7695-4675","10.1109/IPDPS.2012.40","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6267848","","Synchronization;Libraries;Runtime;Instruments;Computer bugs;Semantics;Prototypes","application program interfaces;Linux;message passing;program verification;software libraries","SyncChecker;synchronization errors detection;MPI applications;MPI libraries;message buffer;MPI nonblocking communication;Linux","","5","64","","","","","","IEEE","IEEE Conferences"
"Using BPEL workflow processing for cross-layer orchestrations in IP-over-optical networks: A proof of concept","M. Chamania; E. Demirbilek; A. Jukan; X. Masip-Bruin; M. Yannuzzi","Technische Universit&#x00E4;t Carolo-Wilhelmina zu Braunschweig; Technische Universit&#x00E4;t Carolo-Wilhelmina zu Braunschweig; Technische Universit&#x00E4;t Carolo-Wilhelmina zu Braunschweig; Advanced Network Architectures Lab (CRAAX), Spain; Advanced Network Architectures Lab (CRAAX), Spain","2012 IEEE Network Operations and Management Symposium","","2012","","","1191","1194","The rapid growth of network services is forcing operators to use of inter-layer coordination between the IP and the transport network management systems in order to optimize resource utilization within the carriers network ecosystem. However, the diversity of technology and the related management systems within the IP and transport networks poses significant challenges in developing a single unified management ecosystem for both IP and transport networks. In this work, we present a basic prototype of a Service Oriented Architecture (SOA) based framework proposed in the EU project ONE for programmable orchestration of IP and transport management processes. The proof-of-concept prototype uses web services and BPEL to facilitate programmable inter-layer coordination, and was tested on a number of use cases. We measured the performance penalty of using the SOA framework against the traditional JAVA based provisioning approaches used in integrated multi-layer management, and show that the penalty incurred in execution times were sufficiently small while bringing several significant advantages during the software design, development, integration and maintenance, which is simple and highly effective.","2374-9709;1542-1201;1542-1201","978-1-4673-0269-2978-1-4673-0267-8978-1-4673-0268","10.1109/NOMS.2012.6212049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6212049","","IP networks;Topology;Network topology;Service oriented architecture;Java","computer network management;IP networks;optical fibre networks;service-oriented architecture;specification languages;Web services;workflow management software","BPEL workflow processing;cross-layer orchestrations;IP-over-optical networks;network services;carrier network ecosystem;single unified management ecosystem;service oriented architecture;SOA framework;Web services;programmable interlayer coordination;Java provisioning approach;integrated multilayer management;business process execution language","","1","12","","","","","","IEEE","IEEE Conferences"
"Approximate subgradient projection algorithm for convex feasibility problem","L. Li; Y. Gao","Business School, University of Shanghai for Science and Technology, Shanghai 200093, P. R. China; Business School, University of Shanghai for Science and Technology, Shanghai 200093, P. R. China","Journal of Systems Engineering and Electronics","","2010","21","3","527","530","An ε-subgradient projection algorithm for solving a convex feasibility problem is presented. Based on the iterative projection methods and the notion of ε-subgradient, a series of special projection hyperplanes is established. Moreover, compared with the existing projection hyperplanes methods with subgradient, the proposed hyperplanes are interactive with e, and their ranges are more larger. The convergence of the proposed algorithm is given under some mild conditions, and the validity of the algorithm is proved by the numerical test.","1004-4132","","10.3969/j.issn.1004-4132.2010.03.027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6073148","ε-subgradient;projection algorithm;convex feasibility problem","Projection algorithms;Educational institutions;Systems engineering and theory;Optimization;Program processors;Convergence","","","","","","","","","","","BIAI","BIAI Journals & Magazines"
"Exploring FPGAs capability to host a HPC design","C. Foucher; F. Muller; A. Giulieri","Université de Nice-Sophia Antipolis, Laboratoire d'Électronique, Antennes et Télécommunications (LEAT), CNRS UMR 6071, Bat. 4, 250 rue Albert Einstein, 06560 Valbonne, France; Université de Nice-Sophia Antipolis, Laboratoire d'Électronique, Antennes et Télécommunications (LEAT), CNRS UMR 6071, Bat. 4, 250 rue Albert Einstein, 06560 Valbonne, France; Université de Nice-Sophia Antipolis, Laboratoire d'Électronique, Antennes et Télécommunications (LEAT), CNRS UMR 6071, Bat. 4, 250 rue Albert Einstein, 06560 Valbonne, France","NORCHIP 2010","","2010","","","1","4","Reconfigurable hardware is now used in high performance computers, introducing the high performance reconfigurable computing. Dynamic hardware allows processors to devolve intensive computations to dedicated hardware circuitry optimized for that purpose. Our aim is to make larger use of hardware capabilities by pooling the hardware and software computations resources in a unified design in order to allow replacing the ones by the others depending on the application needs. For that purpose, we needed a test platform to evaluate FPGA capabilities to operate as a high performance computer node. We designed an architecture allowing the separation of a parallel program communication from its kernels computation in order to make easier the future partial dynamic reconfiguration of the processing elements. This architecture implements static softcores as test IPs, keeping in mind that the future platform implementing dynamic reconfiguration will allow changing the processing elements. In this paper, we present this test architecture and its implementation upon Xilinx Virtex 5 FPGAs. We then present a benchmark of the platform using the NAS parallel benchmark integer sort in order to compare various use cases.","","978-1-4244-8973-2978-1-4244-8972-5978-1-4244-8971","10.1109/NORCHIP.2010.5669494","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5669494","FPGA;Architecture Exploration;High Performance Reconfigurable Computing","Computer architecture;Microprocessors;Field programmable gate arrays;Hardware;Benchmark testing;Kernel","field programmable gate arrays;parallel processing;reconfigurable architectures","FPGA;HPC design;reconfigurable hardware;reconfigurable computing;high performance computer node;parallel program communication;static softcore;Xilinx Virtex 5","","","7","","","","","","IEEE","IEEE Conferences"
"Use of mathematical modeling in nuclear measurements projects","H. Toubon; N. Menaa; L. Mirolo; X. Ducoux; R. A. Khalil; P. Chany; A. Devita","AREVA/CANBERRA Nuclear Measurements Business Unit, Saint Quentin-en-Yvelines 78182, France; AREVA/CANBERRA Nuclear Measurements Business Unit, Saint Quentin-en-Yvelines 78182, France; AREVA/CANBERRA Nuclear Measurements Business Unit, Saint Quentin-en-Yvelines 78182, France; AREVA/CANBERRA Nuclear Measurements Business Unit, Saint Quentin-en-Yvelines 78182, France; AREVA/CANBERRA Nuclear Measurements Business Unit, Saint Quentin-en-Yvelines 78182, France; AREVA/BE Nuclear Sites Value development AREVA NC Marcoule BP 76170, 30206 Bagnols Sur Ceze; AREVA/BE MELOX, BP 124, 30206 Bagnols Sur Ceze","2011 2nd International Conference on Advancements in Nuclear Instrumentation, Measurement Methods and their Applications","","2011","","","1","8","Mathematical modeling of nuclear measurement systems is not a new concept. The response of the measurement system is described using a pre-defined mathematical model that depends on a set of parameters. These parameters are determined using a limited set of experimental measurement points e.g. efficiency curve, dose rates... etc. The model that agrees with the few experimental points is called an experimentally validated model. Once these models have been validated, we use mathematical interpolation to find the parameters of interest. Sometimes, when measurements are not practical or are impossible extrapolation is implemented but with care. CANBERRA has been extensively using mathematical modeling for the design and calibration of large and sophisticated systems to create and optimize designs that would be prohibitively expensive with only experimental tools. The case studies that will be presented here are primarily performed with MCNP, CANBERRA's MERCURAD/PASCALYS and ISOCS (InSitu Object Counting Software). For benchmarking purposes, both Monte Carlo and ray-tracing based codes are inter-compared to show models consistency and add a degree of reliability to modeling results.","","978-1-4577-0927-2978-1-4577-0925-8978-1-4577-0926","10.1109/ANIMMA.2011.6172914","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6172914","","Detectors;Geometry;Mathematical model;Nuclear measurements;Calibration;Pollution measurement;Monte Carlo methods","benchmark testing;calibration;dosimetry;interpolation;Monte Carlo methods;ray tracing","mathematical modeling;nuclear measurement systems;experimental measurement points;dose rates;efficiency curve;mathematical interpolation;CANBERRA;calibration;experimental tools;MCNP;MERCURAD/PASCALYS;ISOCS;benchmarking purposes;Monte Carlo codes;ray-tracing based codes;nuclear measurement projects","","1","11","","","","","","IEEE","IEEE Conferences"
"GPGPU acceleration of Cellular Simultaneous Recurrent Networks adapted for maze traversals","K. L. Rice; T. M. Taha; K. M. Iftekharuddin; K. Anderson; T. Salan","Clemson University, SC 29634 USA; University of Dayton, OH 45469 USA; University of Memphis, TN 38152 USA; University of Memphis, TN 38152 USA; University of Memphis, TN 38152 USA","The 2011 International Joint Conference on Neural Networks","","2011","","","2717","2724","At present, a major initiative in the research community is investigating new ways of processing data that capture the efficiency of the human brain in hardware and software. This has resulted in increased interest and development of bio-inspired computing approaches in software and hardware. One such bio-inspired approach is Cellular Simultaneous Recurrent Networks (CSRNs). CSRNs have been demonstrated to be very useful in solving state transition type problems, such as maze traversals. Although powerful in image processing capabilities, CSRNs have high computational demands with increasing input problem size. In this work, we revisit the maze traversal problem to gain an understanding of the general processing of CSRNs. We use a 2.67 GHz Intel Xeon X5550 processor coupled with an NVIDIA Tesla C2050 general purpose graphical processing unit (GPGPU) to create several novel accelerated CSRN implementations as a means of overcoming the high computational cost. Additionally, we explore the use of decoupled extended Kalman filters in the CSRN training phase and find a significant reduction in runtime with negligible change in accuracy. We find in our results that we can achieve average speedups of 21.73 and 3.55 times for the training and testing phases respectively when compared to optimized C implementations. The main bottleneck in training performance was a matrix inversion computation. Therefore, we utilize several methods to reduce the effects of the matrix inversion computation.","2161-4407;2161-4393;2161-4393","978-1-4244-9637-2978-1-4244-9635-8978-1-4244-9636","10.1109/IJCNN.2011.6033575","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6033575","","Training;Computer architecture;Microprocessors;Acceleration;Parallel processing;Runtime;Testing","cellular neural nets;image processing;Kalman filters;matrix inversion;recurrent neural nets","GPGPU acceleration;cellular simultaneous recurrent networks;maze traversals;bioinspired computing;Intel Xeon X5550 processor;NVIDIA Tesla C2050;general purpose graphical processing unit;extended Kalman filters;matrix inversion;image processing;frequency 2.67 GHz","","1","16","","","","","","IEEE","IEEE Conferences"
"Towards Brain First-Aid: A Diagnostic Device for Conscious Awareness","R. C. N. D’Arcy; S. G. Hajra; C. Liu; L. D. Sculthorpe; D. F. Weaver","Institute for Biodiagnostics (Atlantic), National Research Council Canada, Departments of Radiology and Neuroscience, Dalhousie University, Halifax, Halifax, CanadaCanada; Institute for Biodiagnostics (Atlantic), National Research Council Canada, Halifax, Canada; Institute for Biodiagnostics (Atlantic), National Research Council Canada, Halifax, Canada; Institute for Biodiagnostics (Atlantic), National Research Council Canada, Halifax, Canada; School of Biomedical Engineering , Department of Medicine (Neurology), and the Department of Chemistry, Dalhousie University, Halifax, Canada","IEEE Transactions on Biomedical Engineering","","2011","58","3","750","754","When the brain is damaged, evaluating an individual's level of awareness can be a major diagnostic challenge (Is he or she in there?). Existing tests typically rely on behavioral indicators, which are incorrect in as many as one out of every two cases. The current paper presents a diagnostic device that addresses this problem. The technology circumvents behavioral limitations through noninvasive brain wave measurements (electroencephalography, or EEG). Unlike traditional EEG, the device is designed for point-of-care use by incorporating a portable, user-friendly, and stable design. It uses a novel software algorithm that automates subject stimulation, data acquisition/analysis, and the reporting of results. The test provides indicators for five identifiable levels of neural processing: sensation, perception, attention, memory, and language. The results are provided as rapidly obtained diagnostic, reliability, validity, and prognostic scores. The device can be applied to a wide variety of patients across a host of different environments. The technology is designed to be wireless-enabled for remote monitoring and assessment capabilities. In essence, the device is developed to scan for conscious awareness in order to optimize subsequent patient care.","0018-9294;1558-2531","","10.1109/TBME.2010.2090880","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5625897","Health monitoring;neuroscience;point-of-care diagnostics;sensor technologies utilization;wireless reporting and assessment","Electroencephalography;Hardware;Computers;Electrodes;Neuroscience;Testing;Monitoring","data acquisition;electroencephalography;patient care;patient monitoring","brain first aid;diagnostic device;conscious awareness;electroencephalography;EEG;subject stimulation;data acquisition;data analysis;remote monitoring;patient care","Algorithms;Awareness;Consciousness;Diagnosis, Computer-Assisted;Electroencephalography;Humans;Monitoring, Physiologic;Point-of-Care Systems;Signal Processing, Computer-Assisted;Telemedicine;Telemetry","10","22","","","","","","IEEE","IEEE Journals & Magazines"
"How to Shop for Free Online -- Security Analysis of Cashier-as-a-Service Based Web Stores","R. Wang; S. Chen; X. Wang; S. Qadeer","NA; NA; NA; NA","2011 IEEE Symposium on Security and Privacy","","2011","","","465","480","Web applications increasingly integrate third-party services. The integration introduces new security challenges due to the complexity for an application to coordinate its internal states with those of the component services and the web client across the Internet. In this paper, we study the security implications of this problem to merchant websites that accept payments through third-party cashiers (e.g., PayPal, Amazon Payments and Google Checkout), which we refer to as Cashier-as-a-Service or CaaS. We found that leading merchant applications (e.g., NopCommerce and Interspire), popular online stores (e.g., Buy.com and JR.com) and a prestigious CaaS provider (Amazon Payments) all contain serious logic flaws that can be exploited to cause inconsistencies between the states of the CaaS and the merchant. As a result, a malicious shopper can purchase an item at an arbitrarily low price, shop for free after paying for one item, or even avoid payment. We reported our findings to the affected parties. They either updated their vulnerable software or continued to work on the fixes with high priorities. We further studied the complexity in finding this type of logic flaws in typical CaaS-based checkout systems, and gained a preliminary understanding of the effort that needs to be made to improve the security assurance of such systems during their development and testing processes.","2375-1207;1081-6011","978-0-7695-4402-1978-1-4577-0147","10.1109/SP.2011.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5958046","e-Commerce security;web API;Cashier-as-a-Service;logic bug;program verification","Security;Servers;Software;Browsers;Complexity theory;Web services;Google","electronic commerce;financial data processing;Internet;retail data processing;security of data;Web sites","security analysis;cashier-as-a-service based Web stores;third-party services;Internet;merchant Web sites;third-party cashiers;PayPal;Amazon Payments;Google Checkout;NopCommerce;Interspire;Buy.com;JR.com;CaaS-based checkout systems","","31","39","","","","","","IEEE","IEEE Conferences"
"Effects of combined thermal and vibration loadings on the wire bond integrity","K. A. Kamaludin; M. Mirgkizoudi; C. Liu; S. Riches","Wolfson School of Mechanical and Manufacturing Engineering, Loughborough University, Loughborough, LE11 3TU, UK; Wolfson School of Mechanical and Manufacturing Engineering, Loughborough University, Loughborough, LE11 3TU, UK; Wolfson School of Mechanical and Manufacturing Engineering, Loughborough University, Loughborough, LE11 3TU, UK; GE Aviation Systems-Newmarket, 351 Exning Road, Newmarket, Suffolk, CB8 0AU, UK","2011 12th International Conference on Electronic Packaging Technology and High Density Packaging","","2011","","","1","4","The sustainable development of the UK electronics industry requires the establishment of a robust supply chain that uniquely complements the UK's expertise and record of technological innovation. Rather than competing with low labor cost economies, a major role for UK electronics manufacturing is to support and innovate in areas such as aerospace, defence, healthcare, energy and niche high value production, where the UK retains a technological lead. For several decades, the knowledge and infrastructure has been developed for conventional consumer products, which usually represent the high volume rather than high value electronics market. Therefore, an obvious knowledge gap and lack of supporting hardware and software in testing and qualification of electronics under non-consumer oriented harsh conditions are identified. Unfortunately, the fundamentals of device/components failure mechanisms under combined complex harsh conditions do not exist, which will ultimately underpin the advancement in design and optimization of existing or new products. The goal of this work is on improving the understating of the electronic components' behavior in extreme environments. Particularly, it focuses on the identification of the failure modes for wires and wire bonds, as one of the interconnect methods, in combined conditions, such as, thermal and mechanical loadings. Particularly, the work presents a series of tests that have been performed to study the reliability of the electronic packages under the combined conditions of thermal and vibration loadings.","","978-1-4577-1769-7978-1-4577-1770-3978-1-4577-1768","10.1109/ICEPT.2011.6066958","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6066958","","Wires;Vibrations;Substrates;Testing;Loading;Reliability;Space vehicles","electronics industry;electronics packaging;lead bonding;reliability;sustainable development;vibrations;wires (electric)","thermal loading;vibration loading;sustainable development;robust supply chain;UK electronics manufacturing industry;electronics market;device-components failure mechanism;wires failure mode identification;wire bond failure mode identification;mechanical loading;electronic package reliability","","","15","","","","","","IEEE","IEEE Conferences"
"Dynamic Active Storage for High Performance I/O","C. Chen; Y. Chen","NA; NA","2012 41st International Conference on Parallel Processing","","2012","","","379","388","Many high-end computing applications in critical areas of science and technology are becoming more and more data intensive. These applications transfer large amounts of data from storage nodes to compute nodes for processing, which is costly and bandwidth consuming. The data movement often dominates the applications' run time. Active storage provides a promising solution for these applications by moving appropriate computations from compute nodes to storage nodes. The prior research has achieved considerable progress and developed several active storage models. However, the existing studies have neglected the influence of data dependence on the performance of active storage systems. This study shows that the data dependence has a critical impact on active storage, and the ignorance of dependence can lead to waste of the precious bandwidth. To address this issue in active storage, this paper also presents a new Dynamic Active Storage (DAS) architecture that analyzes the bandwidth requirement of operations, determines the applicability for active storage requests, and optimizes data layout on servers to minimize the bandwidth requirement. Experimental tests have been conducted, and the results have confirmed that the proposed DAS architecture outperforms existing active storage systems. The DAS architecture reduces the data movement caused by data dependence and improves applications' performance over existing schemes. It holds a promise for high performance I/O system in high-end computing.","0190-3918;0190-3918;2332-5690","978-1-4673-2508-0978-0-7695-4796","10.1109/ICPP.2012.22","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6337599","dynamic active storage;active storage;high end computing;data intensive computing;parallel I/O;parallel file systems","Strips;Bandwidth;Computer architecture;Servers;Computational modeling;Data models;Distributed databases","data handling;input-output programs;software architecture;storage management","dynamic active storage systems;high performance I/O;high-end computing;science and technology;data dependence;DAS architecture;data layout","","3","32","","","","","","IEEE","IEEE Conferences"
"Towards Autonomic Service Provisioning Systems","M. Mazzucco","NA","2010 10th IEEE/ACM International Conference on Cluster, Cloud and Grid Computing","","2010","","","273","282","This paper discusses our experience in building SPIRE, an autonomic system for service provision. The architecture consists of a set of hosted Web Services subject to QoS constraints, and a certain number of servers used to run session-based traffic. Customers pay for having their jobs run, but require in turn certain quality guarantees: there are different SLAs specifying charges for running jobs and penalties for failing to meet promised performance metrics. The system is driven by an utility function, aiming at optimizing the average earned revenue per unit time. Demand and performance statistics are collected, while traffic parameters are estimated in order to make dynamic decisions concerning server allocation and admission control. Different utility functions are introduced and a number of experiments aiming at testing their performance are discussed. Results show that revenues can be dramatically improved by imposing suitable conditions for accepting incoming traffic, the proposed system performs well under different traffic settings, and it successfully adapts to changes in the operating environment.","","978-1-4244-6988-8978-1-4244-6987-1978-0-7695-4039","10.1109/CCGRID.2010.125","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5493472","Web Services;SLA;QoS;utility function;autonomic computing","Web services;Quality of service;Costs;Delay;Admission control;Clouds;Grid computing;Computer science;Buildings;Computer architecture","constraint handling;quality of service;software fault tolerance;Web services","autonomic service provisioning systems;SPIRE;Web services;QoS constraints;run session based traffic;SLA","","6","25","","","","","","IEEE","IEEE Conferences"
"Design and Realization of a High-Performance Integrated Attitude Determination System and Hybrid Simulation","W. Quan; L. Xu; Y. Ou","NA; NA; NA","2011 Second International Conference on Digital Manufacturing & Automation","","2011","","","355","360","A high-performance integrated attitude determination system based on multi-sensor is proposed and implemented, which is suitable for high resolution of ground stability and agile mobile observation of the new generation earth observation micro-satellites. This system includes the sensitive subsystem, the signal collecting and preprocessing subsystem and the comprehensive processing subsystem. The sensitive subsystem is integrated by four types of sensitive components sub-software, which include the star sensitive probe, the sun sensitive probe, the fiber-optic gyroscope and the magnetic probe. The signal collecting and preprocessing subsystem, implemented by the FPGA module and its periphery circuit, receives output data from the four types of sensitive probe in parallel, and accomplish the data smoothing and preprocessing. According to the sunlight information, starlight information, inertial information and information based on magnetic after they are smoothness and pretreatment, the comprehensive processing subsystem, with ARM module as the core, adopts the combination attitude determination method based on the ant colony particle filter algorithm to realize the high-performance attitude determination of the system. Aiming at the integration needs of micro-satellites, a kind of multi-sensor integrated attitude determination system structure of small volume, light weight, high level of integration, high reliability is designed. Finally, tests and experiments based on the integrated attitude determination system were conducted, which meet the design requirements.","","978-1-4577-0755-1978-0-7695-4455","10.1109/ICDMA.2011.94","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6052025","Integrated Attitude Determination System;Multi-sensor;Ant Colony Particle Filter Algorithm","Position measurement;Probes;Sun;Field programmable gate arrays;Particle filters;Gyroscopes;Optical fiber sensors","aerospace instrumentation;artificial satellites;attitude measurement;fibre optic gyroscopes;field programmable gate arrays;geophysical equipment;optimisation;particle filtering (numerical methods);remote sensing","high performance integrated attitude determination system;hybrid simulation;multisensor system;ground stability;agile mobile observation;earth observation microsatellite;sensitive subsystem;sun sensitive probe;fiber optic gyroscope;magnetic probe;signal collecting subsystem;signal preprocessing subsystem;FPGA module;sunlight information;starlight information;inertial information;ARM module;ant colony particle filter algorithm","","","11","","","","","","IEEE","IEEE Conferences"
"A System for Nuclear Fuel Inspection Based on Ultrasonic Pulse-Echo Technique","Z. D. Thome; W. C. A. Pereira; J. C. Machado; J. M. Seixas; W. Soares-Filho","COPPE/UFRJ, Federal University of Rio de Janeiro, Rio de Janeiro, RJ, Brazil; COPPE/UFRJ, Federal University of Rio de Janeiro, Rio de Janeiro, RJ, Brazil; COPPE/UFRJ, Federal University of Rio de Janeiro, Rio de Janeiro, RJ, Brazil; COPPE/UFRJ, Federal University of Rio de Janeiro, Rio de Janeiro, RJ, Brazil; IPqM, Brazilian Navy Research Institute, Rio de Janeiro, RJ, Brazil","IEEE Transactions on Nuclear Science","","2011","58","5","2452","2458","Nuclear Pressurized Water Reactor (PWR) technology has been widely used for electric energy generation. The follow-up of the plant operation has pointed out the most important items to optimize the safety and operational conditions. The identification of nuclear fuel failures is in this context. The adoption of this operational policy is due to recognition of the detrimental impact that fuel failures have on operating cost, plant availability, and radiation exposure. In this scenario, the defect detection in rods, before fuel reloading, has become an important issue. This paper describes a prototype of an ultrasonic pulse-echo system designed to inspect failed rods (with water inside) from PWR. This system combines development of hardware (ultrasonic transducer, mechanical scanner and pulser-receiver instrumentation) as well as of software (data acquisition control, signal processing and data classification). The ultrasonic system operates at center frequency of 25 MHz and failed rod detection is based on the envelope amplitude decay of successive echoes reverberating inside the clad wall. The echoes are classified by three different methods. Two of them (Linear Fisher Discriminant and Neural Network) have presented 93% of probability to identify failed rods, which is above the current accepted level of 90%. These results suggest that a combination of a reliable data acquisition system with powerful classification methods can improve the overall performance of the ultrasonic method for failed rod detection.","0018-9499;1558-1578","","10.1109/TNS.2011.2164557","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6031787","Neural network;non-destructive testing;nuclear fuel inspection;ultrasound","Fuels;Probes;Acoustics;Electron tubes;Assembly;Inspection;Face","data acquisition;fission reactor fuel;fission reactor operation;fission reactor safety;light water reactors;neural nets;ultrasonic transducers","nuclear fuel inspection;ultrasonic pulse-echo technique;nuclear pressurized water reactor;PWR technology;electric energy generation;operating cost;plant availability;radiation exposure;ultrasonic transducer;mechanical scanner;pulser-receiver instrumentation;data acquisition control;signal processing;data classification;linear Fisher discriminant;neural network;frequency 25 MHz","","","23","","","","","","IEEE","IEEE Journals & Magazines"
"Lossless image coding by cellular neural networks with minimum coding rate learning","K. Takizawa; S. Takenouchi; H. Aomori; T. Otake; M. Tanaka; I. Matsuda; S. Itoh","Department of Electrical Engineering, Tokyo University of Science, 2641 Yamazaki, Noda-shi, Chiba 278-8510 Japan; Department of Electrical Engineering, Tokyo University of Science, 2641 Yamazaki, Noda-shi, Chiba 278-8510 Japan; Department of Electrical Engineering, Tokyo University of Science, 2641 Yamazaki, Noda-shi, Chiba 278-8510 Japan; Department of Software Science, Tamagawa University, 6-1-1 Tamagawagakuen, Machida, Tokyo, 194-8610 Japan; Department of Information and Communication Sciences, Sophia University, 7-1 Kioi-cho, Chiyoda-ku, Tokyo 102-8554 Japan; Department of Electrical Engineering, Tokyo University of Science, 2641 Yamazaki, Noda-shi, Chiba 278-8510 Japan; Department of Electrical Engineering, Tokyo University of Science, 2641 Yamazaki, Noda-shi, Chiba 278-8510 Japan","2011 20th European Conference on Circuit Theory and Design (ECCTD)","","2011","","","33","36","In this paper, a novel lossless image coding scheme using the cellular neural network (CNN) is proposed. From the viewpoint of the optimal lossless coding, our method is optimized for not only mean squared error (MSE) but also a coding rate. The key idea of this work is that the local structure of an image is modeled by six types of CNN templates in order to achieve high prediction performance, and the CNN parameters that gives prediction characteristic are decided by the supervised minimum coding rate learning. Moreover, in the entropy coding layer, the prediction residuals are coded by an adaptive arithmetic encoder with context modeling for high coding efficiency. The effectiveness of proposed algorithm is confirmed by some computer simulations of various standard test images, and its performance is compared with that of conventional hierarchical coding schemes having scalability.","","978-1-4577-0618-9978-1-4577-0617-2978-1-4577-0616","10.1109/ECCTD.2011.6043337","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6043337","","Image coding;Encoding;Minimization;Cellular neural networks;Context modeling;Image edge detection;Shape","cellular neural nets;digital simulation;image coding;learning (artificial intelligence);mean square error methods","lossless image coding;cellular neural networks;mean squared error;supervised minimum coding rate learning;adaptive arithmetic encoder;context modeling;computer simulations;hierarchical coding schemes","","1","8","","","","","","IEEE","IEEE Conferences"
"Discourse marker based topic identification and search results refining","N. Suwandaratna; U. Perera","Informatics Institute of Technology, Colombo, Sri Lanka; Informatics Institute of Technology, Colombo, Sri Lanka","2010 Fifth International Conference on Information and Automation for Sustainability","","2010","","","119","125","Most research oriented search queries consist of multiple topics belonging to one or more domains of knowledge. The objective of such search queries is to find the relationship or impact that each topic has on the other(s). Though web search engines provide easy means to retrieve information off the web, search engines are mainly key word oriented and do not consider the different topics and relationships between such topics present in the results. This project presents a software solution, `SearchDroid' that acts as an intermediate user between a search engine and the end user, refining search results based on different topics identified in the search query and their presence and relationships depicted in search results. A discourse parsing approach has been used to build discourse structures that represent the rhetorical relations of text in search results; this is used to re-rank results based on topics identified in the search query. The project combines linguistics research under discourse parsing with web information retrieval techniques. The lack of literature combining discourse parsing techniques with web information retrieval has been compensated for by introducing a fresh algorithmic approach. An abstract information retrieval mechanism has been created with the use of discourse parsing techniques, and can be integrated into any web information retrieval approach. The system was evaluated by Linguistic Experts, Technical Experts and End users. All experts agreed that a discourse parsing approach was suitable for addressing the problem at hand and that the project had high research value in the aspect of linking linguistics research with web information retrieval research. Hands-on testing of the system by End Users produced high user acceptance of the proposed system.","2151-1802;2151-1810","978-1-4244-8552-9978-1-4244-8549-9978-1-4244-8551","10.1109/ICIAFS.2010.5715646","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5715646","","Search engines;Buildings;Refining;Pragmatics;Robustness;Software","grammars;Internet;knowledge based systems;query processing;search engines;text analysis","discourse marker;topic identification;search result refining;search query;knowledge domain;Web search engine;SearchDroid;discourse parsing approach;query text;Web information retrieval technique;linguistic expert","","1","7","","","","","","IEEE","IEEE Conferences"
"Design of simulation and verification system for rotation strapdown inertial navigation system","J. Cheng; D. Guan; X. Cheng","Automation College, Harbin Engineering University, Harbin, Heilongjiang Province, China; Automation College, Harbin Engineering University, Harbin, Heilongjiang Province, China; Broadcast department, Yantai Broadcast and Television Information Network Ltd., Yantai, Shandong Province, China","2012 IEEE International Conference on Mechatronics and Automation","","2012","","","865","869","Aiming at the problem that quantitative error analysis of strapdown inertial navigation system (SINS) caused by key error cannot be achieved, this paper accomplished the design of simulation and verification system for the single-axis continuous and the to-and-fro rotation SINS. By simulating inertial measurement unit (IMU) outputting information in different single-axis rotation methods, the principle error of SINS is simulated and validated. By designing algorithm to compensate IMU errors online, the actual rotation SINS error caused by IMU random drift error, scale factor error and installation error can be optimized. In this paper, design of individual algorithmic process and functional module are illustrated. The test results show the simulation and verification system can carry out the simulation for the rotation SINS scheme well, and can provide effective support for improvement of rotation scheme technically.","2152-7431;2152-744X","978-1-4673-1278-3978-1-4673-1275-2978-1-4673-1277","10.1109/ICMA.2012.6283256","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6283256","SINS;Rotation modulation;error simulation;software design","Silicon compounds;Navigation;Angular velocity;Analytical models;Data models;Mathematical model;Algorithm design and analysis","aircraft instrumentation;formal verification;inertial navigation;object-oriented methods","verification system;rotation strapdown inertial navigation system;quantitative error analysis;strapdown inertial navigation system;single-axis continuous;to-and-fro rotation SINS;inertial measurement unit simulation;IMU simulation;single-axis rotation methods;IMU errors online compensation;SINS error;IMU random drift error;scale factor error;installation error","","1","7","","","","","","IEEE","IEEE Conferences"
"Keynote talk 2: How to build an industrial R&D center in Vietnam: A case study","H. L. Nguyen","TMA Solutions, Vietnam","The 2011 International Conference on Advanced Technologies for Communications (ATC 2011)","","2011","","","2","2","Summary form only given. To be competitive in today's market, the IT industry faces many challenges in the development and maintenance of enterprise information systems. Engineering these largescaled systems efficiently requires making decisions about a number of issues. In addition, customers expectations imply continuous software delivery in predictable quality. The operation such systems demands for transparency of the software in regard to lifecycle, change and incident management as well as cost efficiency. Addressing these challenges, we learned how to benefit from traditional industries. Contrary to the fact that the IT business calls itself gladly an industry, the industrialization of software engineering in most cases moves on a rather modest level. Industrialization means not only to build a solution or product on top of managed and well-defined processes, but also to have access to structured information about the current conditions of manufacturing at any time. Comparably with test series and assembly lines of the automobile industry, each individual component and each step from the beginning of manufacturing up to the final product should be equipped with measuring points for quality and progress. Even one step further the product itself, after it has left the factory, should be able to continuously provide analytic data for diagnostic reasons. Information is automatically collected and builds the basic essentials for process control, optimization and continuous improvement of the software engineering process. This presentation shows by means of a practical experience report how AdNovum managed to build its software engineering based on a well-balanced system of processes, continuous measurement and control — as well as a healthy portion of pragmatism. We implemented an efficient and predictable software delivery pipeline","2162-1039;2162-1020;2162-1020","978-1-4577-1207-4978-1-4577-1206-7978-1-4577-1205","10.1109/ATC.2011.6027420","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6027420","","","","","","","","","","","","","IEEE","IEEE Conferences"
"Development and application of ECT digital system for online flow measurement","Z. Cui; H. Wang; C. Yang; D. Zhang; Y. Geng","School of Electrical Engineering and Automation, Tianjin University, China; School of Electrical Engineering and Automation, Tianjin University, China; School of Electrical Engineering and Automation, Tianjin University, China; College of Mining Engineering, Taiyuan University of Technology, China; College of Mining Engineering, Taiyuan University of Technology, China","2012 IEEE International Conference on Imaging Systems and Techniques Proceedings","","2012","","","599","604","The paper presents the latest development of a digital-based measurement system for electrical capacitance tomography (ECT), that consists of a high-capacity FPGA and fast digital converters. In this system, most functions, e.g. signal generation, phase-sensitive demodulator and low-pass filters, are implemented digitally in FPGA. Tomographic software in Visual C++ has been developed for real-time data acquisition, image re-construction and visualization and parameter calculation. Having been specially optimized for ECT measurement, the custom-built hardware and software are capable of achieving acquisition rate of &gt;; 1000fps. The developed ECT system has been applied in online monitoring of a pneumatic conveying system that is to be used for mine out area refilling. Regional flow parameters could be obtained according to specific subregion division. In addition, having been tested on the pneumatic conveying system for several days, the ECT system is proved to be reliable and durable.","1558-2809","978-1-4577-1775-8978-1-4577-1776-5978-1-4577-1774","10.1109/IST.2012.6295499","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6295499","","Magnetic circuits;Visualization","C++ language;computerised monitoring;computerised tomography;convertors;conveyors;data acquisition;data visualisation;electric impedance imaging;field programmable gate arrays;flow measurement;flow visualisation;image reconstruction;mining industry;pneumatic systems;production engineering computing","ECT digital system;electrical capacitance tomography;online flow measurement;digital-based measurement system;high-capacity FPGA;fast digital converters;signal generation;phase-sensitive demodulator;low-pass filters;tomographic software;Visual C++;real-time data acquisition;image reconstruction;image visualization;parameter calculation;online monitoring;pneumatic conveying system;mine out area refilling;regional flow parameters","","","16","","","","","","IEEE","IEEE Conferences"
"Digital control issue of high speed switched reluctance motor","Qingqing Ma; Daqiang Bi; Baoming Ge","School of Electrical Engineering, Beijing Jiaotong University, 10044, China; State Key Lab of Power System, Dept. of Electrical Engineering, Tsinghua University, Beijing 10083, China; School of Electrical Engineering, Beijing Jiaotong University, 10044, China","2012 IEEE International Symposium on Industrial Electronics","","2012","","","641","646","Turn-on and turn-off angle greatly affect the electromagnetic torque for switched reluctance motor(SRM), so it is particularly important to optimize these parameters. The high-speed SRM requires the high-frequency in the operation and short response time, which bring some issues, such as the calculation and control of position signal. To address these problems, an online adjustment approach for turn-on and turn-off angle which are beneficial to the performance of high speed SRM. In addition, considering the high changing rate of current and voltage at high-speed, the control system must have excellent anti-interference ability, and some anti-interference measures in the software and hardware are adopted. At last, this paper designs a digital controller for the high-speed switch reluctance motor (SRM) based on DSP TMS320LF2812, including the hardware and software design. The experimental results verify that the proposed on-line adjustment approach and anti-interference measures at the high-speed SRM are feasible. And the tested SRM with an excellent dynamic response can reach 88,000 rpm in short time.","2163-5145;2163-5137;2163-5137","978-1-4673-0158-9978-1-4673-0159-6978-1-4673-0157","10.1109/ISIE.2012.6237164","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6237164","","Reluctance motors;Rotors;Torque;Windings;Stator windings","digital control;digital signal processing chips;machine control;position control;reluctance motors","digital control;high-speed switched reluctance motor;turn-on turn-off angle;electromagnetic torque;switched reluctance motor;SRM;high-speed SRM;position signal control;online adjustment approach;antiinterference ability;DSP TMS320LF2812;hardware-software design;dynamic response","","","14","","","","","","IEEE","IEEE Conferences"
"MPSoC Performance Analysis with Virtual Prototyping Platforms","D. Castells-Rufas; J. Joven; S. Risueno; E. Fernandez; J. Carrabina; T. William; H. Mix","NA; NA; NA; NA; NA; NA; NA","2010 39th International Conference on Parallel Processing Workshops","","2010","","","154","160","There is some consensus that Embedded and HPC domains have to create synergies to face the challenges to create, maintain and optimize software for the future many-core platforms. In this work we show how some HPC performance analysis methods can be successfully adapted to the embedded domain. We propose to use Virtual Prototypes based on Instruction Set Simulators to produce trace files by transparent instrumentation that can be used for post-mortem performance analysis. Transparent instrumentation on ISS kills two birds in one shot: it adds no overhead for trace generation and it solves the problem of trace storage. A virtual prototype is build to generate OTF traces that are later analyzed with Vampir. We show how the performance analysis of the virtual prototype is valuable to optimize a parallel embedded test application, allowing an acceptable speedup factor on 4 processors to be obtained.","0190-3918;1530-2016;2332-5690","978-1-4244-7918-4978-0-7695-4157","10.1109/ICPPW.2010.32","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5599206","MPSoC;Trace Generation;NIOS;FPGA;NoC;Virtual Prototypes;Performance Analysis","Instruments;Program processors;Performance analysis;Prototypes;Communities;Manuals","embedded systems;instruction sets;microprocessor chips;multiprocessing systems;virtual prototyping","MPSoC performance analysis;virtual prototyping platforms;embedded system;many-core platforms;HPC performance analysis methods;instruction set simulators;post-mortem performance analysis;transparent instrumentation;trace storage;OTF trace generation;Vampir;parallel embedded test application","","1","22","","","","","","IEEE","IEEE Conferences"
"Market based approach for solving optimal power flow problem in smart grid","B. HomChaudhuri; M. Kumar; V. Devabhaktuni","School of Dynamic Systems, University of Cincinnati, Cincinnati, OH 45221 USA; School of Dynamic Systems, University of Cincinnati, Cincinnati, OH 45221 USA; EECS Department, University of Toledo, Toledo, OH 43606","2012 American Control Conference (ACC)","","2012","","","3095","3100","Smart grid has generated much attention recently due to its potential in bringing a revolutionary change in the production, distribution, and utilization of power. However, before a smart grid can become fully functional, it requires technological advancements in a number of interdisciplinary domains. Even though smart grid facilitates run-time optimal allocation of power via extensive instrumentation and information accessibility, the process of optimal power allocation becomes challenging due to the massively distributed generation facilities, loads, and due to the intermittency of generation. Optimal power flow problem essentially minimizes the overall cost of power generation while meeting the total load or power demand at the consumer end. In this paper, a Market Based technique has been presented to carry out the optimal power flow in a smart grid. The Market Based Resource Allocation is inspired from the concepts in economic market, where resources are allocated to activities through the process of competitive buying and selling. In the proposed technique, consumers act as potential power buyers and generation units act as the power sellers. The proposed method derives its significance due to its ability in optimizing power flow in a grid in a distributed manner, i.e., from local interactions. This feature provides immense scalability and robustness to uncertainties. In addition, this paper presents the evaluation of the proposed Market Based technique via a simulated scenario of power consumers and producers in a Smart Grid. The IEEE 30-bus system (RTS 79) with six generation units is used to test the proposed method in optimizing the total generation cost, and the results are compared with that obtained from widely used Matpower software.","2378-5861;0743-1619;0743-1619;0743-1619","978-1-4577-1096-4978-1-4577-1095-7978-1-4577-1094-0978-1-4673-2102","10.1109/ACC.2012.6315580","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6315580","","Cost function;Load flow;Smart grids;Resource management;Generators;Power generation","power distribution economics;power generation economics;power markets;power utilisation;resource allocation;smart power grids","optimal power flow problem;smart grid;power production;power distribution;power utilization;optimal power allocation;power generation cost minimization;power demand;market based resource allocation;economic market;IEEE 30-bus system;RTS 79;Matpower software","","4","28","","","","","","IEEE","IEEE Conferences"
"Performance of High-Reliability and High-Linearity InGaP/GaAs HBT PAs for Wireless Communication","M. Tu; H. Ueng; Y. Wang","NA; NA; NA","IEEE Transactions on Electron Devices","","2010","57","1","188","194","An InGaP heterojunction-bipolar-transistor (HBT) power amplifier with the best linearity and high reliability is presented in this paper for use in wireless digital mobile communication systems. We optimized the linearity of a novel HBT device and investigated its reliability. Using SILVACO software, we performed a simulation of the HBT device. The best linearity, which was revealed for the device with a capacitance ratio C<sub>bc</sub> (0/6 V), is 1.25 at a BV<sub>ceo</sub> of 22 V. After the device was fabricated, a reasonably high PAE, i.e., ~ 55%, was obtained at 2.0 GHz, and an adjacent channel power ratio of over -48 dBc was achieved. In the reliability testing, the device, which was stressed at V<sub>ce</sub> = 3 V and J<sub>C</sub> = 25 kA/cm<sup>2</sup> under 85°C ambient temperature and 85% humidity, showed no failure for more than 1100 h. No significant beta degradation was observed under an extreme current J<sub>C</sub> = 200 kA/cm<sup>2</sup> stress under wafer-level electrical/thermal overstress tests.","0018-9383;1557-9646","","10.1109/TED.2009.2035543","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5345815","Adjacent channel power ratio (ACPR);collector-base capacitance;epitaxial;heterojunction bipolar transistor (HBT)","Heterojunction bipolar transistors;Performance evaluation;Layout;Linearity;Metals;Epitaxial growth;Reliability","gallium arsenide;heterojunction bipolar transistors;indium compounds;mobile communication;power amplifiers","high-reliability InGaP/GaAs HBT PA;high-linearity InGaP/GaAs HBT PA;wireless communication;InGaP heterojunction-bipolar-transistor power amplifier;wireless digital mobile communication systems;SILVACO software;voltage 22 V;frequency 2.0 GHz;voltage 3 V;temperature 85 degC;InGaP;GaAs","","22","27","","","","","","IEEE","IEEE Journals & Magazines"
"OpenMAC: A new reconfigurable experimental platform for energy-efficient medium access control protocols","F. Vazquez Gallego; J. Alonso-Zarate; C. Liss; C. Verikoukis","Centre Tecnologic de Telecomunicacions de Catalunya (CTTC), Parc Mediterrani de la Tecnologia (PMT), Av. Carl Friedrich Gauss 7, 08860 Castelldefels, Barcelona, Spain; Centre Tecnologic de Telecomunicacions de Catalunya (CTTC), Parc Mediterrani de la Tecnologia (PMT), Av. Carl Friedrich Gauss 7, 08860 Castelldefels, Barcelona, Spain; InnoRoute GmbH, P.O. Box 260114, 80058 Munich, Germany; Centre Tecnologic de Telecomunicacions de Catalunya (CTTC), Parc Mediterrani de la Tecnologia (PMT), Av. Carl Friedrich Gauss 7, 08860 Castelldefels, Barcelona, Spain","IET Science, Measurement & Technology","","2012","6","3","139","148","OpenMAC is presented in this study as an innovative reconfigurable platform which overcomes the limitations of state-of-the-art experimental tools to test medium access control (MAC) protocols. The purpose of the OpenMAC platform is to simplify the prototyping process by enabling the implementation of MAC protocols designed in C++, relieving the protocol designer from the hardware and timing aspects, and thus avoiding the need to code optimised C/assembly or hardware description language (HDL). Aiming to reduce the hardware design and implementation costs with respect to custom hardware solutions, the OpenMAC platform has been implemented on an inexpensive off-the-shelf reconfigurable field programmable gate array (FPGA)-based development board with a processor embedded in the FPGA. The challenge presented by the proposed platform is to fulfil strict MAC time-constraints with compiled straightforward and clean C++ code. For this purpose, the OpenMAC platform introduces an innovative hardware/software partitioning concept for MAC protocol implementation which is based on a shared-memory architecture. Measurements carried out on an FPGA board demonstrate that this platform meets the short inter-frame space (SIFS) specification of the IEEE 802.11 standard, hence enabling field testing of prototyped MAC protocols.","1751-8822;1751-8830","","10.1049/iet-smt.2011.0069","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200802","","","access protocols;C++ language;cost reduction;field programmable gate arrays;shared memory systems;wireless LAN","reconfigurable experimental platform;energy-efficient medium access control protocol;OpenMAC platform;MAC protocol;hardware design;implementation cost reduction;off-the-shelf reconfigurable field programmable gate array;FPGA-based development board;processor;C++ code;hardware-software partitioning concept;shared-memory architecture;short inter-frame space specification;IEEE 802.11 standard","","2","","","","","","","IET","IET Journals & Magazines"
"Evaluation of a vortex model of buoyancy-driven recirculation in potential flow analysis of data center performance","M. M. Toulouse; D. J. Lettieri; V. P. Carey; C. E. Bash; A. J. Shah","Mechanical Engineering Department, University of California, Berkeley, USA; Mechanical Engineering Department, University of California, Berkeley, USA; Mechanical Engineering Department, University of California, Berkeley, USA; Hewlett Packard Laboratories, Palo Alto, CA, USA; Hewlett Packard Laboratories, Palo Alto, CA, USA","13th InterSociety Conference on Thermal and Thermomechanical Phenomena in Electronic Systems","","2012","","","51","59","The prediction of flow and temperature in data center operation is of particular importance in creating better capacity utilization and lower capital costs. However, it is often a time- and computing-intensive task, and would be well served by more expeditious modeling methods than full Computational Fluid Dynamic (CFD) thermofluidic models. A software package named COMPACT (Compact Model of Potential Flow and Convective Transport) was developed to provide one such alternative. Recent versions of COMPACT take under 10 seconds on a commercially available laptop to characterize a 550 square foot data center; the same room modeled with a CFD solver took 8 hours. Having the ability to create velocity and temperature predictions orders of magnitude faster than conventional CFD allow a variety of data center applications for the model, such as use as a first-order design tool, a potential improvement to plant-based controllers, a tool for system-wide assessment of life-cycle efficiency, and as an initial guess for complex CFD solvers. COMPACT applies convective energy transport equations to a computed potential flow field to approximate a flow and temperature field. The results from this model were compared to experimental measurements taken from a data center at Hewlett-Packard Laboratories in Palo Alto, CA. The presence of high localized temperatures in the model led to the conclusion that recirculation and buoyancy were contributing excessively to error in the model. A novel approach was proposed to account for these effects: a non-iterative (to preserve computational resources) method of vortex superposition, in which hot locations in the original model are analyzed and a corrective flow field consisting of Rankine vortices is superimposed on the solution. An updated model using this approach was tested with further experimental measurements taken from the data center at HP Labs, and identical inputs were used to compare COMPACT to commercially available CFD. These newer results showed a marked decrease in mean deviation of the model from measured temperatures, as well as elimination of the highly localized temperatures which afflicted the original COMPACT results. The vortex superposition model was also “tuned”, with vortex strength optimized for multiple test cases at varying levels of recirculation. In addition, the model-generated velocity and temperature fields were used to locate and quantify the destruction of exergy resulting from the mixing of warmer and cooler air in the room; these results are used as a measure of system efficiency.","1087-9870;1087-9870","978-1-4244-9532-0978-1-4244-9533-7978-1-4244-9531","10.1109/ITHERM.2012.6231413","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6231413","compact;expeditious;modeling;CFD;thermal performance;server room;convective transport;vortex superposition;life cycle exergy assessment;exergy destruction","Temperature measurement;Servers;Computational modeling;Computational fluid dynamics;Mathematical model;Atmospheric modeling;Data models","computer centres;convection;mechanical engineering computing;remaining life assessment;vortices","vortex model evaluation;buoyancy-driven recirculation;potential flow analysis;data center performance;flow prediction;computational fluid dynamic;CFD thermofluidic models;compact model of potential flow and convective transport;COMPACT software package;temperature predictions;first-order design tool;plant-based controllers;life-cycle efficiency assessment;system-wide assessment;convective energy transport equations;Hewlett-Packard laboratory;Rankine vortices;vortex superposition model;vortex strength;model-generated velocity;time 10 s;time 8 hour","","3","13","","","","","","IEEE","IEEE Conferences"
"Design and Analysis of Glass Fiber Reinforced Polymer (GFRP) Leaf Spring","B. B. Deshmukh; S. B. Jaju","NA; NA","2011 Fourth International Conference on Emerging Trends in Engineering & Technology","","2011","","","82","87","Weight reduction is now the main issue in automobile industries. Weight reduction can be achieved primarily by the introduction of better material, design optimization and better manufacturing processes. The achievement of weight reduction with adequate improvement of mechanical properties has made composite a very good replacement material for conventional steel. Selection of material is based on cost and strength of material. The composite materials have more elastic strain energy storage capacity and high strength to weight ratio as compared with those of steel, so multi-leaf steel springs are being replaced by mono-leaf composite springs. The paper gives the brief look on the suitability of composite leaf spring on vehicles and their advantages. The objective of the present work is design, analysis and fabrication of mono composite leaf spring. The design constraints are stress and deflections. The material selected is glass fibre reinforced plastic (GFRP) and the epoxy resin can be used which is more economical to reduce total cost of composite leaf spring with similar mechanical and geometrical properties to the multileaf spring. The composite leaf spring is fabricated by hand lay-up technique and tested. The testing was performed experimentally with the help of UTM and by (FEA) using ANSYS software showing stresses and deflections were verified with analytical and experimental results. Compared to the steel spring, the composite spring has stresses that are much lower, the spring weight is nearly 74% lower.","2157-0485;2157-0477;2157-0477","978-1-4577-1847-2978-0-7695-4561","10.1109/ICETET.2011.61","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6120559","Leaf spring;Composite material;FEM;GFRP","Springs;Glass;Steel;Finite element methods;Stress;Strain","automobile industry;design engineering;finite element analysis;glass fibre reinforced plastics;springs (mechanical)","glass fiber reinforced polymer;GFRP leaf spring design;weight reduction;automobile industries;manufacturing processes;mechanical properties;elastic strain energy storage capacity;composite materials;mono-leaf composite springs;glass fibre reinforced plastic;UTM;ANSYS software;FEA","","1","10","","","","","","IEEE","IEEE Conferences"
"Time - resolved Gamma Ray spectral analysis of planetary neutron and Gamma Ray instrumentation","J. G. Bodnarik; K. G. Burger; A. Burger; L. G. Evans; S. L. Floyd; L. F. Lim; T. P. Mcclanahan; M. Namkung; S. F. Nowicki; A. M. Parsons; J. S. Schweitzer; R. D. Starr; K. G. Stassun; J. I. Trombka","Vanderbilt University, Nashville, TN; Vanderbilt University, Nashville, TN; Fisk University, Nashville, TN; Astrochemistry Laboratory, Code 691.0, NASA Goddard Space Flight Center, Greenbelt, MD; Astrochemistry Laboratory, Code 691.0, NASA Goddard Space Flight Center, Greenbelt, MD; Astrochemistry Laboratory, Code 691.0, NASA Goddard Space Flight Center, Greenbelt, MD; Astrochemistry Laboratory, Code 691.0, NASA Goddard Space Flight Center, Greenbelt, MD; Astrochemistry Laboratory, Code 691.0, NASA Goddard Space Flight Center, Greenbelt, MD; Astrochemistry Laboratory, Code 691.0, NASA Goddard Space Flight Center, Greenbelt, MD; Astrochemistry Laboratory, Code 691.0, NASA Goddard Space Flight Center, Greenbelt, MD; University of Connecticut, Storrs, CT; Astrochemistry Laboratory, Code 691.0, NASA Goddard Space Flight Center, Greenbelt, MD; Fisk University, Nashville, TN; Astrochemistry Laboratory, Code 691.0, NASA Goddard Space Flight Center, Greenbelt, MD","IEEE Nuclear Science Symposuim & Medical Imaging Conference","","2010","","","1","6","The current gamma ray/neutron instrumentation development effort at NASA Goddard Space Flight Center¿s Astrochemistry Laboratory aims to extend the use of neutron interrogation techniques, using a 14 MeV Pulsed Neutron Generator (PNG) combined with neutron and gamma ray detectors, to probe the surface and subsurface of planetary bodies in situ without the need to drill. One aspect of the current work includes the development of taking timed tagged event-byevent data using our custom designed software with the Canberra Lynx Digital Signal Analyzer to provide a unique three-dimensional master data set with channel/energy, time, and intensity information. Since the master data set is not limited to predetermined coincidence timing gates set for a specific nuclear process, the user is allowed the flexibility to slice the data cube in a multitude of ways without loss of information or experimental time due to the need for additional acquisition windows. Time tagged event-by-event data allows the user to isolate a particular energy line from the spectrum over a specific window in time with respect to the PNG pulse, analyze a gamma ray spectrum resulting from either neutron capture, between the burst, or inelastic scattering events, during the neutron burst, and extract data for engineering purposes to optimize timing windows to look at specific elements in different environments. In this paper, we will present the results of our experimental data using the time tagged event-by-event data analysis technique compared with non-time-gated data taken at the test facility at NASA Goddard Space Flight Center. Comparison of these data will show the advantages and validity of this method to obtain more precise, sensitive, and accurate elemental composition measurements.","1082-3654;1082-3654","978-1-4244-9105-6978-1-4244-9106-3978-1-4244-9104","10.1109/NSSMIC.2010.6036247","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6036247","","Neutrons;Logic gates;Instruments;Software;Scattering;Histograms;Gamma rays","aerospace instrumentation;data analysis;gamma-ray spectra;time resolved spectra","time-resolved gamma ray spectral analysis;planetary neutron;gamma ray instrumentation;neutron instrumentation;pulsed neutron generator;time tagged event-by-event data analysis technique","","1","4","","","","","","IEEE","IEEE Conferences"
"An open-source pose estimation system for micro-air vehicles","I. Dryanovski; W. Morris; Jizhong Xiao","Dept. of Computer Science, The Graduate Center, The City University of New York (CUNY), 365 Fifth Avenue, 10016, USA; Electrical Engineering Department, City College of New York, Convent Ave &amp; 140th Street, 10031, USA; Electrical Engineering Department, City College of New York, Convent Ave &amp; 140th Street, 10031, USA","2011 IEEE International Conference on Robotics and Automation","","2011","","","4449","4454","This paper presents the implementation of an open-source 6-DoF pose estimation system for micro-air vehicles and considers the future implications and benefits of open-source robotics. The system is designed to provide high frequency pose estimates in unknown, GPS-denied indoor environments. It requires a minimal set of sensors including a planar laser range-finder and an IMU sensor. The code is optimized to run entirely onboard, so no wireless link and ground station are explicitly needed. A major focus in our work is modularity, allowing each component to be benchmarked individually, or swapped out for a different implementation, without change to the rest of the system. We demonstrate how the pose estimation can be used for 2D SLAM or 3D mapping experiments. All the software and hardware which we have developed, as well as extensive documentation and test data, is available online.","1050-4729;1050-4729;1050-4729","978-1-61284-385-8978-1-61284-386-5978-1-61284-380","10.1109/ICRA.2011.5980315","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5980315","","Lasers;Estimation;Vehicles;Robots;Three dimensional displays;Sensors;Open source software","aircraft control;Global Positioning System;laser ranging;mobile robots;pose estimation;remotely operated vehicles;sensors;SLAM (robots)","microair vehicles;open-source 6-DoF pose estimation system;open-source robotics;GPS-denied indoor environments;planar laser range-finder;IMU sensor;2D SLAM;3D mapping experiments","","16","12","","","","","","IEEE","IEEE Conferences"
"Visualization of C++ Template Metaprograms","Z. Borok-Nagy; V. Majer; J. Mihalicza; N. Pataki; Z. Porkolab","NA; NA; NA; NA; NA","2010 10th IEEE Working Conference on Source Code Analysis and Manipulation","","2010","","","167","176","Template metaprograms have become an essential part of today's C++ programs: with proper template definitions we can force the C++ compiler to execute algorithms at compilation time. Among the application areas of template metaprograms are the expression templates, static interface checking, code optimization with adaptation, language embedding and active libraries. Despite all of its already proven benefits and numerous successful applications there are surprisingly few tools for creating, supporting, and analyzing C++ template metaprograms. As metaprograms are executed at compilation time they are even harder to understand. In this paper we present a code visualization tool, which is utilizing Tem plight, our previously developed C++ template metaprogram debugger. Using the tool it is possible to visualize the instantiation chain of C++ templates and follow the execution of metaprograms. Various presentation layers, filtering of template instances and step-by-step replay of the instantiations are supported. Our tool can help to test, optimize, maintain C++ template metaprograms, and can enhance their acceptance in the software industry.","","978-1-4244-8655","10.1109/SCAM.2010.16","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5601850","C++ template metaprogram;code comprehension","Visualization;Data structures;Context;Instruments;Debugging;Libraries;Programming","C++ language;data visualisation;program compilers;program debugging","C++ template metaprogram visualization;C++ compiler;expression templates;static interface checking;code optimization;language embedding;active libraries;code visualization tool;tem plight;C++ template metaprogram debugger","","","28","","","","","","IEEE","IEEE Conferences"
"Collaborative Migration, Coupling and Simulation of Water Resources Models through OpenMI","K. N. Kokkinos; A. G. Loukas","NA; NA","2010 19th IEEE International Workshops on Enabling Technologies: Infrastructures for Collaborative Enterprises","","2010","","","166","171","The present study describes the migration and coupling of a monthly conceptual rainfall runoff model named UTHBAL and a water reservoir operation model called UTHRL into the OpenMI standard. The models have been introduced by Loukas et al.. We have developed a collaborative decision support framework named UTH-MODELER which automates the migration of the two models into OpenMI. This framework synchronizes spatially and temporarily the two models into a collaborative simultaneous simulation. The produced software allows time series input in various formats and model extensibility. Furthermore, an optimization library for the calibration of the models has been incorporated. The experimental verification of the coupled system has been tested for the case study of the reservoir of the restored Lake Karla in Thessally Greece. The results show that the collaboration process is bounded by the time, space and optimization characteristics of the coupled models.","1524-4547;1524-4547","978-1-4244-7217-8978-1-4244-7216-1978-0-7695-4063","10.1109/WETICE.2010.32","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5541949","Coupling;Migration to OpenMI;Model Integration;Simulation","Collaboration;Water resources;Reservoirs;Computational modeling;Computer interfaces;Educational technology;Collaborative work;Engines;Information technology;Civil engineering","calibration;decision support systems;environmental science computing;groupware;rain;reservoirs;simulation;time series","water resources model;rainfall runoff model;UTHBAL;water reservoir operation model;UTHRL;OpenMI standard;decision support framework;UTH-MODELER;time series;optimization library;coupled system;lake Karla;Thessally Greece;open modeling interface","","","13","","","","","","IEEE","IEEE Conferences"
"Parallel program design for JPEG compression encoding","D. Liu; X. Y. Fan","School of Computer Science and Engineering, Northwestern Polytechnical University, Xi' an Shaanxi 710072, China; School of Computer Science and Engineering, Northwestern Polytechnical University, Xi' an Shaanxi 710072, China","2012 9th International Conference on Fuzzy Systems and Knowledge Discovery","","2012","","","2502","2506","Image compression is a kind of data compression technology. The aim of image compression is to reduce redundant information in image data. However, most image compression algorithms have problems such as computational complexity, computational load and so on. Parallel computing is an effective means to improve the processing speed. With the development of high-performance parallel processing systems, parallel image processing algorithms provides more space for improving image processing speed. And with the improvement of GPU performance, GPU is increasingly applied in the computing-concentrated data operation. According to the parallelism and programmability of CUDA, the acceleration for JPEG compression is addressed in this paper. CUDA makes it possible for GPU to do the general purpose computing. The powerful parallel computing power of CUDA GPU can improve the processing speed of JPEG image compression easily. For the parallel processing features and programmability of CUDA, this paper introduces a method of accelerating image compression based on CUDA. An optimal algorithm is proposed as well. The introduction of CUDA allows the image compression for nearly 20 to 24 times speedup. In the end of the paper, we optimize and test the program, and make the analysis of experimental results. Finally we summarize some hardware and software features of CUDA, and propose a basic method of optimizing CUDA kernels from the analysis of the experiment.","","978-1-4673-0024-7978-1-4673-0025-4978-1-4673-0023","10.1109/FSKD.2012.6234221","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6234221","JPEG image compression;CUDA;GPU;DCT;parallel computing","Graphics processing unit;Discrete cosine transforms;Instruction sets;Image coding;Parallel processing;Transform coding;Quantization","data compression;design engineering;graphics processing units;image coding;parallel architectures;parallel programming","parallel program design;JPEG compression encoding;image compression;data compression technology;redundant information reduction;computational complexity;computational load;parallel computing;high-performance parallel processing systems;parallel image processing algorithms;GPU performance;computing-concentrated data operation;general purpose computing;CUDA kernel optimization","","3","10","","","","","","IEEE","IEEE Conferences"
"Fuzzy Modeling of Uncertainties in Generation Scheduling Integrating Wind Farms and Pumped Storage Plants","H. Siahkali; M. Mokhtary; F. Ahmadi","NA; NA; NA","2011 Asia-Pacific Power and Energy Engineering Conference","","2011","","","1","5","This paper presents a new approach to the fuzzy generation scheduling (GS) problem, considering reserve requirement, load-generation balance and wind power generation constraints. The modeling of constraints is an important issue in power system scheduling. These constraints are therefore ""fuzzy"" in nature, and crisp treatment of them may lead to over conservative solutions. In this paper, a fuzzy optimization-based method is developed to solve power system GS problem using fuzzy membership functions (MFs) for objective and some constraints. This fuzzy GS problem is firstly converted to a crisp formulation and then is solved using GAMS software based on mixed integer nonlinear programming (MINLP). This problem is applied to a test system which has pumped storage power plants to modify the uncertainties of wind power output and other parameters in power system. The results of this problem are compared with the results of crisp solution results and in different aspiration values of the total profit. Numerical testing results show that near optimal schedules are obtained, and the method can provide a good balance between increasing profit and satisfying constraints.","2157-4847;2157-4839;2157-4839","978-1-4244-6255-1978-1-4244-6253-7978-1-4244-6254","10.1109/APPEEC.2011.5748596","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5748596","","Wind power generation;Wind farms;Wind forecasting;Maintenance engineering;Reservoirs;Power systems;Optimal scheduling","fuzzy set theory;integer programming;nonlinear programming;power generation scheduling;pumped-storage power stations;wind power plants","generation scheduling;wind farms;load-generation balance;wind power generation constraints;power system scheduling;fuzzy optimization-based method;fuzzy membership functions;mixed integer nonlinear programming;pumped storage power plants","","","8","","","","","","IEEE","IEEE Conferences"
"Appropriate placement of series compensators to improve transient stability of power system","A. Nasri; M. Ghandhari; R. Eriksson","Department of Electric Power Systems, School of Electrical Engineering at KTH Royal Institute of Technology, Sweden; Department of Electric Power Systems, School of Electrical Engineering at KTH Royal Institute of Technology, Sweden; Department of Electric Power Systems, School of Electrical Engineering at KTH Royal Institute of Technology, Sweden","IEEE PES Innovative Smart Grid Technologies","","2012","","","1","6","Trajectory sensitivity analysis is used to find the best places for installation of thyristor controlled series capacitors (TCSC) to improve transient stability of the power system. Based on the rotor angles of generators, an equivalent angle (δ<sub>eq</sub>) is defined by determining accelerating and decelerating machines, and then using trajectory sensitivities of this angle with respect to the impedances of the transmission lines in the post-fault system, appropriate locations for placing TCSC will be found. Severity of the faults is also considered in this calculation. This method is applied to the IEEE 3-machine 9-bus test system to find the priorities of the transmission lines for installation of TCSC. Simulation with industrial software verifies the obtained results.","2378-8534;2378-8542","978-1-4673-1220-2978-1-4673-1221-9978-1-4673-1219","10.1109/ISGT-Asia.2012.6303243","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6303243","Trajectory sensitivity analysis (TSA);Thyristor Controlled Series Capacitor (TCSC);Transient Stability;Critical Clearing Time (CCT)","Power system stability;Trajectory;Circuit faults;Transient analysis;Sensitivity;Power transmission lines","electric generators;IEEE standards;power system transient stability;power transmission lines;rotors;thyristor applications","series compensators;power system improve transient stability;thyristor controlled series capacitors;TCSC;generator rotor angles;decelerating machines;accelerating machines;transmission lines;IEEE 3-machine 9-bus test system","","2","13","","","","","","IEEE","IEEE Conferences"
"A Remote Sensing Approach for Landslide Hazard Assessment on Engineered Slopes","P. E. Miller; J. P. Mills; S. L. Barr; S. J. Birkinshaw; A. J. Hardy; G. Parkin; S. J. Hall","School of Civil Engineering and Geosciences, Newcastle University, Newcastle upon Tyne, U.K.; School of Civil Engineering and Geosciences, Newcastle University, Newcastle upon Tyne, U.K.; School of Civil Engineering and Geosciences, Newcastle University, Newcastle upon Tyne, U.K.; School of Civil Engineering and Geosciences, Newcastle University, Newcastle upon Tyne, U.K.; Institute of Geography and Earth Sciences, Aberystwyth University, Aberystwyth, U.K.; School of Civil Engineering and Geosciences, Newcastle University, Newcastle upon Tyne, U.K.; Network Rail (LNE), York, U.K.","IEEE Transactions on Geoscience and Remote Sensing","","2012","50","4","1048","1056","Earthworks such as embankments and cuttings are integral to road and rail networks but can be prone to instability, necessitating rigorous and continual monitoring. To date, the potential of remote sensing for earthwork hazard assessment has been largely overlooked. However, techniques such as airborne laser scanning (ALS) are now ripe for addressing these challenges. This research presents the development of a novel hazard assessment strategy, combining high-resolution remote sensing with a numerical modeling approach. The research was implemented at a railway test site located in northern England, U.K.; ALS data and multispectral aerial imagery facilitated the determination of key slope stability variables, which were then used to parameterize a coupled hydrological-geotechnical model, in order to simulate slope behavior under current and future climates. A software toolset was developed to integrate the core elements of the methodology and determine resultant slope failure hazard which could then be mapped and queried within a geographical information system environment. Results indicate that the earthworks are largely stable, which is in broad agreement with the management company's slope hazard grading data, and in terms of morphological analysis, the remote methodology was able to correctly identify 99% of earthworks classed as embankments and 100% of cuttings. The developed approach provides an effective and practicable method for remotely quantifying slope failure hazard at fine spatial scales (0.5 m) and for prioritizing and reducing on-site inspection.","0196-2892;1558-0644","","10.1109/TGRS.2011.2165547","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032742","Airborne laser scanning (ALS);hazard mapping;railway engineering;remote sensing","Hazards;Meteorology;Remote sensing;Stability analysis;Rail transportation;Numerical stability;Vegetation mapping","geographic information systems;geomorphology;geophysical techniques;geotechnical engineering;hazardous areas;numerical analysis;railways;remote sensing;roads","remote sensing approach;landslide hazard assessment;engineered slopes;earthworks;embankments;cuttings;road networks;rail networks;airborne laser scanning;numerical modeling approach;northern England;U.K;ALS data;multispectral aerial imagery;coupled hydrological-geotechnical model;slope behavior;software toolset;slope failure hazard;geographical information system;morphological analysis","","11","38","","","","","","IEEE","IEEE Journals & Magazines"
"A low-cost distributed instrumentation system for monitoring, identifying and diagnosing irregular patterns of behavior in critical ITS components","E. Vorakitolan; N. Mould; J. P. Havlicek; R. D. Barnes; A. R. Stevenson","University of Oklahoma School of Electrical and Computer Engineering, 110 W. Boyd Street, Devon Energy Hall 150, Norman, OK, USA 73019-1102; University of Oklahoma School of Electrical and Computer Engineering, 110 W. Boyd Street, Devon Energy Hall 150, Norman, OK, USA 73019-1102; University of Oklahoma School of Electrical and Computer Engineering, 110 W. Boyd Street, Devon Energy Hall 150, Norman, OK, USA 73019-1102; University of Oklahoma School of Electrical and Computer Engineering, 110 W. Boyd Street, Devon Energy Hall 150, Norman, OK, USA 73019-1102; Oklahoma Department of Transportation, Technology Services Division, Oklahoma City, OK, USA 73105","2011 14th International IEEE Conference on Intelligent Transportation Systems (ITSC)","","2011","","","2045","2050","ITS telecommunication infrastructure and information gathering/distribution equipment such as fiber-optic routing devices, communication huts, uninterrupted power supplies (UPSs), cameras and dynamic message signs (DMSs) are critical to the effective management of resources in emergency situations. Frequently encountered scenarios include evaluating the severity of vehicle collisions to dispatch appropriate law enforcement and ambulatory services, or issuing time-sensitive AMBER alerts to assist in the effort to find missing or kidnapped children. Due to the unacceptably high cost of ITS equipment failures, preventative maintenance and dense operational testing are high priorities. In this paper we present a low-cost distributed instrumentation system (DIS) for continuous monitoring of critical ITS components. Over the last three years, we have developed and deployed the DIS in the Oklahoma Department of Transportation (ODOT) private fiberoptic network that spans several major metropolitan areas in and around Oklahoma City, Tulsa and Lawton. The Oklahoma DIS is responsible for monitoring the health of the entire ODOT ITS communications network as well as the integrity of each camera video signal and the operational status of each DMS. All of the information acquired by the DIS is integrated into an operational summary that is available on a private website for the design and execution of ITS equipment maintenance plans. In Oklahoma, information acquired by the DIS has been successfully integrated into a wide range of operation and maintenance (O&M) planning, which has led to a significant improvement in terms of overall ITS quality of service (QoS) and a quantifiable reduction in wasted costs associated with the premature discarding of energy storage devices.","2153-0017;2153-0009;2153-0009","978-1-4577-2197-7978-1-4577-2198-4978-1-4577-2196","10.1109/ITSC.2011.6082868","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6082868","","Instruments;Monitoring;Maintenance engineering;Batteries;Temperature sensors;Software;Resource management","automated highways;computer network management;computerised monitoring;maintenance engineering;optical fibre networks;telecommunication equipment;telecommunication network planning","low cost distributed instrumentation system;irregular pattern monitoring;irregular pattern identification;irregular pattern diagnostics;critical ITS components;ITS telecommunication infrastructure;information gathering equipment;information distribution equipment;emergency situation;Oklahoma Department of Transportation;private fiber optic network;metropolitan area;operation planning;maintenance planning;ITS quality of service;QoS;intelligent transportation system","","","16","","","","","","IEEE","IEEE Conferences"
"Resilient system design for Prognosis and Health Monitoring of an ocean power generator","A. Marcus; I. Cardei; T. Tavtilov; G. Alsenas","Department of Computer and Electrical Engineering and Computer Science, Florida Atlantic University, Boca Raton, Florida, United States of America; Department of Computer and Electrical Engineering and Computer Science, Florida Atlantic University, Boca Raton, Florida, United States of America; Department of Computer and Electrical Engineering and Computer Science, Florida Atlantic University, Boca Raton, Florida, United States of America; Southeast National Marine Renewable Energy Center, Florida Atlantic University, Boca Raton, Florida, United States of America","2012 IEEE International Systems Conference SysCon 2012","","2012","","","1","8","In this paper we introduce a new methodology that integrates system resilience engineering and hazard analysis into complex system design. We then demonstrate its performance by applying it to the design of a Prognosis and Health Monitoring (PHM) system for an ocean current power generator. Three common methodologies for system hazard analysis were tested by applying them to the PHM system's network topology architecture; STAMP-based Process Analysis (STPA), Hazard and Operability Analysis (HAZOP), and a Resilience Engineering, Heuristic-based approach. While all three approaches adequately revealed most PHM system hazards, which assisted in identifying the means with which to mitigate them, none of the approaches fully addressed the multi-state dimensionality of the sub-components of the system, missing risky and hazardous scenarios. We developed the System Hazard Indication and Extraction Learning Diagnosis (SHIELD) methodology for system hazard analysis and resilient design. SHIELD integrates state space analysis into the hazard analysis process in order to facilitate the location of undiscovered hazard scenarios. Our approach uses recursive, top-down system decomposition with subsystem, interface, and process cycle identification. Then, a bottom-up recursive evaluation is completed where we analyze the subsystem state space and state transitions with regard to hazards/failures in process cycles. This yields a comprehensive list of failure states and scenarios. Finally, a top-down prioritized application of resilient engineering heuristics which address hazard scenarios is prescribed. This final phase results in a comprehensive, complete analysis of complex system architectures forcing resilience into the final system design.","","978-1-4673-0750-5978-1-4673-0748-2978-1-4673-0749","10.1109/SysCon.2012.6189490","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6189490","risk analysis;hazard analysis;system resilience engineering","Hazards;Prognostics and health management;Resilience;Software;Turbines;Monitoring","computerised monitoring;condition monitoring;hazards;hydroelectric power stations;ocean waves;wave power generation","resilient system design;complex system design;system resilience engineering;ocean current power generator;PHM;system network topology architecture;STAMP-based process analysis;hazard and operability analysis;HAZOP;heuristic-based approach;system hazard indication and extraction learning diagnosis;system hazard analysis;SHIELD;state space analysis;top-down system;process cycle identification;bottom-up recursive evaluation;state transition;prognosis and health monitoring","","2","20","","","","","","IEEE","IEEE Conferences"
"Identifying faults in large-scale distributed systems by filtering noisy error logs","X. Rao; H. Wang; D. Shi; Z. Chen; H. Cai; Q. Zhou; T. Sun","National Laboratory for Parallel and Distributed Processing, National University of Defense Technology, Changsha, P.R. China 410073; National Laboratory for Parallel and Distributed Processing, National University of Defense Technology, Changsha, P.R. China 410073; National Laboratory for Parallel and Distributed Processing, National University of Defense Technology, Changsha, P.R. China 410073; National Laboratory for Parallel and Distributed Processing, National University of Defense Technology, Changsha, P.R. China 410073; Computing Platform, Alibaba Cloud Computing Corporation, Hangzhou, P.R. China; Computing Platform, Alibaba Cloud Computing Corporation, Hangzhou, P.R. China; Computing Platform, Alibaba Cloud Computing Corporation, Hangzhou, P.R. China","2011 IEEE/IFIP 41st International Conference on Dependable Systems and Networks Workshops (DSN-W)","","2011","","","140","145","Extracting fault features with the error logs of fault injection tests has been widely studied in the area of large scale distributed systems for decades. However, the process of extracting features is severely affected by a large amount of noisy logs. While the existing work tries to solve the problem by compressing logs in temporal and spatial views or removing the semantic redundancy between logs, they fail to consider the co-existence of other noisy faults that generate error logs instead of injected faults, for example, random hardware faults, unexpected bugs of softwares, system configuration faults or the error rank of a log severity. During a fault feature extraction process, those noisy faults generate error logs that are not related to a target fault, and will strongly mislead the resulted fault features. We call an error log that is not related to a target fault a noisy error log. To filter out noisy error logs, we present a similarity-based error log filtering method SBF, which consists of three integrated steps: (1) model error logs into time series and use haar wavelet transform to get the approximate time series; (2) divide the approximate time series into sub time series by valleys; (3) identify noisy error logs by comparing the similarity between the sub time series of target error logs and the template of noisy error logs. We apply our log filtering method in an enterprise cloud system and show its effectiveness. Compared with the existing work, we successfully filter out noisy error logs and increase the precision and the recall rate of fault feature extraction.","2325-6648;2325-6664","978-1-4577-0375-1978-1-4577-0374-4978-1-4577-0373","10.1109/DSNW.2011.5958800","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5958800","error log;event filtering;fault injection;large scale distributed system","Time series analysis;Noise measurement;Feature extraction;Wavelet transforms;Computer crashes;Complexity theory;Approximation methods","cloud computing;data compression;distributed processing;fault tolerant computing;feature extraction;system monitoring;time series;wavelet transforms","fault identification;noisy error log filtering;fault injection tests;large scale distributed systems;data compression;semantic redundancy;fault feature extraction;time series;wavelet transform;enterprise cloud system","","4","20","","","","","","IEEE","IEEE Conferences"
"Ground truth and evaluation for latent fingerprint matching","A. Mikaelyan; J. Bigun","Halmstad University SE-30118 Halmstad; Halmstad University SE-30118 Halmstad","2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops","","2012","","","83","88","In forensic fingerprint studies annotated databases is important for evaluating the performance of matchers as well as for educating fingerprint experts. We have established ground truths of minutia level correspondences for the publicly available NIST SD27 data set, whose minutia have been extracted by forensic fingerprint experts. We performed verification tests with two publicly available minutia matchers, Bozorth3 and k-plet, yielding Equal Error Rates of 36% and 40% respectively, suggesting that they have similar (poor) ability to separate a client from an impostor in latent versus tenprint queries. However, in an identification scenario, we found performance advantage of k-plet over Bozorth3, suggesting that the former can rank the similarities of fingerprints better. Regardless of the matcher, the general poor performance is a confirmation of previous findings related to latent vs tenprint matching. A finding influencing future practice is that the minutia level matching errors in terms of FA and FR may not be balanced (not equally good) even if FA and FR have been chosen to be so at finger level.","2160-7508;2160-7508;2160-7516","978-1-4673-1612-5978-1-4673-1611-8978-1-4673-1610","10.1109/CVPRW.2012.6239220","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6239220","","NIST;Databases;Humans;Forensics;Engines;Software;Fingerprint recognition","fingerprint identification;image matching","ground truth;latent fingerprint matching;forensic fingerprint studies annotated databases;minutia level correspondences;NIST SD27 data set;Bozorth3;k-plet;tenprint queries;minutia level matching errors","","7","7","","","","","","IEEE","IEEE Conferences"
"Detailed reconstruction of 3D plant root shape","Ying Zheng; S. Gu; H. Edelsbrunner; C. Tomasi; P. Benfey","Dept. of Computer Science, Duke Univ., USA; Dept. of Computer Science, Duke Univ., USA; IST Austria; Dept. of Computer Science, Duke Univ., USA; Dept. of Biology, Duke Univ., USA","2011 International Conference on Computer Vision","","2011","","","2026","2033","We study the 3D reconstruction of plant roots from multiple 2D images. To meet the challenge caused by the delicate nature of thin branches, we make three innovations to cope with the sensitivity to image quality and calibration. First, we model the background as a harmonic function to improve the segmentation of the root in each 2D image. Second, we develop the concept of the regularized visual hull which reduces the effect of jittering and refraction by ensuring consistency with one 2D image. Third, we guarantee connectedness through adjustments to the 3D reconstruction that minimize global error. Our software is part of a biological phenotype/genotype study of agricultural root systems. It has been tested on more than 40 plant roots and results are promising in terms of reconstruction quality and efficiency.","2380-7504;1550-5499;1550-5499","978-1-4577-1102-2978-1-4577-1101-5978-1-4577-1100","10.1109/ICCV.2011.6126475","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6126475","","Visualization;Three dimensional displays;Image reconstruction;Shape;Harmonic analysis;Optimization;Electronic mail","biology computing;calibration;image reconstruction","detailed reconstruction;3D plant root shape;3D reconstruction;2D images;image quality;calibration;harmonic function;root segmentation;regularized visual hull;jittering;refraction;global error;biological phenotype study;biological genotype study;agricultural root systems;reconstruction quality;reconstruction efficiency","","12","24","","","","","","IEEE","IEEE Conferences"
"Poster: The Magic Determination of the Magic Constants by ttgLib Autotuner","M. Pritula; M. Krivov; S. Grizan; P. Ivanov","NA; NA; NA; NA","2012 SC Companion: High Performance Computing, Networking Storage and Analysis","","2012","","","1510","1510","When the program is being optimized for execution on GPU, one has to introduce a lot of performance affected constants that define blocks parameters, data chunks size, parallelism granularity, etc. And the more software is optimized, the more magic constants it introduces. Furthermore, adding multi-GPU system support often requires usage of smart load balancing strategies that considers GPU-specific effects such as potential speed-up from ignoring some accelerators, time volatility of GPU-performance and others. As a result, performance of the target software can be significantly increased just by tuning to the hardware and processing data being used. The authors developed a means of determining the optimal values of these constants called ttgLib autotuner which is capable of monitoring the software at runtime and automatically tuning magic constants as well as performing dynamic load balancing between CPU and multiple GPUs. The performed tests showed the additional speedup upto 50-80% by tuning alone.","","978-0-7695-4956-9978-1-4673-6218","10.1109/SC.Companion.2012.293","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6496076","autotuning;GPU","","","","","","","","","","","","IEEE","IEEE Conferences"
"Triggers, data flow and the synchronization between the Auger surface detector and the AMIGA underground muon counters","Z. Szadkowski","University of Łódź, Department of Physics and Applied Informatics, 90-236, Poland","2010 17th IEEE-NPSS Real Time Conference","","2010","","","1","8","The aim of the AMIGA project (Auger Muons and Infill for the Ground Array) is an investigation of Extensive Air Showers at energies lower than by standard Auger array, where the transition from galactic to extragalactic sources is expected. The Auger array is enlarged by a relatively small dedicated area of surface detectors with nearby buried underground muon counters at half or less the standard 1.5 km grid. Lowering the Auger energy threshold by more than one order of magnitude allows a precise measurement of the cosmic ray spectrum in the very interesting regions of the second knee and the ankle. The paper describes the working principle of the Master/Slave (standard Auger surface detector/the underground muon counters) synchronous data acquisition, general triggering and the extraction of data corresponding to the real events from underground storage buffers applied in two prototypes: A) with 12.5 ns resolution (80 MHz) built from 4 segments: standard Auger Front End Board (FEB) and Surface Single Board Computer (SSBC) (on the surface) and the Digital Board with the FPGA and the Microcontroller Board (underground), B) with 4-times higher: 3.125 ns resolution (320 MHz) built with two segments only: new surface Front End Board supported by the NIOS<sup>®</sup>processor and CycloneIII™ Starter Kit board underground, working also with NIOS<sup>®</sup>virtual processor, which replaces the external TI μC, which in meantime became obsolete. The system with the NIOS<sup>®</sup>processors can remotely modify and update: the AHDL firmware creating the hardware FPGA net structure responsible for the fast DAQ, the internal structure of the NIOS<sup>®</sup>(resources and peripherals) and the NIOS<sup>®</sup>firmware (C code) responsible for software data management. With the standard μC, the μC firmware was fixed and could not be updated remotely. The 80 MHz prototype passed laboratory tests with real scintillators. The 320 MHz prototype (still being optimized) is considered as the ultimate AMIGA design.","","978-1-4244-7110-2978-1-4244-7108-9978-1-4244-7109","10.1109/RTC.2010.5750413","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5750413","","Field programmable gate arrays;Detectors;Radiation detectors;Mesons;Clocks;Random access memory;Synchronization","astronomy computing;cosmic ray energy spectra;cosmic ray muons;cosmic ray showers;data acquisition;field programmable gate arrays;firmware;galactic cosmic rays;muon detection;scintillation counters","extensive air showers;Auger array;extragalactic sources;surface detectors;Auger energy threshold;cosmic ray spectrum;master-slave synchronous data acquisition;general triggering;data extraction;underground storage buffer;Auger front end board;surface single board computer;digital board;microcontroller board;CycloneIII Starter Kit board;NIOS virtual processor;AHDL firmware;hardware FPGA net structure;NIOS firmware;C code;software data management;scintillators;frequency 320 MHz","","2","22","","","","","","IEEE","IEEE Conferences"
"Experimental setup of wide area monitoring using Zigbee IEEE 802.15.4 technology and RF FM technique","A. H. Siddique; B. Barkat; M. Poshtan","Electrical Engineering Department, The Petroleum Institute Abu Dhabi, PI, UAE; Electrical Engineering Department, The Petroleum Institute Abu Dhabi, PI, UAE; Electrical Engineering Department, American University of Dubai, AUD, UAE","World Congress on Sustainable Technologies (WCST-2012)","","2012","","","44","48","It is important for a modern power grid to be smarter in order to provide reliable and sustainable supply of electricity. Traditional way of receiving data from the wired system is a very old and outdated technology. For a quicker and better response from the electric system, it is important to look at wireless systems as a feasible option. In this paper, wireless network based architecture has been adopted for monitoring and optimizing the electric grid. In the proposed system, we have considered the operation station as a smart sensor node and a number of wireless transmitters and receivers have been used in the network. Crossbow wireless network kit with Zigbee technology has been used in our system for wireless data transmission. The system was also tested with RF transmitters and receivers for better understanding of the problem associated with Zigbee and also finding potential ways to solve that problem faced during the lab experiment. LABVIEW software has been used to control and monitor the received data. For using the proposed system for Distributed Generation (DG), a MATLAB based program is proposed.","","978-1-908320-09-4978-1-4673-4442","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6482913","Electric grid;single phase syste;three phase system;wireless smart energy meter;wireless sensor network;Zigbee;neural network","Monitoring;Reliability;Nickel;Connectors;MATLAB;Transmitters;Process control","distributed power generation;frequency modulation;intelligent sensors;power engineering computing;power grids;power system measurement;radio receivers;radio transmitters;wireless sensor networks;Zigbee","wide area monitoring;Zigbee IEEE 802.15.4 technology;RF FM technique;power grid;wireless network based architecture;smart sensor node;wireless transmitters;wireless receivers;crossbow wireless network kit;wireless data transmission;LABVIEW software;distributed generation;MATLAB based program","","","13","","","","","","IEEE","IEEE Conferences"
"Power Amplifier Behavioral Modeling by Neural Networks and Their Implementation on FPGA","R. S. N. Ntoun&#233;; M. Bahoura; C. Park","NA; NA; NA","2012 IEEE Vehicular Technology Conference (VTC Fall)","","2012","","","1","5","In this paper, field programmable gate array (FPGA) implementation of two power amplifier (PA) dynamic behavioral modeling approaches with real-valued time-delay neural network (RVTDNN) and real-valued recurrent neural network (RVRNN) architectures are presented. The proposed PA models are based on the multilayer perceptron (MLP) neural networks with delayed inputs to take into account nonlinearity and memory effects of the PA. The synoptic weights of these neural networks are dynamically updated in order to prevent any eventual change in the PA's characteristics. Both architectures have been optimized to include only six hidden neurons and implemented on FPGA using Xilinx system generator. The FPGA is preferred to the digital signal processor (DSP) because it allows parallel computation tasks and software like flexibility. The modeling performances of these architectures are compared using 16-QAM modulated test signal. The mean square error (MSE) between the desired and the actual outputs of these models are also compared.","1090-3038;1090-3038","978-1-4673-1881-5978-1-4673-1880-8978-1-4673-1879","10.1109/VTCFall.2012.6399346","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6399346","","Neurons;Field programmable gate arrays;Biological neural networks;Computer architecture;Integrated circuit modeling;Computational modeling;Digital signal processing","digital signal processing chips;field programmable gate arrays;mean square error methods;multilayer perceptrons;neural net architecture;power amplifiers;quadrature amplitude modulation;recurrent neural nets","power amplifier behavioral modeling;FPGA;field programmable gate array;power amplifier dynamic behavioral modeling;real-valued time-delay neural network architecture;real-valued recurrent neural network architecture;multilayer perceptron neural network;Xilinx system generator;digital signal processor;DSP;parallel computation task;modeling performance;16-QAM modulated test signal;mean square error;MSE","","2","12","","","","","","IEEE","IEEE Conferences"
"S-preconditioner for Multi-fold Data Reduction with Guaranteed User-Controlled Accuracy","Y. Jin; S. Lakshminarasimhan; N. Shah; Z. Gong; C. S. Chang; J. Chen; S. Ethier; H. Kolla; S. Ku; S. Klasky; R. Latham; R. Ross; K. Schuchardt; N. F. Samatova","NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA","2011 IEEE 11th International Conference on Data Mining","","2011","","","290","299","The growing gap between the massive amounts of data generated by petascale scientific simulation codes and the capability of system hardware and software to effectively analyze this data necessitates data reduction. Yet, the increasing data complexity challenges most, if not all, of the existing data compression methods. In fact, lossless compression techniques offer no more than 10% reduction on scientific data that we have experience with, which is widely regarded as effectively incompressible. To bridge this gap, in this paper, we advocate a transformative strategy that enables fast, accurate, and multi-fold reduction of double-precision floating-point scientific data. The intuition behind our method is inspired by an effective use of preconditioners for linear algebra solvers optimized for a particular class of computational ""dwarfs"" (e.g., dense or sparse matrices). Focusing on a commonly used multi-resolution wavelet compression technique as the underlying ""solver"" for data reduction we propose the S-preconditioner, which transforms scientific data into a form with high global regularity to ensure a significant decrease in the number of wavelet coefficients stored for a segment of data. Combined with the subsequent EQ-calibrator, our resultant method (called S-Preconditioned EQ-Calibrated Wavelets (SPEQC-Wavelets)), robustly achieved a 4- to 5-fold data reduction-while guaranteeing user-defined accuracy of reconstructed data to be within 1% point-by-point relative error, lower than 0.01 Normalized RMSE, and higher than 0.99 Pearson Correlation. In this paper, we show the results we obtained by testing our method on six petascale simulation codes including fusion, combustion, climate, astrophysics, and subsurface groundwater in addition to 13 publicly available scientific datasets. We also demonstrate that application-driven data mining tasks performed on decompressed variables or their derived quantities produce results of comparable quality with the ones for the original data.","2374-8486;1550-4786;1550-4786","978-1-4577-2075-8978-0-7695-4408","10.1109/ICDM.2011.138","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6137233","preconditioners for data mining;data reduction;data mining over decompressed data;in situ data analytics;extreme-scale data analytics","Data models;Vectors;Wavelet transforms;Accuracy;Indexes;Correlation;Sparse matrices","data analysis;data mining;data reduction;mean square error methods;wavelet transforms","S-preconditioner;multifold data reduction;user-controlled accuracy;petascale scientific simulation codes;system hardware capability;system software capability;data analysis;data complexity;lossless data compression techniques;transformative strategy;double-precision floating-point scientific data;linear algebra solvers;multiresolution wavelet compression technique;wavelet coefficients;data reconstruction;Pearson correlation;application driven data mining","","","26","","","","","","IEEE","IEEE Conferences"
"New fault zone identification scheme for busbar using support vector machine","N. G. Chothani; B. R. Bhalja; U. B. Parikh","Department of Electrical Engineering, A.D. Patel Institute of Technology, New Vallabh Vidhyanagar 388121, India; Department of Electrical Engineering, A.D. Patel Institute of Technology, New Vallabh Vidhyanagar 388121, India; Corporate R&D center, ABB Ltd., Vadodara, India","IET Generation, Transmission & Distribution","","2011","5","10","1073","1079","This study presents a new support vector machine (SVM)-based fault zone identification scheme for busbar which correctly identifies faults occurring inside and outside the protection zone of busbar. The proposed scheme utilises one cycle post-fault current signals of all the lines as an input to SVM. In order to achieve the most optimised classifier, Gaussian radial basis function has been used for training of SVM. Feasibility of the proposed scheme has been tested by modelling an existing 400 kV Indian busbar system in PSCAD/EMTDC software package. More than 28 800 fault cases with varying fault resistances, fault inception angles, fault locations, types of faults and source impedances have been generated and used for validation of the proposed scheme. The proposed scheme effectively discriminates between in-zone and out-of-zone faults with very high fault classification accuracy for different fault and system conditions. Moreover, the proposed scheme remains stable during an early and severe current transformer (CT) saturation condition giving an accuracy of 99 for all the fault cases.","1751-8687;1751-8695","","10.1049/iet-gtd.2010.0462","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6036116","","","busbars;current transformers;fault location;Gaussian processes;power engineering computing;radial basis function networks;support vector machines","fault zone identification;busbar;support vector machine;SVM;Gaussian radial basis function;PSCAD/EMTDC software package;varying fault resistances;fault inception angles;fault locations;current transformer saturation","","5","","","","","","","IET","IET Journals & Magazines"
"Online Implementation of Local Linear Neuro-fuzzy Technique on Partially Simulated Reaction Process","B. Jamali; M. A. Ghayyem; H. Jazayeri-Rad; M. Shahbazian","NA; NA; NA; NA","2011 UkSim 13th International Conference on Computer Modelling and Simulation","","2011","","","145","150","This study presents a methodology for on-line identification of the nonlinear reaction process in presence of measurement noise and uncertainty, for accurate simulation of this process, a link between HYSYS (chemical software) and MATLAB was generated by the author. In this link HYSYS simulates the continuous stirred tank reactor (CSTR) and MATLAB performs the data acquisition algorithm. The chemical process used to illustrate this study is the production process of propylene glycol. The local linear models tree (LOLIMOT) identification algorithm has been introduced. The nonlinear model parameters are determined by an incremental tree construction algorithm in a heuristic manner in order to avoid the application of nonlinear optimization techniques. The results show that the LOLIMOT gives best fitting on the train and test data. On the other hand, the limited flexibility of the local estimation reduces the variance error due to the bias/variance dilemma.","","978-1-61284-705-4978-0-7695-4376","10.1109/UKSIM.2011.36","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5754225","CSTR;LOLIMOT;Noisy measurment;Uncertainty;Dynamic link;Adaptive neuro-fuzzy","Estimation;Training;Data models;Inductors;Partitioning algorithms;Biological system modeling","chemical engineering computing;chemical reactors;data acquisition;fuzzy neural nets;organic compounds;production engineering computing;trees (mathematics)","local linear neuro-fuzzy technique;partially simulated reaction process;online identification;nonlinear reaction process;measurement noise;measurement uncertainty;HYSYS;chemical software;MATLAB;continuous stirred tank reactor;data acquisition;chemical process;propylene glycol production process;local linear models tree;LOLIMOT identification algorithm;nonlinear model parameters;incremental tree construction algorithm;local estimation;variance error","","","7","","","","","","IEEE","IEEE Conferences"
"Baseband fading channel simulator for Inter-Vehicle Communication using SystemC-AMS","A. Massouri; A. Lévêque; L. Clavier; M. Vasilevski; A. Kaiser; M. Louërat","IEMN, Institut d'Electronique, de Microlectronique et de Nanotechnologie, France; LIP6, University Pierre & Marie, Curie, France; IEMN, Institut d'Electronique, de Microlectronique et de Nanotechnologie, France; LIP6, University Pierre & Marie, Curie, France; IEMN, Institut d'Electronique, de Microlectronique et de Nanotechnologie, France; LIP6, University Pierre & Marie, Curie, France","2010 IEEE International Behavioral Modeling and Simulation Workshop","","2010","","","36","41","System level modeling and simulation have become a key issue in analyzing, optimizing and designing wireless systems. In this paper, modeling RF front end devices and radio propagation channel especially fading time-variant channel for Inter-Vehicle Communication using SystemC-AMS are investigated. Discrete baseband wireless channel model based on Tapped Delay Line model is presented. Time-variant Rayleigh and Rician fading are implemented in SystemC-AMS. The capabilities of SystemC-AMS are tested and compared to matlab for five vehicles scenario. With this framework, several criteria such as performance and optimal value of the system parameters of a WSN node can be evaluated and predicted before any hardware and software design.","2160-3812;2160-3804","978-1-4244-8997-8978-1-4244-8996","10.1109/BMAS.2010.6156595","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6156595","","Fading;Wireless sensor networks;Wireless communication;Computational modeling;Doppler effect;Solid modeling;Radio frequency","delays;fading channels;radiowave propagation;time-varying channels;wireless sensor networks","baseband fading channel simulator;inter-vehicle communication;SystemC-AMS;system level modeling;system level simulation;RF front end devices;radio propagation channel;fading time-variant channel;discrete baseband wireless channel model;tapped delay line model;time-variant Rayleigh fading;Rician fading;WSN node;software design;hardware design","","1","24","","","","","","IEEE","IEEE Conferences"
"Development of a low cost sun sensor using quadphotodiode","I. Maqsood; T. Akram","Masdar Institute of Science and Technology, Abu-Dhabi 54224, United Arab Emirates; Technical University of Hamburg-Harburg, 21073, Germany","IEEE/ION Position, Location and Navigation Symposium","","2010","","","639","644","This paper describes about the design and hardware implementation of a low cost model of a two axis sun sensor using a quad photo diode, which can determine the azimuth and elevation angles of Sun in sensor's body frame of reference. The sensor uses quad photodiode to obtain angular information of Sun. It weighs less than 100g (which can be further reduced by optimizing the housing design). The relevant software requires 50K bytes of memory and little processing. We have also developed sensor calibration test bed (SCTB) which will also be the part of this paper. SCTB is used to calibrate the developed Sun Senor; during calibration the whole surface of sun sensor (quad photodiode) is scanned for a field of view (FOV) equal to 60 × 60. Step size during calibration is set to one degree so we get elevation and azimuth matrices each having 3721 values. The calibration process is fully automated with the help of algorithms written in Matlab. The step size used in calibration is adjustable and we can calibrate the sensor even less than one degree using this SCTB. Azimuth and elevation matrices generated during calibration are used as error correcting tables during real-time measurements taken by the sun sensor. Sun sensor is calibrated in front of Sun simulator made by the Optical Energy Technologies USA.","2153-3598;2153-358X;2153-358X","978-1-4244-5036-7978-1-4244-5037","10.1109/PLANS.2010.5507186","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5507186","quad photodiode;calibration;Sun sensor;position sensors;Sun Tracker","Costs;Sun;Calibration;Azimuth;Sensor phenomena and characterization;Photodiodes;Optical sensors;Hardware;Mathematical model;Diodes","calibration;optical sensors;photodiodes","sun sensor;quadphotodiode;hardware implementation;sensor calibration test bed;field of view;azimuth matrices;Matlab;Optical Energy Technologies USA","","4","6","","","","","","IEEE","IEEE Conferences"
"A two stage approach for a cost-effective and versatile debugging unit and starter platform","R. Beneder; M. Kramer; P. Brejcha","Institute of Embedded Systems, UAS Technikum Wien, Vienna Austria; Institute of Embedded Systems, UAS Technikum Wien, Vienna Austria; Institute of Embedded Systems, UAS Technikum Wien, Vienna Austria","Proceedings of 2012 IEEE/ASME 8th IEEE/ASME International Conference on Mechatronic and Embedded Systems and Applications","","2012","","","330","335","In courses dedicated to embedded systems software students train their skills and expertise using embedded hardware and the required development tools. In order to gain sustainable knowledge in embedded systems engineering it would be an asset, if the development tools and the hardware are available 24/7 to every student. This implies that the embedded platform and the desired development equipment - e.g. integrated development environment (IDE) and its licenses as well as the debug interface, etc. - have to be supplied by the university. One component in the development toolchain - the debug interface unit - is the target of optimization described in this paper. By not only trying to reduce the costs but also increasing the versatility, the effectiveness of the toolchain is improved. The implementation of such a debugger, the performance tests and the composition of the recommended development tools are described in this paper.","","978-1-4673-2349-9978-1-4673-2347-5978-1-4673-2348","10.1109/MESA.2012.6275584","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6275584","embedded systems;education;OPES;IDE;debug interface","Microcontrollers;Embedded systems;Microprogramming;Universal Serial Bus;Computers;Clocks;Educational institutions","computer debugging;computer science education;educational courses;educational institutions;embedded systems;peripheral interfaces;training","two-stage approach;cost-effective debugging unit;embedded system software students;student skills;educational courses;embedded hardware;embedded systems engineering;universities;debug interface unit;development toolchain;versatile debugging unit","","","5","","","","","","IEEE","IEEE Conferences"
"Fuzzy logic control applied on SI engine concerning the injection time evolution","D. Marin; I. Hiticas; L. Mihon","Hautes Etudes d'Ing&#x00E9;nieur/Lille/France; University Politehnica of Timisoara/M.M.U.T, Romania; University Politehnica of Timisoara/M.M.U.T, Romania","2012 IEEE 13th International Symposium on Computational Intelligence and Informatics (CINTI)","","2012","","","279","284","The papers present studies and researches of fuzzy logic system application, analyzing the evolution of a spark ignition (SI) engine. The advantage offered by the fuzzy logic system allowed us to monitor same of the parameters involved in burning process. As we know, thermal engines must be optimized because of the problems with exhaust gases from fossil fuel, used as energy sources. Our team succeeded to work with MATLAB/Simulink software, which takes into consideration data acquired from the tested vehicle, namely Dacia Logan, with purpose to manage the injection time. The amount of air and the amount of fuel are two elements which must be taken into consideration, because if the amount of fuel varies, than and the amount of air will also varies, with consequences in exhaust. Also we mention and about the AFR (Air/Fuel Ratio), which is a value with variation due to the lambda sensor modification. Lambda sensor also was a parameter control with fuzzy logics controller. The results are very interesting, fuzzy control system allowed us to manage most of the important parameters, and the relation between them help us to affirm important conclusion.","","978-1-4673-5206-2978-1-4673-5205-5978-1-4673-5210","10.1109/CINTI.2012.6496774","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6496774","","","automotive engineering;combustion;fuel systems;fuzzy control;gas sensors;internal combustion engines","fuzzy logic control;SI engine;injection time evolution;fuzzy logic system;spark ignition engine;burning process parameter monitoring;thermal engines;exhaust gases;fossil fuel;energy sources;MATLAB-Simulink software;Dacia Logan;air amount;fuel amount;AFR;air-fuel ratio;lambda sensor modification;parameter control","","","22","","","","","","IEEE","IEEE Conferences"
"Urban traffic signal control using reinforcement learning agents","P. G. Balaji; X. German; D. Srinivasan","Department of Electrical and Computer Engineering, 4 Engineering Drive 3, National University of Singapore; Department of Electrical and Computer Engineering, 4 Engineering Drive 3, National University of Singapore; Department of Electrical and Computer Engineering, 4 Engineering Drive 3, National University of Singapore","IET Intelligent Transport Systems","","2010","4","3","177","188","This study presents a distributed multi-agent-based traffic signal control for optimising green timing in an urban arterial road network to reduce the total travel time and delay experienced by vehicles. The proposed multi-agent architecture uses traffic data collected by sensors at each intersection, stored historical traffic patterns and data communicated from agents in adjacent intersections to compute green time for a phase. The parameters like weights, threshold values used in computing the green time is fine tuned by online reinforcement learning with an objective to reduce overall delay. PARAMICS software was used as a platform to simulate 29 signalised intersection at Central Business District of Singapore and test the performance of proposed multi-agent traffic signal control for different traffic scenarios. The proposed multi-agent reinforcement learning (RLA) signal control showed significant improvement in mean time delay and speed in comparison to other traffic control system like hierarchical multi-agent system (HMS), cooperative ensemble (CE) and actuated control.","1751-956X;1751-9578","","10.1049/iet-its.2009.0096","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5558886","","","control engineering computing;delays;learning (artificial intelligence);multi-agent systems;road traffic;sensors;traffic engineering computing","urban traffic signal control;reinforcement learning;distributed multiagent;urban arterial road network;PARAMICS software;mean time delay;speed;cooperative ensemble;actuated control;intersection sensors","","24","","","","","","","IET","IET Journals & Magazines"
"Does reduced radiation dose adversely affect the ability to detect abnormal myocardial perfusion on computed tomography during vasodilator stress?","A. R. Patel; S. Chandra; N. Kachenoura; J. A. Lodato; H. Ahmad; B. H. Freed; B. Newby; R. M. Lang; V. Mor-Avi","University of Chicago, Department of Medicine, Section of Cardiology, Chicago, Illinois, USA; University of Chicago, Department of Medicine, Section of Cardiology, Chicago, Illinois, USA; University of Chicago, Department of Medicine, Section of Cardiology, Chicago, Illinois, USA; University of Chicago, Department of Medicine, Section of Cardiology, Chicago, Illinois, USA; University of Chicago, Department of Medicine, Section of Cardiology, Chicago, Illinois, USA; University of Chicago, Department of Medicine, Section of Cardiology, Chicago, Illinois, USA; University of Chicago, Department of Medicine, Section of Cardiology, Chicago, Illinois, USA; University of Chicago, Department of Medicine, Section of Cardiology, Chicago, Illinois, USA; University of Chicago, Department of Medicine, Section of Cardiology, Chicago, Illinois, USA","2011 Computing in Cardiology","","2011","","","121","124","The ability of multidetector computed tomography (MDCT) to detect stress-induced myocardial perfusion abnormalities is of great clinical interest as a potential tool for the combined evaluation of coronary stenosis and its significance. However, stress testing requires repeated scanning, which is associated with additional radiation exposure and iodine contrast. Our goal was to determine the effects of reduced tube voltage and contrast dose on the ability to detect perfusion abnormalities. We studied 40 patients referred for CT coronary angiography (CTCA). Images were acquired at rest and during regadenoson stress (256-channel scanner, Philips) using 120kV tube voltage with 80-90 ml contrast in 20 patients and 100kV with 55-70 ml contrast in the remaining 20 patients. Custom software was used to define 3D myocardial segments and measure segmental x-ray attenuation. In each group of patients, myocardial attenuation was averaged for segments supplied by arteries with stenosis causing &gt;;50% narrowing on CTCA, and separately for segments supplied by arteries without significant stenosis. In both groups, myocardial attenuation was equally reduced in segments supplied by diseased arteries, despite the 74% reduction in radiation and the 28% reduction in contrast. Regadenoson stress MDCT imaging can detect hypoperfused myocardium even when imaging settings are optimized to provide a significant reduction in radiation and contrast doses.","2325-8853;0276-6574","978-1-4577-0611-0978-1-4577-0612","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6164517","","Myocardium;Attenuation;Stress;Arteries;Three dimensional displays;Computed tomography","angiocardiography;computerised tomography;dosimetry;medical image processing","radiation dose reduction;abnormal myocardial perfusion detection;vasodilator stress;multidetector computed tomography;stress-induced myocardial perfusion abnormality detection;coronary stenosis;stress testing;radiation exposure;iodine contrast;tube voltage reduction;contrast dose reduction;CT coronary angiography;image acquisition;3D myocardial segment;segmental X-ray attenuation measurement;myocardial attenuation;arteries;regadenoson stress MDCT imaging;hypoperfused myocardium detection;radiation reduction;contrast doses reduction;voltage 120 kV;voltage 100 kV","","","8","","","","","","IEEE","IEEE Conferences"
"FPGA based real-time signal processor for Pulse Doppler Radar","F. B. Khalid; R. A. Amjad; M. A. Chohan; M. M. Khizar","Department of Electrical Engineering, University of Engineering &amp; Technology, Lahore, Pakistan; Technical University Munich, Germany; Embedded Systems Division, Mentor Graphics Corporation, Lahore, Pakistan; Department of Electrical Engineering, University of Engineering &amp; Technology, Lahore, Pakistan","2012 International Conference on Informatics, Electronics & Vision (ICIEV)","","2012","","","362","366","With the advancements of VLSI chips and powerful computational devices, new complex algorithms and systems are being devised to tap the available computational power and to get the maximum performance. One particular system is the signal processor of Pulse Doppler Radar. This paper aims at finding the range and radial velocity of a moving target using Pulse Doppler Radar for land surveillance. Different optimization techniques have been employed to increase the accuracy of the system. The design was first tested on MATLAB for software verification and then on Xilinx Virtex II Pro FPGA to verify if the system meets the real-time constraints.","","978-1-4673-1154-0978-1-4673-1153-3978-1-4673-1152","10.1109/ICIEV.2012.6317426","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6317426","Phase coding;Pulse compression;Fast Fourier Transform;Field Programmable Gate Array;Additive White Gaussian Noise;Pulse Repetition Frequency","Doppler radar;Field programmable gate arrays;Clutter;Radar signal processing;Radar cross section","digital signal processing chips;Doppler radar;field programmable gate arrays;VLSI","FPGA based real-time signal processor;pulse doppler radar;VLSI chips;computational devices;complex algorithms;complex systems;computational power;maximum performance;MATLAB;software verification;Xilinx Virtex II Pro FPGA;real-time constraints;land surveillance","","4","10","","","","","","IEEE","IEEE Conferences"
"Design of a distributed kite power control system","U. Fechner; R. Schmehl","Institute for Applied Sustainable Science, Engineering and Technology, Delft University of Technology, The Netherlands; Institute for Applied Sustainable Science, Engineering and Technology, Delft University of Technology, The Netherlands","2012 IEEE International Conference on Control Applications","","2012","","","800","805","Kite power is a promising innovative technology for converting wind energy into electricity at a higher capacity factor and, for many applications, at a lower cost than conventional wind turbines. However, accessing this potential depends substantially on the availability of sophisticated control systems. Delft University of Technology is developing a kite power generator which operates a tethered inflatable membrane wing in a pumping cycle. The flight trajectory is controlled by an actuator unit suspended below the wing and communicates with the ground station control centre via a fast and reliable wireless link. The link is also used to transmit the data of the on-board sensors to the ground. In a future wind park of many kite power systems, the individual kites and ground stations have to communicate among each other, to avoid collisions and to optimize the total energy output of the park. A preparatory analysis has shown that the current prototype would significantly benefit from a distributed control system approach, achieving higher efficiency and increased operational flexibility. For larger installations a distributed control system would be mandatory anyway. For these reasons, a distributed control system with a flexible architecture has been developed. The unique design and first test results are presented.","1085-1992;1085-1992","978-1-4673-4505-7978-1-4673-4503-3978-1-4673-4504","10.1109/CCA.2012.6402695","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6402695","","Software;Sensors;Computers;Winches;Control systems;Wireless sensor networks;Wireless communication","aerospace components;aerospace control;collision avoidance;distributed control;power generation control;radio links;sensors;trajectory control;wind power","distributed kite power control system design;wind energy conversion;electricity;kite power generator;tethered inflatable membrane wing;pumping cycle;flight trajectory control;actuator unit;ground station control centre;wireless link;onboard sensor;wind park;collision avoidance;distributed control system approach;flexible architecture","","8","10","","","","","","IEEE","IEEE Conferences"
"Hidden Markov Model for eye gaze prediction in networked video streaming","Yunlong Feng; Gene Cheung; Wai-tian Tan; Yusheng Ji","The Graduate University for Advanced Studies, Japan; National Institute of Informatics, Japan; Hewlett-Packard Laboratories, USA; National Institute of Informatics, Japan","2011 IEEE International Conference on Multimedia and Expo","","2011","","","1","6","With the advent of eye gaze tracking technology, eye gaze is increasingly being used as a media interaction trigger in a variety of applications, such as eye typing, video content customization, and network video streaming based on region-of-interest (ROI). The reaction time of a gaze-based networked system, however, is in practice lower-bounded by the round trip time (RTT) of today's networks, which can be large. To improve the efficacy of gaze-based networked systems, in the paper we propose a Hidden Markov Model (HMM)-based gaze prediction strategy to predict future gaze locations to lower end-to-end reaction delay. We first design an HMM with three states corresponding to human's three major types of intrinsic eye movements. HMM parameters are obtained offline on a per-video basis during training phase. During testing phase, a window of noisy gaze observations are collected in real-time as input to a forward algorithm, which computes the most likely HMM state. Given the deduced HMM state, linear prediction is used to predict gaze location RTT seconds into the future. We demonstrate the applicability of our gaze prediction strategy by focusing on ROI-based bit allocation for network video streaming. To reduce transmission rate of a video stream without degrading viewer's perceived visual quality, we allocate more bits to encode the viewer's current spatial ROI, while devoting fewer bits in other spatial regions. The challenge lies in overcoming the delay between the time a viewer's ROI is detected by gaze tracking, to the time the effected video is encoded, delivered and displayed at the viewer's terminal. To this end, we use our proposed gaze-prediction strategy to predict future eye gaze locations, so that optimized bit allocation can be performed for future frames. Our experiments show that bit rate can be reduced by 21% without noticeable visual quality degradation when end-to-end network delay is as high as 200ms.","1945-788X;1945-7871;1945-7871","978-1-61284-350-6978-1-61284-348-3978-1-61284-349","10.1109/ICME.2011.6011935","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6011935","Eye-gaze prediction;network streaming","Hidden Markov models;Indexes;Predictive models;Training;Software","","","","6","17","","","","","","IEEE","IEEE Conferences"
"Automatic assessment system for large groups using Information and Communication Technologies","M. C. Rodriguez-Sanchez; A. Torrado-Carvajal; A. Rodriguez-Moreno; M. A. Moreno-Alvarez; S. Borromeo; J. Vaquero; J. A. Hernandez-Tamames","Dpto. de Tecnolog&#x00ED;a Electr&#x00F3;nica, Universidad Rey Juan Carlos, C\Tulip&#x00E1;n, s/n. M&#x00F3;stoles, Madrid, Spain; Dpto. de Tecnolog&#x00ED;a Electr&#x00F3;nica, Universidad Rey Juan Carlos, C\Tulip&#x00E1;n, s/n. M&#x00F3;stoles, Madrid, Spain; Dpto. de Tecnolog&#x00ED;a Electr&#x00F3;nica, Universidad Rey Juan Carlos, C\Tulip&#x00E1;n, s/n. M&#x00F3;stoles, Madrid, Spain; Dpto. de Tecnolog&#x00ED;a Electr&#x00F3;nica, Universidad Rey Juan Carlos, C\Tulip&#x00E1;n, s/n. M&#x00F3;stoles, Madrid, Spain; Dpto. de Tecnolog&#x00ED;a Electr&#x00F3;nica, Universidad Rey Juan Carlos, C\Tulip&#x00E1;n, s/n. M&#x00F3;stoles, Madrid, Spain; Dpto. de Tecnolog&#x00ED;a Electr&#x00F3;nica, Universidad Rey Juan Carlos, C\Tulip&#x00E1;n, s/n. M&#x00F3;stoles, Madrid, Spain; Dpto. de Tecnolog&#x00ED;a Electr&#x00F3;nica, Universidad Rey Juan Carlos, C\Tulip&#x00E1;n, s/n. M&#x00F3;stoles, Madrid, Spain","Proceedings of the 2012 IEEE Global Engineering Education Conference (EDUCON)","","2012","","","1","6","This paper describes an Innovative System that introduces Information and Communication Technologies (ICTs) for new Educational Methods and Learning Mechanisms in Engineering Education. This implementation fits in the new European Space of Higher Education (ESHE) of the regulatory modifications promulgated by the Declaration of Bolognia. A new framework is proposed in order to optimize the continuous evaluation process for large groups. In this sense, ICTs allow continuous and interactive assessment with automatic and real-time results. A novel economically viable system is presented. It has been tested in the Telecommunication Engineering and Environmental Engineering Degrees at Rey Juan Carlos University. We conclude with an evaluation and a discussion of the system from different stakeholder's perspectives.","2165-9567;2165-9559;2165-9559","978-1-4673-1456-5978-1-4673-1457-2978-1-4673-1455","10.1109/EDUCON.2012.6201172","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6201172","Educational technology;computer aided instruction;continuous assessment;wireless communication;mobile computing","Computers;Smart phones;Software;Education;Databases;IEEE 802.11 Standards","computer aided instruction;educational administrative data processing;educational courses;educational institutions;educational technology;engineering computing;environmental engineering;further education;legislation;telecommunication engineering education","automatic assessment system;information and communication technology;innovative system;ICT;new educational methods;learning mechanisms;engineering education;European Space of Higher Education;ESHE;regulatory modifications;Bolognia declaration;continuous evaluation process;continuous assessment;interactive assessment;economically viable system;telecommunication engineering degree;environmental engineering degrees;Rey Juan Carlos University","","","15","","","","","","IEEE","IEEE Conferences"
"Human-guided grasp measures improve grasp robustness on physical robot","R. Balasubramanian; L. Xu; P. D. Brook; J. R. Smith; Y. Matsuoka","University of Washington, USA; Carnegie Mellon University, USA; University of Washington, USA; Intel Labs Seattle, USA; University of Washington, USA","2010 IEEE International Conference on Robotics and Automation","","2010","","","2294","2301","Humans are adept at grasping different objects robustly for different tasks. Robotic grasping has made significant progress, but still has not reached the level of robustness or versatility shown by human grasping. It would be useful to understand what parameters (called grasp measures) humans optimize as they grasp objects, how these grasp measures are varied for different tasks, and whether they can be applied to physical robots to improve their robustness and versatility. This paper demonstrates a new way to gather human-guided grasp measures from a human interacting haptically with a robotic arm and hand. The results revealed that a human-guided strategy provided grasps with higher robustness on a physical robot even under a vigorous shaking test (91%) when compared with a state-of-the-art automated grasp synthesis algorithm (77%). Furthermore, orthogonality of wrist orientation was identified as a key human-guided grasp measure, and using it along with an automated grasp synthesis algorithm improved the automated algorithm's results dramatically (77% to 93%).","1050-4729","978-1-4244-5038-1978-1-4244-5040","10.1109/ROBOT.2010.5509855","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5509855","","Anthropometry;Robustness;Robotics and automation;Humans;Grasping;Service robots;Grippers;Wrist;Robotic assembly;Open source software","robot kinematics","robotic grasp;human-guided grasp;robotic arm;robotic hand;automated grasp synthesis algorithm","","34","32","","","","","","IEEE","IEEE Conferences"
"Strategic bidding in a day-ahead market by coevolutionary genetic algorithms","F. Careri; C. Genesi; P. Marannino; M. Montagna; S. Rossi; I. Siviero","Department of Electrical Engineering of the University of Pavia, 1, via Ferrata, I-27100, Italy; Department of Electrical Engineering of the University of Pavia, 1, via Ferrata, I-27100, Italy; Department of Electrical Engineering of the University of Pavia, 1, via Ferrata, I-27100, Italy; Department of Electrical Engineering of the University of Pavia, 1, via Ferrata, I-27100, Italy; Department of Electrical Engineering of the University of Pavia, 1, via Ferrata, I-27100, Italy; Department of Electrical Engineering of the University of Pavia, 1, via Ferrata, I-27100, Italy","IEEE PES General Meeting","","2010","","","1","8","In the present work, the problem of energy market price clearing and generation company (Genco) strategic bidding is considered in the framework of existing day-ahead markets with system marginal price auction. The situation of imperfect competition arising when one of the Gencos is large enough to exert market power is considered in detail, showing what bidding behaviors are to be expected when such a market arrangement occurs. The impact that inter-area transmission system congestions may have on the mechanism of system pricing is also addressed. The bidding problem faced by each Genco is formulated as a strategic multi-player game in which the choice between different bidding levels and energy amounts to be sold at the market has to be made. The large size of the problem due to the number of competitors and to the presence of transmission constraints makes the application of classical game theory troublesome. Therefore, an agent based method belonging to the category of coevolutionary genetic algorithm was selected for the solution of this problem. Test cases illustrate the different strategies that the Gencos may implement to optimize their performance at the day-ahead market. Beside some small didactical examples, the situation of the Italian day-ahead market is considered in detail.","1932-5517;1944-9925;1944-9925","978-1-4244-6551-4978-1-4244-6549-1978-1-4244-6550-7978-1-4244-8357","10.1109/PES.2010.5590161","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5590161","Day-ahead market;Game Theory;Coevolutionary Genetic Algorithm;Price maker-price taker Genco;Market Power","Generators;Computer aided software engineering;Biological system modeling;Genetics;Games;Mathematical model;Convergence","genetic algorithms;power markets;power system economics;pricing","day ahead market;coevolutionary genetic algorithms;energy market price clearing;generation company strategic bidding;interarea transmission system congestion","","5","10","","","","","","IEEE","IEEE Conferences"
"Parallel AES algorithm for fast Data Encryption on GPU","Deguang Le; Jinyi Chang; Xingdou Gou; Ankang Zhang; Conglan Lu","School of Computer Science & Engineering, Changshu Institute of Technology, 215500, China; School of Computer Science & Engineering, Changshu Institute of Technology, 215500, China; School of Computer Science & Engineering, Changshu Institute of Technology, 215500, China; School of Computer Science & Engineering, Changshu Institute of Technology, 215500, China; School of Computer Science & Engineering, Changshu Institute of Technology, 215500, China","2010 2nd International Conference on Computer Engineering and Technology","","2010","6","","V6-1","V6-6","With the improvement of cryptanalysis, More and more applications are starting to use Advanced Encryption Standard (AES) instead of Data Encryption Standard (DES) to protect their information security. However, current implementations of AES algorithm suffer from huge CPU resource consumption and low throughput. In this paper, we studied the technologies of GPU parallel computing and its optimized design for cryptography. Then, we proposed a new algorithm for AES parallel encryption, and designed and implemented a fast data encryption system based on GPU. The test proves that our approach can accelerate the speed of AES encryption significantly.","","978-1-4244-6349-7978-1-4244-6347-3978-1-4244-6348","10.1109/ICCET.2010.5486259","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5486259","Advanced Encryption Standard (AES);Graphics Processing Unit (GPU);Parallel Computing","Cryptography;Application software;Information security;Parallel processing;Central Processing Unit;Computer graphics;Algorithm design and analysis;Computer applications;Concurrent computing;Computer science","computer graphic equipment;coprocessors;cryptography;parallel algorithms","parallel AES algorithm;fast data encryption standard;cryptanalysis;advanced encryption standard;information security protection;CPU resource consumption;GPU parallel computing;cryptography design","","5","15","","","","","","IEEE","IEEE Conferences"
"A Survey of RF-Propagation Simulation Tools for Wireless Sensor Networks","M. Korkalainen; M. Sallinen","NA; NA","2010 Fourth International Conference on Sensor Technologies and Applications","","2010","","","342","347","Wireless Sensor Networks are applied in new applications all the time. However, in many cases their performance in a particular application is not be evaluated until the deployment phase. This is often too late and causes extra work and iterations in the design of the network hardware or communication parameters. This can be reduced with careful study before the actual deployment using modeling and simulation tools for testing and optimizing the sensor network beforehand. In this paper we introduce three simulation tools for radio frequency (RF) propagation and estimate their suitability for modeling and simulation of sensor networks. According to the results, we see them as very efficient tools, but also suggest improvements for even more versatile results.","","978-1-4244-7537-7978-1-4244-7538-4978-0-7695-4096","10.1109/SENSORCOMM.2010.58","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5558071","wireless sensor networks;modeling;simulation;RF-propagation;physical channel","Solid modeling;Wireless sensor networks;Three dimensional displays;Analytical models;Computational modeling;Antennas and propagation;Software","radiowave propagation;wireless sensor networks","RF-propagation simulation tools;wireless sensor networks;network hardware;communication parameters","","3","27","","","","","","IEEE","IEEE Conferences"
"FPGA implementation of 2D interactive sound communication system","M. Kang","Dept. of Electrical Engineering, Seoul National University, Seoul, Korea","2011 IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC)","","2011","","","1","6","A 2D interactive sound communication (2DISC) system and the associated system-on-chip are proposed in this paper. The system provides an intelligent way of communicating with stereophonic sound to enhance human cognitive ability during sound communication. The system generates stereophonic sound using mono sound sources and the positions of the listener and the speaker provided by a GPS. This stereophonic sound allows the listener to determine the position of the speaker simply by listening to the sound. Optimized stereophonic sound generating methods are embedded in the system, resulting in a small-sized design. The system can easily replace the currently popular communication systems that use the mono sound sources such as walkie-talkies or citizens' band transceivers. Several tests are carried out successfully using an FPGA board and a PC to demonstrate the applicability of the proposed system.","","978-1-4577-0894-7978-1-4577-0893-0978-1-4577-0892","10.1109/ICSPCC.2011.6061648","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6061648","interactive sound communication;stereophonic sound;head-related transfer function;human cognitive ability","Global Positioning System;Microphones;Ear;SDRAM;Approximation methods;Software;Field programmable gate arrays","field programmable gate arrays;Global Positioning System;Hi-Fi equipment;interactive systems;radio transceivers;system-on-chip","FPGA;2D interactive sound communication system;2DISC system;system-on-chip;stereophonic sound;human cognitive ability;mono sound sources;GPS;walkie-talkies;citizens band transceivers","","","20","","","","","","IEEE","IEEE Conferences"
"Low-Cost Light-Weight Quick-Manufacturable Blades for Human-Scale Wind Turbines","P. A. Melendez-Vega; G. Venkataramanan; D. Ludois; J. Reed","NA; NA; NA; NA","2011 IEEE Global Humanitarian Technology Conference","","2011","","","154","159","A novel blade design for small wind turbines intended for residential use in rural locations and/or developing nations has been developed. Traditionally, blades for these turbines are carved from weather resistant woods which may be costly and difficult to manufacture. This work seeks to reduce cost and construction time by developing blades from readily available PVC drainage pipe. The blade design underwent airfoil optimization regarding the blade twist along its length and drop, via computational fluid dynamic (CFD) simulations with open source software. Blades were built of PVC using the new CFD design and tested on field operation, the manufacturing process as well as their performance was monitored and recorded in terms of comparison with classical wood-based design.","","978-1-61284-634-7978-0-7695-4595","10.1109/GHTC.2011.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6103627","small wind turbine;rural electrification;blades;CFD;PVC pipe","Blades;Materials;Wind turbines;Wind speed;Automotive components;Rotors","aerodynamics;blades;computational fluid dynamics;design engineering;flow simulation;pipes;wind turbines;wood","blade design;quick-manufacturable blade;human-scale wind turbines;weather resistant wood;PVC drainage pipe;airfoil optimization;blade twist;computational fluid dynamic;drop;CFD simulation","","4","11","","","","","","IEEE","IEEE Conferences"
"Reliability-based structural integrity assessment of Liquefied Natural Gas tank with hydrogen blistering defects by MCS method","J. Hu; L. Yan; F. Liu; Q. Duan; Z. Zhang","School of Energy and Power Engineering, Xi'an Jiaotong University, 710049, China; School of Energy and Power Engineering, Xi'an Jiaotong University, 710049, China; School of Energy and Power Engineering, Xi'an Jiaotong University, 710049, China; School of Energy and Power Engineering, Xi'an Jiaotong University, 710049, China; School of Energy and Power Engineering, Xi'an Jiaotong University, 710049, China","2010 Sixth International Conference on Natural Computation","","2010","8","","4194","4198","Hydrogen blistering is one of the serious threats to safe operation of a Liquefied Natural Gas (LNG) tank, therefore safety analysis of hydrogen blistering defects is very important. In order to assess the reliability-based structural integrity of the LNG tank with defects of hydrogen blistering, the following steps were carried out. Firstly, Abaqus code, one of the Finite Element Method (FEM) software, was utilized to calculate 100 J-integral values of crack tip by defining directly. Secondly, the 100 J-integral values of crack tip were used as training data and testing data by Optimized Least Squares Support Vector Machine (OLS-SVM), Least Squares Support Vector Machine (LS-SVM) and Artificial Neural Networks (ANN) to get other 20000 J-integral values of crack tip. Finally, Monte-Carlo Simulation (MCS) was used to assess the reliability-based structural integrity analysis. The results showed that the hydrogen blistering defect with crack will propagate with about 14 percent chance in such a case. It also proved that MCS combined with FEM and SVM was an effective and prospective method for research and application of integrity assessment, which could overcome the data source problem.","2157-9555;2157-9563","978-1-4244-5961-2978-1-4244-5958-2978-1-4244-5959","10.1109/ICNC.2010.5583691","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5583691","J-integral;Support Vector Machine;MCS;Structural integrity","Support vector machines;Artificial neural networks;Finite element methods;Materials;Reliability theory;Liquefied natural gas","cracks;finite element analysis;least squares approximations;Monte Carlo methods;neural nets;petroleum;reliability;structural engineering computing;support vector machines;tanks (containers)","reliability-based structural integrity assessment;liquefied natural gas tank;hydrogen blistering defects;finite element method;Abaqus code;J-integral values;optimized least squares support vector machine;least squares support vector machine;artificial neural networks;Monte Carlo simulation;crack defect","","","11","","","","","","IEEE","IEEE Conferences"
"Auto-tuning PID module of robot motion system","H. Xiao; S. Wang","School of Automation Science and Electrical Engineering, Beihang University, Beijing 100191, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing 100191, China","2011 6th IEEE Conference on Industrial Electronics and Applications","","2011","","","668","673","In order to make the movement of mobile robot more smoothly, this paper designs a Auto-tuning PID module for mobile robot motion system. Based on the systematic mathematical model, this paper presents a least-square real-time parameter estimation algorithm with forgetting factor and incremental PID controller algorithm, and provides PID parameters optimization algorithm for the minimization of objective function. The digital signal controller Microchip dsPIC30F4011 is chosen as the core of master control system to accomplish the design of system software and hardware. The results of spot test in mobile robot's platform indicates that self-tuning PID control system can real-time estimate the controlled object parameter, real-time set adjust the controller parameters, then adapt to controlled object variance. So the auto-tuning PID control module is robust with good performance and anti-disturbance capability.","2158-2297;2156-2318","978-1-4244-8756-1978-1-4244-8754-7978-1-4244-8755-4978-1-4244-8755","10.1109/ICIEA.2011.5975670","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5975670","Auto-tuning PID;Parameter estimate;Mobile robot;Motion Controller;DSC","Brushless DC motors;Mobile robots;Motion control;Algorithm design and analysis;Torque","digital signal processing chips;least squares approximations;mobile robots;motion control;parameter estimation;self-adjusting systems;three-term control","autotuning PID module;mobile robot motion system;least-square real-time parameter estimation algorithm;forgetting factor;incremental PID controller algorithm;PID parameters optimization algorithm;objective function minimization;digital signal controller Microchip dsPIC30F4011;selftuning PID control system","","1","13","","","","","","IEEE","IEEE Conferences"
"FPGA Implementation of Adjustable Wideband Fractional Delay FIR Filters","G. Ramirez-Conejo; J. Diaz-Carmona; A. Ramirez-Agundis; A. Padilla-Medina; J. Delgado-Frias","NA; NA; NA; NA; NA","2010 International Conference on Reconfigurable Computing and FPGAs","","2010","","","406","411","This paper describes a reconfigurable hardware implementation for wideband fractional delay FIR filters. The proposed implementation is based on a multirate Farrow structure, reducing in this way the arithmetic complexity compared to the modified Farrow structure, and allowing on line fractional delay value update. A minimax frequency optimization technique is used for computing the structure coefficients. In order to reduce the resources usage the structure filters multiplications are implemented using distribute arithmetic technique. The resulting filter implementation is tested through software simulation and hardware implementation tools. The filter performance is measured in terms of area, throughput and dynamic power consumption. Accordingly to the obtained results the described structure allows the implementation of wideband fractional delay FIR filters with online factional value update. A fine fractional delay resolution is achieved with the proposed hardware implementation.","2325-6532","978-1-4244-9523-8978-0-7695-4314","10.1109/ReConFig.2010.68","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5695340","FPGA implementation;Digital Filters;FIR;Fractional Delay Filter;Distributed Arithmetic","Finite impulse response filter;Delay;Table lookup;Field programmable gate arrays;Pipelines;Throughput","distributed arithmetic;field programmable gate arrays;FIR filters","FPGA implementation;adjustable wideband fractional delay FIR filters;reconfigurable hardware implementation;on line fractional delay value update;minimax frequency optimization technique;distribute arithmetic technique;fine fractional delay resolution","","4","16","","","","","","IEEE","IEEE Conferences"
"High performance systems: An agent based application power profiling","H. V. Raghu; A. Kumar; B. S. Bindhumadhava","Real Time Systems and Smart Grid Group, Centre for Development of Advanced Computing, ‘C-DAC Knowledge Park’, Bangalore, INDIA; Real Time Systems and Smart Grid Group, Centre for Development of Advanced Computing, ‘C-DAC Knowledge Park’, Bangalore, INDIA; Real Time Systems and Smart Grid Group, Centre for Development of Advanced Computing, ‘C-DAC Knowledge Park’, Bangalore, INDIA","2012 18th International Conference on Advanced Computing and Communications (ADCOM)","","2012","","","59","65","Power measurement and analysis are important aspects for optimizing the power consumption in High Performance Computing (HPC) systems. With the huge increase in the power consumption of HPC systems, it is important to compare systems with metrics based on performance per watt. There are various hardware and software based power measurement techniques available for HPC systems. But, it's a complex task to accurately measure and analyze the power consumption of entire HPC nodes in real time. Hence, we have used hardware based power measurement technique with Multi-Agent based framework for analyzing power in HPC systems at real time. We clearly demonstrated the power consumed while running the various workloads such as High Performance Linpack (HPL) and NAS Parallel Benchmarks (NPB).","","978-1-4799-0801-1978-1-4799-0802-8978-1-4799-0800","10.1109/ADCOM.2012.6563585","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6563585","Power Measurement;High Performance Computing;Multi-Agent Framework;HPC Standard Benchmarks","Power demand;Power measurement;Benchmark testing;Supercomputers;Servers;Memory management","multi-agent systems;parallel processing;power aware computing;power consumption","NPB;NAS parallel benchmarks;HPL;high performance linpack;multiagent based framework;used hardware based power measurement technique;high performance computing systems;HPC;power consumption;agent based application power profiling","","4","17","","","","","","IEEE","IEEE Conferences"
"A Case for Hardware Task Management Support for the StarSS Programming Model","C. Meenderinck; B. Juurlink","NA; NA","2010 13th Euromicro Conference on Digital System Design: Architectures, Methods and Tools","","2010","","","347","354","StarSS is a parallel programming model that eases the task of the programmer. He or she has to identify the tasks that can potentially be executed in parallel and the inputs and outputs of these tasks, while the runtime system takes care of the difficult issues of determining inter task dependencies, synchronization, load balancing, scheduling to optimize data locality, etc. Given these issues, however, the runtime system might become a bottleneck that limits the scalability of the system. The contribution of this paper is two-fold. First, we analyze the scalability of the current software runtime system for several synthetic benchmarks with different dependency patterns and task sizes. We show that for fine-grained tasks the system does not scale beyond five cores. Furthermore, we identify the main scalability bottlenecks of the runtime system. Second, we present the design of Nexus, a hardware support system for StarSS applications, that greatly reduces the task management overhead.","","978-1-4244-7839","10.1109/DSD.2010.63","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5615589","task management;hardware support;StarSS;parallel programming","Scalability;Benchmark testing;Runtime;Hardware;Programming;Neodymium;Manuals","parallel programming;resource allocation;scheduling","hardware task management support;StarSS programming model;parallel programming model;inter task dependency issue;synchronization issue;load balancing issue;scheduling issue;runtime system;Nexus support system","","8","13","","","","","","IEEE","IEEE Conferences"
"Coverage of communication-based sensor nodes deployed location and energy efficient clustering algorithm in WSN","X. Gao; Y. Vanq; D. Zhou","College of Microelectronics, Xidian University, Xi'an 710071, P. R. China; College of Microelectronics, Xidian University, Xi'an 710071, P. R. China; College of Computer Science and Technology, Xidian University, Xi'an 710071, P. R. China","Journal of Systems Engineering and Electronics","","2010","21","4","698","704","An effective algorithm based on signal coverage of effective communication and local energy-consumption saving strategy is proposed for the application in wireless sensor networks. This algorithm consists of two sub-algorithms. One is the multi-hop partition subspaces clustering algorithm for ensuring local energybalanced consumption ascribed to the deployment from another algorithm of distributed locating deployment based on efficient communication coverage probability (DLD-ECCP). DLD-ECCP makes use of the characteristics of Markov chain and probabilistic optimization to obtain the optimum topology and number of sensor nodes. Through simulation, the relative data demonstrate the advantages of the proposed approaches on saving hardware resources and energy consumption of networks.","1004-4132","","10.3969/j.issn.1004-4132.2010.04.025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6075497","wireless sensor network;probability distribution function;Markov chain;received signal strength indicator;Gaussian distribution","Clustering algorithms;Energy consumption;Wireless sensor networks;Partitioning algorithms;Sensors;Testing;Radio transmitters","","","","4","","","","","","","BIAI","BIAI Journals & Magazines"
"GROWDERS, a project to demonstrate grid connected electricity storage in the distribution grid","J. J. van der Burgt; J. H. J. Lemmens; P. D. M. de Boer-Meulman","KEMA Nederland BV, Arnhem, The Netherlands; KEMA Nederland BV, Arnhem, The Netherlands; KEMA Nederland BV, Arnhem, The Netherlands","7th Mediterranean Conference and Exhibition on Power Generation, Transmission, Distribution and Energy Conversion (MedPower 2010)","","2010","","","1","4","In future electricity networks technical challenges regarding distributed energy sources could be solved by using transportable and flexible storage systems. The GROWDERS (Grid Reliability and Operability with Distributed Generation using Flexible Storage) project, funded by the European Commission, aims to demonstrate the technical and economical maturity of these systems. At several test sites across Europe the technology of two Li-ion battery systems and a flywheel system has proven to serve the management of the energy grid. A software planning tool, PLATOS (PLAnning Tool for Optimized Storage), was developed for the optimization of the location, type and size of storage systems for distribution gird applications.","","978-1-84919-319","10.1049/cp.2010.0957","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5716035","Electricity storage;flywheel;grid connected system;Li-ion battery;planning of power systems;renewable energy sources;smart grids","","distributed power generation;secondary cells","electricity storage;electricity network;distributed energy sources;distributed generation;European Commission;flywheel system;Li-ion battery system;Li","","","","","","","","","IET","IET Conferences"
"New rotor structure mitigating vibration and noise in switched reluctance motor","Jie Li; HeXu Sun; Yi Liu","School of Electrical Engineering and Automation, Hebei University of Technology, Tianjin, China; School of Electrical Engineering and Automation, Hebei University of Technology, Tianjin, China; School of Electrical Engineering and Automation, Hebei University of Technology, Tianjin, China","2010 International Conference on Information, Networking and Automation (ICINA)","","2010","2","","V2-80","V2-84","Vibration and noise are the fetters which restrict the development of switched reluctance motor drive; this paper presented a new rotor structure which mitigates motor's vibration and noise effectively. Based on the permeability theory, set holes on the rotor without filling materials in order to reduce the magnetic flux at the aligned position, further more, reduce the radial force amplitude from the motor's noise origin. Also, finite element software is employed to analysis motor's performance under different controlling modes including single phase excited mode, double phase excited mode. Parametric simulations are carried out to analysis how the key structure parameters influence the motor's output performance. The simulation results form the basis of new structure optimization. And, a prototype 8/6 switched reluctance motor is tested, and the obtained test results show that motor with this kind of rotor structure keeps the output capability while has a less acoustic noise and vibration.","2162-5476;2162-5484","978-1-4244-8106-4978-1-4244-8104","10.1109/ICINA.2010.5636789","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5636789","switched reluctance motor;noise;vibration;radial force;finite element simulation;rotor structure","Switches;Reluctance motors;Phase distortion;Integrated circuits;Atmospheric modeling;Analytical models;Magnetic analysis","acoustic noise;finite element analysis;machine theory;magnetic permeability;reluctance motors;rotors;vibrations","rotor structure mitigating vibration;noise mitigation;switched reluctance motor;permeability theory;finite element method;controlling modes;single phase excited mode;double phase excited mode;parametric simulations;acoustic noise","","1","6","","","","","","IEEE","IEEE Conferences"
"Study and verification of a slow speed PM generator with outer rotor for small scale wind turbines","A. Kilk; O. Kudrjavtsev","Department of Fundamentals of Electrical Engineering and Electrical Machines, Tallinn University of Technology, Ehitajate tee 5, 19086 Tallinn, Estonia; Department of Fundamentals of Electrical Engineering and Electrical Machines, Tallinn University of Technology, Ehitajate tee 5, 19086 Tallinn, Estonia","2012 Electric Power Quality and Supply Reliability","","2012","","","1","6","This paper considers the study of a directly driven low-speed permanent magnet synchronous generator specially constructed for a vertical axis wind turbine. The studied PM generator with outer rotor construction offers an alternative to fix the wind turbine directly to the end-disk of rotor. As an advantage of the outer rotor PM generator construction it is easier and more reliable to fix the magnets on the rotor active surface. An analysis to optimize the active length of both stator magnetic core and air-gap has been presented. Cogging torque has been analyzed to reduce it by slot skewing. Electromagnetic simulations of the PM generator were performed by using finite element analysis and the SPEED software. A 5 kVA PM generator was designed, manufactured and tested. The test results of the prototype PM generator have been analyzed and compared with the calculated characteristics and data.","","978-1-4673-1979-9978-1-4673-1980-5978-1-4673-1978","10.1109/PQ.2012.6256198","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6256198","Permanent magnet synchronous generator;radial flux;outer rotor;slot skewing","Generators;Magnetic flux;Rotors;Air gaps;Torque;Forging;Stator windings","","","","2","6","","","","","","IEEE","IEEE Conferences"
"ISDB-Tb field trials and coverage measurements with gap-filler in suburban environments","S. R. Messias de Carvalho; Y. Iano; R. Arthur","EPTV Campinas, R. Regina Nogueira st, 120 Jd. S&#x00E3;o Gabriel, SP CEP 13106080 Brazil; Communication Department, DECOM-FEECUNICAMP, Albert Einstein, 400 Av. Cid. Universit&#x00E1;ria Zeferino Vaz Distrito Bar&#x00E3;o Geraldo, CEP 13083-852 - Campinas SP-Brazil; Telecommunication Department, School of Technology, State University of Campinas, Limeira, Paschoal Marmo st, 1888, Jd. Nova It&#x00E1;lia, CEP 13484-332 Brazil","2011 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)","","2011","","","1","5","The expansion of digital television in South America and Africa demand new cheaper solutions for the coverage problem. The use of gap-fillers and SFN are very important instruments to design broadcasting television. Many countries start trials and tests according their particularities. The solutions found are been related in papers and make a valuable acknowledge for future projects related. Comparison between field application and coverage prediction software optimize the results of broadcasting network. This paper presents the results of a field measurement with Gap-Filler in suburban environment. This paper presents the results of a field measurement and a description of implantation of a Gap-Filler in suburban environment..","2155-5052;2155-5044;2155-5044","978-1-61284-122-9978-1-61284-121-2978-1-61284-120","10.1109/BMSB.2011.5954970","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954970","coverage measurements;ISDB-Tb;Gap-Filler","Receivers;Transmitting antennas;Delay;Digital TV;Digital multimedia broadcasting","digital television;television broadcasting","ISDB-Tb field trials;coverage measurements;gap-filler;suburban environments;digital television;South America;Africa;broadcasting network","","1","7","","","","","","IEEE","IEEE Conferences"
"Research on the design and experiment of power transformer DC bias blocking device","C. Li; Y. Zhang; Y. Lv; S. Sun","Department of Electrical Engineering, Shandong Polytechnic University, Jinan, China; Department of Electrical Engineering, Shandong Polytechnic University, Jinan, China; Shandong Disheng Electronic Co., Ltd. Jinan, China, 250300; 4. Shandong EPRI, Jinan, China; Shandong Disheng Electronic Co., Ltd. Jinan, China, 250300; 4. Shandong EPRI, Jinan, China","2012 2nd International Conference on Consumer Electronics, Communications and Networks (CECNet)","","2012","","","2811","2814","The monitoring and measurement results indicate that, severe influence may be brought to electromagnetic devices of power systems such as power transformers by DC bias effects. So, DC bias blocking device must be designed and manufactured to ensure the safety and stability of the power systems. In this paper, optimized analysis were done on the DC current blocking scheme based on the existed dual protection topology, and the selection and design parameters principle of the device was given according to the relevant principles. Combined with the specific structure and parameters of Shandong peninsula grid, a prototype was developed, and a series of tests were done on it with the data gotten from debugging. The experiment results showed that both the hardware and software can work reliably and the device can be used to the production field.","","978-1-4577-1415-3978-1-4577-1414-6978-1-4577-1413","10.1109/CECNet.2012.6202179","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6202179","Power transformer;DC bias;Dual Protection Topology;Prototype","Capacitors;Switches;Current measurement;Voltage measurement;Current transformers;Power transformers","electromagnetic devices;power grids;power system measurement;power system stability;power transformers","power transformer DC bias blocking device;electromagnetic devices;power transformers;DC bias effects;power systems safety;power systems stability;dual protection topology;Shandong peninsula grid","","","6","","","","","","IEEE","IEEE Conferences"
"Integrated and centralized management of electrical services in power distribution utilities","V. J. Garcia; D. P. Bernardon; M. Sperandio; C. d. Vale","NA; NA; NA; NA","2011 46th International Universities' Power Engineering Conference (UPEC)","","2011","","","1","5","This work proposed an approach to centralized dispatch of service orders in electric utilities. From the knowledge a priori of all orders one day before to be assigned to a given set of the maintenance teams, computational techniques must be applied to obtain the most appropriate assignment of orders to the teams and the corresponding definition of the execution order for each team. The main contribution of this methodology refers to the productivity improvement of the teams. However, considering the significant amount of orders to be dispatched and the set of criteria to be assumed (kind of orders, location and availability of teams, deadlines, priority levels, etc), this problem do not yield a simple approach. Therefore, this work proposes a methodology involving a set of software techniques in order to manage this process and to furnish a feasible assignment, by combining a proper approach for combining the resolution of the clustering and routing combinatorial optimization problems. The proposed methodology was tested with actual scenarios of an electric utility from Brazil.","","978-3-8007-3402","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6125615","","Maintenance engineering;Routing;Manuals;Vehicles;Educational institutions;Clustering algorithms;Operations research","","","","","","","","","","","VDE","VDE Conferences"
"Articular cartilage segmentation in noisy MR images of human knee","H. V. Tran; D. Jiang","School of Engineering, University of Tasmania, Hobart, Australia; School of Engineering, University of Tasmania, Hobart, Australia","2012 Cairo International Biomedical Engineering Conference (CIBEC)","","2012","","","146","149","The segmentation of MR images is of great interests in automatic medical diagnosis. However, such images are corrupted by Rician noise and with fuzzy edges. The non-additive and intensity dependant features of Rician noise make image processistandardng very challenging. In this paper, a combination of techniques are carefully selected, tailored, and organized to improve the image quality for automatic segmentation of the articular cartilage from noisy MR images. A prototype software procedure is developed based on that. First, the Rician noise, known as the major factor contributing to corrupt the magnitude MR images, is investigated. A method based on the least squares approach is employed to estimate the noise standard deviation from the background mode of the image. Then, a total variation noise removal algorithm using iterative scheme is applied to remove the noise. After that, the vector field convolution active contour method is applied to the resultant image for cartilage segmentation. Two different approaches are proposed to define the initial contour to avoid local optimization traps. A test set MR images on 62 slices of human knee is used to illustrate the proposed system. Effectiveness of the proposed procedure is demonstrated using computational experiments in comparison to some existing methods.","2156-6100;2156-6097;2156-6097","978-1-4673-2801-2978-1-4673-2800-5978-1-4673-2799","10.1109/CIBEC.2012.6473331","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6473331","MRI;segmentation;Rician noise;active contour method","Noise;Active contours;Image segmentation;Rician channels;Vectors;Humans;Standards","biological tissues;biomedical MRI;convolution;image denoising;image segmentation;iterative methods;least squares approximations;medical image processing;vectors","articular cartilage segmentation;noisy MR images;human knee;Rician noise;fuzzy edges;least squares approach;total variation noise removal algorithm;vector field convolution","","3","10","","","","","","IEEE","IEEE Conferences"
"NPP ATMS sensor model using fractional Brownian motion and thermal vacuum data","R. V. Leslie; W. J. Blackwell; M. DiLiberto; M. Tolman","Lincoln Laboratory, Massachusetts Institute of Technology, 244 Wood Street, Lexington, 02420, USA; Lincoln Laboratory, Massachusetts Institute of Technology, 244 Wood Street, Lexington, 02420, USA; Lincoln Laboratory, Massachusetts Institute of Technology, 244 Wood Street, Lexington, 02420, USA; Lincoln Laboratory, Massachusetts Institute of Technology, 244 Wood Street, Lexington, 02420, USA","2011 IEEE International Geoscience and Remote Sensing Symposium","","2011","","","2237","2240","Radiometric digital count data are required to test the operational software of the Advanced Technology Microwave Sounder (ATMS) prior to its launch in 2011 to verify effective calibration procedures and optimize performance. These data, however, do not exist prior to launch, thus necessitating a method to build a model of the ATMS that synthesizes realistic data as though the sensor were space-borne. A method to build such a model is described in this paper. The procedure and application are specific to the ATMS but could be applied to other passive microwave radiometers as well.","2153-7003;2153-6996;2153-6996","978-1-4577-1005-6978-1-4577-1003-2978-1-4577-1004","10.1109/IGARSS.2011.6049614","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6049614","","Asynchronous transfer mode;Calibration;Brightness temperature;Temperature measurement;Noise;Radiometers;Data models","Brownian motion;calibration;geophysical equipment;radiometers","NPP ATMS sensor model;fractional Brownian motion;thermal vacuum data;radiometric digital count data;Advanced Technology Microwave Sounder;AD 2011;calibration procedures;passive microwave radiometers","","1","6","","","","","","IEEE","IEEE Conferences"
"Finite Element Analysis for Stress and Magnetic Field of a 40 kA Protection Inductor","L. Qiu; Y. Lv; L. Li","Wuhan Pulsed High Magnetic Field Center, Wuhan, China; Wuhan Pulsed High Magnetic Field Center, Wuhan, China; Wuhan Pulsed High Magnetic Field Center, Wuhan, China","IEEE Transactions on Applied Superconductivity","","2010","20","3","1936","1939","The protection inductor serves for limiting the peak current in order to protect the thyristor switch of the pulsed magnetic field facility in case of a short circuit. Because of the high current and strong magnetic field, the Lorentz force in the protection inductor is large. This paper describes the calculation of the inductance and the optimization of the stresses in the protection inductor. A finite element analysis with the ANSYS software is used to calculate the magnetic field and stresses in the protection inductor. A three-dimensional static finite element analysis model of the inductor has been built for calculating the stresses in the copper coils and stainless steel rings as well as the static inductance. Furthermore, a harmonic finite element analysis model has been built to analyse effects such as the influence of eddy currents in the copper wires and induced current in the stainless steel rings on stress and inductance. Eddy currents cause an uneven distribution of stresses; induced current can decrease the stresses in the copper coils but increase those in the stainless steel rings. Both effects reduce the inductance. The typical maximum stresses in our design are 70 MPa in the copper coils and 210 MPa in the stainless steel rings; these are both below the yield strength of these materials. The inductance is 1.05 mH at the frequency of 50 Hz. The protection inductor has been manufactured according to the design and the performance testing has been successfully completed.","1051-8223;1558-2515;2378-7074","","10.1109/TASC.2010.2044646","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5451070","Electromagnetic analysis;electromagnetic-structural sequential coupling;finite element analysis;protection inductor;structural analysis","Finite element methods;Magnetic analysis;Stress;Magnetic fields;Protection;Inductors;Inductance;Copper;Steel;Coils","finite element analysis;harmonic analysis;magnetic field effects;power inductors;stress analysis;transformer protection","finite element analysis;stress field;magnetic field;protection inductor;peak current;short circuit current;stresses inductor;harmonic analysis;stainless steel rings;eddy currents;current 40 kA;frequency 50 Hz","","4","3","","","","","","IEEE","IEEE Journals & Magazines"
"D5. Wideband tunable MEMS phase shifters for radar phased array antenna","A. K. Aboul-Seoud; A. Hamed; A. E. S. Hafez","Faculty of Engineering, Alexandria University, Alexandria, Egypt; Faculty of Engineering, Alexandria University, Alexandria, Egypt; Faculty of Engineering, Alexandria University, Alexandria, Egypt","2012 29th National Radio Science Conference (NRSC)","","2012","","","593","599","A wideband tunable MicroElectroMechanical System (MEMS) phase shifter is proposed for radar phased array antenna. A tuning circuit is added to modulate a narrow band phase shifter to achieve a bandwidth of about 200 MHz at 3 GHz central frequency which is suitable for radar applications. RF capacitors, inductors, and resistors are selected and optimized for the tuning circuit to achieve a phase response suitable at the pre-mentioned bandwidth. The overall phase response of the phase shifter is an algebraic sum of the narrow band phase shifter and the tuning circuit phase responses. The phase shifter is designed, simulated and tested using Advanced Design System (ADS) software either with or without using tuning circuits. This type of phase shifters can give high performance and resolution at a bandwidth 20 times wider than the normal switched line phase shifter.","","978-1-4673-1887-7978-1-4673-1884-6978-1-4673-1886","10.1109/NRSC.2012.6208570","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6208570","Phased Array Antenna;MEMS Switch;Switched Line Phase Shifter;Tuning circuit;Phase response;Wideband;ADS;Radar","Phase shifters;RLC circuits;Tuning;Bandwidth;Educational institutions;Radar antennas;Radio frequency","antenna phased arrays;capacitors;circuit tuning;inductors;micromechanical devices;phase shifters;phased array radar;radar antennas;resistors","wideband tunable MEMS phase shifters;radar phased array antenna;microelectromechanical system phase shifter;tuning circuit;RF capacitor;inductors;resistors;narrow band phase shifter;advanced design system;bandwidth 200 MHz;frequency 3 GHz","","","13","","","","","","IEEE","IEEE Conferences"
"A new parallel algorithm for simulation of spin-glasses in external fields","A. S. Gevorkyan; H. G. Abajyan; H. S. Sukiasyan","Institute for Informatics and Automation Problems, NAS of Armenia; Institute for Informatics and Automation Problems, NAS of Armenia; Institute of Mathematics, NAS of Armenia","2011 Proceedings of the 34th International Convention MIPRO","","2011","","","162","166","Spin glasses are prototypical models for disordered systems which provide a rich source for investigations of a number of important and difficult applied problems of physics, chemistry, material science, biology, evolution, organization dynamics, hard-optimization, environmental and social structures, human logic systems, financial mathematics etc. Numerical studies of spin-glass systems are difficult to accomplish and in general only small moderate system sizes can be accessed. An effective algorithm for parallel simulation of spin-glass system is developed. In contrast to well known algorithms of Metropolis and others which are based on the Monte Carlo simulations method, the developed algorithm allows with high efficacy to construct stable spin-chains of arbitrary length in parallel and calculate all statistical parameters of spin-glass system of large sizes. We have implemented software using cluster computation (MPI technology) and GPU technology (CUDA programming language is used) as well. Since the ideology of GPU technology is SIMD (Single Instruction Multiple Data) and our implemented algorithm is from that class of problems, we obtained fully parallel implementation. We have tested the developed code on example of simulation 1D spin-glass in external fields and we were convinced of reliability and efficiency of calculations.","","978-953-233-059-5978-1-4577-0996-8978-953-233-067","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5967043","","Mathematical model;Glass;Biological system modeling;Equations;Numerical models;Graphics processing unit;Heuristic algorithms","Monte Carlo methods;numerical analysis;spin glasses","parallel algorithm;spin glass system;external field;prototypical model;disordered system;Metropolis algorithm;Monte Carlo simulations method;stable spin-chains;statistical parameters;cluster computation;GPU technology;single instruction multiple data","","","23","","","","","","IEEE","IEEE Conferences"
"1024-point pipeline FFT processor with pointer FIFOs based on FPGA","Guanwen Zhong; Hongbin Zheng; ZhenHua Jin; Dihu Chen; Zhiyong Pang","School of physics and engineering, Sun Yat-sen University, Guangzhou 510275, China; School of physics and engineering, Sun Yat-sen University, Guangzhou 510275, China; School of physics and engineering, Sun Yat-sen University, Guangzhou 510275, China; School of physics and engineering, Sun Yat-sen University, Guangzhou 510275, China; School of physics and engineering, Sun Yat-sen University, Guangzhou 510275, China","2011 IEEE/IFIP 19th International Conference on VLSI and System-on-Chip","","2011","","","122","125","Design and optimized implementation of a 16-bit and 32-bit 1024-point pipeline FFT processor is presented in this paper. The architecture of the FFT is based on R2<sup>2</sup>SDF algorithm with new pointer FIFO embedded with gray code counters. It is implemented in Spartan-3E, Spartan-6 and Virtex-4 devices and fully tested by method of co-simulation using SMIMS<sup>®</sup>VeriLink<sup>®</sup>as a bridge that connects software(Matlab<sup>®</sup>Simulink<sup>®</sup>) and real hardware-FPGA targets. The implementation results show that our pointer FIFO FFT processor could use lower resource, but achieve higher performance. Our 16-bit 1024-point FFT processor only costs 2580 slices, 2030 slice flip flops and just 2 block RAMs, achieving the maximum clock frequency of 92.6 MHz with the throughput per area of 0.035 Msamples/s/area. Due to the parameterized input wordlength, output wordlength, Twiddle Factors wordlength and processing stages, it is easily to implement a 16-point, 64-point, 256-point, 1024-point,4096-point or higher power of 4 points pointer FIFO FFT processor synthesized from the same code just through modifying the corresponding parameters.","2324-8432;2324-8440","978-1-4577-0170-2978-1-4577-0171-9978-1-4577-0169","10.1109/VLSISoC.2011.6081654","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6081654","FFT;Pointer FIFO;Radix22SDF;Cosimulation","Computer architecture;Random access memory;Pipelines;Reflective binary codes;Radiation detectors;Throughput;Field programmable gate arrays","digital arithmetic;fast Fourier transforms;field programmable gate arrays;flip-flops;Gray codes;microprocessor chips;random-access storage","1024-point pipeline FFT processor;pointer FIFO FFT processor;FPGA;R22SDF algorithm;gray code counters;Virtex-4 devices;Spartan-6 devices;Spartan-3E devices;SMIMS VeriLink simulation;Matlab-Simulink;RAM;flip flops;twiddle factors wordlength;word length 16 bit;word length 32 bit;frequency 92.6 MHz","","2","8","","","","","","IEEE","IEEE Conferences"
"Wireless body area network system based on ECG and accelerometer pattern","E. Kańtoch; M. Smoleń; P. Augustyniak; P. Kowalski","AGH University of Science and Technology, Kraków, Poland; AGH University of Science and Technology, Kraków, Poland; AGH University of Science and Technology, Kraków, Poland; AGH University of Science and Technology, Kraków, Poland","2011 Computing in Cardiology","","2011","","","245","248","Health monitoring and body area network (BAN) applications require wireless intelligent monitoring devices and information systems. The aim of our research is to propose a prototype of wearable wireless monitoring device optimized to supervising the patient and examine the influence of movement on the heart rate during normal daily activities. Main purpose of the proposed system consists in simultaneous acquisition and automatic analysis of two bipolar ECG and three-axes acceleration (ACC) signals measured by means of wireless, battery-operated prototype of Revitus ECG module. The processing of the ECG and ACC data is performed by the custom-developed software installed on PC. All recorded information is uploaded to a purposely-designed medical web server for the storage and display as a web page for authorized doctors or patient's family. The system was tested on 10 healthy volunteers. Each of them was monitored during common daily activities.","2325-8853;0276-6574","978-1-4577-0611-0978-1-4577-0612","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6164548","","Monitoring;Electrocardiography;Wireless sensor networks;Wireless communication;Sensors;Heart rate;Biomedical monitoring","accelerometers;body area networks;computerised monitoring;electrocardiography;medical computing;medical signal detection;telemedicine","wireless body area network system;bipolar ECG signals;electrocardiography;accelerometer pattern;health monitoring;wireless intelligent monitoring devices;information systems;wearable wireless monitoring device;patient supervision;heart rate movement examination;three-axes acceleration signals;Revitus ECG module;ECG data;ACC data;medical Web server","","1","6","","","","","","IEEE","IEEE Conferences"
"Usage of the safety-oriented real-time OASIS approach to build deterministic protection relays","M. Jan; V. David; J. Lalande; M. Pitel","CEA, LIST, Embedded Real Time Systems Laboratory Point Courrier 94, Gif-sur-Yvette, F-9111 France; CEA, LIST, Embedded Real Time Systems Laboratory Point Courrier 94, Gif-sur-Yvette, F-9111 France; Strategy & Innovation, Technology Innovation, Schneider Electric Industries, 37, quai Paul Louis Merlin, 38050 Grenoble cedex, France; Strategy & Innovation, Technology Innovation, Schneider Electric Industries, 37, quai Paul Louis Merlin, 38050 Grenoble cedex, France","International Symposium on Industrial Embedded System (SIES)","","2010","","","128","135","As any safety-related system, medium voltage protection relays have to comply with a Safety Integrated Level (SIL), as defined by the IEC 61508 standard. The safety-function of the software part of protection relays is first to detect any faults within the supervised power network, then ask the tripping of the circuit breakers in order to isolate the faulty portion of the network. However, it is required that detection and isolation of faults must occur within a given time, as specified by the IEC 60255 standard. Schneider Electric currently achieves the demonstration that a protection relay is performing its safety-function within such temporal constraints at the price of a costly phase of tests. The OASIS approach is a complete tool-chain to build safety-critical deterministic real-time systems, which enables the demonstration of the system timeliness. In this paper, we show how we apply the OASIS approach to build a deterministic protection relay system. We designed a software platform called OASISepam, based on an existing product from Schneider Electric, namely the Sepam 10. We show a preliminary evaluation of our implementation over a STR710 ARM-based board that runs the OASIS kernel. Notably, we show that the observed worst-case end-to-end detection time of OASISepam fulfils the specified constraint expressed in the design phase and translated in the OASIS programming model. Consequently, the temporal behaviour of protection relays is mastered, thus reducing application development costs and allowing the optimization of selectivity.","2150-3109;2150-3117","978-1-4244-5841-7978-1-4244-5839-4978-1-4244-5840","10.1109/SIES.2010.5551378","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5551378","Power network protection relay;real-time operating system;deterministic multi-scale time-triggered system;spatial and temporal partitioning;end-to-end temporal constraint","Relays;Real time systems;Circuit faults;Kernel;Hardware;Delay","circuit breakers;IEC standards;power engineering computing;power system protection;real-time systems;relay protection","safety-oriented real-time OASIS;deterministic protection relays;medium voltage protection relays;safety integrated level;IEC 61508 standard;circuit breakers;IEC 60255 standard;Schneider Electric;OASISepam;Sepam 10;STR710 ARM","","5","14","","","","","","IEEE","IEEE Conferences"
"HLS approach in designing FPGA-based custom coprocessor for image preprocessing","M. Samarawickrama; R. Rodrigo; A. Pasqual","Department of Electronic and Telecommunication Engineering, University of Moratuwa, Sri Lanka; Department of Electronic and Telecommunication Engineering, University of Moratuwa, Sri Lanka; Department of Electronic and Telecommunication Engineering, University of Moratuwa, Sri Lanka","2010 Fifth International Conference on Information and Automation for Sustainability","","2010","","","167","171","Control the data flow between device interfaces, processing blocks and memories in a vision system is complex in hardware implementation. In the research, high-level synthesis tool is used to design, implement and test the vision system within the context of required control, synchronization, and parameterization on a processor based platform. In addition, both HLS tools and HDL were used for the development of the processing cores, and the performance of the two versions were analyzed and compared. The operational structures of benchmarked vision core consist of custom vision coprocessor with efficient memory and bus interfaces. The performance properties such as accuracy, throughput and efficiency are measured and presented. Xilinx XC5VLX110T FPGA, has been used for prototype the hardware platforms. According to results, without any complex optimizations, pipeline length and resource utilization was achieved compared with the HDL counterpart. Our image pre-processing architecture which was implemented using HLL is faster than the optimized software implementation on an Intel Core 2 Duo GPU. The development time using AccelDSP was roughly five times shorter than using Verilog. Therefore, the availability of competent high-level synthesis tools will significantly reduce costs and design constraints in embedded image-processing implementations on FPGA.","2151-1802;2151-1810","978-1-4244-8552-9978-1-4244-8549-9978-1-4244-8551","10.1109/ICIAFS.2010.5715654","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5715654","FPGA;reconfigurable architectures;HLS tools;HLL;HDL;RTL modeling;pipelining;performance benchmark;design flow","Field programmable gate arrays;Hardware design languages;Coprocessors;Hardware;Computer architecture;Pixel;Parallel processing","computer vision;coprocessors;digital signal processing chips;field programmable gate arrays;logic design","FPGA-based custom coprocessor design;image preprocessing;data flow control;vision system;processing cores development;vision coprocessor;memory interface;bus interface;performance property;accuracy measurement;throughput measurement;efficiency measurement;Xilinx XC5VLX110T FPGA;Intel Core 2 Duo GPU;AccelDSP;high-level synthesis tools;embedded image processing","","2","12","","","","","","IEEE","IEEE Conferences"
"Optimal Contract Pricing of Distributed Generation in Distribution Networks","J. M. Lopez-Lezama; A. Padilha-Feltrin; J. Contreras; J. I. Munoz","Group of Efficient Energy Management (GIMEL), Universidad de Antioquia, Medellín, Colombia; Universidade Estadual Paulista (UNESP), Ilha Solteira, Brasil; E.T.S. de Ingenieros Industriales, Universidad de Castilla – La Mancha, Ciudad Real, Spain; E.T.S. de Ingenieros Industriales, Universidad de Castilla – La Mancha, Ciudad Real, Spain","IEEE Transactions on Power Systems","","2011","26","1","128","136","This paper proposes a bilevel programming approach to determine the optimal contract price of dispatchable distributed generation (DG) units in distribution systems. Two different agents are considered in the model, namely, the distribution company (DisCo) and the owner of the DG. The former seeks the minimization of the payments incurred in attending the forecasted demand, while the latter seeks the maximization of his profit. To meet the expected demand, the DisCo has the option to purchase energy from any DG unit within its network and directly from the wholesale electricity market. A traditional distribution utility model with no competition among DG units is considered. The proposed model positions the DG owner in the outer optimization level and the DisCo in the inner one. This last optimization problem is substituted by its Karush-Kuhn-Tucker optimality conditions, turning the bilevel programming problem into an equivalent single-level nonlinear programming problem which is solved using commercially available software. Tests are performed in a modified IEEE 34-bus distribution network.","0885-8950;1558-0679","","10.1109/TPWRS.2010.2048132","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463030","Bilevel programming;distributed generation;distribution networks","Contracts;Pricing;Distributed control;Substations;Voltage;Programming profession;Joining processes;Power supplies;Load flow;Power generation","distributed power generation;nonlinear programming;power distribution economics;power markets;pricing","optimal contract pricing;distributed generation;bilevel programming approach;distribution company;wholesale electricity market;traditional distribution utility model;Karush-Kuhn-Tucker optimality conditions;single-level nonlinear programming problem;modified IEEE 34-bus distribution network","","55","20","","","","","","IEEE","IEEE Journals & Magazines"
"A Task Tree Executor Architecture Based on Intel Threading Building Blocks","M. Popovic; M. Djukic; V. Marinkovic; N. Vranic","NA; NA; NA; NA","2012 IEEE 19th International Conference and Workshops on Engineering of Computer-Based Systems","","2012","","","201","209","Software systems based on service oriented architecture principles, which manage critical infrastructures, are typical environments where proper parallel data processing is one of the essential goals to achieve. Designers of such systems are normally expected to optimize the system performance and/or introduce new functionalities by evolving the existing system architecture. Our aim of this paper was to optimize system performance of a SOA-based control system by evolving the architecture of the particular service component within the system, which is responsible for complex calculations on large-scale graph models, under near to real time restrictions. This service component transforms system models into task trees, which are then executed by the runtime library that is referred to as the Task Tree Executor (TTE). The goal of this paper was to introduce finer grained parallelism, thus better multicore CPU utilization, by evolving TTE architecture in such a way that novel architecture executes TTE tasks as Intel TBB tasks rather than Win32/Linux threads, which was the case for the previous TTE architecture. The experimental evaluation based on measuring time needed for TTE reliability estimation, by statistical usage tests, shows that novel TTE architecture provides the speedup of around 8 times, on average, over the previous one. Although the focus of the paper is on a particular component, of a particular system, the approach that we took should be applicable on a broader class of SOA-based systems.","","978-1-4673-0912-7978-0-7695-4664","10.1109/ECBS.2012.8","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6195188","service oriented architecture;architecture evolution;task trees;parallel programming;threading building blocks","Computer architecture;Parallel processing;Games;Libraries;Architecture;Service oriented architecture;Instruction sets","","","","","19","","","","","","IEEE","IEEE Conferences"
"A FPGA/DSP design for real-time fracture detection using Low Transient Pulse","A. Mathur; T. N. Chang","Dept. of Electrical &amp; Computer Engineering, New Jersey Institute of Technology, Newark, 07102, USA; Dept. of Electrical &amp; Computer Engineering, New Jersey Institute of Technology, Newark, 07102, USA","2010 IEEE International Symposium on Industrial Electronics","","2010","","","2753","2758","This work presents the hardware and software architecture for the detection of fractures and edges in materials. While the detection method is based on the novel concept of Low Transient Pulse (LTP), the overall system implementation utilizes two microelectronics technologies: Digital Signal Processor (DSP) and Field Programmable Gate Array (FPGA). The DSP carries out the analysis of the received signal at a much lower rate hence can accommodate a large number of signal channels. The FPGA runs at a higher frequency (62.5MHz) to generate the LTP signal and to calculate the envelope of the received signal (sampled at 1MHz). This research elaborates on designing a Quadrature AM (QAM) demodulator on the FPGA for the received signal from the ultrasound and for the detection of the presence of edges/fracture on the test materials. The complete system is designed and optimized in a high level Matlab\Simulink fixed point simulation. Different sub-blocks are then implemented and optimized on the FPGA where efforts are taken to maintain its accuracy with the simulated model. In this work, the technology is applied to determine the location of the Sawbone plate edges based on the received signals which are then passed through a QAM demodulator in the FPGA and a peak detector in the DSP to calculate the echo peak times.","2163-5137;2163-5145","978-1-4244-6392-3978-1-4244-6390-9978-1-4244-6391","10.1109/ISIE.2010.5636657","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5636657","","Field programmable gate arrays;Digital signal processing;Hardware;IIR filters;Quadrature amplitude modulation;Transient analysis;Acoustics","demodulators;design engineering;digital signal processing chips;field programmable gate arrays;fracture;logic design;mathematics computing;peak detectors;plates (structures);quadrature amplitude modulation;ultrasonic applications","real-time fracture detection;low transient pulse;field programmable array design;digital signal processor design;quadrature AM demodulator;ultrasound signal;Matlab\Simulink;Sawbone plate edge;peak detector;echo peak time","","1","28","","","","","","IEEE","IEEE Conferences"
"W-TREMORS, a wireless monitoring system for earthquake engineering","E. Goldoni; P. Gamba","Department of Electronics, University of Pavia, Italy; Department of Electronics, University of Pavia, Italy","2010 IEEE Workshop on Environmental Energy and Structural Monitoring Systems","","2010","","","26","31","Wireless Sensor Network is an emerging technology which can significantly reduce the cost and the time needed to monitor the conditions of civil structures. This work presents W-TREMORS, a new wireless sensing system for high-frequency distributed data acquisition. Our sensor network is based on the low-power and low-data rate standard IEEE 802.15.4, and we used inexpensive hardware. Nevertheless, the system meets requirements imposed by Structural Health Monitoring applications thanks to an efficient use of hardware resources and adopting a novel communication protocol which optimize bandwidth utilization. In addition, we developed a complete software architecture to integrate the Wireless Sensor Network with an existing measurement system. Our prototype WSN was tested through shaking tests in a controlled environment to validate the approach, and to identify problems which should be addressed before using the system on real-world buildings. Preliminary results show that our solution can effectively monitor seismic events providing high reliability and good performances.","","978-1-4244-6276-6978-1-4244-6274-2978-1-4244-6276","10.1109/EESMS.2010.5634185","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5634185","","Wireless sensor networks;Synchronization;Monitoring;Protocols;Wireless communication;Hardware;Sensors","condition monitoring;data acquisition;earthquake engineering;personal area networks;protocols;structural engineering;wireless sensor networks","wireless sensor network;earthquake engineering;wireless monitoring system;W-TREMORS;civil structures;distributed data acquisition;low-power standard;low-data rate standard;IEEE 802.15.4;structural health monitoring;hardware resources;communication protocol;bandwidth utilization;real-world buildings;monitor seismic events;reliability","","4","21","","","","","","IEEE","IEEE Conferences"
"An automatic mesh scheme for IEEE 802.11 wireless mesh networks","H. Tseng; Y. Chuang; S. Sheu","Cloud Computing Center for Mobile Appl., Indu. Tech. Research Institute, Taiwan, R.O.C.; Dept. of Elec. Engineering, Fu Jen Catholic University, Taiwan, R.O.C.; Dept. of Communication Engineering, National Central University, Taiwan, R.O.C.","2011 7th International Wireless Communications and Mobile Computing Conference","","2011","","","47","52","Wireless mesh networks (WMNs) is a promising technology that provides broadband wireless Internet access. It is one of the most important challenges to establish multiple connections among stations (STAs) in WMNs. In this paper, we propose a novel automatic mesh scheme (AMS) of layer 2 to form WMN in IEEE 802.11 wireless networks. AMS establishes multiple connections from a STA to neighbor STAs and determines proper routing path effectively, where three objectives are optimized, i.e., minimizing the average hop count, balancing traffic load and without modifying the communicating procedure in IEEE 802.11 standard. AMS owns self-organization capacity, establishes multiple connections and determines routing path automatically. Our AMS only uses simple software functions without the cost of extra hardware overhead. Simulation results show that the proposed scheme significantly improves the system performance. Importantly, AMS that works very well has been implemented and exhaustively tested in a realistic test-bed.","2376-6492;2376-6506","978-1-4244-9538-2978-1-4244-9539-9978-1-4244-9537","10.1109/IWCMC.2011.5982505","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5982505","wireless mesh networks;IEEE 802.11;layer 2","Logic gates;Wireless communication","Internet;telecommunication network routing;telecommunication standards;telecommunication traffic;wireless LAN;wireless mesh networks","automatic mesh scheme;IEEE 802.11;wireless mesh networks;wireless Internet access;routing path;average hop count;balancing traffic load;self-organization capacity;hardware overhead","","","15","","","","","","IEEE","IEEE Conferences"
"PYNPOINT: An image processing package for finding exoplanets","A. Amara; S. P. Quanz","NA; NA","Monthly Notices of the Royal Astronomical Society","","2012","427","2","948","955","We present the scientific performance results of PYNPOINT, our Python-based software package that uses principal component analysis to detect and estimate the flux of exoplanets in two-dimensional imaging data. Recent advances in adaptive optics and imaging technology at visible and infrared wavelengths have opened the door to direct detections of planetary companions to nearby stars, but image processing techniques have yet to be optimized. We show that the performance of our approach gives a marked improvement over what is presently possible using existing methods such as LOCI. To test our approach, we use real angular differential imaging (ADI) data taken with the adaptive optics-assisted high resolution near-infrared camera NACO at the VLT. These data were taken during the commissioning of the apodizing phase plate (APP) coronagraph. By inserting simulated planets into these data, we test the performance of our method as a function of planet brightness for different positions on the image. We find that in all cases PYNPOINT has a detection threshold that is superior to that given by our LOCI analysis when assessed in a common statistical framework. We obtain our best improvements for smaller inner working angles (IWAs). For an IWA of ∼0.29 arcsec we find that we achieve a detection sensitivity that is a factor of 5 better than LOCI. We also investigate our ability to correctly measure the flux of planets. Again, we find improvements over LOCI, with PYNPOINT giving more stable results. Finally, we apply our package to a non-APP data set of the exoplanet β Pictoris b and reveal the planet with high signal-to-noise. This confirms that PYNPOINT can potentially be applied with high fidelity to a wide range of high-contrast imaging data sets.","0035-8711;1365-2966","","10.1111/j.1365-2966.2012.21918.x","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8183347","methods: data analysis;techniques: image processing;planets and satellites: detection","","","","","1","","","","","","","OUP","OUP Journals & Magazines"
"Challenges, benefits and opportunities in operating cabled ocean observatories: Perspectives from NEPTUNE Canada","C. R. Barnes; M. M. R. Best; F. R. Johnson; L. Pautet; B. Pirenne","NEPTUNE Canada, University of Victoria, PO Box 1700 STN CSC, Victoria British Columbia, V8W2Y2 Canada; NEPTUNE Canada, University of Victoria, PO Box 1700 STN CSC, Victoria British Columbia, V8W2Y2 Canada; NEPTUNE Canada, University of Victoria, PO Box 1700 STN CSC, Victoria British Columbia, V8W2Y2 Canada; NEPTUNE Canada, University of Victoria, PO Box 1700 STN CSC, Victoria British Columbia, V8W2Y2 Canada; NEPTUNE Canada, University of Victoria, PO Box 1700 STN CSC, Victoria British Columbia, V8W2Y2 Canada","2011 IEEE Symposium on Underwater Technology and Workshop on Scientific Use of Submarine Cables and Related Technologies","","2011","","","1","7","The advent of the first cabled ocean observatories, with several others being planned, demonstrates the challenges, benefits and opportunities for ocean science and commercial applications. Examples are drawn primarily from NEPTUNE Canada (NC), which completed installation of the subsea infrastructure and 60 diverse instruments in 2009, with 40 more in 2010, thereby establishing the world's first regional cabled ocean observatory, northeast Pacific Ocean, off British Columbia's coast. Initial data flow started in December 2009. Another 30 instruments will be deployed in 2011-12. Introducing abundant power and high bandwidth communications into a range of ocean environments allows discrimination between short and long-term events, interactive experiments, real time data and imagery, and complex multidisciplinary teams interrogating a vast database over the observatory's 25-year design life. Scientific priorities and observatory node sites were identified through workshops. Alcatel-Lucent Submarine Networks designed, manufactured and installed the 800km backbone cable and five nodes (stepping 10kV DC to 400V DC). Node sites are located at the coast (Folger Passage), continental slope (ODP 889; Barkley Canyon), abyssal plain (ODP 1027), and ocean-spreading ridge (Endeavour), in water depths of 100-2660m. Principal scientific themes are: plate tectonic processes and earthquake dynamics; dynamic processes of seabed fluid fluxes and gas hydrates; regional ocean/climate dynamics and effects on marine biota; deep-sea ecosystem dynamics; and engineering and computational research. The Data Management and Archive System (DMAS) provides controls for the observatory network and transparent access to other data providers using interoperability techniques within a Web 2.0 environment. Users can perform data visualization and analysis on-line with either default or custom processing code, as well as simultaneously interacting with each other. Oceans 2.0 is adding tools to perform software-aided feature detection and classification of sounds in acoustic data streams. New knowledge and scientific interpretations are addressing important science applications of the observatory: ocean/climate change, ocean acidification, recognizing and mitigating natural hazards, non-renewable and renewable natural resources. Challenges are considerable: technical innovations, enlarging the user base, management, funding, maximizing educational/outreach activities. Socio-economic benefits are substantial: not only the transformation of ocean sciences but with many applications in sectors such as sovereignty, security, transportation, data services, and public policy. Opportunities for commercialization of technologies and data services/products are being facilitated by the Centre of Enterprise and Engagement (www.onccee.ca) within Ocean Networks Canada (www.networkscanada.ca) that manages the NC and VENUS observatories (www.neptunecanada.ca; www.uvic.venus.ca). Cabled ocean observatories are transforming the ocean sciences and will result in a progressive wiring of the oceans. They are designed to be expandable in footprint, nodes and instruments, and the range of scientific questions, and to provide facilities for testing technology prototypes. They will provide a wealth of new research opportunities and socio-economic benefits.","","978-1-4577-0164-1978-1-4577-0165-8978-1-4577-0163","10.1109/UT.2011.5774134","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5774134","","Oceans;Observatories;Instruments;Real time systems;Venus;Optical fiber cables;Monitoring","geophysics computing;open systems;submarine cables","cabled ocean observatories;NEPTUNE Canada;ocean science;subsea infrastructure;high bandwidth communications;backbone cable;continental slope;abyssal plain;ocean-spreading ridge;plate tectonic processes;earthquake dynamics;seabed fluid fluxes;gas hydrates;regional ocean dynamics;climate dynamics;data management and archive system;interoperability;Web 2.0;Oceans 2.0;size 800 km;depth 100 m to 2660 m","","1","5","","","","","","IEEE","IEEE Conferences"
"The BREAK|THROUGH Game: A new way to learn innovation practices","M. Flores; N. Leon; H. Aguayo","CEMEX Research Group AG, Processes and IT, Switzerland; Instituto Technologico y de Estudios Superiores de Monterrey (ITESM), Mexico; Instituto Technologico y de Estudios Superiores de Monterrey (ITESM), Mexico","2010 IEEE International Technology Management Conference (ICE)","","2010","","","1","8","This paper will highlight the results of a collaborative research project carried out by CEMEX Research Group and the Instituto Tecnologico and Studios Superiores de Monterrey (ITESM) to develop a new game to share and diffuse Innovation best practices to CEMEX employees. In fact, during the last years there has been a growing interest to learn how the innovation process can be optimized by teaching innovation best practices to CEMEX employees in a different way rather from the usual on-site trainings or virtual courses. As a consequence, taking advantage of the INNOVATEC program from ITESM where researchers and students from this academic institution collaborate with companies to develop new products, a joint project was realized during one year by three professors, four students from different backgrounds together with a team in CEMEX. As a result, a prototype, both paper based and software, for the Breakthrough Game was developed and tested. For its development, the CEMEX LEAD framework to manage collaborative research projects was applied.","","978-1-62748-686","10.1109/ICE.2010.7477032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7477032","Innovation;learning with games;University-Industry collaboration;LEAD Framework","Games;Lead;Decision making","computer based training;educational institutions;industrial training;innovation management;management education;organisational aspects;personnel;teaching","Breakthrough game;innovation practice teaching;CEMEX Research Group;Instituto Tecnologico and Studios Superiores de Monterrey;ITESM;CEMEX employees;innovation process;on-site trainings;virtual courses;INNOVATEC program;product development;CEMEX LEAD framework","","","16","","","","","","IEEE","IEEE Conferences"
"A high quality event capture scheme for WSN-based structural health monitoring","Chao Yang; Jiannong Cao; Xuefeng Liu; Lijun Chen; Daoxu Chen","Department of Computer Science &amp; Technology, Nanjing University, China; Department of Computing, The Hong Kong Polytechnic University, China; Department of Computing, The Hong Kong Polytechnic University, China; Department of Computer Science &amp; Technology, Nanjing University, China; Department of Computer Science &amp; Technology, Nanjing University, China","2012 IEEE Global Communications Conference (GLOBECOM)","","2012","","","622","627","In recent years, there has been an increasing interest in the adoption of wireless sensor networks (WSNs) for structural health monitoring (SHM). However, considering the large amount of sampled data, limited power supply and wireless bandwidth of WSNs, it is generally not possible for sensor nodes to monitor structural condition continuously especially for long-term SHM. From SHM perspective, it is highly desirable to collect data during the occurrence of some certain kinds of events such as earthquakes, large wind, etc, since data collected during these periods are more informative for damage detection purpose. However, these events in SHM occur infrequently and if happen, only last for a very short period of time. To effectively capture these short events using energy-limited wireless sensor nodes is a challenging task that has not been addressed in literature. In this paper, we propose a fast and reliable scheme to capture these short-term events. In terms of hardware, the radio-triggered unit and the vibration-triggered unit are designed in our motes. From software perspective, we develop the event capture scheme to realize fast, reliable and energy-efficient event detection under noisy environment. We mainly study the very first problem of the scheme: sentry selection. The optimization problem is formulated and we show that the problem is a general case of the classical k-center problem. Then we propose a greedy algorithm to solve this problem, and conduct simulation to test the effectiveness of the proposed algorithm.","1930-529X;1930-529X","978-1-4673-0921-9978-1-4673-0920-2978-1-4673-0919","10.1109/GLOCOM.2012.6503182","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6503182","","","condition monitoring;greedy algorithms;structural engineering;wireless sensor networks","event capture scheme;WSN;structural health monitoring;SHM;structural condition monitoring;damage detection;event detection;sentry selection;greedy algorithm","","","15","","","","","","IEEE","IEEE Conferences"
"Concept and design of “flat-plate” CPV module based on Ring-Array Concentrator","S. V. Vasylyev; V. P. Vasylyev; V. A. Sergeev","S.V.V. Technology Innovations, Inc., 1832 Tribute Road, S-te C, Sacramento, CA 95815, USA; S.V.V. Technology Innovations, Inc., 1832 Tribute Road, S-te C, Sacramento, CA 95815, USA; S.V.V. Technology Innovations, Inc., 1832 Tribute Road, S-te C, Sacramento, CA 95815, USA","2010 35th IEEE Photovoltaic Specialists Conference","","2010","","","003087","003091","A high-concentration (400X-1100X) CPV module is investigated in view of developing a planar concentrator PV panel with the form factor approaching that of flat-plate solar panels. The module employs the Ring-Array Concentrator (RAC), a novel point-focus optical design based on the concept of a Reflective Lens<sup>™</sup>(RL). RLs, which can also be referenced to as refractionless lenses, combine the high collecting power of mirrors with the design flexibility and rear-focus operation of lenses. The receiver of the module has passive cooling and incorporates a III-V PV cell and a bypass diode. A number of individual square-aperture modules are arranged into a densely packed planar array to form a rigid concentrator PV panel. The optical layout and general design of the module are presented. The optical performance of the RAC-PV module was modeled using a dedicated software system for simulation, calculation and optimization of RAC (RACsolver<sup>™</sup>). It is shown that a square-shaped focal spot corresponding to the most common shape of concentrator PV cells can be obtained while maintaining acceptable flux uniformity and avoiding secondary optics and associated losses. In order to maximize the acceptance angle, however, it is shown that certain low-loss focus improvers can be employed. We demonstrate that such focus improvers can be either reflective or refractive (with TIR) and they can also be low-profile, virtually lossless due to the minimum number of reflections and simultaneously serve as a cell encapsulant. We present and analyze the ray paths as well as flux maps in the receiver plane with and without focus improvers. The misalignment effects on the optical performance are also discussed. A prototype 3×2 mini-panel has been used to test the optical model. It is concluded that the current optical concept and proposed CPV module design allow for reducing the optical losses at the targeted high and uniform concentrations as well as can result in a panel-like appearance of the product being developed.","0160-8371;0160-8371","978-1-4244-5892-9978-1-4244-5890-5978-1-4244-5891","10.1109/PVSC.2010.5614474","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5614474","","Optical reflection;Optical receivers;Lenses;Optical refraction;Optical design","photovoltaic power systems;solar absorber-convertors","planar concentrator PV panel;flat plate solar panel;ring array concentrator;reflective lens;rear focus operation;passive cooling;bypass diode;optical loss;CPV module design","","1","8","","","","","","IEEE","IEEE Conferences"
"Ankle-angle analysis of drop-foot patients in walking with FES","N. Yang; F. Duan; S. Liu; L. Dai; J. T. C. Tan; B. Xu","Department of Automation, Nankai University; Department of Automation, Nankai University, The Jiangnan Institute of Modern Industry; Department of Automation, Nankai University; Department of Automation, Nankai University; National Institute of Information, 2-1-2 Hitosubashi, Chiyoda-ku, Tokyo, Japan; School of Electronic and Information Engineering South China University of Technology","2012 IEEE International Conference on Robotics and Biomimetics (ROBIO)","","2012","","","831","836","The purpose of this study is to develop a potential method to quantify the effect of functional electrical stimulation (FES) on hemiplegic drop-foot (HDF) patients' movements. This method evaluates the effect of FES by comparing the ankle-angles of movements with and without FES. The change of the ankle-angles is one of the significant characteristics of the recovery of drop-foot. Therefore, this measurement method evaluates FES-assisted walking quantitatively through biomechanical properties of human gait. A group of subjects were asked to walk in the laboratory. Their walking performance was recorded by Vicon motion capture system. For each subject, a static trial and six walking trials were recorded. The data were sent to computer to establish the 3D model by Vicon Nexus software to calculate the ankle-angles of the drop-foot. The changes of the ankle-angles are obvious in the graphs. Statistical data analysis procedure for hypothesis testing was used to get a reliable result. Ankle-angles of movements were calculated to P values (P&lt;;0.05). The results show that FES is effective and the method is reliable. These clinical trial results suggest that using motion capture is a reliable and practicable way to evaluate hemiplegic drop-foot patients' dorsiflexion. This method will probably be used in carrying out a potential optimized stimulation intensity envelope for hemiplegic drop-foot stimulator. And the biomechanics of the ankle joint during the gait cycle also provide a way to the stroke patients to observe their recovery.","","978-1-4673-2127-3978-1-4673-2125-9978-1-4673-2126","10.1109/ROBIO.2012.6491071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6491071","","","bioelectric phenomena;biomedical optical imaging;diseases;gait analysis;legged locomotion;neuromuscular stimulation;patient rehabilitation;statistical analysis","ankle-angle analysis;drop-foot patients;FES;functional electrical stimulation;hemiplegic drop-foot;HDF;biomechanical properties;Vicon motion capture system;statistical data analysis","","1","21","","","","","","IEEE","IEEE Conferences"
"Wireless Sensor Networks: Trends, Power Consumption and Simulators","K. Lahmar; R. Cheour; M. Abid","NA; NA; NA","2012 Sixth Asia Modelling Symposium","","2012","","","200","204","WSN technology involves a set of challenges ranging from determining resource characteristics of the wireless network, when and how to optimise energy and how much processing to do in each node, to principles for management of entire sensor networks and how the data produced by sensor networks can be managed, used and presented. A simulation environment seemed to be then essential, since the test of the hardware and software is hard and difficult. Beyond cost and time savings, the simulator offers the great opportunity to extend the potential and the features of WSN. As there are many simulators, we make an attempt in this paper to specify the important ones related to energy consumption. In fact, we discuss the performance, reliability and usefulness of each of them to better see the disadvantages of these simulators and their strong points to be able to develop a better model.","2376-1164;2376-1172","978-1-4673-1957-7978-0-7695-4730","10.1109/AMS.2012.50","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6243947","Wireless Sensor Networks;energy consumption;energy consumption model;Simulators;NS-2;PowerTOSSIM;MATSNEL;OMNET","Wireless sensor networks;Power demand;Energy consumption;Hardware;Batteries;Wireless communication;Computer architecture","telecommunication network management;telecommunication network reliability;telecommunication power supplies;wireless sensor networks","wireless sensor networks;WSN technology;power consumption;sensor network management principles;simulation environment;energy consumption;cost savings;time savings;dynamic power management techniques;DPM;dynamic voltage-and-frequency scaling;DVFS","","6","19","","","","","","IEEE","IEEE Conferences"
"[Copyright notice]","","","2nd International Symposium on Search Based Software Engineering","","2010","","","iv","iv","The following topics are dealt with: search based software engineering; elementary landscape analysis; mask coding representation; software development process; requirements interaction management; interactive GA; ant colony optimization; search based software testing; C language; mask-coding representation; software sensor; fault detection; resource scheduling; and distributed architecture.","","978-1-4244-8341","10.1109/SSBSE.2010.3","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5634817","","","C language;fault diagnosis;genetic algorithms;program testing;resource allocation;search problems;software engineering","search based software engineering;C language;mask coding representation;software development process;requirement interaction management;ant colony optimization;search based software testing;fault detection;functional hardware-in-the-loop testing;software sensor;resource scheduling;genetic algorithm;distributed architecture","","","","","","","","","IEEE","IEEE Conferences"
"[Front cover]","","","2011 IEEE AUTOTESTCON","","2011","","","c1","c1","The following topics are dealt with: DoD ATS family; vector network analyzer; ATE measurement; synthetic instrumentation; scalable benchtop test system; IEEE P1693 MIPSS standard; RF test platform; business case template; organizational-level testers; intermittent failure detection routine; transportability microcosm; TPS transport optimization; integrated model-based fault detection; data-driven fault detection; fault diagnosis; automotive electric power steering system; nondestructive high voltage low energy intermittent fault location system; electrical ground support equipment development; satellite testing; command and telemetry processing interface; system design issues; connector diagnosis; wiring diagnosis; cable diagnosis; signal generation; software design issues; data analysis; ATS architecture; information management; analog health assessment; test measurement; DoD Initiatives; economic issues; and switching system.","1558-4550;1088-7725;1088-7725","978-1-4244-9363-0978-1-4244-9362-3978-1-4244-9361","10.1109/AUTEST.2011.6058797","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6058797","","","aerospace testing;automatic test equipment;command and control systems;data analysis;failure analysis;fault diagnosis;ground support systems;information management;network analysers;software architecture;software tools;statistical analysis;telemetry;testing;wiring","DoD ATS family;vector network analyzer;ATE measurement;synthetic instrumentation;scalable benchtop test system;IEEE P1693 MIPSS standard;RF test platform;business case template;organizational-level testers;intermittent failure detection routine;transportability microcosm;TPS transport optimization;integrated model-based fault detection;data-driven fault detection;fault diagnosis;automotive electric power steering system;nondestructive high voltage low energy intermittent fault location system;electrical ground support equipment development;satellite testing;command and telemetry processing interface;system design issues;connector diagnosis;wiring diagnosis;cable diagnosis;signal generation;software design issues;data analysis;ATS architecture;information management;analog health assessment;test measurement;DoD Initiatives;economic issues;switching system","","","","","","","","","IEEE","IEEE Conferences"
"[Front cover]","","","2011 First International Conference on Instrumentation, Measurement, Computer, Communication and Control","","2011","","","C1","C1","The following topics are dealt with: testability design and actuator technology; measurement signal processing; automatic test systems; parallel computing; array computing; network computing; fuzzy neural computing; data processing; artificial intelligence; pattern recognition; simulated testing of computer equipment; network connectivity discovery; software bug mining; software module design entities and integration technology; information security and network protection; advanced encoding-decoding; modulation-demodulation and multiplexing; routing technology; software defined radio; radar technology; cognitive radio; wireless network protocol analysis; adaptive, robust, distributed and optimized control; embedded systems; navigation, guidance, control and instrumentation, and; automatic online test and diagnosis for control equipment.","","978-0-7695-4519","10.1109/IMCCC.2011.261","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6154292","","","adaptive control;artificial intelligence;computer equipment testing;decoding;demodulation;distributed control;embedded systems;encoding;fuzzy neural nets;modulation;multiplexing;parallel processing;pattern recognition;program debugging;robust control;security of data;signal processing;software radio","testability design;actuator technology;measurement signal processing;automatic test systems;parallel computing;array computing;network computing;fuzzy neural computing;data processing;artificial intelligence;pattern recognition;simulated testing;computer equipment;network connectivity discovery;software bug mining;software module design;information security;network protection;advanced encoding-decoding;modulation-demodulation;multiplexing;routing technology;software defined radio;radar technology;cognitive radio;wireless network protocol analysis;adaptive control;robust control;distributed control;optimized control","","","","","","","","","IEEE","IEEE Conferences"
"[Title page i]","","","2010 13th Euromicro Conference on Digital System Design: Architectures, Methods and Tools","","2010","","","i","i","The following topics are dealt with: digital system design; system and circuit synthesis; systems and networks on chip; reconfigurable computing; system-level energy optimization of HW/SW embedded systems; multicore systems; fault tolerance in digital system design; embedded-software specification, modeling, verification and test; flexible digital radio; wireless sensor networks; dependability and testing in digital systems; emerging technologies; and software engineering and advanced applications.","","978-1-4244-7839","10.1109/DSD.2010.1","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5615475","","","digital radio;embedded systems;fault tolerance;formal specification;formal verification;hardware-software codesign;multiprocessing systems;network synthesis;network-on-chip;program testing;reconfigurable architectures;wireless sensor networks","digital system design;circuit synthesis;networks on chip;reconfigurable computing;system-level energy optimization;HW/SW embedded systems;multicore systems;fault tolerance;embedded-software specification;software verification;software testing;flexible digital radio;wireless sensor networks;software engineering","","","","","","","","","IEEE","IEEE Conferences"
"[Title page i - Volume 1]","","","2012 International Conference on Computer Science and Electronics Engineering","","2012","1","","i","i","The following topics are dealt with: environmental cost internalization effect analysis; pricing decision theory; international carbon emission trading; device description technology; proportional-derivative controller; chaotic financial system; rolling bearing fault diagnosis; multichannel MAC protocol; 802.11-based wireless mesh networks; Uyghur broadcast news continues speech sensitive-word spotting system; viscoelastic ray tracing simulation; underground cavity stability; spacecraft attitude control; inertia matrix generalized mixed vector variational-like inequalities; stub tooth involute gears; expression recognition; BP neural network; file access security; CFD numerical simulation; exhaust gas flow pattern; hot galvanizing bath; grading steel ball; wear discrete element method; orbit determination; space-based optical observations; Adaboost blob tracking; parallel switching power supply module system; multisource heterogeneous data integration; DSP-based SVPWM vector control system; asynchronous motor scanned document image segmentation; Voronoi diagram; aircraft object recognition model; adaptive threshold Canny edge detection algorithm; data synchronization; open service-aware mobile network API; OAuth based authentication; Open Telco API; online fresh supermarket logistics delivery information system; image encryption algorithm; fast 1-DDCT algorithm; parallel computing; safety education system; virtual reality technology; water quality; aeronautical component repair business optimization; high-speed BiCMOS fully differential operational amplifier; cloud computing; enterprise information system; software development cost; Java multithread technology; wireless sensor network; NC remote video monitoring; military communication network; e-government affair system; particle swarm optimized wavelet neural network; face recognition; grass stem biomechanics; RFID system; Internet of Things; gyroscope test instrument rotating floor DSP control system; and AC synchronous generators.","","978-0-7695-4647-6978-1-4673-0689","10.1109/ICCSEE.2012.81","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6187941","","","access protocols;aircraft maintenance;application program interfaces;asynchronous generators;attitude control;authorisation;backpropagation;biomechanics;cloud computing;computational fluid dynamics;computational geometry;data integration;decision theory;environmental economics;face recognition;fault diagnosis;file organisation;finite element analysis;galvanising;gears;government data processing;image coding;information systems;military communication;neural nets;parallel processing;particle swarm optimisation;pricing;radiofrequency identification;ray tracing;rolling bearings;signal processing;space vehicles;speech recognition;test equipment;three-term control;video surveillance;virtual reality;water quality;wear;wireless mesh networks;wireless sensor networks","environmental cost internalization effect analysis;pricing decision theory;international carbon emission trading;device description technology;proportional-derivative controller;chaotic financial system;rolling bearing fault diagnosis;multichannel MAC protocol;802.11-based wireless mesh networks;Uyghur broadcast news continues speech sensitive-word spotting system;viscoelastic ray tracing simulation;underground cavity stability;spacecraft attitude control;inertia matrix;generalized mixed vector variational-like inequalities;stub tooth involute gears;expression recognition;BP neural network;file access security;CFD numerical simulation;exhaust gas flow pattern;hot galvanizing bath;grading steel ball wear;discrete element method;orbit determination;space-based optical observations;Adaboost blob tracking;parallel switching power supply module system;multisource heterogeneous data integration;DSP-based SVPWM vector control system;asynchronous motor;scanned document image segmentation;Voronoi diagram;aircraft object recognition model;adaptive threshold Canny edge detection algorithm;data synchronization;open service-aware mobile network API;OAuth based authentication;Open Telco API;online fresh supermarket logistics delivery information system;image encryption algorithm;fast 1-DDCT algorithm;parallel computing;safety education system;virtual reality technology;water quality;aeronautical component repair business optimization;high-speed BiCMOS fully differential operational amplifier;cloud computing;enterprise information system;software development cost;Java multithread technology;wireless sensor network;NC remote video monitoring;military communication network;e-government affair system;particle swarm optimized wavelet neural network;face recognition;grass stem biomechanics;RFID system;Internet of Things;gyroscope test instrument rotating floor DSP control system;AC synchronous generators","","","","","","","","","IEEE","IEEE Conferences"
"[Title page i]","","","2010 XXIX International Conference of the Chilean Computer Science Society","","2010","","","i","i","The following topics are dealt with: information retrieval; genetic algorithm; ant colony optimization; data clustering; software engineering; Web service; OCL specification; aspect oriented programming; software testing; software development; software component selection; textual DSL development; software product line; fuzzy rule based automated trading agents; multiagent system; collaborative system; robotic agents; data warehouse; machine learning; automated text binary classification; human-computer interaction; occluded virtual 3D environment; virtual museum exhibition; user interfaces; color image processing; error backpropagation; gray-box neural model; compressed integer encoding; Chilean Internet routes; decision support system; and medium-sized emergencies.","1522-4902;1522-4902","978-1-4577-0073-6978-0-7695-4400","10.1109/SCCC.2010.1","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5750431","","","aspect-oriented programming;backpropagation;data compression;data warehouses;decision support systems;genetic algorithms;groupware;hidden feature removal;image colour analysis;information retrieval;Internet;knowledge based systems;learning (artificial intelligence);man-machine systems;multi-agent systems;pattern clustering;software engineering;solid modelling;text analysis;virtual reality","information retrieval;genetic algorithm;ant colony optimization;data clustering;software engineering;Web service;OCL specification;aspect oriented programming;software testing;software development;software component selection;textual DSL development;software product line;fuzzy rule based automated trading agents;multiagent system;collaborative system;robotic agents;data warehouse;machine learning;automated text binary classification;human-computer interaction;occluded virtual 3D environment;virtual museum exhibition;user interfaces;color image processing;error backpropagation;gray-box neural model;compressed integer encoding;Chilean Internet routes;decision support system;medium-sized emergencies","","","","","","","","","IEEE","IEEE Conferences"
"[Copyright notice]","","","2011 14th Euromicro Conference on Digital System Design","","2011","","","iv","iv","The following topics are dealt with: system and circuit synthesis; systems on chip; networks on chip; reconfigurable computing; system-level energy optimization; HW/SW embedded systems; digital systems testing; fault tolerance; flexible digital radio; arithmetic unit design; embedded-software specification; applications-specific computing; security applications; energy-aware design and multicore systems.","","978-1-4577-1048","10.1109/DSD.2011.3","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6037500","","","circuit testing;digital radio;fault tolerant computing;hardware-software codesign;multiprocessing systems;network synthesis;network-on-chip;security of data","systems on chip;networks on chip;reconfigurable computing;system-level energy optimization;HW/SW embedded systems;digital systems testing;fault tolerance;flexible digital radio;arithmetic unit design;embedded-software specification;applications-specific computing;security applications;energy-aware design;multicore systems","","","","","","","","","IEEE","IEEE Conferences"
"[Front cover]","","","2011 9th IEEE International Conference on ASIC","","2011","","","c1","c1","The following topics are dealt with: VLSI design and circuits; CMOS image sensor; macro-pipelined architecture; timing optimisation; high reliable digital signal processor; differential fault analysis; photoelectric encoder system; multi-standard decoder; logic gate; high speed face detection architecture; power management IC; current mode DC-DC converter; CMOS low-dropout; analogue technique; class-AB CMOS buffer; fully differential amplifier; double charge pump circuit; CMOS charge pump; signal conditioner IC; EEG-ECG signal recording applications; chip multiprocessor; VHDL and system Verilog; high speed CMOS image; homogeneous multiprocessor system-on-chip; quartz crystal microbalance biosensors; NoC-based multicore architecture; wireless communications; robust frame synchronisation; high-efficiect baseband transceiver; test, reliability, and fault tolerance; NAND flash memories; SRAM weak cell testing; immune latch circuit design; electrostatic discharge protection; silicon nanowire technology; software-hardware co-debug platform; GGnMOS ESD protection device; memristor logic, and resistive memory; phase change memory; multidielectric Green function; three-dimensional network-on-chip; low-leakage flip-flop FPGA emulation; dual-output basic logic element; high speed ASIC design; mixed polarity Reed-Muller expression; annealing genetic algorithm; MEMS technology, device, and circuit; buck DC-DC converter; power HEMT; MEMS gyroscope drive; inductorless CMOS LNA; second-order temperature compensation; current source inverter structure; low-kickback preamplifier; large-signal MOSFET modelling; pipelined ADC; wireless transceiver and building blocks; wireless sensor network; reconfigurable low pass filter; and embedded system.","2162-755X;2162-7541;2162-7541","978-1-61284-193-9978-1-61284-192-2978-1-61284-191","10.1109/ASICON.2011.6157042","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6157042","","","analogue-digital conversion;biosensors;circuit optimisation;CMOS image sensors;DC-DC power convertors;electrocardiography;electroencephalography;embedded systems;flip-flops;hardware description languages;logic gates;low-pass filters;micromechanical devices;MOSFET;reliability;transceivers;VLSI","VLSI design;CMOS image sensor;macropipelined architecture;timing optimisation;reliable digital signal processor;differential fault analysis;photoelectric encoder system;multi-standard decoder;logic gate;high speed face detection architecture;power management IC;current mode DC-DC converter;CMOS low-dropout;analogue technique;class-AB CMOS buffer;fully differential amplifier;double charge pump circuit;CMOS charge pump;signal conditioner IC;EEG-ECG signal recording application;chip multiprocessor;VHDL;system Verilog;speed CMOS image;homogeneous multiprocessor system-on-chip;quartz crystal microbalance biosensor;NoC-based multicore architecture;wireless communication;robust frame synchronisation;high-efficiect baseband transceiver;reliability;fault tolerance;NAND flash memory;SRAM weak cell testing;immune latch circuit design;electrostatic discharge protection;silicon nanowire technology;software-hardware co-debug platform;GGnMOS ESD protection device;memristor logic;resistive memory;phase change memory;multidielectric Green function;three-dimensional network-on-chip;low-leakage flip-flop FPGA emulation;dual-output basic logic element;high speed ASIC design;mixed polarity Reed-Muller expression;annealing genetic algorithm;MEMS technology;buck DC-DC converter;power HEMT;MEMS gyroscope drive;inductorless CMOS LNA;second-order temperature compensation;current source inverter structure;low-kickback preamplifier;large-signal MOSFET modelling;pipelined ADC;wireless transceiver;wireless sensor network;reconfigurable low pass filter;embedded system","","","","","","","","","IEEE","IEEE Conferences"
"2011 Table of contents","","","2011 48th ACM/EDAC/IEEE Design Automation Conference (DAC)","","2011","","","1","23","The following topics are dealt with: design methods and trends for automotive architectures; performance and reliability of flash memory systems; system level power management; design for manufacturability; thermal management and modeling for integrated circuits; design and synthesis of biological circuits; sweet streams embedded in multicores; late flow optimization and rectification; routing; 3D applications; embedded multiprocessor software synthesis; analog and mixed-signal design; scaling and security; system level analysis and design; space exploration; validation and test; leakage power optimization; punctual software; system verification; clocks and circuits; model reduction and accelerated extraction; pre-silicon verification methods for post-silicon validation; logic synthesis; 3D IC design; power integrity; circuit reliability; CMOS sensors for biomedical &amp; biological applications; CAD techniques; intelligent simulation; NOC design; and embedded computing.","85-644924;0738-100x;0738-100x","978-1-4503-0636-2978-1-4503-0636-2978-1-4503-0636","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5981913","","","automobiles;biomedical engineering;CAD;CMOS image sensors;design for manufacture;flash memories;integrated circuit design;integrated circuit modelling;integrated circuit reliability;mixed analogue-digital integrated circuits;network routing;thermal management (packaging)","design methods and trends;automotive architectures;performance and reliability;flash memory systems;system level power management;design for manufacturability;thermal management;integrated circuits modeling;integrated circuits design;biological circuits;sweet streams;multicores;late flow optimization;rectification;routing;3D applications;embedded multiprocessor software synthesis;analog design;mixed signal design;scaling and security;system level analysis;space exploration;leakage power optimization;punctual software;system verification;clocks and circuits;model reduction;accelerated extraction;pre silicon verification methods;post silicon validation;logic synthesis;3D IC design;power integrity;circuit reliability;CMOS sensors;biomedical applications;biological applications;CAD techniques;intelligent simulation;NOC design;embedded computing","","","","","","","","","IEEE","IEEE Conferences"
"[Front matter]","","","2011 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)","","2011","","","1","2","The following topics are dealt with: memory; delay test; manufacturing-aware optimizing power; formal verification; GPU programming; behavioral modeling; timing analysis; parallel embedded software; caches; digital print automation; high-level synthesis; sequential synthesis; NoC design; logic switch; nonvolatile memory; clock network synthesis; EDA; brain-inspired architectures; logic level synthesis; analog circuit sizing; system-level power management; asic and system-level communication synthesis.","1558-2434;1092-3152;1092-3152","978-1-4577-1400-9978-1-4577-1399-6978-1-4577-1398","10.1109/ICCAD.2011.6105288","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6105288","","","cache storage;formal verification;graphics processing units;network synthesis;network-on-chip;parallel programming;random-access storage","memory;delay test;manufacturing-aware optimizing power;formal verification;GPU programming;parallel embedded software;caches;NoC design;sequential synthesis;high-level synthesis;brain-inspired architectures;logic level synthesis;system-level communication synthesis","","","","","","","","","IEEE","IEEE Conferences"
"[Front cover]","","","2012 IEEE International Conference on Technology Enhanced Education (ICTEE)","","2012","","","c1","c1","The following topics are dealt with: technology enhanced education; personalized e-learning system; software engineering virtual labs; students e-dissertation; self directed adaptive e-portfolio architecture; computer literacy; educational technology acceleration; ACM-learning architecture; social media; second generation tag cloud; educational Web site; Web based collaborative learning environments; intelligent question answering conversational agent; optimized e-allotment algorithm; educational databases; augmented vocational tools; video lectures; collaborative educational network; online student evaluation; educational informatics; EduPad; tablet based educational system; mobile learning; adaptive test sheet generation; Kloud; virtual elastic knowledge cloud; LabVIEW; mobile vocational education; My Desk; cloud subset preserving mining; ontology supported inference system; profile-learning based differential assessment method; remote virtual laboratory; student learning process tracing; online courses; and virtual learning tool.","","978-1-4577-0726-1978-1-4577-0725-4978-1-4577-0724","10.1109/ICTEE.2012.6208590","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6208590","","","cloud computing;computer aided instruction;data mining;educational administrative data processing;educational courses;groupware;mobile computing;social networking (online);virtual instrumentation","technology enhanced education;personalized e-learning system;software engineering virtual labs;students e-dissertation;self directed adaptive e-portfolio architecture;computer literacy;educational technology acceleration;ACM-learning architecture;social media;second generation tag cloud;educational Web site;Web based collaborative learning environments;intelligent question answering conversational agent;optimized e-allotment algorithm;educational databases;augmented vocational tools;video lectures;collaborative educational network;online student evaluation;educational informatics;EduPad;tablet based educational system;mobile learning;adaptive test sheet generation;Kloud;virtual elastic knowledge cloud;LabVIEW;mobile vocational education;My Desk;cloud subset preserving mining;ontology supported inference system;profile-learning based differential assessment method;remote virtual laboratory;student learning process tracing;online courses;virtual learning tool","","","","","","","","","IEEE","IEEE Conferences"
"[Title page]","","","Perspective Technologies and Methods in MEMS Design","","2011","","","1","1","The following topics are dealt with: various aspects of the advanced microelectronic design, testing and manufacturing including: 1. Analysis, modelling, research and design methods of microsensors and microactuators; 2. Software systems, models, algorithms, methods and strategies of embedded systems design; 3. Issues of testing, verification, reliability and optimization in embedded systems modelling and design; 4. Sensors and actuators systems, nanotechnology; 5. Applications for electron device design; 6. Information Technology. Engineering application of informatics. Engineering education. Computer linguistics for data mining and artificial intelligence.","","978-966-2191-18-9978-1-4577-0639","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5960387","","","design;electronic engineering computing;microactuators;micromechanical devices;microsensors","microelectronic design;manufacturing;testing;microsensors;microactuators;software systems;embedded systems design;nanotechnology;electron device design;computer linguistics;data mining;artificial intelligence","","","","","","","","","IEEE","IEEE Conferences"
