"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication_Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,"Reference Count","Copyright Year","License","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"A non-pheromone based intelligent swarm optimization technique in software test suite optimization","D. J. Mala; M. Kamalapriya; R. Shobana; V. Mohan","Thiagarajar College of Engineering, Madurai, Tamil Nadu, India; Thiagarajar College of Engineering, Madurai, Tamil Nadu, India; Thiagarajar College of Engineering, Madurai, Tamil Nadu, India; Thiagarajar College of Engineering, Madurai, Tamil Nadu, India","2009 International Conference on Intelligent Agent & Multi-Agent Systems","","2009","","","1","5","In our paper, we applied a non-pheromone based intelligent swarm optimization technique namely artificial bee colony optimization (ABC) for test suite optimization. Our approach is a population based algorithm, in which each test case represents a possible solution in the optimization problem and happiness value which is a heuristic introduced to each test case corresponds to the quality or fitness of the associated solution. The functionalities of three groups of bees are extended to three agents namely Search Agent, Selector Agent and Optimizer Agent to select efficient test cases among near infinite number of test cases. Because of the parallel behavior of these agents, the solution generation becomes faster and makes the approach an efficient one. Since, the test adequacy criterion we used is path coverage; the quality of the test cases is improved during each iteration to cover the paths in the software. Finally, we compared our approach with Ant Colony Optimization (ACO), a pheromone based optimization technique in test suite optimization and finalized that, ABC based approach has several advantages over ACO based optimization.","","978-1-4244-4710","10.1109/IAMA.2009.5228055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5228055","Software Testing;Test Optimization;ABC (Artificial Bee Colony) Optimization;Agents;Test adequacy criterion;ACO (Ant Colony Optimization)","Particle swarm optimization;Software testing;Ant colony optimization;System testing;Genetic algorithms;Artificial intelligence;Software quality;Neural networks;Educational institutions;Cost function","optimisation;program testing","nonpheromone based intelligent swarm optimization technique;software test suite optimization;artificial bee colony optimization;population based algorithm;search agent;selector agent;optimizer agent;test adequacy criterion;path coverage;ant colony optimization;pheromone based optimization technique","","12","8","","","","","","IEEE","IEEE Conferences"
"Incorporating varying requirement priorities and costs in test case prioritization for new and regression testing","K. Ramasamy; S. Arul Mary","Department of Information Technology, Bharathidasan Institute of Technology, Bharathidasan University, Trichy-24, Tamilnadu, India; Department of Information Technology, Bharathidasan Institute of Technology, Bharathidasan University, Trichy-24, Tamilnadu, India","2008 International Conference on Computing, Communication and Networking","","2008","","","1","9","Test case prioritization schedules the test cases in an order that increases the effectiveness in achieving some performance goals. One of the most important performance goals is the rate of fault detection. Test cases should run in an order that increases the possibility of fault detection and also that detects the most severe faults at the earliest in its testing life cycle. Test case prioritization techniques have proved to be beneficial for improving regression testing activities. While code coverage based prioritization techniques are found to be taken by most scholars, test case prioritization based on requirements in a cost effective manner has not been taken for study so far. Hence, in this paper, we propose to put forth a model for system level test case prioritization (TCP) from software requirement specification to improve user satisfaction with quality software that can also be cost effective and to improve the rate of severe fault detection. The proposed model prioritizes the system test cases based on the six factors: customer priority, changes in requirement, implementation complexity, usability, application flow and fault impact. The proposed prioritization technique is experimented in three phases with student projects and two sets of industrial projects and the results show convincingly that the proposed prioritization technique improves the rate of severe fault detection.","","978-1-4244-3594-4978-1-4244-3595","10.1109/ICCCNET.2008.4787662","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4787662","Fault severity;Rate of fault detection;Regression testing;Requirement weight;Requirement cost;Test case prioritization;system testing;customer satisfaction","Costs;Fault detection;System testing;Software testing;Software quality;Life testing;Usability;Application software;Software engineering;Job shop scheduling","formal specification;program testing;regression analysis;scheduling;software fault tolerance;software quality","varying requirement priorities;test case prioritization scheduling;regression testing;fault detection;code coverage based prioritization techniques;software requirement specification;user satisfaction;software quality;customer priority;implementation complexity;application flow;fault impact","","5","30","","","","","","IEEE","IEEE Conferences"
"Historical Value-Based Approach for Cost-Cognizant Test Case Prioritization to Improve the Effectiveness of Regression Testing","H. Park; H. Ryu; J. Baik","NA; NA; NA","2008 Second International Conference on Secure System Integration and Reliability Improvement","","2008","","","39","46","Regression testing has been used to support software testing activities and assure the acquirement of appropriate quality through several versions of a software program. Regression testing, however, is too expensive because it requires many test case executions, and the number of test cases increases sharply as the software evolves. In this paper, we propose the Historical Value-Based Approach, which is based on the use of historical information, to estimate the current cost and fault severity for cost-cognizant test case prioritization. We also conducted a controlled experiment to validate the proposed approach, the results of which proved the proposed approachpsilas usefulness. As a result of the proposed approach, software testers who perform regression testing are able to prioritize their test cases so that their effectiveness can be improved in terms of average percentage of fault detected per cost.","","978-0-7695-3266","10.1109/SSIRI.2008.52","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4579792","Test Case Prioritization;Regression Testing;Software Testing;Software Engineering","Testing;Software;Software systems;Fault detection;Software testing;Current measurement;Probability","program testing;quality assurance;software cost estimation;software fault tolerance;software quality","historical value-based approach;cost-cognizant test case prioritization;regression testing;software testing activity;software program quality assurance;software program version;software fault severity detection;software cost estimation","","20","22","","","","","","IEEE","IEEE Conferences"
"A stochastic combinatorial optimization model for test sequence optimization","S. Wang; Y. Ji; S. Yang","Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China","2009 ISECS International Colloquium on Computing, Communication, Control, and Management","","2009","2","","311","315","Traditional test sequence optimization methods for path testing have three problems: 1) fake test results may occur; 2) unnecessary repetitive tests may exist; 2) actual test coverage rate could be low. These problems are because the effects of the faults on the test sequence execution are not considered by traditional methods. To solve these problems, we defined a stochastic combinatorial optimization model in this paper. At the same time, we constructed a multistage dynamic combinatorial optimization model to solve it. In each stage, the stochastic optimization is transferred into a deterministic optimization and the faults are taken as the stage changing factors. Simulation results show that the effective test efficiency and test coverage rate are evidently increased by this stochastic combinatorial optimization model.","2154-9613;2154-963X","978-1-4244-4247","10.1109/CCCM.2009.5270436","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5270436","stochastic combinatorial optimization model;test sequence optimization;test efficiecy;test coverage rate","Stochastic processes;Software testing;System testing;Optimization methods;Mathematical model;Automatic control;Communication system control;Automation;Laboratories;Information science","combinatorial mathematics;optimisation;program testing;software quality;stochastic processes","stochastic combinatorial optimization model;test sequence optimization;path testing;fake test result;unnecessary repetitive test;test sequence execution;multistage dynamic combinatorial optimization model;deterministic optimization;software testing;software quality","","","12","","","","","","IEEE","IEEE Conferences"
"Applying Particle Swarm Optimization to Prioritizing Test Cases for Embedded Real Time Software Retesting","K. H. S. Hla; Y. Choi; J. S. Park","NA; NA; NA","2008 IEEE 8th International Conference on Computer and Information Technology Workshops","","2008","","","527","532","In recent years, complex embedded systems are used in every device that is infiltrating our daily lives. Since most of the embedded systems are multi-tasking real time systems, the task interleaving issues, dead lines and other factors needs software units retesting to follow the subsequence changes. Regression testing is used for the software maintenance that revalidates the old functionality of the software unit. Testing is one of the most complex and time-consuming activities, in which running of all combination of test cases in test suite may require a large amount of efforts. Test case prioritization techniques can take advantage that orders test cases, which attempts to increase effectiveness in regression testing. This paper proposes to use particle swarm optimization (PSO) algorithm to prioritize the test cases automatically based on the modified software units. Regarding to the recent investigations, PSO is a multi-object optimization technique that can find out the best positions of the objects. The goal is to prioritize the test cases to the new best order, based on modified software components, so that test cases, which have new higher priority, can be selected in the regression testing process. The empirical results show that by using the PSO algorithm, the test cases can be prioritized in the test suites with their new best positions effectively and efficiently.","","978-0-7695-3242-4978-0-7695-3239","10.1109/CIT.2008.Workshops.104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4568558","embedded system;real time software system;test case coverage;evolutionary structural testing;regression testing;particle swarm optimization","Particle swarm optimization;Software testing;Embedded software;Aerospace testing;System testing;Embedded system;Embedded computing;Real time systems;Aerospace engineering;Interleaved codes","embedded systems;multiprogramming;particle swarm optimisation;program testing;software maintenance","particle swarm optimization;embedded real time software retesting;complex embedded systems;multitasking real time systems;software units;regression testing;software maintenance;multiobject optimization technique","","14","13","","","","","","IEEE","IEEE Conferences"
"Automatic Test Data Generation Based on Ant Colony Optimization","K. Li; Z. Zhang; W. Liu","NA; NA; NA","2009 Fifth International Conference on Natural Computation","","2009","6","","216","220","Software testing is a crucial measure used to assure the quality of software. Path testing can detect bugs earlier because of it performs higher error coverage. This paper presents a model of generating test data based on an improved ant colony optimization and path coverage criteria. Experiments show that the algorithm has a better performance than other two algorithms and improve the efficiency of test data generation notably.","2157-9555;2157-9563","978-0-7695-3736","10.1109/ICNC.2009.239","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5367156","Automatic Test;Data Generation;Ant Colony Optimization;Path coverage criteria","Automatic testing;Ant colony optimization;Software testing;Educational institutions;Petroleum;Genetic algorithms;Scheduling algorithm;Data engineering;Software measurement;Software quality","automatic test pattern generation;automatic test software;optimisation;program testing;software quality","automatic test data generation;ant colony optimization;software testing;software quality;path testing;path coverage criteria","","8","10","","","","","","IEEE","IEEE Conferences"
"Model for optimizing software testing period using non homogenous poisson process based on cumulative test case prioritization","P. R. Srivastava","Computer Science and Information Systems Group, Birla Institute of Technology and Science (BITS) Pilani-333031, INDIA","TENCON 2008 - 2008 IEEE Region 10 Conference","","2008","","","1","6","Most of the software organizations has trouble when deciding the release dates their product. This difficulty is due to the fact that an under tested software could lead to many bugs propping up at the client side which would in turn lead to expensive bug-fixes and more importantly loss of customer goodwill. On the other hand, testing beyond certain time would lead to loss of revenue to the organization due to the dilution of the early bird advantage. The aim of our paper is optimizes the time and cost of entire software. In this paper we used non homogeneous Poisson process model based on cumulative priority. Our paper also tries to answer when to release any software.","2159-3442;2159-3450","978-1-4244-2408-5978-1-4244-2409","10.1109/TENCON.2008.4766422","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4766422","Non Homogenous Poisson Process;Cumulative Priority;Software Life Cycle Length;Software Release Time;Optimal Testing Policy","Software testing;Computer bugs;Cost function;Logic testing;Stochastic processes;System testing;Computer science;Information systems;Birds;Life testing","higher order statistics;program debugging;program testing;software cost estimation;stochastic processes","software testing period optimization;nonhomogenous Poisson process model;cumulative test case prioritization;software organization;software release date;software bug fixing;software cost optimization;software time optimization","","1","14","","","","","","IEEE","IEEE Conferences"
"Experimental Comparison of Code-Based and Model-Based Test Prioritization","B. Korel; G. Koutsogiannakis","NA; NA","2009 International Conference on Software Testing, Verification, and Validation Workshops","","2009","","","77","84","During regression testing, a modified system needs to beretested using the existing test suite. Since test suites may be very large, developers are interested in detecting faults in the system as early as possible. Test prioritization orders test cases for execution to increase potentially the chances of early fault detection during retesting. Code-based test prioritization methods are based on the source code of the system, whereas model-based test prioritization methods are based on system models. System modeling is a widely used technique to model state-based systems. Models can be used not only during software development but also during testing. In this paper, we briefly overview codebased and model-based test prioritization. In addition, we present an experimental study in which the code based test prioritization and the model-based test prioritization are compared.","","978-0-7695-3671-2978-1-4244-4356","10.1109/ICSTW.2009.45","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4976373","Test prioritization;Model Based Test Prioritization;Code Based Test Prioritization","System testing;Software testing;Fault detection;Modeling;Conferences;Computer science;USA Councils;Programming;Embedded system;Automata","program testing;software fault tolerance","code-based test prioritization;model-based test prioritization;regression testing;fault detection;software development","","10","25","","","","","","IEEE","IEEE Conferences"
"The impact of test case reduction and prioritization on software testing effectiveness","S. ur Rehman Khan; I. ur Rehman; S. U. R. Malik","Department of Computer Science COMSATS Institute of Information Technology Islamabad, Pakistan; Department of Computer Science COMSATS Institute of Information Technology Islamabad, Pakistan; Department of Computer Science COMSATS Institute of Information Technology Islamabad, Pakistan","2009 International Conference on Emerging Technologies","","2009","","","416","421","Software testing is critical but most expensive phase of Software Development Life Cycle (SDLC). Development organizations desire to thoroughly test the software. But this exhaustive testing is impractical due to resource constraints. A large number of test suites are generated using automated tools. But the real challenge is the selection of subset of test cases and/or high order test cases crucial to validate the System Under Test (SUT). Test case reduction and prioritization techniques help test manager to solve this problem at a little cost. In this paper, we investigate their impact on testing process effectiveness using previous empirical studies. The results indicate that these techniques improve the testing effectiveness significantly. At the end, a case study is presented that suggests different useful combinations of these techniques, which are helpful for different testing scenarios.","","978-1-4244-5630-7978-1-4244-5631","10.1109/ICET.2009.5353136","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5353136","Software testing;exhaustive testing;test case reduction;test case prioritization","Software testing;Costs;System testing;Automatic testing;Life testing;Computer science;Information technology;Programming;Standards development;Computer aided software engineering","program testing;software engineering","test case reduction;software testing;software development life cycle;prioritization techniques","","2","26","","","","","","IEEE","IEEE Conferences"
"Prioritizing User-Session-Based Test Cases for Web Applications Testing","S. Sampath; R. C. Bryce; G. Viswanath; V. Kandimalla; A. G. Koru","NA; NA; NA; NA; NA","2008 1st International Conference on Software Testing, Verification, and Validation","","2008","","","141","150","Web applications have rapidly become a critical part of business for many organizations. However, increased usage of Web applications has not been reciprocated with corresponding increases in reliability. Unique characteristics, such as quick turnaround time, coupled with growing popularity motivate the need for efficient and effective Web application testing strategies. In this paper, we propose several new test suite prioritization strategies for Web applications and examine whether these strategies can improve the rate of fault detection for three Web applications and their preexisting test suites. We prioritize test suites by test lengths, frequency of appearance of request sequences, and systematic coverage of parameter-values and their interactions. Experimental results show that the proposed prioritization criteria often improve the rate of fault detection of the test suites when compared to random ordering of test cases. In general, the best prioritization metrics either (1) consider frequency of appearance of sequences of requests or (2) systematically cover combinations of parameter-values as early as possible.","2159-4848","978-0-7695-3127","10.1109/ICST.2008.42","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4539541","web application testing;pair-wise interaction coverage;pair-wise interaction testing;user-session-based testing;test case prioritization","System testing;Fault detection;Software testing;Application software;Frequency;Information systems;Computer science;Robustness;Computer bugs;Web server","Internet;program testing;software fault tolerance","user-session-based test case;Web application testing;application reliability;test suite prioritization;fault detection;prioritization metrics;request sequence","","30","30","","","","","","IEEE","IEEE Conferences"
"Prioritizing test cases for resource constraint environments using historical test case performance data","Y. Fazlalizadeh; A. Khalilian; M. A. Azgomi; S. Parsa","Computer Engineering Department, Iran University of Science and Technology, Tehran, Iran; Computer Engineering Department, Iran University of Science and Technology, Tehran, Iran; Computer Engineering Department, Iran University of Science and Technology, Tehran, Iran; Computer Engineering Department, Iran University of Science and Technology, Tehran, Iran","2009 2nd IEEE International Conference on Computer Science and Information Technology","","2009","","","190","195","Regression testing has been widely used to assure the acquirement of appropriate quality through several versions of a software program. Regression testing, however, is too expensive in that, it requires many test case executions and a large number of test cases. To provide the missing flexibility, researchers introduced prioritization techniques. The aim in this paper has been to prioritize test cases during software regression test. To achieve this, a new equation is presented. The proposed equation considers historical effectiveness of the test cases in fault detection, each test case's execution history in regression test and finally the last priority assigned to the test case. The results of applying the proposed equation to compute the priority of regression test cases for two benchmarks, known as Siemens suite and Space program, demonstrate the relatively faster fault detection in resource and time constrained environments.","","978-1-4244-4519-6978-1-4244-4520","10.1109/ICCSIT.2009.5234968","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5234968","software regression test;test case prioritization;history-based prioritization;historical fault detection effectiveness","Software testing;Equations;Fault detection;Time factors;Software performance;Life testing;History;Data engineering;Information analysis;Performance analysis","program testing;regression analysis;software fault tolerance;software quality","resource constraint environment;historical test case performance data;software program quality;prioritization techniques;software regression test;Space program;Siemens suite;fault detection","","6","18","","","","","","IEEE","IEEE Conferences"
"Jtop: Managing JUnit Test Cases in Absence of Coverage Information","L. Zhang; J. Zhou; D. Hao; L. Zhang; H. Mei","NA; NA; NA; NA; NA","2009 IEEE/ACM International Conference on Automated Software Engineering","","2009","","","677","679","Test case management may make the testing process more efficient and thus accelerate software delivery. With the popularity of using JUnit for testing Java software, researchers have paid attention to techniques to manage JUnit test cases in regression testing of Java software. Typically, most existing test case management tools are based on the coverage information. However, coverage information may need extra efforts to obtain. In this paper, we present an Eclipse IDE plug-in (named Jtop) for managing JUnit test cases in absence of coverage information. Jtop statically analyzes the program under test and its corresponding JUnit test cases to perform the following management tasks: regression test case selection, test suite reduction and test case prioritization. Furthermore, Jtop also enables the programmer to manually manipulate test cases through a graphical user interface.","1938-4300","978-1-4244-5259-0978-0-7695-3891","10.1109/ASE.2009.22","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5431709","regression test case selection;test case prioritization;test suite reduction","Software testing;Electronic equipment testing;Automatic testing;Graphical user interfaces;Displays;Java;Conference management;Technology management;Engineering management;Software engineering","graphical user interfaces;Java;program testing;software management","Jtop;managing JUnit test cases;coverage Information;test case management;software delivery;Java software;test case selection;test suite reduction;test case prioritization;graphical user interface;software testing","","2","10","","","","","","IEEE","IEEE Conferences"
"Test Case Prioritization Based on Varying Testing Requirement Priorities and Test Case Costs","X. Zhang; C. Nie; B. Xu; B. Qu","Southeast University, China; Southeast University, China; Southeast University, China; Southeast University, China","Seventh International Conference on Quality Software (QSIC 2007)","","2007","","","15","24","Test case prioritization is an effective and practical technique in regression testing. It schedules test cases in order of precedence that increases their ability to meet some performance goals, such as code coverage, rate of fault detection. In previous work, the test case prioritization techniques and metrics usually assumed that testing requirement priorities and test case costs are uniform. In this paper, basing on varying testing requirement priorities and test case costs, we present a new, general test case prioritization technique and an associated metric. The case study illustrates that the rate of ""units-oftesting-requirement-priority-satisfied- per-unit-test-case-cost"" can be increased, and then the testing quality and customer satisfaction can be improved.","1550-6002;2332-662X","0-7695-3035-4978-0-7695-3035","10.1109/QSIC.2007.4385476","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4385476","","Costs;Software testing;Fault detection;Software quality;Life testing;Programming;Feedback;Computer science;Processor scheduling;Customer satisfaction","program testing;software reliability","test case prioritization;varying testing requirement priorities;test case costs;regression testing;fault detection;code coverage;testing requirement priorities;customer satisfaction;testing quality;software testing","","14","23","","","","","","IEEE","IEEE Conferences"
"How Can Optimization Models Support the Maintenance of Component-Based Software?","V. Cortellessa; P. Potena","NA; NA","2009 1st International Symposium on Search Based Software Engineering","","2009","","","97","100","The maintenance phase of software systems is ever more increasing its incidence, in terms of effort, to the whole software lifecycle. Therefore the introduction of automated techniques that can help software maintainers to take decision on the basis of quantitative evaluation would be a suitable phenomenon.Search-based techniques offer today a very promising view on the automation of searching processes in the software engineering domain. Component-based software is a very interesting paradigm to apply such type of techniques, for example for component selection. In this paper we introduce optimization techniques to manage the problem of failures at maintenance time. In particular,we introduce two approaches that provide maintenance actions to be taken in order to overcome system failures in case of monitored and non-monitored software systems.","","978-0-7695-3675","10.1109/SSBSE.2009.22","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5033187","Component-Based Software;Software Maintenance;Optimization model;Software Cost;Software Reliability","Software maintenance;Software systems;Condition monitoring;Software engineering;Automation;Cost function;Software reliability;Fault detection;Testing;Performance evaluation","object-oriented programming;optimisation;software maintenance;software reliability;system monitoring;system recovery","optimization model;component-based software;software maintenance;software lifecycle;quantitative evaluation;searching process;software engineering domain;system failure;software system monitor;software reliability","","3","11","","","","","","IEEE","IEEE Conferences"
"Test Case Prioritization for Multiple Processing Queues","B. Qu; C. Nie; B. Xu","NA; NA; NA","2008 International Symposium on Information Science and Engineering","","2008","2","","646","649","Test case prioritization is an effective technique that helps to increase the rate of fault detection or code coverage in regression testing. However, all existing methods can only prioritize test cases to a single queue. Once there are two or more machines that participate in testing, all exiting techniques are not applicable any more. To extend the prioritization methods to parallel scenario, this paper defines the prioritization problem in such scenario and applies the task scheduling method to prioritization algorithms to help partitioning a test suite into multiple prioritized subsets. Besides, this paper also discusses the limitation of previous metrics and proposes a new measure of effectiveness of prioritization methods in a parallel scenario. Finally, a case study is performed to illustrate the algorithms and metrics presented in this article.","2160-1283;2160-1291","978-0-7695-3494-7978-1-4244-2727","10.1109/ISISE.2008.106","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4732476","Regression testing;parallel scenario;metrics;test case prioritization","Partitioning algorithms;Scheduling algorithm;Information science;Computer science;Fault detection;Life testing;Software testing;Performance evaluation;Education;Feedback","parallel processing;program diagnostics;program testing;scheduling;software metrics;software prototyping","multiple processing queue;test case prioritization;fault detection;regression testing;parallel scenario;task scheduling;software metrics;software life cycle","","","10","","","","","","IEEE","IEEE Conferences"
"An Optimization Method of Test Suite in Regression Test Model","W. Ding; J. Kou; K. Li; Z. Yang","NA; NA; NA; NA","2009 WRI World Congress on Software Engineering","","2009","4","","180","183","Original test cases should be reused and new ones should be supplemented in regression test. For optimizing test suite, pair-wise combination test cases generating method is adopted in this paper, which was realized by ant colony algorithm with monolepsis diagnostic. When original and new generated test suite was united, improved greedy arithmetic was adopted to reduce test suite and random off-trap strategy was introduced. The test case study result shows the method can reduce the scale of test suite effectively and decrease the regression test cost.","","978-0-7695-3570","10.1109/WCSE.2009.311","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5319529","regression test;test suite;ant colony optimization algorithm;random off-trap strategy","Optimization methods;Software testing;Arithmetic;System testing;Software quality;Ant colony optimization;Cost function;Software engineering;Petroleum;Algebra","greedy algorithms;optimisation;program testing;regression analysis","optimization method;test suite;regression test model;pair-wise combination test;ant colony algorithm;monolepsis diagnostic;greedy arithmetic;random off-trap strategy","","3","8","","","","","","IEEE","IEEE Conferences"
"Prioritizing JUnit test cases in absence of coverage information","L. Zhang; J. Zhou; D. Hao; L. Zhang; H. Mei","Key Laboratory of High Confidence Software Technologies, Ministry of Education, Institute of Software, School of Electronics Engineering and Computer Science, Peking University, Beijing, 100871, P. R. China; Key Laboratory of High Confidence Software Technologies, Ministry of Education, Institute of Software, School of Electronics Engineering and Computer Science, Peking University, Beijing, 100871, P. R. China; Key Laboratory of High Confidence Software Technologies, Ministry of Education, Institute of Software, School of Electronics Engineering and Computer Science, Peking University, Beijing, 100871, P. R. China; Key Laboratory of High Confidence Software Technologies, Ministry of Education, Institute of Software, School of Electronics Engineering and Computer Science, Peking University, Beijing, 100871, P. R. China; Key Laboratory of High Confidence Software Technologies, Ministry of Education, Institute of Software, School of Electronics Engineering and Computer Science, Peking University, Beijing, 100871, P. R. China","2009 IEEE International Conference on Software Maintenance","","2009","","","19","28","Better orderings of test cases can detect faults in less time with fewer resources, and thus make the debugging process earlier and accelerate software delivery. As a result, test case prioritization has become a hot topic in the research of regression testing. With the popularity of using the JUnit testing framework for developing Java software, researchers also paid attention to techniques for prioritizing JUnit test cases in regression testing of Java software. Typically, most of them are based on coverage information of test cases. However, coverage information may need extra costs to acquire. In this paper, we propose an approach (named Jupta) for prioritizing JUnit test cases in absence of coverage information. Jupta statically analyzes call graphs of JUnit test cases and the software under test to estimate the test ability (TA) of each test case. Furthermore, Jupta provides two prioritization techniques: the total TA based technique (denoted as JuptaT) and the additional TA based technique (denoted as JuptaA). To evaluate Jupta, we performed an experimental study on two open source Java programs, containing 11 versions in total. The experimental results indicate that Jupta is more effective and stable than the untreated orderings and Jupta is approximately as effective and stable as prioritization techniques using coverage information at the method level.","1063-6773","978-1-4244-4897-5978-1-4244-4828","10.1109/ICSM.2009.5306350","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306350","","Software testing;Electronic equipment testing;Java;Software debugging;Fault detection;Costs;Laboratories;Educational technology;Computer science education;Computer science","Java;program testing;software engineering","JUnit test cases prioritization;coverage information;Java software development;debugging process;regression testing;test ability estiimation;Jupta approach;TA prioritization techniques","","14","22","","","","","","IEEE","IEEE Conferences"
"Prioritized Test Generation Strategy for Pair-Wise Testing","J. Gao; J. Zhu","NA; NA","2009 15th IEEE Pacific Rim International Symposium on Dependable Computing","","2009","","","99","102","Pair-wise testing is widely used to detect faults in software systems. In many applications where pair-wise testing is needed, the whole test set can not be run completely due to time or budget constraints. In these situations, it is essential to prioritize the tests. In this paper, we drive weight for each value of each parameter, and adapt UWA algorithm to generate an ordered pair-wise coverage test suite. UWA algorithm is to accord weights set for each value of each parameter of the system, then produce ordered pair-wise coverage test set for having generated but unordered one. Finally, a greedy algorithm is adopted to prioritize generated pair-wise coverage test set with driven weights, so that whenever the testing is interrupted, interactions deemed, most important are tested.","","978-0-7695-3849","10.1109/PRDC.2009.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5368244","pair-wise testing;test prioritization;weights","System testing;Software testing;Fault detection;Computer science;Greedy algorithms;Software systems;Application software;Time factors;Costs;Fault diagnosis","program testing","prioritized test generation;pair-wise testing;software fault detection;UWA algorithm;updating weight algorithm;ordered pair-wise coverage test","","","16","","","","","","IEEE","IEEE Conferences"
"An Improved AETG Test Suite Optimization Method Based on Regressing Test Model","K. Li; Z. Yang","NA; NA","2008 International Conference on Computer Science and Software Engineering","","2008","2","","52","55","Optimizing test suite can reduce the cost of time and resources, and improve the efficiency of regression test when test cases are generated. In this paper, the effect of factorspsila combination in systems is considered sufficiently, and an improved AETG pair-wise cover technique is proposed. The method of schema matching is used in the process of selecting level values of factors, so that the quantity of redundant test cases and the selecting time can be decreased. The method can generate fewer test cases sooner than the original one by performance analyses, and achieve the aim of optimizing new generated test suite.","","978-0-7695-3336","10.1109/CSSE.2008.540","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721999","regression test;test suite optimization;pair-wise;AETG;schema matching","Optimization methods;Software testing;System testing;Software quality;Educational institutions;Petroleum;Electronic mail;Cost function;Computer science;Software engineering","optimisation;program testing","AETG test suite optimization;regressing test model;AETG pair-wise cover technique;schema matching","","","7","","","","","","IEEE","IEEE Conferences"
"Applying Interface-Contract Mutation in Regression Testing of Component-Based Software","S. Hou; L. Zhang; T. Xie; H. Mei; J. Sun","Institute of Software, School of Electronics Engineering and Computer Science, Peking University, Beijing, 100871, China; Key laboratory of High Confidence Software Technologies (Peking University), Ministry of Education, Beijing, 100871, China, houss@sei.pku.edu.cn; Institute of Software, School of Electronics Engineering and Computer Science, Peking University, Beijing, 100871, China; Key laboratory of High Confidence Software Technologies (Peking University), Ministry of Education, Beijing, 100871, China, zhanglu@sei.pku.edu.cn; Department of Computer Science, North Carolina State University, Raleigh, NC 27695, xie@csc.ncsu.edu; Institute of Software, School of Electronics Engineering and Computer Science, Peking University, Beijing, 100871, China; Key laboratory of High Confidence Software Technologies (Peking University), Ministry of Education, Beijing, 100871, China, meih@sei.pku.edu.cn; Institute of Software, School of Electronics Engineering and Computer Science, Peking University, Beijing, 100871, China; Key laboratory of High Confidence Software Technologies (Peking University), Ministry of Education, Beijing, 100871, China, sjs@sei.pku.edu.cn","2007 IEEE International Conference on Software Maintenance","","2007","","","174","183","Regression testing, which plays an important role in software maintenance, usually relies on test adequacy criteria to select and prioritize test cases. However, with the wide use and reuse of black-box components, such as reusable class libraries and COTS components, it is challenging to establish test adequacy criteria for testing software systems built on components whose source code is not available. Without source code or detailed documents, the misunderstanding between the system integrators and component providers has become a main factor of causing faults in component-based software. In this paper, we apply mutation on interface contracts, which can describe the rights and obligations between component users and providers, to simulate the faults that may occur in this way of software development. The mutation adequacy score for killing the mutants of interface contracts can serve as a test adequacy criterion. We performed an experimental study on three subject systems to evaluate the proposed approach together with four other existing criteria. The experimental results show that our adequacy criterion is helpful for both selecting good-quality test cases and scheduling test cases in an order of exposing faults quickly in regression testing of component-based software.","1063-6773","978-1-4244-1255-6978-1-4244-1256","10.1109/ICSM.2007.4362630","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4362630","","Genetic mutations;Software testing;Contracts;Electronic equipment testing;Software maintenance;System testing;Computer science;Software libraries;Software systems;Programming","object-oriented programming;program testing;software maintenance;software packages;software quality","interface-contract mutation;regression testing;component-based software;software maintenance;black-box component;COTS component;software system testing;source code;software development","","7","35","","","","","","IEEE","IEEE Conferences"
"Test Optimization Using Software Virtualization","S. Seetharaman; B. V. S. Krishna Murthy","Alcatel; NA","IEEE Software","","2006","23","5","66","69","Virtualization of operating systems has recently become a hype, although the concept is very old. Virtualization lets us group resources logically and thus abstract from the dependencies that physical implementations create. For operating systems, virtualization lets us reduce resource constraints and expenses for a multitude of interacting hardware and operating systems. This is helpful in scenarios with many such dependencies, such as testing. This article briefly introduces virtualization from a tester's perspective. It summarizes experiences and compares different virtualization approaches: architectural, logical, and physical. Only the test itself remains a necessary reality","0740-7459;1937-4194","","10.1109/MS.2006.143","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1687863","software testing;virtual machines;open source","Software testing;Hardware;Virtual manufacturing;Platform virtualization;Operating systems;Voice mail;Personal communication networks;Test equipment;Resource virtualization;Virtual machining","operating systems (computers);optimisation;program testing;virtual machines","software test optimization;software virtualization;operating system","","9","3","","","","","","IEEE","IEEE Journals & Magazines"
"Combinatorial Interaction Regression Testing: A Study of Test Case Generation and Prioritization","X. Qu; M. B. Cohen; K. M. Woolf","Department of Computer Science and Engineering, University of Nebraska-Lincoln, xqu@cse.unl.edu; Department of Computer Science and Engineering, University of Nebraska-Lincoln, myra@cse.unl.edu; Department of Computer Science and Engineering, University of Nebraska-Lincoln, kwoolf@cse.unl.edu","2007 IEEE International Conference on Software Maintenance","","2007","","","255","264","Regression testing is an expensive part of the software maintenance process. Effective regression testing techniques select and order (or prioritize) test cases between successive releases of a program. However, selection and prioritization are dependent on the quality of the initial test suite. An effective and cost efficient test generation technique is combinatorial interaction testing, CIT, which systematically samples all t-way combinations of input parameters. Research on CIT, to date, has focused on single version software systems. There has been little work that empirically assesses the use of CIT test generation as the basis for selection or prioritization. In this paper we examine the effectiveness of CIT across multiple versions of two software subjects. Our results show that CIT performs well in finding seeded faults when compared with an exhaustive test set. We examine several CIT prioritization techniques and compare them with a re-generation/prioritization technique. We find that prioritized and re-generated/prioritized CIT test suites may find faults earlier than unordered CIT test suites, although the re-generated/prioritized test suites sometimes exhibit decreased fault detection.","1063-6773","978-1-4244-1255-6978-1-4244-1256","10.1109/ICSM.2007.4362638","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4362638","","Software testing;Fault detection;System testing;Life testing;Software systems;Computer science;Software maintenance;Costs;Performance evaluation;Software quality","configuration management;program diagnostics;program testing;software fault tolerance;software maintenance;software quality","combinatorial interaction regression testing;test case generation;software maintenance;test case prioritization;software quality;software version;fault detection","","44","19","","","","","","IEEE","IEEE Conferences"
"Reducing Field Failures in System Configurable Software: Cost-Based Prioritization","H. Srikanth; M. B. Cohen; X. Qu","NA; NA; NA","2009 20th International Symposium on Software Reliability Engineering","","2009","","","61","70","System testing of configurable software is an expensive and resource constrained process. Insufficient testing often leads to escaped faults in the field where failures impact customers and are costly to repair. Prior work has shown that it is possible to efficiently sample configurations for testing using combinatorial interaction testing, and to prioritize these configurations to increase the rate of early fault detection. The underlying assumption to date has been that there is no added complexity to configuring a system level environment over a user configurable one; i.e. the time required to setup and test each individual configuration is nominal. In this paper we examine prioritization of system configurable software driven not only by fault detection but also by the cost of configuration and setup time that moving between different configurations incurs. We present a case study on two releases of an enterprise software system using failures reported in the field. We examine the most effective prioritization technique and conclude that (1) using failure history of configurations can improve the early fault detection rate, but that (2) we must consider fault detection rate over time, not by the number of configurations tested. It is better to test related configurations which incur minimal setup time than to test fewer, more diverse configurations.","1071-9458;2332-6549","978-1-4244-5375-7978-0-7695-3878","10.1109/ISSRE.2009.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5362084","configuration prioritization;regression testing;combinatorial interaction testing","Software systems;System testing;Costs;Fault detection;Software testing;Databases;Operating systems;Environmental economics;Programming;Application software","electronic commerce;program testing;software fault tolerance","system configurable software;cost-based prioritization;configurable software system testing;resource constrained process;combinatorial interaction testing;fault detection;system level environment;enterprise software system;prioritization technique","","14","20","","","","","","IEEE","IEEE Conferences"
"Testing Processes in Business-Critical Chain Software Lifecycle","G. R. N.","NA","2009 WRI World Congress on Software Engineering","","2009","4","","238","242","The business-critical chain lifecycle is an agile software development lifecycle that aims at aligning the software project deliverables to attain the business objectives based on business priorities. The traditional software development projects work on the assumption that all `equal effort consumers' would be treated equally and worked upon. The agile methodology ensures that the delivery cycles are reduced thus introducing agility in the way business is supported by underlying technologies. The proposed lifecycle model introduces new variables pertaining to the business value generation each finished piece of code would produce. Hence the software project processes have to be modified to cater to it. Even the usual agile lifecycle testing strategies need to be modified to suit the proposed model. The test plans, test estimations, test resource management, quality control, regression plans and automation road map and plans have to be customized to cater to the new life cycle model. The secondary project management activities such as risk management, procurement management, etc also may to be modified with respect to the testing processes. My paper aims at using the critical chain principles and proposes a software lifecycle model that can cater to business priorities and aligning the testing processes not only to development cycles but also to the actual business value created. In this paper I would take a case study and compare and contrast when project uses the regular models and this new model. I would also provide guidelines to use this lifecycle model and modify regular project management activities to cater to the new model with emphasis on the testing processes. The paper would try to provide the ideal scenarios; in project teams, in consulting firms and in new customers and expectations; where such a model could provide high impact on the way consulting companies can do successful projects and creating more value to the customers.","","978-0-7695-3570","10.1109/WCSE.2009.424","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5319542","","Life testing;Software testing;Automatic testing;Project management;Programming;Risk management;Life estimation;Resource management;Quality control;Automation","business data processing;life testing;program testing;project management;software quality","testing processes;business-critical chain software lifecycle;agile software development lifecycle;business priorities;software development projects;business value generation;software project processes;agile lifecycle testing strategies;test plans;test estimations;test resource management;quality control;regression plans;automation road map;secondary project management","","","1","","","","","","IEEE","IEEE Conferences"
"Building Prioritized Pairwise Interaction Test Suites with Ant Colony Optimization","X. Chen; Q. Gu; X. Zhang; D. Chen","NA; NA; NA; NA","2009 Ninth International Conference on Quality Software","","2009","","","347","352","Interaction testing offers a stable cost-benefit ratio in identifying faults. But in many testing scenarios, the entire test suite cannot be fully executed due to limited time or cost. In these situations, it is essential to take the importance of interactions into account and prioritize these tests. To tackle this issue, the biased covering array is proposed and the Weighted Density Algorithm (WDA) is developed. To find a better solution, in this paper we adopt ant colony optimization (ACO) to build this prioritized pairwise interaction test suite (PITS). In our research, we propose four concrete test generation algorithms based on Ant System, Ant System with Elitist, Ant Colony System and Max-Min Ant System respectively. We also implement these algorithms and apply them to two typical inputs and report experimental results. The results show the effectiveness of these algorithms.","1550-6002;2332-662X","978-1-4244-5913-1978-1-4244-5912-4978-0-7695-3828","10.1109/QSIC.2009.52","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381411","Software testing;prioritized interaction testing;ant colony optimization","Ant colony optimization;Software testing;Software quality;Concrete;System testing;Partitioning algorithms;Laboratories;Computer science;Fault diagnosis;Costs","minimax techniques;program testing","prioritized pairwise interaction test suites;ant colony optimization;biased covering array;weighted density algorithm;ant system with elitist;ant colony system;max-min ant system","","12","19","","","","","","IEEE","IEEE Conferences"
"Towards Destructive Software Testing","K. Akingbehin","University of Michigan-Dearborn","5th IEEE/ACIS International Conference on Computer and Information Science and 1st IEEE/ACIS International Workshop on Component-Based Software Engineering,Software Architecture and Reuse (ICIS-COMSAR'06)","","2006","","","374","377","Traditional software testing checks to see if a software product meets specifications. This generally involves testing to see if the software performs all the functions called for in the software requirements specifications (SRS). In contrast, this work-in-progress paper proposes a testing paradigm that does not have this objective. The proposed testing paradigm tests to see if a software product exhibits proper behavior when subject to improper usage or improper input. For lack of a more descriptive name and in compliance with similar testing performed on hardware systems, the new paradigm is called ""destructive testing"". As presented in this paper, destructive testing does not replace conventional testing, rather destructive testing supplements conventional testing (calls for additional testing beyond conventional testing). The paper discusses other uses of the term ""destructive testing"" as applied to software systems. Conventional testing techniques are ranked based on applicability to destructive testing. Techniques of incorporating destructive testing requirements into the SRS are proposed, the need and rational for destructive testing is discussed, and ongoing and future work in destructive testing is outlined","","0-7695-2613","10.1109/ICIS-COMSAR.2006.83","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1652020","software;engineering;testing;destructive","Software testing;System testing;Performance evaluation;Automobiles;Software performance;Hardware;Information science;Robustness;Software quality;Accidents","formal specification;program testing","destructive software testing;software requirement specifications;software product behavior;hardware systems","","1","12","","","","","","IEEE","IEEE Conferences"
"Using String Distances for Test Case Prioritisation","Y. Ledru; A. Petrenko; S. Boroday","NA; NA; NA","2009 IEEE/ACM International Conference on Automated Software Engineering","","2009","","","510","514","Test case prioritisation aims at finding an ordering which enhances a certain property of an ordered test suite. Traditional techniques rely on the availability of code or a specification of the program under test. In this paper, we propose to use string distances on the text of test cases for their comparison and elaborate a prioritisation algorithm. Such a prioritisation does not require code and can be useful for initial testing and in cases when code is difficult to instrument. We also briefly report on preliminary results of an experiment where the proposed prioritisation technique was compared with random permutations and four classical string distance metrics were evaluated.","1938-4300","978-1-4244-5259-0978-0-7695-3891","10.1109/ASE.2009.23","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5431745","Software Engineering;Testing and Debugging;Testing tools;Test case prioritisation;String distance","Software testing;Hamming distance;Automatic testing;Application software;Software engineering;Instruments;Computer industry;Software debugging;Software tools;Costs","program testing;software tools","string distance metrics;test case prioritisation algorithm;ordered test suite;program testing;random permutations","","11","5","","","","","","IEEE","IEEE Conferences"
"On the Use of Mutation Faults in Empirical Assessments of Test Case Prioritization Techniques","Hyunsook Do; G. Rothermel","Department of Computer Science and Engineering, University of Nebraska-Lincoln, Lincoln, NE 68588-0115; Department of Computer Science and Engineering, University of Nebraska-Lincoln, Lincoln, NE 68588-0115","IEEE Transactions on Software Engineering","","2006","32","9","733","752","Regression testing is an important activity in the software life cycle, but it can also be very expensive. To reduce the cost of regression testing, software testers may prioritize their test cases so that those which are more important, by some measure, are run earlier in the regression testing process. One potential goal of test case prioritization techniques is to increase a test suite's rate of fault detection (how quickly, in a run of its test cases, that test suite can detect faults). Previous work has shown that prioritization can improve a test suite's rate of fault detection, but the assessment of prioritization techniques has been limited primarily to hand-seeded faults, largely due to the belief that such faults are more realistic than automatically generated (mutation) faults. A recent empirical study, however, suggests that mutation faults can be representative of real faults and that the use of hand-seeded faults can be problematic for the validity of empirical results focusing on fault detection. We have therefore designed and performed two controlled experiments assessing the ability of prioritization techniques to improve the rate of fault detection of test case prioritization techniques, measured relative to mutation faults. Our results show that prioritization can be effective relative to the faults considered, and they expose ways in which that effectiveness can vary with characteristics of faults and test suites. More importantly, a comparison of our results with those collected using hand-seeded faults reveals several implications for researchers performing empirical studies of test case prioritization techniques in particular and testing techniques in general","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.92","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1707670","Regression testing;test case prioritization;program mutation;empirical studies.","Genetic mutations;Computer aided software engineering;Fault detection;Software testing;System testing;Life testing;Software maintenance;Performance evaluation;Costs;Software measurement","program diagnostics;program testing","regression testing;software life cycle;test case prioritization technique;fault detection;hand-seeded fault;mutation fault","","97","41","","","","","","IEEE","IEEE Journals & Magazines"
"Application of software testability measurement model SPM to software testing","P. Xu; Y. Wang; Z. Shen","Department of System Engineering Beihang University Beijing, China; Department of System Engineering Beihang University Beijing, China; Department of System Engineering Beihang University Beijing, China","2009 8th International Conference on Reliability, Maintainability and Safety","","2009","","","733","737","Towards the contradiction in software testing between requirements of high efficiency and limited resources, research on application of software testability measurement model SPM to software testing is conducted. Trough analyzing execution probability(E), infection probability(I), propagation probability(P) and testability value(T) in determining the quantity of software test case, software testing intensity, regression testing intensity, optimizing software testing sequence and analyzing performance bottleneck, an application approach of SPM model to software testing is proposed. By validation, it is proved that it is effective and can improve testing efficiency.","","978-1-4244-4903-3978-1-4244-4905","10.1109/ICRMS.2009.5270093","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5270093","software testability;measurement;software testing;SPM;PIE","Software testing;Application software;Software measurement;Scanning probe microscopy;Object oriented modeling;System testing;Performance analysis;Relays;Probability;Systems engineering and theory","probability;program testing;regression analysis","software testability measurement model;execution probability;infection probability;propagation probability;testability value;software test case;software testing intensity;regression testing intensity;optimizing software testing sequence","","","9","","","","","","IEEE","IEEE Conferences"
"Research and Establishment of Quality Cost Oriented Software Testing Model","Y. Yuan; S. Gu","Microprocessor R&D Center, Computer Architecture Group of Computer Science Department, PKUING UNIVERSITY, Beijing, 100871, P.R. China. yuanyuyu@mprc.pku.edu.cn; College of Software Engineering, Graduate School of the Chinese Academy of Sciences, Beijing, 100049, P. R. China. Shen.GU@gemplus.com, Shen_GU@hotmail.com","2006 Canadian Conference on Electrical and Computer Engineering","","2006","","","2410","2415","In a competitive market, quality is essential to the software. Software test plays an important role during the software-developing period. Any saving on software test will greatly reduce the total cost of software. The key point of this paper is to build a software test process with the cost control management and to make tradeoff between the quality and the cost. Typical software testing model are analyzed and compared. A novel QC 3-D test model is introduced. The model combined with software test process, software quality cost and testing type. The computational formula is defined to calculate software quality cost. A software quality balance law is introduced to balance control cost and failure cost. By adjusting software test method and strategy, organization can reach the objective of the best balance between software quality and cost. An enterprise case is studied and the result is analyzed to prove the basic law. The result shows an optimistic point that will prove the accurate of the model and the law","0840-7789","1-4244-0038-41-4244-0038","10.1109/CCECE.2006.277513","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4054588","Quality Cost;Software Testing Model;Software Testing Process;Testing Type","Costs;Software testing;Software quality;Life testing;Computer bugs;System testing;Quality management;Control systems;Software maintenance;Software measurement","program testing;software cost estimation;software quality","quality cost oriented software testing model;QC 3-D test model;software quality balance law","","3","9","","","","","","IEEE","IEEE Conferences"
"Configuration aware prioritization techniques in regression testing","Xiao Qu","Department of Computer Science and Engineering, University of Nebraska - Lincoln, 68588-0115, USA","2009 31st International Conference on Software Engineering - Companion Volume","","2009","","","375","378","Configurable software lets users customize applications in many ways, and is becoming increasingly prevalent. Regression testing is an important but expensive way to build confidence that software changes introduce no new faults as software evolves, resulting in many attempts to improve its performance given limited resources. Whereas problems such as test selection and prioritization at the test case level have been extensively researched in the regression testing literature, they have rarely been considered for configurations, though there is evidence that we should not ignore the effects of configurations on regression testing. This research intends to provide a framework for configuration aware prioritization techniques, evaluated through empirical studies.","","978-1-4244-3495","10.1109/ICSE-COMPANION.2009.5071025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5071025","","System testing;Fault detection;Sampling methods;Software testing;Software performance;Web pages;Software maintenance;Computer science;Application software;Internet","program testing;software engineering","configuration aware prioritization technique;regression testing;configurable software;test selection","","1","17","","","","","","IEEE","IEEE Conferences"
"Resource-Aware Test Suite Optimization","X. Zhang; H. Shan; J. Qian","NA; NA; NA","2009 Ninth International Conference on Quality Software","","2009","","","341","346","Regression testing is an expensive and frequently executed activity, which is often performed in a resource constrained execution environment. As resource-aware test suite optimization problem can be reduced to the 0-1 knapsack problem, which can be efficiently approximated with multiple algorithms. This paper proposes a new resource-aware test suite optimization technique and corresponding metric. The effectiveness of the technique and metric are illustrated by a case study.","1550-6002;2332-662X","978-1-4244-5913-1978-1-4244-5912-4978-0-7695-3828","10.1109/QSIC.2009.51","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381418","regression testing;test suite optimization;0-1 knapsack;resource-aware","Software testing;Costs;Space technology;Software performance;Software quality;Constraint optimization;Computer science;Educational institutions;Information science;Performance evaluation","knapsack problems;optimisation;program testing;regression analysis","resource-aware test suite optimization;regression testing;resource constrained execution environment;knapsack problem","","","11","","","","","","IEEE","IEEE Conferences"
"How Well Do Test Case Prioritization Techniques Support Statistical Fault Localization","B. Jiang; Z. Zhang; T. H. Tse; T. Y. Chen","NA; NA; NA; NA","2009 33rd Annual IEEE International Computer Software and Applications Conference","","2009","1","","99","106","In continuous integration, a tight integration of test case prioritization techniques and fault-localization techniques may both expose failures faster and locate faults more effectively. Statistical fault-localization techniques use the execution information collected during testing to locate faults. Executing a small fraction of a prioritized test suite reduces the cost of testing, and yet the subsequent fault localization may suffer. This paper presents the first empirical study to examine the impact of test case prioritization on the effectiveness of fault localization. Among many interesting empirical results, we find that coverage-based and random techniques can be more effective than distribution-based techniques in supporting statistical fault localization.","0730-3157;0730-3157","978-0-7695-3726","10.1109/COMPSAC.2009.23","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5254274","Continuous integration;software process integration;test case prioritization;fault localization","Software testing;Application software;Costs;Software debugging;Computer applications;Resumes;Australia Council;Computer science;Feedback","cost reduction;program compilers;program debugging;program testing;random processes;software cost estimation;software fault tolerance;statistical distributions","test case prioritization technique;statistical fault localization technique;continuous integration;prioritized test suite;testing cost reduction;empirical study;coverage-based technique;distribution-based technique;random technique;software process integration;debugging technique;code compilation","","23","29","","","","","","IEEE","IEEE Conferences"
"IntelligenTester - Software Test Sequence Optimization Using Graph Based Intelligent Search Agent","D. J. Mala; V. Mohan","Thiagarajar Coll. of Eng., Madurai; NA","International Conference on Computational Intelligence and Multimedia Applications (ICCIMA 2007)","","2007","1","","22","27","Software testing plays a vital role in quality software development. Usually, the number of test cases required to develop error-free software, will be very high. Since, exhaustive testing is not possible; the test cases that we need to generate should be optimal and also should cover the entire software and reveal as many errors as possible. In the proposed approach, the intelligent search agent (ISA) will take the decision of optimized test sequences by searching through the SUT, which is represented as a graph in which each node is associated with a heuristic value and each edge is associated with an edge weight. The intelligent agent will find the best sequence by following the nodes that satisfy the fitness criteria and generates the optimized test sequences from the set of all test paths of the SUT. Finally, we compared ISA with ACO and proved that ISA is taking less time and cost in generating optimal test sequences.","","0-7695-3050","10.1109/ICCIMA.2007.161","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4426547","","Intelligent agent;Software testing;Instruction sets;Competitive intelligence;Computational intelligence;Ant colony optimization;Educational institutions;Computer architecture;Application software;Postal services","program testing;software agents;software quality","IntelligenTester;software testing;graph based intelligent search agent;software quality;software development;error-free software;optimized test sequences;edge weight","","3","12","","","","","","IEEE","IEEE Conferences"
"Using the Multi-Attribute Global Inference of Quality (MAGIQ) Technique for Software Testing","J. D. McCaffrey","NA","2009 Sixth International Conference on Information Technology: New Generations","","2009","","","738","742","The Multi-Attribute Global Inference of Quality (MAGIQ) technique is a simple way to assign a single measure of overall quality to each of a set of similar software systems. Software testing activities can produce a wide range of useful information such as bug counts, performance metrics, and mean time to failure data. However, techniques to aggregate quality and testing metrics into a single quality meta-value are not widely known or used. The MAGIQ technique uses rank order centroids to convert system comparison attributes into normalized numeric weights, and then computes an overall measure of quality as a weighted (by comparison attributes) sum of system ratings. MAGIQ was originally developed to validate the results of analytic hierarchy process (AHP) analyses. Although MAGIQ has not been subjected to extensive research, the technique has proven highly useful in practice.","","978-1-4244-3770-2978-0-7695-3596","10.1109/ITNG.2009.81","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5070708","Decision support systems;management decision-making;software metrics;software quality;software testing","Software testing;Software systems;Software measurement;Aggregates;Decision making;Software quality;Information technology;Decision support systems;Quality management;Software metrics","program testing;software metrics;software quality","software testing;Multi-Attribute Global Inference of Quality;software system;testing metrics;analytic hierarchy process analysis","","3","9","","","","","","IEEE","IEEE Conferences"
"Prioritization of Scenarios Based on UML Activity Diagrams","S. P.G.; H. Mohanty","NA; NA","2009 First International Conference on Computational Intelligence, Communication Systems and Networks","","2009","","","271","276","Increased size and complexity of software requires better methods for different activities in the software development lifecycle. Quality assurance of software is primarily done by means of testing, an activity that faces constraints of both time and resources. Hence, there is need to test effectively within the constraints in order to maximize throughput i.e. rate of fault detection, coverage, etc. Test case prioritization involves techniques aimed at finding the best prioritized test suite. In this paper, we propose a prioritization technique based on UML activity diagrams. The constructs of an activity diagram are used to prioritize scenarios. Preliminary results obtained on a case-study indicate that the technique is effective in extracting the critical scenarios from the activity diagram.","","978-0-7695-3743","10.1109/CICSYN.2009.74","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5231936","UML;Prioritization","Unified modeling language;Software testing;Programming;Software quality;Fault detection;Throughput;Computational intelligence;Computer networks;Communication system software;Quality assurance","program testing;software development management;software metrics;software quality;Unified Modeling Language","UML activity diagrams;software complexity;software development lifecycle;quality assurance;software testing;test case prioritization;prioritization technique","","2","11","","","","","","IEEE","IEEE Conferences"
"A Statistical Approach for Improving the Performance of a Testing Methodology for Measurement Software","G. Betta; D. Capriglione; A. Pietrosanto; P. Sommella","NA; NA; NA; NA","IEEE Transactions on Instrumentation and Measurement","","2008","57","6","1118","1126","This paper describes the significant enhancements brought to an original methodology designed for testing measurement software. In a previous paper, the authors proposed a black-box seven-step procedure that allows the functional verification of complex instrument software to be performed. The main features of the procedure are concerned with the following: (1) the ability of reproducing actual correlations among the software inputs and (2) the need for a limited number of test cases. Making use of innovative statistical techniques, the methodology performance and reliability have been enhanced. Two further steps have been added with the aim of improving the correlation coefficient assessments and providing the estimations with a confidence level. Finally, a new strategy has been studied to optimize the number of test cases. The effects of the new solutions on the performance of the methodology are evaluated by applying the procedure to a complex software module employed in an automotive system. A comparison with the previous methodology version is also reported.","0018-9456;1557-9662","","10.1109/TIM.2007.915143","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4444151","Black-box approach;instrument software;reliability;software engineering;test methodology;Black-box approach;instrument software;reliability;software engineering;test methodology","Software testing;Software measurement;Software performance;Software safety;Instruments;Automotive engineering;Software engineering;Application software;Software maintenance;Software quality","automatic test software;program testing;software reliability;statistical testing","statistical approach;measurement software testing methodology;reliability enhancement;automotive system;instrument software","","14","25","","","","","","IEEE","IEEE Journals & Magazines"
"A Coverage Relationship Model for Test Case Selection and Ranking for Multi-version Software","W. T. Tsai; X. Zhou; R. A. Paul; Y. Chen; X. Bai","NA; NA; NA; NA; NA","10th IEEE High Assurance Systems Engineering Symposium (HASE'07)","","2007","","","105","112","Testing a group of software artifacts that implement the same specification can be time consuming, especially when the test case repository is large. In the meantime, some test cases may cover the same aspects in the software, thus it is not necessary to apply all the test cases. This paper proposes a model-based adaptive test (MAT) case selection and ranking technique to eliminate duplicate test cases, i.e., test cases with the similar coverage, and rank the test cases according to their potency and coverage. This technique can be applied in various domains where multiple versions of applications are available to test, such as web service testing, n-version applications, regression testing, and standard-based testing. The MAT is based a statistical model based on earlier testing results, and the model can determine the next sets of test cases to minimize the testing effort. The MAT is then applied to testing of multi-versioned web services and the results shows that the MAT can reduce testing effort while still maintain the effectiveness of testing.","1530-2059","0-7695-3043-5978-0-7695-3043","10.1109/HASE.2007.64","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4404732","Adaptive testing;statistical testing;test case selection and ranking;regression testing;group;testing;web services.","Software testing;Web services;Statistical analysis;System testing;Standards publication;Computer science;USA Councils;Application software;Software standards;Standards development","program testing;statistical analysis;Web services","coverage relationship model;test case selection;multiversion software;software artifacts;model-based adaptive test;ranking technique;statistical model;Web services","","11","13","","","","","","IEEE","IEEE Conferences"
"Tag-Based Techniques for Black-Box Test Case Prioritization for Service Testing","L. Mei; W. K. Chan; T. H. Tse; R. G. Merkel","NA; NA; NA; NA","2009 Ninth International Conference on Quality Software","","2009","","","21","30","A web service may evolve autonomously, making peer web services in the same service composition uncertain as to whether the evolved behaviors may still be compatible to its originally collaborative agreement. Although peer services may wish to conduct regression testing to verify the original collaboration, the source code of the former service can be inaccessible to them. Traditional code-based regression testing strategies are inapplicable. The rich interface specifications of a web service, however, provide peer services with a means to formulate black-box testing strategies. In this paper, we formulate new test case prioritization strategies using tags embedded in XML messages to reorder regression test cases, and reveal how the test cases use the interface specifications of services. We evaluate experimentally their effectiveness on revealing regression faults in modified WS-BPEL programs. The results show that the new techniques can have a high probability of outperforming random ordering.","1550-6002;2332-662X","978-1-4244-5913-1978-1-4244-5912-4978-0-7695-3828","10.1109/QSIC.2009.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381531","test case prioritization;black-box regression testing;WS-BPEL;service testing;encapsulation testing","Web services;Software testing;XML;Fault detection;Logic testing;Collaborative software;Councils;Software quality;Australia;International collaboration","program testing;regression analysis;Web services","tag-based technique;black-box test case prioritization;service testing;Web service;service composition;collaborative agreement;peer services;code-based regression testing;black-box testing;XML messages;WS-BPEL programs","","11","27","","","","","","IEEE","IEEE Conferences"
"A Markov Decision Approach to Optimize Testing Profile in Software Testing","D. Zhang; C. Nie; B. Xu","NA; NA; NA","2008 The 9th International Conference for Young Computer Scientists","","2008","","","1205","1210","In this paper, we demonstrate an approach to optimize software testing, minimize the expected cost with given software parameters of concern. Taking software testing process as a Markov decision process, a Markov decision model of software testing is proposed, and by using a learning strategy based on the Cross-Entropy method to optimize the software testing, we obtain the optimal testing profile. Simulation results show that this learning strategy reduces significantly in expected cost comparing with random testing, moreover, this learning strategy is more feasible and significantly in reducing the number of test cases required to detect and revealing a certain number of software defects than random testing.","","978-0-7695-3398","10.1109/ICYCS.2008.322","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4709145","Markov decision process;software testing;cross-entropy method;optimal testing profile;random testing","Software testing;Optimal control;Software reliability;Cost function;Statistical analysis;Uncertainty;Educational institutions;Optimization methods;Software systems;Fault detection","decision theory;learning (artificial intelligence);Markov processes;minimisation;program testing;software cost estimation","software testing profile optimization;Markov decision approach;expected cost minimization;software parameter;learning strategy;cross-entropy method","","1","13","","","","","","IEEE","IEEE Conferences"
"Automatic Generating All-Path Test Data of a Program Based on PSO","A. Li; Y. Zhang","NA; NA","2009 WRI World Congress on Software Engineering","","2009","4","","189","193","Generating path test data is one of hot points in the research field of software test. Automatic path test data generation is vital to improve the efficiency of path test and reduce test cost. Particle swarm optimization (PSO) is employed to generate automatically path test data in this paper. Automatic all paths test data generation of a program is challenged. Based on PSO, a method of generating all paths test data of a program is presented. A new fitness function is constructed, and the frequencies for all paths are registered. Generation results show that the efficiency has the enhancement of all paths test data generation compared with single path test data generation.","","978-0-7695-3570","10.1109/WCSE.2009.98","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5319531","Software testing;path test data generating;PSO","Automatic testing;Software testing;Genetic algorithms;Frequency;Random number generation;Software engineering;Computer science;Costs;Particle swarm optimization;Data analysis","optimisation;program testing","automatic generating all-path test data;PSO;particle swarm optimization;software testing","","6","9","","","","","","IEEE","IEEE Conferences"
"A path-oriented test data generation approach for automatic software testing","Xiaofeng Xu; Yan Chen; Xiaochao Li; Donghui Guo","Dept. of Physics, Xiamen University, Fujian, China; Dept. of Communication Engineering, Xiamen University, Fujian, China; Dept. of Electronic Engineering, Xiamen University, Fujian, China; Dept. of Electronic Engineering, Xiamen University, Fujian, China","2008 2nd International Conference on Anti-counterfeiting, Security and Identification","","2008","","","63","66","The clonal selection (CS) algorithm is an optimization algorithm based upon the clonal selection principle in the biological immune system. This paper presents a novel approach that uses CS algorithm for path-oriented test data generation. The approach takes a selected path as a target and executes sequences of operators iteratively for test case to generate. An affinity function which is made up of a similarity and penalty value is developed to guide the test generator to make successive modifications of the test data, so that test data can ever closer to satisfy the requirement. The comparison results show that this approach is more efficient than those based on other heuristic algorithms in finding solutions.","2163-5048;2163-5056","978-1-4244-2584-6978-1-4244-2585","10.1109/IWASID.2008.4688344","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4688344","Automatic Software Testing;Clonal Selection (CS);Path-Oriented Test Data Generation;Affinity function","Software testing;Automatic testing;Encoding;Software algorithms;Data engineering;Iterative algorithms;Heuristic algorithms;Cloning;Electronic equipment testing;Biological information theory","optimisation;program testing","path-oriented test data generation;automatic software testing;clonal selection algorithm;optimization algorithm;affinity function","","1","5","","","","","","IEEE","IEEE Conferences"
"System Test Planning of Software: An Optimization Approach","K. Chari; A. Hevner","Department of Information Systems and Decision Sciences, University of South Florida, 4202 East Fowler Avenue, CIS 1040, Tampa, FL 33620-7800; Department of Information Systems and Decision Sciences, University of South Florida, 4202 East Fowler Avenue, CIS 1040, Tampa, FL 33620-7800","IEEE Transactions on Software Engineering","","2006","32","7","503","5099","This paper extends an exponential reliability growth model to determine the optimal number of test cases to be executed for various use case scenarios during the system testing of software. An example demonstrates a practical application of the optimization model for system test planning","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.70","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1677535","Nonlinear programming;reliability;test management.","Software testing;System testing;Software reliability;Software systems;Cost function;Software quality;Application software;Quality control;Software maintenance;Law","optimisation;program testing;software reliability","exponential reliability growth model;test case;use case scenario;system test planning;optimization;nonlinear programming;software testing","","7","30","","","","","","IEEE","IEEE Journals & Magazines"
"Optimal Testing Resource Allocation for Modular Software Based on a Software Reliability Growth Model: A Dynamic Programming Approach","M. G. M. Khan; N. Ahmad; L. S. Rafi","NA; NA; NA","2008 International Conference on Computer Science and Software Engineering","","2008","2","","759","762","In this paper, two problems of optimal resource allocation to modules during testing phase are studied: (1) maximization of the number of faults removed when the amount of testing-effort is fixed, and (2) maximization of the number of faults removed satisfying a certain percentage of initial faults to be removed with a fixed amount of testing-effort. These optimization problems are formulated as nonlinear programming problems (NLPP), which are modeled by a software reliability growth model based on a non-homogeneous Poisson process which incorporated the exponentiated Weibull testing-effort functions. A solution procedure is then developed using dynamic programming technique to solve the NLPPs. Finally, numerical examples illustrate the solution procedure and the results are compared with that of Kapur et al.[1].","","978-0-7695-3336","10.1109/CSSE.2008.1394","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722161","Software reliability growth model;resource allocation problem;nonlinear programming problem;dynamic programming technique;inflection S-shaped models.","Software testing;Resource management;Software reliability;Dynamic programming;System testing;Computer science;Software engineering;Functional programming;Fault detection;Phase detection","dynamic programming;nonlinear programming;program testing;resource allocation;software reliability;stochastic processes","optimal testing resource allocation;modular software;software reliability growth model;dynamic programming;nonlinear programming problems;nonhomogeneous Poisson process;exponentiated Weibull testing-effort functions","","5","8","","","","","","IEEE","IEEE Conferences"
"Automated test program generation for an industrial optimizing compiler","Chen Zhao; Yunzhi Xue; Qiuming Tao; Liang Guo; Zhaohui Wang","Institute of Software, Chinese Academy of Sciences, P.O. BOX 8718, Beijing, China; Institute of Software, Chinese Academy of Sciences, P.O. BOX 8718, Beijing, China; Institute of Software, Chinese Academy of Sciences, P.O. BOX 8718, Beijing, China; Institute of Software, Chinese Academy of Sciences, P.O. BOX 8718, Beijing, China; Institute of Software, Chinese Academy of Sciences, P.O. BOX 8718, Beijing, China","2009 ICSE Workshop on Automation of Software Test","","2009","","","36","43","This paper presents joint research and practice on automated test program generation for an industrial compiler, UniPhier, by Matsushita Electric Industrial Co., Ltd. (MEI) and Institute of Software, Chinese Academy of Sciences (ISCAS) since Sept. 2002. To meet the test requirements of MEI's engineers, we proposed an automated approach to produce test programs for UniPhier, and as a result we developed an integrated tool named JTT. Firstly, we show the script-driven test program generation process in JTT. Secondly, we show how to produce test programs automatically, based on a temporal-logic model of compiler optimizations, to guarantee the execution of optimizing modules under test during compilation. JTT has gained success in testing UniPhier: even after benchmark testing and comprehensive manual testing, JTT still found 6 new serious defects.","","978-1-4244-3711","10.1109/IWAST.2009.5069039","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5069039","","Automatic testing;Optimizing compilers;High level synthesis;Multilevel systems;Program processors;Software testing;Benchmark testing;Logic testing;Computer industry;Character generation","automatic programming;optimising compilers;program testing;temporal logic","automated test program generation;industrial optimizing compiler;Matsushita Electric Industrial Co., Ltd;Institute of Software;Chinese Academy of Sciences;script-driven test program generation process;temporal-logic model","","3","15","","","","","","IEEE","IEEE Conferences"
"Test Selection Prioritization Strategy","R. Subramanyan; C. J. Budnik","NA; NA","2009 33rd Annual IEEE International Computer Software and Applications Conference","","2009","2","","545","549","A wide divergence is observed in projects between test activities planned in the test plan and the actual tests that can be executed. Estimates for test execution computed during the planning are inaccurate without test design. The actual time and resources available are usually less than planned. Assuming that time and resources cannot be changed, a dynamic selection of tests for execution that maximizes quality is required.","0730-3157;0730-3157","978-0-7695-3726","10.1109/COMPSAC.2009.190","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5254084","test selection;test prioritization;test strategy","System testing;Software testing;Fault detection;Life testing;Unified modeling language;Planning;USA Councils;Project management;Tellurium;Computer applications","program testing;project management;software metrics;software quality","test selection prioritization strategy;test activity planning;test execution;quality maximization;complexity metrics;software project","","1","10","","","","","","IEEE","IEEE Conferences"
"Prioritized Constraints with Data Sampling Scores for Automatic Test Data Generation","X. Ma; J. J. Li; D. M. Weiss","Avaya Labs Research; Avaya Labs Research; Avaya Labs Research","Eighth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing (SNPD 2007)","","2007","3","","1129","1134","Many automatic test data generation approaches use constraint solvers to find data values. One problem with this method is that it cannot generate test data when the constraints are not solvable, either because there is no solution or the constraints are too complex. We propose a constraint prioritization method using data sampling scores to generate valid test data even when a set of constraints is not solvable. Our case study illustrates the effectiveness of this method.","","0-7695-2909-7978-0-7695-2909","10.1109/SNPD.2007.523","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4288019","Test case generation;test automation;constraint;solving;software reliability.","Sampling methods;Automatic testing;Software testing;System testing;Writing;Automation;Costs;Programming;Software engineering;Artificial intelligence","automatic test software;constraint handling;program testing;software reliability","data sampling scores;automatic test data generation;constraint solving;constraint prioritization;software reliability","","","9","","","","","","IEEE","IEEE Conferences"
"Using the Case-Based Ranking Methodology for Test Case Prioritization","P. Tonella; P. Avesani; A. Susi","ITC-irst, Trento, Italy; ITC-irst, Trento, Italy; ITC-irst, Trento, Italy","2006 22nd IEEE International Conference on Software Maintenance","","2006","","","123","133","The test case execution order affects the time at which the objectives of testing are met. If the objective is fault detection, an inappropriate execution order might reveal most faults late, thus delaying the bug fixing activity and eventually the delivery of the software. Prioritizing the test cases so as to optimize the achievement of the testing goal has potentially a positive impact on the testing costs, especially when the test execution time is long. Test engineers often possess relevant knowledge about the relative priority of the test cases. However, this knowledge can be hardly expressed in the form of a global ranking or scoring. In this paper, we propose a test case prioritization technique that takes advantage of user knowledge through a machine learning algorithm, case-based ranking (CBR). CBR elicits just relative priority information from the user, in the form of pairwise test case comparisons. User input is integrated with multiple prioritization indexes, in an iterative process that successively refines the test case ordering. Preliminary results on a case study indicate that CBR overcomes previous approaches and, for moderate suite size, gets very close to the optimal solution","1063-6773","0-7695-2354","10.1109/ICSM.2006.74","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021329","","Software testing;System testing;Fault detection;Machine learning algorithms;Application software;Automatic testing;Humans;Delay;Software debugging;Cost function","learning (artificial intelligence);program testing","case-based ranking;test case prioritization;test case execution order;fault detection;software testing;machine learning;test case ordering","","20","16","","","","","","IEEE","IEEE Conferences"
"Automated Test Data Generation using Search Based Software Engineering","M. Harman","King's College London","Second International Workshop on Automation of Software Test (AST '07)","","2007","","","2","2","Generating test data is a demanding process. Without automation, the process is slow, expensive and error-prone. However, techniques to automate test data generation must cater for a bewildering variety of functional and non-functional test adequacy criteria and must either implicitly or explicitly solve problems involving state propagation and constraint satisfaction. This talk will show how optimisation techniques associated with search based software engineering (SBSE) have been used to automate test data generation. The talk will survey the area and present the results of recent work on characterising, transforming and eliding test data search landscapes.","","978-0-7695-2971","10.1109/AST.2007.4","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4296713","","Automatic testing;Software testing;Software engineering;Genetics;USA Councils;Stress;Application software;Evolutionary computation;Automation;Data security","automatic testing;program testing","automated test data generation;search based software engineering;optimisation","","12","34","","","","","","IEEE","IEEE Conferences"
"Software-Based Self-Test of Processors under Power Constraints","Jun Zhou; H. -. Wunderlich","Institute of Computer Architecture and Computer Engineering, University of Stuttgart Pfaffenwaldring 47, D-70569 Stuttgart, Germany; NA","Proceedings of the Design Automation & Test in Europe Conference","","2006","1","","1","6","Software-based self-test (SBST) of processors offers many benefits, such as dispense with expensive test equipments, test execution during maintenance and in the field or initialization tests for the whole system. In this paper, for the first time a structural SBST methodology is proposed which optimizes energy, average power consumption, test length and fault coverage at the same time","1530-1591;1558-1101","3-9810801-1","10.1109/DATE.2006.243798","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1656919","Test program generation;processor test;low power test","Built-in self-test;Circuit testing;Circuit faults;Energy consumption;System testing;Automatic testing;Software testing;Test equipment;Power generation;Design for testability","automatic test software;built-in self test;logic testing;microprocessor chips","software-based self-test;power constraints;test equipments;test execution;field tests;initialization tests;test program generation;processor test","","13","25","","","","","","IEEE","IEEE Conferences"
"Improved Extremal Optimization for Constrained Pairwise Testing","J. Yuan; C. Jiang; Z. Jiang","NA; NA; NA","2009 International Conference on Research Challenges in Computer Science","","2009","","","108","111","Pairwise testing, which requires that every combination of valid values of each pair of system factors be covered by at lease one test case, plays an important role in software testing since many faults are caused by unexpected 2-way interactions among system factors. In real systems, constraints usually exist between values, which means that some values cannot coexist in a valid test. Although meta-heuristic strategies like simulated annealing can generally discover smaller pairwise test suite in the presence of constraints, they may cost more time to perform search, compared with greedy algorithms. We propose a new method, improved extremal optimization, for constructing constrained pairwise test suites. Experimental results show that improved extremal optimization gives similar size of resulting pairwise test suite and yields a 13% reduction in solution time over simulated annealing.","","978-1-4244-5410-5978-1-4244-5409-9978-0-7695-3927","10.1109/ICRCCS.2009.35","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5401305","heuristics;extremal optimization;simulated annealing;software testing;covering arrays","Constraint optimization;Software testing;System testing;Simulated annealing;Costs;Greedy algorithms;Computer science;Educational technology;Laboratories;Embedded system","program testing;simulated annealing","constrained pairwise testing;improved extremal optimization;software testing;meta-heuristic strategies;simulated annealing;greedy algorithms","","3","14","","","","","","IEEE","IEEE Conferences"
"Quota-constrained test-case prioritization for regression testing of service-centric systems","Shan-Shan Hou; Shan-Shan Hou; Lu Zhang; Lu Zhang; Tao Xie; Jia-Su Sun; Jia-Su Sun","Key Lab. of High Confidence Software Technol., Peking Univ., Beijing; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing; Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing","2008 IEEE International Conference on Software Maintenance","","2008","","","257","266","Test-case prioritization is a typical scenario of regression testing, which plays an important role in software maintenance. With the popularity of Web services, integrating Web services to build service-centric systems (SCSs) has attracted attention of many researchers and practitioners. During regression testing, as SCSs may use up constituent Web servicespsila request quotas (e.g., the upper limit of the number of requests that a user can send to a Web service during a certain time range), the quota constraint may delay fault exposure and the subsequent debugging. In this paper, we investigate quota-constrained test-case prioritization for SCSs, and propose quota-constrained strategies to maximize testing requirement coverage. We divide the testing time into time slots, and iteratively select and prioritize test cases for each time slot using integer linear programming (ILP). We performed an experimental study on our strategies together with three other strategies, and the results show that with the constraint of request quotas, our strategies can schedule test cases for execution in an order with higher effectiveness in exposing faults and achieving total and additional branch coverage.","1063-6773","978-1-4244-2613-3978-1-4244-2614","10.1109/ICSM.2008.4658074","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658074","","Testing;Web services;Software;Partitioning algorithms;Integer linear programming;Computer science;Laboratories","integer programming;linear programming;program testing;regression analysis;software maintenance;Web services","quota-constrained test-case prioritization;regression testing;service-centric systems;software maintenance;Web services;quota constraint;integer linear programming","","10","22","","","","","","IEEE","IEEE Conferences"
"Generating a Test Strategy with Bayesian Networks and Common Sense","J. -. Gras; R. Gupta; E. Perez-Minana","Motorola Labs, UK; NA; NA","Testing: Academic & Industrial Conference - Practice And Research Techniques (TAIC PART'06)","","2006","","","29","40","Testing still represents an important share of the overall development effort and, coming late in the software life cycle, it is on the critical path both from a schedule and quality perspective. In an effort to conduct smarter software testing, Motorola Labs have developed the Bayesian test assistant (BTA), an advanced decision support tool to optimize all verification and validation activities, in development and system testing. With Bayesian networks, the theory underlying BTA, Motorola Labs built a library of causal models to predict, from key process, people and product factors, the quality of artefacts at each step of the software development. In this paper we present how BTA links the predictions from development models by mapping dependencies between components or subsystems to predict the level of risk in each system feature. As a result, and well before system testing starts, BTA generates a test strategy that optimizes the writing of test cases. During system test, BTA scores test cases to select an optimum set for each test step, leading to a faster discovery of defects. We also describe how BTA was deployed on large telecomm system releases in several Motorola organizations and the improvement driven so far in system testing","","0-7695-2672","10.1109/TAIC-PART.2006.10","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1691667","software testing;test strategy;Bayesian;networks;defect prediction.","Bayesian methods;System testing;Software testing;Predictive models;Life testing;Software quality;Scheduling;Software libraries;Programming;Writing","belief networks;decision making;program debugging;program testing;program verification;software quality;software tools;systems analysis","test strategy;Bayesian network;software system testing;software life cycle;software quality;Bayesian test assistant;decision support tool;software verification;software validation;software system development;software defect discovery;telecomm system release;Motorola Labs","","3","27","","","","","","IEEE","IEEE Conferences"
"Evolving the Quality of a Model Based Test Suite","U. Farooq; C. P. Lam","NA; NA","2009 International Conference on Software Testing, Verification, and Validation Workshops","","2009","","","141","149","Redundant test cases in newly generated test suites often remain undetected until execution and waste scarce project resources. In model-based testing, the testing process starts early on in the developmental phases and enables early fault detection. The redundancy in the test suites generated from models can be detected earlier as well and removed prior to its execution. The article presents a novel model-based test suite optimization technique involving UML activity diagrams by formulating the test suite optimization problem as an Equality Knapsack Problem. The aim here is the development of a test suite optimization framework that could optimize the model-based test suites by removing the redundant test cases. An evolution-based algorithm is incorporated into the framework and is compared with the performances of two other algorithms. An empirical study is conducted with four synthetic and industrial scale Activity Diagram models and results are presented.","","978-0-7695-3671-2978-1-4244-4356","10.1109/ICSTW.2009.27","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4976379","Model Based Testing;Test Suite Optimization;UML","Software testing;Automatic testing;Stochastic processes;Fault detection;Unified modeling language;Programming;Costs;Conferences;Information science;Redundancy","evolutionary computation;knapsack problems;optimisation;program testing;software maintenance;software quality;Unified Modeling Language","model-based test suite optimization technique;software quality;software evolving;software testing;UML activity diagram;equality knapsack problem;evolution-based algorithm","","8","25","","","","","","IEEE","IEEE Conferences"
"Action-driven automation test framework for Graphical User Interface (GUI) software testing","Li Feng; Sheng Zhuang","Sybase Software (China), LongDong Ave. 3000#, Shanghai, 201203, China; Sybase Software (China), LongDong Ave. 3000#, Shanghai, 201203, China","2007 IEEE Autotestcon","","2007","","","22","27","In this paper we describe the design and implementation of an action-driven automation test framework especially for GUI software testing. The idea of action-driven automation test framework comes from the core concept of ""quality assurance (QA)"". Better quality can be ensured by increasing the coverage of test cases on the software but the process of creating large number of test cases has to be optimized. With this goal the framework was designed to primarily increase the efficiency and flexibility in composing test cases and simplify the process of learning the test cases. This paper describes the background, features, and implementation details of the framework.","1088-7725;1558-4550","978-1-4244-1238-9978-1-4244-1239","10.1109/AUTEST.2007.4374197","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4374197","","Software testing;Graphical user interfaces;Automatic testing;Logic testing;Engines;Design automation;Quality assurance;Software quality;Data mining;Filling","automatic test software;graphical user interfaces;program testing","action driven automation test framework;graphical user interface software testing;GUI;quality assurance","","3","5","","","","","","IEEE","IEEE Conferences"
"An Empirical Study on Bayesian Network-based Approach for Test Case Prioritization","S. Mirarab; L. Tahvildari","NA; NA","2008 1st International Conference on Software Testing, Verification, and Validation","","2008","","","278","287","A cost effective approach to regression testing is to prioritize test cases from a previous version of a software system for the current release. We have previously introduced a new approach for test case prioritization using Bayesian Networks (BN) which integrates different types of information to estimate the probability of each test case finding bugs. In this paper, we enhance our BN-based approach in two ways. First, we introduce a feedback mechanism and a new change information gathering strategy. Second, a comprehensive empirical study is performed to evaluate the performance of the approach and to identify the effects of using different parameters included in the technique. The study is performed on five open source Java objects. The obtained results show relative advantage of using feedback mechanism for some objects in terms of early fault detection. They also provide insight into costs and benefits of the various parameters used in the approach.","2159-4848","978-0-7695-3127","10.1109/ICST.2008.57","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4539555","Test Case Prioritization;Empirical Study;Regression Testing","Bayesian methods;Software testing;System testing;Feedback;Costs;Software systems;Computer bugs;Performance evaluation;Java;Fault detection","Bayes methods;Java;program testing;regression analysis","Bayesian network-based approach;test case prioritization;regression testing;information gathering strategy;open source Java;feedback mechanism","","17","22","","","","","","IEEE","IEEE Conferences"
"A Novel Approach of Prioritizing Use Case Scenarios","D. Kundu; D. Samanta","NA; NA","14th Asia-Pacific Software Engineering Conference (APSEC'07)","","2007","","","542","549","Modern softwares are very large and complex. As the size and complexity of software increases, software developers feel an urgent need for a better management of different activities during the course of software development. In this paper, we present an approach of use case scenario prioritization suitable for project planning at an early phase of the software development. We consider only use case model in our work. For prioritization, we focus on how critical a scenario path is, which essentially depends on density of overlapping of sub path of a scenario path with other scenario path(s) of a use case. Our proposed approach provides an analytical solution on use case scenario prioritization and is very much effective in project management related activities as substantiated by our experimental results.","1530-1362;1530-1362","0-7695-3057","10.1109/ASPEC.2007.66","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4425898","Use case model;UML;software quality;scenario prioritization;project management","Programming;Unified modeling language;Software quality;Software engineering;Information technology;Project management;Computer industry;Software testing;Software development management;Documentation","formal specification;project management;software cost estimation;software development management;software metrics;software prototyping;software quality;Unified Modeling Language","use case scenario prioritization;software complexity;software project planning;software development life cycle;software project management;software quality metric;software cost metric;UML use case model","","6","25","","","","","","IEEE","IEEE Conferences"
"Dynamic Analysis of Algebraic Structure to Optimize Test Generation and Test Case Selection","A. J. H. Simons; W. Zhao","NA; NA","2009 Testing: Academic and Industrial Conference - Practice and Research Techniques","","2009","","","33","42","Where no independent specification is available, object-oriented unit testing is limited to exercising all interleaved method paths, seeking unexpected failures. A recent trend in unit testing, that interleaves dynamic analysis between each test cycle, has brought useful reductions in test-set sizes by pruning redundant prefix paths. This paper describes a dynamic approach to analyzing the algebraic structure of test objects, such that prefix paths ending in observer or transformer operations yielding unchanged, or derived states may be detected and pruned on-the-fly during testing. The fewer retained test cases are so close to the ideal algebraic specification cases that a tester can afford to confirm or reject these cases interactively, which are then used as a test oracle to predict many further test outcomes during automated testing. The algebra-inspired algorithms are incorporated in the latest version of the JWalk lazy systematic unit testing tool suite, which discovers key test cases, while pruning many thousands of redundant test cases.","","978-1-4244-4977-4978-0-7695-3820","10.1109/TAICPART.2009.28","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381647","unit testing;test generation;test case selection;dynamic analysis;lazy systematic testing;JWalk","Automatic testing;System testing;Software testing;Java;Feedback;Computer industry;Failure analysis;Optimization methods;Computer science;Object detection","algebraic specification;program testing","dynamic analysis;algebraic structure;test generation optimization;test case selection;object-oriented unit testing;test cycle;test-set size;test object;prefix path;algebraic specification;test oracle;automated testing;algebra-inspired algorithm;JWalk lazy systematic unit testing tool","","1","23","","","","","","IEEE","IEEE Conferences"
"Study on Test Generation of Sequential Circuits Based on Particle Swarm Optimization and Ant Algorithm","W. Jian; X. Chuanpei","NA; NA","2008 International Conference on Computer Science and Software Engineering","","2008","1","","149","152","This paper presents a new method of automatic test generation for sequential circuits based on particle swarm optimization and ant algorithms. The hybrid algorithm utilizes characteristics of ant algorithm such as positive feedback and keeps basic characteristic of particle swarm optimization to guide the search further. Experiments based on the hybrid algorithm model are implemented. It can achieve higher fault coverages when compared with the results based on particle swarm optimization, yet execute time of test generation for both of them is nearly identical, and more compact test sets are achieved when compared with the results based on ant algorithm.","","978-0-7695-3336","10.1109/CSSE.2008.953","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721713","sequential circuit;test generation;particle swarm optimization;ant algorithm","Circuit testing;Sequential analysis;Sequential circuits;Particle swarm optimization;Signal processing algorithms;Electronic equipment testing;Feedback;Automatic testing;Software testing;Software algorithms","particle swarm optimisation;sequential circuits","sequential circuits;particle swarm optimization;ant algorithm;positive feedback;hybrid algorithm model","","1","7","","","","","","IEEE","IEEE Conferences"
"Study of the Test Flow Optimization Method in Radar Fault Isolation","Z. Li; H. Liu; D. Wen; Q. Zheng; X. Song","NA; NA; NA; NA; NA","2009 IEEE Circuits and Systems International Conference on Testing and Diagnosis","","2009","","","1","4","In order to optimize test flow after the default flow was modified by hand, a new software framework for the radar fault isolation was illustrated. This framework separated all mapping algorithms from the test flow graph so as to modify flow and to insert mapping algorithm dynamically in testing process. Based on this framework, a kind of optimization method of test flow was proposed and studied. By defining an objective function, we could evaluate all candidate test flows so as to get a optimizing flow. An example explained how to search the flow from candidate flows.","2324-8475;2324-8491","978-1-4244-2587","10.1109/CAS-ICTD.2009.4960880","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4960880","","Optimization methods;Automatic testing;Electronic equipment testing;Isolation technology;Software testing;Flow graphs;System testing;Radar detection;Fault detection;Condition monitoring","built-in self test;fault diagnosis;flow graphs","test flow optimization method;radar fault isolation;test flow graph;software framework;automatic test","","1","10","","","","","","IEEE","IEEE Conferences"
"Adaptive Random Test Case Prioritization","B. Jiang; Z. Zhang; W. K. Chan; T. H. Tse","NA; NA; NA; NA","2009 IEEE/ACM International Conference on Automated Software Engineering","","2009","","","233","244","Regression testing assures changed programs against unintended amendments. Rearranging the execution order of test cases is a key idea to improve their effectiveness. Paradoxically, many test case prioritization techniques resolve tie cases using the random selection approach, and yet random ordering of test cases has been considered as ineffective. Existing unit testing research unveils that adaptive random testing (ART) is a promising candidate that may replace random testing (RT). In this paper, we not only propose a new family of coverage-based ART techniques, but also show empirically that they are statistically superior to the RT-based technique in detecting faults. Furthermore, one of the ART prioritization techniques is consistently comparable to some of the best coverage-based prioritization techniques (namely, the ""additional"" techniques) and yet involves much less time cost.","1938-4300","978-1-4244-5259-0978-0-7695-3891","10.1109/ASE.2009.77","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5431769","Adaptive random testing;test case prioritization","Software testing;Subspace constraints;Fault detection;Automatic testing;Programming;Greedy algorithms;Software engineering;Costs;Australia Council;Computer science","fault diagnosis;program testing;regression analysis;software engineering","adaptive random test case prioritization;regression testing;random selection;coverage based ART techniques;fault detection","","75","28","","","","","","IEEE","IEEE Conferences"
"An approach of optimal path generation using ant colony optimization","P. R. Srivastava; K. Baby; G. Raghurama","Computer science and Information, System Group(CSIS, Group), Birla Institute of Technology and Science, (BITS), Pilani, Rajasthan, India -333031; Student, ME(SS), CSIS Group, Birla Institute of Technology and Science, (BITS), Pilani, Rajasthan, India -333031; Electrical &Electronic and Instrumentation Group, Birla Institute of Technology and Science, (BITS), Pilani, Rajasthan, India -333031","TENCON 2009 - 2009 IEEE Region 10 Conference","","2009","","","1","6","Software Testing is one of the indispensable parts of the software development lifecycle and structural testing is one of the most widely used testing paradigms to test various software. Structural testing relies on code path identification, which in turn leads to identification of effective paths. Aim of the current paper is to present a simple and novel algorithm with the help of an ant colony optimization, for the optimal path identification by using the basic property and behavior of the ants. This novel approach uses certain set of rules to find out all the effective/optimal paths via ant colony optimization (ACO) principle. The method concentrates on generation of paths, equal to the cyclomatic complexity. This algorithm guarantees full path coverage.","2159-3442;2159-3450","978-1-4244-4546-2978-1-4244-4547","10.1109/TENCON.2009.5396088","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5396088","Software Testing;Ant Colony Optimization (ACO);Path Testing;Optimal Path;Control Flow Graph(CGF);Cyclomatic Complexity (CC)","Ant colony optimization;Software testing;Life testing;Electronic equipment testing;System testing;Glass;Computer science;Programming;Optimal control;Software engineering","optimisation;program testing;software engineering","optimal path generation;ant colony optimization;software testing;software development lifecycle;structural testing;code path identification;optimal path identification;cyclomatic complexity","","14","19","","","","","","IEEE","IEEE Conferences"
"Automated Test Case Generation and Its Optimization for Path Testing Using Genetic Algorithm and Sampling","D. Mohapatra; P. Bhuyan; D. P. Mohapatra","NA; NA; NA","2009 WASE International Conference on Information Engineering","","2009","1","","643","646","Software testing is an important activity of the software development process, and automated test case generation contributes to reduce cost and time efforts. In this paper, we have used genetic algorithm to optimize the test cases that are generated using the category-partition and test harness patterns. In order to investigate the effectiveness of the approach, a graph is derived using category-partition and genetic algorithm is used to optimize the test cases to cover the entire possible path. The optimal test suites are devised by the method of sampling statistics.","","978-0-7695-3679","10.1109/ICIE.2009.22","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5211510","Test Case;Test Harness;Genetic Algorithm;Sampling;Optimal test Suite.","Automatic testing;Genetic algorithms;Sampling methods;Biological cells;Software testing;Programming;Statistical analysis;Genetic mutations;Genetic engineering;Costs","genetic algorithms;program testing","automated test case generation;path testing;optimization;genetic algorithm;software testing;software development process;category-partition;test harness patterns;sampling statistics","","5","7","","","","","","IEEE","IEEE Conferences"
"TestFilter: A Statement-Coverage Based Test Case Reduction Technique","S. Khan; A. Nadeem; A. Awais","Center for Software Dependabilit, Mohammad Ali Jinnah University, Islamabad, Pakistan. contact2saif@yahoo.com; Center for Software Dependabilit, Mohammad Ali Jinnah University, Islamabad, Pakistan. anadeem@jinnah.edu.pk; COMSATS Institute of Information Technology, Islamabad Campus, Pakistan. ali.awais@hotmail.com","2006 IEEE International Multitopic Conference","","2006","","","275","280","Software testing is an important but expensive phase of software development life cycle. During software testing and retesting, development organizations always desire to validate the software from different views. But exhaustive testing requires program execution with all possible combinations of values for program variables, which is impractical due to resource constraints. For many applications, it is possible to generate test cases automatically. But the core problem is the selection of effective test cases necessary to validate the program during the maintenance phase. This target can only be achieved by eliminating all the redundant test cases from the generated pool of test suites. In this paper, we propose a novel test case reduction technique called TestFilter that uses the statement-coverage criterion for reduction of test cases. To demonstrate the applicability of this approach, we conduct an experimental study. The results show that our technique is beneficial in identifying non-redundant test cases at a little cost. Ultimately it is beneficial to optimize time &amp; cost spent on testing and it is also helpful during regression testing.","","1-4244-0794-X1-4244-0795","10.1109/INMIC.2006.358177","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4196420","Software testing;testing efficiency;test case reduction","Software testing;Automatic testing;Life testing;Programming;Software maintenance;Information technology;Application software;Cost function;Software quality;Standards development","program testing;software engineering","TestFilter;statement-coverage based test case reduction technique;software testing;software development","","4","18","","","","","","IEEE","IEEE Conferences"
"Weighted Proportional Sampling : AGeneralization for Sampling Strategies in Software Testing","G. Padilla; C. M. De Oca; I. Yen; F. Bastani","Research Center in Mathematics, Mexico. Department of Computer Science. gpadilla@cimat.mx; Research Center in Mathematics, Mexico. Department of Computer Science. moca@cimat.mx; University of Texas at Dallas, USA. Department of Computer Science. ilyen@utdallas.edu; University of Texas at Dallas, USA. Department of Computer Science. bastani@utdallas.edu","2006 3rd International Conference on Electrical and Electronics Engineering","","2006","","","1","4","Current activities to measure the quality of software products rely on software testing. The size and complexity of software systems make it almost impossible to perform complete coverage testing. During the past several years, many techniques to improve the test effectiveness (i.e., the ability to find faults) have been proposed to address this issue. Two examples of such strategies are random testing and partition testing. Both strategies follow an input domain sampling to perform the testing process. The procedure and assumptions for selecting these points seem to be different for both strategies: random testing considers only the probability of each sub-domain (i.e. uniform sampling) while partition testing considers only the sampling rate of each sub-domain (i.e., proportional sampling). This paper describes a more general sampling strategy, named weighted proportional sampling strategy. This strategy unifies both strategies into a general model that encompasses both of them as special cases. This paper also proposes an optimization model to determine the number of sampled points depending on the sampling strategy","","1-4244-0402-91-4244-0403","10.1109/ICEEE.2006.251865","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4017950","Software Testing;Sampling;Proportationa;Uniform","Sampling methods;Software testing;Performance evaluation;System testing;Computer science;Software systems;Mathematics;Current measurement;Software measurement;Software quality","program testing","weighted proportional sampling strategy;optimization model;software testing","","","9","","","","","","IEEE","IEEE Conferences"
"The Research of Test Case Generation and Its Optimization Methods Based on Orthogonal Test Method and Greedy Algorithm","P. Tian; H. Leng; S. Yang; Y. Wang","NA; NA; NA; NA","2009 International Conference on Intelligent Human-Machine Systems and Cybernetics","","2009","1","","474","477","In software testing, the large number of test cases affected the progress of software testing seriously. In order to generate effective test cases with high-quality data. In this paper, a method which is about the test case generation and its optimization methods based on orthogonal test method and greedy algorithm is presented. First of all, the basic concepts, main features and the steps of generating test cases of the orthogonal test method are introduced; then, the optimal method of generated test cases using the greedy algorithm is gave; Finally, the test cases generation and its optimization are explained by using orthogonal test method and greedy algorithm, and which is applied to practical engineering tests.","","978-0-7695-3752","10.1109/IHMSC.2009.126","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5336118","","Optimization methods;Greedy algorithms;Software testing;System testing;Electronic mail;Intelligent systems;Man machine systems;Cybernetics;Design engineering;Design methodology","greedy algorithms;optimisation;program testing","test case generation;optimization;orthogonal test method;greedy algorithm;software testing","","","5","","","","","","IEEE","IEEE Conferences"
"The Applications of Pressure Test in the B/S System","D. Liu; L. Li; X. Yang; H. Zhai","NA; NA; NA; NA","2009 International Conference on Computational Intelligence and Software Engineering","","2009","","","1","4","Pressure test is an essential work of Web application performance test and one of the important means to ensure software quality and performance. Pressure test can find a lot of unexposed problems under normal circumstances, which means that it can find more system bugs than others. Pressure test tests the load conditions of system, devises test scheme for the system performance indicators and verifies whether the system performance indicators meet the established target or not through the test results. This paper comprehensively investigates the pressure test in the case of a typical B/S system-Reporting Platform. By adopting the software performance testing concept and considering the impact factors on system function and evaluation index system of website operation, we carried out pressure test over the Reporting Platform Web site using VSTS (Full life-cycle management tools: Visual Studio 2005 Team System) testing tools and developed a process for conducting such tests. In depth analysis of the results of the pressure test, we found the bottlenecks which impacted the system performance, so as to lay the foundation for optimizing, adjusting the system and improving the system performance.","","978-1-4244-4507","10.1109/CISE.2009.5363411","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5363411","","System testing;Software testing;System performance;Life testing;Application software;Software quality;Computer bugs;Software performance;Web sites;Software development management","Internet;program testing;software performance evaluation;software quality","pressure test application;B/S system;Web application performance test;software quality;system load condition;software performance testing;Website operation evaluation index system;reporting platform Web site;full life cycle management tool;visual studio 2005 team system;VSTS testing tool","","","6","","","","","","IEEE","IEEE Conferences"
"Multi-clock Domain SoC Test Scheduling Based on Ant Colony Optimization Algorithm","J. Shao; Y. Li; J. Wang; Y. Huang; X. Yu","NA; NA; NA; NA; NA","2009 Fourth International Conference on Internet Computing for Science and Engineering","","2009","","","47","51","This paper presents a variant test scheduling for multi-clock domain SoC. Test resources are allocated reasonably by using ant colony optimization, and test application time is minimized. Experimental results demonstrate that the proposed method improves test application time on previous methods.","","978-1-4244-6755-6978-1-4244-6754-9978-0-7695-4027","10.1109/ICICSE.2009.45","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5521635","test scheduling;ant colony optimization;multi-clock domain","Ant colony optimization;Scheduling algorithm;Frequency;Circuit testing;Clocks;Processor scheduling;Automatic testing;Application software;Bandwidth;Educational institutions","integrated circuit testing;optimisation;system-on-chip","multiclock domain SoC test scheduling;ant colony optimization algorithm;system-on-a-chip","","1","5","","","","","","IEEE","IEEE Conferences"
"A Max-Min Multiobjective Technique to Optimize Model Based Test Suite","U. Farooq; C. P. Lam","NA; NA","2009 10th ACIS International Conference on Software Engineering, Artificial Intelligences, Networking and Parallel/Distributed Computing","","2009","","","569","574","Generally, quality software production seeks timely delivery with higher productivity at lower cost. Redundancy in a test suite raises the execution cost and wastes scarce project resources. In model-based testing, the testing process starts with earlier software developmental phases and enables fault detection in earlier phases. The redundancy in the test suites generated from models can be detected earlier as well and removed prior to its execution. The paper presents a novel max-min multiobjective technique incorporated into a test suite optimization framework to find a better trade-off between the intrinsically conflicting goals. For illustration two objectives i.e. coverage and size of a test suite were used however it can be extended to more objectives. The study is associated with model based testing and reports the results of the empirical analysis on four UML based synthetic as well as industrial Activity Diagram models.","","978-0-7695-3642","10.1109/SNPD.2009.33","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5286609","Model Based Testing;Multiobjective Evolutionary Algorithm;Test Suite Optimization;UML","Software testing;Automatic testing;Redundancy;Fault detection;Distributed computing;Productivity;Costs;Unified modeling language;Algorithm design and analysis;Software engineering","evolutionary computation;minimax techniques;program testing;software quality;travelling salesman problems","max-min multiobjective technique;model based test suite optimisation;software quality production;execution cost;wastes scarce project resources;software developmental phases;test suite size;test suite coverage;UML;industrial activity diagram model;synthetic activity diagram model;fault detection","","3","15","","","","","","IEEE","IEEE Conferences"
"An Optimization Algorithm for Computing the Minimal Test Set of Circuits","C. Lu; X. Deng; C. Liu; Y. Wang","NA; NA; NA; NA","2008 International Symposium on Intelligent Information Technology Application Workshops","","2008","","","717","720","With the extraordinary development of electronic technology, complexity of circuit system increases sharply. More and more electric circuits are composed of digital and analog signal simultaneously, resulting in greater difficulty in circuit testing. Theory of discrete event system (DES) and relevant researches provide uniform, efficient and systematic methods for testing these mixed-signal circuits. In view of the circuit testing based on DES theory, how to find out the minimal test set of circuits is important part to shorten complicated work. Considering advantageous characteristics of the minimal test set, a novel approach involving with minimizing mixed-signal circuit testing-set algorithm is put forward, which is in light of discrete particle swarm optimization (DPSO) and can finally realize optimization. By virtue of ""velocity-position"" model referred to competition and cooperation among particles of swarm, an optimum solution is searched in result space. Experimental results testify high efficiency and feasibility of this proposed approach, which is superior to other optimization algorithms.","","978-0-7695-3505","10.1109/IITA.Workshops.2008.186","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4732037","minimal observable event set;discrete particle swarm optimization;hybrid circuit;discrete event system","Circuit testing;Particle swarm optimization;Optimization methods;System testing;Algorithm design and analysis;Chaos;Space technology;Information technology;Application software;Electronic equipment testing","discrete event systems;integrated circuit testing;mixed analogue-digital integrated circuits;particle swarm optimisation","discrete event system;DES theory;mixed-signal circuit testing-set algorithm;discrete particle swarm optimization;DPSO algorithm;circuit minimal test set;velocity-position model","","1","11","","","","","","IEEE","IEEE Conferences"
"AutoFlow: An automatic debugging tool for AspectJ software","S. Zhang; Z. Gu; Y. Lin; J. Zhao","School of Software, Shanghai Jiao Tong University, 800 Dongchuan Road, 200240, China; School of Software, Shanghai Jiao Tong University, 800 Dongchuan Road, 200240, China; School of Software, Shanghai Jiao Tong University, 800 Dongchuan Road, 200240, China; School of Software, Shanghai Jiao Tong University, 800 Dongchuan Road, 200240, China","2008 IEEE International Conference on Software Maintenance","","2008","","","470","471","Aspect-oriented programming (AOP) is gaining popularity with the wider adoption of languages such as AspectJ. During AspectJ software evolution, when regression tests fail, it may be tedious for programmers to find out the failure-inducing changes by manually inspecting all code editing. To eliminate the expensive effort spent on debugging, we developed AutoFlow, an automatic debugging tool for AspectJ software. AutoFlow integrates the potential of delta debugging algorithm with the benefit of change impact analysis to narrow down the search for faulty changes. It first uses change impact analysis to identify a subset of responsible changes for a failed test, then ranks these changes according to our proposed heuristic (indicating the likelihood that they may have contributed to the failure), and finally employs an improved delta debugging algorithm to determine a minimal set of faulty changes. The main feature of AutoFlow is that it can automatically reduce a large portion of irrelevant changes in an early phase, and then locate faulty changes effectively.","1063-6773","978-1-4244-2613-3978-1-4244-2614","10.1109/ICSM.2008.4658109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658109","","Debugging;Software;Programming;Java;Testing;Software engineering;Algorithm design and analysis","program debugging;program testing;software tools","AutoFlow;automatic debugging tool;AspectJ software;aspect-oriented programming;regression tests;delta debugging algorithm;change impact analysis","","2","8","","","","","","IEEE","IEEE Conferences"
"Optimization in Software Design & Integration Platform","L. S. Globa; N. A. Alekseyev; N. Pingina","National Technical University of Ukraine "Kyiv Polytechnical Institute", prosp. Peremogy, 37, Kyiv, 03056, Ukraine, E-mail: gls@densoft.com.ua, princess-net@bigmir.net; National Technical University of Ukraine "Kyiv Polytechnical Institute", prosp. Peremogy, 37, Kyiv, 03056, Ukraine, E-mail: Nikolay.A.Alexeyev@its.ntu-kpi.kiev.ua, princess-net@bigmir.net; National Technical University of Ukraine "Kyiv Polytechnical Institute", prosp. Peremogy, 37, Kyiv, 03056, Ukraine, E-mail: princess-net@bigmir.net","2007 9th International Conference - The Experience of Designing and Applications of CAD Systems in Microelectronics","","2007","","","318","319","The possibility of usage of Intel VTune products, that main purpose is the optimization of software work in one multiprocessor, for maximization of work effectiveness on solving difficult tasks on several servers is described.","","966-533-587","10.1109/CADSM.2007.4297565","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4297565","integration platform;software design;distributed computing;optimization","Design optimization;Software design;Systems engineering and theory;Information systems;System testing;Software tools;Displays;Yarn;Distributed computing;Data engineering","optimisation;program processors;software engineering","optimization;software design;integration platform;Intel VTune products;multiprocessor;distributed computing","","1","2","","","","","","IEEE","IEEE Conferences"
"Test Case Prioritization Based on Analysis of Program Structure","Z. Ma; J. Zhao","NA; NA","2008 15th Asia-Pacific Software Engineering Conference","","2008","","","471","478","Test case prioritization techniques have been empirically proved to be effective in improving the rate of fault detection in regression testing. However, most of previous techniques assume that all the faults have equal severity, which dose not meet the practice. In addition, because most of the existing techniques rely on the information gained from previous execution of test cases or source code changes, few of them can be directly applied to non-regression testing. In this paper, aiming to improve the rate of severe faults detection for both regression testing and non-regression testing, we propose a novel test case prioritization approach based on the analysis of program structure. The key idea of our approach is the evaluation of testing-importance for each module (e.g., method) covered by test cases. As a proof of concept, we implement $Apros$, a test case prioritization tool, and perform an empirical study on two real, non-trivial Java programs. The experimental result represents that our approach could be a promising solution to improve the rate of severe faults detection.","1530-1362;1530-1362","978-0-7695-3446","10.1109/APSEC.2008.63","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724580","Test case prioritization;call graph;program analysis","Fault detection;Software testing;Computer science;Computer bugs;Software engineering;Performance evaluation;Java;Information analysis;Fault location;System testing","Java;program testing;software engineering","test case prioritization;program structure analysis;regression testing;nontrivial Java programs","","11","21","","","","","","IEEE","IEEE Conferences"
"Modeling Software Testing Process Using HTCPN","L. Cai; W. Tong; B. Zhu; J. Zhang","NA; NA; NA; NA","2009 Fourth International Conference on Frontier of Computer Science and Technology","","2009","","","429","434","Software testing process has being separated gradually from software development. As a relatively independent process, software testing process simulation and optimization has been also started to catch people's attention. It is particularly important to improve the reuse of software testing. This paper proposes a systematic way to model the software testing processes STP-NET based on HTCPN, in order to reuse them. The performance of the process, such as average time and average waiting time, is simulated and analyzed using CPN Tools. According to the results, the process configuration parameters, the license allocation model of automatic testing tool, can be optimized. The efficiency of the testing process can be improved by simulation and optimization using CPN Tools.","2159-6301;2159-631X","978-1-4244-5467-9978-1-4244-5466-2978-0-7695-3932","10.1109/FCST.2009.53","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5392882","Testing Process;Model;Petri Net","Software testing;Petri nets;Business process re-engineering;Logic testing;Unified modeling language;Space technology;Technology management;Application software;Computer science;Laboratories","graph colouring;Petri nets;program testing;software tools","software testing process;HTCPN;software development;STP-NET;process configuration parameters;automatic testing tool;hierarchical time colored Petri Nets","","","11","","","","","","IEEE","IEEE Conferences"
"Providing the Guideline of Determining Quality Checklists Priorities Based on Evaluation Records of Software Products","C. Lee; B. Lee; C. Wu","NA; NA; NA","2008 15th Asia-Pacific Software Engineering Conference","","2008","","","169","176","COTS (commercial-off-the-shelf) software products are usually provided in a packaged style without the source code but with many ready-to-use functions. Generally, their vendors are reluctant to disclose the source code. Thus, the major way of quality evaluation and certification requires dynamic behavior testing, essentially black-box testing. Since observing every aspect of external software behavior is almost impossible, it is crucial to designate an adequate range for quality evaluation such as an adequate number of quality checklists or product quality metrics for external behavior testing. Hence, to establish rules of selecting quality evaluation criteria in systematic ways, there have been attempts to analyze and utilize the past records of software evaluation. In this paper, multiple characteristics of software are mapped as nodes to affect and determine the priority ranks of external software quality metrics on Bayesian belief network. The nodes are set to be under the influence of multiple inheritances so that every external characteristic of COTS software is considered thoroughly.","1530-1362;1530-1362","978-0-7695-3446","10.1109/APSEC.2008.75","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724545","software quality;checklist;priority","Guidelines;Software quality;Computer science;Certification;Software testing;Bayesian methods;Embedded software;Application software;Software engineering;Software packages","belief networks;software metrics;software packages;software quality","commercial-off-the-shelf software;dynamic behavior testing;black-box testing;quality evaluation criteria;software quality metrics;Bayesian belief network","","","14","","","","","","IEEE","IEEE Conferences"
"Generation of pairwise test sets using a simulated bee colony algorithm","J. D. McCaffrey","Microsoft MSDN / Volt VTE, USA","2009 IEEE International Conference on Information Reuse & Integration","","2009","","","115","119","Pairwise testing is a combinatorial technique used to reduce the number of test case inputs to a system in situations where exhaustive testing with all possible inputs is not feasible. The generation of pairwise test sets with a minimal size is an NP-complete problem and several deterministic algorithms have been published. This paper presents the results of generating pairwise test sets using a simulated bee colony algorithm. Compared to published results for seven benchmark problems, the simulated bee colony approach produced test sets which were comparable or better in terms of size for all seven problems. However, the simulated bee colony approach required significantly longer generation time than deterministic approaches in all cases. The results demonstrate that the generation of pairwise test sets using a simulated bee colony algorithm is possible, and suggest that the approach may be useful in testing scenarios where pairwise test set data will be reused.","","978-1-4244-4114-3978-1-4244-4116","10.1109/IRI.2009.5211598","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5211598","Combinatorial mathematics;pairwise testing;simulated bee colony algorithm;software quality;software testing","System testing;Software testing;Iterative algorithms;NP-complete problem;Benchmark testing;Combinatorial mathematics;Software algorithms;Software quality;Software systems;Performance evaluation","optimisation;program testing;set theory;software quality","pairwise test sets generation;simulated bee colony algorithm;pairwise testing;combinatorial technique;exhaustive testing;NP-complete problem;software quality;software testing situations","","8","16","","","","","","IEEE","IEEE Conferences"
"Parallel testing techniques for optimizing test program execution and reducing test time","S. Delgado","National Instruments, 11500 N Mopac Expressway, Austin, TX - 78759, USA","2008 IEEE AUTOTESTCON","","2008","","","439","441","Reducing test time continues to be a priority for test program developers as the complexity of next generation products and devices increases. Test program developers must provide complete test coverage while maintaining or reducing the test time of previous versions as complexity and feature concentration increases. Developers can use different techniques for reducing productspsila test times, such as running tests in parallel which reduces test time without sacrificing test coverage or quality. Other methods include decreasing test coverage by omitting certain lower priority tests or decreasing the quality of tests by only covering a subset of the ranges across which functionality is tested. This paper will discuss techniques for running tests in parallel for different test tasks and cover the factors that affect each type of taskpsilas performance.","1088-7725;1558-4550","978-1-4244-2225-8978-1-4244-2226","10.1109/AUTEST.2008.4662654","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4662654","parallel;ats;multicore;processor;autoscheduling;test;executive","Instruments;Software;Computer architecture;Magnetic cores;Pipeline processing;Testing;Operating systems","automatic testing","parallel testing;optimizing test program;test time reduction;test time execution;running tests;test tasks;task performance","","","","","","","","","IEEE","IEEE Conferences"
"Optimizing Test Process Action Plans by Blending Testing Maturity Model and Design of Experiments","H. Oh; B. Choi; H. Han; W. E. Wong","NA; NA; NA; NA","2008 The Eighth International Conference on Quality Software","","2008","","","57","66","Software testing is a core activity in quality assurance. To improve the test process, we can use best practice models which describe in detail what to do in organizational test processes. The improvement activities using best practice models are performed as follows: checking the current status of test processes, suggesting and planning new actions, and implementing the actions. However, it is difficult to apply all of these actions to the organization due to the limitation of resources. In this paper, we suggest a strategy for optimizing test process action plans. The background of this research is TMM (Testing Maturity Model), which is the most representative test process model. By applying design of experiments to the TMM assessment procedure, we can accept the actions selectively by statistical significance and find the best solution.","1550-6002;2332-662X","978-0-7695-3312","10.1109/QSIC.2008.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4601528","SW Testing;TMM;DOE","Organizations;Software testing;Capability maturity model;Testing;Software;Planning;Training","Capability Maturity Model;program testing;quality assurance;software quality","software testing;test process action plan optimization;software quality assurance;testing maturity model;design of experiment;TMM assessment procedure","","2","17","","","","","","IEEE","IEEE Conferences"
"Handling Constraints for Search Based Software Test Data Generation","R. Sagarna; X. Yao","NA; NA","2008 IEEE International Conference on Software Testing Verification and Validation Workshop","","2008","","","232","240","A major issue in software testing is the automatic generation of the inputs to be applied to the programme under test. To solve this problem, a number of approaches based on search methods have been developed in the last few years, offering promising results for adequacy criteria like, for instance, branch coverage. We devise branch coverage as the satisfaction of a number of constraints. This allows to formulate the test data generation as a constrained optimisation problem or as a constraint satisfaction problem. Then, we can see that many of the generators so far have followed the same particular approach. Furthermore, this constraint-handling point of view overcomes this limitation and opens the door to new designs and search strategies that, to the best of our knowledge, have not been considered yet. As a case study, we develop test data generators employing different penalty objective functions or multiobjective optimisation. The results of the conducted preliminary experiments suggest these generators can improve the performance of classical approaches.","","978-0-7695-3388","10.1109/ICSTW.2008.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4567013","","Software testing;Automatic testing;Search methods;System testing;Evolutionary computation;Computer science;Constraint optimization;Software systems;Automation;Probability distribution","constraint handling;constraint theory;optimisation;program testing;search problems","search based software test data generation;software testing;branch coverage;constraint satisfaction problem;constraint-handling;penalty objective functions;multiobjective optimisation","","10","23","","","","","","IEEE","IEEE Conferences"
"Predicting Fault Proneness of Classes Trough a Multiobjective Particle Swarm Optimization Algorithm","A. B. d. Carvalho; A. Pozo; S. Vergilio; A. Lenz","NA; NA; NA; NA","2008 20th IEEE International Conference on Tools with Artificial Intelligence","","2008","2","","387","394","Software testing is a fundamental software engineering activity for quality assurance that is also traditionally very expensive. To reduce efforts of testing strategies, some design metrics have been used to predict the fault-proneness of a software class or module. Recent works have explored the use of machine learning (ML) techniques for fault prediction. However most used ML techniques can not deal with unbalanced data and their results usually have a difficult interpretation. Because of this, this paper introduces a multi-objective particle swarm optimization (MOPSO) algorithm for fault prediction. It allows the creation of classifiers composed by rules with specific properties by exploring Pareto dominance concepts. These rules are more intuitive and easier to understand because they can be interpreted independently one of each other. Furthermore, an experiment using the approach is presented and the results are compared to the other techniques explored in the area.","1082-3409;2375-0197","978-0-7695-3440","10.1109/ICTAI.2008.76","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4669800","Particle Swarm Optimization;Multiobjective Optimization;Fault prediction;Software mining","Particle swarm optimization;Object oriented modeling;Software testing;Machine learning algorithms;Machine learning;Bayesian methods;Support vector machines;Software engineering;Quality assurance;Costs","learning (artificial intelligence);object-oriented programming;Pareto optimisation;particle swarm optimisation;program diagnostics;program testing;quality assurance;software metrics;software quality","software testing;software engineering;quality assurance;design metrics;fault-proneness prediction;software class;machine learning;multiobjective particle swarm optimization algorithm;Pareto dominance concept","","3","39","","","","","","IEEE","IEEE Conferences"
"Improving Testing Efficiency using Cumulative Test Analysis","I. Holden; D. Dalton","IBM, UK; NA","Testing: Academic & Industrial Conference - Practice And Research Techniques (TAIC PART'06)","","2006","","","152","158","It can be impossible to thoroughly test complex software projects with a large library of tests to be run in many environments and configurations. The cumulative test analysis (CTA) technique described reduces the time to find defects by prioritising and minimising the testing. Tests are chosen to target the product areas having the highest risk of defects. Test effectiveness, test code coverage, product code changes and changes to dependencies are monitored and analysed to prioritise the testing. Test results from build to build are accumulated. Build reports clearly identify areas at risk, test results, and the tests that must be run. Experiences with a prototype tool are discussed and conclusions drawn from the use of CTA show that defects are found sooner, more time is available for writing new tests and the focus of test execution moves towards product quality instead of simply test results","","0-7695-2672","10.1109/TAIC-PART.2006.13","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1691682","","Software testing;Product codes;Monitoring;Prototypes;Software libraries;Writing;Performance evaluation;Software prototyping;Software tools;Feedback","program testing;project management;risk management;software management","test efficiency improvement;cumulative test analysis;software project testing;CTA technique;risk areas","","","5","","","","","","IEEE","IEEE Conferences"
"Search-Based Testing Guidance Using Dominances vs. Control Dependencies","A. S. Ghiduk","NA","2009 16th Asia-Pacific Software Engineering Conference","","2009","","","145","151","Search-based optimization techniques have been utilized for a number of software engineering activities. The representation of the problem and the definition of the fitness function (FF) are two key ingredients for the application of search-based optimization to software engineering problems. Therefore, a well-defined fitness function is essential to the effectiveness and efficiency of the search-based testing (SBT). Several search based test-data generation techniques have been developed. A wide range of these techniques utilized the control dependencies (CD) for guiding the search in the direction of finding test data. To direct the SBT to generate test data, Ghiduk et al. have presented a search-based technique that utilizes the dominances (Dom) between the nodes of the control-flow graph (CFG) of the program under test. In this paper, we investigate the efficiency of dominances in a control-flow graph against the control dependencies in guiding the search for breeding test data. The paper provides a number of structures which cause problems for the search that is guided by the control dependencies to find the test data. The paper introduces two schemes for overcoming these problems. The first scheme improves the functions of the previous work to overcome control-dependencies problems. The second scheme presents a general form for a fitness function in terms of dominances and postdominances nodes. This function will significantly enhance the efficiency of the SBT; consequently the search-based testing overcomes the control-dependencies problems.","1530-1362;1530-1362","978-0-7695-3909","10.1109/APSEC.2009.13","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5358573","search-based testing;genetic algorithms;test-data generation;dominance;control dependecnies","Software engineering;Genetic algorithms;Switches;Software testing;Application software;Ant colony optimization;Simulated annealing;Particle swarm optimization;Cost function","optimisation;program testing;search problems;software engineering","search-based testing guidance;dominances;control dependencies;search-based optimization techniques;software engineering activities;search based test-data generation techniques;control-flow graph","","","13","","","","","","IEEE","IEEE Conferences"
"Search Algorithms for Regression Test Case Prioritization","Z. Li; M. Harman; R. M. Hierons","NA; NA; NA","IEEE Transactions on Software Engineering","","2007","33","4","225","237","Regression testing is an expensive, but important, process. Unfortunately, there may be insufficient resources to allow for the reexecution of all test cases during regression testing. In this situation, test case prioritization techniques aim to improve the effectiveness of regression testing by ordering the test cases so that the most beneficial are executed first. Previous work on regression test case prioritization has focused on greedy algorithms. However, it is known that these algorithms may produce suboptimal results because they may construct results that denote only local minima within the search space. By contrast, metaheuristic and evolutionary search algorithms aim to avoid such problems. This paper presents results from an empirical study of the application of several greedy, metaheuristic, and evolutionary search algorithms to six programs, ranging from 374 to 11,148 lines of code for three choices of fitness metric. The paper addresses the problems of choice of fitness metric, characterization of landscape modality, and determination of the most suitable search technique to apply. The empirical results replicate previous results concerning greedy algorithms. They shed light on the nature of the regression testing search space, indicating that it is multimodal. The results also show that genetic algorithms perform well, although greedy approaches are surprisingly effective, given the multimodal nature of the landscape","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.38","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4123325","Search techniques;test case prioritization;regression testing.","Greedy algorithms;Cost function;Genetic algorithms;Software testing;Libraries;Fault detection","genetic algorithms;greedy algorithms;program testing;search problems","regression testing;test case prioritization technique;greedy algorithm;metaheuristics;evolutionary search algorithm;fitness metric;genetic algorithm","","258","22","","","","","","IEEE","IEEE Journals & Magazines"
"Measurement and control for risk-based test cases and activities","E. Souza; C. Gusmao; K. Alves; J. Venancio; R. Melo","Department of Systems and Computing at Pernambuco University, Rua Benfica, 455, Madalena, 50750-410, Recife - PE, Brasil; Department of Systems and Computing at Pernambuco University, Rua Benfica, 455, Madalena, 50750-410, Recife - PE, Brasil; Department of Systems and Computing at Pernambuco University, Rua Benfica, 455, Madalena, 50750-410, Recife - PE, Brasil; Department of Systems and Computing at Pernambuco University, Rua Benfica, 455, Madalena, 50750-410, Recife - PE, Brasil; Department of Systems and Computing at Pernambuco University, Rua Benfica, 455, Madalena, 50750-410, Recife - PE, Brasil","2009 10th Latin American Test Workshop","","2009","","","1","6","Risk-based testing is an approach that consists of a set of activities regarding risk factors identification related to software requirements. Once identified, the risks are prioritized according to its likelihood and impact, and the test cases are projected based on the strategies for treatment of the identified risk factors. Then, test efforts are continuously adjusted according the risk monitoring. Most risk-based testing approaches focuses on activities related to risk identification, analysis and prioritizing. However, metrics are fundamental as they quantify characteristics of a process or product and support software project management activities. In this light, this paper proposes and discusses risk-based testing metrics to measure and control test cases and test activities progress, efforts and costs.","2373-0862","978-1-4244-4207-2978-1-4244-4206","10.1109/LATW.2009.4813802","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4813802","Software Testing;Risk-based Testing;Metrics;Control and Measurement","Software testing;Costs;Software quality;System testing;Monitoring;Software measurement;Risk analysis;Control systems;Paramagnetic resonance;Project management","program testing;software metrics;software quality","risk factors identification;software requirements;software project management;risk-based testing metrics","","5","21","","","","","","IEEE","IEEE Conferences"
"Prioritization of Regression Tests using Singular Value Decomposition with Empirical Change Records","M. Sherriff; M. Lake; L. Williams","NA; NA; NA","The 18th IEEE International Symposium on Software Reliability (ISSRE '07)","","2007","","","81","90","During development and testing, changes made to a system to repair a detected fault can often inject a new fault into the code base. These injected faults may not be in the same files that were just changed, since the effects of a change in the code base can have ramifications in other parts of the system. We propose a methodology for determining the effect of a change and then prioritizing regression test cases by gathering software change records and analyzing them through singular value decomposition. This methodology generates clusters of files that historically tend to change together. Combining these clusters with test case information yields a matrix that can be multiplied by a vector representing a new system modification to create a prioritized list of test cases. We performed a post hoc case study using this technique with three minor releases of a software product at IBM. We found that our methodology suggested additional regression tests in 50% of test runs and that the highest-priority suggested test found an additional fault 60% of the time.","1071-9458;2332-6549","0-7695-3024-9978-0-7695-3024","10.1109/ISSRE.2007.25","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4402199","","Singular value decomposition;System testing;Software testing;Programming;Fault detection;Matrix decomposition;Software reliability;Reliability engineering;Lakes;Software performance","program testing;singular value decomposition;software maintenance","regression tests prioritization;singular value decomposition;empirical change records;detected fault repair;software change records;system modification;software product;regression tests","","25","25","","","","","","IEEE","IEEE Conferences"
"Optimization of test engineering utilizing evolutionary computation","J. Engler","Rockwell Collins, Inc., 400 Collins Rd N.E., Cedar Rapids, IA 52498, USA","2009 IEEE AUTOTESTCON","","2009","","","447","452","Test engineering often experiences pressures to produce test stations and software in a short time frame with constrained budgets. Since test is a negative influence towards product costs, it is crucial to optimize the processes of test station software creation as well as the configuration of the test station itself. This paper introduces novel methodologies for optimized station configuration and automated station software generation. These two optimizations utilize evolutionary computation to automatically generate software for the test station and to offer optimal configurations of the station based upon testing requirements. Presented is a modified genetic programming algorithm for the creation of test station software (e.g. COTS software drivers). The genetic algorithm is improved through use of adaptive memory to recall historic schemas of high fitness. From the automated software generation an optimal station configuration is produced based upon the requirements of the testing to be performed. This system has been implemented in industry and an actual industrial case study is presented to illustrate the efficiency of this novel optimization technique. Comparisons with standard genetic programming techniques are offered to further illustrate the efficiency of this methodology.","1088-7725;1558-4550","978-1-4244-4980-4978-1-4244-4981","10.1109/AUTEST.2009.5314025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5314025","","Evolutionary computation;Software testing;Automatic testing;Genetic programming;Time factors;Cost function;Optimization methods;Software algorithms;Genetic algorithms;Software performance","automatic test pattern generation;automatic test software;genetic algorithms","test engineering optimization;evolutionary computation;test station software creation;automated station software generation;testing requirements;genetic programming algorithm;adaptive memory","","1","8","","","","","","IEEE","IEEE Conferences"
"An Empirical Evaluation of Test Suite Reduction for Boolean Specification-Based Testing (Short Paper)","X. Zhang; B. Xu; Z. Chen; C. Nie; L. Li","NA; NA; NA; NA; NA","2008 The Eighth International Conference on Quality Software","","2008","","","270","275","As the cost of executing and maintaining a large test suite is always expensive, many test suite reduction techniques have been proposed and some empirical studies of test suite reduction techniques have already been reported. The aim of this paper is to provide a guideline for choosing the appropriate test suite reduction techniques for Boolean specification-based testing. Four typical heuristic reduction strategies: G, GE, GRE, and H are introduced to be compared empirically. Furthermore, testing requirement optimization is combined to enhance these four reduction strategies. This paper presents the empirical evaluation of these reduction strategies based on a set of Boolean specifications from TCAS II.","1550-6002;2332-662X","978-0-7695-3312","10.1109/QSIC.2008.25","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4601554","software testing;test suite reduction;boolean testing;testing requirement;optimization","Testing;Complexity theory;Optimization;Size measurement;Software;Time measurement;Knowledge engineering","Boolean functions;formal specification;program testing","test suite reduction;Boolean specification-based testing;software testing","","1","13","","","","","","IEEE","IEEE Conferences"
"Automatic Test Data Generation Based on SAMPSO Algorithm","F. Wei; S. Jiang","NA; NA","2009 International Conference on Computational Intelligence and Software Engineering","","2009","","","1","4","This paper proposes an automatic test data generation method based on Simple and Adaptive Mutation Particle Swarm Optimization algorithm. According to the particle velocity independency in the evolution, this algorithm removes particle velocity , only the position of particle control the process of evolution, avoiding problems such as slow of convergence in the late evolutionary and low-precision radiation of particle that particle velocity brings about; according to fit variance and current optimum solution, we find the current mutation rate of best particle, the operation of mutation can improve ability of global searching in the earlier evolutionary. Test examples show that it is better than basic particle swarm optimization algorithm and can improve the efficiency of automated test data generation.","","978-1-4244-4507","10.1109/CISE.2009.5366342","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5366342","","Automatic testing;Particle swarm optimization;Iterative algorithms;Genetic mutations;Software testing;Logic testing;Computer science;Paper technology;Electronic mail;Automatic control","data handling;particle swarm optimisation;program testing","automatic test data generation method;adaptive mutation particle swarm optimization algorithm;particle velocity;software testing;simple mutation particle swarm optimization algorithm","","","15","","","","","","IEEE","IEEE Conferences"
"Widening the Goal Posts: Program Stretching to Aid Search Based Software Testing","K. Ghani; J. A. Clark","NA; NA","2009 1st International Symposium on Search Based Software Engineering","","2009","","","122","131","Search based software testing has emerged in recent years as an important research area within automated software test data generation. The general approach of couching the satisfaction of test goals as numerical optimisation problems has been applied to a variety of problems such as satisfying structural coverage criteria, specification falsification, exception generation, breaking unit pre-conditions and software hazard discovery. However, some test goals may be hard to satisfy. For example, a program branch may be difficult to reach via a search based technique, because the domain of the data that causes it to be taken is exceedingly small or the non-linearity of the ""fitness landscape'' precludes the provision of effective guidance to the search for test data. In this paper we propose to ""stretch'' relevant conditions within a program to make them easier to satisfy. We find test data that satisfies the corresponding test goal of the stretched program. We then seek to transform the stretched program by stages back to the original, simultaneously migrating the obtained test data to produce test data that satisfies the goal for the original program. The ""stretching'' device is remarkably simple and shows significant promise for obtaining hard-to-find test data and also gives efficiency improvements over standard search based testing approaches.","","978-0-7695-3675","10.1109/SSBSE.2009.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5033191","Program Stretching;Program Transformation;Search based test data generation;Simulated Annealing","Software testing;Software engineering;Automatic testing;Costs;Computer science;Hazards;Performance evaluation;Programming;Automation;System testing","numerical analysis;optimisation;program testing","search based software testing;program stretching;automated software test data generation;numerical optimisation problems;structural coverage criteria;specification falsification;exception generation;breaking unit pre-conditions;software hazard discovery","","3","35","","","","","","IEEE","IEEE Conferences"
"Study of the test flow optimization method based on utility function with discounted reward factor","Z. i; D. eng; J. hou; P. iu; D. hang","College of Electrical Engineering, Hohai University, 210098, Nanjing, China; College of Electrical Engineering, Hohai University, 210098, Nanjing, China; College of Electrical Engineering, Hohai University, 210098, Nanjing, China; College of Electrical Engineering, Hohai University, 210098, Nanjing, China; College of Electrical Engineering, Hohai University, 210098, Nanjing, China","2009 IEEE International Conference on Intelligent Computing and Intelligent Systems","","2009","2","","13","17","A novel software framework for radar fault diagnosis is proposed. The framework separates all diagnosis algorithms from the test flows in fault isolation process. In the framework, the test flow and the executing loop of diagnostic algorithms are parallel processing, and test flow can be optimized dynamically. Define a utility function with discounted reward factor. Based on the utility function and framework, the paper utilizes an example to explains how to search the best flow from candidate flows.","","978-1-4244-4754-1978-1-4244-4738","10.1109/ICICISYS.2009.5358069","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5358069","Test flow;Optimization method;Fault diagnosis;Untility function","Optimization methods;Radar;Software testing;Fault diagnosis;Isolation technology;Diagnostic expert systems;System testing;Switches;Software algorithms;Automatic testing","fault diagnosis;parallel processing;radar computing","test flow optimization method;utility function;discounted reward factor;software framework;radar fault diagnosis;fault isolation process;parallel processing","","","23","","","","","","IEEE","IEEE Conferences"
"Search-based testing of complex simulink models containing stateflow diagrams","A. Windisch","Technische Universität Berlin, Daimler Center for Automotive IT Innovations - DCAITI, Germany","2009 31st International Conference on Software Engineering - Companion Volume","","2009","","","395","398","Model-based software design is constantly becoming more important and thus requiring systematic model testing. Test case generation constitutes a critical activity that is cost-intensive, time-consuming and error-prone when done manually. Hence, an automation of this process is required. One automation approach is search-based testing for which the task of generating test data is transformed into an optimization problem which is solved using metaheuristic search techniques. However, only little work has been done so far applying search-based testing techniques to continuous functional models, such as SIMULINK STATEFLOW models. This paper presents the current state of my thesis developing a new approach for automatically generating continuous test data sets achieving high structural model coverage for SIMULINK models containing STATEFLOW diagrams using search-based testing. The expected contribution of this work is to demonstrate how search-based testing techniques can be applied successfully to continuous functional models and how to cope with the arising problems such as generating and optimizing continuous signals, covering structural model elements and dealing with the complexity of the models.","","978-1-4244-3495","10.1109/ICSE-COMPANION.2009.5071030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5071030","","Signal generators;System testing;Automatic testing;Software testing;Automation;Embedded system;Automotive engineering;Embedded software;Communication industry;Signal processing","optimisation;program testing;systems analysis","search-based testing;Simulink models;Stateflow diagrams;model-based software design;systematic model testing;test case generation;optimization problem","","5","8","","","","","","IEEE","IEEE Conferences"
"Optimal testing resource allocation models for modular software","R. Rajan; R. B. Misra","NA; NA","RAMS '06. Annual Reliability and Maintainability Symposium, 2006.","","2006","","","104","109","In this paper, we propose a generic cost function, which incorporates failure cost as a quality measure. Simple and fast solution algorithms are presented to help managers to make best decisions. Proposed model helps the project managers to determine the amount of tests required by each module before actual testing begins. The purpose of this paper is to formulate the optimization problems which software managers face and to propose simple and fast solution procedures to handle them","0149-144X","1-4244-0007-41-4244-0008","10.1109/RAMS.2006.1677358","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1677358","","Software testing;Resource management;System testing;Cost function;Software reliability;Software systems;Software safety;Project management;Application software;Life testing","optimisation;program testing;resource allocation;software cost estimation;software quality;software reliability","optimal testing;resource allocation model;modular software;generic cost function;optimization problem;software reliability","","1","16","","","","","","IEEE","IEEE Conferences"
"MILU: A Customizable, Runtime-Optimized Higher Order Mutation Testing Tool for the Full C Language","Y. Jia; M. Harman","NA; NA","Testing: Academic & Industrial Conference - Practice and Research Techniques (taic part 2008)","","2008","","","94","98","This paper introduces MILU, a C mutation testing tool designed for both first order and higher order mutation testing. All previous mutation testing tools apply all possible mutation operators to the program under test. By contrast, MILU allows customization of the set of mutation operators to be applied. To reduce runtime cost, MILU uses a novel 'test harness' technique to embed mutants and their associated test sets into a single-invocation procedure.","","978-0-7695-3383","10.1109/TAIC-PART.2008.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4670308","MILU;mutation testing tool","Genetic mutations;Testing;Educational institutions;Runtime;Costs;Animals;Horses","C language;program testing;software tools","C language;runtime-optimized higher order mutation testing tool;mutation operator;test harness technique;single-invocation procedure;MiLu","","48","22","","","","","","IEEE","IEEE Conferences"
"Intra Component GUI Test Case Generation Technique","M. U. Hayat; N. Qadeer","Mohammad Ali Jinnah University, Islamabad. umar_h@hotmail.com; Federal Urdu University (FUUAST), Islamabad. nauman.qadeer@fuuastisb.edu.pk","2007 International Conference on Information and Emerging Technologies","","2007","","","1","5","GUI is rapidly growing and has become the critical component for all software applications approximately. At present, GUI techniques for generating the test cases are difficult to understand and implement. GUI testing has become time consuming and hectic due to the fact that it contains lot of permutation of interactions. Since exhaustive testing is not possible, it is mandatory to optimize the testing process so that it can easily be scalable and increase the software reliability. The aim of this paper is to optimize the GUI test cases. We have proposed a technique for optimized test case generation with the help of interaction and termination events within one GUI component and present the evaluation of the technique with the help of a case study at the end of this paper.","","978-1-4244-1246-4978-1-4244-1247","10.1109/ICIET.2007.4381328","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4381328","GUI;GUI Testing;Test Case Generation","Graphical user interfaces;Software testing;Application software;Software reliability;Costs;Navigation;Controllability;Scalability;Software systems;Software maintenance","graphical user interfaces;program testing;software reliability","intra component GUI test case generation technique;software reliability;optimization","","","11","","","","","","IEEE","IEEE Conferences"
"Briefing power/reliability optimization in embedded software design","F. Vargas; C. A. Rocha; B. Pianta; M. P. Garcia; C. L. Ongil; M. G. Valderas; L. Entrena","Electrical Engineering Dept., Catholic University - PUCRS, Av. Ipiranga, 6681., 90619-900 Porto Alegre - Brazil; Electrical Engineering Dept., Catholic University - PUCRS, Av. Ipiranga, 6681., 90619-900 Porto Alegre - Brazil; Electrical Engineering Dept., Catholic University - PUCRS, Av. Ipiranga, 6681., 90619-900 Porto Alegre - Brazil; Microelectronic Design and Applications, University Carlos III of Madrid, Av. Universidad, 30., 28911, Leganés, Spain; Microelectronic Design and Applications, University Carlos III of Madrid, Av. Universidad, 30., 28911, Leganés, Spain; Microelectronic Design and Applications, University Carlos III of Madrid, Av. Universidad, 30., 28911, Leganés, Spain; Microelectronic Design and Applications, University Carlos III of Madrid, Av. Universidad, 30., 28911, Leganés, Spain","2009 15th IEEE International On-Line Testing Symposium","","2009","","","185","186","We propose an approach to optimize the number of checkpoints to be inserted along with an application code. The approach is based on a profiling process that analyzes the application code control-flow graph to find the best trade-off between the minimum number of checkpoints to be inserted in the code for a given fault detection coverage, with minimum impact in terms of power increase. The checkpoints are verified at runtime by the processor against compilation-time pre-computed values every time the processor reaches these points. Experiments with a PIC18 microcontroller have been carried out to demonstrate the benefits from using the proposed approach.","1942-9398;1942-9401","978-1-4244-4596-7978-1-4244-4595","10.1109/IOLTS.2009.5196007","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5196007","","Design optimization;Embedded software;Software design;Counting circuits;Application software;Frequency;Instruments;Power system reliability;Histograms;Energy consumption","checkpointing;data flow graphs;embedded systems;object-oriented methods;optimising compilers;power aware computing;program diagnostics;software fault tolerance","power-reliability optimization;embedded software design;checkpoint;application code analysis;profiling process;control-flow graph;fault detection coverage","","1","3","","","","","","IEEE","IEEE Conferences"
"Test Case Prioritization Using Relevant Slices","D. Jeffrey; N. Gupta","The University of Arizona, USA; The University of Arizona, USA","30th Annual International Computer Software and Applications Conference (COMPSAC'06)","","2006","1","","411","420","Software testing and retesting occurs continuously during the software development lifecycle to detect errors as early as possible. The sizes of test suites grow as software evolves. Due to resource constraints, it is important to prioritize the execution of test cases so as to increase chances of early detection of faults. Prior techniques for test case prioritization are based on the total number of coverage requirements exercised by the test cases. In this paper, we present a new approach to prioritize test cases based on the coverage requirements present in the relevant slices of the outputs of test cases. We present experimental results comparing the effectiveness of our prioritization approach with that of existing techniques that only account for total requirement coverage, in terms of ability to achieve high rate of fault detection. Our results present interesting insights into the effectiveness of using relevant slices for test case prioritization","0730-3157;0730-3157","0-7695-2655","10.1109/COMPSAC.2006.80","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4020103","","Software testing;Fault detection;Computer science;Programming;Life testing;Computer errors;Time factors","program testing;software fault tolerance","test case prioritization;software testing;software development lifecycle;fault detection","","41","17","","","","","","IEEE","IEEE Conferences"
"Automated functional testing of search engine","Lingzi Jin","New Media Technology Department, Yell Group, Queens Walk, Reading, UK, RG1 7PT","2009 ICSE Workshop on Automation of Software Test","","2009","","","97","100","This paper describes the construction of an automated test framework for search engine and reports our current practice in establishing a process for test automation. The paper presents the technical solutions that overcome the difficulties in search engine testing, which involve large volume of data, complicated ranking rules and randomization in ordering search results. One reason that many test automation efforts failed in industry is because automated scripts are not developed and structured for future maintenance and evolution. The test framework enables effective and efficient reuse and maintenance of test scripts. The paper also discusses issues related to the process of test automation. They are equally as important as overcoming technical barriers. All of these discussed above have been applied to the automated functional testing of a commercial search engine.","","978-1-4244-3711","10.1109/IWAST.2009.5069046","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5069046","","Automatic testing;Search engines;Automation;Logic testing;Software testing;Displays;Databases;Quality assurance;Project management;Filters","automatic testing;search engines","automated functional testing;test automation;search engine testing;ranking rules;randomization","","","8","","","","","","IEEE","IEEE Conferences"
"Accelerated 65nm Yield Ramp through Optimization of Inspection on Process-Design Sensitive Test Chips","S. Chong; E. Rying; A. Perry; S. Lam; M. A. S. Lawrence; A. Stamper","IBM Microelectronics Division, Hopewell Junction, NY, USA; PDF Solutions, Inc., San Jose, CA, USA; KLA-Tencor Corporation, Hopewell Junction, NY, USA; PDF Solutions, Inc., San Jose, CA, USA; IBM Microelectronics Division, Hopewell Junction, NY, USA; IBM Microelectronics Division, Hopewell Junction, NY, USA","2007 IEEE/SEMI Advanced Semiconductor Manufacturing Conference","","2007","","","69","73","This paper describes an integrated methodology that combines short-flow test chips useful for exploring process-design systematic as well as random failure modes and an advanced inspection tool platform to characterize and monitor key Defects-of-Interest for accelerated defect-based yield learning at the 65 nm technology node. Utilization of a unique fast electrical testing scheme, rapid analysis software along with optimized inspection facilitated shorter learning cycles for accelerated process development. Knowledge derived from the CVreg- based inspection setup in a leading 300 mm fab was successfully transferred to manufacturing to facilitate inspection optimization for key Defects-of-Interest on product wafers.","1078-8743;2376-6697","1-4244-0652-81-4244-0653","10.1109/ASMC.2007.375083","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4259249","Characterization Vehicle®;CV® Test chips;Defect Inspection;Defect Overlay Analysis;S/N analysis;KLA-Tencor;2800;Process-Design interactions","Life estimation;Inspection;System testing;Failure analysis;Semiconductor device testing;Performance analysis;Image sensors;Optical sensors;Optimization methods;Condition monitoring","electronic engineering computing;failure analysis;inspection;integrated circuit design;integrated circuit testing;integrated circuit yield;life testing","process-design sensitive test chips;yield ramp;integrated methodology;random failure modes;short-flow test chips;advanced inspection tool platform;defects-of-interest monitoring;accelerated defect-based yield learning;unique fast electrical testing scheme;rapid analysis software;optimized inspection;accelerated process development;CV- based inspection setup;size 300 mm;size 65 nm","","2","4","","","","","","IEEE","IEEE Conferences"
"Research and Implementation of the Hardware/Software Co-design Based on Structure Test Model of SoC","G. Jing; Z. Chen; X. Yu","NA; NA; NA","2009 Second International Symposium on Computational Intelligence and Design","","2009","1","","526","530","The structure test of system-on-chip is modeled based on ITC '02 test benchmark circuits. In this paper, the idea of hardware /software co-design is used for the division and design of hardware and software in SoC testing. The principles of the hardware/software co-design and testing procedures of the structure test model of SoC is introduced in this paper. We present a systematic test specification to restrict the subsequent testing activities in SoC testing. Moreover, the division of TAM and design of IEEE std 1500 wrapper are studied. Finally we put forward an optimization strategy suitable for SoC testing.","","978-0-7695-3865","10.1109/ISCID.2009.138","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5368740","the hardware/software co-design;SoC testing;structure test model;IEEE std 1500","Hardware;Software testing;Circuit testing;System-on-a-chip;System testing;Benchmark testing;Costs;Integrated circuit testing;Computer science;Integrated circuit modeling","hardware-software codesign;logic testing;system-on-chip","hardware-software codesign;system-on-chip structure test model;ITC '0 2 test benchmark circuits;systematic test specification;IEEE std 1500 wrapper","","","7","","","","","","IEEE","IEEE Conferences"
"Leveraging Design Patterns and Design of Experiments for Testing Software","K. Araujo; J. B. Bowles","NA; NA","2007 Annual Reliability and Maintainability Symposium","","2007","","","238","243","The inclusion of software design patterns in a thoughtful development process may be leveraged to expedite the testing process. Our results imply that the use of a testing strategy designed to access subsystem interfaces such as the facade pattern is a more efficient approach than statistical experimental design, one of the more traditional testing strategies. Statistical experiments are attractive if one is concerned with factor interactions but are more time-consuming to set up both in the selection of factors and levels and in the execution of repeated test cases","0149-144X","0-7803-9766-50-7803-9767","10.1109/RAMS.2007.328062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4126356","","Software testing;Software design;US Department of Energy;Educational institutions;Design for experiments;Design optimization;Performance evaluation","design of experiments;program testing;software engineering","design of experiments;software testing;software design patterns;testing strategy","","","14","","","","","","IEEE","IEEE Conferences"
"Functional Search-based Testing from State Machines","R. Lefticaru; F. Ipate","NA; NA","2008 1st International Conference on Software Testing, Verification, and Validation","","2008","","","525","528","The application of metaheuristic search techniques in test data generation has been extensively investigated in recent years. Most studies, however, have concentrated on the application of such techniques in structural testing. The use of search-based techniques in functional testing is less frequent, the main cause being the implicit nature of the specification. This paper investigates the use of search-based techniques for functional testing, having the specification in form of a state machine. Its purpose is to generate input data for chosen paths in a state machine, so that the parameter values provided to the methods satisfy the corresponding guards and trigger the desired transitions. A general form of a fitness function for an individual path is presented and this approach is empirically evaluated using three search techniques: simulated annealing, genetic algorithms and particle swarm optimization.","2159-4848","978-0-7695-3127","10.1109/ICST.2008.32","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4539585","search-based testing;genetic algorithms;simulated annealing;particle swarm optimization;finite state machine","Software testing;Automatic testing;Simulated annealing;Application software;Unified modeling language;Genetic algorithms;Particle swarm optimization;Automata;Fault detection;Space exploration","finite state machines;genetic algorithms;particle swarm optimisation;program testing;search problems;simulated annealing","functional search-based testing;state machines;metaheuristic search techniques;test data generation;structural testing;simulated annealing;genetic algorithms;particle swarm optimization","","14","28","","","","","","IEEE","IEEE Conferences"
"Study on the Automatic Test Framework Based on Three-tier Data Driven Mechanism","T. Wu; Y. Wan; Y. Xi; C. Chen","NA; NA; NA; NA","2009 Eighth IEEE/ACIS International Conference on Computer and Information Science","","2009","","","996","1001","Through analyzing on work principle of GUI automated testing tools, automatic test script language, characteristic of graphical user interface application and analysis of testing process, this paper presents an automatic test framework based on three-tier data driven mechanism. This framework provides optimized automatic test template and support library, decreases developing time of automatic test script, implements script and user defined function reuse, and does test aiming at different test size. The experiment shows that it has higher test efficiency than DDE automatic test framework.","","978-0-7695-3641","10.1109/ICIS.2009.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5223223","automatic test;test framework;data driven mechanism","Automatic testing;Automatic control;Software testing;Graphical user interfaces;System testing;Software libraries;Information analysis;Algorithms;Concrete;Computer interfaces","authoring languages;graphical user interfaces;program testing;software libraries;software reusability","automatic test framework;three-tier data driven mechanism;GUI;software testing tool;automatic test script language;graphical user interface;support library;user defined function reuse","","","8","","","","","","IEEE","IEEE Conferences"
"An Evaluation of Model Checkers for Specification Based Test Case Generation","G. Fraser; A. Gargantini","NA; NA","2009 International Conference on Software Testing Verification and Validation","","2009","","","41","50","Under certain constraints the test case generation problem can be represented as a model checking problem, thus enabling the use of powerful model checking tools to perform the test case generation automatically. There are, however, several different model checking techniques, and to date there is little evidence and comparison on which of these techniques is best suited for test case generation. This paper presents the results of an evaluation of several different model checkers on a set of realistic formal specifications given in the SCR notation. For each specification test cases are generated for a set of coverage criteria with each of the model checkers using different configurations. The evaluation shows that the best suited model checking technique and optimization very much depend on the specification that is used to generate test cases. However, from the experiments we can draw general conclusions about which optimizations are useful and which model checking technique is best suited for which type of model. Finally, we demonstrate that by combining several model checking techniques it is possible to significantly speed up test case generation and also achieve full test coverage for cases where none of the techniques by itself would succeed.","2159-4848","978-1-4244-3775-7978-0-7695-3601","10.1109/ICST.2009.33","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4815336","test case generation;model checking;specification based testing;model based testing;automated testing","Logic testing;Software testing;Automatic testing;Power generation;Formal specifications;Thyristors;System testing;Performance evaluation;Programming;Automation","formal specification;program testing","model checkers evaluation;specification based test case generation;model checking problem;formal specifications;model checking technique;software testing;software development process","","6","33","","","","","","IEEE","IEEE Conferences"
"White-box approaches for improved testing and analysis of configurable software systems","M. Nita; D. Notkin","Computer Science & Engineering, University of Washington, USA; Computer Science & Engineering, University of Washington, USA","2009 31st International Conference on Software Engineering - Companion Volume","","2009","","","307","310","There is a significant conceptual gap between the source code of a configurable system and the runtime behaviors of its individual configurations. In the source, configurations are woven together into a conceptually unified program. At runtime, however, they are largely treated as independent executables. This gap leads to static analyses that, by acting on the source representing the entire configurable system, yield imprecise results with respect to individual executables. Testing, in contrast, acts on individual executables without leveraging the configurable codebase per se. In this paper, we sketch a research path that seeks to narrow the configuration source-runtime gap, based on the observation that most configurations share significant amounts of source-level structure (hence ldquowhite-boxrdquo) with other, related, configurations. We seek to identify and exploit this structure to reduce analysis and testing effort by sharing analysis and test results among related configurations.","","978-1-4244-3495","10.1109/ICSE-COMPANION.2009.5071008","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5071008","","Software testing;System testing;Software systems;Runtime;Costs;Computer science;Optimizing compilers;Code standards;Laboratories;Programming profession","program diagnostics;program testing;systems analysis","white-box approaches;configurable software system testing;source code;runtime behaviors;static analysis","","3","13","","","","","","IEEE","IEEE Conferences"
"A Match-Based Approach to Optimize Conformance Test Sequence Generation Using Mp-Method","J. Xiao","NA","2009 International Forum on Computer Science-Technology and Applications","","2009","1","","392","395","An important issue in protocol conformance testing is how to generate test sequences in an efficient and effective way that achieves the required fault detection coverage. We proposed an approach for finding shorter test sequences for protocol conformance testing based on the Wp method in our previous work. While the method generated good quality test sequences, an extra leading sequence may have to be added if the final test sequence generated was not started from the same starting state of the given FSM. A new approach is proposed in this paper to overcome this problem thus to improve the quality of the final test sequence. The new test sequence generated always starts from the same starting state of the given FSM. This effectively reduces the length of generated test sequence.","","978-1-4244-5423-5978-1-4244-5422-8978-0-7695-3930","10.1109/IFCSTA.2009.101","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5385050","conformance testing;test sequence;Wp method;match;asymmetric traveling salesman problem (ATSP)","Testing;Protocols;Fault detection;Traveling salesman problems;Computer applications;Application software;Computer security;Australia;Control system synthesis;Communication system control","conformance testing;finite state machines;optimisation;protocols","match based approach;conformance test sequence generation optimization;mp method;protocol conformance testing;fault detection coverage;wp method;FSM","","1","10","","","","","","IEEE","IEEE Conferences"
"Exploiting Thread-Level Parallelism in Functional Self-Testing of CMT Processors","A. Apostolakis; M. Psarakis; D. Gizopoulos; A. Paschalis; I. Parulkar","NA; NA; NA; NA; NA","2009 14th IEEE European Test Symposium","","2009","","","33","38","Major microprocessor vendors have integrated functional software-based self-testing in their manufacturing test flows during the last decade. Functional self-testing is performed by test programs that the processor executes at-speed from on-chip memory. Multiprocessors and multithreaded architectures are constantly becoming the typical general-purpose computing paradigm, and thus the various existing uniprocessor functional self-testing schemes must be adopted and adjusted to meet the testing requirements of complex multiprocessors. A major challenge in porting a functional self-testing approach from the uniprocessor to the multiprocessor case is to take advantage of the inherent execution parallelism offered by the multiple cores and the multiple threads in order to reduce test execution time. In this paper, we study the application of functional self-testing to chip multithreaded (CMT) processors. We propose a method that exploits thread-level parallelism (TLP) to speed up the execution of self-test routines in every physical core of a multiprocessor chip. The proposed method effectively splits the self-test routines into shorter ones, assigns the new routines to the hardware threads of the core and schedules their execution in order to minimize the core idle intervals due to cache misses or long latency operations and maximize the utilization of core computing resources. We demonstrate our method in the open-source CMT multiprocessor model, Sunpsilas OpenSPARC T1, which contains eight CPU cores, each one supporting four hardware threads. Our experimental results show a self-test execution speedup of more than three times compared to the single thread execution.","1530-1877;1558-1780","978-0-7695-3703","10.1109/ETS.2009.31","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5170456","Chip multithreading;multiprocessors;micro-processor testing;functional self-testing;software-based self-testing;test time optimization","Built-in self-test;Automatic testing;Yarn;Parallel processing;Hardware;Microprocessors;Manufacturing;Software testing;Performance evaluation;Computer architecture","automatic testing;cache storage;microprocessor chips;multiprocessing systems;multi-threading;processor scheduling","thread-level parallelism;CMT processor functional self-testing;on-chip memory;multithreaded architecture;general-purpose computing paradigm;chip multithreaded processor;functional self-testing approach;uniprocessor;inherent execution parallelism;cache operation;Sun OpenSPARC Tl;hardware thread;software-based self-testing;CPU core;single thread execution","","6","11","","","","","","IEEE","IEEE Conferences"
"Computer aided optimized design and simulation of synthetic test circuit for testing 800kV rating circuit breakers","J. G. Jamnani; S. A. Kanitkar","Department of Electrical Engineering, Institute of Technology, Nirma University, Ahmedabad, India; Department of Electrical Engineering, Faculty of Technology and Engineering, M.S. University, Baroda, India","TENCON 2009 - 2009 IEEE Region 10 Conference","","2009","","","1","6","Development in electrical power transmission system requires the use of circuit breakers with increasing breaking capacity. At present circuit breakers are to be installed on 245 kV to 1100 kV power system with short circuit ratings up to 120 kA. To test high voltage CBs, direct testing using the power system or short circuit alternators are not feasible. The testing of high voltage CBs of larger capacity requires very large capacity of testing station. To increase testing plant power is neither an economical nor a very practical solution. Synthetic testing is an alternative equivalent method for testing of high voltage circuit breakers and is accepted by the standards. This paper presents a TRV rating concepts, IEC standards TRV envelopes and types of synthetic test circuits and their comparison. Analysis and mathematical modeling of 4-parameters TRV synthetic test circuit is presented. In order to find the possible combinations of circuit components and to optimize the values of capacitance of capacitor banks for the desired frequencies of a particular rating of circuit-breaker, the program/software has been developed by using MATLAB and Visual Basic 6. Design and simulation of 4-parameters TRV synthetic testing circuit (Weil - Dobke type) is done by using PSIM simulator as per new TRV requirements given in IEC 62271-100 (2008). The circuit is designed and simulated for both terminal faults as well as short line faults test duty for 800 kV rating circuit-breakers.","2159-3442;2159-3450","978-1-4244-4546-2978-1-4244-4547","10.1109/TENCON.2009.5396232","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5396232","A.C. high voltage circuit breakers;direct testing;synthetic tests;TRV circuits;Terminal and Short-line faults","Circuit testing;Design optimization;Computational modeling;Circuit simulation;Computer simulation;Circuit breakers;Voltage;Power system simulation;System testing;IEC standards","CAD;circuit breakers;fault diagnosis;IEC standards;power engineering computing;power transmission;testing","computer aided optimized design;circuit breakers;electrical power transmission system;short circuit ratings;high voltage CB;power system alternator;short circuit alternator;TRV rating concepts;IEC standards;MATLAB;Visual Basic 6;4-parameters TRV synthetic testing circuit;PSIM simulator;IEC 62271-100 (2008);terminal faults;short line faults;voltage 245 kV to 1100 kV","","","13","","","","","","IEEE","IEEE Conferences"
"PISRAT: Proportional Intensity-Based Software Reliability Assessment Tool","K. Shibata; K. Rinsaka; T. Dohi","NA; NA; NA","13th Pacific Rim International Symposium on Dependable Computing (PRDC 2007)","","2007","","","43","52","In this paper we develop a software reliability assessment tool, called PISRAT: Proportional intensity-based software reliability assessment tool, by using several testing metrics data as well as software fault data observed in the testing phase. The fundamental idea is to use the proportional intensity-based software reliability models proposed by the same authors. PISRAT is written in Java language with 54 classes and 8.0 KLOC, where JDK1.5.0_9 and JFreeChart are used as the development kit and the chart library, respectively. This tool can support (i) the parameter estimation of software reliability models via the method of maximum likelihood, (ii) the goodness-of-fit test under several optimization criteria, (iii) the assessment of quantitative software reliability and prediction performance. To our best knowledge, PISRAT is the first freeware for dynamic software reliability modeling and measurement with time- dependent testing metrics.","","0-7695-3054","10.1109/PRDC.2007.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4459637","","Software reliability;Software testing;Software tools;Java;Software libraries;Parameter estimation;Predictive models;Maximum likelihood estimation;Optimization methods;Software measurement","Java;maximum likelihood estimation;program testing;public domain software;software metrics;software reliability;software tools","proportional intensity-based software reliability assessment tool;software fault data testing metrics;Java language;Java classes;KLOC;JDK1.5.0_9;JFreeChart;development kit;chart library;parameter estimation;software reliability models;maximum likelihood;quantitative software reliability;freeware;dynamic software reliability modeling;dynamic software reliability measurement;time-dependent testing metrics","","5","40","","","","","","IEEE","IEEE Conferences"
"High-level optimization of built-in self test for analog to digital converters","D. De Venuto; L. Reyneri","Politecnico di Bari, Italy; NA","MELECON 2006 - 2006 IEEE Mediterranean Electrotechnical Conference","","2006","","","101","104","This paper presents the analysis and optimization of a cheap polynomial fitting method for built-in analog to digital converters testing. Optimization has been carried on using a high-level mixed-signal cosimulation and codesign tool called CodeSimulink. Measurements have validated the high-level model, which therefore proved to be reliable. The optimization carried on by using the proposed approach, allowed to reach the same accuracy (ap90 dB) achieved by the more expensive FFT-based test strategy. The high-level model has then been automatically converted into a VHDL file, which can then be compiled to either FPGA or built-in as an ASIC into the converter itself","2158-8473;2158-8481","1-4244-0087","10.1109/MELCON.2006.1653046","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1653046","High-level synthesis;HW/SW codesign;high-resolution ADC testing;FPGA based test method","Automatic testing;Analog-digital conversion;Polynomials;Field programmable gate arrays;Pulse width modulation;Application specific integrated circuits;Test equipment;Built-in self-test;Signal generators;Harmonic distortion","analogue-digital conversion;built-in self test;circuit testing;curve fitting;hardware description languages;hardware-software codesign;polynomials","high-level optimization;built-in self test;analog to digital converters;polynomial fitting method;high-level mixed-signal cosimulation;codesign tool;CodeSimulink;FFT;VHDL file;FPGA;ASIC","","","8","","","","","","IEEE","IEEE Conferences"
"Choosing the Right Prioritisation Method","S. Hatton","NA","19th Australian Conference on Software Engineering (aswec 2008)","","2008","","","517","526","There are many methods available for prioritising software requirements. Choosing the most suitable one can often be quite difficult. A number of factors need to be considered such as the project development methodology being used, the amount of time available, the amount of information known about requirements, the stage of the project and the degree of information about priority required. This paper examines the type of information available at different stages in a project and matches it to the properties of prioritisation methods. It then recommends the usage of specific prioritisation methods at certain stages of a project.","1530-0803;2377-5408","978-0-7695-3100","10.1109/ASWEC.2008.4483241","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4483241","Requirements Prioritisation;Software Engineering;Analytical Hierarchy Process;Hundred Dollar Method;Simple Ranking;MoSCoW;Efficient Prioritisation","Australia;Software engineering;Iterative methods;Dynamic programming;Computer science;Road transportation;Scheduling;Decision making;Computer industry;System testing","software engineering;systems analysis","software requirements;prioritisation methods;software development process","","8","30","","","","","","IEEE","IEEE Conferences"
"An evaluation of Differential Evolution in software test data generation","R. Landa Becerra; R. Sagarna; X. Yao","The Centre of Excellence for Research in Computational Intelligence and Applications (CERCIA), School of Computer Science, University of Birmingham, Edgbaston, B15 2TT, UK; The Centre of Excellence for Research in Computational Intelligence and Applications (CERCIA), School of Computer Science, University of Birmingham, Edgbaston, B15 2TT, UK; The Centre of Excellence for Research in Computational Intelligence and Applications (CERCIA), School of Computer Science, University of Birmingham, Edgbaston, B15 2TT, UK","2009 IEEE Congress on Evolutionary Computation","","2009","","","2850","2857","One of the main tasks software testing involves is the generation of the test inputs to be used during the test. Due to its expensive cost, the automation of this task has become one of the key issues in the area. Recently, this generation has been explicitly formulated as the resolution of a set of constrained optimisation problems. Differential Evolution (DE) is a population based evolutionary algorithm which has been successfully applied in a number of domains, including constrained optimisation. We present a test data generator employing DE to solve each of the constrained optimisation problems, and empirically evaluate its performance for several DE models. With the aim of comparing this technique with other approaches, we extend the experiments to the Breeder Genetic Algorithm and face it to DE, and compare different test data generators in the literature with the DE approach. The results present DE as a promising solution technique for this real-world problem.","1089-778X;1941-0026","978-1-4244-2958-5978-1-4244-2959","10.1109/CEC.2009.4983300","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4983300","","Software testing;Constraint optimization;Evolutionary computation;Application software;Genetic algorithms;Benchmark testing;Instruments;Costs;Automation;Automatic testing","automatic test pattern generation;constraint handling;evolutionary computation;optimisation;program testing","software test data generation;differential evolution;software testing;constrained optimisation problem;population-based evolutionary algorithm;constraint-handling formulation","","1","31","","","","","","IEEE","IEEE Conferences"
"Test case prioritization based on data reuse an experimental study","L. Lima; J. Iyoda; A. Sampaio; E. Aranha","Centro de Informática, Universidade Federal de Pernambuco, Recife-PE, Brazil; Centro de Informática, Universidade Federal de Pernambuco, Recife-PE, Brazil; Centro de Informática, Universidade Federal de Pernambuco, Recife-PE, Brazil; Escola de Ciência e Tecnologia, Universidade Federal do Rio Grande do Norte, Natal-RN, Brazil","2009 3rd International Symposium on Empirical Software Engineering and Measurement","","2009","","","279","290","The order in which tests are executed can significantly impact the total test execution time. In this paper, we evaluate two test prioritization techniques (manual and automatic) in the context of mobile phone testing. The manual technique produces test sequences created by test experts, while the automatic one generates sequences mechanically based on the permutation of the tests. Both techniques take into account a data reuse: the more the data is reused among tests, the faster the sequence is executed. In order to evaluate the benefits of these two techniques, we carried out an experiment with 8 testers and 2 test suites arranged in a 2times2 Latin square design replicated four times. The automatic technique reduced approximately 25% of the data generation time and 13.5% of the execution time. The automatic technique is clearly better than the manual one with respect to the generation of sequences. Our experiment showed that the automatic technique also generates sequences whose execution is faster than those created manually by test experts.","1949-3770;1949-3789","978-1-4244-4842-5978-1-4244-4841","10.1109/ESEM.2009.5315980","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5315980","","Automatic testing;Software testing;Manuals;Software engineering;Software measurement;Time measurement;Mobile handsets;Computer industry;Automation;Costs","mobile computing;program testing;software reusability","test case prioritization;data reuse;experimental study;manual testing;automatic testing;mobile phone testing","","4","20","","","","","","IEEE","IEEE Conferences"
"On Testing and Evaluating Service-Oriented Software","W. Tsai; X. Zhou; Y. Chen; X. Bai","Arizona State University; VMware; Arizona State University; Tsinghua University, China","Computer","","2008","41","8","40","46","As service-oriented architecture matures and more Web services become available, developers must test an ever-increasing volume of services. A framework that defines and evaluates test-case potency based on coverage relationships can reduce testing effort while maintaining testing's effectiveness.","0018-9162;1558-0814","","10.1109/MC.2008.304","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4597136","test case selection;test case design;test case ranking;Web services;verification.","Software testing;Service oriented architecture;Web services;Collaborative software;Security;Protocols;Markup languages;Regulators;Publishing;Application software","program testing;software maintenance;Web services","service-oriented software testing;service-oriented software evaluation;service-oriented architecture;Web services;test-case potency","","56","8","","","","","","IEEE","IEEE Journals & Magazines"
"Two-Dimensional Software Reliability Models and Their Application","T. Ishii; T. Dohi","Hiroshima University; Hiroshima University","2006 12th Pacific Rim International Symposium on Dependable Computing (PRDC'06)","","2006","","","3","10","In general, the software-testing time may be measured by two kinds of time scales: calendar time and test-execution time. In this paper, we develop two-dimensional software reliability models with two-time measures and incorporate both of them to assess the software reliability with higher accuracy. Since the resulting software reliability models are based on the familiar non-homogeneous Poisson processes with two-time scales, which are the natural extensions of one-dimensional models, it is possible to treat both the time data simultaneously and effectively. We investigate the dependence of test-execution time as a testing effort on the software reliability assessment, and validate quantitatively the software reliability models with two-time scales. We also consider an optimization problem when to stop the software testing in terms of two-time measurements","","0-7695-2724","10.1109/PRDC.2006.64","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4041882","Software reliability;Test-execution time;Testing;effort;Two-dimensional models;Non-homogeneous;Poisson processes;Bivariate fault-detection time distributions","Software reliability;Application software;Software testing;Software measurement;Calendars;Time measurement;Software debugging;Reliability engineering;Stochastic processes;Parameter estimation","program testing;software fault tolerance","two-dimensional software reliability models;software-testing;calendar time;test-execution time;nonhomogeneous Poisson process;one-dimensional models;optimization problem","","9","26","","","","","","IEEE","IEEE Conferences"
"Testing Optimization for Mission-Critical, Complex, Distributed Systems","M. G. Stochel; R. Sztando","NA; NA","2008 32nd Annual IEEE International Computer Software and Applications Conference","","2008","","","847","852","The goal of the research was to optimize the regression testing of the software application to address the identified problem of a missing, unclear or even contradictory requirement. The approach was mainly aimed at regression test prioritization and selection of regression test cases per test campaigns. A combination of subjective data based on expert knowledge and objective historical data were the inputs to the model where the output was determining the quality of test selection. Proposed model is aimed at finding newly introduced defects, and it could be extremely useful when the system is in the state of cleaning up or ordering requirements.","0730-3157;0730-3157","978-0-7695-3262","10.1109/COMPSAC.2008.96","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4591679","mission critical;testing optimization;six sigma;DMAIC;public safety","Testing;Software;Data models;Knowledge engineering;Stability analysis;Organizations;Optimization","distributed processing;program testing;regression analysis","mission-critical systems;complex systems;distributed systems;regression testing optimization;software application;regression test prioritization;regression test case selection","","3","9","","","","","","IEEE","IEEE Conferences"
"Integrated software environment development","L. S. Globa; T. N. Kot","Institute of Telcommunication Systems at the National Technical University of Ukraine ¿Kiev Politechnical Institute¿, 03056, Ukraine; Institute of Telcommunication Systems at the National Technical University of Ukraine ¿Kiev Politechnical Institute¿, 03056, Ukraine","2008 18th International Crimean Conference - Microwave & Telecommunication Technology","","2008","","","384","385","The article deals with the compare of standard, new suggested information system (IS) development logical stages and realizing IS like system based on multithread processing. Developed systems, functioning in the telecommunication environment, which provides access to different services working separately and often being physically distributed, canpsilat be designed as single thread application. Standard and suggested stages are integrated; new CASE-toolkit is being developed for high-quality IS development in the shortest time. The main features of such toolkit is support of prototyping, early testing, analysis, easy reengineering, computing processes in distributed IS optimization and multithread computing in global environment including wireless channels. The work includes some new and fundamental approaches and suggests specific concepts to IS development.","","978-966-335-166-7978-966-335-169","10.1109/CRMICO.2008.4676424","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4676424","","Distributed computing;Telecommunication computing;Application software;Software tools;Information systems;Yarn;Prototypes;Testing;Distributed information systems;Design optimization","computer aided software engineering;information systems;integrated software;multi-threading;program testing;software prototyping;systems re-engineering","integrated software environment development;information system;multithread processing;CASE-toolkit;IS development;software prototyping;software testing;software reengineering;distributed IS optimization;multithread computing","","","6","","","","","","IEEE","IEEE Conferences"
"Partheno-Genetic Algorithm for Test Instruction Generation","Z. Ming; X. Jiang; J. Bai","NA; NA; NA","2008 The 9th International Conference for Young Computer Scientists","","2008","","","1187","1192","Test case generation is the classic method in finding software defects, and test instruction generation is one of its typical applications in embedded chipset systems.In this paper, the optimized partheno-genetic algorithm(PGA) is proposed after a 0-1 integer programming model is set up for instruction-set test cases generation problem. Based on simulation, the proposed model and algorithm achieve a convincing computational performance, in most cases 50%~70%, instruction-set test cases with better ability of error detecting obtained using this algorithm could save the execution time up to 3 seconds. Besides, it also avoids the problem of using complicated crossover and mutation operations that traditional genetic algorithm shave.","","978-0-7695-3398","10.1109/ICYCS.2008.453","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4709142","Partheno-Genetic algorithm;test case;test instruction generation;multidimensional knapsack problem","Software testing;System testing;Embedded software;Application software;Optimization methods;Software algorithms;Linear programming;Computational modeling;Computer aided instruction;Genetic mutations","automatic test pattern generation;embedded systems;genetic algorithms;integer programming;program testing;software reliability","test instruction generation;software defect finding;optimized partheno-genetic algorithm;embedded chipset system;integer programming model;instruction-set test case generation;error detection","","","18","","","","","","IEEE","IEEE Conferences"
"A Leveled Examination of Test-Driven Development Acceptance","D. S. Janzen; H. Saiedian","Cal Poly at San Luis Obispo; University of Kansas, USA","29th International Conference on Software Engineering (ICSE'07)","","2007","","","719","722","Test-driven development (TDD) has garnered considerable attention in professional settings and has made some inroads into software engineering and computer science education. A series of leveled experiments were conducted with students in beginning undergraduate programming courses through upper-level undergraduate, graduate, and professional training courses. This paper reports that mature programmers who try TDD are more likely to choose TDD over a similar test-last approach. Additionally this research reveals differences in programmer acceptance of TDD between beginning programmers who were reluctant to adopt TDD and more mature programmers who were more willing to adopt TDD. Attention is given to confounding factors, and future studies aimed at resolving these factors are identified. Finally proposals are made to improve early programmer acceptance of TDD.","0270-5257;1558-1225","0-7695-2828","10.1109/ICSE.2007.8","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4222633","","Programming profession;Automatic testing;Software engineering;Software testing;Optimized production technology;Software quality;Automatic programming;Productivity;Computer languages;Industrial training","computer science education;program testing;software engineering","leveled examination;test-driven development acceptance;software engineering;computer science education;undergraduate programming courses","","8","12","","","","","","IEEE","IEEE Conferences"
"Test Yield Improvement of Class II Bluetooth Devices Through Power Output Optimization Via Circuit Element Tuning","J. C. Rabago; A. N. Quillosa","NXP Semiconductors, Philips Ave. Light Industry & Science Park I (LISP), Cabuyao, Laguna, Philippines 4025. Email: joyce.c.rabago@nxp.com; NXP Semiconductors, Philips Ave. Light Industry & Science Park I (LISP), Cabuyao, Laguna, Philippines 4025. Email: anton.n.quillosa@nxp.com","2006 International Conference on Electronic Materials and Packaging","","2006","","","1","7","To guarantee that products meet acceptance test specifications, final test is performed at the end of the assembly process. This is done to screen-out units that are possible defects before they are being delivered to the customer. Bluetooth devices also undergo final test using parameters that are defined by the Bluetooth specifications. One of the parameters in the final test is P_2480 (output power at channel 78 with a frequency of 2480 MHz), which ensures that the output power level of the device is within the limits that are defined by the customer. This study was conducted for the reason that power rejects most specifically on channel 78, are becoming primary in the major factors that contribute in low-test yield. Improving the product's output power performance through evaluations and tuning of elements that make up the circuit, will reduce if not eliminate low power rejects on channel 78 (2480 MHz), and therefore increase the final test yield.","","978-1-4244-0833-7978-1-4244-0834","10.1109/EMAP.2006.4430653","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4430653","","Circuit testing;Bluetooth;Circuit optimization;Mobile handsets;Power generation;Radio control;Radio frequency;Assembly;Frequency shift keying;Interference","assembling;automatic test equipment;Bluetooth;circuit optimisation;circuit testing;circuit tuning;system-in-package","class II Bluetooth devices;power output optimization;circuit element tuning;assembly process;test yield improvement;acceptance test;power measurement;Advance Design System simulation software;automatic test equipment;system-in-a-package;frequency 2480 MHz","","","3","","","","","","IEEE","IEEE Conferences"
"VAST: Virtualization-Assisted Concurrent Autonomous Self-Test","H. Inoue; Y. Li; S. Mitra","Departments of Electrical Engineering and Computer Science, Stanford University; System IP Core Research Laboratories, NEC Corporation; Departments of Electrical Engineering and Computer Science, Stanford University; Departments of Electrical Engineering and Computer Science, Stanford University","2008 IEEE International Test Conference","","2008","","","1","10","Virtualization-assisted concurrent, autonomous self-test, or VAST, enables a multi-/many-core system to test itself, concurrently during normal operation, without any user-visible downtime. Such on-line self-test is required for large-scale robust systems with built-in support for circuit failure prediction, failure detection, diagnosis, and self-healing. The main idea behind VAST is hardware and software co-design of on-line self-test features in a multi-/many-core system through integration of: 1. multi-/many-core architecture, 2. virtualization software, and, 3. special self-test techniques such as BIST (built-in self-test) or CASP (concurrent autonomous chip self-test using stored patterns). As a result, optimized trade-offs in system design complexity, system performance and power impact, and test thoroughness are possible. Experimental results from an actual multi-core system demonstrate that: 1. VAST is practical and effective; and, 2. Special VAST-supported self-test policies enable extremely thorough on-line self-test with very small performance impact.","1089-3539;2378-2250","978-1-4244-2402-3978-1-4244-2403","10.1109/TEST.2008.4700583","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700583","","Built-in self-test;System testing;Circuit testing;Automatic testing;Large-scale systems;Robustness;Hardware;Computer architecture;Design optimization;System performance","built-in self test;circuit testing;concurrent engineering;logic testing","VAST;virtualization-assisted concurrent autonomous self-test;online self-test;large-scale robust systems;circuit failure prediction;failure detection;self-healing;virtualization software;built-in self-test","","35","52","","","","","","IEEE","IEEE Conferences"
"Development of an Improved GUI Automation Test System Based on Event-Flow Graph","Y. Lu; D. Yan; S. Nie; C. Wang","NA; NA; NA; NA","2008 International Conference on Computer Science and Software Engineering","","2008","2","","712","715","A more highly automated graphic user interface (GUI) test model, which is based on the event-flow graph, is proposed. In the model, an automation tool is first used to carry out reverse engineering for a GUI test sample so as to obtain the event-flow graph. Then an improved ant colony optimization algorithm and a goal-directed searching approach are adopted to create GUI test sample cases. Moreover, a corresponding prototype system based on Microsoft UI automation framework is developed.","","978-0-7695-3336","10.1109/CSSE.2008.1336","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722150","","Graphical user interfaces;Automation;Automatic testing;System testing;Software testing;Artificial intelligence;Software engineering;Reverse engineering;Ant colony optimization;Prototypes","graph theory;graphical user interfaces;optimisation;program testing;search problems","GUI automation test system;event-flow graph;graphic user interface;reverse engineering;ant colony optimization algorithm;goal-directed searching","","5","11","","","","","","IEEE","IEEE Conferences"
"Prioritizing component compatibility tests via user preferences","I. Yoon; A. Sussman; A. Memon; A. Porter","Department of Computer Science, University of Maryland, College Park, MD, 20742 USA; Department of Computer Science, University of Maryland, College Park, MD, 20742 USA; Department of Computer Science, University of Maryland, College Park, MD, 20742 USA; Department of Computer Science, University of Maryland, College Park, MD, 20742 USA","2009 IEEE International Conference on Software Maintenance","","2009","","","29","38","Many software systems rely on third-party components during their build process. Because the components are constantly evolving, quality assurance demands that developers perform compatibility testing to ensure that their software systems build correctly over all deployable combinations of component versions, also called configurations. However, large software systems can have many configurations, and compatibility testing is often time and resource constrained. We present a prioritization mechanism that enhances compatibility testing by examining the ldquomost importantrdquo configurations first, while distributing the work over a cluster of computers. We evaluate our new approach on two large scientific middleware systems and examine tradeoffs between the new prioritization approach and a previously developed lowest-cost-configuration-first approach.","1063-6773","978-1-4244-4897-5978-1-4244-4828","10.1109/ICSM.2009.5306357","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306357","","System testing;Software systems;Software testing;Quality assurance;Costs;Performance evaluation;Middleware;Software libraries;Computer science;Educational institutions","middleware;object-oriented programming;program testing;software engineering","user preferences;compatibility testing prioritization;third-party components;component configurations;software systems;middleware systems;computer clusters","","","14","","","","","","IEEE","IEEE Conferences"
"A new method of test data generation for branch coverage in software testing based on EPDG and Genetic Algorithm","Ciyong Chen; Xiaofeng Xu; Yan Chen; Xiaochao Li; Donghui Guo","The Department of Physics, Xiamen University, 361005, China; The Department of Physics, Xiamen University, 361005, China; The School of Information Science and Technology, Xiamen University, 361005, China; The School of Information Science and Technology, Xiamen University, 361005, China; The Department of Physics, Xiamen University, 361005, China","2009 3rd International Conference on Anti-counterfeiting, Security, and Identification in Communication","","2009","","","307","310","A new method called EPDG-GA which utilizes the edge partitions dominator graph (EPDG) and genetic algorithm (GA) for branch coverage testing is presented in this paper. First, a set of critical branches (CBs) are obtained by analyzing the EPDG of the tested program, while covering all the CBs implies covering all the branches of the control flow graph (CFG). Then, the fitness functions are instrumented in the right position by analyzing the pre-dominator tree (PreDT), and two metrics are developed to prioritize the CBs. Coverage-Table is established to record the CBs information and keeps track of whether a branch is executed or not. GA is used to generate test data to cover CBs so as to cover all the branches. The comparison results show that this approach is more efficient than random testing approach.","2163-5048;2163-5056","978-1-4244-3883-9978-1-4244-3884","10.1109/ICASID.2009.5276897","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5276897","Test Data Generation;Edge Partitions;Genetic Algorithm;Branch Coverage","Software testing;Genetic algorithms;Tree graphs;Instruments;Partitioning algorithms;Costs;Automatic testing;Physics;Information science;Flow graphs","flow graphs;genetic algorithms;graph theory;program testing","test data generation method;branch coverage software testing;edge partitions dominator graph;genetic algorithm;critical branch;control flow graph;fitness function;pre-dominator tree","","1","11","","","","","","IEEE","IEEE Conferences"
"Enabling run-time memory data transfer optimizations at the system level with automated extraction of embedded software metadata information","A. Bartzas; M. Peon-Quiros; S. Mamagkakis; F. Catthoor; D. Soudris; J. M. Mendias","VLSI Design & Testing Center, Democritus Univ. Thrace, 67100, Xanthi, Greece; DACYA, Univ. Complutense de Madrid, 28040, Spain; IMEC vzw., Kepeldreef 75, 3001, Leuven, Belgium; IMEC vzw., Kepeldreef 75, 3001, Leuven, Belgium; VLSI Design & Testing Center, Democritus Univ. Thrace, 67100, Xanthi, Greece; DACYA, Univ. Complutense de Madrid, 28040, Spain","2008 Asia and South Pacific Design Automation Conference","","2008","","","434","439","The information about the run-time behavior of software applications is crucial for enabling system level optimizations for embedded systems. This embedded Software Metadata information is especially important today, because several complex multi-threaded applications are mapped on the memory of a single embedded system. Each thread is triggered at run-time by different input events that can not be predicted at design-time. New methods and tools are needed to automatically profile and analyze the dynamic data access behavior of simultaneously executing threads in order to enable memory data transfer optimizations. In this paper, we propose such a method and tool which extract the necessary Software Metadata information to enable these data transfer optimizations at the system level. We assess the effectiveness of our approach with the results for five real-life software applications using seven real-life run-time input traces.","2153-6961;2153-697X","978-1-4244-1921-0978-1-4244-1922","10.1109/ASPDAC.2008.4483990","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4483990","","Runtime;Data mining;Embedded software;Yarn;Embedded system;Application software;Software tools;Design optimization;Data analysis;Optimization methods","electronic data interchange;embedded systems;meta data;optimisation;software agents","run-time memory;data transfer;optimizations;automated extraction;embedded software;complex multithread;single embedded system;dynamic data access;software metadata information;real-life software","","2","21","","","","","","IEEE","IEEE Conferences"
"Optimum Accelerated Life Test Plans for Log-Location-Scale Distributions with Multiple Objectives","H. Xu; H. Fei","NA; NA","2009 WRI World Congress on Software Engineering","","2009","2","","402","406","Most of the previous work on planning accelerated life tests (ALTs) is focused on a sole estimating objective, such as some specified 100p-th quantitle lifetime, the reliability of the product over some specified period of time, and accelerating factor. However, it is impossible to estimate only a single objective parameter after conducting such costly tests. In this paper, we consider optimum constant stress ALT plans with multiple estimating objectives. We suggest two kinds of design criteria: one is minimizing the absolute difference between the variance of each objective and its minimum variance, and the other is minimizing the relative difference. A Weibull model is used to compare the proposed plans with other test plans in the literature for various combinations of planned value of the model parameters. The results show that the proposed plans provide acceptable precision of estimation, and the precision is much higher than the precision of estimation based on D-optimal plans.","","978-0-7695-3570","10.1109/WCSE.2009.346","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5319640","multiple objective optimization;testing design;constant stress;accelerated life testing","Life estimation;Life testing;Stress;Software testing;Mathematics;Lifetime estimation;Costs;Reliability engineering;Acceleration;Software engineering","life testing;statistical testing;Weibull distribution","optimum accelerated life test plans;log-location-scale distributions;design criteria;Weibull model;D-optimal plans","","3","16","","","","","","IEEE","IEEE Conferences"
"Research on the Resource Management in ATS","Z. Jiajing; M. Chen; L. Jinning","Department of Missile Engineering, Ordnance Engineering College, ShiJiazhuang 050003 China; Department of Missile Engineering, Ordnance Engineering College, ShiJiazhuang 050003 China; Department of Missile Engineering, Ordnance Engineering College, ShiJiazhuang 050003 China","2007 8th International Conference on Electronic Measurement and Instruments","","2007","","","1-939","1-942","The test task of ATS generally needs various test resources for its completion. One main task of TPS runtime system in the ATS software is to find out the matching test resources for every specific test task and the reasonable coincidence relations between Test/ Excitation channels. The ATS software development method based on signal-oriented is an approach to resolve the problem. The interchangeability of instruments, the portability of software and the interoperability of the systems are the goals pursued in the design of nowadays ATS software. In this ATS software background, how to manage effectively the test resources of the system, especially how to satisfy the demand of multitask in parallel test system for the test resources optimizing allocation is a key point for the design of ATS software. Therefore, the paper puts forward a dynamic test resource management method that is based on the resource lifecycle manager. At the same time according to the model of ATS test resource capability that is signal-oriented, the method of resource matching based on the extraction of the keywords is put forward. As for the demand of multitask in parallel test system for the test resources, the resource allocation method based on the generalized resource load is brought forward. The resource management methods brought forward not only enable the rapid matching of the test resources to be achieved effectively but also can realize the portability of test software and the interchangeability of instruments.","","978-1-4244-1135-1978-1-4244-1136","10.1109/ICEMI.2007.4350609","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4350609","Auto test system(ATS);resource management;resource capability;resource matching;parallel test","Resource management;Instruments;Software testing;System testing;Life testing;Electronic equipment testing;Signal resolution;Programming;Switches;Missiles","automatic test software;computerised instrumentation;resource allocation;software engineering","resource management;auto test system;software development method;software portability;systems interoperability;parallel test system;resources optimizing allocation;resource lifecycle manager;resource allocation method;instrument interchangeability","","","7","","","","","","IEEE","IEEE Conferences"
"Study on Test Databank Construction And Algorithm of Test Paper Generation System","X. Sun","NA","2009 Second International Symposium on Electronic Commerce and Security","","2009","2","","297","302","With the help of examination theory and education mathematics, the requirements of the test paper are analyzed in details. The mathematical model of intelligent test paper generation system is set up. The paper is a tentative probe into the relevant problems of building such a databank and in particular into the policy and algorithm of test paper composition. The method of test paper auto-generation based on general discrete particle swarm optimization is proposed.","","978-0-7695-3643","10.1109/ISECS.2009.232","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5209651","Test databank;Intelligent test paper buildup;General particle swarm optimization model;Discrete particle swarm optimization algorithm;Gd-PSO","System testing;Particle swarm optimization;Electronic equipment testing;Automatic testing;Software testing;Education;Performance evaluation;Paper technology;Algorithm design and analysis;Probes","computational complexity;education;particle swarm optimisation","test databank construction;intelligent test paper generation system;general discrete particle swarm optimization","","1","16","","","","","","IEEE","IEEE Conferences"
"Study on Algorithm of Intelligent Test Paper","X. Sun","NA","2008 International Conference on MultiMedia and Information Technology","","2008","","","86","89","The requirements of the test paper are analyzed in details. The mathematical model of intelligent test paper generation system is set up. The method of test paper auto-generation based on general discrete particle swarm optimization is proposed.","","978-0-7695-3556","10.1109/MMIT.2008.128","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5089065","Intelligent test paper buildup;General particle swarm optimization model;Discrete particle swarm optimization algorithm;Gd-PSO","System testing;Particle swarm optimization;Performance evaluation;Automatic testing;Pareto optimization;Mathematical model;Application software;Probes;Genetic algorithms;Information technology","computer aided instruction;particle swarm optimisation","mathematical model;intelligent test paper generation system;test paper auto-generation;general discrete particle swarm optimization","","","13","","","","","","IEEE","IEEE Conferences"
"Using Statistical Models to Predict Software Regressions","A. Tarvo","NA","2008 19th International Symposium on Software Reliability Engineering (ISSRE)","","2008","","","259","264","Incorrect changes made to the stable parts of a software system can cause failures - software regressions. Early detection of faulty code changes can be beneficial for the quality of a software system when these errors can be fixed before the system is released. In this paper, a statistical model for predicting software regressions is proposed. The model predicts risk of regression for a code change by using software metrics: type and size of the change, number of affected components, dependency metrics, developerpsilas experience and code metrics of the affected components. Prediction results could be used to prioritize testing of changes: the higher is the risk of regression for the change, the more thorough testing it should receive.","1071-9458;2332-6549","978-0-7695-3405","10.1109/ISSRE.2008.21","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700331","software metrics;software regression;testing;statistical model","Predictive models;Software systems;Software metrics;Computer bugs;Manufacturing;Software testing;Software reliability;Reliability engineering;Fault detection;Computer industry","program debugging;software metrics;software quality;statistical analysis","statistical models;software regressions;software system;faulty code;software quality;software metrics;dependency metrics","","2","12","","","","","","IEEE","IEEE Conferences"
"Rank-based quality measurement of software systems in standardized source code","M. R. Masud; M. A. Khaer; M. M. A. Hashem","Department of Computer Science and Engineering, Khulna University of Engineering and Technology (KUET), Bangladesh; Department of Computer Science and Engineering, Khulna University of Engineering and Technology (KUET), Bangladesh; Department of Computer Science and Engineering, Khulna University of Engineering and Technology (KUET), Bangladesh","2007 10th international conference on computer and information technology","","2007","","","1","6","We propose a new and straight forward approach to measure design quality of object-oriented software systems. We use well known object oriented design quality metrics and find correlation among them to formulate a quality rank that is an indicator to the overall quality of any object oriented software. We tested our proposed formula on several open source software systems of different levels of design quality and validate by comparing the test result with expected levels of design quality.","","978-1-4244-1550-2978-1-4244-1551","10.1109/ICCITECHN.2007.4579434","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4579434","Object Oriented;quality rank;design metrics;quality measurement","","object-oriented programming;public domain software;software metrics;software quality","rank-based quality measurement;software systems;standardized source code;object-oriented software system;object oriented design quality metrics;open source software systems","","","13","","","","","","IEEE","IEEE Conferences"
"Scroll Plate Optimization Based on Improved Genetic-Particle Swarm Optimization Algorithm","Bin Peng; Zhenquan Liu; Hongsheng Zhang; Li Zhang","College of Mechano-Electronic Engineering, Lanzhou Univ. of Tech., Lanzhou 730050. E-mail: Pengb2000@163.com; NA; NA; NA","2006 6th World Congress on Intelligent Control and Automation","","2006","1","","3681","3685","The part optimization is very important for scroll compressor design. According to existing problems of current optimization algorithm and actual optimization problems, the improved genetic-particle swarm optimization algorithm (IGA-PSO) is proposed for scroll plate optimization. The optimization method integrates crossover of genetic algorithm (GA) and evolutionary mechanism of particle swarm optimization (PSO), the main structure parameters are been as control variable, the optimization mathematics model is developed, making use of crossover of GA and evolutionary mechanism of PSO, IGA-PSO realizes the purpose of minimizing the value of objective function. IGA-PSO is applied to scroll plate optimization on computer, it is shown that the improved approach converges to better solution much faster than the earlier reported approaches through compared with other methods and tested of prototype performance. All the results supply theory and technology support for wide application of PSO in engineering","","1-4244-0332","10.1109/WCICA.2006.1713057","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1713057","IGA-PSO;scroll compressor;scroll plate;optimization","Particle swarm optimization;Optimization methods;Design optimization;Genetic algorithms;Mathematics;Mathematical model;Testing;Prototypes;Application software;Design engineering","CAD;compressors;design engineering;genetic algorithms;mechanical engineering computing;particle swarm optimisation;plates (structures)","scroll plate optimization;improved genetic-particle swarm optimization algorithm;scroll compressor design;genetic algorithm;evolutionary mechanism;optimization mathematics model","","2","11","","","","","","IEEE","IEEE Conferences"
"A Family of Software Reliability Growth Models","H. A. Stieber","University of Applied Sciences Nuremberg, Germany","31st Annual International Computer Software and Applications Conference (COMPSAC 2007)","","2007","2","","217","224","Software reliability growth models (SRGM's for short) are used to control the software testing process and to make it more effective. This paper describes a family of SRGM's which depends continuously on a parameter. All of the models of this family allow the prediction of lambda<sub>0</sub>, the failure rate at the beginning of the test and N<sub>0</sub>, the overall number of faults at the beginning of the test. The models in this family range from ""generally optimistic"" to ""generally pessimistic"". The well known and widely used basic execution time model of J. Musa belongs to this family. The new models are the result of a general theory of software reliability growth models developed by the author (to be published). Models of this family are applied to real data. It is described how this family of models can be used for feedback control of the software test process.","0730-3157","0-7695-2870","10.1109/COMPSAC.2007.20","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4291127","","Software reliability;Software testing;Predictive models;Feedback control;Reliability theory;Computer applications;Application software;Difference equations;Differential equations","program testing;software reliability","software reliability growth models;software testing process","","6","11","","","","","","IEEE","IEEE Conferences"
"Test Case Prioritization for Black Box Testing","B. Qu; C. Nie; B. Xu; X. Zhang","Southeast University, Nanjing, China; Southeast University, Nanjing, China; Southeast University, Nanjing, China; Southeast University, Nanjing, China","31st Annual International Computer Software and Applications Conference (COMPSAC 2007)","","2007","1","","465","474","Test case prioritization is an effective and practical technique that helps to increase the rate of regression fault detection when software evolves. Numerous techniques have been reported in the literature on prioritizing test cases for regression testing. However, existing prioritization techniques implicitly assume that source or binary code is available when regression testing is performed, and therefore cannot be implemented when there is no program source or binary code to be analyzed. In this paper, we presented a new technique for black box regression testing, and we performed an experiment to measure our technique. Our results show that the new technique is helpful to improve the effectiveness of fault detection when performing regression test in black box environment.","0730-3157","0-7695-2870","10.1109/COMPSAC.2007.209","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4291039","","Fault detection;Software testing;Binary codes;Performance evaluation;History;Computer science;Runtime;Performance analysis;Life testing;Sorting","fault diagnosis;program testing;regression analysis","test case prioritization;regression fault detection;black box regression testing","","19","15","","","","","","IEEE","IEEE Conferences"
"How do scientists develop and use scientific software?","J. E. Hannay; C. MacLeod; J. Singer; H. P. Langtangen; D. Pfahl; G. Wilson","Dept. of Software Engineering, Simula Research Laboratory, Dept. of Informatics, Univ. of Oslo, Norway; Dept. of Computer Science, University of Toronto, Canada; Software Engineering Group, National Research Council of Canada, Canada; Center for Biomedical Computing, Simula Research Laboratory, Dept. of Informatics, Univ. of Oslo, Norway; Dept. of Software Engineering, Simula Research Laboratory, Dept. of Informatics, Univ. of Oslo, Norway; Dept. of Computer Science, University of Toronto, Canada","2009 ICSE Workshop on Software Engineering for Computational Science and Engineering","","2009","","","1","8","New knowledge in science and engineering relies increasingly on results produced by scientific software. Therefore, knowing how scientists develop and use software in their research is critical to assessing the necessity for improving current development practices and to making decisions about the future allocation of resources. To that end, this paper presents the results of a survey conducted online in October-December 2008 which received almost 2000 responses. Our main conclusions are that (1) the knowledge required to develop and use scientific software is primarily acquired from peers and through self-study, rather than from formal education and training; (2) the number of scientists using supercomputers is small compared to the number using desktop or intermediate computers; (3) most scientists rely primarily on software with a large user base; (4) while many scientists believe that software testing is important, a smaller number believe they have sufficient understanding about testing concepts; and (5) that there is a tendency for scientists to rank standard software engineering concepts higher if they work in large software development projects and teams, but that there is no uniform trend of association between rank of importance of software engineering concepts and project/team size.","","978-1-4244-3737","10.1109/SECSE.2009.5069155","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5069155","","Software testing;Software engineering;Knowledge engineering;Resource management;Computer science education;Supercomputers;Peer to peer computing;Automatic testing;Software standards;Standards development","scientific information systems;software engineering","scientific software;supercomputers;software testing;software engineering;software development projects","","54","12","","","","","","IEEE","IEEE Conferences"
"Optimizing a Structural Constraint Solver for Efficient Software Checking","J. H. Siddiqui; D. Marinov; S. Khurshid","NA; NA; NA","2009 IEEE/ACM International Conference on Automated Software Engineering","","2009","","","615","619","Several static analysis techniques, e.g., symbolic execution or scope-bounded checking, as well as dynamic analysis techniques, e.g., specification-based testing, use constraint solvers as an enabling technology. To analyze code that manipulates structurally complex data, the underlying solver must support structural constraints. Solving such constraints can be expensive due to the large number of aliasing possibilities that the solver must consider. This paper presents a novel technique to selectively reduce the number of test cases to be generated. Our technique applies across a class of structural constraint solvers. Experimental results show that the technique enables an order of magnitude reduction in the number of test cases to be considered.","1938-4300","978-1-4244-5259-0978-0-7695-3891","10.1109/ASE.2009.52","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5431722","Software Testing;Bounded Exhaustive Testing","Constraint optimization;Automatic testing;Software engineering;Software testing;Marine technology;Data structures;XML;Constraint theory;Performance evaluation;Computer bugs","program diagnostics;program testing","structural constraint solver;software checking;static analysis technique;dynamic analysis technique;test cases number reduction;magnitude reduction","","1","12","","","","","","IEEE","IEEE Conferences"
"Multi-criteria Human Resource Allocation for Optimization Problems Using Multi-objective Particle Swarm Optimization Algorithm","Z. Jia; L. Gong","NA; NA","2008 International Conference on Computer Science and Software Engineering","","2008","1","","1187","1190","Multi-criteria human resource allocation involves deciding how to divide human resource of limited availability among multiple demands in a way that optimizes current objectives. This paper aims to solve the multi-criteria optimal allocation of human resources issues using multi-objective particle swarm optimization (MOPSO). In this paper, we tackled this problem via a multi-objective decision-making model using a multi-objective PSO. We developed the Mathematical model of human resource optimization allocation using the competency model theory, and then in order to obtain a set of Pareto solutions efficiently, we proposed the multi-objective PSO (MOPSO) approach based on the decision-making model for solving combinatorial optimization problems. According to the proposed method, we applied the MOPSO to seek feasible solutions for the problem. The effectiveness of the proposed algorithm was validated by its application to an illustrative example dealing with multi-objective resource allocation problem.","","978-0-7695-3336","10.1109/CSSE.2008.1506","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721965","Multi-criteria human resource allocation problem (mchRAP);Multi-objective optimization model;Multi-Objective Particle Swarm Optimization (MOPSO);competence model","Humans;Resource management;Particle swarm optimization;Mathematical model;Software testing;Cost function;Decision making;Pareto optimization;System testing;Medical services","decision making;operations research;particle swarm optimisation","multi-criteria human resource allocation;multi-objective particle swarm optimization algorithm;multi-criteria optimal allocation;multi-objective decision-making model;Pareto solutions;competency model theory","","1","10","","","","","","IEEE","IEEE Conferences"
"A Model for Reuse and Optimization of Embedded Software Components","M. Akerholm; J. Froberg; K. Sandstrom; I. Crnkovic","MRTC, Mälardalen University, Västerås, Sweden. mikael.akerholm@mdh.se; MRTC, Mälardalen University, Västerås, Sweden; MRTC, Mälardalen University, Västerås, Sweden; MRTC, Mälardalen University, Västerås, Sweden","2007 29th International Conference on Information Technology Interfaces","","2007","","","567","572","In software engineering for embedded systems generic reusable software components must often be discarded in favor of using resource optimized solutions. In this paper we outline a model that enables the utilization of component-based principles even for embedded systems with high optimization demands. The model supports the creation of component variants optimized for different scenarios, through the introduction of an entrance preparation step and an ending verification step into the component design process. These activities are proposed to be supported by tools working on metadata associated with components, where the metadata is possible to automatically retrieve from many development tools. This paper outlines the theoretical model that is the basis for our current realization work.","1330-1012","953-7138-09-7953-7138-10","10.1109/ITI.2007.4283834","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4283834","Optimization;Embedded;Reuse;Component-Based Software Engineering","Embedded software;Embedded system;Testing;Software engineering;Application software;Software systems;Software reusability;Design optimization;Process design;Predictive models","embedded systems;meta data;object-oriented programming;resource allocation;software reusability;software tools","embedded software component reusability;software engineering;resource optimized solutions;ending verification step;component design process;metadata;development tools","","3","22","","","","","","IEEE","IEEE Conferences"
"Spectral Debugging with Weights and Incremental Ranking","L. Naish; H. J. Lee; K. Ramamohanarao","NA; NA; NA","2009 16th Asia-Pacific Software Engineering Conference","","2009","","","168","175","Software faults can be diagnosed using program spectra. The program spectra considered here provide information about which statements are executed in each one of a set of test cases. This information is used to compute a value for each statement which indicates how likely it is to be buggy, and the statements are ranked according to these values. We present two improvements to this method. First, we associate varying weights with failed test cases --- test cases which execute fewer statements are given more weight and have more influence on the ranking. This generally improves diagnosis accuracy, with little additional cost. Second, the ranking is computed incrementally. After the top-ranked statement is identified, the weights are adjusted in order to compute the rest of the ranking. This further improves accuracy. The cost is more significant, but not prohibitive.","1530-1362;1530-1362","978-0-7695-3909","10.1109/APSEC.2009.32","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5358598","software fault diagnosis;spectral debugging;weights;incremental ranking","Software testing;Costs;Fault diagnosis;Software engineering;Australia;Programming;Computer bugs;Software debugging;Instruments;Automatic testing","program debugging;software fault tolerance","spectral debugging;weights ranking;incremental ranking;software faults","","8","20","","","","","","IEEE","IEEE Conferences"
"Assemblies of Software Components","J. Jiang; Y. Xiao; H. Zhu; S. Zhou","NA; NA; NA; NA","2009 WRI World Congress on Software Engineering","","2009","3","","311","315","Component-based software engineering (CBSE) is founded on a paradigm: complex systems can be obtained by assembling components, thereby main work focuses on the methods and techniques of component modeling, adapting and assembling. However, these attempts have not established a more systematic approach for component assembly yet. In the paper, we investigate operations of changing components, assembly relationships and assembly equivalences in order to construct a set of assembly rules, optimize solutions to the assemblies of components and further establish a systematic component-based development method.","","978-0-7695-3570","10.1109/WCSE.2009.392","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5319416","Component;assembly;equivalence","Assembly systems;Software engineering;Qualifications;Testing;Software systems;Computational modeling;Optimization methods;Software maintenance;Software reusability;Application software","object-oriented programming;software maintenance;software reusability","component-based software engineering;CBSE;complex system;software component;component modeling;component assembly;assembly rule;software maintenance;software reuse","","","6","","","","","","IEEE","IEEE Conferences"
"Java Based Test Case Generation and Optimization Using Evolutionary Testing","C. S. S. Dharsana; D. N. Jennifer; A. Askarunisha; N. Ramaraj","NA; NA; NA; NA","International Conference on Computational Intelligence and Multimedia Applications (ICCIMA 2007)","","2007","1","","44","49","Testing is an iterative process of both validating functionality and even more important attempting to break the softwareTest case is used for testing which is a set of test data and test programs (test scripts) and their expected results. This paper details on the tool TCGOJ, test case generation and optimization for Java programs, that generates test cases for any Java programs and produces an optimized result of test cases. Genetic algorithm is used to create and optimize chromosomes which describe the test cases. Mutation and crossover are done for gaining maximum coverage.","","0-7695-3050","10.1109/ICCIMA.2007.445","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4426551","","Java;Software testing;Biological cells;Automatic testing;Genetic algorithms;Genetic mutations;Educational institutions;Character generation;Computational intelligence;Strontium","genetic algorithms;iterative methods;Java;program testing","Java;test case generation;test case optimization;evolutionary testing;iterative process;softwareTest case;program testing;test scripts;TCGOJ;genetic algorithm","","7","5","","","","","","IEEE","IEEE Conferences"
"Resource allocation model for software module testing","R. Rajan; R. B. Misra","NA; NA","RAMS '06. Annual Reliability and Maintainability Symposium, 2006.","","2006","","","92","97","The goal of this paper is two-fold: one is to introduce failure rate model in the presence of fault correction activity. The other objective is to formulate the optimization problems which software managers face and to propose simple and fast solution procedures to handle them. In this article software reliability growth model which considers fault correction is used. This paper addresses the problem of optimally allocating testing resources to various modules so that the software as a whole must not exceed defined budget. Simple and fast solution methodology is proposed to help managers to make best decisions. Using these algorithms, project managers can allocate limited testing resources easily and efficiently in the presence of fault correction activity","0149-144X","1-4244-0007-41-4244-0008","10.1109/RAMS.2006.1677356","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1677356","","Resource management;Software testing;Software reliability;System testing;Life testing;Fault detection;Software algorithms;Project management;Costs;Software systems","optimisation;program testing;resource allocation;software fault tolerance","resource allocation model;software module testing;optimization problem;software reliability growth model;fault correction","","","20","","","","","","IEEE","IEEE Conferences"
"RiTMO: A Method for Runtime Testability Measurement and Optimisation","A. Gonzalez-Sanchez; E. Piel; H. Gross","NA; NA; NA","2009 Ninth International Conference on Quality Software","","2009","","","377","382","Runtime testing is emerging as the solution for the integration and assessment of highly dynamic, high availability software systems where traditional development-time integration testing is too costly, or cannot be performed. However, in many situations, an extra effort will have to be invested in implementing appropriate measures to enable runtime tests to be performed without affecting the running system or its environment. This paper introduces a method for the improvement of the runtime testability of a system, which provides an optimal implementation plan for the application of measures to avoid the runtime tests' interferences. This plan is calculated considering the trade-off between testability and implementation cost. The computation of the implementation plan is driven by an estimation of runtime testability, and based on a model of the system. Runtime testability is estimated independently of the test cases and focused exclusively on the architecture of the system at runtime.","1550-6002;2332-662X","978-1-4244-5913-1978-1-4244-5912-4978-0-7695-3828","10.1109/QSIC.2009.56","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381399","runtime testability;cig;runtime testing;optimization","Optimization methods;System testing;Software testing;Performance evaluation;Availability;Software systems;Runtime environment;Interference;Costs;Computer architecture","program testing;software metrics","RiTMO;runtime testability measurement;runtime testing;software systems;development-time integration testing;runtime testability optimisation","","4","13","","","","","","IEEE","IEEE Conferences"
"An Evolution Strategy with stochastic ranking for solving reactive power optimization","H. Geng; Q. Song; F. Jiao; Y. Sun","College of Computer & Software, Nanjing University of Information Science & Technology, China; College of Computer & Software, Nanjing University of Information Science & Technology, China; College of Computer & Software, Nanjing University of Information Science & Technology, China; College of Computer & Software, Nanjing University of Information Science & Technology, China","2009 2nd International Conference on Power Electronics and Intelligent Transportation System (PEITS)","","2009","1","","14","17","This paper presents an algorithm for solving reactive power optimization problem through the application of Evolution Strategy (ES) with stochastic ranking. In order to better improve the optimization performance and practicality, the coding method for integer data of transformer tap position is designed deliberately and the self-adaptive optimization termination condition based on variance is also presented. Under simulated conditions, the proposed method has been tested on IEEE-14 and IEEE-118 bus systems. The optimal reactive power results obtained using improved ES are compared with initial power loss. It is shown that our strategy can decrease respectively nearly 4.43% and 4.3% of initial loss.","","978-1-4244-4544","10.1109/PEITS.2009.5407050","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5407050","Reactive Power Optimization;Evolution Strategy;Stochastic Ranking","Stochastic processes;Reactive power;Power generation;Design optimization;Stochastic systems;Constraint optimization;Quadratic programming;Intelligent transportation systems;Educational institutions;Information science","encoding;optimisation;stochastic systems;transformers","evolution strategy;stochastic ranking;reactive power optimization;coding method;IEEE-14 bus systems;IEEE-118 bus systems;power loss;transformer tap position","","1","18","","","","","","IEEE","IEEE Conferences"
"Test component assignment in a performance testing environment","L. Eros; T. Csondes","Department of Telecommunications and Media Informatics, Budapest University of Technology and Economics, Hungary; Test Competence Center, Ericsson Hungary Ltd., Hungary","2008 16th International Conference on Software, Telecommunications and Computer Networks","","2008","","","399","403","In this paper we are going to introduce the problem of assigning test components to hosts of a performance (or load) testing environment, and its two novel solutions. When testing the performance of a device (system under test-SUT), the test environment simulates the latter real-life environment of the SUT. The number of hosts in the test environment is however way less than the number of hosts the SUT will have to serve in its real-life environment. Thus, real-life hosts are simulated by software entities, the so-called test components that have to be optimally assigned and then executed on the hosts of the test environment (testing hosts). Our goal is to emulate all the test components by as few testing hosts as possible, that is, to maximize the load on the testing hosts. The problem to be solved is a special case of the task assignment problem for which many solutions have been developed. Our solutions presented in this paper are, however, optimized for distributing load testing traffic. Thus the possibilities and restrictions we had to take into account are very different from those of the classical task assignment case. One of the solutions we present extends existing bin packing heuristics, while the other one solves a series of integer linear programs to make the assignments. Our simulations have shown that by applying our solutions, the average load level on testing hosts can be significantly increased.","","978-953-6114-97-9978-953-290-009","10.1109/SOFTCOM.2008.4669518","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4669518","","System testing;Software testing;Heuristic algorithms;Traffic control;Informatics;Environmental economics;Telecommunication traffic;Stress;Emulation","automatic test software;bin packing;digital simulation;integer programming;linear programming","test component assignment;performance testing environment;load testing traffic distribution;system under test;SUT;real-life host simulation;software entity;bin packing heuristic;integer linear program","","","9","","","","","","IEEE","IEEE Conferences"
"Search Based Approaches to Component Selection and Prioritization for the Next Release Problem","P. Baker; M. Harman; K. Steinhofel; A. Skaliotis","Motorola Labs, Viables Estate, UK; Motorola Labs, Viables Estate, UK; Motorola Labs, Viables Estate, UK; Motorola Labs, Viables Estate, UK","2006 22nd IEEE International Conference on Software Maintenance","","2006","","","176","185","This paper addresses the problem of determining the next set of releases in the course of software evolution. It formulates both ranking and selection of candidate software components as a series of feature subset selection problems to which search based software engineering can be applied. The approach is automated using greedy and simulated annealing algorithms and evaluated using a set of software components from the component base of a large telecommunications organization. The results are compared to those obtained by a panel of (human) experts. The results show that the two automated approaches convincingly outperform the expert judgment approach","1063-6773","0-7695-2354","10.1109/ICSM.2006.56","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021335","","Simulated annealing;Software engineering;Educational institutions;Software algorithms;Robustness;Humans;Programming;Software systems;Software testing;Planing","greedy algorithms;object-oriented programming;simulated annealing;software maintenance;software selection","software component selection;software component prioritization;next release problem;search based software engineering;software evolution;software ranking;feature subset selection problem;greedy algorithm;simulated annealing algorithm;large telecommunications organization","","38","19","","","","","","IEEE","IEEE Conferences"
"Verification and Validation of (Real Time) COTS Products using Fault Injection Techniques","R. Barbosa; N. Silva; J. Duraes; H. Madeira","Critical Software, S.A., Coimbra, Portugal; Critical Software, S.A., Coimbra, Portugal; CISUC, University of Coimbra, Portugal; CISUC, University of Coimbra, Portugal","2007 Sixth International IEEE Conference on Commercial-off-the-Shelf (COTS)-Based Software Systems (ICCBSS'07)","","2007","","","233","242","With the goal of reducing time to market and project costs, the current trend of real time business and mission critical systems is evolving from the development of custom made applications to the use of commercial off the shelf (COTS) products. Obviously, the same confidence and quality of the custom made software components is expected from the commercial applications. In most cases, such products (COTS) are not designed with stringent timing and/or safety requirements as priorities. Thus, to decrease the gap between the use of custom made components and COTS components, this paper presents a methodology for evaluating COTS products in the scope of dependable, real time systems, through the application of fault injection techniques at key points of the software engineering process. By combining the use of robustness testing (fault injection at interface level) with software fault injection (using educated fault injection operators), a COTS component can be assessed in the context of the system it will belong to, with special emphasis given to timing and safety constraints that are usually imposed by the target real time dependable environment. In the course of this work, three case studies have been performed to assess the methodology using realistic scenarios that used common COTS products. Results for one case study are presented","","0-7695-2785","10.1109/ICCBSS.2007.45","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4127318","","Real time systems;Application software;Timing;Software safety;Time to market;Costs;Business;Mission critical systems;Software quality;Product safety","program testing;program verification;software fault tolerance;software packages","software verification;software validation;commercial off the shelf;software components;software engineering;robustness testing;software fault injection","","7","6","","","","","","IEEE","IEEE Conferences"
"Search-Based Performance Testing of Applications with Composite Services","Y. Gu; Y. Ge","NA; NA","2009 International Conference on Web Information Systems and Mining","","2009","","","320","324","Software performance testing aims to discover faults related to QoS of a system, such as response time. This paper proposes a method to generate performance test cases automatically based on Genetic Algorithms for any system consisting of composite services. It considers users' experience in the performance test model. Based on the system's workflow, the usage pattern of a system is modeled as QoS-sensitivity factors based on performance requirements and then detailed modified CFG is derived. A genetic algorithm is used as an optimization approach to look automatically for a test case with maximum/minimum QoS values which has a high possibility to violate performance requirements. Preliminary experiments show the generated test data is better than random testing.","","978-0-7695-3817","10.1109/WISM.2009.73","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5369373","Performance Testing;QoS;Genetic Algorithms","System testing;Automatic testing;Genetic algorithms;Delay;Quality of service;Pattern analysis;Application software;Educational institutions;Occupational stress;Telecommunication traffic","genetic algorithms;program testing;software performance evaluation","search-based performance testing;composite services;software performance testing;genetic algorithm;performance test model;usage pattern;QoS sensitivity factors;quality of service;random testing","","6","9","","","","","","IEEE","IEEE Conferences"
"Hardware Software Partitioning using Particle Swarm Optimization Technique","M. B. Abdelhalim; A. E. Salama; S. E. -. Habib","Faculty of Engineering / Cairo University, Giza, Egypt. Email: mbakr@ieee.org; Faculty of Engineering / Cairo University, Giza, Egypt. Email: aesalama@ieee.org; Faculty of Engineering / Cairo University, Giza, Egypt. Email: seraged@ieee.org","2006 6th International Workshop on System on Chip for Real Time Applications","","2006","","","189","194","In this paper the authors investigate the application of the particle swarm optimization (PSO) technique for solving the hardware/software partitioning problem. The PSO is attractive for the hardware/software partitioning problem as it offers reasonable coverage of the design space together with O(n) main loop's execution time, where n is the number of proposed solutions that will evolve to provide the final solution. The authors carried out several tests on a hypothetical, relatively-large hardware/software partitioning problem using the PSO algorithm as well as the genetic algorithm (GA), which is another evolutionary technique. The authors found that PSO outperforms GA in the cost function and the execution time. For the case of unconstrained design problem, the authors tested several hybrid combinations of PSO and GA algorithm; including PSO then GA, GA then PSO, GA followed by GA, and finally PSO followed by PSO. We found that a PSO followed by GA algorithm gives small or no improvement at all, while a GA then PSO algorithm gives the same results as the PSO alone. The PSO algorithm followed by another PSO round gave the best result as it allows another round of domain exploration. The second PSO round assign new randomized velocities to the particles, while keeping best particle positions obtained in the first round. The paper proposes to name this successive PSO algorithm as the re-excited PSO algorithm","","1-4244-0898","10.1109/IWSOC.2006.348234","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4155287","Embedded systems;Hardware/Software codesign;Hardware/Software Partitioning;Particle Swarm Optimization Algorithm;Genetic Algorithm;Evolutionary Algorithms;re-excited PSO","Hardware;Particle swarm optimization;Partitioning algorithms;Genetic algorithms;Costs;Clustering algorithms;Application software;Embedded system;Software algorithms;Embedded software","embedded systems;genetic algorithms;hardware-software codesign;particle swarm optimisation","hardware software partitioning;particle swarm optimization;genetic algorithm;evolutionary technique;hybrid combinations;embedded systems;hardware-software co-design","","16","41","","","","","","IEEE","IEEE Conferences"
"A process framework for customising software quality models","Mbusi Sibisi; C. C. van Waveren","Research and Development Department of Altech UEC Technologies (Pty) Ltd, South Africa; Department of Engineering and Technology Management at the University of Pretoria, South Africa","AFRICON 2007","","2007","","","1","8","The quality objective of many software organisations is to deliver software products that meet and or exceed customer expectations. The key to achieving this is to capture these expectations at the beginning of the project by clearly defining all quality requirements. The characteristics particularly defined in ISO/IEC 9126-1 (2001) provide the framework for specifying quality requirements. The ISO/IEC 9126-1 quality model is intended to be applicable to any type of software product or intermediate product. Before application this model needs to be tailored to a specific software and specific need. Since these characteristics cannot be directly measured this makes it difficult to directly prioritise and choose the most relevant characteristics and sub-characteristics. Hence a process framework that will link these characteristics and sub- characteristics to user needs is required. This will in turn help customise software quality models like ISO/IEC 9126-1 (2001) and other general software quality models. A process framework for customising software quality models is proposed in the text and it is further shown how this framework was applied in a real working environment in an attempt to quantitatively validate it. The results collected in the study showed that the framework could be used reliably in customising a generic software quality model at characteristic level only. The deviations at sub-characteristic level were due to unclear questions in the generated Generic Quality Questionnaire that resulted in misunderstandings. And the metrics used to create these questions were not fully tested for validity and reliability due to time constraints. Enhancements are discussed in the study and it is further shown how reliability can also be achieved at sub-characteristic level.","2153-0025;2153-0033","978-1-4244-0986-0978-1-4244-0987","10.1109/AFRCON.2007.4401495","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4401495","Software Quality;Software Quality Models;Software Measurement;Software Metrics;Software Quality Standards","Software quality;ISO standards;IEC standards;Software measurement;Software standards;Standards publication;Design engineering;Africa;Application software;Testing","formal specification;software metrics;software process improvement;software quality","process framework;software quality model;software product;customer expectation;software quality requirement specification","","5","18","","","","","","IEEE","IEEE Conferences"
"Adaptive Probabilistic Model for Ranking Code-Based Static Analysis Alerts","S. S. Heckman","North Carolina State University, USA","29th International Conference on Software Engineering (ICSE'07 Companion)","","2007","","","89","90","Software engineers tend to repeat mistakes when developing software. Automated static analysis tools can detect some of these mistakes early in the software process. However, these tools tend to generate a significant number of false positive alerts. Due to the need for manual inspection of alerts, the high number of false positives may make an automated static analysis tool too costly to use. In this research, we propose to rank alerts generated from automated static analysis tools via an adaptive model that predicts the probability an alert is a true fault in a system. The model adapts based upon a history of the actions the software engineer has taken to either filter false positive alerts or fix true faults. We hypothesize that by providing this adaptive ranking, software engineers will be more likely to act upon highly ranked alerts until the probability that remaining alerts are true positives falls below a subjective threshold.","","0-7695-2892","10.1109/ICSECOMPANION.2007.16","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4222694","","Reliability engineering;Software tools;Inspection;Filters;Costs;Testing;Equations;Predictive models;History;Application software","probability;software engineering;software tools","adaptive probabilistic model;ranking code-based static analysis alerts;software engineers;automated static analysis tools;adaptive model","","1","4","","","","","","IEEE","IEEE Conferences"
"Generation of test data using meta heuristic approach","P. R. Srivastava; V. Ramachandran; M. Kumar; G. Talukder; V. Tiwari; P. Sharma","Computer Science and Information System Group, BITS Pilani 333031 INDIA; Computer Science and Information System Group, BITS Pilani 333031 INDIA; Computer Science and Information System Group, BITS Pilani 333031 INDIA; Computer Science and Information System Group, BITS Pilani 333031 INDIA; Computer Science and Information System Group, BITS Pilani 333031 INDIA; Computer Science and Information System Group, BITS Pilani 333031 INDIA","TENCON 2008 - 2008 IEEE Region 10 Conference","","2008","","","1","6","Software testing is of huge importance to development of any software. The prime focus is to minimize the expenses on the testing. In software testing the major problem is generation of test data. Several metaheuristic approaches in this field have become very popular. The aim is to generate the optimum set of test data, which would still not compromise on exhaustive testing of software. Our objective is to generate such efficient test data using genetic algorithm and ant colony optimization for a given software. We have also compared the two approaches of software testing to determine which of these are effective towards generation of test data and constraints if any.","2159-3442;2159-3450","978-1-4244-2408-5978-1-4244-2409","10.1109/TENCON.2008.4766707","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4766707","Software Testing;Genetic Algorithm (GA);Ant Colony Optimization (ACO);Fitness Function","Software testing;Genetic algorithms;Ant colony optimization;Genetic mutations;Computer science;System testing;Information systems;Wheels;Search problems;Chemicals","genetic algorithms;program testing","test data generation;meta heuristic approach;software testing;software development;exhaustive testing;genetic algorithm;ant colony optimization","","12","9","","","","","","IEEE","IEEE Conferences"
"Research on Technology of Mini-terms Optimization for Logic Function","F. Li; J. Qiu; J. Chen; X. Gu; D. Ji","NA; NA; NA; NA; NA","2009 WRI World Congress on Software Engineering","","2009","2","","451","455","A logic optimization algorithm of logic function based on mini-terms is presented. It can find the implement covering by computing on-sets covering and generate the implicants which can cover the on-sets without computing prime implicants. It can reduce the spending memory. We designed and realized a software system on Boolean function logic optimization with max-input variables 128, max-output variables 256 and max products 20000. It overpasses the test of benchmark. By analyzing the tested data randomly, it shows that it is much more effective for multi-outputs logic functions on a large scale than the other common algorithms. But the efficiency of logic optimization is not evident for the circuit with single output or input variables which present with small probability.","","978-0-7695-3570","10.1109/WCSE.2009.427","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5319605","Mini-terms;Logic Function;Optimization;implicants;multi-input;multi-output","Logic functions;Circuit testing;Software systems;Boolean functions;Logic design;Design optimization;Benchmark testing;Data analysis;Algorithm design and analysis;Logic testing","Boolean functions;optimisation","mini-terms optimization;logic function;on-sets covering;Boolean function logic optimization","","1","15","","","","","","IEEE","IEEE Conferences"
"Defect Detection Efficiency: Test Case Based vs. Exploratory Testing","J. Itkonen; M. V. Mantyla; C. Lassenius","Helsinki University of Technology, Finland; Helsinki University of Technology, Finland; Helsinki University of Technology, Finland","First International Symposium on Empirical Software Engineering and Measurement (ESEM 2007)","","2007","","","61","70","This paper presents a controlled experiment comparing the defect detection efficiency of exploratory testing (ET) and test case based testing (TCT). While traditional testing literature emphasizes test cases, ET stresses the individual tester's skills during test execution and does not rely upon predesigned test cases. In the experiment, 79 advanced software engineering students performed manual functional testing on an open-source application with actual and seeded defects. Each student participated in two 90-minute controlled sessions, using ET in one and TCT in the other. We found no significant differences in defect detection efficiency between TCT and ET. The distributions of detected defects did not differ significantly regarding technical type, detection difficulty, or severity. However, TCT produced significantly more false defect reports than ET. Surprisingly, our results show no benefit of using predesigned test cases in terms of defect detection efficiency, emphasizing the need for further studies of manual testing.","1949-3770;1949-3789","0-7695-2886-4978-0-7695-2886","10.1109/ESEM.2007.56","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343733","","Software testing;Automatic testing;Software engineering;Performance evaluation;Automation;System testing;Humans;Design optimization;Documentation;Software performance","program debugging;program testing","defect detection efficiency;exploratory testing;test case based testing;software testing","","19","27","","","","","","IEEE","IEEE Conferences"
"Investigating the Effect of Refactoring on Software Testing Effort","K. O. Elish; M. Alshayeb","NA; NA","2009 16th Asia-Pacific Software Engineering Conference","","2009","","","29","34","Refactoring, the process of improving the design of existing code by changing its internal structure without affecting its external behavior, tends to improve software quality by improving design, improving readability, and reducing bugs. There are many different refactoring methods, each having a particular purpose and effect. Consequently, the effect of refactorings on software quality attribute may vary. Software testing is an external software quality attributes that takes lots of time and effort to make sure that the software performs as intended. In this paper, we propose a classification of refactoring methods based on their measurable effect on software testing effort. This, in turn, helps the software developers decide which refactoring methods to apply in order to optimize a software system with regard to the testing effort.","1530-1362;1530-1362","978-0-7695-3909","10.1109/APSEC.2009.14","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5358476","refactoring;software metrics;testing effort","Software testing","program testing;software metrics","software testing;refactoring methods;software quality","","4","25","","","","","","IEEE","IEEE Conferences"
"Incremental call graph reanalysis for AspectJ software","Y. Lin; S. Zhang; J. Zhao","School of Software, Shanghai Jiao Tong University, 800 Dongchuan Road, Shanghai 200240, China; School of Software, Shanghai Jiao Tong University, 800 Dongchuan Road, Shanghai 200240, China; School of Software, Shanghai Jiao Tong University, 800 Dongchuan Road, Shanghai 200240, China","2009 IEEE International Conference on Software Maintenance","","2009","","","306","315","Program call graph representation can be used to support many tasks in compiler optimization, program comprehension, and software maintenance. During software evolution, the call graph needs to remain fairly precise and be updated quickly in response to software changes. In this paper, we present an approach to incremental update, instead of exhaustive analysis of the initially constructed call graph in AspectJ software. Our approach first decomposes the source code edits between the updated and initial software versions into a set of atomic change representations, which capture the semantic differences. Then, we explore the relationship between atomic changes and call graph to incrementally update the initially constructed graph, instead of rebuilding it from the ground up. We implement the reanalysis approach on top of the ajc AspectJ compiler and perform an empirical study on 24 versions of eight AspectJ benchmarks. The experiment result shows that our approach can reduce a large portion of unnecessary reanalysis cost as program changes occur, and significant savings are observed for the incremental reconstruction of AspectJ call graph in comparison with an exhaustive analysis, with no loss in precision.","1063-6773","978-1-4244-4897-5978-1-4244-4828","10.1109/ICSM.2009.5306311","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306311","","Performance analysis;Software maintenance;Costs;Program processors;Optimizing compilers;Java;Software testing;Software debugging;Software systems;Computer bugs","object-oriented programming;program compilers;software maintenance","incremental call graph reanalysis;AspectJ software;program call graph representation;compiler optimization;program comprehension;software maintenance;software evolution;source code;atomic change representations;AspectJ compiler","","2","30","","","","","","IEEE","IEEE Conferences"
"A New Strategy for Pairwise Test Case Generation","Y. Cui; L. Li; S. Yao","NA; NA; NA","2009 Third International Symposium on Intelligent Information Technology Application","","2009","3","","303","306","Pairwise testing has become an important approach to software testing because it often provides effective error detection at low cost, and a key problem of it is the test case generation method. As the part of an effort to develop an optimized strategy for pairwise testing, this paper proposes an efficient pairwise test case generation strategy, called VIPO (Variant of In-Parameter-order), which is a variant of IPO strategy. We compare its effectiveness with some existing strategies including IPO, Tconfig, Pict and AllPairs. Experimental results demonstrate that VIPO outperformed them in terms of the number of generated test case within reasonable execution times, in most cases.","","978-0-7695-3859","10.1109/IITA.2009.416","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5369430","pairwise testing;test case generation;horizontal growth","Software testing;System testing;Information technology;Application software;Computer science;Computer errors;Costs;Performance evaluation;Benchmark testing;Software quality","combinatorial mathematics;error detection;program testing","pairwise test case generation;software testing;effective error detection;VIPO;variant of in-parameter-order;combinatorial testing","","10","16","","","","","","IEEE","IEEE Conferences"
"A low-overhead method of embedded software profiling","L. Fagui; L. Shengwen; X. Ran; L. Chunwei","School of Computer Science & Engineering, South China University of Technology, Guangzhou, P. R. China; School of Computer Science & Engineering, South China University of Technology, Guangzhou, P. R. China; School of Computer Science & Engineering, South China University of Technology, Guangzhou, P. R. China; School of Computer Science & Engineering, South China University of Technology, Guangzhou, P. R. China","2009 ISECS International Colloquium on Computing, Communication, Control, and Management","","2009","4","","436","439","With the continuous development of embedded technology, embedded applications are becoming increasingly popular in daily lives, and respectively embedded software is in great demand. As a result, how to profile embedded software to guarantee its quality becomes a focus of attention. This paper focuses on this issue. It analyzes current methods of embedded software profiling and their defects, and then proposes a low-overhead and more accurate method. It also performs some strict experiments to prove that it is effective and much better than current methods.","2154-9613;2154-963X","978-1-4244-4247","10.1109/CCCM.2009.5267623","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5267623","low-overhead;embedded software;profiling","Embedded software;Instruments;Hardware;Software testing;Application software;Software performance;Costs;Software libraries;Embedded system;Software tools","embedded systems;software performance evaluation;software quality","embedded software profiling;low-overhead method;continuous development;embedded software quality;software performance optimization;embedded software development","","2","9","","","","","","IEEE","IEEE Conferences"
"Improved Ant Colony Optimization with Particle Swarm Optimization Operator Solving Continuous Optimization Problems","Y. Xiao; X. Song; Z. Yao","NA; NA; NA","2009 International Conference on Computational Intelligence and Software Engineering","","2009","","","1","3","Ant colony optimization (ACO) has the disadvantages such as easily relapsing into local optima and. Aimed at improving this problem existed in ACO, several new betterments are proposed and evaluated. In particular, pheromone mutation and particle swarm optimization operator were inducted. Then an improved ant colony optimization with particle swarm optimization operator was put forward. It was tested by a set of benchmark continuous function optimization problems. And the results of the examples show that it can not easily run into the local optimum and can converge at the global optimum.","","978-1-4244-4507","10.1109/CISE.2009.5363391","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5363391","","Ant colony optimization;Particle swarm optimization;Cities and towns;Genetic mutations;Benchmark testing;Traveling salesman problems;Continuous production;Convergence","particle swarm optimisation","ant colony optimization;particle swarm optimization;benchmark continuous function optimization","","2","3","","","","","","IEEE","IEEE Conferences"
"A PSO Algorithm Based on Orthogonal Test Design","S. Wang; L. Chen","NA; NA","2009 Fifth International Conference on Natural Computation","","2009","3","","190","194","We present a particle swarm optimization algorithm OT-PSO using orthogonal test technique. Based on the classical PSO, OT-PSO searches for local optimum in the neighbor area of the global best solution by using the method of orthogonal test design. OT-PSO can probe the solutions uniformly distributed in the search space and select the better ones. Using those better solutions OT-PSO can guide the particles searching towards the correct direction in the latter iterations so as to speed up the convergence, get more precise solutions and avoid local optimum. Our experimental results show that OT-PSO algorithm not only has faster convergence speed, but also can improve the accuracy of solutions effectively and enhance the robustness of the algorithm.","2157-9555;2157-9563","978-0-7695-3736","10.1109/ICNC.2009.351","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5366260","Orthogonal test Design;particle swarm optimization;local search","Testing;Algorithm design and analysis;Particle swarm optimization;Genetic algorithms;Robustness;Software algorithms;Design optimization;Multicast algorithms;Streaming media;Routing","particle swarm optimisation;search problems","orthogonal test design;PSO algorithm;particle swarm optimization algorithm;orthogonal test technique;search space method;OT-PSO algorithm","","2","18","","","","","","IEEE","IEEE Conferences"
"RF multi-DUT testing technology for RF WLP","Hyunho Kim; Yongdeuk Ye; Sanghyun Choi; J. Lim; Soongyu Yim; Sung Yi","Samsung Electro-Mechanics CO., LTD, 314, Maetan3-Dong, Yeongtong-Gu, Suwon, Gyunggi-Do, Korea 443-743; Samsung Electro-Mechanics CO., LTD, 314, Maetan3-Dong, Yeongtong-Gu, Suwon, Gyunggi-Do, Korea 443-743; Samsung Electro-Mechanics CO., LTD, 314, Maetan3-Dong, Yeongtong-Gu, Suwon, Gyunggi-Do, Korea 443-743; Samsung Electro-Mechanics CO., LTD, 314, Maetan3-Dong, Yeongtong-Gu, Suwon, Gyunggi-Do, Korea 443-743; Samsung Electro-Mechanics CO., LTD, 314, Maetan3-Dong, Yeongtong-Gu, Suwon, Gyunggi-Do, Korea 443-743; Samsung Electro-Mechanics CO., LTD, 314, Maetan3-Dong, Yeongtong-Gu, Suwon, Gyunggi-Do, Korea 443-743","2008 2nd Electronics System-Integration Technology Conference","","2008","","","547","550","Test efficiency of packaged electronics strongly depends on test time and test cost because test time increase causes the increase of test cost. In this paper, with RF WLP devices fabricated by wafer level package processes, we have demonstrated RF multi-DUT testing technology with superior measurement stability and enhanced package yield in RF wafer level test. We could know that RF multi-DUT testing technology for RF WLP devices in wafer level testing has test time reduction of 61.9% compared to test time of RF single-DUT testing through analyzing test time. This technology can offer a number of significant advantages such as productivity improvement and superior capabilities for high performance RF wafer level testing. RF multi-DUT testing technology is essential in addressing future needs for testing RF WLP devices on the wafer.","","978-1-4244-2813-7978-1-4244-2814","10.1109/ESTC.2008.4684408","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4684408","RF multi-DUT testing;test efficiency;test time;wafer level testing;RF WLP","Radio frequency;Electronic equipment testing;Costs;Wafer scale integration;Electronics packaging;Optimized production technology;Software testing;System testing;Temperature dependence;Temperature sensors","circuit stability;semiconductor device packaging;semiconductor device testing;wafer level packaging","RF multi-DUT testing technology;RF WLP;electronics packaging;wafer level package processes;measurement stability;package yield enhancement;wafer level test;productivity improvement;superior capabilities","","","7","","","","","","IEEE","IEEE Conferences"
"Simulation Optimization Based on the Hypothesis Testing and ITO Process","W. Dong; D. Zhang","Wuhan University, China; Wuhan University, China","Third International Conference on Natural Computation (ICNC 2007)","","2007","5","","660","665","The ITO algorithm, which is a new kind of simulation optimization methods and is based on the hypothesis testing and Ito stochastic process, is proposed in this paper. The local strategies and global strategies can be utilized to design the ITO algorithm. The experimental results show that the ITO algorithm is suitable for the simulation optimization problems. The methods to improve the evaluation credibility in the simulation optimization are also discussed. Moreover, in this paper, the credibility based on the hypothesis testing is also proposed.","2157-9555;2157-9563","0-7695-2875-9978-0-7695-2875","10.1109/ICNC.2007.665","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4344922","","Indium tin oxide;Computational modeling;Optimization methods;Stochastic processes;Computer simulation;Software testing;Algorithm design and analysis;Constraint optimization;Software engineering;Software algorithms","optimisation;simulation;stochastic processes","simulation optimization;hypothesis testing;ITO process;Ito stochastic process","","3","11","","","","","","IEEE","IEEE Conferences"
"Structural Testing of Executables","S. Bardin; P. Herrmann","NA; NA","2008 1st International Conference on Software Testing, Verification, and Validation","","2008","","","22","31","Verification is usually performed on a high-level view of the software, either specification or program source code. However in certain circumstances verification is more relevant when performed at the machine code level. This paper focuses on automatic test data generation from a standalone executable. Low-level analysis is much more difficult than high-level analysis since even the control-flow graph is not available and bit-level instructions have to be modelled faithfully. We show how ""path-based"" structural test data generation can be adapted from structured language to machine code, using both state-of-the-art technologies and innovative techniques. Our results have been implemented in a tool named OSMOSE and encouraging experiments have been conducted.","2159-4848","978-0-7695-3127","10.1109/ICST.2008.8","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4539529","automatic test data generation;structural testing;executable","Performance evaluation;Computer languages;Software testing;Code standards;Optimizing compilers;Program processors;Security;Information analysis;Software reliability;Electronic mail","program testing;program verification","structural testing;software verification;program source code;automatic test data generation;structural test data generation","","21","23","","","","","","IEEE","IEEE Conferences"
"Application of non-parametric statistics of the parametric response for defect diagnosis","R. Gudavalli; W. R. Daasch; P. Nigh; D. Heaberlin","Integrated Circuits Design and Test Laboratory, Electrical and Computer Engineering, Portland State University, Oregon 97201, USA; Integrated Circuits Design and Test Laboratory, Electrical and Computer Engineering, Portland State University, Oregon 97201, USA; IBM Corporation, Test Strategy and Development IBM Systems and Technology Group, 1000 River Road, MS-963G, Essex Junction, VT 05495, USA; IBM Corporation, Test Strategy and Development IBM Systems and Technology Group, 1000 River Road, MS-963G, Essex Junction, VT 05495, USA","2009 International Test Conference","","2009","","","1","10","This paper presents a method using only the rank of the measurements to separate a part's elevated response to parametric tests from its non-elevated response. The effectiveness of the proposed method is verified on the 130nm ASIC. Good die responses are correlated for same parametric tests at different conditions such as temperature, voltage and or other stress. Nonparametric correlation methods are used to calculate the intra-die correlation. When intra-die correlation is found to be low the elevated vectors that lower correlation are extracted and input to IDDQ-based diagnostic tools. Monte-Carlo simulations are described to obtain confidence bounds of the correlation for good die test response.","1089-3539;1089-3539;2378-2250","978-1-4244-4868-5978-1-4244-4867","10.1109/TEST.2009.5355728","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5355728","","Parametric statistics;Circuit testing;Temperature;Integrated circuit testing;System testing;Fault diagnosis;Inspection;Scattering;Displays;Application software","application specific integrated circuits;integrated circuit testing;Monte Carlo methods;nonparametric statistics","nonparametric statistics;parametric response;defect diagnosis;parametric test;nonelevated response;ASIC;nonparametric correlation;intradie correlation;IDDQ-based diagnostic tool;Monte-Carlo simulation;die test response;size 130 nm","","2","6","","","","","","IEEE","IEEE Conferences"
"Evolution Strategy Based Automated Software Clustering Approach","B. Khan; S. Sohail; M. Y. Javed","NA; NA; NA","2008 Advanced Software Engineering and Its Applications","","2008","","","27","34","In the software development life cycle, maintenance is a key phase that determines long term and effective use of any software. Maintenance can become very lengthy and costly for large software systems when structure of the system is complicated. One of the factors complicating the structure of the software system is subsystem boundaries becoming ambiguous due to system evolution, lack of up to date documentation and high turn over rate of software professionals (leading to non availability of original designers of the software systems). Software module clustering helps software professionals to recover high-level structure of the system by decomposing the system into smaller manageable subsystems, containing interdependent modules. Automated approaches simplify the software clustering process, which otherwise is quite a tedious task for medium and large software systems. We treat software clustering as an optimization problem and propose an automated technique to get near optimal decompositions of relatively independent subsystems, containing interdependent modules. We propose the use of self adaptive Evolution Strategies to search a large solution space consisting of modules and their relationships. We compare our proposed approach with a widely used genetic algorithm based approach on a number of test systems. Our proposed approach shows considerable improvement in terms of quality and effectiveness of the solutions for all tests cases.","","978-0-7695-3432","10.1109/ASEA.2008.17","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721307","","Software systems;Software maintenance;Application software;Genetic algorithms;Software engineering;Educational institutions;Programming;Documentation;Availability;Software design","evolutionary computation;program testing;software development management;software maintenance;software quality;system documentation","evolution strategy;automated software clustering approach;software development life cycle;software maintenance;software systems;subsystem boundary;system evolution;system documentation;software module clustering;software professionals;high-level structure;interdependent modules;optimal decompositions;software quality","","4","19","","","","","","IEEE","IEEE Conferences"
"Load Testing for Web Applications","Y. Pu; M. Xu","NA; NA","2009 First International Conference on Information Science and Engineering","","2009","","","2954","2957","The performance testing criteria was analyzed, including response time, concurrency users, throughout and performance counter. Performance testing is necessary for the system reliability. Load testing can be used for software troubleshooting and optimizing. With the LoadRunner and TestDirector testing tools, a load testing scheme based on an online examination system was designed.","2160-1283;2160-1291","978-1-4244-5728-1978-1-4244-4909-5978-0-7695-3887","10.1109/ICISE.2009.720","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5454995","","Automatic testing;System testing;Software testing;Application software;Delay;Concurrent computing;Software performance;System performance;Computer bugs;Reliability engineering","Internet;program testing;software performance evaluation","load testing;Web application;performance testing criteria;response time;concurrency users;performance counter;system reliability;software troubleshooting;LoadRunner testing tools;TestDirector testing tools;online examination system","","1","7","","","","","","IEEE","IEEE Conferences"
"Gaussion Mutation Particle Swarm Optimization with Dynamic Adaptation Inertia Weight","L. Li; X. He","NA; NA","2009 WRI World Congress on Software Engineering","","2009","4","","454","459","An improved PSO with decreasing inertia weight is proposed in this paper, which is different from the inertia weight of standard PSO. In addition, a new social component instead of the old one to make more explore and a tiny Gauss perturbation joined in the position equation to help maintain swarm diversity. Four standard test functions with asymmetric initial range settings are used to prove its validity. Experimental results verify its superiority both in convergent speed and solution precision. Conclusions are drawn in the end.","","978-0-7695-3570","10.1109/WCSE.2009.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5319596","Gaussion mutation;Particle Swarm Optimization;inertia weight","Gaussian processes;Genetic mutations;Particle swarm optimization;Equations;Cultural differences;Testing;Neural networks;Software engineering;Mathematics;Software standards","Gaussian processes;optimisation","Gaussion mutation;particle swarm optimization;dynamic adaptation inertia weight;Gauss perturbation;position equation;standard test functions","","2","15","","","","","","IEEE","IEEE Conferences"
"A Novel Generation Algorithm of Pair-Wise Testing Cases","J. Gao; Y. Hu","NA; NA","2009 15th IEEE Pacific Rim International Symposium on Dependable Computing","","2009","","","43","48","Pair-wise testing is a practical and effective method which has already been used in the software testing. Extensive research has been made on the generation of pair-wise testing. In order to make it easy to analyze the current generation methods, we propose a method to ease the process. That is we transform the problem of pair-wise testing to a graphic one. The IPO algorithm is based on parameters and can ensure the optimization of test cases in each expansion. Though it has many advantages, it is still not sustainable enough because of its flexibility. We studied the three elements which affect its sustainability. The three elements are the horizontal growth of pair-wise testing, the combination of pair-wise testing cases and the extension sequence of the parameters to be extended. Thus we propose a HIPO algorithm based on IPO algorithm to solve those problems. The HIPO algorithm inherits the merits of high extension of IPO algorithm and introduces a new concept of contribution extent. It adopts the methods of preferential sequence as well as minimization algorithm to optimize the problems above. We develop the test case generation tool based on the HIPO algorithm by means of .Net technology. And we also prove its effectiveness in our experiment.","","978-0-7695-3849","10.1109/PRDC.2009.16","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5368228","testing;testing data;pair wise testing;IPO algorithm","Software testing;Costs;Software algorithms;Computer science;Electronic mail;System testing;Graphics;Minimization methods;Optimization methods;Statistical analysis","minimisation;program testing","test case generation;pair-wise testing;software testing;HIPO algorithm;optimization;preferential sequence;minimization","","","13","","","","","","IEEE","IEEE Conferences"
"Energy efficient software-based self-test for wireless sensor network nodes","R. Zhang; Z. Zilic; K. Radecka","Dept. of Electr. & Compupt. Eng., McGill Univ., Montreal, Que., Canada; Dept. of Electr. & Compupt. Eng., McGill Univ., Montreal, Que., Canada; NA","24th IEEE VLSI Test Symposium","","2006","","","6 pp.","191","We consider self-testing of complete wireless nodes in the field through a low-energy software-based self-test (SBST) method. Energy consumption is optimized both for individual components such as a CPU, embedded memories, and an RF module, as well as at the system level, considering the interplay between module tests. We first derive a scheme for software-based tests with the least amount of cycles and with operands of least Hamming distance and weight. Time interleaving of module tests at the system level further reduces the overall test energy consumption","1093-0167;2375-1053","0-7695-2514","10.1109/VTS.2006.32","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1617587","","Energy efficiency;Built-in self-test;Wireless sensor networks;Radio frequency;Energy consumption;Software testing;Circuit testing;System testing;Interleaved codes;Microcontrollers","automatic test software;wireless sensor networks","software-based self-test;wireless sensor network nodes;complete wireless nodes;SBST method;CPU;embedded memories;RF module;Hamming distance;Hamming weight;test energy consumption","","8","20","","","","","","IEEE","IEEE Conferences"
"Managing evolving hardware and software requirements","S. J. O'Donnell; A. Zarcone","Lockheed Martin, 12506 Lake Underhill Road, Orlando, Florida 32825, USA; Galileo Avionica, Strada Privata Aeroporto Caselle, 10077 S.Maurizio C.se (TO) ITALY","2009 IEEE AUTOTESTCON","","2009","","","130","135","Lockheed Martin Simulation, Training and Support (LM-STS) has designed and fielded the LM-STAR<sup>reg</sup> family of test systems to numerous customers to meet their production delivery schedules. The LM-STAR systems test avionics and other electronic systems and subsystems in factories and depots on multiple U.S. and International platforms supplying the end user with accurate fault diagnostics to repair Units Under Test (UUT). Galileo Avionica has acquired an LM-STAR system for their production facility and have successfully rehosted legacy Test Program Sets (TPS) from the Consolidated Automated Support System (CASS) to LM-STAR. Galileo Avionica needed a new variant of LM-STAR to support the unique new requirements of their customer. The system needed to be expandable and flexible to meet these needs, and system performance had to be optimized for high speed synchronous/asynchronous data I/O. The new configuration posed many technological challenges from both a hardware and software standpoint that had to be overcome. An aggressive schedule coupled with limited budget presented obstacles. The project also had dynamic test requirements. This paper will describe how a project can still meet time-to-market, cost, and quality objectives while addressing a myriad of requirement changes without spiraling out of control.","1088-7725;1558-4550","978-1-4244-4980-4978-1-4244-4981","10.1109/AUTEST.2009.5314031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5314031","","Hardware;System testing;Electronic equipment testing;Production facilities;Management training;Production systems;Aerospace electronics;Automatic testing;System performance;Software performance","aerospace computing;aerospace simulation;aerospace testing;avionics;computer based training;engineering education;fault diagnosis;formal specification;formal verification;software architecture;systems analysis","evolving hardware management;software requirement management;Lockheed Martin Simulation-Training and Support;LM-STS;LM-STAR;production delivery schedule;avionics testing;electronic system testing;fault diagnostics;unit under test;UUT;legacy test program set;consolidated automated support system;CASS;high speed synchronous data I/O;asynchronous data I/O;aggressive schedule;limited budget;software architecture","","4","1","","","","","","IEEE","IEEE Conferences"
"An Efficient Test Pattern Selection Method for Improving Defect Coverage with Reduced Test Data Volume and Test Application Time","Z. Wang; K. Chakrabarty","Duke University, Durham, NC; Duke University, Durham, NC","2006 15th Asian Test Symposium","","2006","","","333","338","Testing using n-detection test sets, in which a fault is detected by n (n &gt; 1) input patterns, is being increasingly advocated to increase defect coverage. However, the data volume for an n-detection test set is often too large, resulting in high testing time and tester memory requirements. Test set selection is necessary to ensure that the most effective patterns are chosen from large test sets in a high-volume production testing environment. Test selection is also useful in a time-constrained wafer-sort environment. The authors use a probabilistic fault model and the theory of output deviations for test set selection - the metric of output deviation is used to rank candidate test patterns without resorting to fault grading. To demonstrate the quality of the selected patterns, experimental results were presented for resistive bridging faults and non-feedback zero-resistance bridging faults in the ISCAS benchmark circuits. Our results show that for the same test length, patterns selected on the basis of output deviations are more effective than patterns selected using several other methods","1081-7735;2377-5386","0-7695-2628","10.1109/ATS.2006.260952","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4030788","","Circuit faults;Production;Circuit testing;Fault detection;Integrated circuit interconnections;Integrated circuit modeling;Application software;Electrical fault detection;Semiconductor device modeling;Benchmark testing","automatic test pattern generation;fault simulation;integrated circuit testing","test pattern selection;defect coverage;reduced test data;test application time;test set selection;probabilistic fault model;output deviations;resistive bridging faults;nonfeedback zero-resistance bridging;ISCAS benchmark","","10","24","","","","","","IEEE","IEEE Conferences"
"Compatibility and Regression Testing of COTS-Component-Based Software","L. Mariani; S. Papagiannakis; M. Pezze","Universita degli studi di Milano Bicocca, Italy; Universita degli studi di Milano Bicocca, Italy; Universita degli studi di Milano Bicocca, Italy","29th International Conference on Software Engineering (ICSE'07)","","2007","","","85","95","Software engineers frequently update COTS components integrated in component-based systems, and can often chose among many candidates produced by different vendors. This paper tackles both the problem of quickly identifying components that are syntactically compatible with the interface specifications, but badly integrate in target systems, and the problem of automatically generating regression test suites. The technique proposed in this paper to automatically generate compatibility and prioritized test suites is based on behavioral models that represent component interactions, and are automatically generated while executing the original test suites on previous versions of target systems.","0270-5257;1558-1225","0-7695-2828","10.1109/ICSE.2007.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4222571","","Software testing;System testing;Automatic testing;Binary codes;Software systems;Natural languages;Java;Control systems;Lead;XML","formal specification;object-oriented programming;program testing;software packages","COTS component-based software;commecial off-the-shelf;compatibility testing;regression testing;interface specification;component interaction representation","","20","25","","","","","","IEEE","IEEE Conferences"
"Application of system models in regression test suite prioritization","B. Korel; G. Koutsogiannakis; L. H. Tahat","Computer Science Department, Illinois Institute of Technology, Chicago, 60616, USA; Computer Science Department, Illinois Institute of Technology, Chicago, 60616, USA; Gulf University for Science & Tech., P.O. Box 7207, Hawally 32093, Kuwait","2008 IEEE International Conference on Software Maintenance","","2008","","","247","256","During regression testing, a modified system needs to be retested using the existing test suite. Since test suites may be very large, developers are interested in detecting faults in the system as early as possible. Test prioritization orders test cases for execution to increase potentially the chances of early fault detection during retesting. Most of the existing test prioritization methods are based on the code of the system, but model-based test prioritization has been recently proposed. System modeling is a widely used technique to model state-based systems. The existing model based test prioritization methods can only be used when models are modified during system maintenance. In this paper, we present model-based prioritization for a class of modifications for which models are not modified (only the source code is modified). After identification of elements of the model related to source-code modifications, information collected during execution of a model is used to prioritize tests for execution. In this paper, we discuss several model-based test prioritization heuristics. The major motivation to develop these heuristics was simplicity and effectiveness in early fault detection. We have conducted an experimental study in which we compared model-based test prioritization heuristics. The results of the study suggest that system models may improve the effectiveness of test prioritization with respect to early fault detection.","1063-6773","978-1-4244-2613-3978-1-4244-2614","10.1109/ICSM.2008.4658073","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658073","","Fault detection;Testing;Heuristic algorithms;Modeling;Life estimation;Software systems;USA Councils","program diagnostics;program testing;software maintenance","fault detection;regression test suite prioritization;state-based system modeling;system maintenance;source-code modification","","24","24","","","","","","IEEE","IEEE Conferences"
"Search-based Testing using State-based Fitness","R. Lefticaru; F. Ipate","NA; NA","2008 IEEE International Conference on Software Testing Verification and Validation Workshop","","2008","","","210","210","This paper investigates the adequacy of a general form of fitness function, based on a state-based specification, by characterizing the associated search problem and the dynamics of the applied metaheuristic. The measurement approach of the fitness landscape is experimented on various examples and suggests some tuning choices to be made for the metaheuristic considered.","","978-0-7695-3388","10.1109/ICSTW.2008.35","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4567009","","Software testing;Autocorrelation;Simulated annealing;Genetic algorithms;Search problems;Performance evaluation;Computer science;Mathematics;Calendars;Particle swarm optimization","formal specification;program testing;search problems","search-based testing;state-based fitness;fitness function;state-based specification;applied metaheuristic","","1","3","","","","","","IEEE","IEEE Conferences"
"Reliability Growth Modeling for Software Fault Detection Using Particle Swarm Optimization","A. Sheta","Associate Professor with the Computers and Systems Department, Electronics Research Institute (ERI), El-Tahrir Street, Dokky, Giza, Egypt, E-mail: asheta2@yahoo.com","2006 IEEE International Conference on Evolutionary Computation","","2006","","","3071","3078","Modeling the software testing process to obtain the predicted faults (failures) depends mainly on representing the relationship between execution time (or calendar time) and the failure count or accumulated faults. A number of unknown function parameters such as the mean failure function mu(t;beta) and the failure intensity function lambda(t;beta) are estimated using either least-square or maximum likelihood estimation techniques. Unfortunately, the model parameters are normally in nonlinear relationships. This makes traditional parameter estimation techniques suffer many problems in finding the optimal parameters to tune the model for a better prediction. In this paper, we explore our preliminary idea in using particle swarm optimization (PSO) technique to help in solving the reliability growth modeling problem. The proposed approach will be used to estimate the parameters of the well known reliability growth models such as the exponential model, power model and S-shaped models. The results are promising.","1089-778X;1941-0026","0-7803-9487","10.1109/CEC.2006.1688697","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1688697","","Fault detection;Particle swarm optimization;Software testing;Software reliability;Parameter estimation;Predictive models;Project management;Software engineering;NASA;Neural networks","least mean squares methods;maximum likelihood estimation;particle swarm optimisation;program testing;reliability;software fault tolerance","reliability growth modeling;software fault detection;particle swarm optimization;software testing process;least-square estimation;maximum likelihood estimation technique;parameter estimation","","8","38","","","","","","IEEE","IEEE Conferences"
"How International Standards Such as ATML and IEEE 1641 STD can Make the Realisation of an Open System Architecture on a Common Test Platform a Reality","C. Gorringe","MIEEE, EADS Test & Services (UK) Ltd., 29-31 Cobham Road, Wimborne, Dorset BH21 7PF, +44 (0)1202 872800, chris.gorringe@eads-ts.com","2006 IEEE Autotestcon","","2006","","","731","738","A perspective on how the DoD and MoD are integrating open standards into their ATS frameworks and policy in the search for a common test platform architecture solution for use on all test platforms. The paper examines the two approaches being taken and draws on their commonality to propose how open standards can help meet both their aims and circumstances. Benefits such as TPS interoperability, re-host and re-use are examined and contrasted for open systems versus common architecture to identify the practical implication for real systems. The life cycle cost of support for the system is identified and the trade-off in cost between fast optimal TPSs and fully interoperable TPSs is considered. The paper goes on to show the difference between using information models utilizing a development process versus the use of run time interfaces and how they can lead to different solutions to the same basic problem but with different peripheral benefits. In conclusion an approach to maximize benefit between the two framework groups is considered whilst maintaining individual priorities.","1088-7725;1558-4550","1-4244-0052-X1-4244-0051","10.1109/AUTEST.2006.283756","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4062469","","Open systems;System testing;Automatic testing;Software testing;Cost function;Logistics;Computer industry;Defense industry;Software standards;Standards organizations","automatic test equipment;IEEE standards;military systems;software architecture","international standards;ATML;IEEE 1641 STD;open system architecture;DoD;MoD;ATS frameworks;common test platform architecture;life cycle support cost;run time interfaces","","","2","","","","","","IEEE","IEEE Conferences"
"An Effective Technique for the Automatic Generation of Diagnosis-Oriented Programs for Processor Cores","P. Bernardi; E. E. S. Sanchez; M. Schillaci; G. Squillero; M. Sonza Reorda","NA; NA; NA; NA; NA","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","","2008","27","3","570","574","A large part of microprocessor cores in use today are designed to be cheap and mass produced. The diagnostic process, which is fundamental to improve yield, has to be as cost effective as possible. This paper presents a novel approach to the construction of diagnosis-oriented software-based test sets for microprocessors. The methodology exploits existing manufacturing test sets designed for software-based self-test and improves them by using a new diagnosis-oriented approach. Experimental results are reported in this paper showing the feasibility, robustness, and effectiveness of the approach for diagnosing stuck-at faults on an Intel i8051 processor core.","0278-0070;1937-4151","","10.1109/TCAD.2008.915541","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4454005","Fault diagnosis;optimization;simulation;testing","Costs;Microprocessors;Software testing;Manufacturing;Automatic testing;Application software;Built-in self-test;Microcontrollers;Fault diagnosis;Circuit faults","automatic programming;automatic test pattern generation;automatic test software;built-in self test;microprocessor chips","automatic generation;diagnosis-oriented programs;microprocessor cores;diagnostic process;diagnosis-oriented software-based test sets;manufacturing test sets;software-based self-test;stuck-at faults diagnosis;Intel i8051 processor core","","18","14","","","","","","IEEE","IEEE Journals & Magazines"
"When Politics Overshadow Software Quality","S. Stribrny; F. B. Mackin","NA; NA","IEEE Software","","2006","23","5","72","73","Software quality is subject to the politics of 1) control, when one person can tell others how things will be done; 2) position, when an individual's rank in the corporate structure influences outcomes; 3) power, when one individual has power over another, as with setting salaries. In turn, the politics of control, position, and power affect what questions are asked, who may ask them, and what the ""right"" questions are. And the question asked can control outcomes. When all these political factors - control, position, power, and questions - converge, the politics of communications and marketing come into play. These determine what people hear and how interaction styles affect outcomes. Everyone in the corporate hierarchy must consider these political realities","0740-7459;1937-4194","","10.1109/MS.2006.145","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1687865","software quality;time-to-market;management-employee communications;politics","Software quality;Quality management;Logic testing;System testing;Quality assurance;Engineering management;Roads;Customer satisfaction;Production;Time to market","software management;software quality","software quality;political factor;software management","","","","","","","","","IEEE","IEEE Journals & Magazines"
"A Hybrid Approach to Build Prioritized Pairwise Interaction Test Suites","X. Chen; Q. Gu; X. Wang; A. Li; D. Chen","NA; NA; NA; NA; NA","2009 International Conference on Computational Intelligence and Software Engineering","","2009","","","1","4","Traditional interaction testing aims to build test suites that cover all t-way interactions of inputs. But in many test scenarios, the entire test suites cannot be fully run due to the limited budget. Therefore it is necessary to take the importance of interactions into account and prioritize these tests of the test suite. In the paper, we use the hybrid approach to build prioritized pairwise interaction test suites (PITS). It adopts a one-test-at-a-time strategy to construct final test suites. But to generate a single test it firstly generates a candidate test and then applies a specific metaheuristic search strategy to enhance this test. Here we experiment four different metaheuristic search strategies. In the experiments, we compare our approach to weighted density algorithm (WDA). Meanwhile, we also analyze the effectiveness of four different search strategies and the effectiveness of the increasing iterations. Empirical results demonstrate the effectiveness of our proposed approach.","","978-1-4244-4507","10.1109/CISE.2009.5365886","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5365886","","Software testing;Greedy algorithms;Simulated annealing;Laboratories;Computer science;Statistical analysis;Programming;Costs;Application software;Ant colony optimization","program testing","prioritized pairwise interaction test suites;t-way interactions;metaheuristic search strategy;weighted density algorithm","","1","11","","","","","","IEEE","IEEE Conferences"
"Software Reliability Metrics Selecting Method Based on Analytic Hierarchy Process","H. Li; M. Lu; Q. Li","BeiHang University, China; BeiHang University, China; BeiHang University, China","2006 Sixth International Conference on Quality Software (QSIC'06)","","2006","","","337","346","It is very important to select and use appropriate software reliability metrics in software reliability engineering. This paper proposes a framework for selecting software reliability metrics based on analytic hierarchy process (AHP) and expert judgment. Selecting criteria and the metrics for selection are identified. In each development phase, the grading of metrics according to every criterion are given by experts qualitatively, and then analyzed synthetically to calculate the weights of metrics using AHP. A preliminary application is practised, and the metrics whose weights are top-ranked are recommended and analyzed. Sensitivity and consistency of this method are also analyzed. Compared with general selecting criteria, the method studied in this paper can be used to select appropriate metrics correctly, stably and systemically. Furthermore, the final selection results are accordant with engineering experience, and using the metrics recommended will make software reliability evaluation more reliable and effective","1550-6002;2332-662X","0-7695-2718","10.1109/QSIC.2006.59","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4032303","","Software reliability;Reliability engineering;Software quality;Software testing;Decision making;Systems engineering and theory;Software safety;Costs;Software engineering;Application software","program testing;software metrics;software quality;software reliability","software reliability metrics;analytic hierarchy process;software engineering;sensitivity analysis;consistency analysis","","2","11","","","","","","IEEE","IEEE Conferences"
"Extension of Rank Test for Sequences over GF(q)","A. Kakhbod; M. D. Alian; S. Mohajer","Department of Electrical Engineering Isfahan University of Technology. ali_kakhbod@ec.iut.ac.ir; NA; NA","2006 2nd International Conference on Information & Communication Technologies","","2006","1","","1391","1395","Pseudo-random and random sequences are widely used in many applications, such as stream cipher systems. Statistical tests are usually used to evaluate randomness of sequences. Binary matrix rank test is one method to evaluate randomness of sequences. This test is based on constructing matrices whose rows are successive sub-strings of the sequence, and check for linear dependency among the rows or columns of the constructed matrices. In this paper, we represent new method to testing for randomness based on linear dependency among fixed-length substrings of sequences over GF(q)","","0-7803-9521","10.1109/ICTTA.2006.1684584","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1684584","","Testing;Random sequences;Probability;Computer simulation;Application software;Cryptography;Computational modeling;Mathematical model;Binary sequences","Galois fields;matrix algebra;random processes;random sequences;statistical testing","pseudorandom sequences;binary matrix rank test;sequence randomness;linear dependency;constructed matrices;fixed-length substrings","","1","4","","","","","","IEEE","IEEE Conferences"
"Scheduling Test Execution of WBEM Applications","L. Yu; Y. Su; Q. Wang","NA; NA; NA","2009 16th Asia-Pacific Software Engineering Conference","","2009","","","323","330","When test execution of software applications, more often than not testers compete for some testing resources while holding other resources idle. How to optimize utility of testing resources, balance workload of testers and minimize test execution time is the research focus of this paper. We cluster and sequence test cases in order to minimize resource conflicts among testers; and modify the ATC (Apparent Tardiness Cost) algorithm to optimize both global test tasks and individual tester's tasks. We applied the proposed approach to scheduling test execution of Web-based Enterprise Management (WBEM) applications in a well-known hardware-software vendor and obtained promising results.","1530-1362;1530-1362","978-0-7695-3909","10.1109/APSEC.2009.27","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5358720","WBEM;Testing execution;Resource conflicts;Clustering analysis;Sequencing;Scheduling;Setup time","Software testing;Application software;Central Processing Unit;Hard disks;Resource management;System testing;Environmental management;Technology management;Computer networks;Automatic testing","Internet;program testing;scheduling","test execution scheduling;WBEM applications;software applications;test execution time minimization;apparent tardiness cost algorithm;global test tasks;individual tester tasks;Web-based enterprise management;hardware-software vendor","","1","16","","","","","","IEEE","IEEE Conferences"
"Efficient Software Test Case Generation Using Genetic Algorithm Based Graph Theory","V. Rajappa; A. Biradar; S. Panda","NA; NA; NA","2008 First International Conference on Emerging Trends in Engineering and Technology","","2008","","","298","303","In orthodox software testing approach we generally use modeling based testing approach for generating the test cases of a given problem. This leads to confusion of the test input and the expected output for a given test case. More over we also can miss some of the test cases due to lack of clarity in the test paths. To over come such sort of predictive modeling we propose graph theory based genetic approach to generate test cases for software testing. At first we will create a directed graph of all the intermediate state of the system for the expected behavior of the system. Then we will create a population of all the nodes of the graph as the base population of genetic algorithm. From this population we can find a pair of node the parents and perform genetic crossover and mutation on them for the getting the optimum child nodes as the out put. We should continue this process of genetic operation until all the nodes are covered or any of the nodes, which are visited more than once, should be discarded form the population. Then follow the same process for the generation of test case in the real time system. This technique will be more concrete in case of network testing or any of the system testing where the predictive model based tests are not optimized to produced the out put.","2157-0477;2157-0485","978-0-7695-3267","10.1109/ICETET.2008.79","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4579914","Genetic Algorithm (GA);Crossover (CO)","Genetic algorithms;Graph theory;Software;Pediatrics;Genetic programming;Software testing;Genetics","genetic algorithms;graph theory;program testing","orthodox software testing;genetic algorithm;graph theory;predictive modeling;real time system","","10","10","","","","","","IEEE","IEEE Conferences"
"Covering arrays for efficient fault characterization in complex configuration spaces","C. Yilmaz; M. B. Cohen; A. A. Porter","Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA; NA; NA","IEEE Transactions on Software Engineering","","2006","32","1","20","34","Many modern software systems are designed to be highly configurable so they can run on and be optimized for a wide variety of platforms and usage scenarios. Testing such systems is difficult because, in effect, you are testing a multitude of systems, not just one. Moreover, bugs can and do appear in some configurations, but not in others. Our research focuses on a subset of these bugs that are ""option-related""-those that manifest with high probability only when specific configuration options take on specific settings. Our goal is not only to detect these bugs, but also to automatically characterize the configuration subspaces (i.e., the options and their settings) in which they manifest. To improve efficiency, our process tests only a sample of the configuration space, which we obtain from mathematical objects called covering arrays. This paper compares two different kinds of covering arrays for this purpose and assesses the effect of sampling strategy on fault characterization accuracy. Our results strongly suggest that sampling via covering arrays allows us to characterize option-related failures nearly as well as if we had tested exhaustively, but at a much lower cost. We also provide guidelines for using our approach in practice.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.8","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1583600","Software testing;distributed continuous quality assurance;fault characterization;covering arrays.","System testing;Sampling methods;Computer bugs;Software systems;Costs;Quality assurance;Predictive models;Software design;Design optimization;Guidelines","program testing;program debugging;software fault tolerance;configuration management","covering arrays;fault characterization;complex configuration spaces;software system testing;software system bug detection;option-related failure characterization;distributed continuous quality assurance","","122","20","","","","","","IEEE","IEEE Journals & Magazines"
"Transition Faults Testing Based on Functional Delay Tests","E. Bareisa; V. Jusas; K. Motiejunas; R. Seinauskas","Software Engineering Department, Kaunas University of Technology, Student¿ 50-406., LT-51368 Kaunas, Lithuania; Software Engineering Department, Kaunas University of Technology, Student¿ 50-406., LT-51368 Kaunas, Lithuania; Software Engineering Department, Kaunas University of Technology, Student¿ 50-406., LT-51368 Kaunas, Lithuania; Software Engineering Department, Kaunas University of Technology, Student¿ 50-406., LT-51368 Kaunas, Lithuania","2007 IEEE Design and Diagnostics of Electronic Circuits and Systems","","2007","","","1","5","Rapid advances of semiconductor technology lead to higher circuit integration as well as higher operating frequencies. The statistical variations of the parameters during the manufacturing process as well as physical defects in integrated circuits can sometimes degrade circuit performance without altering its logic functionality. These faults are called delay faults. In this paper we consider the quality of the tests generated for two types of delay faults, namely, functional delay and transition faults. We compared the test quality of functional delay tests in regard to transition faults and vice versa. We have performed various comprehensive experiments with combinational benchmark circuits. The experiments exhibit that the test sets, which are generated according to the functional delay fault model, obtain high fault coverages of transition faults. However, the functional delay fault coverages of the test sets targeted for the transition faults are low. It is very likely that the test vectors based on the functional delay fault model can cover other kinds of the faults. Another advantage of test set generated at the functional level is that it is independent of and effective for any implementation and, therefore, can be generated at early stages of the design process.","","1-4244-1161-01-4244-1162","10.1109/DDECS.2007.4295315","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4295315","","Delay;Circuit faults;Circuit testing;Integrated circuit technology;Lead compounds;Frequency;Manufacturing processes;Degradation;Circuit optimization;Logic circuits","automatic test pattern generation;combinational circuits;fault diagnosis;integrated circuit testing;logic testing","transition faults testing;functional delay tests;semiconductor technology;circuit integration;statistical variations;manufacturing process;physical defects;integrated circuits;circuit performance;logic functionality;delay faults;combinational benchmark circuits;functional delay fault coverages","","","17","","","","","","IEEE","IEEE Conferences"
"A Dynamic Partitioning Approach for GUI Testing","K. Cai; L. Zhao; F. Wang","Beijing University of Aeronautics and Astronautics, China; Beijing University of Aeronautics and Astronautics, China; Beijing University of Aeronautics and Astronautics, China","30th Annual International Computer Software and Applications Conference (COMPSAC'06)","","2006","2","","223","228","Previous works on GUI testing are mainly concerned with how to define or generate GUI test cases. The issue of how to employ generated GUI test cases or primitive actions is seldom discussed. In this paper we propose a dynamic partitioning approach for GUI testing to address the issue. In this approach, the given GUI primitive actions are dynamically partitioned into two disjoint classes: one comprising prioritized primitive actions and the other comprising non-prioritized ones. The testing process is divided into two stages and contains two feedback loops. The first stage prioritizes primitive actions and the second stage selects and performs prioritized primitive actions. The first feedback loop is local and occurs in the second stage, which adjusts the memberships of primitive actions after they are performed. The second feedback loop is global and occurs between the first and second stages. It switches GUI testing from the second stage to the first stage upon no prioritized primitive actions are available. Two testing experiments with real GUI applications show that the proposed dynamic partitioning approach can really work in practice and may significantly outperform the random testing approach in the sense that the dynamic partitioning approach uses fewer primitive actions to achieve given testing goals and behaves more stable. The dynamic partitioning approach adopts explicit feedback mechanisms and contributes to the emerging area of software cybernetics that explores the interplay between software and control","0730-3157;0730-3157","0-7695-2655","10.1109/COMPSAC.2006.94","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4020171","","Graphical user interfaces;Aerodynamics;Automatic testing;Feedback loop;Computational Intelligence Society;Application software;Switches;Cybernetics;Software testing;Materials testing","automatic test pattern generation;graphical user interfaces;program control structures;program testing","dynamic partitioning;GUI testing;feedback loops;random testing;software cybernetics","","3","12","","","","","","IEEE","IEEE Conferences"
"Testing and Analysis of Access Control Policies","E. Martin","North Carolina State University, USA","29th International Conference on Software Engineering (ICSE'07 Companion)","","2007","","","75","76","Policy testing and analysis are important techniques for high assurance of correct specification of access control policies. We propose a set of testing and analysis techniques for access control policies and tools for empirically investigating and evaluating the proposed techniques. We propose a fault model for access control policies and investigate various fault types and their frequencies of occurrence in policy development; we develop a mutation testing framework that implements the fault model; we propose and investigate various coverage criteria for testing access control policies; we develop various test generation techniques and evaluate them using the coverage criteria and mutation testing framework; we develop a policy model to facilitate refactoring, performance optimizations, dependency identification, and other types of static analysis. To make our discussion concrete, we choose to present our techniques in the context of XACML. Note that since XACML is an application- independent, generic access control policy language, our techniques can be equally applied to test policies written in other languages.","","0-7695-2892","10.1109/ICSECOMPANION.2007.73","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4222687","","Access control;Genetic mutations;Frequency;Fault diagnosis;Optimization;Performance analysis;Concrete;Software testing;Specification languages;XML","authorisation;program diagnostics;program testing","mutation testing framework;fault model;access control policy testing;test generation technique;XACML;generic access control policy language","","4","4","","","","","","IEEE","IEEE Conferences"
"Optimising Project Feature Weights for Analogy-Based Software Cost Estimation using the Mantel Correlation","J. W. Keung; B. Kitchenham","NA; NA","14th Asia-Pacific Software Engineering Conference (APSEC'07)","","2007","","","222","229","Software cost estimation using analogy is an important area in software engineering research. Previous research has demonstrated that analogy is a viable alternative to other conventional estimation methods in terms of predictive accuracy. One of the important research areas for analogy is how to determine suitable project feature weights. This can be achieved by using an extensive project feature weights search, where the quality measure is optimised. However, this approach suffers similar issues as the brute-force feature selection approach in analogy. We propose a novel method to deal with this issue based upon the use of the Mantel randomisation test. Specifically, we determine project feature weights based on the strength of correlation between the distance matrix of project features and the distance matrix of known effort values of the dataset. We demonstrate the procedure on a specific dataset, showing the use of the Mantel correlation to identify whether analogy is appropriate, and whether the project feature weights can be determined by statistical inference. Our results also show improved prediction accuracy when multiple project features are used with determined weights. Our method, thus, provides a sound statistical basis for analogy.","1530-1362;1530-1362","0-7695-3057","10.1109/ASPEC.2007.42","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4425858","","Cost function;Accuracy;Life estimation;Euclidean distance;Software engineering;Testing;Spatial databases;Australia;Statistical analysis;Sensitivity analysis","matrix algebra;optimisation;project management;software cost estimation;statistical testing","analogy-based software cost estimation;Mantel correlation;software engineering;project feature weight optimisation;brute-force feature selection approach;Mantel randomisation test;distance matrix;statistical inference","","6","15","","","","","","IEEE","IEEE Conferences"
"A Test Based Multidimensional Performance Model for a Mission Critical System Server","K. Rogoz; K. Figura","NA; NA","2009 International Conference on Software Testing Verification and Validation","","2009","","","269","278","This paper presents an approach to performance modeling of servers managing radio networks, applied in Motorola Government &amp; Public Safety Department. The primary goal of creating the performance model for mission-critical systems is to verify if strict requirements are met even under the highest possible load. By determining capacity of various hardware configurations, the model serves also as a cost optimization tool for a customer. In large radio systems the cost of devices is tremendous, therefore the method of determining the most economical hardware capable of handling the intended load, is vital. The last but not least purpose of performance tests, being the key part of modeling, is creating a product enhancement plan by finding bottlenecks to widen and indicating parameters that need tuning. The presented method includes the following steps:(1) creation of a predictive model, (2) planning and execution of the set of tests to get real performance measurements, (3) model validation by applying mathematical regression to compare the test results with the theoretical model. In the industry practice the product is under continuous development, so the predictive model for every release is based on experience from previous releases. Many detected weaknesses and performance problems are postponed to the next release. The performance model covers them all and evolves together with the system. The paper describes the full cycle of modeling of a group of Unix-based servers controlling a radio network system. The paper is founded on interesting cases from real-life industrial testing.","2159-4848","978-1-4244-3775-7978-0-7695-3601","10.1109/ICST.2009.40","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4815359","Performance testing;test strategy planning;system capacity;predictive model validation;model evolution;mathematical regression;Unix server monitoring;resource utilization;CPU;memory;swapping","System testing;Multidimensional systems;Mission critical systems;Predictive models;Network servers;Radio network;Hardware;Mathematical model;Radio spectrum management;Government","file servers;optimisation;program testing;regression analysis;safety-critical software;Unix","test based multidimensional performance;mission critical system server;cost optimization tool;predictive model;mathematical regression;Unix-based server;radio network","","","8","","","","","","IEEE","IEEE Conferences"
"Quasi-Renewal Time-Delay Fault-Removal Consideration in Software Reliability Modeling","S. Hwang; H. Pham","NA; NA","IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans","","2009","39","1","200","209","Software reliability growth models based on a nonhomogeneous Poisson process (NHPP) have been considered as one of the most effective among various models since they integrate the information regarding testing and debugging activities observed in the testing phase into the software reliability model. Although most of the existing NHPP models have progressed successfully in their estimation/prediction accuracies by modifying the assumptions with regard to the testing process, these models were developed based on the instantaneous fault-removal assumption. In this paper, we develop a generalized NHPP software reliability model considering quasi-renewal time-delay fault removal. The quasi-renewal process is employed to estimate the time delay due to identifying and prioritizing the detected faults before actual code change in the software reliability assessment. Model formulation based on the quasi-renewal time-delay assumption is provided, and the generalized mean value function (MVF) for the proposed model is derived by using the method of steps. The general solution of the MVFs for the proposed model is also obtained for some specific existing models. The numerical examples, based on a software failure data set, show that the consideration of quasi-renewal time-delay fault-removal assumption improves the descriptive properties of the model, which means that the length of time delay is getting decreased since testers and programmers adapt themselves to the working environment as testing and debugging activities are in progress.","1083-4427;1558-2426","","10.1109/TSMCA.2008.2007982","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4694128","Mean value function (MVF);nonhomogeneous Poisson process (NHPP);quasi-renewal process;software reliability engineering;time-delay fault removal;Mean value function (MVF);nonhomogeneous Poisson process (NHPP);quasi-renewal process;software reliability engineering;time-delay fault removal","Software reliability;Software testing;Predictive models;Delay effects;Accuracy;Delay estimation;Fault diagnosis;Fault detection;Software debugging;Programming profession","delays;estimation theory;fault diagnosis;program debugging;program testing;software fault tolerance;stochastic processes","software reliability growth model;quasi renewal time-delay fault removal assumption;nonhomogeneous Poisson process;software testing;software debugging activity;time delay estimation;fault detection;software reliability assessment;model formulation;generalized mean value function","","27","31","","","","","","IEEE","IEEE Journals & Magazines"
"Specification-based Test Generation and Optimization Using Model Checking","H. Zeng; H. Miao; J. Liu","Wuhan University; Shanghai University, Shanghai 200072, China; China Normal University, Shanghai 200062, China","First Joint IEEE/IFIP Symposium on Theoretical Aspects of Software Engineering (TASE '07)","","2007","","","349","355","The capability of model checkers to construct counterexamples provides a basis for automated test generation. However, many model checking-based testing approaches just focus on generating test sets with respect to some coverage criteria. Such test sets generally are large and inefficient because of much redundancy. We propose an on-the-fly approach that performs test generation and redundancy elimination by turns. Our approach employs a test-tree to pick out and represent a subset of tests with equal coverage for a test criterion and no redundancy. Along with model checking for a property, a new test sequence is derived from the counterexample and is used to detect redundant properties, and then is winnowed by the test-tree as well. We demonstrate the approach by applying some small examples to our prototyped algorithm.","","0-7695-2856-2978-0-7695-2856","10.1109/TASE.2007.46","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4239978","","Software testing;Automatic testing;Laboratories;Software engineering;Automation;Formal specifications;Costs;Programming;Application software;Time factors","automatic test pattern generation;formal specification;program testing;redundancy","specification-based test generation;model checking;automated test generation;redundancy;test-tree","","8","20","","","","","","IEEE","IEEE Conferences"
"Test Case Reduction Technique for BPEL-based Testing","W. Dong","NA","2008 International Symposium on Electronic Commerce and Security","","2008","","","814","817","Testing is necessary for evaluating the functional correctness, performance and reliability. The BPEL-based Web service composition having a large number of input parameters and the assigned values of individual parameters, the number of corresponding combination of input parameters is huge. A method needs to be established in order to limit the number of test cases and reduce space- and time-consuming. A reduction technique based on pair-wise combination of input parameters is presented in this paper. Relationship between BPEL (Business Process Execution Language) concepts and HPN (high-level Petri nets) is analyzed. After translation, the equivalent HPN of the Web service composition based on BPEL can be verified on existing mature tool according to the steps to reduce the size of test suite discussed in this paper. The method base on above technique can efficiently limit the number of test cases and reduce space- and time-consuming in theory and in practice.","","978-0-7695-3258","10.1109/ISECS.2008.194","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4606181","Business Process Execution Language for Web Services;Reduction Technique;Test Suite","Optimized production technology;Testing;Web services;Construction industry;Software testing;Software;Analytical models","business data processing;Petri nets;program testing;specification languages;Web services","test case reduction;BPEL-based testing;functional correctness;Web service composition;Business Process Execution Language;high-level Petri nets","","5","12","","","","","","IEEE","IEEE Conferences"
"A Task Scheduling Algorithm of Single Processor Parallel Test System","J. Zhuo; C. Meng; M. Zou","College of Ordnance Engineering, China; College of Ordnance Engineering, China; Army of 65185, China","Eighth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing (SNPD 2007)","","2007","1","","627","632","The purpose of this paper is to implement parallel test in the single processor auto test system and to improve the test efficiency with a lower test cost. The main factor that impacts the test efficiency of test system is the performance of the parallel task scheduling algorithm. This paper puts forward a heuristic parallel task scheduling algorithm: scheduling-Q which can meet the characteristics of the auto test system. Every test tasks uses some resources to put test the units under test. So, we can use the multi-threading technique to implement single processor parallel test. In test system some test tasks can be executed with different resource allocations. The task scheduling algorithm: scheduling-Q adapts well to this characteristic. It schedules the test tasks according to the task's earliest starting time and the test generalized resource loading. The generalized resource loading is embodied as task resources set loading based on resources allocation mode and task resources set loading based on task's starting time. The test resources with bigger loading have more opportunities to obtain task and are always in a busy state. Thus resources loadings can be balanced to a degree. So the parallel performance of test system can be improved with the algorithm. In addition, the algorithm adopts the strategy of heuristic local optimum search. The time complexity of the algorithm is decreased obviously.","","0-7695-2909-7978-0-7695-2909","10.1109/SNPD.2007.383","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4287583","Auto test system(ATS);Parallel test;Multi-threading technique;Task scheduling algorithm;Algorithm time complexity.","Scheduling algorithm;System testing;Software testing;Processor scheduling;Resource management;Parallel processing;Costs;Computer architecture;Switches;Software engineering","automatic testing;computational complexity;multi-threading;optimisation;parallel processing;processor scheduling;resource allocation;search problems;task analysis","parallel testing;single processor auto test system;heuristic parallel task scheduling algorithm;scheduling-Q algorithm;multithreading technique;generalized resource loading;resources allocation mode;task resources set loading;heuristic local optimum search;time complexity","","1","12","","","","","","IEEE","IEEE Conferences"
"Effective RTL Method to Develop On-Line Self-Test Routine for the Processors Using the Wavelet Transform","A. Asghari; S. A. Motamedi; S. Attarchi","NA; NA; NA","Seventh IEEE/ACIS International Conference on Computer and Information Science (icis 2008)","","2008","","","33","38","In this paper, we introduce a new efficient register transfer level (RTL) method to develop on-line self- test routines. We consider some prioritizations to select the components and instructions of the processor. In addition, we choose test patterns based on spectral RTL test pattern generation (TPG) strategy. For the purpose of spectral analysis, we use the wavelet transform. Also, we use a few extra instructions for the purpose of the signature monitoring to detect control flow errors. We demonstrate that the combination of these three strategies is effective for developing small test programs with high fault coverage in a small test development time. In this case, we only need the instruction set architecture (ISA) and RTL information. Our method not only provides a simple and fast algorithm for on-line self-test applications, also gains the advantages of utilizing lower memory and reducing the test generation time complexities in comparison with proposed methods so far. We focus on the application of this approach for Parwan processor. We develop a self-test routine using our proposed method for Parwan processor and demonstrate the effectiveness of our proposed methodology for on-line testing by presenting experimental results for Parwan processor.","","978-0-7695-3131","10.1109/ICIS.2008.56","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4529795","Processor testing;On-line low-cost testing;Spectral test pattern generating;Software-based self-testing (SBST);Register transfer level (RTL)","Built-in self-test;Wavelet transforms;Automatic testing;Registers;Test pattern generators;Spectral analysis;Wavelet analysis;Monitoring;Error correction;Instruction sets","automatic test pattern generation;circuit complexity;error detection;fault diagnosis;instruction sets;integrated circuit testing;microprocessor chips;wavelet transforms","register transfer level method;Parwan processor online self-test routine;spectral RTL test pattern generation strategy;wavelet transform;signature monitoring;control flow error detection;fault coverage;instruction set architecture;test generation time complexity","","","14","","","","","","IEEE","IEEE Conferences"
"Variable Strength Interaction Testing with an Ant Colony System Approach","X. Chen; Q. Gu; A. Li; D. Chen","NA; NA; NA; NA","2009 16th Asia-Pacific Software Engineering Conference","","2009","","","160","167","Interaction testing (also called combinatorial testing) is an cost-effective test generation technique in software testing. Most research work focuses on finding effective approaches to build optimal t-way interaction test suites. However, the strength of different factor sets may not be consistent due to the practical test requirements. To solve this problem, a variable strength combinatorial object and several approaches based on it have been proposed. These approaches include simulated annealing (SA) and greedy algorithms. SA starts with a large randomly generated test suite and then uses a binary search process to find the optimal solution. Although this approach often generates the minimal test suites, it is time consuming. Greedy algorithms avoid this shortcoming but the size of generated test suites is usually not as small as SA. In this paper, we propose a novel approach to generate variable strength interaction test suites (VSITs). In our approach, we adopt a one-test-at-a-time strategy to build final test suites. To generate a single test, we adopt ant colony system (ACS) strategy, an effective variant of ant colony optimization (ACO). In order to successfully adopt ACS, we formulize the solution space, the cost function and several heuristic settings in this framework. We also apply our approach to some typical inputs. Experimental results show the effectiveness of our approach especially compared to greedy algorithms and several existing tools.","1530-1362;1530-1362","978-0-7695-3909","10.1109/APSEC.2009.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5358586","variable strength interaction testing;ant colony system","System testing;Software testing;Greedy algorithms;Software engineering;Simulated annealing;Cost function;Laboratories;Computer science;Ant colony optimization;Programming","greedy algorithms;program testing;search problems;simulated annealing","variable strength interaction testing;ant colony system approach;software testing;cost-effective test generation technique;simulated annealing;greedy algorithms;binary search process;cost function","","20","25","","","","","","IEEE","IEEE Conferences"
"Systematic Testing of Model-Based Code Generators","I. Stuermer; M. Conrad; H. Doerr; P. Pepper","NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2007","33","9","622","634","Unlike for conventional compilers for imperative programming languages such as C or ADA, no established methods for safeguarding artifacts generated by model-based code generators exist despite progress in the field of formal verification. Several test approaches dominate the engineering practice. This paper describes a general and tool-independent test architecture for code generators used in model-based development. We evaluate the effectiveness of our test approach by means of testing optimizations performed by the TargetLink code generator, a widely accepted and complex development tool used in automotive model-based development.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70708","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4288195","Testing and Debugging","System testing;Automatic testing;Program processors;Computer languages;Automotive engineering;Performance evaluation;Formal verification;Computer architecture;Debugging","program compilers;program testing;program verification;software tools","systematic testing;model-based code generators;imperative programming languages;formal verification;tool-independent test architecture;optimization testing;TargetLink code generator;software tools;automotive model-based development","","23","61","","","","","","IEEE","IEEE Journals & Magazines"
"Repository based Infrastructures for effective Automotive Software Creation - an Experience Report","O. Kath; M. Born; M. Soden","ikv++ technologies ag, Germany; ikv++ technologies ag, Germany; ikv++ technologies ag, Germany","31st Annual International Computer Software and Applications Conference (COMPSAC 2007)","","2007","2","","642","650","The development processes for software based automobile functions is getting more and more complex within OEM and supplier organizations. Safety regulations in many target markets require the stakeholders to integrate the development processes with multifaceted orthogonal analysis and design procedures. The main problems to overcome are that development models, documents and artifacts getting out of consistency very soon in the traditional processes, various tools are used in the development, but their integration is poor, automation doesn't exist between the development process phases, concurrent work on specifications and models is practically impossible and the analysis of changes impact means manual, costly review of whole specification documents, ikv is working on software development process optimization and automation for some years now. We identified repository based tool integration infrastructures, fine grained version control, consistency control and impact analysis as well as automation to be the main ingredients to overcome the mentioned drawbacks. In this position paper, we elaborate more on these mechanisms and explain technical solutions and experiences from their application.","0730-3157","0-7695-2870","10.1109/COMPSAC.2007.179","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4291190","","Automotive engineering;Automation;Programming;Automatic control;Safety;Application software;Software testing;Software maintenance;Automobiles;Computer industry","automobile industry;production engineering computing;software engineering;supply chains","repository based infrastructures;automotive software creation;software based automobile functions;OEM;supplier organizations;safety regulations;target markets;stakeholders;multifaceted orthogonal analysis;concurrent work;specification documents;software development process optimization","","1","8","","","","","","IEEE","IEEE Conferences"
"An Automatic Framework for Efficient Software Performance Evaluation and Optimization","C. Hsu; M. Devetsikiotis","North Carolina State University, USA; North Carolina State University, USA","40th Annual Simulation Symposium (ANSS'07)","","2007","","","99","105","Performance evaluation has been an important part of software development. Fast and accurate software performance evaluation can help not only in understanding the behavior of software during development, but also in capacity planning during deployment. In this paper, we propose an automatic framework that can be used to optimize software performance indicators. For optimization, we use response surface methodology (RSM) for its simplicity and its ability to describe the behavior of a system in a whole neighborhood. To efficiently obtain the response data for different parameter settings, a reusable importance sampling (IS) simulation and testing method can be applied. We describe here this novel method based on a combined response surface and importance sampling (RS-IS) framework and we illustrate its usefulness via simulated examples that minimize total cost in a capacity planning problem","1080-241X","0-7695-2814","10.1109/ANSS.2007.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4127207","","Software performance;Capacity planning;Response surface methodology;Monte Carlo methods;Hardware;Software testing;Cost function;Table lookup;Programming;Optimization methods","software development management;software performance evaluation","software performance evaluation;software performance optimization;software development;capacity planning;response surface methodology;importance sampling simulation;importance sampling testing","","","10","","","","","","IEEE","IEEE Conferences"
"The Realization and Optimization of Secure Hash Algorithm (SHA-1) Based on LEON2 Coprocessor","X. Hong; N. Hui-ming; Y. Jiang-yu","NA; NA; NA","2008 International Conference on Computer Science and Software Engineering","","2008","3","","853","858","This paper describes the design flow of Secure Hash Algorithm (SHA-1) which based on LEON2 coprocessor, and provides a series of optimized design proposals. First, modify the interface of the coprocessor to increase the bandwidth; second, make use of the principle of data-driven to expand the instruction set; next, through optimizing the critical path design, dynamic generating the variables Wt (0lestles79) and optimizing the output to increase the speed, decrease the scale and lower the dynamic power; finally, modify the software package binutils-2.16.1 to support the expanded assembler instructions. This design is described in VHDL, and has passed Xilinx Virtex-5 FPGA testing verifies. Because of the characteristics of low cost and high performance, this design is available for various kinds of security domains.","","978-0-7695-3336","10.1109/CSSE.2008.358","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722477","SHA-1;LEON2 Coprocessor;FPGA;SPARC V8","Coprocessors;Design optimization;Algorithm design and analysis;Proposals;Bandwidth;Power generation;Software packages;Assembly;Field programmable gate arrays;Testing","coprocessors;cryptography;instruction sets;program assemblers;software packages","secure hash algorithm;LEON2 coprocessor;instruction set;software package;assembler instructions;VHDL;Xilinx Virtex-5 FPGA testing;security domains","","1","8","","","","","","IEEE","IEEE Conferences"
"Low Energy On-Line SBST of Embedded Processors","A. Merentitis; N. Kranitis; A. Paschalis; D. Gizopoulos","Department of Informatics & Telecommunications, University of Athens, Greece. amer@di.uoa.gr; Department of Informatics & Telecommunications, University of Athens, Greece. nkran@di.uoa.gr; Department of Informatics & Telecommunications, University of Athens, Greece. paschali@di.uoa.gr; Department of Informatics, University of Piraeus, Greece. dgizop@unipi.gr","2008 IEEE International Test Conference","","2008","","","1","10","Software-based self-test (SBST) has emerged as an effective strategy for on-line testing of processors integrated in non-safety critical applications. Among the various systems that fall in the previous category, wireless sensor networks (WSN) are often deployed in harsh environments where the possibility of permanent and especially intermittent faults due to environmental hazards is significantly increased, thus on-line and in-field testing is necessary to guarantee the accuracy of the sensed values. At the same time, on-line testing of processors integrated in WSN has the extra requirement of minimum energy consumption, because these devices are operating on battery, cannot be connected to any external power supply, and the battery duration determines the lifetime of the system. In this paper we present a methodology for the optimization of SBST routines from the energy perspective. Techniques utilized for achieving energy reduction include energy aware loop synthesis, loop transformations, instruction substitution and register renaming. Simulation results show that the energy savings at processor level are up to 35.6%.","1089-3539;2378-2250","978-1-4244-2402-3978-1-4244-2403","10.1109/TEST.2008.4700581","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700581","","Wireless sensor networks;System testing;Batteries;Built-in self-test;Automatic testing;Software testing;Application software;Hazards;Life testing;Energy consumption","automatic testing;wireless sensor networks","low energy online SBST;embedded processors;software based self test;effective strategy;online testing;wireless sensor networks;in field testing","","3","30","","","","","","IEEE","IEEE Conferences"
"Low Dimensional Simplex Evolution--A Hybrid Heuristic for Global Optimization","C. Luo; B. Yu","Jilin University, China; Dalian University of Technology, China","Eighth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing (SNPD 2007)","","2007","2","","470","474","In this paper, a new real-coded evolutionary algorithm - low dimensional simplex evolution (LDSE) for global optimization is proposed. It is a hybridization of two well known heuristics, the differential evolution (DE) and the Nelder-Mead method. LDSE takes the idea of DE to randomly select parents from the population and perform some operations with them to generate new individuals. Instead of using the evolutionary operators of DE such as mutation and cross-over, we introduce operators based on the simplex method, which makes the algorithm more systematic and parameter-free. The proposed algorithm is very easy to implement, and its efficiency has been studied on an extensive testbed of 50 test problems from M.M. Ali et al. Numerical results show that the new algorithm outperforms DE in terms of number of function evaluations (nfe) and percentage of success (ps).","","0-7695-2909-7978-0-7695-2909","10.1109/SNPD.2007.58","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4287730","global optimization;real-coded;evolutionary;algorithm;differential evolution;low dimensional simplex evolution","Evolutionary computation;Testing;Convergence;Mathematics;Stochastic processes;Genetic programming;Design optimization;Software engineering;Artificial intelligence;Distributed computing","evolutionary computation;mathematical operators;optimisation","low dimensional simplex evolution;global optimization;percentage of success;number of function evaluations;differential evolution method;Nelder-Mead method;evolutionary operators;mutation operator;cross-over operator","","5","10","","","","","","IEEE","IEEE Conferences"
"Predicting Attack-prone Components","M. Gegick; P. Rotella; L. Williams","NA; NA; NA","2009 International Conference on Software Testing Verification and Validation","","2009","","","181","190","Limited resources preclude software engineers from finding and fixing all vulnerabilities in a software system. This limitation necessitates security risk management where security efforts are prioritized to the highest risk vulnerabilities that cause the most damage to the end user. We created a predictive model that identifies the software components that pose the highest security risk in order to prioritize security fortification efforts. The input variables to our model are available early in the software life cycle and include security-related static analysis tool warnings, code churn and size, and faults identified by manual inspections. These metrics are validated against vulnerabilities reported by testing and those found in the field. We evaluated our model on a large Cisco software system and found that 75.6% of the system's vulnerable components are in the top 18.6% of the components predicted to be vulnerable. The model's false positive rate is 47.4% of this top 18.6% or 9.1% of the total system components. We quantified the goodness of fit of our model to the Cisco data set using a receiver operating characteristic curve that shows 94.4% of the area is under the curve.","2159-4848","978-1-4244-3775-7978-0-7695-3601","10.1109/ICST.2009.36","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4815350","Security;metric;predict;attack-prone;classification and regression tree","Predictive models;Software systems;Data security;Reliability engineering;Software testing;Input variables;Inspection;Costs;System testing;Risk management","inspection;object-oriented programming;program diagnostics;program testing;security of data;software metrics;software tools","attack-prone component prediction;software engineering;vulnerability fixing;security risk management;predictive model;software life cycle;static analysis tool warnings;manual inspections;metrics;Cisco software system","","14","33","","","","","","IEEE","IEEE Conferences"
"Software integration and verification process and quality improvements on CPP system","Z. Antolic; S. Golubic","R&D Center, Ericsson Nikola Tesla d.d., Zagreb, Croatia; NA","MELECON 2006 - 2006 IEEE Mediterranean Electrotechnical Conference","","2006","","","769","772","This paper gives an overview of few initiatives regarding process and quality improvements taken within software development projects for CPP (connectivity packet platform) system in order to minimize development time, reduce development cost and improve the software product quality. Some changes in development and verification process toward integration centric engineering are described and discusses, as well as software quality rank model, and fault-slip-through measurements. In addition, some proposals and future directions in the area of software integration and verification are given","2158-8473;2158-8481","1-4244-0087","10.1109/MELCON.2006.1653212","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1653212","","Software quality;Costs;Programming;System testing;Software measurement;Software testing;Computer industry;Packaging;Research and development;Density measurement","software development management;software quality","software integration;verification process;software development projects;connectivity packet platform;software product quality;software quality rank model;fault-slip-through measurements","","","8","","","","","","IEEE","IEEE Conferences"
"Deterministic Models of Software Aging and Optimal Rejuvenation Schedules","A. Andrzejak; L. Silva","Zuse Institute Berlin (ZIB), Takustraße 7, 14195 Berlin, Germany. Email: andrzejak@zib.de; Dep. Engenharia Informática, Univ. Coimbra, Portugal. Email: luis@dei.uc.pt","2007 10th IFIP/IEEE International Symposium on Integrated Network Management","","2007","","","159","168","Automated modeling of software aging processes is a prerequisite for cost-effective usage of adaptive software rejuvenation as a self-healing technique. We consider the problem of such automated modeling in server-type applications whose performance degrades depending on the ""work"" done since last rejuvenation, for example the number of served requests. This type of performance degradation - caused mostly by resource depletion - is common, as we illustrate in a study of the popular Axis Soap server 1.3. In particular, we propose deterministic models for approximating the leading indicators of aging and an automated procedure for statistical testing of their correctness. We further demonstrate how to use these models for finding optimal rejuvenation schedules under utility functions. Our focus is on the important case that the utility function is the average of a performance metric (such as maximum service rate). We also consider optional SLA constraints under which the performance should never drop below a specified level. Our approach is verified by a study of the aging processes in the Axis Soap 1.3 server. The experiments show that the deterministic modeling technique is appropriate in this case, and that the optimization of rejuvenation schedules can greatly improve the average maximum service rate of an aging application.","1573-0077","1-4244-0798-21-4244-0799","10.1109/INM.2007.374780","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4258532","","Aging;Application software;Degradation;Software performance;Yarn;Operating systems;Service oriented architecture;Space vehicles;Military communication;Production systems","program testing;scheduling;software fault tolerance;software maintenance;software metrics;software performance evaluation;statistical testing;utility programs","software aging deterministic model;optimal adaptive software rejuvenation schedule;cost-effective usage;self-healing technique;server-type application;resource depletion;utility function;software performance metric;SLA constraint;software reliability;statistical software correctness testing","","21","26","","","","","","IEEE","IEEE Conferences"
"Prioritizing Use Cases to Aid Ordering of Scenarios","S. P.G.; H. Mohanty","NA; NA","2009 Third UKSim European Symposium on Computer Modeling and Simulation","","2009","","","136","141","Models are used as the basis for design and testing of software. The unified modeling language (UML) is used to capture and model the requirements of a software system. One of the major requirements of a development process is to detect defects as early as possible. Effective prioritization of scenarios helps in early detection of defects as well maximize effort and utilization of resources. Use case diagrams are used to represent the requirements of a software system. In this paper, we propose using data captured from the primitives of the use case diagrams to aid in prioritization of scenarios generated from activity diagrams. Interactions among the primitives in the diagrams are used to guide prioritization. Customer prioritization of use cases is taken as one of the factors. Preliminary results on a case study indicate that the technique is effective in prioritization of test scenarios.","","978-1-4244-5345-0978-0-7695-3886","10.1109/EMS.2009.78","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5358815","UML;use case diagram;activity diagram;scenarios;prioritization","Software testing;Unified modeling language;Software systems;Fault detection;Computational modeling;Computer simulation;Costs;Performance evaluation;Visualization;Machine learning algorithms","program testing;Unified Modeling Language","use case diagram;software design;software testing;unified modeling language;customer prioritization","","1","15","","","","","","IEEE","IEEE Conferences"
"Interpreting a Successful Testing Process: Risk and Actual Coverage","M. Stoelinga; M. Timmer","NA; NA","2009 Third IEEE International Symposium on Theoretical Aspects of Software Engineering","","2009","","","251","258","Testing is inherently incomplete; no test suite will ever be able to test all possible usage scenarios of a system. It is therefore vital to assess the implication of a system passing a test suite. This paper quantifies that implication by means of two distinct, but related, measures: the risk quantifies the confidence in a system after it passes a test suite, i.e., the number of faults still expected to be present (weighted by their severity); the actual coverage quantifies the extent to which faults have been shown absent, i.e., the fraction of possible faults that has been covered. We provide evaluation algorithms that calculate these metrics for a given test suite, as well as optimisation algorithms that yield the best test suite for a given optimisation criterion.","","978-0-7695-3757","10.1109/TASE.2009.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5198509","formal testing;probabilistic;coverage;risk","System testing;Software testing;Costs;Software engineering;Computer science;Solid modeling;Mathematical model;Error probability;Random variables;Risk management","program testing;risk analysis;software reliability","testing process;test suite;risk measures;actual coverage measures;faults fraction;optimisation algorithms","","2","21","","","","","","IEEE","IEEE Conferences"
"Scheduling in Performance Test environment","F. Bozoki; T. Csondes","Ericsson Hungary Ltd., Budapest Hungary; Ericsson Hungary Ltd., Budapest Hungary","2008 16th International Conference on Software, Telecommunications and Computer Networks","","2008","","","404","408","Nowadays automatic testing is getting more and more important in the telecommunication world. The sooner a fault is discovered the cheaper it is to correct it. If a fault is discovered during the development process the cost of the correction is significantly smaller. There are different test strategies, with different approaches like, conformance test, system test and performance test. The system test takes place after a successful conformance test. Performance test is analyzing the load characteristics of the system under test (SUT). In this article we describe the main attributes of performance testing, where the main challenge is to generate the expected load without having as complex hardware as the SUT is itself. Most of the papers, presented in this subject are focusing on the characteristics of the generated load, but not the way how to achieve it. These papers usually have the assumption that the load can be generated by deploying more hardware resources. Other papers propose new extensions for test description languages such SDL or TTCN-3. In this article we intend to describe a finite state machine (FSM) based model and an algorithm which improves the efficiency of scheduling in this performance test environment. We present an architecture based on the so called virtual threads, an algorithm to optimize the scheduling between these threads, and an example to demonstrate the algorithm.","","978-953-6114-97-9978-953-290-009","10.1109/SOFTCOM.2008.4669519","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4669519","","System testing;Yarn;Hardware;Scheduling algorithm;Automata;Operating systems;Automatic testing;Costs;Performance analysis;Character generation","finite state machines;multi-threading;program testing;scheduling","performance test environment;automatic testing;telecommunication world;fault discovery;development process;test strategy;conformance test;load test;system under test;SUT;hardware resource;test description language;SDL;TTCN-3;finite state machine based model;virtual thread;scheduling optimization algorithm","","1","8","","","","","","IEEE","IEEE Conferences"
"Software Defect Prediction Using Call Graph Based Ranking (CGBR) Framework","B. Turhan; G. Kocak; A. Bener","NA; NA; NA","2008 34th Euromicro Conference Software Engineering and Advanced Applications","","2008","","","191","198","Recent research on static code attribute (SCA) based defect prediction suggests that a performance ceiling has been achieved and this barrier can be exceeded by increasing the information content in data. In this research we propose static call graph based ranking (CGBR) framework, which can be applied to any defect prediction model based on SCA. In this framework, we model both intra module properties and inter module relations. Our results show that defect predictors using CGBR framework can detect the same number of defective modules, while yielding significantly lower false alarm rates. On industrial public data, we also show that using CGBR framework can improve testing efforts by 23%.","1089-6503;2376-9505","978-0-7695-3276","10.1109/SEAA.2008.52","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4725722","defect prediction;call graph;cost-benefit analysis","Predictive models;Costs;Software maintenance;Humans;Software measurement;Software quality;Software testing;Software engineering;Application software;Data engineering","graph theory;software quality","software defect prediction;call graph based ranking framework;static code attribute;performance ceiling;SCA;intra module properties;inter module relations","","12","30","","","","","","IEEE","IEEE Conferences"
"Diagnosing Silicon Failures Based on Functional Test Patterns","C. Yen; T. Lin; H. Lin; K. Yang; T. Liu; Y. Hsu","Springsoft, Inc., Taiwan; Springsoft, Inc., Taiwan; Springsoft, Inc., Taiwan; Novas Software, Inc., USA; Novas Software, Inc., USA; Novas Software, Inc., USA","Seventh International Workshop on Microprocessor Test and Verification (MTV'06)","","2006","","","94","98","Identifying the root-cause of silicon failures is crucial for silicon debug and yield improvement. However, due to the low visibility of silicon data, root-cause identification tends to be a painful process. In this paper, we develop a systematic framework to diagnose silicon failures under functional test patterns. We propose a novel scan-dump approach to isolate critical cycles. Within the critical cycles, we apply logic-reasoning techniques including active-path-tracing (AP) and what-if (WI) analysis to automatically extract and rank failure candidates. We apply our framework on an industrial circuit and demonstrate the promising results.","1550-4093;2332-5674","0-7695-2839","10.1109/MTV.2006.9","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4197228","Silicon debug;design for debug;fault diagnosis","Silicon;Design for disassembly;Circuit testing;Clocks;Failure analysis;Fault diagnosis;Data engineering;Design engineering;Software testing;Software performance","computer debugging;reasoning about programs;silicon;system recovery","silicon failures diagnosis;functional test patterns;silicon debug;scan-dump approach;logic reasoning;active-path-tracing;what-if analysis","","9","7","","","","","","IEEE","IEEE Conferences"
"Predicting Defects and Changes with Import Relations","A. Schroter","Saarland University, Germany","Fourth International Workshop on Mining Software Repositories (MSR'07:ICSE Workshops 2007)","","2007","","","31","31","Lowering the number of defects and estimating the development time of a software project are two important goals of software engineering. To predict the number of defects and changes we train models with import relations. This enables us to decrease the number of defects by more efficient testing and to assess the effort needed in respect to the number of changes.","2160-1852;2160-1860","0-7695-2950","10.1109/MSR.2007.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4228668","","Predictive models;Java;Software engineering;Support vector machines;Testing;Project management;History;Software libraries;Computer languages;Optimized production technology","configuration management;Java;object-oriented programming;program debugging;program testing;software fault tolerance","defect prediction;change prediction;software project development time estimation;software engineering;import relation;program testing;Java","","","3","","","","","","IEEE","IEEE Conferences"
"Optimizing Test Data Volume Using Hybrid Compression","B. Keller; S. Bhatia; T. Bartenstein; B. Foutz; A. Uzzaman","NA; NA; NA; NA; NA","2008 17th Asian Test Symposium","","2008","","","157","162","This paper describes a simple means to enable direct diagnosis by bypassing MISRs on a small set of tests while achieving ultimate output compression using MISRs for the majority of tests. By combining two compression schemes, XOR and MISRs in the same device, it becomes possible to have high compression and still support volume diagnostics.","1081-7735;2377-5386","978-0-7695-3396","10.1109/ATS.2008.78","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4711576","Test Data Compression;OPMISR+;XOR","Circuit testing;System testing;Software testing;Performance evaluation;Logic testing;Automatic test pattern generation;Design optimization;Error analysis;Costs;Digital circuits","data compression;logic testing;system-on-chip","test data volume;hybrid compression;MISR;multiple input signature register;XOR;system-on-chip","","","12","","","","","","IEEE","IEEE Conferences"
"Signal Generation for Search-Based Testing of Continuous Systems","A. Windisch; N. Al Moubayed","NA; NA","2009 International Conference on Software Testing, Verification, and Validation Workshops","","2009","","","121","130","Test case generation constitutes a critical activity in software testing that is cost-intensive, time-consuming and error-prone when done manually. Hence, an automation of this process is required. One automation approach is search-based testing for which the task of generating test data is transformed into an optimization problem which is solved using metaheuristic search techniques. However, only little work has been done so far applying search-based testing techniques to systems that depend on continuous input signals. This paper proposes two novel approaches to generating input signals from within search-based testing techniques for continuous systems. These approaches are then shown to be very effective when experimentally applied to the problem of approximating a set of realistic signals.","","978-0-7695-3671-2978-1-4244-4356","10.1109/ICSTW.2009.16","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4976377","","Signal generators;System testing;Continuous time systems;Software testing;Automatic testing;Automation;Automotive engineering;Genetic programming;Engines;Conferences","continuous systems;program testing;search problems","signal generation;search-based testing;continuous systems;test case generation;software testing;automation approach;optimization problem;metaheuristic search techniques","","5","16","","","","","","IEEE","IEEE Conferences"
"Characterizing the Relative Significance of a Test Smell","B. V. Rompaey; B. D. Bois; S. Demeyer","University Of Antwerp; University Of Antwerp; University Of Antwerp","2006 22nd IEEE International Conference on Software Maintenance","","2006","","","391","400","Test code, just like any other code we write, erodes when frequently changed. As such, refactoring, which has been shown to impact maintainability and comprehensibility, can be part of a solution to counter this erosion. We propose a metric-based heuristical approach, which allows to rank occurrences of so-called test smells (i.e. symptoms of poorly designed tests) according to their relative significance. This ranking can subsequently be used to start refactoring. Through an open-source case study, ArgoUML, we demonstrate that we are able to identify those test cases who violate unit test criteria","1063-6773","0-7695-2354","10.1109/ICSM.2006.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021366","","Software testing;System testing;Software maintenance;Costs;Counting circuits;Open source software;Application software;Computer bugs;Software systems;Measurement units","software maintenance;software metrics;Unified Modeling Language","test smell relative significance;test code;code refactoring;code maintainability;code comprehensibility;metric-based heuristical approach;poorly designed tests;open-source case study;ArgoUML","","4","20","","","","","","IEEE","IEEE Conferences"
"The Keystone to Support a Generic Test Process:  Separating the ""What"" from the ""How""","M. Smith; N. Thompson","NA; NA","2008 IEEE International Conference on Software Testing Verification and Validation Workshop","","2008","","","342","353","A generic test process should support all approaches ""formal to informal"" and mixed approaches within, and across, all test levels. But widely-accepted formal approaches, from current standards, certification syllabi and tools, have explicitly-defined test cases as the key underlying entity: not all real-life approaches use explicit test cases. Also, testing is maturing beyond merely ""finding bugs"" we need better control, informing stakeholders in a language they understand. So, we separate the ""what"" from the ""how"" in the test process, using a structure which accommodates existing views yet provides flexibility to support varied approaches. We consider test analysis and test design as distinct activities with differently-structured work products with complex inter-relationships. We question the current scope and usage of test design techniques. Rather, test analysis produces test conditions which form the ""keystone"" for a generic test process supporting optimised test design and execution, plus measurement, monitoring and control of test and development processes.","","978-0-7695-3388","10.1109/ICSTW.2008.46","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4567031","","System testing;Software testing;Certification;Automatic testing;Documentation;Educational institutions;ISO standards;Computer bugs;Design optimization;Condition monitoring","program testing;systems analysis","generic test process;formal approaches;test analysis;test design;test conditions","","1","10","","","","","","IEEE","IEEE Conferences"
"A model to predict anti-regressive effort in Open Source Software","A. Capiluppi; J. Fernandez-Ramil","Centre of Research on Open Source Software Department of Computing and Informatics, University of Lincoln, UK, acapiluppi@lincoln.ac.uk; Computing Department, The Open University, UK, j.f.ramil@open.ac.uk","2007 IEEE International Conference on Software Maintenance","","2007","","","194","203","Accumulated changes on a software system are not uniformly distributed: some elements are changed more often than others. For optimal impact, the limited time and effort for complexity control, called anti-regressive work, should be applied to the elements of the system which are frequently changed and are complex. Based on this, we propose a maintenance guidance model (MGM) which is tested against real-world data. MGM takes into account several dimensions of complexity: size, structural complexity and coupling. Results show that maintainers of the eight open source systems studied tend, in general, to prioritize their anti-regressive work in line with the predictions given by our MGM, even though, divergences also exist. MGM offers a history-based alternative to existing approaches to the identification of elements for anti-regressive work, most of which use static code characteristics only.","1063-6773","978-1-4244-1255-6978-1-4244-1256","10.1109/ICSM.2007.4362632","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4362632","ANTI-REGRESSIVE WORK;COUPLING;EMPIRICAL STUDIES;MCCABE CYCLOMATIC COMPLEXITY;MAINTENANCE;METRICS;OPEN SOURCE;SoFTWARE EVOLUTION","Predictive models;Open source software;Software systems;Informatics;Optimal control;Control systems;Software testing;System testing;Documentation;Knowledge management","public domain software;software metrics","open source software;complexity control;antiregressive work;maintenance guidance model","","2","28","","","","","","IEEE","IEEE Conferences"
"Implementation of the Software Quality Ranks method in the legacy product development environment","L. Hribar; A. Burilovic; D. Huljenic","Ericsson Nikola Tesla R&D Center, Croatia; Ericsson Nikola Tesla R&D Center, Croatia; Ericsson Nikola Tesla R&D Center, Croatia","2009 10th International Conference on Telecommunications","","2009","","","141","145","Software quality ranks (SQR) is an important method to manage and improve software quality. Component software quality has a major influence in development project lead time and cost. SQR enables better management and visibility of the quality effort associated with the component implementation. It also provides a roadmap for continuous improvement leading to value add quality attributes like low maintenance, self optimizing software and short development lifecycles. SQR method focuses attention to prioritizing the quality investment on design component level through different quality assurance mechanisms (basic test, code review, desk checks, documentation and other actions). The resulting design delivery to verification phase will be more predictable quality software with shorter lead-time and time-to-market (TTM).","","978-953-184-130-6978-953-184-131","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5206363","","Software quality;Product development;Quality management;Software maintenance;Time to market;Costs;Continuous improvement;Investments;Quality assurance;Testing","quality assurance;software development management;software quality","software quality ranks method;legacy product development;component software quality;design component level;quality assurance mechanisms","","1","16","","","","","","IEEE","IEEE Conferences"
"Value-Oriented Requirements Prioritization in a Small Development Organization","J. Azar; R. K. Smith; D. Cordes","Orasi Software; University of Alabama; University of Alabama","IEEE Software","","2007","24","1","32","37","Requirements engineering, especially requirements prioritization and selection, plays a critical role in overall project development. In small companies, this often difficult process can affect not only project success but also overall company survivability. A value-oriented prioritization (VOP) framework can help this process by clarifying and quantifying the selection and prioritization issues. A case study of a small development company shows a successful VOP deployment that improved communications and saved time by focusing requirements decisions for new product releases on core company values","0740-7459;1937-4194","","10.1109/MS.2007.30","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4052549","requirements/specifications;software engineering;value-based software engineering","Marketing and sales;Project management;Costs;Aerospace testing;Software development management;Programming;Scheduling;Risk analysis;Process planning;Decision making","DP industry;formal specification;formal verification;software development management","value-oriented requirement prioritization;small development organization;requirement engineering;requirement selection;project development;value-based software engineering","","38","13","","","","","","IEEE","IEEE Journals & Magazines"
"Test Input Generation Using UML Sequence and State Machines Models","A. Bandyopadhyay; S. Ghosh","NA; NA","2009 International Conference on Software Testing Verification and Validation","","2009","","","121","130","We propose a novel testing approach that combines information from UML sequence models and state machine models. Current approaches that rely solely on sequence models do not consider the effects of the message path under test on the states of the participating objects. Dinh-Trong et al. proposed an approach to test input generation using information from class and sequence models.We extend their variable assignment graph (VAG) based approach to include information from state machine models. The extended VAG (EVAG) produces multiple execution paths representing the effects of the messages on the states of their target objects.We performed mutation analysis on the implementation of a video store system to demonstrate that our test inputs are more effective than those that cover only sequence diagram paths.","2159-4848","978-1-4244-3775-7978-0-7695-3601","10.1109/ICST.2009.23","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4815344","class models;model-based testing;sequence models;state machine models;test input generation","Unified modeling language;System testing;Software testing;Performance evaluation;Genetic mutations;Performance analysis;Computer science;Knowledge engineering;Collaborative work;Optimized production technology","finite state machines;program testing;Unified Modeling Language","test input generation;UML sequence models;state machines models;variable assignment graph;extended VAG;video store system","","9","14","","","","","","IEEE","IEEE Conferences"
"Research on an Optimized Method for Submarine Torpedo Attacking with Incomplete Information","S. Zhufeng; H. Wenbing; X. Mingyan; X. Changyou","NA; NA; NA; NA","2008 International Conference on Computer Science and Software Engineering","","2008","1","","552","556","The anti-submarine ability of the warship improved quickly as the increasing threat of the submarine. On the other hand, the submarine commander should make tactical decision quickly and accurately under high threat environment. This fact means that not only should be launch torpedo snugly, quickly and accurately, but also make this decision with incomplete target information. This paper takes a new approach to find the possibility distribution of the target with limited information. Based upon the results, particle swarm optimization method was applied to find the optimized torpedo advance angle. A test-bed was constructed to demonstrate the process of torpedo attacking and validation of the optimized method. Simulation results illustrated that the optimize torpedo advance angle exist and could cover a wide scope of target movement information error. Thus provides the submarine commander an optimized method to use torpedo in such a situation.","","978-0-7695-3336","10.1109/CSSE.2008.613","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721809","torpedo;submarine;incomplete information","Optimization methods;Underwater vehicles;Information analysis;Sonar detection;Computer science;Software engineering;Sun;Cities and towns;Particle swarm optimization;Testing","missiles;particle swarm optimisation;ships;underwater vehicles","submarine torpedo attacks;antisubmarine ability;warships;tactical decision;target information;particle swarm optimization method;optimized torpedo advance angle;target movement information error;submarine commander","","","3","","","","","","IEEE","IEEE Conferences"
"Visualizing the structure of field testing problems","B. Chan; Y. Zou; A. E. Hassan; A. Sinha","Dept. of Elec. and Comp. Engineering, Queen's University, Kingston, Ontario, Canada; Dept. of Elec. and Comp. Engineering, Queen's University, Kingston, Ontario, Canada; School of Computing, Queen's University, Kingston, Ontario, Canada; Handheld Software Research In Motion (REVI) Waterloo, Ontario, Canada","2009 IEEE International Conference on Software Maintenance","","2009","","","429","432","Field testing of a software application prior to general release is an important and essential quality assurance step. Field testing helps identify unforeseen problems. Extensive field testing leads to the reporting of a large number of problems which often overwhelm the allocated resources. Prior efforts focus primarily on studying the reported problems in isolation. We believe that a global view of the interdependencies between these problems will help in rapid understanding and resolution of reported problems. We present a visualization that highlights the commonalities between reported problems. The visualization helps developers identify two patterns that they can use to prioritize and focus their efforts. We demonstrate the applicability of our visualization through a case study on problems reported during field testing efforts for two releases of a large scale enterprise application.","1063-6773","978-1-4244-4897-5978-1-4244-4828","10.1109/ICSM.2009.5306297","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306297","","Visualization;Testing","data visualisation;software development management","field testing problem visualisation;large scale enterprise application;software application;quality assurance;unforeseen problem identification;problem resolution","","","9","","","","","","IEEE","IEEE Conferences"
"Fast Track: Supporting Unsafe Optimizations with Software Speculation","K. Kelsey; C. Zhang; C. Ding","University of Rochester, USA; University of Rochester, USA; University of Rochester, USA","16th International Conference on Parallel Architecture and Compilation Techniques (PACT 2007)","","2007","","","414","414","The shift in processor technology toward multi-core, multi-processors opens new opportunities for software speculation where program code is speculatively executed to improve speed at the cost of having handle errors. In this paper we describe a new use of software speculation in a system we call fast track in which we can use some unsafely optimized code while still ensuring correctness.","1089-795X","0-7695-2944-5978-0-7695-2944","10.1109/PACT.2007.4336242","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4336242","","Optimizing compilers;Runtime;Program processors;Master-slave;Testing;Kirk field collapse effect;Computer science;Cost function;Computer errors;Programming profession","optimising compilers;parallel programming","unsafe optimization;software speculation;processor technology;program code speculative execution;fast track system;unsafely optimized code","","","6","","","","","","IEEE","IEEE Conferences"
"Research on implementation of OFDM burst packet transmission on software radio platform of FPGA","Liu Liu; Tao Cheng; Qi Xiaoyu; Qiu Jiahui","School of Electronics and Information Engineering, Beijing JiaoTong University, China; School of Electronics and Information Engineering, Beijing JiaoTong University, China; School of Electronics and Information Engineering, Beijing JiaoTong University, China; School of Electronics and Information Engineering, Beijing JiaoTong University, China","2009 11th International Conference on Advanced Communication Technology","","2009","01","","646","650","Fast verification of the prototype design on software radio platform has become one of the most important research topics in communication systems. Based on OFDM receiver which is implemented on FPGA, the paper focuses on many key issues in the implementation of the strategy in software wireless platform such as OFDM frame synchronization, frequency compensation, symbol timing, etc. The proposed corresponding optimizations of these algorithms significantly reduced the usage of hardware resources while ensuring the receiver's highest frequency as 50 MHz. According to the theoretical analysis and the test result on the platform, the receiver can achieve 0 PER (packet error ratio) at 30 MHz IF after the modification and optimization of the algorithms.","1738-9445","978-89-5519-138-7978-89-5519-139","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4810034","Orthogonal Frequency Division Multiplexing (OFDM);FPGA;Optimization of Algorithm","OFDM;Software radio;Field programmable gate arrays;Software prototyping;Software design;Communication systems;Receivers;Frequency synchronization;Timing;Hardware","field programmable gate arrays;OFDM modulation;optimisation;radio receivers;software radio;synchronisation","OFDM transmission;burst packet transmission;software radio;FPGA;communication systems;OFDM receiver;synchronization;frequency compensation;symbol timing;algorithm optimization;packet error ratio;frequency 50 MHz","","","6","","","","","","IEEE","IEEE Conferences"
"Generating Method of Pair-Wise Covering Test Data Based on ACO","K. Li; Z. Yang","NA; NA","2008 International Workshop on Education Technology and Training & 2008 International Workshop on Geoscience and Remote Sensing","","2008","2","","776","779","Optimizing test suite can reduce the cost of time and resources, and improve the efficiency of regression test when test cases are generated. The generation of pair-wise covering test data is an NP question, which can be solved by heuristic method, greedy arithmetic and algebra method at present. In this paper, ant colony arithmetic is adopted, which is a new way to solve the pair-wise test data generating question. It can generate fewer test cases which can cover more pair combinations,and can solve questions with fast calculate speed. The method can get the goal of optimizing in the process of regression test. The result shows that the method is feasible.","","978-0-7695-3563","10.1109/ETTandGRS.2008.358","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5070476","","Ant colony optimization;Cost function;Arithmetic;Software testing;Management training;Algebra;Hardware;Computer science education;Educational technology;Geoscience and remote sensing","computational complexity;optimisation;program testing;regression analysis","pairwise covering test data;regression test;test cases;NP question;heuristic method;greedy arithmetic;algebra method;ant colony arithmetic","","2","9","","","","","","IEEE","IEEE Conferences"
"Accelerating the performance of particle swarm optimization for embedded applications","G. S. Tewolde; D. M. Hanna; R. E. Haskell","Department of Electrical and Computer Engineering, Kettering University, Flint, MI 48504 USA; Electrical & Computer Engineering Dept., Oakland University, Rochester, MI 48309, USA; Electrical & Computer Engineering Dept., Oakland University, Rochester, MI 48309, USA","2009 IEEE Congress on Evolutionary Computation","","2009","","","2294","2300","The ever increasing popularity of particle swarm optimization (PSO) algorithm is recently attracting attention to the embedded computing world. Although PSO is in general considered to be computationally efficient algorithm, its direct software implementation on complex problems, targeted on low capacity embedded processors could however suffer from poor execution performance. This paper first evaluates the performance of the standard PSO algorithm on a typical embedded platform (using a 16-bit microcontroller). Then, a modular, flexible and reusable architecture for a hardware PSO engine, for accelerating the algorithm's performance, will be presented. Finally, implementation test results of the proposed architecture targeted on Field Programmable Gate Array (FPGA) technology will be presented and its performance compared against software executions on benchmark test functions.","1089-778X;1941-0026","978-1-4244-2958-5978-1-4244-2959","10.1109/CEC.2009.4983226","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4983226","","Acceleration;Particle swarm optimization;Embedded computing;Software performance;Computer architecture;Software testing;Field programmable gate arrays;Application software;Software algorithms;Embedded software","embedded systems;field programmable gate arrays;particle swarm optimisation;software architecture;software performance evaluation","particle swarm optimization;embedded computing;software implementation;embedded processors;architecture;field programmable gate array;FPGA;benchmark test functions","","9","33","","","","","","IEEE","IEEE Conferences"
"Implementation of ATML Test Results on the USAF Versatile Depot Automated Test Station","K. Coggins; M. Skiba","402 Software Maintenance Group, United States Air Force, Robins AFB, GA 31098, USA; Vektrex, 10225 Barnes Canyon Road A213, San Diego, CA 92121, USA","2008 IEEE AUTOTESTCON","","2008","","","607","612","The VDATS (Versatile Depot Automatic Test Station) is a Department of Defense (DoD) approved Family of Testers for the Air Force. Conceptualization began in 2006 and VDATS became a reality shortly after with the first article tester demonstrated at IEEE AUTOTESTCON 2007. Forward looking in their approach, the USAF VDATS team made the determination early in the program to employ the DoD ATS Framework where possible to standardize both hardware and software interfaces. ATML (Automatic Test Markup Language) is a DoD ATS Framework component and an emerging IEEE standard that defines a universal framework for ATS software interfaces. ATML Test Results supports the storage of results of an individual Test Program Set (TPS) execution cycle in a standardized format utilizing Extensible Markup Language (XML). ATML shows great promise toward reducing support and maintenance costs and improving interoperability. The VDATS program has enabled the Air Force to rehost scores of test programs from unsupportable legacy test systems; each test program containing test report formats unique to each legacy test system. Incorporating a universal model for test data capture and reporting is one key benefit of ATML Test Results. In addition, for VDATS, the Air Force is seeking to maximize the value of ATML Test Results by taking advantage of the inherent analysis capabilities, including the detection of problem areas in test systems, TPS design optimization, and monitoring of UUT (Unit Under Test) component failure rates. The VDATS ATML Initiative leverages the SBIR program to integrate ATML Test Results into VDATS and develop some useful analysis tools. This paper will present an overview of the VDATS software, Test Results implementation, key decisions, and actual test and analysis results based on real UUTs.","1088-7725;1558-4550","978-1-4244-2225-8978-1-4244-2226","10.1109/AUTEST.2008.4662690","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4662690","VDATS;ATML;Test Results;Versatile Depot Automated Test Station;DoD ATS Framework;OSS&E","XML;Force;Maintenance engineering;Instruments;US Department of Defense;Software;Weapons","automatic test equipment;military computing;XML","versatile depot automated test station;department of defense;air force;automatic test markup language;test program set execution cycle;extensible markup language;unsupportable legacy test systems","","","13","","","","","","IEEE","IEEE Conferences"
"A Survey of Coverage-Based Testing Tools","Q. Yang; J. J. Li; D. M. Weiss","NA; NA; NA","The Computer Journal","","2009","52","5","589","597","Test coverage is sometimes used to measure how thoroughly software is tested and developers and vendors sometimes use it to indicate their confidence in the readiness of their software. This survey studies and compares 17 coverage-based testing tools primarily focusing on, but not restricted to, coverage measurement. We also survey features such as program prioritization for testing, assistance in debugging, automatic generation of test cases and customization of test reports. Such features make tools more useful and practical, especially for large-scale, commercial software applications. Our initial motivations were both to understand the available test coverage tools and to compare them to a tool that we have developed, called eXVantage (a tool suite that includes code coverage testing, debugging, performance profiling and reporting). Our study shows that each tool has some unique features tailored to its application domains. The readers may use this study to help pick the right coverage testing tools for their needs and environment. This paper is also valuable to those who are new to the practice and the art of software coverage testing, as well as those who want to understand the gap between industry and academia.","0010-4620;1460-2067","","10.1093/comjnl/bxm021","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8130777","code coverage;coverage-based testing tool;prioritization;test case generation;dominator analysis","","","","","15","","","","","","","OUP","OUP Journals & Magazines"
"Cost minimization and its realization on induction motor design via SPEED/PC-IMD","C. T. Raj; S. P. Srivastava; P. Agarwal","Department of Electrical Engineering, Indian Institute of Technology Roorkee, India; Department of Electrical Engineering, Indian Institute of Technology Roorkee, India; Department of Electrical Engineering, Indian Institute of Technology Roorkee, India","2008 Annual IEEE India Conference","","2008","1","","46","50","This paper presents an optimal design of poly-phase induction motor using Particle Swarm Optimization (PSO). The optimization algorithm considers the cost of active material as objective function and nine performance related items as constraints. The PSO algorithm was implemented on two test motors (7.5 kW and 110 kW) and their results are compared with the Constrained Rosen Brock Method (Hill algorithm) and normal design. Optimized variables are realized by SPEED (Scottish Power Electronics and Electric Drives) software. Optimal design results are theoretically justified. Some standard benchmarking problems are used to validate the PSO algorithm. C++ code is used for implementing entire algorithms.","2325-940X;2325-9418","978-1-4244-3825-9978-1-4244-2747-5978-1-4244-3825","10.1109/INDCON.2008.4768799","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4768799","design optimization;induction motor;particle swarm optimization;SPEED/PC-IMD software","Induction motors;Cost function;Particle swarm optimization;Equations;Testing;Stochastic processes;Constraint optimization;Algorithm design and analysis;Power electronics;Software design","electric machine CAD;induction motors;particle swarm optimisation","cost minimization;poly-phase induction motor design;Scottish power electronics and electric drives;SPEED/PC-IMD software;C++ code;particle swarm optimization;constrained Rosen Brock method;power 110 kW;power 7.5 kW","","","22","","","","","","IEEE","IEEE Conferences"
"Multi-objective generation dispatch using Particle Swarm Optimisation","C. Rani; M. Rajesh Kumar; K. Pavan","School of Electrical Sciences, Vellore Institute of Technology-Deemed University, Tamil Nadu, India; School of Electrical Sciences, Vellore Institute of Technology-Deemed University, Tamil Nadu, India; School of Electrical Sciences, Vellore Institute of Technology-Deemed University, Tamil Nadu, India","2006 India International Conference on Power Electronics","","2006","","","421","424","The advancement in power systems has led to the development of generation dispatch (GD) that is difficult to solve by classical optimisation method. The proposed paper work is to evolve simple and effective method for optimum generation dispatch to minimise the fuel cost, environmental cost and security requirement of power networks. The approach is based on the bi-criterion global optimisation and particle swarm optimisation (PSO) technique. The proposed technique is tested on 3-area interconnected and longitudinal system. The effectiveness of the proposed optimisation is verified in simulation studies using MATLAB software. The PSO based approach has been extended to evaluate the trade-off curve between the fuel cost of power production and the environmental cost according to the bi-criterion objective function.","2160-3162;2160-3170","978-1-4244-3450-3978-1-4244-3451","10.1109/IICPE.2006.4685410","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4685410","generation dispatch;multi-objective global optimisation;particle swarm optimisation;bi-criterion constant","Particle swarm optimization;Cost function;Power system security;Power generation;Fuels;Optimization methods;System testing;Power system interconnection;MATLAB;Production","distribution networks;mathematics computing;particle swarm optimisation;power systems","multiobjective generation dispatch;particle swarm optimisation;power systems;optimum generation dispatch;fuel cost;environmental cost;security requirement;power networks;bi-criterion global optimisation;3-area interconnected system;MATLAB software;power production","","8","12","","","","","","IEEE","IEEE Conferences"
"Generation of test scripts for application with GUI optimized for manual execution","A. V. Barancev; S. G. Groshev; V. A. Omelchenko","Institute for System, Programming of RAS, Russia; Institute for System, Programming of RAS, Russia; Institute for System, Programming of RAS, Russia","2009 5th Central and Eastern European Software Engineering Conference in Russia (CEE-SECR)","","2009","","","137","142","The article describes the method for constructing sequences of user actions that are optimized for manual execution, based on the model in the form of a diagram of states and transitions. Scenarios for such implementation requires, primarily, when the GUI is the only way to interact with the application, but its implementation does not provide or make economically viable software emulation effects through it. Secondly, the test kit to perform the manual test may be necessary to assess the practicality of the graphical user interface to verify its compliance with selected standards and user acceptance testing.","","978-1-4244-5665-9978-1-4244-5664","10.1109/CEE-SECR.2009.5501173","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5501173","Graphical user interface (GUI);automation testing","Graphical user interfaces;Automatic testing;Optimization methods;Application software;Emulation;Performance evaluation;Automation;Java","graphical user interfaces;program testing","GUI;software emulation effects;graphical user interface;user acceptance testing;test script generation","","","","","","","","","IEEE","IEEE Conferences"
"A Replicated Quantitative Analysis of Fault Distributions in Complex Software Systems","C. Andersson; P. Runeson","Department of Computer Science, Lund University, Box 118, SE-221 00 Lund, Sweden; Department of Computer Science, Lund University, Box 118, SE-221 00 Lund, Sweden","IEEE Transactions on Software Engineering","","2007","33","5","273","286","To contribute to the body of empirical research on fault distributions during development of complex software systems, a replication of a study of Fenton and Ohlsson is conducted. The hypotheses from the original study are investigated using data taken from an environment that differs in terms of system size, project duration, and programming language. We have investigated four sets of hypotheses on data from three successive telecommunications projects: 1) the Pareto principle, that is, a small number of modules contain a majority of the faults (in the replication, the Pareto principle is confirmed), 2) fault persistence between test phases (a high fault incidence in function testing is shown to imply the same in system testing, as well as prerelease versus postrelease fault incidence), 3) the relation between number of faults and lines of code (the size relation from the original study could be neither confirmed nor disproved in the replication), and 4) fault density similarities across test phases and projects (in the replication study, fault densities are confirmed to be similar across projects). Through this replication study, we have contributed to what is known on fault distributions, which seem to be stable across environments.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.1005","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4160967","Empirical research;replication;software fault distributions.","Software systems;System testing;Software engineering;Computer languages;Quality management;Telecommunication switching;Conducting materials","Pareto optimisation;software fault tolerance","Pareto principle;complex software system;software fault distribution","","70","34","","","","","","IEEE","IEEE Journals & Magazines"
"Incorporating fault debugging activities into software reliability models: a simulation approach","S. S. Gokhale; M. R. Lyu; K. S. Trivedi","Dept. of Comput. Sci. & Eng., Connecticut Univ., Storrs, CT, USA; Dept. of Comput. Sci. & Eng., Connecticut Univ., Storrs, CT, USA; Dept. of Comput. Sci. & Eng., Connecticut Univ., Storrs, CT, USA","IEEE Transactions on Reliability","","2006","55","2","281","292","A large number of software reliability growth models have been proposed to analyse the reliability of a software application based on the failure data collected during the testing phase of the application. To ensure analytical tractability, most of these models are based on simplifying assumptions of instantaneous & perfect debugging. As a result, the estimates of the residual number of faults, failure rate, reliability, and optimal software release time obtained from these models tend to be optimistic. To obtain realistic estimates, it is desirable that the assumptions of instantaneous & perfect debugging be amended. In this paper we discuss the various policies according to which debugging may be conducted. We then describe a rate-based simulation framework to incorporate explicit debugging activities, which may be conducted according to the different debugging policies, into software reliability growth models. The simulation framework can also consider the possibility of imperfect debugging in conjunction with any of the debugging policies. Further, we also present a technique to compute the failure rate, and the reliability of the software, taking into consideration explicit debugging. An economic cost model to determine the optimal software release time in the presence of debugging activities is also described. We illustrate the potential of the simulation framework using two case studies.","0018-9529;1558-1721","","10.1109/TR.2006.874911","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1638411","Debugging;imperfect debugging;software reliability growth models","Software debugging;Software testing;Fault detection;Costs;Software reliability;Application software;Computational modeling;Failure analysis;Object detection;Phase detection","program debugging;digital simulation;software fault tolerance;program testing;software cost estimation","software fault debugging;software reliability growth models;rate-based simulation framework;explicit debugging activities;imperfect debugging;failure rate;economic cost model;optimal software release time","","32","36","","","","","","IEEE","IEEE Journals & Magazines"
"Characterizing Performance of an Intelligent Satellite QoS Optimization System","W. Almuhtadi; J. Tang; D. Murphy","NA; NA; NA","2007 Canadian Conference on Electrical and Computer Engineering","","2007","","","781","783","Countries with a vast range of intense weather conditions and broad geographic areas such as Canada can benefit greatly from the promise of advanced satellite and wireless quality of service (QoS) techniques. As these new services are researched and developed, their characteristics must be measured, verified and documented in order to ensure their design goals are met. This paper is about the design and development of a reliable automated network performance test platform that will be used to test the adaptive intelligent satellite QoS optimization system. A research team at Algonquin College, Ottawa Canada, has conceived and developed an original test platform, targeting the open-source Linuxreg operating system and development environment. This test platform will provide the performance data necessary to validate and optimize the innovative and unique algorithms that characterize the dynamic atmospheric conditions that can lead to signal degradation, thus enabling adaptive, real-time quality of service (QoS) for delivering reliable satellite-based services. The central component of this test platform, a program known as CommTest, will exercise the satellite forward and return channels using a number of typical traffic profiles (voice, video and data) between one or more ground terminals in order to extract and analyze key performance metrics. We illustrate the proposed test configuration of the device under test (DUT), the intelligent satellite QoS optimization system. The satellite encoding schemes of the forward and return channels are transparent to the TCP/IP protocol suite, and thus to CommTest as well. In this scenario, the performance of the DUT can be characterized in terms of simulated real world network traffic conditions as a 1<sup>st</sup> phase of testing. The 2<sup>nd</sup> phase of testing will proceed to implement CommTest on real satellite wireless networks at the Telesat site.","0840-7789","1-4244-1021-51-4244-1020","10.1109/CCECE.2007.200","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4232859","","Satellites;Quality of service;System testing;Automatic testing;Intelligent networks;Design optimization;Educational institutions;Open source software;Operating systems;Degradation","geographic information systems;Linux;optimisation;public domain software;quality of service;radio access networks;satellite communication;transport protocols","intelligent satellite;QoS;optimization system;weather conditions;geographic areas;open-source system;Linux;operating system;CommTest;TCP/IP protocol;satellite wireless networks","","1","5","","","","","","IEEE","IEEE Conferences"
"Automated fundamental analysis for stock ranking and growth prediction","A. Islam; H. Zaman; R. Ahmed","Department of Computer Science and Engineering, Bangladesh University of Engineering and Technology, Dhaka-1000, Bangladesh; Department of Computer Science and Engineering, Bangladesh University of Engineering and Technology, Dhaka-1000, Bangladesh; Department of Computer Science and Engineering, Bangladesh University of Engineering and Technology, Dhaka-1000, Bangladesh","2009 12th International Conference on Computers and Information Technology","","2009","","","145","150","In this paper we present the automated ranking by fundamental analysis (ARFA), a new fundamental analysis (FA) tool developed for aiding the research of fundamental indicators. ARFA provides a flexible and easy to use yet powerful platform to create and test new fundamental indicators for analyzing and comparing fundamentals of the companies in a stock market. ARFAoffers a software interface for FA that is straight forward, easy to learn and at the same time exceptionally expressive without the need of any programming or customization. In this work, we present a detailed description of ARFAs indicator creation platform with demonstration of its power by showing a number of well-known indicators written in ARFAs terminology. ARFA is intended for researcheras well as share market investors. It is web-based, free and open source. ARFA has a simple programming interface for future extensions of its terminology andability with easily pluggable modules.","","978-1-4244-6284-1978-1-4244-6281-0978-1-4244-6283","10.1109/ICCIT.2009.5407149","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5407149","Automated ranking;Data Processor;Dynamic variable;Fundamental Analysis;Fundamental indicator;Parser","Economic forecasting;Investments;Information analysis;Business;Companies;Power engineering and energy;Testing;Terminology;System analysis and design;Input variables","application program interfaces;investment;public domain software;stock markets;user interfaces","automated fundamental analysis;stock ranking;growth prediction;automated ranking;fundamental analysis tool;fundamental indicators;stock market;software interface;indicator creation platform;market investors;open source;programming interface;pluggable modules","","","11","","","","","","IEEE","IEEE Conferences"
"Software Quality Requirements: How to Balance Competing Priorities","J. D. Blaine; J. Cleland-Huang","independent consultant; DePaul University","IEEE Software","","2008","25","2","22","24","The elicitation, analysis, and specification of quality requirements involve careful balancing of a broad spectrum of competing priorities. Developers must therefore focus on identifying qualities and designing solutions that optimize the product's value to its stakeholders.","0740-7459;1937-4194","","10.1109/MS.2008.46","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4455627","Quality requirements;non-functional requirements;product value;architectural qualities","Software quality;Security;Safety;Usability;Testing;Terminology;Dictionaries;Taxonomy;Delay;Costs","","","","19","5","","","","","","IEEE","IEEE Journals & Magazines"
"Combining K-means and particle swarm optimization for dynamic data clustering problems","Y. Kao; S. Lee","Department of Information Management, Tatung University, Taipei, 104, Taiwan; Department of Information Management, Tatung University, Taipei, 104, Taiwan","2009 IEEE International Conference on Intelligent Computing and Intelligent Systems","","2009","1","","757","761","This paper presents a new dynamic data clustering algorithm based on K-means and combinatorial particle swarm optimization, called KCPSO. Unlike the traditional K-means method, KCPSO does not need a specific number of clusters given before performing the clustering process and is able to find the optimal number of clusters during the clustering process. In each iteration of KCPSO, a discrete PSO is used to optimize the number of clusters with which the K-means is used to find the best clustering result. KCPSO has been developed into a software system and evaluated by testing some datasets. Encouraging results show that KCPSO is an effective algorithm for solving dynamic clustering problems.","","978-1-4244-4754-1978-1-4244-4738","10.1109/ICICISYS.2009.5358020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5358020","Particle Swarm Optimization;K-means;Data clustering;Dynamic clustering","Particle swarm optimization;Clustering algorithms;Heuristic algorithms;Clustering methods;Iterative algorithms;Information management;Data mining;Software systems;Software testing;System testing","particle swarm optimisation;pattern clustering","dynamic data clustering problems;K-means method;combinatorial particle swarm optimization","","14","15","","","","","","IEEE","IEEE Conferences"
"Test Machine Scheduling and Optimization for z/ OS","M. Kaplan; T. Kimbrel; K. Mckenzie; R. Prewitt; M. Sviridenko; C. Williams; C. Yilmaz","IBM T. J. Watson Research Center, Yorktown Heights, NY, 10598, USA. email: mmk@us.ibm.com; IBM T. J. Watson Research Center, Yorktown Heights, NY, 10598, USA. email: kimbrel@us.ibm.com; IBM Systems & Technology Group, Poughkeepsie, NY, 12601, USA. email: kmckenzi@us.ibm.com; IBM Systems & Technology Group, Poughkeepsie, NY, 12601, USA. email: prewitt@us.ibm.com; IBM T. J. Watson Research Center, Yorktown Heights, NY, 10598, USA. email: sviri@us.ibm.com; IBM T. J. Watson Research Center, Yorktown Heights, NY, 10598, USA. email: clayw@us.ibm.com; IBM T. J. Watson Research Center, Yorktown Heights, NY, 10598, USA. email: cyilmaz@us.ibm.com","2007 IEEE Symposium on Computational Intelligence in Scheduling","","2007","","","27","34","We describe a system for solving a complex scheduling problem faced by software product test organizations. Software testers need time on test machines with specific features and configurations to perform the test tasks assigned to them. There is a limited number of machines with any given configuration, and this makes the machines scarce resources. Deadlines are always short. Thus, testers must reserve time on machines. Managing a schedule for a large test organization is a difficult task to perform manually. Requirements change frequently, making the task even more onerous, yet scheduling is done by hand in most teams. Our scheduling system is able to take into account the many and varied constraints and preferences that a team of human users inevitably has","","1-4244-0704","10.1109/SCIS.2007.367666","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4218593","","System testing;Software testing;Processor scheduling;Job shop scheduling;Operating systems;Availability;Performance evaluation;Software systems;Computational intelligence;Automatic testing","operating systems (computers);program testing;scheduling;team working","test machine scheduling;optimization;z/OS;complex scheduling problem;software testing;test organization;scheduling system","","","8","","","","","","IEEE","IEEE Conferences"
"Towards Good Enough Testing: A Cognitive-Oriented Approach Applied to Infotainment Systems","A. Eltaher","NA","2008 23rd IEEE/ACM International Conference on Automated Software Engineering","","2008","","","525","528","This contribution outlines a cognitive-oriented approach to construct test systems that can ""partially "" imitate several cognitive paradigms of skilled human testers. For example, learning, reasoning, optimization, etc. Hence, a reasonable portion of the workload done by a human tester would be shifted to the test system itself. This consequently leads to a substantial reduction in the development time and cost; yet the test efficiency is not sacrificed.","1938-4300","978-1-4244-2188-6978-1-4244-2187","10.1109/ASE.2008.98","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4639391","","Humans;Testing;Unified modeling language;Cognition;Software;Optimization;Knowledge based systems","program testing","cognitive-oriented approach;infotainment systems;test systems;substantial reduction","","3","10","","","","","","IEEE","IEEE Conferences"
"Novel Cross-Loopback Based Test Approach for Specification Test of Multi-Band, Multi-Hardware Radios","V. Natarajan; G. Srinivasan; A. Chatterjee; C. Force","Georgia Tech, USA; Georgia Tech, USA; Georgia Tech, USA; Texas Instruments, USA","25th IEEE VLSI Test Symposium (VTS'07)","","2007","","","297","302","Recent advances in radio systems engineering have enabled the design of multiple RF front ends (transmitters and receivers), each servicing specific RF communication standards across different frequency bands in a single electronic package. While the design aspects of such radios have been rigorously researched, the test aspects are still evolving. This paper proposes a novel `alternate cross-loopback testing' scheme for a multi-hardware radio using an `optimized' multi-tone test input. In this approach, the transmitter corresponding to one RF standard is used to test receivers corresponding to the same or different communication standards and operating frequency bands. The test approach is compatible with half-duplex operation of the transceiver front-ends and works with all the signal modulation and demodulation software turned ""off"". Cross-loopback is achieved via a broadband mixer, a programmable VCO and attenuator in the loopback path. The multi-sine tests allow accurate prediction of all transmitter and receiver specifications","1093-0167;2375-1053","0-7695-2812","10.1109/VTS.2007.43","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4209928","","Radio frequency;Radio transmitters;Communication standards;Receivers;Design engineering;Systems engineering and theory;Electronics packaging;Software testing;Transceivers;Demodulation","attenuators;integrated circuit testing;programmable circuits;radio receivers;radio transmitters;transceivers;voltage-controlled oscillators","cross-loopback based test approach;specification test;multiband multihardware radios;radio systems engineering;RF front ends;transmitters;receivers;RF communication standards;electronic package;alternate cross-loopback testing;multitone test input;operating frequency bands;half-duplex operation;transceiver front-ends;signal modulation;demodulation software;broadband mixer;programmable voltage controlled oscillator;attenuator;multisine tests","","3","15","","","","","","IEEE","IEEE Conferences"
"Software release optimization for a non-kalman filter SRGM","X. Jiang; D. Du; T. G. Ray; S. N. Ghazavi","College of Engineering, Louisiana State University, Baton Rouge, USA; Faculty of Administration, University of New Brunswick, Fredericton, Canada; College of Engineering, Louisiana State University, Baton Rouge, USA; College of Engineering, Louisiana State University, Baton Rouge, USA","2007 IEEE International Conference on Industrial Engineering and Engineering Management","","2007","","","1181","1185","The optimality of the one-bug-look-ahead (OLA) software release policy proposed in Morali, N. and Soyer, R. (2003) for a non- Kalman filter type of software reliability growth model (SRGM) is disproved in Jiang, X., et al., (2007). The remaining question - what is the optimal policy - is answered in this paper. It turns out that the optimal policy has a control-limit structure similar to OLA, but is not as impatient. A key step in the research is the identification of a Markov structure implied in the Bayesian updating mechanism which in turn reduces the optimal stopping formulation to an easily solvable Markov decision problem. A numerical example based on the NTDS data is included for illustration.","2157-3611;2157-362X","978-1-4244-1528-1978-1-4244-1529","10.1109/IEEM.2007.4419378","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4419378","Bayesian inference;Markov decision process;software reliability;optimal release","Filters;Bayesian methods;Software reliability;Software testing;Random variables;Educational institutions;Reliability engineering;Optimal control;History;Exponential distribution","Bayes methods;Markov processes;program debugging;program testing;software reliability","software release optimization;nonKalman filter SRGM;one-bug-look-ahead software release policy;software reliability growth model;optimal policy;control-limit structure;Markov structure;Bayesian updating mechanism;Markov decision problem;NTDS data","","","12","","","","","","IEEE","IEEE Conferences"
"Agent multiplication: An economical large-scale testing environment for system management solutions","K. D. Ryu; D. Daly; M. Seminara; S. Song; P. G. Crumley","IBM T.J. Watson Research Center Yorktown Heights, NY, USA; IBM T.J. Watson Research Center Yorktown Heights, NY, USA; IBM T.J. Watson Research Center Yorktown Heights, NY, USA; Department of Computer Science University of Maryland, College Park, USA; IBM T.J. Watson Research Center Yorktown Heights, NY, USA","2008 IEEE International Symposium on Parallel and Distributed Processing","","2008","","","1","8","System management solutions are designed to scale to thousands or more machines and networked devices. However, it is challenging to test and verify the proper operation and scalability of management software given the limited resources of a testing lab. We have developed a method called agent multiplication, in which one physical testing machine is used to represent hundreds of client machines. This provides the necessary client load to test the performance and scalability of the management software and server within limited resources. In addition, our approach guarantees that the test environment remains consistent between test runs, ensuring that test results can be meaningfully compared. We used agent multiplication to test and verify the operation of a server managing 4,000 systems. We exercised the server functions with only 8 test machines. Applying this test environment to an early version of a real enterprise system management solution we were able to uncover critical bugs, resolve race conditions, and examine and adjust thread prioritization levels for improved performance.","1530-2075","978-1-4244-1693-6978-1-4244-1694","10.1109/IPDPS.2008.4536552","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4536552","","Environmental economics;Large-scale systems;System testing;Environmental management;Software testing;Scalability;Resource management;Software performance;Computer bugs;Yarn","client-server systems;program debugging;program testing;program verification;software agents;software management;systems analysis","agent multiplication;economical large-scale testing environment;system management solutions;management software;physical testing machine;client machines;real enterprise system management solution;critical bugs","","1","17","","","","","","IEEE","IEEE Conferences"
"Towards Interactive Fault Localization Using Test Information","D. Hao; L. Zhang; H. Mei; J. Sun","Peking University; Peking University; Peking University; Peking University","2006 13th Asia Pacific Software Engineering Conference (APSEC'06)","","2006","","","277","284","Finding the location of a fault is a central task of debugging. Typically, a developer employs an interactive process for fault localization. To accelerate this task, several approaches have been proposed to automate fault localization. In practice, testing-based fault localization (TBFL), which uses test information to locate faults, has become a research focus. However, experimental results reported in the literature showed that current automation of fault localization can only serve as a means to confirming the search space and prioritizing search sequences, not a substitute of the interactive fault localization process. In this paper, we propose an approach based on test information to support the entire interactive fault localization process. During this process, the information gathered from previous interaction steps can be used to provide the ranking of suspicious statements for the current interaction step. As a feasibility study of our approach, we performed an experiment on applying our approach together with some other TBFL approaches on the Siemens programs, which have been used in the literature. Our experimental results show the effectiveness of our approach.","1530-1362;1530-1362","0-7695-2685","10.1109/APSEC.2006.59","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4137428","","Software testing;Humans;Electronic equipment testing;System testing;Software systems;Sun;Software debugging;Computer science;Acceleration;Automation","program debugging;program testing;software engineering","interactive fault localization;test information;debugging;testing-based fault localization;Siemens programs","","2","28","","","","","","IEEE","IEEE Conferences"
"Optimization of Interacting Controllers Using K-wise Tests","S. Goren","Computer Engineering Department, Bahçe&#x0015F;ehir University, Istanbul, Turkey, e-mail: sgoren@bahcesehir.edu.tr","2007 2nd International Design and Test Workshop","","2007","","","169","174","Digital systems are represented as a network of interacting components, and often their control units are as well modeled as a network of finite-state machines (FSMs). It is possible that when an FSM ( M<sub>2</sub>) is driven by another FSM (M<sub>1</sub>) as opposed to being driven from outside, some of its states and transitions may never be visited. Such cases may occur when M<sub>2</sub>is for example designed as a library component and hence is designed to work in a greater context - not only with M<sub>1</sub>. Hence, M<sub>2</sub>may be logic-minimized - called hierarchical optimization (HO), or the composite machine, M<sub>1</sub>rarr M<sub>2</sub>may be minimized - called global optimization (GO). We do HO as it is faster though sometimes less globally optimal than GO. Compared to the best previous HO method, we are significantly faster. We extract the composite machine M<sub>1</sub>rarr M<sub>2</sub>only once. We do combinational simulation as opposed to sequential. Instead of randomly picking from permissible test sequences, we filter out the ones that are k-wise complete. We inject faults and simulate in a topological order and remove multiple redundancies. As a result of our run-time efficiencies, we are also able to handle bigger FSMs than previous literature.","2162-0601;2162-061X","978-1-4244-1824-4978-1-4244-1825","10.1109/IDT.2007.4437453","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4437453","","Design optimization;Redundancy;System testing;Runtime;Reachability analysis;Digital systems;Libraries;Software testing;Hardware;Minimization methods","cascade control;control engineering computing;finite state machines;hierarchical systems;multivariable control systems;optimisation","interacting controllers;K-wise tests;digital systems;finite-state machines;FSM;hierarchical optimization method;composite machine;global optimization method;combinational simulation","","","20","","","","","","IEEE","IEEE Conferences"
"Analysis of the OLSR Protocol by Using Formal Passive Testing","C. Andrés; S. Maag; A. Cavalli; M. G. Merayo; M. Núñez","NA; NA; NA; NA; NA","2009 16th Asia-Pacific Software Engineering Conference","","2009","","","152","159","In this paper we apply a passive testing methodology to the analysis of a non-trivial system. In our framework, so-called invariants provide us with a formal representation of the requirements of the system. In order to precisely express new properties in multi-node environments, in this paper we introduce a new kind of invariants. We apply the resulting framework to perform a complete study of a MANET routing protocol: The optimized link state routing protocol.","1530-1362;1530-1362","978-0-7695-3909","10.1109/APSEC.2009.37","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5358585","Protocol Testing;Formal Methods;Timed Systems;Passive Testing;MANET routing protocols","Routing protocols;System testing;Runtime;Software testing;Mobile ad hoc networks;Software engineering;Electronic mail;Acoustic testing;Performance evaluation;Telecommunication computing","ad hoc networks;formal verification;mobile communication;program testing;routing protocols","OLSR protocol;optimized link state routing protocol;formal passive testing;formal representation;multinode environments;MANET routing protocol","","1","30","","","","","","IEEE","IEEE Conferences"
"Automated pseudo-live testing of firewall configuration enforcement","E. Al-Shaer; A. El-Atawy; T. Samak","NA; NA; NA","IEEE Journal on Selected Areas in Communications","","2009","27","3","302","314","Network security devices such as firewalls and intrusion detection systems are constantly updated in their implementation to accommodate new features, performance standards and to utilize new hardware optimization. Reliable, yet practical, testing techniques for validating the configuration enforcement after every new software and firmware update become necessary to assure correct configuration realization. Generating random traffic to test the firewall configuration enforcement is not only inaccurate but also impractical as it requires an infeasible number of test cases for a reasonable testing coverage. In addition, in most cases the policies used during testing are manually generated or have limited configuration profiles. We present a framework for automatic testing of the firewall configuration enforcement using efficient and flexible policy and traffic generation. In a typical test session, a large set of different policies are generated based on the access-control list (ACL) grammar and according to custom profiles. Test packets are generated to particularly consider critical segments of the tested policies and to achieve high coverage of the testing space. We also describe our implementation of a fully-automated framework, which includes ACL grammar modeling, the policy generation, test cases generation, capturing and analyzing firewall output, and creating detailed test reports. Our evaluation results show that our security configuration testing is not only achievable but it also offers high coverage with significant degree of confidence.","0733-8716;1558-0008","","10.1109/JSAC.2009.090406","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4808474","security configuration testing, firewall testing, policy enforcement validation, security evaluation, automated security analysis, policy generation, network security","Automatic testing;Filtering;Security;Intrusion detection;Hardware;Software testing;Microprogramming;Random number generation;Access control;Protection","authorisation;computer networks;firmware;optimisation;program testing","firewall configuration enforcement;automated pseudo-live testing;network security;intrusion detection systems;hardware optimization;firmware;traffic generation;access-control list grammar;policy generation;test cases generation","","16","23","","","","","","IEEE","IEEE Journals & Magazines"
"Testing of Image Processing Algorithms on Synthetic Data","K. v. Neumann-Cosel; E. Roth; D. Lehmann; J. Speth; A. Knoll","NA; NA; NA; NA; NA","2009 Fourth International Conference on Software Engineering Advances","","2009","","","169","172","In this paper, it is shown that synthetic images can be used to test specific use cases of a lane tracking algorithm which has been developed by Audi AG. This was achieved by setting up a highly configurable and extendable simulation framework ldquovirtual test driverdquo. The main components are a traffic simulation, visualization and a sensor model which supplies ground truth data about the street lanes. Additionally, the visualization is used to generate synthetic camera sensor data. The testbed also contains a realistic driving dynamics simulation and a real image processing soft ECU (which is represented as a standard PC in the early development stages). One of the modules on the image processing ECU is a lane tracking algorithm. The algorithm is designed to calculate the transition curves while driving. This information can be used as input for driving assistance functions, e.g. lane departure warning. By running the lane tracker on a synthetic image it is possible to compare the results of the lane tracker with the ground truth data provided by the simulation. In this particular case, the information has been used to test and optimize parts of the systems by using specific and determined scenarios in the simulation.","","978-1-4244-4779-4978-0-7695-3777","10.1109/ICSEA.2009.34","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5298451","image processing;testing;synthetic data;simulation;advanced driver assistance systems","Image processing;Roads;System testing;Software testing;Traffic control;Data visualization;Cameras;Image generation;Automatic testing;Software algorithms","data visualisation;image processing;tracking;traffic engineering computing;virtual reality","image processing;synthetic data;lane tracking;Audi AG;virtual test drive;traffic simulation;visualization","","3","7","","","","","","IEEE","IEEE Conferences"
"A hybrid particle swarm optimizer","Xiaoling Wu; Min Zhong","School of Computer, Wuhan University, China; School of Computer, Wuhan University, China","2009 Asia-Pacific Conference on Computational Intelligence and Industrial Applications (PACIIA)","","2009","2","","279","282","Particle Swarm Optimization (PSO) is a recently proposed population-based evolutionary algorithm, which shows good search abilities in many optimization problems. However, PSO easily suffers from premature convergence when solving multimodal problems. In this paper, a hybrid PSO algorithm, called HPSO, is proposed by employing an improved crossover operator to deal with multimodal problems. In order to verify the performance of the proposed approach, six well-known multimodal benchmark problems were selected into our experiments. The simulation results show that the proposed approach HPSO outperforms standard PSO and classical evolutionary programming (CEP) in all test cases.","","978-1-4244-4606","10.1109/PACIIA.2009.5406657","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5406657","Particle swarm optimization (PSO);crossover;optimization","Particle swarm optimization;Genetic mutations;Convergence;Evolutionary computation;Computational intelligence;Computer industry;Application software;Hybrid power systems;Benchmark testing;Genetic programming","evolutionary computation;particle swarm optimisation","hybrid particle swarm optimizer;population based evolutionary algorithm;optimization problem;multimodal problem;crossover operator;classical evolutionary programming","","","13","","","","","","IEEE","IEEE Conferences"
"SMES Optimization Benchmark Extended: Introducing Pareto Optimal Solutions Into TEAM22","P. Alotto; U. Baumgartner; F. Freschi; M. Jaindl; A. Kostinger; C. Magele; W. Renhart; M. Repetto","NA; NA; NA; NA; NA; NA; NA; NA","IEEE Transactions on Magnetics","","2008","44","6","1066","1069","In 1996, a superconducting magnetic energy storage arrangement was selected to become a benchmark problem for testing different optimization algorithms, both deterministic and stochastic ones. Since the forward problem can be solved semianalytically by Biot-Savart's law, this benchmark became quite popular. Nevertheless, the demands on optimization software have increased dramatically since then. To give an example, methods looking for Pareto-optimal points rather than for a single solution only have been introduced by several groups. In this paper, a proposal for an extended version of the benchmark problem will be made and some results will be presented.","0018-9464;1941-0069","","10.1109/TMAG.2007.916091","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4526788","Optimization benchmark problem;Pareto optimality;superconducting magnetic energy storage (SMES) arrangement","Samarium;Pareto optimization;Constraint optimization;Superconducting magnetic energy storage;Benchmark testing;Superconducting coils;Current density;Stochastic processes;Proposals;Magnetic fields","electromagnetism;Pareto optimisation;superconducting magnet energy storage","superconducting magnetic energy storage;pareto optimal solutions;SMES optimization benchmark problem;Biot-Savarts law;optimization algorithms;software","","34","9","","","","","","IEEE","IEEE Journals & Magazines"
"An Immune Genetic Algorithm for Software Test Data Generation","A. Bouchachia","University of Klagenfurt, 9020, Austria","7th International Conference on Hybrid Intelligent Systems (HIS 2007)","","2007","","","84","89","This paper aims at incorporating immune operators in genetic algorithms as an advanced method for solving the problem of test data generation. The new proposed hybrid algorithm is called immune genetic algorithm (IGA). A full description of this algorithm is presented before investigating its application in the context of software test data generation using some benchmark programs. Moreover, the algorithm is compared with other evolutionary algorithms.","","0-7695-2946-1978-0-7695-2946","10.1109/HIS.2007.37","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4344032","","Genetic algorithms;Software testing;Immune system;Hybrid power systems;System testing;Benchmark testing;Software quality;Instruments;Ant colony optimization;Cost function","genetic algorithms;program testing","immune genetic algorithm;software test data generation;test data generation;evolutionary algorithms","","3","13","","","","","","IEEE","IEEE Conferences"
"Novel Adaptive Hybrid Optimization (AHO) Technique Using Biologically-Inspired Algorithms with FLC","M. S. Soliman; G. Tan; M. Y. Abdullah","NA; NA; NA","2008 International Conference on Computer Science and Software Engineering","","2008","1","","1230","1233","A novel adaptive hybrid biologically-inspired algorithm has been proposed in this paper especially for function optimization problems. Four algorithms were studied in the paper, including classical particle swarm optimization (PSO), genetic algorithm (GA), hybrid particle swarm optimizations and hybrid genetic algorithm. The main idea is to incorporate PSO with GA, which can be achieved by a fuzzy logic controller (FLC). Using of a series of benchmark functions (BF) shows that the proposed adaptive hybrid optimization (AHO) possesses a better ability to find the global optimum than the standard PSO algorithm, Genetic algorithm and other hybrid techniques GA-PSO and PSO-GA. For varying series of BF test system parameters, fast acting FLC works as intelligent switching technique agent.","","978-0-7695-3336","10.1109/CSSE.2008.462","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721976","adaptive;hybrid;genetic algorithm;biologically-inspired;PSO","Genetic algorithms;Particle swarm optimization;Fuzzy logic;Evolution (biology);Genetic mutations;Power system modeling;Biology;Software algorithms;Design optimization;Recurrent neural networks","fuzzy control;genetic algorithms;particle swarm optimisation","adaptive hybrid optimization technique;biologically-inspired algorithms;particle swarm optimization;genetic algorithm;GA;PSO;hybrid particle swarm optimizations;hybrid genetic algorithm;fuzzy logic controller;benchmark functions","","","14","","","","","","IEEE","IEEE Conferences"
"Jumble Java Byte Code to Measure the Effectiveness of Unit Tests","S. A. Irvine; Tin Pavlinic; L. Trigg; J. G. Cleary; S. Inglis; M. Utting","Reel Two Ltd.; Reel Two Ltd.; Reel Two Ltd.; Reel Two Ltd.; Reel Two Ltd.; University of Waikato","Testing: Academic and Industrial Conference Practice and Research Techniques - MUTATION (TAICPART-MUTATION 2007)","","2007","","","169","175","Jumble is a byte code level mutation testing tool for Java which inter-operates with JUnit. It has been designed to operate in an industrial setting with large projects. Heuristics have been included to speed the checking of mutations, for example, noting which test fails for each mutation and running this first in subsequent mutation checks. Significant effort has been put into ensuring that it can test code which uses custom class loading and reflection. This requires careful attention to class path handling and coexistence with foreign class-loaders. Jumble is currently used on a continuous basis within an agile programming environment with approximately 370,000 lines of Java code under source control. This checks out project code every fifteen minutes and runs an incremental set of unit tests and mutation tests for modified classes. Jumble is being made available as open source.","","0-7695-2984-4978-0-7695-2984","10.1109/TAIC.PART.2007.38","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4344121","","Java;Genetic mutations;System testing;Software testing;Tin;Reflection;Programming environments;Writing;Programming profession;Investments","Java;optimisation;program testing;public domain software;software tools","Jumble;Java;byte code level mutation testing tool;JUnit;heuristics;subsequent mutation checking;class path handling;agile programming environment;open source","","20","16","","","","","","IEEE","IEEE Conferences"
"The Thermodynamic Particle Swarm Optimizer","Y. Wu; Y. Li; X. Xu; S. Peng","NA; NA; NA; NA","2008 International Conference on Computer Science and Software Engineering","","2008","1","","1203","1206","This paper has presented a novel optimization algorithm - thermodynamic particle swarm optimizers (TDPSO). It combines the simplified evolutionary equation and the thermodynamically strategy.The simplified equation without the velocity variable has drastically reduced computation costs to achieve faster convergence. Inspired by the free energy principle of the thermo-dynamical theoretics, TDPSO algorithm has defined the rating-based entropy (RE)concept and a component thermodynamic replacement(CTR) rule. These definitions are applied to control the optimal process and to achieve the potential of finding a global optimum. Compared with other improved PSO techniques, the experimental results describe how-to make the TDPSO benefit from the thermodynamics.","","978-0-7695-3336","10.1109/CSSE.2008.1248","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721969","","Thermodynamics;Particle swarm optimization;Equations;Software engineering;Optimal control;Process control;Clustering algorithms;Testing;Simulated annealing;Computer science","entropy;evolutionary computation;particle swarm optimisation;thermodynamics","thermodynamic particle swarm optimizer;novel optimization algorithm;TDPSO;evolutionary equation;thermo-dynamical theoretics;rating-based entropy;component thermodynamic replacement","","","12","","","","","","IEEE","IEEE Conferences"
"Static Analysis of Object References in RMI-Based Java Software","M. Sharp; A. Rountev","IEEE Computer Society; NA","IEEE Transactions on Software Engineering","","2006","32","9","664","681","Distributed applications provide numerous advantages related to software performance, reliability, interoperability, and extensibility. This paper focuses on distributed Java programs built with the help of the remote method invocation (RMI) mechanism. We consider points-to analysis for such applications. Points-to analysis determines the objects pointed to by a reference variable or a reference object field. Such information plays a fundamental role as a prerequisite for many other static analyses. We present the first theoretical definition of points-to analysis for RMI-based Java applications, and we present an algorithm for implementing a flow- and context-insensitive points-to analysis for such applications. We also discuss the use of points-to information for corrupting call graph information, for understanding data dependencies due to remote memory locations, and for identifying opportunities for improving the performance of object serialization at remote calls. The work described in this paper solves one key problem for static analysis of RMI programs and provides a starting point for future work on improving the understanding, testing, verification, and performance of RMI-based software","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.93","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1707666","RMI;object-oriented software;distributed software;program analysis;points-to analysis;reference analysis;class analysis;call graph construction;side-effect analysis.","Java;Application software;Software performance;Information analysis;Software testing;Algorithm design and analysis;Performance analysis;Computer Society;Middleware;Optimization","application program interfaces;Java;object-oriented programming;program diagnostics;remote procedure calls","static program analysis;object reference;RMI;distributed Java program;remote method invocation;points-to analysis;call graph information;data dependency;remote memory location;object serialization;object-oriented software","","3","53","","","","","","IEEE","IEEE Journals & Magazines"
"Challenges for embedded software development","M. V. Woodward; P. J. Mosterman","The MathWorks, Inc., 3 Apple Hill Drive, Natick, MA, 01760, USA; The MathWorks, Inc., 3 Apple Hill Drive, Natick, MA, 01760, USA","2007 50th Midwest Symposium on Circuits and Systems","","2007","","","630","633","Embedded software development is becoming increasingly difficult because of technical and commercial pressures. This paper assesses the state of the embedded software development discipline, examines where it is now by posing a set of challenges, evaluates the state of development of key technologies, and lastly presents a vision for how embedded software development might proceed in fifteen years time.","1548-3746;1558-3899","978-1-4244-1175-7978-1-4244-1176","10.1109/MWSCAS.2007.4488660","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4488660","embedded software development;Model-Based Design","Embedded software;Costs;Process design;Embedded system;System testing;Design optimization;Software prototyping;Programming;Circuits and systems;System recovery","embedded systems;software engineering","embedded software development;technical pressure;commercial pressure;model-based design","","8","23","","","","","","IEEE","IEEE Conferences"
"A genetic algorithm optimized SDT for state feedback control","P. Arena; F. Brunno; R. Caponetto","NA; NA; NA","International Symposium on Power Electronics, Electrical Drives, Automation and Motion, 2006. SPEEDAM 2006.","","2006","","","722","726","The paper presents an approach for the automatic optimal state feedback controller design and its download on an 8-bit microcontroller. Once the system model has been defined, the proposed procedure makes it possible to find the optimal gains of a feedback state controller which are compatible with the integer representation of the microcontroller. A software tool has been developed in order to support the design, implementation and testing phases of the control system","","1-4244-0193","10.1109/SPEEDAM.2006.1649864","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1649864","","Genetic algorithms;State feedback;Control systems;Optimal control;Automatic control;Microcontrollers;Software tools;Software testing;Vectors;Design optimization","control engineering computing;control system synthesis;genetic algorithms;microcontrollers;optimal control;software tools;state feedback;testing","genetic algorithm;state feedback control;automatic optimal controller;8-bit microcontroller;optimal gains;integer representation;software development tool","","","4","","","","","","IEEE","IEEE Conferences"
"Beampattern optimization based on predicted array manifold","C. Sun; B. Yang","Institute of Acoustical Engineering, Northwestern Polytechnical University, Xi'an, Shaanxi, China, 710072. csun@nwpu.edu.cn; Institute of Acoustical Engineering, Northwestern Polytechnical University, Xi'an, Shaanxi, China, 710072. yangbo@mail.nwpu.edu.cn","OCEANS 2007","","2007","","","1","4","A method for obtaining the array manifold in sensor array beampattern optimization is proposed in this paper. The proposed method is based on sound field prediction, using software packages ANSYS and SYSNOISE to simulate the process of measuring the array manifold in practical systems. For a given array supporting structure, the numerical boundary element method is used to calculate the array outputs when a test signal is assumed in the far-field of the array so as to obtain the sensor responses including both amplitude and phase of each element. Varying the relative incident angle of the test signal to the array will yield a collection of array responses in different directions and therefore form the array manifold. SYSNOISE provides a powerful tool to implement the idea of predicting the sensor responses. Since SYSNOISE itself does not possess the ability to generate the boundary grids, the finite element software ANSYS is used to fulfill it. A 14-element uniform arc array was constructed and tested in an anechoic watertank. Beampatterns based on the measured array manifold and that predicted shown very good consistence.","0197-7385","978-0933957-35","10.1109/OCEANS.2007.4449288","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4449288","","Sensor arrays;Phased arrays;Testing;Acoustic sensors;Optimization methods;Software packages;Predictive models;Software measurement;Boundary element methods;Mesh generation","acoustic field;acoustic transducer arrays;array signal processing;boundary-elements methods;finite element analysis;oceanographic techniques;optimisation;underwater sound","predicted array manifold;sensor array beampattern optimization;sound field prediction;software package;ANSYS;SYSNOISE;numerical boundary element method;array output;sensor response;finite element software;uniform arc array;anechoic watertank","","","10","","","","","","IEEE","IEEE Conferences"
"An Improved Meta-heuristic Search for Constrained Interaction Testing","B. J. Garvin; M. B. Cohen; M. B. Dwyer","NA; NA; NA","2009 1st International Symposium on Search Based Software Engineering","","2009","","","13","22","Combinatorial interaction testing (CIT) is a cost-effective sampling technique for discovering interaction faults in highly configurable systems. Recent work with greedy CIT algorithms efficiently supports constraints on the features that can coexist in a configuration. But when testing a single system configuration is expensive, greedy techniques perform worse than meta-heuristic algorithms because they produce larger samples. Unfortunately, current meta-heuristic algorithms are inefficient when constraints are present. We investigate the sources of inefficiency, focusing on simulated annealing, a well-studied meta-heuristic algorithm. From our findings we propose changes to improve performance, including a reorganized search space based on the CIT problem structure. Our empirical evaluation demonstrates that the optimizations reduce run-time by three orders of magnitude and yield smaller samples. Moreover, on real problems the new version compares favorably with greedy algorithms.","","978-0-7695-3675","10.1109/SSBSE.2009.25","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5033175","combinatorial interaction testing;simulated annealing;highly-configurable software;constrained covering arrays","Simulated annealing;System testing;Greedy algorithms;Costs;Software engineering;Software testing;Sampling methods;Computer science;Performance evaluation;Runtime","program testing;sampling methods;search problems","constrained interaction testing;improved meta-heuristic search;combinatorial interaction testing;cost-effective sampling technique;configurable system;greedy algorithm","","34","24","","","","","","IEEE","IEEE Conferences"
"Modeling Database Interactions in Web Applications and Generating Test Cases","B. Song; H. Miao; Z. Chen","NA; NA; NA","2009 WRI World Congress on Software Engineering","","2009","4","","216","221","Database has been widely used in Web applications and database interactions influence the navigations of Web applications. Besides, database interactions are among the most essential functional features in current Web applications. In this paper, special care is taken on database interactions in modeling and testing Web applications. GFSMs (Guarded Finite State Machines), which are augmented FSMs are employed as a tool to model database interactions. A GFSM-TT(GFSM test-tree) is constructed from GFSM. From the test-tree, minimal test set is derived to be employed to generate test. Finally, an algorithm is designed to optimize the test paths by decreasing the overlap. The approach we proposed can yield substantial results with test paths and state transitions are all less.","","978-0-7695-3570","10.1109/WCSE.2009.297","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5319538","Web applications;Database Interactions;FSM;Generating Test cases;Test Tree","Web pages;Spatial databases;Application software;Web server;Pressing;Software testing;Navigation;Displays;Software engineering;Laboratories","database management systems;finite state machines;program testing","database interactions modeling;Web application testing;Web application modeling;guarded finite state machines;test paths","","","10","","","","","","IEEE","IEEE Conferences"
"Realization on PSO Based Induction Motor Design Via SPEED/PC-IMD","C. T. Raj; S. P. Srivastava; P. Agarwal","NA; NA; NA","2009 International Conference on Advanced Computer Control","","2009","","","65","69","This paper presents an optimal design and its realization of poly-phase induction motor using particle swarm optimization (PSO). The optimization algorithm considers the efficiency, starting torque and temperature rise as objective function (which are considered separately) and nine performance related items as constraints. The PSO algorithm was implemented on a test motor and the results are compared with the simulated annealing (SA) technique and normal design. From the test results PSO gave better results and more suitable to motorpsilas design optimization. Optimized variables are realized by PC-IMD (induction motor drives) of SPEED (Scottish Power Electronics and Electric Drives) software. C++ code is used for implementing entire algorithms.","","978-0-7695-3516-6978-1-4244-3330","10.1109/ICACC.2009.131","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4777310","Design;Induction motor;Particle Swarm Optimization;SPEED Software","Induction motors;Testing;Particle swarm optimization;Constraint optimization;Torque;Temperature;Simulated annealing;Algorithm design and analysis;Design optimization;Induction motor drives","electric machine CAD;induction motors;particle swarm optimisation;simulated annealing","poly-phase induction motor design;particle swarm optimization;optimization algorithm;simulated annealing technique;SPEED software;C++ code;induction motor drives;PC-IMD","","","22","","","","","","IEEE","IEEE Conferences"
"Universal test system architecture in mechatronics - An approach for systematization of today's existing test tools","D. Korotkiy; H. Dettmering","Institute of Information Technology in Mechanical Engineering, Technische Universitaet Muenchen, 85748 Garching near Munich, Germany; Prozesswerk GmbH, 85748 Garching near Munich, Germany","2009 IEEE International Conference on Industrial Technology","","2009","","","1","5","In this paper an universal architecture for test systems is presented, which can be used as basis for systematization of today's existing test tools during all stages of the development process in mechatronics. The defined architecture improves the general partition of a test system with five functional levels and enables to assign the tasks of a test system to these levels. The universal test system architecture facilitates two main aims: (1) optimization of the application of test tools and tool chains and (2) assistance during the development of a new test system.","","978-1-4244-3506-7978-1-4244-3507","10.1109/ICIT.2009.4939560","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4939560","","System testing;Mechatronics;Computer architecture;Information technology;Mechanical engineering;Software testing;Home appliances;Embedded software;Failure analysis;Risk analysis","mechatronics;optimisation;quality assurance","universal test system architecture;mechatronics;test tools;optimization","","","5","","","","","","IEEE","IEEE Conferences"
"IFAO-Simo: A spatial-simulation based facility network optimization framework","Ming Xie; Wei Wang; Wenjun Yin; Jin Dong","IBM China Research Laboratory Building 19 Zhongguancun Software Park, 8 Dongbeiwang WestRoad, Haidian District, Beijing 100094, P.R. CHINA; IBM China Research Laboratory Building 19 Zhongguancun Software Park, 8 Dongbeiwang WestRoad, Haidian District, Beijing 100094, P.R. CHINA; IBM China Research Laboratory Building 19 Zhongguancun Software Park, 8 Dongbeiwang WestRoad, Haidian District, Beijing 100094, P.R. CHINA; IBM China Research Laboratory Building 19 Zhongguancun Software Park, 8 Dongbeiwang WestRoad, Haidian District, Beijing 100094, P.R. CHINA","2007 Winter Simulation Conference","","2007","","","2221","2226","This paper describes an innovative framework, iFAO-Simo, which integrates optimization, simulation and GIS (geographic information system) techniques to handle complex spatial facility network optimization problems ever challenged from retailing, banking and logistics nowadays. At the top level of iFAO-Simo, an optimization engine serves to generate and test candidate solutions iteratively by use of optimization algorithms such as Tabu Search and Genetic Algorithms. For each scenario given by the candidate solutions, a discrete event simulation engine is triggered to simulate customer and facility behaviors based on a GIS platform to characterize and visualize the spatial, dynamic and indeterministic environments. As the result, the target measures can be easily calculated to evaluate the solution and feedback to the optimization engine. This paper studies a real case of banking branch network optimization problem, and the results show that iFAO-Simo provides a useful way to handle complex spatial optimization problems.","0891-7736;1558-4305","978-1-4244-1305-8978-1-4244-1306","10.1109/WSC.2007.4419857","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4419857","","Geographic Information Systems;Engines;Banking;Discrete event simulation;Logistics;Testing;Iterative algorithms;Genetic algorithms;Visualization;Feedback","business data processing;discrete event simulation;facilities planning;geographic information systems;optimisation","iFAO-Simo;spatial simulation;facility network optimization framework;geographic information system;complex spatial facility network optimization;optimization engine;tabu search;genetic algorithms;discrete event simulation engine;spatial environment;dynamic environment;indeterministic environment;banking branch network optimization problem","","2","11","","","","","","IEEE","IEEE Conferences"
"Scientific computing with graphical system Design","R. Berger","National Instruments, Austin, TX 78759 USA","2009 IEEE Long Island Systems, Applications and Technology Conference","","2009","","","1","2","Competing in today's global economy requires companies to rapidly enter the market with innovative products that offer increased functionality and operate flawlessly. The graphical system design approach for test, control, and embedded design meets this need by providing a unified platform for designing, prototyping, and deploying applications. Graphical system Design empowers engineers to integrate real-world signals sooner for earlier error detection, reuse code for maximum efficiency, benefit immediately from advances in computing technology, and optimize system performance in a way that outpaces traditional design methodologies.","","978-1-4244-2347-7978-1-4244-2348","10.1109/LISAT.2009.5031557","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5031557","Programming;Programming environments;Software reliability;Software verification and validation","Scientific computing;System testing;Control systems;Prototypes;Design engineering;Systems engineering and theory;Computers;Design optimization;System performance;Design methodology","computer graphics;program testing;program verification;software reliability;software reusability","scientific computing;graphical system design;innovative product;embedded design testing;code error detection;code reuse;software reliability;software verification","","","","","","","","","IEEE","IEEE Conferences"
"Research on an Improved Evolution Algorithm and its Application in Function Optimization Problem","J. Li; J. Yan; G. Zhai","NA; NA; NA","2009 International Workshop on Intelligent Systems and Applications","","2009","","","1","4","An improved evolution algorithm (IEA) is proposed in this paper. It has some new features: 1) using multi-parent search strategy and stochastic ranking strategy and a simple diversity rules to maintain the diversity of the population; 2) using a hybrid self-adaptive crossover-mutation operator, which can enhance the search ability and exploit the optimum offspring; The algorithm of this paper is tested on 13 benchmark optimization problems with linear or/and nonlinear constraints and compared with other evolutionary algorithms. The experimental results demonstrate that the performance of IEA outperforms other evolutionary algorithms in terms of the quality of the final solution and the stability; and its computational cost is lower than the cost required by the other techniques compared.","","978-1-4244-3894-5978-1-4244-3893","10.1109/IWISA.2009.5073055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5073055","","Constraint optimization;Evolutionary computation;Evolution (biology);Stochastic processes;Benchmark testing;Genetic programming;Educational institutions;Computer science;Application software;Automatic testing","evolutionary computation;optimisation;search problems;stochastic processes","improved evolution algorithm;IEA problem;function optimization problem;multiparent search strategy;self-adaptive crossover-mutation operator;linear-nonlinear constraint;stochastic ranking strategy","","1","12","","","","","","IEEE","IEEE Conferences"
"Exploiting Constraint Solving History to Construct Interaction Test Suites","M. B. Cohen; M. B. Dwyer; J. Shi","University of Nebraska; University of Nebraska; University of Nebraska","Testing: Academic and Industrial Conference Practice and Research Techniques - MUTATION (TAICPART-MUTATION 2007)","","2007","","","121","132","Researchers have explored the application of combinatorial interaction testing (CIT) methods to construct samples to drive systematic testing of software system configurations. Applying CIT to highly-configurable software systems is complicated by the fact that, in many such systems, there are constraints between specific configuration parameters that render certain combinations invalid. In recent work, automated constraint solving methods have been combined with search-based CIT methods to address this problem with promising results. In this paper, we observe that the pattern of computation in greedy CIT algorithms leads to sequences of constraint solving problems that are closely related to one another. We propose two techniques for exploiting the history of constraint solving: (1) using incremental algorithms that are present within available constraint solvers and (2) mining constraint solver data structures to extract information that can be used to reduce the CIT search space. We evaluate the cost-effectiveness of these reductions on four real-world highly-configurable software systems and on a population of synthetic examples that share the characteristics of those four systems. In combination our techniques reduce the cost of CIT in the presence of constraints to that of traditional unconstrained CIT methods without sacrificing the quality of solutions.","","0-7695-2984-4978-0-7695-2984","10.1109/TAIC.PART.2007.17","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4344107","","History;System testing;Software systems;Software testing;Costs;Computer industry;Data mining;Data structures;Constraint optimization;Constraint theory","combinatorial mathematics;constraint handling;data mining;data structures;greedy algorithms;program testing;search problems","constraint solving;interaction test suites;combinatorial interaction testing;software system configurations;automated constraint solving methods;greedy CIT algorithms;incremental algorithms;constraint solver data structures","","11","24","","","","","","IEEE","IEEE Conferences"
"Zoltar: A Toolset for Automatic Fault Localization","T. Janssen; R. Abreu; A. J. C. v. Gemund","NA; NA; NA","2009 IEEE/ACM International Conference on Automated Software Engineering","","2009","","","662","664","Locating software components which are responsible for observed failures is the most expensive, error-prone phase in the software development life cycle. Automated diagnosis of software faults can improve the efficiency of the debugging process, and is therefore an important process for the development of dependable software. In this paper we present a toolset for automatic fault localization, dubbed Zoltar, which hosts a range of spectrum-based fault localization techniques featuring BARINEL, our latest algorithm. The toolset provides the infrastructure to automatically instrument the source code of software programs to produce runtime data, which is subsequently analyzed to return a ranked list of diagnosis candidates. Aimed at total automation (e.g., for runtime fault diagnosis), Zoltar has the capability of instrumenting the program under analysis with fault screeners as a run-time replacement for design-time test oracles.","1938-4300","978-1-4244-5259-0978-0-7695-3891","10.1109/ASE.2009.27","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5431712","Zoltar;Fault Localization tool;Debugging","Fault diagnosis;Instruments;Runtime;Fault detection;Embedded software;Software tools;Software debugging;Testing;Probability;Software engineering","program debugging;software engineering;software tools","zoltar;automatic fault localization;software components;error prone phase;software development;automated diagnosis;software faults;debugging process;spectrum based fault localization techniques;BARINEL;software programs;runtime data production;runtime replacement","","24","12","","","","","","IEEE","IEEE Conferences"
"Design & implementation of a standard gain horn antenna for optimized gain and radiation pattern using MathCAD & HFSS","S. Ikram; G. Ahmad","EE Department, NUCES-FAST, Lahore Campus, Pakistan; SEPS, University of Surrey, Guildford, United Kingdom","2008 Second International Conference on Electrical Engineering","","2008","","","1","5","This paper is about the design and implementation of a standard gain horn antenna using MathCAD & High frequency structure simulator (HFSS) Software. The horn was designed by implementing a procedure in MathCAD. Simulation & optimization of the horn was carried out in HFSS based on finite element method (FEM). The reason to simulate the horn in HFSS is the fact to achieve optimum gain and equal radiation patterns in both E-plane & H-plane (pencil beam). Itpsilas also easy in HFSS to optimize the various dimensions of the horn for optimum results. After the simulation & optimization, the horn was manufactured and tested. The results of the finalized simulation and actually manufactured horn are very close to each other.","","978-1-4244-2292-0978-1-4244-2293","10.1109/ICEE.2008.4553906","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4553906","Standard gain horn antenna;Pyramidal horn antenna;Radiation pattern;MathCAD;High Frequency Structure Simulator (HFSS) Software","Horn antennas;Design optimization;Antenna radiation patterns;Frequency;Antenna measurements;Apertures;Software standards;Virtual manufacturing;Testing;Gain measurement","antenna radiation patterns;electrical engineering computing;finite element analysis;horn antennas;optimisation","gain horn antenna;antenna radiation pattern;MathCAD;frequency structure simulator software;finite element method","","8","5","","","","","","IEEE","IEEE Conferences"
"The Current State and Future of Search Based Software Engineering","M. Harman","King¿s College London","Future of Software Engineering (FOSE '07)","","2007","","","342","357","This paper describes work on the application of optimization techniques in software engineering. These optimization techniques come from the operations research and metaheuristic computation research communities. The paper briefly reviews widely used optimization techniques and the key ingredients required for their successful application to software engineering, providing an overview of existing results in eight software engineering application domains. The paper also describes the benefits that are likely to accrue from the growing body of work in this area and provides a set of open problems, challenges and areas for future work.","","0-7695-2829","10.1109/FOSE.2007.29","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4221631","","Software engineering;Application software;Software testing;Educational institutions;Gold;Optimizing compilers;Computer science;Instruments;Councils;Operations research","operations research;optimisation;software engineering","software engineering;optimization;operations research;metaheuristic computation research","","185","91","","","","","","IEEE","IEEE Conferences"
"Instruction-set simulator design and realization based on the virtual instruction","Zhang Youwei; Liu Xiaochun; Wang Yonghong","Institute of Surveying and Mapping, Information Engineering University, Zhengzhou, China; Institute of Surveying and Mapping, Information Engineering University, Zhengzhou, China; Institute of Surveying and Mapping, Information Engineering University, Zhengzhou, China","2009 2nd IEEE International Conference on Computer Science and Information Technology","","2009","","","347","351","Instruction-set simulator is very important method and technique in reverse engineering analysis, compiling optimization and code coverage testing. It provides the test platform for analyzing software and the accurate data for program comprehension. An instruction-set simulator is designed in this article, it can simulate various processor's object program. This paper also provides a method to construct virtual instruction set, especially describes design memory simulating and instruction system simulating in the instruction-set simulator.","","978-1-4244-4519-6978-1-4244-4520","10.1109/ICCSIT.2009.5234931","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5234931","instruction-set;simulation;virtual instruction;execute unit;addressing modes","Analytical models;Information analysis;Software safety;Reverse engineering;Software testing;Floating-point arithmetic;Registers;Design engineering;Design optimization;Control system synthesis","instruction sets;program testing;reverse engineering;software architecture","instruction set simulator design;reverse engineering analysis;compiling optimization;code coverage testing;software analysis;program comprehension;processor object program;virtual instruction set","","","8","","","","","","IEEE","IEEE Conferences"
"A framework to evaluate multi-objective optimization algorithms in multi-agent negotiations","M. Ziadloo; S. S. Ghamsary; N. Mozayani","Computer Engineering Dep., Iran University of Science and Technology, Tehran, Iran; Method Ltd., Tehran, Iran; Computer Engineering Dep., Iran University of Science and Technology, Tehran, Iran","2009 IEEE International Conference on Computational Intelligence for Measurement Systems and Applications","","2009","","","264","267","Multi-objective optimization algorithms are designed to find Pareto frontier set. This set plays a major role in multi-agent systems' negotiations. Different applications might be interested in different parts of Pareto frontier. In this paper we present a framework to show how a multi-objective optimization algorithm is evaluated against others. We used eleven algorithms implemented in MOMHLib++ library to test our framework on a two agent negotiation of binary issues and binary dependency. But our framework is easily expandable to higher number of objectives and all types of negotiations. Our analysis shows that a single scalarization value of Pareto frontier is not enough to compare multi-objective optimization algorithms, as it is done in most cases.","2159-1547;2159-1555","978-1-4244-3819-8978-1-4244-3820","10.1109/CIMSA.2009.5069962","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5069962","Multi-objective optimization;Pareto frontier;multi-agent systems;negotiation","Pareto optimization;Multiagent systems;Application software;Algorithm design and analysis;Testing;Computational intelligence;Design optimization;Libraries;Pareto analysis;Upper bound","digital libraries;multi-agent systems;Pareto optimisation","multiobjective optimization algorithms;Pareto frontier set;multiagent system negotiation;MOMHLib++ library","","1","5","","","","","","IEEE","IEEE Conferences"
"Tutorial IND2B: Structured Embedded Configuration and Test","C. J. Clark","Intellitech Corp.","20th International Conference on VLSI Design held jointly with 6th International Conference on Embedded Systems (VLSID'07)","","2007","","","17","17","Embedded infrastructure IP to optimize chip-level manufacturing test and debugging has become common practice. However, adopting the same approach for boards and systems requires a different family of infrastructure IP. This tutorial introduces a structured, standards-based approach to PCB self-test and FPGA configuration and presents how it can optimize manufacturing test and debugging, leverage ASIC level DFT, and support configurability, especially in today's re configurable products. Today's complex IC relies on scan and BIST for the majority of the test coverage achieved. Software driven functional self-test for PCBs and Systems now requires a solid foundation of scan-based self-test due to the complexity and skill required to achieve high fault coverage and useful diagnostics. Off-the-shelf infrastructure IP based on IEEE standards will enable system designers to build in the field re-configurable and high quality self-testable products with a minimum of engineering time and effort. Furthermore, the convergence of the FPGA configuration standard and scan-based test presents the designers new opportunities to increase fault coverage, lower their manufacturing test costs, field support costs, and extend their products' useful life with in-the-field updates. Infrastructure IP for the board and system level will save engineering time and will reduce design risk since they are pre-engineered and leverage IEEE standards. The standards-based solutions are re-usable, from one phase of a single product life cycle to the next, and from one product design to the next","1063-9667;2380-6923","0-7695-2762","10.1109/VLSID.2007.164","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4092010","","Built-in self-test;Field programmable gate arrays;IP networks;Conferences;Tutorials;Manufacturing;IEEE Standards","boundary scan testing;fault diagnosis;field programmable gate arrays;IEEE standards;integrated circuit testing;logic testing;printed circuit manufacture;printed circuit testing;program debugging","chip-level manufacturing test;debugging;FPGA configuration;ASIC;DFT;functional self-test;scan-based self-test;fault coverage;fault diagnostics;off-the-shelf infrastructure IP;IEEE standards;product life cycle","","","","","","","","","IEEE","IEEE Conferences"
"Particle Swarm Optimization with Hybrid Velocity Updating Strategies","X. Wu; M. Zhong","NA; NA","2009 Third International Symposium on Intelligent Information Technology Application","","2009","2","","336","339","Particle Swarm Optimization (PSO) is a recently proposed population-based evolutionary algorithm, which shows good performance in many optimization problems. To achieve better performance, this paper presents a new variant of PSO algorithm called PSO with Hybrid Velocity Updating Strategies (HVS-PSO). HVS-PSO employs another two velocity updating strategies besides the original velocity updating strategy. Experimental studies on six well-known benchmark problems show that HVS-PSO outperforms PSO with inertia weight (PSO-w), local version of PSO with inertia weight (PSO-w-local), and fully informed particle swarm (FIPS) on majority of test problems.","","978-0-7695-3859","10.1109/IITA.2009.265","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5369390","Particle swarm optimization (PSO);veloctiy updating strategy;optimization","Particle swarm optimization;Evolutionary computation;Convergence;Benchmark testing;Information technology;Application software;Stochastic processes;Insects;Animals;Birds","evolutionary computation;particle swarm optimisation;search problems","particle swarm optimization;hybrid velocity updating strategies;PSO;population-based evolutionary algorithm;inertia weight;fully informed particle swarm;search abilities","","1","11","","","","","","IEEE","IEEE Conferences"
"Ordering Coverage Goals in Model Checker Based Testing","G. Fraser; F. Wotawa","NA; NA","2008 IEEE International Conference on Software Testing Verification and Validation Workshop","","2008","","","31","40","Model-based testing techniques select test cases according to test goals, which might be coverage criteria or mutation adequacy. Complex criteria and large models lead to large test suites, and a test case created for one coverage item might cover several other items as well. Therefore, test case generation is optimized in order to avoid unnecessary test cases and minimize the test generation and execution costs. Because of this optimization the order in which test goals are selected is expected to have an impact on both the performance of the test case generation and the size of resulting test suites, although finding the optimal order is not feasible in general. In this paper we report on experiments to determine the effects of the order in which test goals are selected on performance and the size of resulting test suites, and evaluate different heuristics to select test goals such that time and size are minimized. The approach described is based on test case generation with model checkers, and experimentation leads to the conclusion that the order matters less than expected.","","978-0-7695-3388","10.1109/ICSTW.2008.31","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4566989","","Software testing;Genetic mutations;System testing;Cost function;Cities and towns;Technological innovation;Monitoring;Conferences","program testing;program verification","ordering coverage goals;model checker based testing;test suites;test case generation","","3","20","","","","","","IEEE","IEEE Conferences"
"Automated Generation and Assessment of Autonomous Systems Test Cases","K. J. Barltrop; K. H. Friberg; G. A. Horvath","Jet Propulsion Laboratory, California Institute of Technology, 4800 Oak Grove Drive, Pasadena, CA 91109. 818-354-6412, Kevin.J.Barltrop@jpl.nasa.gov; Jet Propulsion Laboratory, California Institute of Technology, 4800 Oak Grove Drive, Pasadena, CA 91109. 818-393-4077, Kenneth.H.Friberg@jpl.nasa.gov, Friberg.Autonomy@gmail.com; Jet Propulsion Laboratory, California Institute of Technology, 4800 Oak Grove Drive, Pasadena, CA 91109. 818-39 3-6234, Gregory.A.Horvath@jpl.nasa.gov","2008 IEEE Aerospace Conference","","2008","","","1","10","Verification and validation testing of autonomous spacecraft routinely culminates in the exploration of anomalous or faulted mission-like scenarios. Prioritizing which scenarios to develop usually comes down to focusing on the most vulnerable areas and ensuring the best return on investment of test time. Rules-of-thumb strategies often come into play, such as injecting applicable anomalies prior to, during, and after system state changes; or, creating cases that ensure good safety-net algorithm coverage. Although experience and judgment in test selection can lead to high levels of confidence about the majority of a system's autonomy, it's likely that important test cases are overlooked. One method to fill in potential test coverage gaps is to automatically generate and execute test cases using algorithms that ensure desirable properties about the coverage. For example, generate cases for all possible fault monitors, and across all state change boundaries. Of course, the scope of coverage is determined by the test environment capabilities, where a faster-than-real-time, high-fidelity, software-only simulation would allow the broadest coverage. Even real-time systems that can be replicated and run in parallel, and that have reliable set-up and operations features provide an excellent resource for automated testing. Making detailed predictions for the outcome of such tests can be difficult, and when algorithmic means are employed to produce hundreds or even thousands of cases, generating predicts individually is impractical, and generating predicts with tools requires executable models of the design and environment that themselves require a complete test program. Therefore, evaluating the results of large number of mission scenario tests poses special challenges. A good approach to address this problem is to automatically score the results based on a range of metrics. Although the specific means of scoring depends highly on the application, the use of formal scoring metrics has high value in identifying and prioritizing anomalies, and in presenting an overall picture of the state of the test program. In this paper we present a case study based on automatic generation and assessment of faulted test runs for the Dawn mission, and discuss its role in optimizing the allocation of resources for completing the test program.","1095-323X","978-1-4244-1487-1978-1-4244-1488","10.1109/AERO.2008.4526484","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4526484","","Automatic testing;System testing;Space vehicles;Orbital robotics;Power system protection;Propulsion;Laboratories;Space technology;Investments;Software testing","aerospace testing;automatic testing;space vehicles","automated autonomous systems test case generation;validation testing;verification testing;faulted mission-like scenarios;rules-of-thumb strategies;safety-net algorithm coverage;software-only simulation;automated testing;formal scoring metrics;Dawn mission","","3","1","","","","","","IEEE","IEEE Conferences"
"Software environment for synthesis of testable FSM through decomposition","S. Devadze; A. Sudnitson","Department of Computer Engineering, Faculty of Information Technology, Tallinn University of Technology, Raja 15, 12618, Estonia; Department of Computer Engineering, Faculty of Information Technology, Tallinn University of Technology, Raja 15, 12618, Estonia","2008 26th International Conference on Microelectronics","","2008","","","433","436","This paper presents a method and software for constructing of testabie finite state machines (FSM). The proposed method of impiementing test for FSM has severaf advantages in comparison with common soiutions. However, this method imposes some constraints on FSM synthesis process, which can be satisfied by using specific decomposition technique. The presented software environment (caifed D&amp;S) is capabie to perform various kinds of FSM decomposition and synthesis, inciuding the one needed for constructing of testabie FSM.","","978-1-4244-1881-7978-1-4244-1882","10.1109/ICMEL.2008.4559314","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4559314","","Software testing;Built-in self-test;Circuit testing;Circuit synthesis;Performance evaluation;Test pattern generators;Pattern analysis;Automata;Logic testing;Design optimization","built-in self test;finite state machines;logic CAD","software environment;testable FSM;finite state machines;FSM decomposition","","1","12","","","","","","IEEE","IEEE Conferences"
"An optimization ant colony algorithm for composition of semantic Web services","K. Yan; G. Xue; S. Yao","Lab of Network Intelligent Computing, Yunnan University, Kunming, China; School of Software, Yunnan University, Kunming, China; School of Software, Yunnan University, Kunming, China","2009 Asia-Pacific Conference on Computational Intelligence and Industrial Applications (PACIIA)","","2009","2","","262","265","Automatic composition of semantic Web services by requests is one of difficult problems in the field of Web services. In this paper we investigate Web services ontology and ant colony (AC) algorithm and propose a method of composition of semantic Web services (ACAC) that is based on ant colony algorithm. We generate a graph of input and output of semantic Web services, using this method, and then the composition of Web services is transformed into finding a satisfying path in the graph. So AC algorithm is used for efficient composition of Web services. Finally, we have tested ACAC with a test set and the results show that the algorithm can successfully compose the semantic Web services and is also able to ensure the quality and efficiency of composition.","","978-1-4244-4606","10.1109/PACIIA.2009.5406653","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5406653","ontology;Semantic Web Services;composition of Web services;Ant Colony Algorithm","Ant colony optimization;Semantic Web;Web services;Ontologies;Computational intelligence;Testing;Distributed computing;Robustness;Computer industry;Application software","ontologies (artificial intelligence);optimisation;semantic Web;Web services","optimization;ant colony algorithm;semantic Web services ontology;automatic composition","","4","17","","","","","","IEEE","IEEE Conferences"
"Automatic Path-Oriented Test Data Generation Using a Multi-population Genetic Algorithm","Y. Chen; Y. Zhong","NA; NA","2008 Fourth International Conference on Natural Computation","","2008","1","","566","570","Automatic path-oriented test data generation is an undecidable problem and genetic algorithm (GA) has been used to test data generation since 1992. In favor of MATLAB, a multi-population genetic algorithm (MPGA) was implemented, which selects individuals for free migration based on their fitness values. Applying MPGA to generating path-oriented test data generation is a new and meaningful attempt. After depicting how to transform path-oriented test data generation into an optimization problem, basic process flow of path-oriented test data generation using GA was presented. Using a triangle classifier as program under test, experimental results show that MPGA based approach can generate path-oriented test data more effectively and efficiently than simple GA based approach does.","2157-9555;2157-9563","978-0-7695-3304","10.1109/ICNC.2008.388","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4666909","","Automatic testing;Genetic algorithms;Software testing;Costs;Input variables;Computer applications;Educational institutions;MATLAB;Robustness;Search methods","flow graphs;genetic algorithms;program testing","automatic path-oriented test data generation;multipopulation genetic algorithm;MPGA algorithm;optimization problem;triangle classifier;control flow graph;software testing","","16","25","","","","","","IEEE","IEEE Conferences"
"Software reliability prediction using multi-objective genetic algorithm","S. H. Aljahdali; M. E. El-Telbany","Computer Sciences Department, Al-Taif University, SAUDI ARABIA; Computers Engineering Department, Al-Taif University, SAUDI ARABIA","2009 IEEE/ACS International Conference on Computer Systems and Applications","","2009","","","293","300","Software reliability models are very useful to estimate the probability of the software fail along the time. Several different models have been proposed to predict the software reliability growth (SRGM); however, none of them has proven to perform well considering different project characteristics. The ability to predict the number of faults in the software during development and testing processes. In this paper, we explore Genetic Algorithms (GA) as an alternative approach to derive these models. GA is a powerful machine learning technique and optimization techniques to estimate the parameters of well known reliably growth models. Moreover, machine learning algorithms, proposed the solution overcome the uncertainties in the modeling by combining multiple models using multiple objective function to achieve the best generalization performance where. The objectives are conflicting and no design exists which can be considered best with respect to all objectives. In this paper, experiments were conducted to confirm these hypotheses. Then evaluating the predictive capability of the ensemble of models optimized using multi-objective GA has been calculated. Finally, the results were compared with traditional models.","2161-5322;2161-5330","978-1-4244-3807-5978-1-4244-3806","10.1109/AICCSA.2009.5069339","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5069339","","Software reliability;Genetic algorithms;Software systems;Predictive models;Reliability engineering;Software testing;Uncertainty;Computational intelligence;Parameter estimation;Stochastic processes","genetic algorithms;learning (artificial intelligence);parameter estimation;program testing;software reliability","software reliability prediction;multiobjective genetic algorithm;probability estimation;software fail;software development;software testing;machine learning;optimization;parameter estimation","","12","27","","","","","","IEEE","IEEE Conferences"
"An experimental study of methods for executing test suites in memory constrained environments","S. Bhadra; A. Conrad; C. Hurkes; B. Kirklin; G. M. Kapfhammer","Milcord LLC, USA; Allegheny College, USA; Allegheny College, USA; Allegheny College, USA; Allegheny College, USA","2009 ICSE Workshop on Automation of Software Test","","2009","","","27","35","Software for memory constrained mobile devices is often implemented in the Java programming language because the Java compiler and virtual machine (JVM) provide enhanced safety, portability, and the potential for run-time optimization. However, testing time may increase substantially when memory is limited and the JVM employs a compiler to create native code bodies. This paper furnishes an empirical study that identifies the fundamental trade-offs associated with a method that uses adaptive native code unloading to perform memory constrained testing. The experimental results demonstrate that code unloading can reduce testing time by 17% and the code size of the test suite and application under test by 68% while maintaining the overall size of the JVM. We also find that the goal of reducing the space overhead of an automated testing technique is often at odds with the objective of decreasing the time required to test. Additional experiments reveal that using a complete record of test suite behavior, in contrast to a sample-based profile, does not enable the code unloader to make decisions that markedly reduce testing time. Finally, we identify test suite and application behaviors that may limit the effectiveness of our method for memory constrained test execution and we suggest ways to mitigate these challenges.","","978-1-4244-3711","10.1109/IWAST.2009.5069038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5069038","","Software safety;Java;Automatic testing;Computer languages;Program processors;Virtual machining;Safety devices;Runtime;Constraint optimization;Optimizing compilers","Java;mobile computing;program compilers;program testing;virtual machines","memory constrained mobile devices;Java programming language;Java compiler;virtual machine;adaptive native code unloading;automated testing technique","","","24","","","","","","IEEE","IEEE Conferences"
"Workload Reduction for Multi-input Feedback-Directed Optimization","P. Berube; J. N. Amaral; R. Ho; R. Silvera","NA; NA; NA; NA","2009 International Symposium on Code Generation and Optimization","","2009","","","59","69","Feedback-directed optimization is an effective technique to improve program performance, but it may result in program performance and compiler behavior that is sensitive to both the selection of inputs used for training and the actual input in each run of the program. Cross-validation over a workload of inputs can address the input-sensitivity problem, but introduces the need to select a representative workload of minimal size from the population of available inputs. We present a compiler-centric clustering methodology to group similar inputs so that redundant inputs can be eliminated from the training workload. Input similarity is determined based on the compile-time code transformations made by the compiler after training separately on each input. Differences between inputs are weighted by a performance metric based on cross-validation in order to account for code transformation differences that have little impact on performance. We introduce the <i>CrossError</i> metric that allows the exploration of correlations between transformations based on the results of clustering. The methodology is applied to several SPEC benchmark programs, and illustrated using selected case studies.","","978-0-7695-3576","10.1109/CGO.2009.23","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4907651","compilers;workload reduction;clustering;feedback-directed optimization","Program processors;Optimizing compilers;Software performance;Instruments;Frequency measurement;Radio spectrum management;Frequency conversion;Testing;Time measurement;Humans","optimising compilers;software metrics;software performance evaluation","multi input feedback-directed optimization;program performance;compiler behavior;workload reduction;cross-validation;input-sensitivity problem;compiler-centric clustering methodology;compile-time code transformations;CrossError metric;SPEC benchmark programs;performance evaluation","","","16","","","","","","IEEE","IEEE Conferences"
"Parallel test tasks scheduling on multi-core platform","L. Wang; J. Fang; C. Gao","Engineering School of the Air Force Engineering University Xi'an, 710038, China; Engineering School of the Air Force Engineering University Xi'an, 710038, China; Engineering School of the Air Force Engineering University Xi'an, 710038, China","2008 IEEE AUTOTESTCON","","2008","","","504","507","Aimed at the problem that traditional test program couldnpsilat effectively enhance the test efficiency, three decompose method which based on mission, data and data stream were brought forward to solve the test problem of parallel decompose. Multi-core technology was imported in parallel test platform to make the real time ability of the parallel mission improved. In order to work out the issue that test program based on single-core couldnpsilat utilize MCP technology effectively, schedule strategy of parallel test mission is advanced to optimize test flow and mission schedule. According to the result of experimentation, performance of plat based on MCP enhanced 30%~50% against single-core plat.","1088-7725;1558-4550","978-1-4244-2225-8978-1-4244-2226","10.1109/AUTEST.2008.4662669","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4662669","multi-core;parallel test;test process;parallel;schedule strategy","Schedules;Prefetching;Testing;Computer architecture;Programming;Software architecture;Processor scheduling","automatic testing;computerised instrumentation;parallel programming","parallel test tasks scheduling;multi core platform;parallel test platform;real time ability","","","11","","","","","","IEEE","IEEE Conferences"
"A novel PSO-IE3D based design and optimization of a low profile Dual Slot Microstrip Patch Antenna","R. N. Biswas; A. Kar","Department of Electronics & Telecomm. Engineering, Jadavpur Universit, Kolkata-700032, India; Department of Electronics & Telecomm. Engineering, Jadavpur Universit, Kolkata-700032, India","TENCON 2008 - 2008 IEEE Region 10 Conference","","2008","","","1","4","A dual slot microstrip path antenna has been designed and developed for operation around 2 GHz frequencies. The antenna parameters were designed using the well-known IE3D software package while the important characteristics of the antenna were optimized using adaptive particle swarm optimization (APSO) technique. The combined APSO-IE3D method gives a novel approach for best possible implementation of a probe fed low-profile dual slot patch antenna design. Simulation results show that the antenna has a bandwidth of 28.7 MHz (&lt;-10 dB) in the frequency range of 1.95-2.0 GHz. The antenna has been fabricated on a substrate with dielectric constant of 2.4 and thickness of 1.5875 mm important antenna parameters such as gain, VSWR, directivity and return-loss were measured using network analyzers and antenna test-bench. The measured values of the antenna parameters were found to match well within tolerable limits with that of the simulated results. The basic principle of the APSO- IE3D based design of microstrip patch antenna along with the simulated and the experimental results for a dual-slot patch are presented for comparison in this work.","2159-3442;2159-3450","978-1-4244-2408-5978-1-4244-2409","10.1109/TENCON.2008.4766781","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4766781","Adaptive PSO (APSO);EM Simulator (IE3D);Dual Slot Microstrip Patch Antenna","Design optimization;Microstrip antennas;Patch antennas;Frequency;Adaptive arrays;Antenna measurements;Dielectric measurements;Slot antennas;Software packages;Particle swarm optimization","electrical engineering computing;microstrip antennas;network analysers;particle swarm optimisation;slot antennas;UHF antennas","PSO-IE3D optimization;dual slot microstrip patch antenna;antenna parameter measurement;IE3D software package;adaptive particle swarm optimization;APSO technique;antenna bandwidth;dielectric constant;network analyzers;antenna test-bench;frequency 1.95 GHz to 2 GHz;bandwidth 28.7 MHz;size 1.5875 mm","","3","10","","","","","","IEEE","IEEE Conferences"
"Optimal Resource Allocation for Batch Testing","F. Chang; J. Ren; R. Viswanathan","NA; NA; NA","2009 International Conference on Software Testing Verification and Validation","","2009","","","91","100","Batch resource allocation problem arises in the context of executing a sequence of automated system tests or distributed computations where resources are pooled together and flexibly matched with requests. Minimizing resource allocation for a batch of processes reduces the resource management (e.g., setup) cost for the batch while allowing more users to share the resource pool simultaneously. The salient characteristic of the batch resource allocation problem is that while resources can be reused across different processes they are subject to mutually exclusive use for any individual process. We show that resource allocation for a single process can be solved in polynomial time whereas the general optimization problem is NP-complete. This motivates us to consider heuristics that can yield close to optimum solutions in polynomial time. We design several such heuristics and present their experimental comparison. Our experiments show that a technique based on a min-cost max-flow algorithm combined with ranked removal yields the best solution while having smallest running time.","2159-4848","978-1-4244-3775-7978-0-7695-3601","10.1109/ICST.2009.25","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4815341","","Resource management;System testing;Automatic testing;Cellular phones;Software testing;Costs;Polynomials;Distributed computing;Automation;Filling","batch processing (computers);optimisation;program testing;resource allocation","optimal resource allocation;batch testing;resource management;batch resource allocation problem;NP-complete problem;polynomial time;distributed system","","3","20","","","","","","IEEE","IEEE Conferences"
"Calibration parameters principles for MATLAB S-functions using CANApe","M. Muresan; D. Pitica; G. Chindris","Applied Electronics Department, Technical University of Cluj-Napoca, 24-26 Baritiu Street, 400027 Cluj-Napoca, Romania; Applied Electronics Department, Technical University of Cluj-Napoca, 24-26 Baritiu Street, 400027 Cluj-Napoca, Romania; Applied Electronics Department, Technical University of Cluj-Napoca, 24-26 Baritiu Street, 400027 Cluj-Napoca, Romania","2008 31st International Spring Seminar on Electronics Technology","","2008","","","105","110","In the development of embedded software one of the harder tasks are testing and optimize the ECUs. The basic way to test and optimize is by measuring and calibrating the process. One of the common methods to solve this problem is dasiaHardware-in-the-Looppsila(HIL). With this method, the real ECU is place in a simulative environment for the software under test. The software runs on the real target hardware. The basic idea with dasiaSoftware-in-the-Looppsila(SIL) is the same, except that all runs on standard PC hardware. [1] Both for HIL and SIL many simulated models for automotive industry are made in MATLAB/Simulink. A way how to put ECU's software into MATLAB/Simulink in order to have a complete testing environment will reduce considerably the testing costs and time to market. To have a unitary view both for HIL and SIL we must use the same approach. To keep the consistency we should use the same measurement and application tool, in our case CANApe. This will be done using XCP protocol based on Ethernet TCP/IP transport layer. [2] The entire ECU's software will be wrapped into a MATLAB/Simulink S-Function. The obtained S-Function will be a part of the entire model, with the possibility of having access to the inside variables of the S-Function.","2161-2528;2161-2064","978-1-4244-3973-7978-1-4244-3974-4978-1-4244-3972","10.1109/ISSE.2008.5276436","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5276436","","Calibration;MATLAB;Software testing;Hardware;Mathematical model;Embedded software;Automotive engineering;Costs;Time to market;Application software","automobiles;control engineering computing;program testing;traffic engineering computing","calibration parameters;MATLAB S-functions;CANApe;embedded software development;ECU testing;ECU optimization;Hardware-in-the-Loop testing;software testing;Software-in-the-Loop testing;automotive industry;MATLAB Simulink;XCP protocol;Ethernet TCP-IP transport layer","","","7","","","","","","IEEE","IEEE Conferences"
"Using Program Data-State Diversity in Test Data Search","M. Alshraideh; L. Bottaci","The University Of Hull, UK; NA","Testing: Academic & Industrial Conference - Practice And Research Techniques (TAIC PART'06)","","2006","","","107","114","Search-based automatic software test data generation for structural testing depends on the instrumentation of the test goal to construct a many-valued function which is then optimised. The method encounters difficulty when the search is in a region in which the function is not able to discriminate between different candidate test cases because it returns a constant value. A typical example of this problem arises in the instrumentation of branch predicates that depend on the value of a Boolean-valued (flag) variable. Existing transformation techniques can solve many cases of the problem but there are situations for which transformation techniques are inadequate. This paper presents a technique for directing the search when the function that instruments the test goal is not able to discriminate candidate test inputs. The new technique depends on introducing program data-state diversity as an additional search goal. The search is guided by a new evaluation (cost) function made up of two parts, one depends on the conventional instrumentation of the test goal, the other depends on the diversity of the data-states produced during execution of the program under test. The method is demonstrated for a number of example programs for which existing methods are inadequate","","0-7695-2672","10.1109/TAIC-PART.2006.37","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1691676","","Automatic testing;Instruments;Cost function;Software testing;Computer science;Search methods;Genetic algorithms","data flow analysis;optimising compilers;program control structures;program testing;search problems","program data-state diversity;search-based automatic software test data generation;structural testing;many-valued function construction;branch predicates;Boolean-valued variable;program transformation technique","","","18","","","","","","IEEE","IEEE Conferences"
"Fast Multi-Swarm Optimization for Dynamic Optimization Problems","C. Li; S. Yang","NA; NA","2008 Fourth International Conference on Natural Computation","","2008","7","","624","628","In the real world, many applications are non-stationary optimization problems. This requires that the optimization algorithms need to not only find the global optimal solution but also track the trajectory of the changing global best solution in a dynamic environment. To achieve this, this paper proposes a multi-swarm algorithm based on fast particle swarm optimization for dynamic optimization problems. The algorithm employs a mechanism to track multiple peaks by preventing overcrowding at a peak and a fast particle swarm optimization algorithm as a local search method to find the near optimal solutions in a local promising region in the search space. The moving peaks benchmark function is used to test the performance of the proposed algorithm. The numerical experimental results show the efficiency of the proposed algorithm for dynamic optimization problems.","2157-9555;2157-9563","978-0-7695-3304","10.1109/ICNC.2008.313","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4668051","","Particle swarm optimization;Equations;Computer science;Convergence;Evolutionary computation;Application software;Trajectory;Particle tracking;Search methods;Benchmark testing","particle swarm optimisation;search problems","multiswarm optimization;dynamic optimization problem;particle swarm optimization;search method;moving peak benchmark function","","34","18","","","","","","IEEE","IEEE Conferences"
"An Experimental Study of Self-Optimizing Dense Linear Algebra Software","M. Kulkarni; K. Pingali","NA; NA","Proceedings of the IEEE","","2008","96","5","832","848","Memory hierarchy optimizations have been studied by researchers in many areas including compilers, numerical linear algebra, and theoretical computer science. However, the approaches taken by these communities are very different. The compiler community has invested considerable effort in inventing loop transformations like loop permutation and tiling, and in the development of simple analytical models to determine the values of numerical parameters such as tile sizes required by these transformations. Although the performance of compiler-generated code has improved steadily over the years, it is difficult to retarget restructuring compilers to new platforms because of the need to develop analytical models manually for new platforms. The search for performance portability has led to the development of self-optimizing software systems. One approach to self-optimizing software is the generate-and-test approach, which has been used by the dense numerical linear algebra community to produce high- performance BLAS and fast Fourier transform libraries. Another approach to portable memory hierarchy optimization is to use the divide-and-conquer approach to implementing cache- oblivious algorithms. Each step of divide-and-conquer generates problems of smaller size. When the working set of the subproblems fits in some level of the memory hierarchy, that subproblem can be executed without capacity misses at that level. Although all three approaches have been studied extensively, there are few experimental studies that have compared these approaches. How well does the code produced by current self-optimizing systems perform compared to hand-tuned code? Is empirical search essential to the generate-and- test approach or is it possible to use analytical models with platform-specific parameters to reduce the size of the search space? The cache-oblivious approach uses divide-and-conquer to perform approximate blocking; how well does approximate blocking perform compared to precise blocking? This paper addresses such questions for matrix multiplication, which is the most important dense linear algebra kernel.","0018-9219;1558-2256","","10.1109/JPROC.2008.917732","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4484942","Algorithms;cache blocking;cache memories;computer performance;linear algebra;matrix multiplication;memory architecture;tiling;Algorithms;cache blocking;cache memories;computer performance;linear algebra;matrix multiplication;memory architecture;tiling","Linear algebra;Analytical models;Optimizing compilers;Computer science;Software systems;Software performance;Fast Fourier transforms;Software libraries","cache storage;divide and conquer methods;linear algebra;mathematics computing;matrix multiplication;memory architecture;program compilers;self-adjusting systems","self-optimizing dense linear algebra software;self-optimizing software system;generate-and-test approach;portable memory hierarchy optimization;divide-and-conquer;cache-oblivious approach;approximate blocking;matrix multiplication;dense linear algebra kernel;cache blocking;cache memory;memory architecture;compiler generated-code","","3","24","","","","","","IEEE","IEEE Journals & Magazines"
"Test set problem for multilevel systems","S. Zelenskyy","National Taras Shevchenko University of Kyiv, Volodymirsta Str.64, Kyiv, 01033, Ukraine","2008 International Conference on "Modern Problems of Radio Engineering, Telecommunications and Computer Science" (TCSET)","","2008","","","142","143","The problem of investigation of the test set for multilevel systems is viewed. It is offered an approach to finding a set of tests that exercises all the transitions of the system.","","978-966-553-678","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5423577","Multilevel systems;Testing;General hierarchies;Fractional flow;Approximation algorithm","Unified modeling language;Testing;Multilevel systems;Computational modeling;Frequency modulation;Optimized production technology;Bismuth","directed graphs;neural nets;object-oriented languages;program testing;Unified Modeling Language","test set problem;multilevel systems;object-oriented software;UML modelling languages;artificial neural networks;lower level modules;flattened graph;testing graph;directed graph","","","5","","","","","","IEEE","IEEE Conferences"
"A Novel Evolutionary Algorithm with Component-Based Model","S. Li; Y. Li; B. Wei; Y. Wu","NA; NA; NA; NA","2009 International Conference on Computational Intelligence and Software Engineering","","2009","","","1","5","This paper presents a component-based model with a novel ranking method (CMR) for constrained evolutionary optimization. In general, many constraint-handling technique inevitably solve two important problems: (1) how to generate the feasible solutions, (2) how to direct the search to find the feasible optimal solution. For the first problem, this paper introduces a component-based model. The model is useful for exploiting valuable information from infeasible solutions and for transforming infeasible solutions into feasible ones. Furthermore, a new ranking strategy is designed for the second problem. The new algorithm is tested on several well-known benchmark functions, and the empirical results suggest that it continuously found the optimums in 30 runs and has better standard deviations for robustness and stability.","","978-1-4244-4507","10.1109/CISE.2009.5366201","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5366201","","Evolutionary computation;Constraint optimization;Robust stability;Software engineering;Computer science;Paper technology;Benchmark testing;Upper bound;Hybrid power systems;Mathematical programming","benchmark testing;constraint handling;evolutionary computation;object-oriented programming;stability","novel evolutionary algorithm;component based model;novel ranking method;constrained evolutionary optimization;constraint handling technique;generate the feasible solutions;feasible optimal solution;exploiting valuable information;transforming infeasible solutions;benchmark functions;standard deviations robustness stability","","","13","","","","","","IEEE","IEEE Conferences"
"A SQFD Approach for Service System Design Evaluation and Optimization","S. Liu; X. Xu; Z. Wang","NA; NA; NA","2009 International Conference on Interoperability for Enterprise Software and Applications China","","2009","","","23","27","To provide right service to the right customer at the right time and with reasonable price is the optimal goal of the service providers. Therefore in order to develop a competitive service system it is important to pay more attention on customerpsilas actual needs and the transformation of customerpsilas needs into the service system. With the experiences accumulated in developing and implementing some typical IT systems in manufacturing enterprises from past decade, ICES research center proposed SQFD, a lifecycle service quality assurance approach, to assist service providers to build better quality service system. SQFD not only adopted QFD approach in build-time service quality design, but also further extended it into run-time performance monitoring and evaluation, and bottom-up system optimization. As fundamental element of SQFD a model of service quality indicators is also presented in the paper.","","978-0-7695-3652","10.1109/I-ESA.2009.29","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5260882","service qualityt function deployment;service quality indicator;quality evaluation;service optimization","Design optimization;Technological innovation;Product development;Innovation management;Application software;Production systems;Companies;Context modeling;Collaboration;System testing","customer services;quality function deployment","service quality function deployment;SQFD approach;service system design;customer service;service provider;competitive service system;customer need;a lifecycle service quality assurance;build-time service quality design;run-time performance monitoring;run-time performance evaluation;bottom-up system optimization","","2","10","","","","","","IEEE","IEEE Conferences"
"Multidimensional loop fusion for low-power","D. Lazorenko","G.E. Pukhov Institute for Modeling in Energy Engineering, National Academy of Sciences of Ukraine, 15 General Naumov Street, Kyiv, 03164, Ukraine","Proceedings of IEEE East-West Design & Test Symposium (EWDTS'08)","","2008","","","92","95","Development of semiconductor technology has led to advent of complex digital systems, such as portable, embedded, SoCs, and FPGA devices. Complexity of modern applications and deep-submicron technologies make low-power design attitude compulsory. The higher the level of abstraction of a design that power optimizations are applied, the higher are potential savings. Memory is known to be extremely power consuming. A new technique of loop fusion to optimize a behavioral description of an application before the hardware/software partitioning is presented in this paper.","","978-1-4244-3402-2978-1-4244-3403","10.1109/EWDTS.2008.5580154","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5580154","","Arrays;Optimization;Power demand;Signal processing;Ribs;Power dissipation;Software","circuit optimisation;hardware-software codesign;low-power electronics;semiconductor storage","multidimensional loop fusion;semiconductor technology;deep-submicron technologies;low-power design;power optimizations;memory;behavioral description;hardware-software partitioning","","","8","","","","","","IEEE","IEEE Conferences"
"A SW performance estimation framework for early system-level-design using fine-grained instrumentation","T. Kempf; K. Karuri; S. Wallentowitz; G. Ascheid; R. Leupers; H. Meyr","Inst. for Integrated Signal Process. Syst., Aachen Univ., Germany; Inst. for Integrated Signal Process. Syst., Aachen Univ., Germany; Inst. for Integrated Signal Process. Syst., Aachen Univ., Germany; Inst. for Integrated Signal Process. Syst., Aachen Univ., Germany; Inst. for Integrated Signal Process. Syst., Aachen Univ., Germany; Inst. for Integrated Signal Process. Syst., Aachen Univ., Germany","Proceedings of the Design Automation & Test in Europe Conference","","2006","1","","6 pp.","","The increasing demands of high-performance in embedded applications under shortening time-to-market has prompted system architects in recent time to opt for multi-processor systems-on-chip (MP-SoCs) employing several programmable devices. The programmable cores provide a high amount of flexibility and reusability, and can be optimized to the requirements of the application to deliver high-performance as well. Since application software forms the basis of such designs, the need to tune the underlying SoC architecture for extracting maximum performance from the software code has become imperative. In this paper, we propose a framework that enables software development, verification and evaluation from the very beginning of MP-SoC design cycle. Unlike traditional SoC design flows where software design starts only after the initial SoC architecture is ready, our framework allows a co-development of the hardware and the software components in a tightly coupled loop where the hardware can be refined by considering the requirements of the software in a stepwise manner. The key element of this framework is the integration of a fine-grained software instrumentation tool into a system-level-design (SLD) environment to obtain accurate software performance and memory access statistics. The accuracy of such statistics is comparable to that obtained through instruction set simulation (ISS), while the execution speed of the instrumented software is almost an order of magnitude faster than ISS. Such a combined design approach assists system architects to optimize both the hardware and the software through fast exploration cycles, and can result in far shorter design cycles and high productivity. We demonstrate the generality and the efficiency of our methodology with two case studies selected from two most prominent and computationally intensive embedded application domains","1530-1591;1558-1101","3-9810801-1","10.1109/DATE.2006.243830","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1656926","","Instruments;Application software;Software performance;Hardware;Computer architecture;Software design;Statistics;Time to market;Optimized production technology;Programming","hardware-software codesign;program verification;software performance evaluation;software tools;statistics;system-on-chip","performance estimation framework;fine-grained instrumentation;time-to-market;multiprocessor systems-on-chip;application software;software development;software verification;software evaluation;software instrumentation tool;system-level-design environment;software performance;memory access statistics;instruction set simulation","","29","23","","","","","","IEEE","IEEE Conferences"
"A Comparative Landscape Analysis of Fitness Functions for Search-Based Testing","R. Lefticaru; F. Ipate","NA; NA","2008 10th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing","","2008","","","201","208","Landscape analysis of fitness functions is an important topic. This paper makes an attempt to characterize the search problems associated with the fitness functions used in search-based testing, employing the following measures: diameter, autocorrelation and fitness distance correlation. In a previous work, a general form of objective functions for structural search-based software testing was tailored for state-based testing. A comparison is performed in this paper between the general fitness functions and some problem-specific fitness functions, taking into account their performance with different search methods.","","978-0-7695-3523","10.1109/SYNASC.2008.69","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5204812","search-based testing;finite state machines;metaheuristic search techniques;fitness functions;landscape analysis","Automatic testing;Software testing;Search methods;Particle swarm optimization;Search problems;Genetic algorithms;Simulated annealing;Application software;Scientific computing;Algorithm design and analysis","finite state machines;program testing;search problems","landscape analysis;fitness function;search-based testing;search problem;fitness distance correlation;search-based software testing;state-based testing;finite state machine","","4","34","","","","","","IEEE","IEEE Conferences"
"CacheBit: A Multisource-Multitarget Cache Instrumentation Tool","C. Xu; J. Gu; L. Lin; A. Liang; H. Guan","NA; NA; NA; NA; NA","2009 Third IEEE International Conference on Secure Software Integration and Reliability Improvement","","2009","","","35","39","The cache utilization seriously impacts on program performance, whereas the cache behavior is transparent to developers so that they cannot get opportunity to optimize their programs by reducing cache miss rates. This paper introduces a useful instrumentation tool, Cachebit, which is built with Crossbit - a dynamic binary translation (DBT) framework with intermediate instruction layer. Cachebit simulates cache behavior and presents statistics of cache profile at runtime. After running programs on Crossbit with Cachebit available, cache profile information can be reported to help developers rewrite and improve their programs. Moreover, by setting different cache characteristics (including cache size, block size, associative pattern, replacement policy, hierarchy number), Cachebit can simulate a variety of real cache architectures. Comparing with other analogous tools, Cachebit gives a multisource-multitarget testing platform, that is, programs can be tested on isomerous platforms.","","978-0-7695-3758","10.1109/SSIRI.2009.72","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5325393","instrumentation;cahce simulating;dynamic binary translation","Instruments;Testing;Delay;Software performance;Software tools;Runtime;Chaos;Computer science;Statistics;Microcomputers","cache storage;program testing;software performance evaluation","CacheBit;multisource-multitarget cache instrumentation tool;program performance;optimization;Crossbit;dynamic binary translation;program testing","","","10","","","","","","IEEE","IEEE Conferences"
"An Improved Discrete Particle Swarm Optimization Algorithm for a Single Batch-Processing Machine with Non-identical Job Sizes","D. Lu; H. Chen; W. Zhang; R. Xu","NA; NA; NA; NA","2009 10th ACIS International Conference on Software Engineering, Artificial Intelligences, Networking and Parallel/Distributed Computing","","2009","","","87","92","This paper aims at minimizing the makespan for a single batch-processing machine with non-identical job sizes (NSBM) using discrete particle swarm optimization algorithm (DPSO). PSO is a natural continuous function algorithm and there is some obstacle to solve combinatorial optimization problems. Recently, some hybrid PSO or DPSO have been presented by researchers to solve discrete problems in practical applications but no application to NSBM problems hitherto. DPSO is improved in some operators, adaptive factors and some other details in this paper. The computational simulation results and comparisons show that the DPSO algorithm is competitive for the NSBM problems.","","978-0-7695-3642","10.1109/SNPD.2009.76","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5286689","batch-processing machines;non-identical job sizes;scheduling;discrete particle swarm optimization algorithm","Particle swarm optimization;Job shop scheduling;Scheduling algorithm;Software algorithms;Processor scheduling;Circuit testing;Ant colony optimization;Stochastic processes;Software engineering;Artificial intelligence","batch processing (industrial);combinatorial mathematics;particle swarm optimisation;single machine scheduling","discrete particle swarm optimization algorithm;single batch-processing machine;continuous function algorithm;combinatorial optimization problems;nonidentical job sizes","","","20","","","","","","IEEE","IEEE Conferences"
"Tuning Word Retiming of a Digitally-Controlled Oscillator Using RF Built-In Self Test","I. Bashir; R. B. Staszewski; O. Eliezer","Texas Instruments, Dallas, TX 75243, USA. email: imran.bashir@ti.com; Texas Instruments, Dallas, TX 75243, USA; Texas Instruments, Dallas, TX 75243, USA","2006 IEEE Dallas/CAS Workshop on Design, Applications, Integration and Software","","2006","","","103","106","This paper represents an innovative solution to reduce phase noise present on the GMSK modulated spectrum in an all-digital RF transmitter employing a discrete-time oscillator. The root cause of this issue is the improper timing of oscillator tuning word update with respect to the digitally-controlled oscillator (DCO) capacitor state. Using the built-in self test (BIST) circuit in the all-digital PLL (ADPLL), the DCO tuning word update timing is calibrated such that the impact on the phase noise is compensated and desired modulated spectrum margin from the specification is achieved","","1-4244-0669-21-4244-0670","10.1109/DCAS.2006.321044","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4115123","","Tuning;Oscillators;Radio frequency;Automatic testing;Phase noise;Phase modulation;Timing;Circuit optimization;Circuit testing;Transmitters","built-in self test;circuit tuning;digital phase locked loops;interference suppression;minimum shift keying;radiofrequency oscillators;transmitters","digitally-controlled oscillator;RF built-in self test;phase noise reduction;GMSK modulated spectrum;all-digital RF transmitter;discrete-time oscillator;oscillator tuning word update timing;built-in self test circuit;all-digital PLL;DCO tuning word update timing","","5","5","","","","","","IEEE","IEEE Conferences"
"Search Based Software Engineering for Program Comprehension","M. Harman","King's College London, UK","15th IEEE International Conference on Program Comprehension (ICPC '07)","","2007","","","3","13","Search based software engineering (SBSE) is an approach to software engineering in which search based optimization algorithms are used to identify optimal or near optimal solutions and to yield insight. SBSE techniques can cater for multiple, possibly competing objectives and/or constraints and applications where the potential solution space is large and complex. Such situations are common in software engineering, leading to an increasing interest in SBSE. This paper provides a brief overview of SBSE, explaining some of the ways in which it has already been applied to program-comprehension related activities. The paper also outlines some possible future applications of and challenges for the further application of SBSE to program comprehension.","1092-8138","0-7695-2860","10.1109/ICPC.2007.35","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4268235","","Software engineering;Application software;Software testing;Biomedical engineering;Software algorithms;Educational institutions;Cost function;Automatic testing;Optimizing compilers;Quality assessment","optimisation;software engineering","program comprehension;search based software engineering;optimization algorithms","","16","82","","","","","","IEEE","IEEE Conferences"
"Optimizing the Power Consumption of Economic Agents with the help of an Intelligent Analysis and Decision System","I. Miciu","Dipl. Eng., IPA SA - AUTOMATION ENGINEERING; Calea Floreasca 167b, 72321 Bucharest, Romania. Tel: (+40-1) 3180026, Fax: (+40-1) 3181620, E-mail: ionmiciu@ipa.ro","2006 IEEE International Conference on Automation, Quality and Testing, Robotics","","2006","1","","381","386","The described automatic system ensures the optimizing of electric network operating through the decreasing or elimination of power losses in the local consumption network as a result of the detection and correction, on operator command, of the operating deficiencies of the consumers connected to this network. The automatic optimizing system accomplishes the following functions: 1) the command of the medium voltage network that feeds the consumers in the feeding points tracking the state of the specific parameters (active and reactive electric power, power factor, voltage, current, frequency, harmonics) and the deviations from the pre-established domains of these parameters; this command is made on three levels: level 1: the feeding points toward the system (transforming stations TS); level 2: the feeding points toward the consumers (the feeding points FP, the transforming posts TP); level 3: the command, control and actuating equipments of the consumers. 2) the command of the electric parameters specific to the consumers with the purpose of optimizing their operating; 3) the automated identification, by the optimizing system, of the consumers connected to the network following the algorithm: 4) the consumer type (resistive, capacitive, inductive, etc.); 5) the importance of the consumer from the technological process, in which it is involved, point of view; 6) the connection point to the network; 7) the type reactive power of the consumer; 8) the type active power of the consumer; 9) the real reactive power of the consumer (size and circulating way); 10) the real active power of the consumer; 11) the (measured) real and type electric parameters of the consumer (voltage, current, power factor, operating regimes, etc.) and their deviations from the pre-established domains; 12) the operating way of the consumer (the regime operating period, the real operating period, the real and type rated capacity); 13) the fault rate on a pre-established period; 14) the wear degree of the respective consumer","","1-4244-0360-X1-4244-0361","10.1109/AQTR.2006.254565","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4022887","electric networks;feeding stations;electric energy;optimizing;automation;software","Energy consumption;Power system economics;Power generation economics;Intelligent agent;Reactive power;Reactive power control;Medium voltage;Feeds;Frequency;Power system harmonics","decision support systems;distribution networks;knowledge based systems;power consumption;power engineering computing;power stations","power consumption;economic agents;intelligent analysis;decision system;electric network optimization;local consumption network;automatic optimizing system;medium voltage network;automated identification;type reactive power;type active power;electric networks;feeding stations;electric energy","","","5","","","","","","IEEE","IEEE Conferences"
"Research on Least Squares Support Vector Machine Combinatorial Optimization Algorithm","L. Taian; W. Yunjia; L. Wentong","NA; NA; NA","2009 International Forum on Computer Science-Technology and Applications","","2009","1","","452","454","LS-SVM(least squares support vector machine) has been widely used in engineering practice. However, the solving of LS-SVM still remains difficult under the condition of large sample. Based on algorithm of combinatorial optimization, this paper put forward the combinatorial optimization least squares support vector machine algorithm. On several different data aggregation of dimensions, the numerical value experiment and comparison are carried out on traditional LS-SVM algorithm, COLS-SVM algorithm and its improvement algorithm. The numerical value test has shown that COLS-SVM algorithm and its improvement algorithm are effective and have certain advantages on time and regression accuracy, compared with traditional LS-SVM algorithm.","","978-1-4244-5423-5978-1-4244-5422-8978-0-7695-3930","10.1109/IFCSTA.2009.116","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5385037","Least squares support vector machine;Sparse method;Combinatorial optimization algorithm;Linear equations least squares support vector machine","Least squares methods;Support vector machines;Equations;Optimization methods;Sparse matrices;Testing;Computer applications;Application software;Educational institutions;Informatics","combinatorial mathematics;least squares approximations;optimisation;support vector machines","least squares support vector machine;SVM;combinatorial optimization algorithm;data aggregation","","1","13","","","","","","IEEE","IEEE Conferences"
"Performance assessment of DMOEA-DD with CEC 2009 MOEA competition test instances","M. Liu; X. Zou; Y. Chen; Z. Wu","School of Computer Sciences, Wuhan University, 430072, China; School of Mathematics and Statistics, Wuhan University, 430072, China; School of Mathematics and Statistics, Wuhan University, 430072, China; State Key Laboratory of Software Engineering, Wuhan University, 430072, China","2009 IEEE Congress on Evolutionary Computation","","2009","","","2913","2918","In this paper, the DMOEA-DD, which is an improvement of DMOEA by using domain decomposition technique, is applied to tackle the CEC 2009 MOEA competition test instances that are multiobjective optimization problems (MOPs) with complicated Pareto set (PS) geometry shapes. The performance assessment is given by using IGD as performance metric.","1089-778X;1941-0026","978-1-4244-2958-5978-1-4244-2959","10.1109/CEC.2009.4983309","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4983309","","Testing;Evolutionary computation;Thermodynamics;Shape;Entropy;Pareto optimization;Computational geometry;Measurement;Genetics;Temperature distribution","evolutionary computation;geometry;Pareto optimisation;set theory","DMOEA-DD;CEC 2009 MOEA competition test instance;domain decomposition technique;multiobjective optimization problem;Pareto set geometry shape;dynamical multiobjective evolutionary algorithm;performance metric","","24","8","","","","","","IEEE","IEEE Conferences"
"Optimum Design of Circuit Fault Diagnosis Software Based on Fault Dictionary Method","W. Su; T. Fan; Y. Wang","NA; NA; NA","2008 International Symposium on Intelligent Information Technology Application Workshops","","2008","","","749","752","This paper discusses the design procedure of digital circuit fault diagnosis software by using optimization method of data stream through the analysis of the file format of LASAR circuit simulation result fault dictionary, pin connection table and node truth table.","","978-0-7695-3505","10.1109/IITA.Workshops.2008.122","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4732045","","Fault diagnosis;Circuit faults;Dictionaries;Circuit testing;Circuit simulation;Software testing;Digital circuits;System testing;Application software;Probes","circuit CAD;circuit simulation;fault diagnosis;optimisation","optimum design;fault dictionary method;digital circuit fault diagnosis software;optimization method;data stream;file format;circuit simulation;pin connection table;node truth table","","","6","","","","","","IEEE","IEEE Conferences"
"Memetic Algorithm with Local Search Chaining for Continuous Optimization Problems: A Scalability Test","D. Molina; M. Lozano; F. Herrera","NA; NA; NA","2009 Ninth International Conference on Intelligent Systems Design and Applications","","2009","","","1068","1073","Memetic algorithms arise as very effective algorithms to obtain reliable and high accurate solutions for complex continuous optimization problems. Nowadays, higher dimensional optimization problems are an interesting field of research, that introduces new problems for the optimization process, making recommendable to test the scalable capacities of optimization algorithms. In particular, in memetic algorithms, a higher dimensionality increases the domain space around each solution, requiring that the local search method must be applied with a high intensity. In this work, we present a preliminar study of a memetic algorithm that assigns to each individual a local search intensity that depends on its features, by chaining different local search applications. This algorithm has obtained good results in continuous optimization problems and we study whether, using this intensity adaptation mechanism with the scalable LS method MTS-LS2, the algorithm is scalable enough for being a good algorithm for medium and high-dimensional problems. Experiments are carried out to test the ability of being scalable, and results obtained show that the proposal is scalable in many of the functions, scalable and non-scalable, of the benchmark used.","2164-7143;2164-7151","978-1-4244-4735-0978-0-7695-3872","10.1109/ISDA.2009.143","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5364186","memetic algorithm;adaptive local search intensity;large scale problems;continuous optimization","Scalability;Design optimization;Proposals;Intelligent systems;Artificial intelligence;Application software;System testing;Computer languages;Computer science;Search methods","optimisation;search problems","memetic algorithm;local search chaining;scalability test;complex continuous optimization problem;higher dimensional optimization problem;local search intensity;intensity adaptation mechanism;scalable LS method","","8","17","","","","","","IEEE","IEEE Conferences"
"Lightweight Elicitation and Analysis of Software Product Quality Goals: A Multiple Industrial Case Study","J. Vanhanen; M. V. Mäntylä; J. Itkonen","NA; NA; NA","2009 Third International Workshop on Software Product Management","","2009","","","42","52","We developed and used a method that gathers relevant stakeholders to elicit, prioritize, and elaborate the quality goals of a software product. It is designed to be lightweight and easy to learn compared to methods for a more comprehensive analysis of non-functional requirements. The method and the resulting quality goals are meant especially for improving the software product management process. We used it in four software product companies, and report lessons learned and evaluation of the method based on practitioners' comments. We found it better to set the goals first for the product in general before discussing a specific release project. In addition to identifying goals that needed improvement, the practitioners considered identifying already achieved goals relevant, but they were neg- lected unless explicitly considered. Using ISO 9126 as a checklist after brainstorming did not add many goals. Prioritization was challenging due to numerous relevant perspectives. Conceiving measures for impor- tant goals seemed to concretize them.","","978-1-4244-7693-0978-0-7695-4098","10.1109/IWSPM.2009.5","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5457326","","Software quality;Computer industry;Programming;Computer architecture;Usability;Product design;Quality management;ISO standards;Software maintenance;Software testing","formal specification;ISO standards;software management;software quality","lightweight elicitation;software product quality analysis;multiple industrial case study;nonfunctional requirements;software product management process;ISO 9126","","6","20","","","","","","IEEE","IEEE Conferences"
"Research of Hydropower Stations Optimal Operation Based on the Discrete Differential Dynamic Programming - Progressive Optimization Algorithm Combination Method","X. Yingji; M. Yadong; X. Jiangsong","NA; NA; NA","2009 Seventh ACIS International Conference on Software Engineering Research, Management and Applications","","2009","","","25","29","The progressive optimization algorithm over-depends on the initial state, it needs to assume certain different types of initial test strategies and corresponding initial test paths, the calculation process repeats 2 to 5 times. Considering the shortfalls mentioned above, this paper presents discrete differential dynamic programming-progressive optimum combination method which is applied in the operation of hydropower station and designs a solution based on the method to optimize the operation of hydropower stations. Calculation example shows that this method is effective, highly reliable in global optimization and full of prospects.","","978-0-7695-3903","10.1109/SERA.2009.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381814","discrete differential dynamic programming progressive optimality algorithm optimal operation","Hydroelectric power generation;Dynamic programming;Optimization methods;Reservoirs;Water storage;Water resources;Software algorithms;Electronic mail;Software testing;Power system security","dynamic programming;hydroelectric power stations","hydropower stations optimal operation;discrete differential dynamic programming;progressive optimization algorithm combination method;global optimization","","1","6","","","","","","IEEE","IEEE Conferences"
"Application and Comparison of Particle Swarm Optimization and Genetic Algorithm in Strategy Defense Game","P. Huo; S. C. K. Shiu; H. Wang; B. Niu","NA; NA; NA; NA","2009 Fifth International Conference on Natural Computation","","2009","5","","387","392","Particle swarm optimization (PSO) is similar to genetic algorithm (GA) but employs different strategies and computational effort. Strategic defense military games require a high degree of coordination among the characters and thus are suitable to test the performance of algorithms. In this paper, we design a scenario of tower defense game and compare the performance of PSO and GA in terms of the damage value (fitness) and the convergence speed. The comparative analysis shows the similar optimum cannon placement is obtained using PSO and GA with similar effectiveness. In addition, the results of execution time (&gt;80 seconds) indicate that the single implement of PSO or GA is unsatisfied for real time strategy (RTS) games.","2157-9555;2157-9563","978-0-7695-3736","10.1109/ICNC.2009.552","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5364640","Particle swarm optimization;genetic algorithm;Strategy Defense Game","Particle swarm optimization;Genetic algorithms;Military computing;Artificial intelligence;Testing;Poles and towers;Computer applications;Convergence;Computer industry;Application software","computer games;genetic algorithms;military systems;particle swarm optimisation","particle swarm optimization;genetic algorithm;real time strategy defense military game;tower defense game;PSO;GA;damage value;convergence speed;optimum cannon placement;execution time","","7","16","","","","","","IEEE","IEEE Conferences"
"Scalability solutions for program comprehension through dynamic analysis","A. Zaidman","Dept. of Math. & Comput. Sci., Antwerp Univ., Belgium","Conference on Software Maintenance and Reengineering (CSMR'06)","","2006","","","4 pp.","330","Dynamic analysis has long been a subject of study in the context of (compiler) optimization, program comprehension, test coverage, etc. Ever-since, the scale of the event trace has been an important issue. This scalability issue finds its limits on the computational front, where time and/or space complexity of algorithms become too large to be handled by a computer, but also on the cognitive front, where the results presented to the user become too large to be easily understood. This research focuses on delivering a number of program comprehension solutions that help software engineers to focus on the software system during their initial program exploration and comprehension phases. The key concepts we use in our techniques are ""frequency of execution"" and runtime ""coupling"". To validate our techniques we used a number of open-source software systems, as well as an industrial legacy application","1534-5351","0-7695-2536","10.1109/CSMR.2006.46","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1602389","","Scalability;Open source software;Software systems;Program processors;Optimizing compilers;Testing;Frequency;Runtime;Computer industry;Application software","reverse engineering;software maintenance;system monitoring","program comprehension;dynamic analysis;event trace;time complexity;space complexity;software engineering;program exploration;open-source software systems;industrial legacy application","","6","13","","","","","","IEEE","IEEE Conferences"
"Effectively Combining Software Verification Strategies: Understanding Different Assumptions","D. Owen; D. Desovski; B. Cukic","West Virginia University, Morgantown, WV; West Virginia University, Morgantown, WV; West Virginia University, Morgantown, WV","2006 17th International Symposium on Software Reliability Engineering","","2006","","","321","330","In this paper we describe an experiment in which inconsistent results between two tools for testing formal models (and a third used to determine which of the two was correct) led us to a more careful look at the way each tool was being used and a clearer understanding of the output of the tools. For the experiment, we created error-seeded versions of an SCR specification representing a real-world personnel access control system. They were checked using the model checker SPIN and Lurch, our random testing tool for finite-state models. In one case a property violation was detected by Lurch, an incomplete tool, but missed by SPIN, a model checking tool designed for complete verification. We used the SCR Toolset and the Salsa invariant checker to determine that the violation detected by Lurch was indeed present in the specification. We then looked more carefully at how we were using SPIN in conjunction with the SCR Toolset and, eventually, made adjustments so that SPIN also detected the property violation initially detected only by Lurch. Once it was clear the tools were being used correctly and would give consistent results, we did an experiment to determine how they could be combined to optimize completeness and efficiency. We found that combining tools made it possible to verify the specifications faster and with much less memory in most cases","1071-9458;2332-6549","0-7695-2684","10.1109/ISSRE.2006.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021998","","Thyristors;Software tools;Software testing;Logic;Computer science;Error correction;Personnel;Access control;Costs;Automatic testing","formal specification;program verification;software tools","software verification;formal model testing;formal specification;model checking;SPIN;Lurch;random testing tool;finite-state models;property violation;Software Cost Reduction Toolset;Salsa invariant checker","","2","20","","","","","","IEEE","IEEE Conferences"
"Implementation of non-traditional optimization techniques (PSO, CPSO, HDE) for the optimal load flow solution","R. Mageshvaran; I. Jacob Raglend; V. Yuvaraj; P. G. Rizwankhan; T. Vijayakumar; Sudheera","School of Electrical Sciences, Vellore Institute of Technology, India; School of Electrical Sciences, Vellore Institute of Technology, India; School of Electrical Sciences, Vellore Institute of Technology, India; School of Electrical Sciences, Vellore Institute of Technology, India; School of Electrical Sciences, Vellore Institute of Technology, India; School of Electrical Sciences, Vellore Institute of Technology, India","TENCON 2008 - 2008 IEEE Region 10 Conference","","2008","","","1","6","This paper presents an approach to obtain the optimal load flow solution using three different intelligent techniques such as particle swarm optimization (PSO), crazy particle swarm optimization (CPSO) and hybrid differential evolution (HDE) subject to various system constraints. The above optimization techniques have a capability to provide global optimal solution in problem domains where a complete traversion of the whole search space is completely infeasible. The proposed method has been tested on Ward and Hale six bus system and IEEE-14 bus test system. The solutions obtained are quite encouraging and useful in solving the optimal load flow problem. The algorithm and simulation are carried using Matlab software.","2159-3442;2159-3450","978-1-4244-2408-5978-1-4244-2409","10.1109/TENCON.2008.4766839","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4766839","Particle Swarm Optimization;Crazy Particle Swarm Optimization;Hybrid Differential Evolution;Optimal Load Flow","Load flow;Voltage;Particle swarm optimization;Reactive power;Power system analysis computing;Power system modeling;Power generation;Equations;Jacobian matrices;Load flow analysis","evolutionary computation;load flow;particle swarm optimisation;power engineering computing","nontraditional optimization technique;optimal load flow solution;crazy particle swarm optimization;hybrid differential evolution;Ward-Hale six bus system;IEEE-14 bus test system","","4","11","","","","","","IEEE","IEEE Conferences"
"An Improved Ant Colony Optimization and Its Applications in Flow-Shop Problems","X. Song; K. Wang; Y. Xiao","NA; NA; NA","2009 International Conference on Computational Intelligence and Software Engineering","","2009","","","1","4","Ant colony optimization (ACO) is easily relapsed into local optimization and stagnation. In order to ameliorate this problem existed in ACO, several new improvements are proposed and evaluated. Such as, stochastic search strategy and pheromone mutation were inducted. Then an improved ant colony optimization with pheromone mutation (PMACO) was put forward. It was tested by a set of benchmark travelling salesman problems from the travelling salesman problem library and some flow-shop problems. The results of the examples show that it can not easily run into the local optimum and can converge at the global optimum. It performs better than the other algorithms such as genetic algorithm in solving flowshop problems.","","978-1-4244-4507","10.1109/CISE.2009.5363536","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5363536","","Ant colony optimization;Genetic mutations;Cities and towns;Traveling salesman problems;Stochastic processes;Genetic algorithms;Application software;Benchmark testing;Libraries;Optimization methods","flow shop scheduling;genetic algorithms;travelling salesman problems","flow-shop problems;local optimization;local stagnation;stochastic search strategy;improved ant colony optimization with pheromone mutation;genetic algorithm;benchmark travelling salesman problems;travelling salesman problem library","","","4","","","","","","IEEE","IEEE Conferences"
"Design optimisation of electromagnetic devices using continuum design sensitivity analysis combined with commercial EM software","D. -. Kim; J. K. Sykulski; D. A. Lowther","NA; NA; NA","IET Science, Measurement & Technology","","2007","1","1","30","36","The paper deals with two types of optimisation problem: optimised source distribution and the shape optimum design, using continuum design sensitivity analysis (CDSA) in combination with standard electromagnetic (EM) software. Fast convergence and compatibility with existing EM software are the distinctive features of the proposed implementation. To verify the advantages and also to facilitate understanding of the method itself, two design optimisation problems have been tested using both 2D and 3D models: the first is an MRI design problem related to finding an optimum permanent magnet distribution, and the second is a pole shape design problem to reduce the cogging torque in a BLDC","1751-8822;1751-8830","","10.1049/iet-smt:20060024","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4105897","","","computational electromagnetics;electromagnetic devices;optimisation","design optimisation;electromagnetic devices;continuum design sensitivity analysis;EM software;optimised source distribution;shape optimum design;CDSA;2D models;3D models;MRI design problem;pennanent magnet distribution;pole shape design problem;cogging torque;BLDC","","7","","","","","","","IET","IET Journals & Magazines"
"Evolution and Search Based Metrics to Improve Defects Prediction","S. Kpodjedo; F. Ricca; G. Antoniol; P. Galinier","NA; NA; NA; NA","2009 1st International Symposium on Search Based Software Engineering","","2009","","","23","32","Testing activity is the most widely adopted practice to ensure software quality. Testing effort should be focused on defect prone and critical resources i.e., on resources highly coupled with other entities of the software application.In this paper, we used search based techniques to define software metrics accounting for the role a class plays in the class diagram and for its evolution over time. We applied Chidamber and Kemerer and the newly defined metrics to Rhino, a Java ECMA script interpreter, to predict version 1.6R5 defect prone classes. Preliminary results show that the new metrics favorably compare with traditional object oriented metrics.","","978-0-7695-3675","10.1109/SSBSE.2009.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5033176","Defects Prediction;Software evolution;Error-Correcting Graph Matching (ECGM) algorithm;Class Rank;Evolution Cost","Chromium;Software testing;Software quality;Software engineering;Java;Costs;Object oriented modeling;Predictive models;Regression tree analysis;Application software","error correction;software engineering","defects prediction;software evolution;error-correcting graph matching;class rank;evolution cost","","6","24","","","","","","IEEE","IEEE Conferences"
"Optimal High-Resolution Spectral Analyzer","A. Tchegho; H. Mattes; S. Sattler","Institute of Electronic Design Automation, Technische Universitaet Muenchen, Munich, Germany. aurelien.tchegho@tum.de; Dept. of Analog Design for Test, Infineon Technologies, Munich, Germany. heinz.mattes@infineon.com; Dept. of Analog Design for Test, Infineon Technologies, Munich, Germany. sebastian.sattler@infineon.com","2008 Design, Automation and Test in Europe","","2008","","","62","67","This paper presents a new application field for the Goertzel algorithm. The test of mixed-signal circuits involves the generation and analysis of signals. A standard method for the signal analysis is the fast Fourier transform (FFT algorithms). Such complex algorithms are not suitable for BIST (built-in self-test) or BOST (built-off self-test) solutions due to their high demand for resources. In this paper, the Goertzel algorithm will be presented as an alternative to FFT algorithms. A new optimized structure of the Goertzel algorithm and its implementation in an FPGA (field programmable gate array) is presented. A comparison within the scope of the production test of RF transceiver devices shows a considerable reduction of the test time (factor 6) and resources (factor 10) compared to a FFT software solution respectively hardware solution.","1530-1591;1558-1101","978-3-9810801-3-1978-3-9810801-4","10.1109/DATE.2008.4484661","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4484661","","Spectral analysis;Built-in self-test;Circuit testing;Signal analysis;Field programmable gate arrays;Software testing;Signal generators;Fast Fourier transforms;Production;Radio frequency","field programmable gate arrays;logic testing;spectral analysers","high-resolution spectral analyzer;Goertzel algorithm;mixed-signal circuit testing;signal generation;FPGA;Field Programmable Gate Array;RF transceiver production test","","2","13","","","","","","IEEE","IEEE Conferences"
"Electronic circuit design using multiobjective optimization","J. Michal; J. Michal; J. Dobes","Czech Tech. Univ. in Prague, Prague; Klicperova 12, Prague; Czech Tech. Univ. in Prague, Prague","2007 50th Midwest Symposium on Circuits and Systems","","2007","","","734","737","This paper presents a variation and extension of a previously existing method for multiobjective optimization known as goal attainment method (GAM). The method GAM is in this research combined with a mechanism that automatically provides a set of parameters (weights, coordinates of the reference point) for which the method generates noninferior solutions uniformly spread over a suitably chosen part of the Pareto front. The resulting set of solutions is then presented in a graphic form to the designer so that the solution representing the most satisfactory tradeoff can be easily chosen. The whole algorithm was implemented as a program and tested on two RF design examples (an LNA and a power amplifier), whose optimization results are also presented in the paper.","1548-3746;1558-3899","978-1-4244-1175-7978-1-4244-1176","10.1109/MWSCAS.2007.4488683","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4488683","Multiobjective optimization;Pareto optimality;Pareto front","Electronic circuits;Design optimization;Constraint optimization;Algorithm design and analysis;Weight control;Circuit synthesis;Design engineering;Silicon;Software systems;Graphics","circuit optimisation;integrated circuit design;Pareto optimisation","electronic circuit design;multiobjective optimization;goal attainment method;Pareto front","","2","5","","","","","","IEEE","IEEE Conferences"
"Hardware-in-the-Loop Simulation Software for Regulator Tests and Optimization","A. Beguin; P. Allenbach; S. Keller; J. Simond; S. Brausewetter; J. Koutnik","NA; NA; NA; NA; NA; NA","2007 IEEE Industry Applications Annual Meeting","","2007","","","2422","2428","This paper presents a real-time extension, called SIMSEN-RT, of the simulation software SIMSEN. SIMSEN-RT allows carrying out hardware-in-the-loop (HIL) simulations of power systems using hardware regulators. It can therefore be used to fine-tune regulator parameters or to test the regulator behavior in different operating conditions (emergency shutdown, islanded operation, etc.). SIMSEN-RT runs on a standard PC and needs no expensive hardware. As it is based on an existing simulation tool, hardware regulators can be tested in any power system configuration. SIMSEN RT will be used by Voith Siemens Hydro for HIL tests of turbine governors and voltage controllers.","0197-2618","978-1-4244-1259-4978-1-4244-1260","10.1109/07IAS.2007.365","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4348114","","Regulators;Software testing;Power system simulation;Hardware;System testing;Circuit testing;Circuit simulation;Software standards;Power generation;Real time systems","power engineering computing;power system control;voltage regulators","hardware-in-the-loop simulation software;SIMSEN-RT simulation software;hardware regulators;power system control;emergency shutdown;voltage controllers","","1","11","","","","","","IEEE","IEEE Conferences"
"A Genetic Algorithm Based Method of Fault Maintenance in Software-Intensive System","B. XIiong; L. Zhang; N. Yang; J. Li","NA; NA; NA; NA","2009 First International Workshop on Education Technology and Computer Science","","2009","3","","1056","1059","This paper analyzes the problem of fault maintenance in software-Intensive system. The problem is described and modeled by a genetic algorithm (GA) based method. After a complete investigation on elements involved in the problem, the controls parameters in GA are confirmed. The feasibility and the degree of accuracy of the GA based method are tested by an example in the end.","","978-0-7695-3557-9978-1-4244-3581","10.1109/ETCS.2009.774","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4959489","Software Intensive System;Method of Fault Maintenance;Genetic Algorithm","Genetic algorithms;Software maintenance;Equations;Hardware;Optimization methods;Computer science education;Educational technology;Paper technology;Computer science;Algorithm design and analysis","genetic algorithms;software fault tolerance;software maintenance","genetic algorithm based method;fault maintenance;software-intensive system","","","8","","","","","","IEEE","IEEE Conferences"
"The IP Lookup Mechanism in a Linux Software Router: Performance Evaluation and Optimizations","R. Bolla; R. Bruschi","DIST - Department of Communications, Computer and Systems Science, University of Genoa, Via Opera Pia 13, 16145 Genova, Italy. raffaele.bolla@unige.it; DIST - Department of Communications, Computer and Systems Science, University of Genoa, Via Opera Pia 13, 16145 Genova, Italy. roberto.bruschi@unige.it","2007 Workshop on High Performance Switching and Routing","","2007","","","1","6","In the last years, networking device architectures based on open source software, like Linux, have aroused lively interest from both scientific and industrial communities. The key to this success can probably be found in the flexibility and fast development time of the software approach, and the reliability level guaranteed by large communities of users and developers. In this contest, our aim is to customize, to analyze and to optimize a Linux based architecture for an exclusive networking use. In particular, the objective of this work is to study, to evaluate the performance and to optimize the IP lookup mechanism included in the Linux kernel. With this aim we have released a set of kernel patches to enhance the performance of the system by optimizing the IP lookup mechanism. We present a complete set of benchmarking results with both internal and external measurements.","2325-5552;2325-5560","1-4244-1205-61-4244-1206","10.1109/HPSR.2007.4281242","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4281242","Open Router;IP lookup;Linux Router","Linux;Software performance;Kernel;Open source software;Testing;Computer architecture;Performance analysis;Computer industry;Computer networks;Communication industry","IP networks;Linux;operating system kernels;public domain software;software reliability","IP lookup mechanism;Linux software router;open source software;software reliability;Linux kernel","","6","23","","","","","","IEEE","IEEE Conferences"
"Bridge the gap between simulation and test: An OSA-compliant Virtual Test Environment","P. Lu; D. Glaser; G. Uygur; S. Weichslgartner; K. Helmreich; A. Lechner","Reliable Circuits and Systems, Friedrich-Alexander-University Erlangen-Nuremberg, Paul-Gordan-Str. 5, 91052, Germany; Reliable Circuits and Systems, Friedrich-Alexander-University Erlangen-Nuremberg, Paul-Gordan-Str. 5, 91052, Germany; Reliable Circuits and Systems, Friedrich-Alexander-University Erlangen-Nuremberg, Paul-Gordan-Str. 5, 91052, Germany; Reliable Circuits and Systems, Friedrich-Alexander-University Erlangen-Nuremberg, Paul-Gordan-Str. 5, 91052, Germany; Reliable Circuits and Systems, Friedrich-Alexander-University Erlangen-Nuremberg, Paul-Gordan-Str. 5, 91052, Germany; Konrad-Technologie GmbH, Fritz-Reichele-Ring. 5, 78315 Radolfzell, Germany","2009 IEEE AUTOTESTCON","","2009","","","214","219","Virtual test (VT) is a promising technique that facilitates test development and cuts time-to-market especially for analog/mixed-signal/RF devices. While the concept has been around for more than a decade and its benefits are widely acknowledged, to date it has not become a standard technique in the day-to-day business of test development. The major difficulties are: weak integration between test environment and simulation environment, lack of flexible and sophisticated simulation library for test resource and insufficient simulation efficiency. The paper discusses solutions for addressing the gaps and presents a platform independent - yet easy integratable virtual test environment (VTE). It is achieved by following steps: first, OSA-compliance - using industry standards with wide acceptance including data format, service API and protocols to pave the way for easy integration. Second, achieving interaction with the test program by modeling test resources with software control interface, this allows simulation models to be dynamically assembled together and exchanging information with the test program during run-time. At last, modeling optimization - the models of test resources in our VTE have different abstraction levels supporting both static and dynamic assembly during dynamic test program execution. Analysis is performed with respect to accuracy vs. efficiency during model assembling phase to cut computational burden. In the end, a seamless integration of VT approach into test development flow is explained.","1088-7725;1558-4550","978-1-4244-4980-4978-1-4244-4981","10.1109/AUTEST.2009.5314059","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5314059","","Bridges;Software testing;Assembly;Time to market;Radio frequency;Standards development;Libraries;Protocols;Runtime;Performance analysis","application program interfaces;digital simulation;program testing;software libraries;system monitoring","OSA-compliant VTE;Open Standard Approach;virtual test environment;time-to-market;simulation environment;sophisticated simulation library;service API;application programming interface;test program;software control interface;run-time behaviour","","","14","","","","","","IEEE","IEEE Conferences"
"Optimizing probabilities of real-time test case execution","N. Wolovick; P. R. D'Argenio; H. Qu","Universidad Nacional de Cordoba; Universidad Nacional de Cordoba; Imperial College","2009 International Conference on Software Testing Verification and Validation","","2009","","","1","10","Model-based test derivation for real-time system has been proven to be a hard problem for exhaustive test suites. Therefore, techniques for real-time testing do not aim to exhaustiveness but Instead respond to particular coverage criteria. Since it Is not feasible to generate complete test suites for real time systems, It IsI very Important that test case are executed In a way that they can achieve the best possible resuIlt As a consequence, It is imperative to Increase the probabilty of success of a test case execution (by 'success' we actually mean 'the test finds an error'). This work presents a technique to guide the execution of a test case towards a particular objective with the highest possible probability. Thke technique takes as a starting point a model described In terms of an input/output stochastic automata, where input actions are fully controlled by the tester and the occurrece time of output action responds to uniform distributions. Derived test cases are sequences of Inputs and outputs actions. This work discusses several techniques to obtain the optimum times In which the tester must feed the inputs of the test case in order to achieve maxhmum probabilty of success in a test case execution. In particular~, we show this optimization problem Is equivalent to maximizing the sectional volume of a convex polytope when the probabilty distributions Involved are uniform.","2159-4848","978-1-4244-3775-7978-0-7695-3601","10.1109/ICST.2009.59","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4967617","","System testing;Real time systems;Automatic control;Software testing;Automata;Automatic testing;Educational institutions;Stochastic processes;Feeds;Life testing","","","","2","29","","","","","","IEEE","IEEE Conferences"
"Control-Flow Analysis and Representation for Aspect-Oriented Programs","J. Zhao","Shanghai Jiao Tong University, China","2006 Sixth International Conference on Quality Software (QSIC'06)","","2006","","","38","48","Aspect-oriented programming (AOP) has been proposed as a technique for improving the separation of concerns in software design and implementation. The field of AOP has, so far, focused primarily on problem analysis, language design, and implementation. Even though the importance of program comprehension and software maintenance is known, it has received little attention in the aspect-oriented paradigm. However, as the software systems coded in AOP languages are accumulated, the development of techniques and tools to support program comprehension and software maintenance tasks for aspect-oriented software will become important. In order to understand and maintain aspect-oriented programs, abstract models for representing these programs are needed. In this paper, we present techniques to construct control-flow representations for aspect-oriented programs, and discuss some applications of the representations in a program comprehension and maintenance environment","1550-6002;2332-662X","0-7695-2718","10.1109/QSIC.2006.20","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4032267","","Software maintenance;Computer languages;Software design;Software systems;Software tools;Software testing;Object oriented programming;Program processors;Optimizing compilers;Computer science","data flow analysis;formal specification;object-oriented programming;optimising compilers;software maintenance","control-flow analysis;control-flow representation;aspect-oriented programs;software design;problem analysis;language design;program comprehension;software maintenance;software systems;abstract models;program representation","","6","19","","","","","","IEEE","IEEE Conferences"
"Wire-once infrastructure for optimal test lab efficiency","T. Smith; J. Alnwick","Optical Transport Specialist, MRV Communications, USA; Test Automation Specialist, MRV Communications, USA","2008 IEEE AUTOTESTCON","","2008","","","421","427","This paper examines the network infrastructure of many labs, showing how relying on manual configuration of test networks adversely affects the productivity of a test lab. We will then discuss implementation of physical layer switching, an important tool for managing and automating the physical cabling of test networks. Optimizing the lab for responsiveness and best use of capital expenditures and setting the stage for the next generation of lab management.","1088-7725;1558-4550","978-1-4244-2225-8978-1-4244-2226","10.1109/AUTEST.2008.4662651","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4662651","","Optical switches;Switches;Optical fibers;Optical fiber cables;Physical layer;Testing;Software","laboratories;test facilities","wire-once infrastructure;optimal test laboratory efficiency;network infrastructure;physical layer switching;laboratory management","","","","","","","","","IEEE","IEEE Conferences"
"Toward the Use of Automated Static Analysis Alerts for Early Identification of Vulnerability- and Attack-prone Components","Michael; L. Williams","North Carolina State University; North Carolina State University","Second International Conference on Internet Monitoring and Protection (ICIMP 2007)","","2007","","","18","18","Extensive research has shown that software metrics can be used to identify fault- and failure-prone components. These metrics can also give early indications of overall software quality. We seek to parallel the identification and prediction of fault- and failure-prone components in the reliability context with vulnerability- and attack-prone components in the security context. Our research will correlate the quantity and severity of alerts generated by source code static analyzers to vulnerabilities discovered by manual analyses and testing. A strong correlation may indicate that automated static analyzers (ASA), a potentially early technique for vulnerability identification in the development phase, can identify high risk areas in the software system. Based on the alerts, we may be able to predict the presence of more complex and abstract vulnerabilities involved with the design and operation of the software system. An early knowledge of vulnerability can allow software engineers to make informed risk management decisions and prioritize redesign, inspection, and testing efforts. This paper presents our research objective and methodology.","","0-7695-2911","10.1109/ICIMP.2007.46","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4271764","","Fault diagnosis;Software systems;Software metrics;Software quality;Security;Testing;Risk analysis;Knowledge engineering;Risk management;Inspection","program diagnostics;program testing;security of data;software metrics;software quality;software reliability","automated static analysis;software metrics;failure-prone component identification;software quality;reliability context;security context;software testing;vulnerability identification","","8","43","","","","","","IEEE","IEEE Conferences"
"An adaptive discrete particle swarm optimization for TSP problem","Jing He; Dejia Shi; Li Wang","School of Computer and Electronic Engineering, Hunan University of Commerce, Changsha, China 410205; School of Computer and Electronic Engineering, Hunan University of Commerce, Changsha, China 410205; School of Computer and Electronic Engineering, Hunan University of Commerce, Changsha, China 410205","2009 Asia-Pacific Conference on Computational Intelligence and Industrial Applications (PACIIA)","","2009","2","","393","396","An adaptive discrete particle swarm optimization (PSO) method is presented to solve the generalized traveling salesman problem (GTSP). The generalized vertex employed to represent the problem, by which the GTSP and TSP can be handled in uniform style. An uncertain searching strategy and local searching techniques are also employed to accelerate the convergent speed. Numerical results show the effectiveness of the proposed method.","","978-1-4244-4606","10.1109/PACIIA.2009.5406574","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5406574","Generalized Traveling Salesman Problem;Particle Swarm Optimization","Particle swarm optimization;Traveling salesman problems;Application software;Biological cells;Genetic algorithms;Birds;Algorithm design and analysis;Benchmark testing;Dynamic programming;Space technology","numerical analysis;particle swarm optimisation;travelling salesman problems","adaptive discrete particle swarm optimization;TSP problem;uncertain searching strategy;local searching techniques","","","13","","","","","","IEEE","IEEE Conferences"
"Differential Evolution Strategy for Constrained Global Optimization and Application to Practical Engineering Problems","H. Kim; J. Chong; K. Park; D. A. Lowther","NA; NA; NA; NA","IEEE Transactions on Magnetics","","2007","43","4","1565","1568","In this paper, some modifications to the differential evolution strategy are proposed for the constrained global optimization problem. The scaling factor F is varied randomly within some range and an auxiliary set is employed to enhance the diversity of the population. The newly generated trial vector is compared with the nearest parent. The simple feasibility rule is used to treat the constraints. The effectiveness of the proposed method is demonstrated by solving some practical engineering problems. Finally, the method was applied to the robust design of a gas circuit breaker to increase the small current interruption performance","0018-9464;1941-0069","","10.1109/TMAG.2006.892100","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4137736","Constrained global optimization;differential evolution strategy (DES);feasibility rule;gas circuit breaker (GCB)","Constraint optimization;Genetic mutations;Convergence;Robustness;Circuit breakers;Application software;Robust control;Euclidean distance;Circuit testing;Genetic algorithms","circuit breakers;optimisation","differential evolution strategy;constrained global optimization;scaling factor;trial vector generation;gas circuit breaker robust design;current interruption performance","","52","10","","","","","","IEEE","IEEE Journals & Magazines"
"Evolutionary Multi-objective Optimization of Business Processes","A. Tiwari; K. Vergidis; B. Majeed","Lecturer at Manufacturing Department, School of Industrial and Manufacturing Science, Cranfield University, MK43 0AL, UK (Phone no.: +44 (0) 1234 754073, Ext. 4250; fax: +44 (0) 1234 750852; e-mail: a.tiwari@cranfield.ac.uk); NA; NA","2006 IEEE International Conference on Evolutionary Computation","","2006","","","3091","3097","Most of the current attempts for business process optimisation are manual without involving any formal automated methodology. This paper proposes a framework for multi-objective optimisation of business processes. The framework uses a generic business process model that is formally defined and specifies process cost and duration as objective functions. The business process model is programmed and incorporated into a software platform where a selection of multi-objective optimisation algorithms is applied to five test problems. The test problems are business process designs of varying complexities and are optimised with three popular optimisation techniques (NSGA2, SPEA2 and MOPSO algorithms). The results indicate that although the business process optimisation is a highly constrained problem with fragmented search space, multi-objective optimisation algorithms such as NSGA2 and SPEA2 produce a satisfactory number of alternative optimised business processes. However, the performance of the optimisation algorithms drops sharply with even a slight increase in problem complexity. This paper also discusses the directions for future research in this area.","1089-778X;1941-0026","0-7803-9487","10.1109/CEC.2006.1688700","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1688700","","Object oriented modeling;Process design;Constraint optimization;Cost function;Design optimization;Manufacturing industries;Power system modeling;Competitive intelligence;Optimization methods;Software algorithms","business process re-engineering;evolutionary computation;optimisation;search problems;software engineering","evolutionary multiobjective optimization;business processes;business process optimisation;formal automated methodology;business process model;software platform;optimisation techniques;search space;NSGA2;SPEA2","","8","15","","","","","","IEEE","IEEE Conferences"
"Using Virtualization to Improve Software Rejuvenation","L. M. Silva; J. Alonso; J. Torres","University of Coimbra, Portugal; Universitat de Politecnica de Catalunya, Spain; Universitat de Politecnica de Catalunya and Barcelona Supercomputing Center, Spain","IEEE Transactions on Computers","","2009","58","11","1525","1538","In this paper, we present an approach for software rejuvenation based on automated self-healing techniques that can be easily applied to off-the-shelf application servers. Software aging and transient failures are detected through continuous monitoring of system data and performability metrics of the application server. If some anomalous behavior is identified, the system triggers an automatic rejuvenation action. This self-healing scheme is meant to disrupt the running service for a minimal amount of time, achieving zero downtime in most cases. In our scheme, we exploit the usage of virtualization to optimize the self-recovery actions. The techniques described in this paper have been tested with a set of open-source Linux tools and the XEN virtualization middleware. We conducted an experimental study with two application benchmarks (Tomcat/Axis and TPC-W). Our results demonstrate that virtualization can be extremely helpful for failover and software rejuvenation in the occurrence of transient failures and software aging.","0018-9340;1557-9956;2326-3814","","10.1109/TC.2009.119","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5184821","Software rejuvenation;software aging;virtualization;self-healing.","Software;Servers;Aging;Middleware;Data mining;Transient analysis;Availability","fault tolerant computing;Linux;middleware;public domain software;software metrics;software reusability;software tools","software rejuvenation;automated self-healing techniques;application servers;system data metrics;open source Linux tools;XEN virtualization middleware;software aging detection","","22","45","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic generation of test case based on GATS algorithm","X. Shen; Q. Wang; P. Wang; B. Zhou","Institute of Data and Knowledge Engineering, Henan University, Kaifeng, Henan Province 475001, China; Institute of Data and Knowledge Engineering, Henan University, Kaifeng, Henan Province 475001, China; Computer and Information Engineering department, Henan University, Kaifeng, Henan Province 475004, China; Computer and Information Engineering department, Henan University, Kaifeng, Henan Province 475004, China","2009 IEEE International Conference on Granular Computing","","2009","","","496","500","A kind of software test data automated generation method based on genetic algorithm and tabu search algorithm is proposed. Having both local search capabilities of tabu search algorithm and global search capability of genetic algorithm, this tabu genetic algorithm combines tabu search algorithm with genetic algorithm. The experiment results show that the tabu genetic algorithm with tabu search as mutation operator is effective on generating test cases and its optimizing performance is superior to the simple genetic algorithm.","","978-1-4244-4830","10.1109/GRC.2009.5255070","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5255070","","Automatic testing;Genetic algorithms;Software testing;Software algorithms;Hybrid power systems;Biological information theory;Evolution (biology);Artificial intelligence;Knowledge engineering;Genetic mutations","genetic algorithms;program testing;search problems","software test data automated generation method;genetic algorithm;tabu search algorithm;global search capability;automatic generation","","5","12","","","","","","IEEE","IEEE Conferences"
"Quantum-Behaved Particle Swarm Optimization with Normal Cloud Mutation Operator","J. Zhao; J. Sun; W. Xu","NA; NA; NA","2009 International Conference on Computational Intelligence and Software Engineering","","2009","","","1","4","The mutation mechanism is introduced into Quantum-behaved Particle Swarm Optimization to increase its global search ability and escape from local minima. Based on the properties of randomness and stable tendency of normal cloud model, this paper proposed a Quantum-behaved Particle Swarm Optimization with Normal Cloud Mutation Operator (QPSO-NCM). This method is tested and compared with particle swarm optimization (PSO), PSO-NCM and QPSO. The experimental results show that QPSO-NCM performs better than the others algorithms.","","978-1-4244-4507","10.1109/CISE.2009.5364714","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5364714","","Particle swarm optimization;Clouds;Genetic mutations;Convergence;Evolutionary computation;Information technology;Equations;Testing;Sun;Quantum computing","particle swarm optimisation","quantum behaved particle swarm optimization;normal cloud mutation operator;global search ability;local minima;normal cloud model","","1","12","","","","","","IEEE","IEEE Conferences"
"A multi-objective constrained optimization algorithm based on infeasible individual stochastic binary-modification","H. Geng; Song Qing-Xi; Wu Ting-Ting; Liu Jing-Fa","College of Computer & Software, Nanjing University of Information Science & Technology, China; College of Computer & Software, Nanjing University of Information Science & Technology, China; College of Computer & Software, Nanjing University of Information Science & Technology, China; College of Computer & Software, Nanjing University of Information Science & Technology, China","2009 IEEE International Conference on Intelligent Computing and Intelligent Systems","","2009","1","","89","93","During solving the constrained multi-objective optimization problems with evolutionary algorithms, constraint handling is a principal problem. Analyzing the existing constraint handling methods, a novel constraint handling strategy based on infeasible individual stochastic binary-modification is proposed in the paper. Its key point lies in modifying randomly infeasible individual into feasible one according to predefined modification rate (R<sub>m</sub>) during evolutionary optimization. Finally, the proposed strategy is applied to the constrained multi-objective optimization evolutionary algorithm, and then the algorithm is tested on 7 benchmark problems and the comparison between our strategy and Deb's constrained-domination principle demonstrates that our strategy optimizes 30% faster than Deb's in the circumstances to preserve equivalent distribution and convergence of the solutions found.","","978-1-4244-4754-1978-1-4244-4738","10.1109/ICICISYS.2009.5357931","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5357931","Evolutionary Multi-objective Optimization;Constraint Handling;Stochastic Binary-Modification","Constraint optimization;Stochastic processes;Evolutionary computation;Algorithm design and analysis;Educational institutions;Information science;Software algorithms;Information analysis;Benchmark testing;Problem-solving","constraint handling;evolutionary computation","multiobjective constrained optimization algorithm;infeasible individual stochastic binary-modification;evolutionary algorithms;constraint handling methods;evolutionary optimization;Deb constrained-domination principle","","1","15","","","","","","IEEE","IEEE Conferences"
"Risk-Based Adaptive Group Testing of Semantic Web Services","X. Bai; R. S. Kenett","NA; NA","2009 33rd Annual IEEE International Computer Software and Applications Conference","","2009","2","","485","490","Comprehensive testing is necessary to ensure the quality of complex Web services that are loosely coupled, dynamic bound and integrated through standard protocols. Testing of such web services can be however very expensive due to the diversified user requirements and the large numbers of service combinations delivered by the open platform. Group testing was introduced in our previous research as a selective testing technique to reduce test cost and improve test efficiencies. It applies test cases efficiently so that the largest percent of problematic web service is detected as early as possible. The paper proposes a risk-based approach to group test selection. With this approach, test cases are categorized and scheduled with respect to the risks of their target service features. The approach is based on the assumption that for a service-based system, the tolerance to a featurepsilas failure is an inverse ratio to its risk. The risky features should be tested earlier and with more tests. We specially address the problem in the context of semantic Web Services and report a first attempt for an ontology-based quantitative risk assessment. The paper also discusses risk-based group testing process and strategies for ranking and ruling-out services of the test groups, at each risk level. Runtime monitoring mechanism is incorporated to detect the dynamic changes in service configuration and composition so that the risks can be continuously adjusted online.","0730-3157;0730-3157","978-0-7695-3726","10.1109/COMPSAC.2009.180","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5254074","Web Services;Group Testing;Risk-Based Testing","Testing;Semantic Web;Web services;Protocols;Costs;Context-aware services;Ontologies;Risk management;Runtime;Monitoring","ontologies (artificial intelligence);program testing;risk management;semantic Web;system monitoring;Web services","risk-based adaptive group testing;semantic Web service;comprehensive testing;open platform;selective testing technique;test case;ontology-based quantitative risk assessment;ruling-out service;runtime monitoring mechanism;service configuration;service composition","","6","37","","","","","","IEEE","IEEE Conferences"
"Replacement and Structure of S-Boxes in Rijndael","D. Wang; S. Sun","NA; NA","2008 International Conference on Computer Science and Software Engineering","","2008","3","","782","784","This paper describes the designation and structure of S-boxes which are the fundament of the Rijndael algorithm and finds the optimization of them using four kinds of testing methods. Firstly, it constructs different S-boxes by Matlab using 30 irreducible polynomials the maximum power of which is eight in a finite field. Through taking their performance analysis, we find that they have the similar performances in differential testing and linear or correlation testing. But in avalanche testing and Boolean expressions testing, the S-boxes which are constructed by No. 9 and No. 18 irreducible polynomial are optimal structure respectively.","","978-0-7695-3336","10.1109/CSSE.2008.296","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722459","replacement;s-boxes;rijndael","Testing;Polynomials;Computer science;Galois fields;Software algorithms;Performance analysis;Cryptography;Software engineering;Algorithm design and analysis;Design optimization","cryptography;mathematics computing;polynomials","S-box structure;S-box replacement;Rijndael algorithm;optimization;Matlab;irreducible polynomial;finite field;differential testing;correlation testing;linear testing;avalanche testing;Boolean expression testing;security division cipher strength;cryptography","","3","7","","","","","","IEEE","IEEE Conferences"
"Requirements elicitation through model-driven evaluation of software components","L. Chung; Weimin Ma; K. Cooper","Dept. of Comput. Sci., Texas Univ., Dallas, TX, USA; Dept. of Comput. Sci., Texas Univ., Dallas, TX, USA; Dept. of Comput. Sci., Texas Univ., Dallas, TX, USA","Fifth International Conference on Commercial-off-the-Shelf (COTS)-Based Software Systems (ICCBSS'05)","","2006","","","10 pp.","","The use of software components is perceived to significantly shorten development time and cost, while improving quality, in developing a large, complex software system. A key premise to this perception seems to be the ability to effectively search, match, rank, and select software components, during the requirements engineering process. In this paper, we present a technique for eliciting requirements by using model-driven evaluation of software components, where the evaluation revolves around ""models"" of software components and ""models"" of the component-based application (CBA). As part of our ongoing project, component-aware requirements engineering (CARE), this model-driven evaluation technique is intended to match the models of the stakeholders' needs for the component-based application against the models of the capabilities of the set of components that are currently available. More specifically, this technique allows for an integrated use of several searching/matching techniques, such as keyword-based search, case-based reasoning (CBR) and analytic hierarchy process (AHP), in evaluating models of components' requirements against models of the requirements of the stakeholders of the CBA being elicited incrementally. The model-driven evaluation technique is illustrated using a home appliance control system (HACS) example.","","0-7695-2515","10.1109/ICCBSS.2006.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1595762","","Application software;Software quality;Software systems;Costs;Computer science;Home appliances;Control system synthesis;Embedded software;Computer architecture;Testing","object-oriented programming;formal specification;formal verification;software quality;software performance evaluation","software component;model-driven evaluation;component-based application;component aware requirements engineering;searching technique;matching technique;keyword-based search;case-based reasoning;analytic hierarchy process;home appliance control system","","2","26","","","","","","IEEE","IEEE Conferences"
"A Product Engineering Approach to Software Development","K. V. Nori; N. Swaminathan","Business Systems &amp; Cybernetics Centre Tata Consultancy Services, Hyderabad, India; NA","19th Conference on Software Engineering Education and Training Workshops (CSEETW'06)","","2006","","","1","1","Software engineering currently takes an activity-oriented view of development. Our approach to achieving qualities is rooted in process engineering: identifying and performing a set of requirements, design and construction activities, with testing to inspect if the desired product has been obtained, and relying on process optimization over time to help us in defining the right set of activities. In contrast, design in other engineering disciplines is centered on product engineering, systematically engineering in desired qualities through successive stages of development. Process engineering is then applied to this conceptually sound fundamental structure to optimize process performance parameters. This paper asserts that software development can and should be grounded in product engineering, and presents a conceptual model that enables a systematic approach to achieving qualities. Key enablers for this are a clear separation of quality concerns and the use of analysis to validate designs. Analysis techniques for software qualities have been available for many years, yet they are not viewed as basic knowledge in Computer Science and Software Engineering curricula. We argue that to put software engineering education on a firm footing, it is necessary to shift our conceptual basis for software development from process engineering to product engineering.","","0-7695-2647","10.1109/CSEETW.2006.2","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1644072","","Programming;Acoustical engineering;Design engineering;Software engineering;Systems engineering and theory;Performance evaluation;Testing;Design optimization;Software quality;Computer science","","","","1","2","","","","","","IEEE","IEEE Conferences"
"Application of a Genetic Algorithm to the Design Optimization of a Multilayer Probe Card for Wafer-Level Testing","D. Liu; M. Shih; C. Chang","NA; NA; NA","IEEE Transactions on Electronics Packaging Manufacturing","","2009","32","1","49","58","The number of input and output pads on high-performance IC devices has increased in recent years, and hence wafer-level testing is conventionally performed using a probe card with a multilayer needle layout. This paper employs ANSYS commercial software and a Genetic Algorithm (GA) to optimize the design parameters of a multilayer needle probe card such that the scrub marks produced by the different needle layers are of approximately equal length. A dummy probe card containing both a conventional multilayer needle layout and the optimized needle layout is then fabricated and used in a series of single-contact probing tests. The results reveal that the scrub marks produced by the optimized needle layout are both shorter and of a more uniform length that those produced by the conventional needle design. For both needle layouts, a lower and more stable contact resistance is obtained as the overdrive distance is increased. Finally, a multicontact probing test is performed to evaluate the effect on the contact resistance of probe tip contamination following repeated surface contacts. The results show that the needles in the optimized layout are less heavily contaminated than those in the conventional layout, and hence the contact resistance is both lower and more stable. As a consequence, the probe card requires cleaning less frequently and hence its service life is improved.","1521-334X;1558-0822","","10.1109/TEPM.2008.2010776","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4747380","Finite-element method (FEM);genetic algorithm (GA);multilayer needle card;wafer-level probing test","Genetic algorithms;Design optimization;Nonhomogeneous media;Probes;Needles;Contact resistance;Performance evaluation;Integrated circuit layout;Integrated circuit testing;Algorithm design and analysis","finite element analysis;genetic algorithms;integrated circuit design;integrated circuit testing;multilayers;needles;probes","genetic algorithm;design optimization;multilayer probe card;wafer-level testing;high-performance IC devices;multilayer needle layout;ANSYS;needle design;multicontact probing test;contact resistance;probe tip contamination;three-dimensional finite element model","","4","16","","","","","","IEEE","IEEE Journals & Magazines"
"A* Algorithm Analysis and Optimization: In Network Game Design","X. Guo; P. Guo","NA; NA","2009 International Conference on Computational Intelligence and Software Engineering","","2009","","","1","4","In this paper, we first describe the basic principle and the method of the A* algorithm. And we analyze the reason that the A* algorithm influences speed when it is searching for the optimum route in network game map. Then we give the optimization scheme from the aspects of node data structure to the maintenance of the open queue. At the same time, the optimization scheme is evaluated and tested by a number of different game maps. Finally, through experiment, we can have the conclusion that the improvable A* algorithm is practical and feasible. And the A* algorithm can increase the speed of playing the game and can been implemented in the specific network game.","","978-1-4244-4507","10.1109/CISE.2009.5366757","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5366757","","Algorithm design and analysis;Design optimization;Costs;Information analysis;Educational institutions;Cities and towns;Data structures;Switches;Design engineering;Testing","data structures;game theory;optimisation;search problems","A* algorithm analysis;optimization;network game design;data structure","","1","5","","","","","","IEEE","IEEE Conferences"
"Cognitive Radio Software Testbed using Dual Optimization in Genetic Algorithm","J. M. Kim; S. H. Sohn; N. Han; G. Zheng; Y. M. Kim; J. K. Lee","INHA-WiTLAB, Inha University, Incheon, Korea. jaekim@inha.ac.kr, miraju81@hanmail.net; INHA-WiTLAB, Inha University, Incheon, Korea. sunnyshon@gmail.com, miraju81@hanmail.net; INHA-WiTLAB, Inha University, Incheon, Korea. neil_han@ieee.org, miraju81@hanmail.net; INHA-WiTLAB, Inha University, Incheon, Korea. gbzheng@gmail.com, miraju81@hanmail.net; INHA-WiTLAB, Inha University, Incheon, Korea. ymym@hanmail.net, miraju81@hanmail.net; INHA-WiTLAB, Inha University, Incheon, Korea. miraju81@hanmail.net","2008 3rd International Conference on Cognitive Radio Oriented Wireless Networks and Communications (CrownCom 2008)","","2008","","","1","6","Cognitive Radio is considered as a potential solution to improve spectrum utilization via opportunistic spectrum sharing. In this paper, we present a software testbed which is developed to implement the cognitive radio system. The basic functions of cognitive radio are explored and implemented, including spectrum sensing and dynamic spectrum management. The testbed is able to verify the overall performance of cognitive radio technology. Furthermore, it provides an important system model to verify the effectiveness of the new algorithms in future study. The simulation on the testbed demonstrates that primary and secondary users can coexist. This is able to convince regulatory authorities as well as primary users to enable the implementation of cognitive radio technology.","2166-5370;2166-5419","978-1-4244-2301-9978-1-4244-2302","10.1109/CROWNCOM.2008.4562484","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4562484","Cognitive Radio;Software Testbed;System Modeling;Genetic Algorithm","Cognitive radio;Software testing;Genetic algorithms;Chromium;System testing;Radio spectrum management;Frequency;Engines;Software systems;Modeling","cognitive radio;genetic algorithms;software radio;telecommunication computing;telecommunication network management","cognitive radio software testbed;dual optimization;genetic algorithm;opportunistic spectrum sharing;dynamic spectrum management","","7","11","","","","","","IEEE","IEEE Conferences"
"An Optimization Method for Real-Time Natural Phenomena Simulation on WinCE Platform","N. Liu; R. Li; Z. Yang; H. Chao","Sun Yat-Sen University; Sun Yat-Sen University; Sun Yat-Sen University; Sun Yat-Sen University","31st Annual International Computer Software and Applications Conference (COMPSAC 2007)","","2007","2","","57","62","We present a new optimized algorithm of particle system for simulating flame, so that it can be implemented on WinCE platform which has only limited resources. Our approach mainly intends to store some traces first and then load them randomly during the simulation to reduce the computation complexity of the particle system. We have tested this approach in the simulation of a flame, and our method shows a significant performance gain with little loss in the visual appearance of the simulation, this indicates the potential to extend our approach to particle simulation involved with other natural phenomena for resource limited platforms.","0730-3157","0-7695-2870","10.1109/COMPSAC.2007.72","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4291102","","Optimization methods;Computational modeling;Fires;Sun;Application software;Information science;Fractals;Fuzzy systems;Computer simulation;Layout","computational complexity;operating systems (computers);optimisation;user interfaces","optimization method;real-time natural phenomena simulation;particle system complexity;resource limited platforms","","1","9","","","","","","IEEE","IEEE Conferences"
"Improved Dynamic Location Reference Method Agora-C Based on Rule Optimization","X. Dai; W. Lv; T. Zhu","NA; NA; NA","2008 International Conference on Computer Science and Software Engineering","","2008","1","","139","144","Agora-C is a dynamic location reference method, which can match locations between dissimilar maps effectively. It references objects of the road network by a condensed selection of its real-world geographic properties which are stored in a digital map database, instead of utilizing pre-defined object IDs or look-up tables. This method is lack of practicability because of huge encoding bytes and low transmission efficiency caused by weaknesses in encoding rules and encoding formats. According to the basic principles, this paper brought forward an improved Agora-C encoding method based on rule optimization. It reduced redundant and useless information with the help of modifying encoding rules and re-arranging scattered encoding format. An experimental system was implemented to verify these improvements. A contrast test showed that the improved method reduced about half of the encoding bytes on average, which helped to improve the transmission efficiency and make Agora-C meet more practical application needs.","","978-0-7695-3336","10.1109/CSSE.2008.254","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721711","Dynamic location reference;Agora-C;encoding rules;encoding formats","Optimization methods;Encoding;Roads;Decoding;Intrusion detection;Intelligent transportation systems;Geometry;Computer science;Software engineering;Databases","geographic information systems;optimisation;table lookup;traffic engineering computing","dynamic location reference method;Agora-C encoding method;rule optimization;digital map database;look-up tables;intelligent transportation system","","1","9","","","","","","IEEE","IEEE Conferences"
"Application of New Information Multi-variable Optimizing Grey Model NMGM(1,n,q,r) to the Load-Strain Relation","W. Xiao; D. Liao; Y. Luo","NA; NA; NA","2009 Pacific-Asia Conference on Knowledge Engineering and Software Engineering","","2009","","","39","42","Structural parameters and material behaviors of an in-service structure are changed due to various external factors influence. Its structural strength is hard to be clearly expressed by mathematical formula. Based on the Grey system and the new information principle, taking the mean relative error as objective function and taking the modified nth component of each variable as initial value of response function and finding the optimum background value coefficient q and modified value r of initial value of response function by optimizing, new information multi-variable optimization Grey model NMGM(1, n, q, r) of the load-strain function in actual work condition was put forward. The failure load of the structure was forecasted. Calculation results show that this method is very effective to data processing which has high precision and easy to use. The method has wide applications in data processing in various engineering fields.","","978-1-4244-5324-5978-0-7695-3916","10.1109/KESE.2009.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5383627","NMGM(1;n;q;r) Model;optimization;Grey system;data processing;new information principle","Data processing;Predictive models;Data engineering;Differential equations;Educational institutions;Mechanical engineering;Art;Load forecasting;System testing;Knowledge engineering","grey systems;multivariable systems;optimisation","Load-strain Relation;in-service structure;Grey system;response function;new information multi-variable optimization Grey model;load-strain function;data processing","","","9","","","","","","IEEE","IEEE Conferences"
"Prioritizing Coverage-Oriented Testing Process - An Adaptive-Learning-Based Approach and Case Study","F. Belli; M. Eminov; N. Gokce","University of Paderborn, Germany; Mugla University, Turkey.; Mugla University, Turkey","31st Annual International Computer Software and Applications Conference (COMPSAC 2007)","","2007","2","","197","203","This paper proposes a graph-model-based approach to prioritizing the test process. Tests are ranked according to their preference degrees which are determined indirectly, i.e., through classifying the events. To construct the groups of events, unsupervised neural network is trained by adaptive competitive learning algorithm. A case study demonstrates and validates the approach.","0730-3157","0-7695-2870","10.1109/COMPSAC.2007.169","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4291124","","System testing;Costs;Mathematical model;Neural networks;Time factors;Robustness;Constraint optimization","graph theory;neural nets;program testing;unsupervised learning","coverage-oriented testing process;adaptive learning;graph-model-based approach;unsupervised neural network;adaptive competitive learning algorithm","","4","19","","","","","","IEEE","IEEE Conferences"
"Ranking with Query Influence Weighting for document retrieval","Zhen Liao; Y. L. Huang; Mao Qiang Xie; J. Liu; Y. Wang; M. Lu","College of Information Technical Science, Nankai University, Tianjin 300071, China; College of Software, Nankai University, Tianjin 300071, China; College of Software, Nankai University, Tianjin 300071, China; College of Information Technical Science, Nankai University, Tianjin 300071, China; College of Information Technical Science, Nankai University, Tianjin 300071, China; College of Information Technical Science, Nankai University, Tianjin 300071, China","2009 International Conference on Machine Learning and Cybernetics","","2009","2","","1177","1182","Ranking continuously plays an important role in document retrieval and has attracted remarkable attentions. Existing ranking methods conduct the loss function for each query independently but ignore the fact that minimizing the loss of one query may increase that of another if they are contradictory. In principle, the punishment for errors of important queries should be enlarged. In this paper we propose a new approach ldquoQuery Influence Weightingrdquo, which adopts ldquoQuery Influence Weightingrdquo algorithm for computing query importance and incorporates the importance into the loss function for guiding the model constructing. We conduct a ranking model based on a state-of-art method named Ranking SVM. Experimental results on two public datasets show that the ldquoQuery Influence Weightingrdquo approach outperforms conventional Ranking SVM and other baselines. We further analyze the influence consistency on training and testing datasets and validate the effectiveness of our approach.","2160-133X;2160-1348","978-1-4244-3702-3978-1-4244-3703","10.1109/ICMLC.2009.5212411","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5212411","Learning to Rank;Document Retrieval;Ranking SVM;Query Influence Weighting","Machine learning;Cybernetics","document handling;query processing;support vector machines","query influence weighting;document retrieval;loss function;query importance;ranking model;ranking SVM","","","15","","","","","","IEEE","IEEE Conferences"
"Generating pairwise combinatorial test set using artificial parameters and values","M. I. Younis; K. Z. Zamli; N. A. M. Isa","School of Electrical and Electronic Engineering, Universiti Sains Malaysia, Engineering Campus, 14300 Nibong Tebal, Penang, Malaysia; School of Electrical and Electronic Engineering, Universiti Sains Malaysia, Engineering Campus, 14300 Nibong Tebal, Penang, Malaysia; School of Electrical and Electronic Engineering, Universiti Sains Malaysia, Engineering Campus, 14300 Nibong Tebal, Penang, Malaysia","2008 International Symposium on Information Technology","","2008","3","","1","8","In order to meet market demands for quality software products, software engineers are increasingly under pressure to test more lines of codes. To maintain acceptable test coverage, software engineers need to consider a significantly large number of test set. Many combinations of possible input parameters, hardware/software environments, and system conditions need to be tested and verified against for conformance. Often, this results into combinatorial explosion problem (i.e. too many test data set too consider). Earlier work suggests that pairwise sampling strategy based on parameter interactions of variables can be effective. This paper discusses an efficient pairwise strategy, termed RA and ORA, that can systematically minimize the pairwise test set generated from higher order test parameters to lower order ones. In doing so, this paper demonstrates and compares the results against existing strategies including IRPS, IPO, GA, ACA, Jenny and All Pairs.","2155-8973;2155-899X","978-1-4244-2327-9978-1-4244-2328","10.1109/ITSIM.2008.4632001","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4632001","","Software;Testing;Hardware;Gallium;Optimization;Consumer electronics;Explosions","combinatorial mathematics;conformance testing;optimisation;sampling methods;software quality","pairwise combinatorial test set;artificial parameters;artificial values;market demands;software products;combinatorial explosion problem;pairwise sampling strategy","","","15","","","","","","IEEE","IEEE Conferences"
"Speaking Truth to Power","G. Booch","IBM","IEEE Software","","2007","24","2","12","13","Whenever the author conducts an architectural assessment for software development projects, he endeavors to speak truth to power: those with true power never fear the truth. Sam Guckenheimer has observed that in software code there is truth. Code represents the stark reality of a software development organization's labor. There is also truth to be found in a system's architecture. Every system's architecture is molded by the forces that swirl around it, and the collective concerns of all the stakeholders represent the most dynamic forces shaping a system. The software development organization's unique task is to address all the essential concerns of all the important stakeholders and to ensure that they aren't blindsided by unexpected problems and stakeholders. This is why employing a process that incrementally and iteratively grows a system's architecture through the regular release of testable executables is so important. Such a process lets the software team engage the right stakeholders at the right time and to make the right decisions, neither too early nor too late. In creating a software-intensive system that's both relevant and beautiful, every stakeholder, no matter how close or how far from the code, deserves the truth","0740-7459;1937-4194","","10.1109/MS.2007.53","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4118643","architectural assessment;stakeholder prioritization","Computer architecture;System testing;Orbits;Social network services;Real time systems;Privacy;Humans;Electronic mail;Aging;History","software architecture;software development management","architectural assessment;truth to power;soft-ware development;system architecture;stakeholders;testable executables;software team;software-intensive system","","1","","","","","","","IEEE","IEEE Journals & Magazines"
"Differential Evolution Strategy for Constrained Global Optimization and Application to Practical Engineering Problems","Hong-Kyu Kim; Jin-Kyo Chong; D. A. Lowther","NA; NA; NA","2006 12th Biennial IEEE Conference on Electromagnetic Field Computation","","2006","","","238","238","In this paper, some modifications to the differential evolution strategy are proposed for the constrained global optimization problem. The scaling factor F is varied randomly and an auxiliary set is introduced to preserve the diversity of the population. The newly generated candidate population is compared to the nearest parent. The simple feasibility rule is employed to treat the constraints. The test results show that the proposed technique is an improvement in terms of accuracy and robustness","","1-4244-0320","10.1109/CEFC-06.2006.1633028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1633028","","Constraint optimization;Robustness;Testing;Application software;Cost function;Convergence;Chromium;Optimization methods","evolutionary computation;optimisation","global optimization;differential evolution strategy;population diversity;scaling factor;candidate population","","","2","","","","","","IEEE","IEEE Conferences"
"A Hybrid Algorithm of Converse Ant Colony Optimization for Solving JSP","X. Song; Y. Cao","NA; NA","2009 International Conference on Computational Intelligence and Software Engineering","","2009","","","1","4","A hybrid algorithm of converse ant colony optimization (HCACO) is proposed, which is used to overcome the disadvantages of the slow convergence speed and stagnation behavior when solving job shop problem (JSP). In order to improve the probability of escaping from the local optimization, we induct converse ants into the ant colony. At the same time, each solution of ACO with certain probability pursues the process of parallel enhanced SA algorithm to accelerate the coverage speed. Compared with PGA and ACO, HCACO algorithm is simulated for benchmark instances and it illustrates that the hybrid algorithm shows more better and efficient results.","","978-1-4244-4507","10.1109/CISE.2009.5366443","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5366443","","Ant colony optimization;Job shop scheduling;Control engineering;Acceleration;Electronics packaging;Benchmark testing;Computational modeling;Evolutionary computation;Traveling salesman problems;Concurrent computing","job shop scheduling;optimisation;probability","hybrid algorithm;converse ant colony optimization;job shop problem;probability","","","7","","","","","","IEEE","IEEE Conferences"
"Optimized hardware implementation of FFT processor","A. A. A. Sallab; H. Fahmy; M. Rashwan","Electronics and Communications Department, Faculty of Engineering, Cairo University, Cairo, Egypt; Electronics and Communications Department, Faculty of Engineering, Cairo University, Cairo, Egypt; Electronics and Communications Department, Faculty of Engineering, Cairo University, Cairo, Egypt","2009 4th International Design and Test Workshop (IDT)","","2009","","","1","5","Fast Fourier transform (FFT) is an essential component in many digital signal processing and communications systems. The performance of the FFT component is a key factor in evaluating the overall system performance, and it is common to use it as a benchmark for the whole system. Many attempts have been made to enhance the FFT performance, both on algorithm and implementation levels. Software and hardware designs exist to implement this component. In this paper, an optimized hardware implementation of FFT processor on FPGA is presented, where the steps of radix-2 FFT algorithm are well analyzed and an optimized design is developed as a result, with full exploitation of the hardware platform capabilities to achieve optimum performance. The performance results of the proposed design are demonstrated, and compared to other related works and reference designs.","2162-0601;2162-061X","978-1-4244-5750-2978-1-4244-5748-9978-1-4244-5750","10.1109/IDT.2009.5404139","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5404139","","Hardware;Signal processing algorithms;Design optimization;Algorithm design and analysis;Fast Fourier transforms;Digital signal processing;System performance;Software design;Field programmable gate arrays;Performance analysis","digital signal processing chips;fast Fourier transforms;field programmable gate arrays;logic design","FFT processor;fast Fourier transform;digital signal processing;hardware designs;software designs;FPGA;radix-2 FFT algorithm;optimized design","","4","11","","","","","","IEEE","IEEE Conferences"
"Test and Processing of Gun's Shooting Vibration Signal Based on Wavelet Threshold De-Noising","J. Yin; J. Zheng; H. Teng; F. Li","NA; NA; NA; NA","2009 First International Conference on Information Science and Engineering","","2009","","","675","677","The gun brings with acute vibration during its shooting, which greatly impacts gun's firing accuracy and reliability. If vibration response has been tested and analyzed when the gun shooting, gun's dynamic characteristic would be obtained which can afford important references for gun's optimization and technology improving. The paper designs the testing system for gun's shooting vibration, according to the test state and requirement during gun's shooting. With the problem of noise interference inside the signal, wavelet threshold de-noising is adopted to decrease noise when the principle of heuristic threshold and improved threshold function are used. Then the characteristics in time and frequency domain are enhanced.","2160-1283;2160-1291","978-1-4244-5728-1978-1-4244-4909-5978-0-7695-3887","10.1109/ICISE.2009.1162","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5454573","","Signal processing;Noise reduction;System testing;Software testing;Circuit testing;Vibration measurement;Data analysis;Hardware;Reliability engineering;Signal design","frequency-domain analysis;interference (signal);military computing;optimisation;signal denoising;vibrations;weapons","gun shooting vibration signal;wavelet threshold denoising;acute vibration;gun firing accuracy;gun firing reliability;gun dynamic characteristic;gun optimization;testing system;noise interference;heuristic threshold;time domain;frequency domain","","","8","","","","","","IEEE","IEEE Conferences"
"Automated software diversity for hardware fault detection","G. Gaiswinkler; A. Gerstinger","Elektrobit Austria GmbH, Kaiserstrasse 45 / Stiege 2, 1070 Vienna, Austria; Institute of Computer Technology, Vienna University of Technology, Gusshausstrasse 27-29, 1040, Austria","2009 IEEE Conference on Emerging Technologies & Factory Automation","","2009","","","1","7","Software in dependable systems must be able to tolerate or detect faults in the underlying infrastructure, such as the hardware. This paper presents a cost efficient automated method how register faults in the microprocessor can be detected during execution. This is done with the help of using compiler options to generate diverse binaries. The efficacy of this approach has been analyzed with the help of a CPU emulator, which was modified exactly for this purpose. The promising results show, that by using this approach, it is possible to automatically detect the vast majority of the injected register faults. In our simulations, two diverse versions have-despite of experiencing the same fault during execution - never delivered the same incorrect result, so we could detect all injected faults.","1946-0740;1946-0759","978-1-4244-2727-7978-1-4244-2728","10.1109/ETFA.2009.5347167","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5347167","","Hardware;Fault detection;Testing;Program processors;Application software;Microprocessors;Registers;Optimizing compilers;Optimization methods;Software systems","microprocessor chips;safety-critical software;software fault tolerance","automated software diversity;hardware fault detection;software fault tolerant;microprocessor;CPU emulator;injected register faults;safety-critical software","","7","10","","","","","","IEEE","IEEE Conferences"
"Optimization of the substrate parameters for EM-simulators","F. Korndörfer; F. Sischka","IHP, Im Technologiepark 25, Frankfurt(Oder) D-015236 Germany; Agilent Technologies, Böblingen D-71004, Germany","2007 69th ARFTG Conference","","2007","","","1","6","A technique for extraction and optimization of silicon substrate parameters is presented. The parameters are optimized to get the best fit between measured and simulated S-parameters for passive elements. Test structures were fabricated in a SiGe:C process with 5 metallization layers. The measurements were performed on-wafer. Loss and relative permittivity ¿<sub>r</sub> of each insulation layer is extracted and optimized separately.","","978-0-7803-9762-0978-0-7803-9763","10.1109/ARFTG.2007.5456329","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5456329","Optimization methods;scattering parameters measurement;dielectric measurements;parameter estimation;simulation software;calibrated EM-simulations","Electrical resistance measurement;Testing;Capacitance measurement;Contact resistance;Probes;Dielectric measurements;Scattering parameters;Performance evaluation;Scanning electron microscopy;Permittivity measurement","carbon;electromagnetic fields;electromagnetic wave scattering;Ge-Si alloys;metallisation;optimisation;permittivity;S-parameters","electromagnetic field simulators;optimization;silicon substrate parameters;simulated S-parameters;passive elements;metallization layers;permittivity;SiGe:C","","","2","","","","","","IEEE","IEEE Conferences"
"Compiler optimizations to reduce security overhead","Tao Zhang; Xiaotong Zhuang; S. Pande","Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA, USA; Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA, USA; Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA, USA","International Symposium on Code Generation and Optimization (CGO'06)","","2006","","","12 pp.","357","In this work, we present several compiler optimizations to reduce the overhead due to software protection. We first propose an aggressive rematerialization algorithm which attempts to maximally realize non-trusted values from other trusted values thereby avoiding the security cost for those non-trusted values. We further propose a compiler technique to utilize the secure storage in our machine model efficiently. To optimize the security cost on data that has to be stored in non-trusted storage, we propose a data grouping technique. Security operations can be performed over the group of data instead of over each piece separately. We show an interesting application of the data grouping technique to reduce the security cost. We test the effectiveness of our optimizations on a recently proposed software protection scheme that involves large overhead. Our results show that the above optimizations are effective and reduce the security overhead significantly.","","0-7695-2499","10.1109/CGO.2006.8","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1611553","","Optimizing compilers;Data security;Costs;Hardware;Cryptography;Software protection;Secure storage;Application software;Software performance;Educational institutions","optimising compilers;security of data","compiler optimizations;security overhead reduction;data grouping technique;software protection scheme","","1","29","","","","","","IEEE","IEEE Conferences"
"A Multimodal and Multilevel Ranking Scheme for Large-Scale Video Retrieval","S. C. H. Hoi; M. R. Lyu","NA; NA","IEEE Transactions on Multimedia","","2008","10","4","607","619","A critical issue of large-scale multimedia retrieval is how to develop an effective framework for ranking the search results. This problem is particularly challenging for content-based video retrieval due to some issues such as short text queries, insufficient sample learning, fusion of multimodal contents, and large-scale learning with huge media data. In this paper, we propose a novel multimodal and multilevel (MMML) ranking framework to attack the challenging ranking problem of content-based video retrieval. We represent the video retrieval task by graphs and suggest a graph based semi-supervised ranking (SSR) scheme, which can learn with small samples effectively and integrate multimodal resources for ranking smoothly. To make the semi-supervised ranking solution practical for large-scale retrieval tasks, we propose a multilevel ranking framework that unifies several different ranking approaches in a cascade fashion. We have conducted empirical evaluations of our proposed solution for automatic search tasks on the benchmark testbed of TRECVID2005. The promising empirical results show that our ranking solutions are effective and very competitive with the state-of-the-art solutions in the TRECVID evaluations.","1520-9210;1941-0077","","10.1109/TMM.2008.921735","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4523959","Content-based video retrieval;graph representation;multilevel ranking;multimodal fusion;multimedia retrieval;semi-supervised ranking;support vector machines","Large-scale systems;Search engines;Content based retrieval;Video sharing;Optical character recognition software;Information retrieval;Benchmark testing;Acoustical engineering;Video signal processing;Image retrieval","content-based retrieval;learning (artificial intelligence);multimedia systems;video retrieval","multilevel ranking scheme;multimodal ranking scheme;large-scale video retrieval;large-scale multimedia retrieval;content-based video retrieval;short text queries;large-scale learning;semisupervised ranking scheme","","22","44","","","","","","IEEE","IEEE Journals & Magazines"
"Cost-effective allocation of NASA's rocket propulsion test assets","A. K. Gupta; A. Q. Tu","The Aerospace Corporation, 200 N. Aviation Boulevard, El Segundo, CA 90245, USA; The Aerospace Corporation, 200 N. Aviation Boulevard, El Segundo, CA 90245, USA","2009 IEEE Aerospace conference","","2009","","","1","15","NASA's vision for space exploration requires the development of several new rocket propulsion test articles such as the J-2X engine and the Orion propulsion. Enhanced testing capabilities are needed to properly address the feasibility, performance and lifetime aspects of new rocket propulsion technologies. The aerospace corporation performed a study to evaluate options for NASA's existing and proposed rocket propulsion test assets that can meet emerging NASA, DOD and commercial requirements over the next decade. Alternate options for test facility allocation are developed with the objective of minimizing total cost over the planning horizon. The mathematical problem presented a special challenge in terms of multiple scenarios that exist for the current and intermittent states of these facilities while not in testing mode. Determining the optimal mix require a sophisticated model that can handle the six different states a facility can be in at a given point in time and ten different types of costs that may incur depending on the state. Costs are incurred when the facility is in active state as well as inactive state waiting for the next planned tests. Mathematical optimization and search algorithms were used to develop alternative solutions that satisfied the defined constraints. The problem also presented a challenge for commercial optimization software and was therefore augmented with internally developed algorithms.","1095-323X;1095-323X","978-1-4244-2621-8978-1-4244-2622","10.1109/AERO.2009.4839734","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4839734","","Asset management;Rockets;Propulsion;Aerospace testing;Costs;Space exploration;Engines;Life testing;Space technology;Performance evaluation","aerospace computing;aerospace industry;aerospace propulsion;aerospace test facilities;aerospace testing;resource allocation;rocket engines","cost-effective allocation;NASA rocket propulsion test assets;space exploration;J-2X engine;Orion propulsion;test facility allocation;mathematical optimization;search algorithms;commercial optimization software","","","","","","","","","IEEE","IEEE Conferences"
"BIST Driven Power Conscious Post-Manufacture Tuning of Wireless Transceiver Systems Using Hardware-Iterated Gradient Search","V. Natarajan; S. K. Devarakond; S. Sen; A. Chatterjee","NA; NA; NA; NA","2009 Asian Test Symposium","","2009","","","243","248","In this paper, a fast RF BIST-driven post-manufacture tuning methodology for yield improvement of RF transceiver systems is presented. The core algorithms optimize multiple transceiver performance metrics concurrently using a hardware-iterated gradient search algorithm that uses diagnostic BIST data to guide the tuning of circuit and software level parameters. Intelligent ¿initial guess¿ values for the circuit and software tuning knobs at the start of the tuning process allow rapid convergence. Power consumption is given key consideration through the tuning process. Further, self-tuning is performed with little or no external tester support. The viability of the proposed scheme has been demonstrated through an experimental RF hardware prototype. Experimental results demonstrate significant yield recovery while allowing up to 10X savings in test/tuning time.","1081-7735;2377-5386","978-0-7695-3864","10.1109/ATS.2009.75","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5359347","","Built-in self-test;Transceivers;Circuit optimization;Tuning;Radio frequency;Software algorithms;Measurement;Software performance;Convergence;Energy consumption","built-in self test;circuit tuning;gradient methods;optimisation;transceivers","RF BIST-driven post-manufacture tuning methodology;wireless transceiver systems;hardware-iterated gradient search;RF transceiver systems;core algorithms;optimize multiple transceiver performance metrics;software level parameters;intelligent initial guess values;software tuning knobs;self-tuning;RF hardware prototype","","11","21","","","","","","IEEE","IEEE Conferences"
"The Design and Optimization of a 25 kS/s 10 bit Micropower Current S/H Cell for Weak Current Bio-medical Applications","K. L. Tsang; J. Yuan","NA; NA","4th IEEE International Symposium on Electronic Design, Test and Applications (delta 2008)","","2008","","","11","14","In this paper, a micropower current sample-and-hold front-end is designed for weak current bio-medical applications in a 0.35-mum standard CMOS process. The design reduces the distortion of the current-mode sample-and-hold stage by exploiting the exponential I-V relationship of weakly-inverted MOS transistors. The design and optimization process for the stage is introduced in the paper. With the optimized design, SPICE simulation shows that the sample-and-hold stage can achieve over 60 dB SNDR at the sampling rate of 25 kS/s, with the input signal at the Nyquist frequency of 12.4 kHz. The input current range is 100 nA. The power consumption of the stage is about 3.6 muW.","","978-0-7695-3110","10.1109/DELTA.2008.121","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4459501","","Design optimization;Application software;MOSFETs;Signal design;Electrodes;Energy consumption;DNA;Radiofrequency interference;Electronic equipment testing;Process design","biomedical electronics;CMOS analogue integrated circuits;lab-on-a-chip;MOSFET;sample and hold circuits;SPICE","micropower current S-H cell;weak current bio-medical applications;sample-and-hold front-end;I-V relationship;weakly-inverted MOS transistors;SPICE simulation;optimization process;CMOS process;lab-on-chip diagnostics;size 0.35 mum;frequency 12.4 kHz;current 100 nA;power 3.6 muW","","","6","","","","","","IEEE","IEEE Conferences"
"Optimization Design and Parametric Modelling of Gear Reducer","L. Zhu; G. Wang","NA; NA","2009 Second International Conference on Information and Computing Science","","2009","4","","125","127","The purpose of this paper is to establish a mathematical model for optimization design of three-stage cylindrical gear reducer and carry out a research on parametric design based on Pro/Engineer. In the optimization design, the minimum of overall gearpsilas center distance in reducer is the objective function, the pinionspsila tooth numbers, normal modules and the first two-stage transmission ratios are used as design variables. The optimization results of the gear reducer can be used in the parametric modelling with the help of Pro/Toolkit development module and Visual C++. The motional condition and working process of the gear reducer are shown by modelling and simulation. The three-stage cylindrical gear reducer can be optimized and designed parametrically in the software, which have the applicable significance of actual engineering and can be the base of the design of other kinds of gear reducer.","2160-7443;2160-7451","978-0-7695-3634","10.1109/ICIC.2009.341","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5169141","gear;reducer;optimization design;parametric modelling","Design optimization;Parametric statistics;Gears;Design engineering;Teeth;Mathematical model;Testing;Mechanical engineering;Cost function;Production facilities","CAD;gears;mechanical engineering computing;optimisation","optimization design;parametric modelling;cylindrical gear reducer;Pro/Engineer;Pro/Toolkit development module;Visual C++;pinion tooth numbers","","","6","","","","","","IEEE","IEEE Conferences"
"Hydro thermal scheduling using particle swarm optimization","C. Samudi; G. P. Das; P. C. Ojha; T. S. Sreeni; S. Cherian","Kalki Communication Technologies Ltd., Bangalore, India; Kalki Communication Technologies Ltd., Bangalore, India; Kalki Communication Technologies Ltd., Bangalore, India; Kalki Communication Technologies Ltd., Bangalore, India; Kalki Communication Technologies Ltd., Bangalore, India","2008 IEEE/PES Transmission and Distribution Conference and Exposition","","2008","","","1","5","This paper presents a new approach of particle swarm optimization (PSO) algorithm for short term hydro thermal scheduling (HTS) problems. Various possible particle selections have been studied and its effects on the global optima have been discussed. The effectiveness and stochastic nature of proposed algorithm has been tested with standard test case and the results have been compared with earlier works. This paper also describes software developed for short term hydro-thermal scheduling by considering hydro economic dispatch and thermal unit commitment. The proposed algorithm is ideally suitable for hydro-thermal co-ordination problems, hydro economic dispatch problems with unit commitment, thermal economic dispatch with unit commitment problems and scheduling of hydraulically coupled plants.","2160-8555;2160-8563","978-1-4244-1903-6978-1-4244-1904","10.1109/TDC.2008.4517221","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4517221","Particle Swarm Optimization;Hydrothermal coordination","Particle swarm optimization;Scheduling algorithm;Stochastic processes;Power generation economics;Power generation;Communications technology;High temperature superconductors;Testing;Dynamic programming;Mathematical model","hydrothermal power systems;particle swarm optimisation;power generation dispatch;power generation scheduling","hydro thermal scheduling;particle swarm optimization;PSO;hydro economic dispatch;thermal unit commitment;hydraulically coupled plants","","12","21","","","","","","IEEE","IEEE Conferences"
"Optimization of Calibration Procedure for Reverberation Chamber Based on NI LabVIEW Platform","W. Qingguo; C. Erwei","National Key Lab on EM and EM protection, Mechanical Engineering College, Shijiazhuang 050003 China; National Key Lab on EM and EM protection, Mechanical Engineering College, Shijiazhuang 050003 China","2007 International Symposium on Electromagnetic Compatibility","","2007","","","154","157","As an alternative technique, reverberation chamber has been more and more accepted and applied for EMC test, especially for EMS tests for electronic equipment, ordnance electronics and also advanced MIMO technologies for wireless communication. In this paper, the test standards are first analyzed and compared. Secondly, by considering the economy and time saving factors, an optimized programming procedure and program structure are designed and presented for realizing the calibration test based on NI LabVIEW platform. Finally, some experimental results for a newly built reverberation chamber are presented.","2158-110X;2158-1118","978-1-4244-1371-3978-1-4244-1372","10.1109/ELMAGC.2007.4413454","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4413454","reverberation chamber;standards;control software;LabVIEW","Calibration;Reverberation chamber;Electronic equipment testing;Electromagnetic compatibility;Medical services;Electronic equipment;MIMO;Communications technology;Wireless communication;Design optimization","calibration;MIMO communication;optimisation","calibration procedure optimization;reverberation chamber;LabVIEW platform;electronic equipment;MIMO technologies;wireless communication","","1","9","","","","","","IEEE","IEEE Conferences"
"Simulation System of Automated Three-Dimensional Garage Based on Configuration Software","Jianjun Meng; Zeqing Yang; Chenghui Yang; Pengjun Li","Institute of Mech-Electronic Technology, Lanzhou Jiaotong University, Lanzhou, 730070. E-mail: mengjj@mail.lzjtu.cn; NA; NA; NA","2006 6th World Congress on Intelligent Control and Automation","","2006","2","","6198","6202","Combining the field condition with operation requirements in large-scale automated three-dimensional garage, a simulation system was developed by adopting configuration technique. Ant colony algorithm was utilized to assign berth address rationally. Configuration software was used for establishing simulation model and data dictionary. This simulation system mimicked the tasking process of three-dimensional garage. It also had the functions such as communication interface and database interface. Finally, it was tested and satisfactory results were obtained. When applied to the development and field training of relevant engineering project, this system can shorten project cycle effectively, lower costs and reduce impact in production and operation","","1-4244-0332","10.1109/WCICA.2006.1714274","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1714274","automated three-dimensional garage;configuration software;simulation;ant colony algorithm","Large-scale systems;Dictionaries;Databases;Testing;Systems engineering and theory;Costs;Production systems;Software algorithms;Intelligent control;Automation","artificial life;configuration management;database management systems;digital simulation;optimisation;traffic control","simulation system;configuration software;large-scale automated 3D garage;ant colony algorithm;data dictionary;communication interface;database interface;engineering project","","3","","","","","","","IEEE","IEEE Conferences"
"A Novel Bicriteria Scheduling Heuristics Providing a Guaranteed Global System Failure Rate","A. Girault; H. Kalla","INRIA and Grenoble University, Saint-Ismier Cedex; University of Batna, Batna","IEEE Transactions on Dependable and Secure Computing","","2009","6","4","241","254","We propose a new framework for the (length and reliability) bicriteria static multiprocessor scheduling problem. Our first criterion remains the schedule's length, which is crucial to assess the system's real-time property. For our second criterion, we consider the global system failure rate, seen as if the whole system were a single task scheduled onto a single processor, instead of the usual reliability, because it does not depend on the schedule length like the reliability does (due to its computation in the classical exponential distribution model). Therefore, we control better the replication factor of each individual task of the dependency task graph given as a specification, with respect to the desired failure rate. To solve this bicriteria optimization problem, we take the failure rate as a constraint, and we minimize the schedule length. We are thus able to produce, for a given dependency task graph and multiprocessor architecture, a Pareto curve of nondominated solutions, among which the user can choose the compromise that fits his or her requirements best. Compared to the other bicriteria (length and reliability) scheduling algorithms found in the literature, the algorithm we present here is the first able to improve significantly the reliability, by several orders of magnitude, making it suitable to safety-critical systems.","1545-5971;1941-0018;2160-9209","","10.1109/TDSC.2008.50","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4609387","Reliability;bicriteria optimization;Pareto optima;static multiprocessor scheduling;reliability block diagrams;safety-critical systems.;Reliability;Testing;and Fault-Tolerance;Real-time and embedded systems;Reliability;availability;and serviceability;Fault tolerance","Processor scheduling;Real time systems;Exponential distribution;Scheduling algorithm;Distributed computing;Constraint optimization;Pareto optimization;Memory architecture;Embedded system;Hardware","exponential distribution;graph theory;minimisation;multiprocessing systems;Pareto optimisation;processor scheduling;program diagnostics;real-time systems;safety-critical software;system recovery","bicriteria static multiprocessor heuristic scheduling algorithm;global system failure rate;schedule length minimization framework;real-time system property;single task scheduling;classical exponential distribution model;replication factor;dependency task graph;bicriteria optimization problem;multiprocessor architecture;Pareto optimal curve;safety-critical system;reliability block diagram","","33","28","","","","","","IEEE","IEEE Journals & Magazines"
"An Evaluation of Similarity Coefficients for Software Fault Localization","R. Abreu; P. Zoeteweij; A. J. c. Van Gemund","Delft University of Technology; Delft University of Technology; Delft University of Technology","2006 12th Pacific Rim International Symposium on Dependable Computing (PRDC'06)","","2006","","","39","46","Automated diagnosis of software faults can improve the efficiency of the debugging process, and is therefore an important technique for the development of dependable software. In this paper we study different similarity coefficients that are applied in the context of a program spectral approach to software fault localization (single programming mistakes). The coefficients studied are taken from the systems diagnosis/automated debugging tools Pinpoint, Tarantula, and AMPLE, and from the molecular biology domain (the Ochiai coefficient). We evaluate these coefficients on the Siemens Suite of benchmark faults, and assess their effectiveness in terms of the position of the actual fault in the probability ranking of fault candidates produced by the diagnosis technique. Our experiments indicate that the Ochiai coefficient consistently outperforms the coefficients currently used by the tools mentioned. In terms of the amount of code that needs to be inspected, this coefficient improves 5% on average over the next best technique, and up to 30% in specific cases","","0-7695-2724","10.1109/PRDC.2006.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4041886","","Fault diagnosis;Software debugging;Software reliability;Computer bugs;Embedded system;Software testing;System testing;Particle measurements;Mathematics;Computer science","fault diagnosis;probability;program debugging;program diagnostics;software fault tolerance","similarity coefficients;software fault localization;automated software fault diagnosis;dependable software;program spectral approach;automated debugging tools;Pinpoint;Tarantula;AMPLE;molecular biology;Ochiai coefficient;Siemens Suite;benchmark faults;probability ranking","","112","20","","","","","","IEEE","IEEE Conferences"
"Simulating embedded targets for efficient code implementation","M. Muresan; D. Pitica","Applied Electronics Department, Technical University of Cluj-Napoca, Romania; Applied Electronics Department, Technical University of Cluj-Napoca, Romania","2009 32nd International Spring Seminar on Electronics Technology","","2009","","","1","4","Nowadays design of embedded systems for controlling processes requires high complexity software modules. In this way, the automatic code generation approach for designing such embedded systems seems to be the most competitive solution in terms of cost-effectiveness and performance. The solution holds some of the advantages: most of the software tooling required for designing an embedded controller will consist in a control algorithm simulator (as Matlab/Simulink) and code generation software for deploying the algorithm into the embedded controller along with some drawbacks: lack of code efficiency and a difficult optimization process. For that solution, code generation templates and S-function compilers are widely spread and cover a large family of microcontrollers and microprocessor. The paper will present a method for developing an embedded target simulator for efficient code implementation along with a step-by-step procedure for code optimization. Deployment tests and experimental results will be also covered for a simple PID algorithm as test reference.","2161-2528;2161-2064","978-1-4244-4260","10.1109/ISSE.2009.5206997","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5206997","","Embedded software;Embedded system;Software tools;Automatic generation control;Software algorithms;Testing;Automatic control;Process control;Control systems;Algorithm design and analysis","embedded systems;microcomputers;microcontrollers;program compilers;software tools;three-term control","embedded targets;efficient code implementation;embedded systems;high complexity software modules;automatic code generation;software tooling;embedded controller;control algorithm simulator;Matlab;Simulink;code generation software;optimization;S-function compilers;microcontrollers;microprocessor;code optimization;PID algorithm","","2","6","","","","","","IEEE","IEEE Conferences"
"Selecting the right data storage approach for an automatic test system","C. Bright; W. Logan","National Instruments, 11500 N. Mopac Expwy, Austin, TX 78759, USA; National Instruments, 11500 N. Mopac Expwy, Austin, TX 78759, USA","2008 IEEE AUTOTESTCON","","2008","","","490","492","When developing an automatic test system, data storage is many times an afterthought. Engineers spend months or even years evaluating different hardware options, software development environments, and approaches to overall test system architecture. However, little forethought is given on how to best store the data acquired from these systems to optimize the post-processing and reporting of the results. Without proper data storage, issues can quickly arise that limit the ability to analyze measurement data, exchange technical information and results, and derive decisions from current and past tests. This paper will examine several common data storage choices including ASCII, binary, XML and databases, and the advantages and disadvantages of each approach in an automatic test system. This paper will discuss these storage formats and the tradeoffs made with each in an automatic test system. The paper will also propose a new hybrid file format and discuss how it can be integrated into existing systems.","1088-7725;1558-4550","978-1-4244-2225-8978-1-4244-2226","10.1109/AUTEST.2008.4662665","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4662665","file IO;data storage;ASCII;binary;database","Databases;Time division multiplexing;XML;Data models;Memory;Correlation;Instruments","automatic test software;software engineering;XML","automatic test system;data storage;hardware options;software development environments;exchange technical information;ASCII;XML;hybrid file format","","","","","","","","","IEEE","IEEE Conferences"
"Type Inference for Soft-Error Fault-Tolerance Prediction","G. Munkby; S. Schupp","NA; NA","2009 IEEE/ACM International Conference on Automated Software Engineering","","2009","","","65","75","Software systems are becoming increasingly vulnerable to a new class of soft errors, originating from voltage spikes produced by cosmic radiation. The standard technique for assessing the source-level impact of these soft errors, fault injection - essentially a black-box testing technique - provides limited high-level information. Since soft errors can occur anywhere, even control-structured white-box techniques offer little insight. We propose a type-based approach, founded on data-flow structure, to classify the usage pattern of registers and memory cells. To capture all soft errors, the type system is defined at the assembly level, close to the hardware, and allows inferring types in the untyped assembly representation. In a case study, we apply our type inference scheme to a prototype brake-by-wire controller, developed by Volvo Technology, and identify a high correlation between types and fault-injection results. The case study confirms that the inferred types are good predictors for soft-error impact.","1938-4300","978-1-4244-5259-0978-0-7695-3891","10.1109/ASE.2009.61","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5431783","fault injection;assembly type system;test selection and prioritization","Fault tolerance;Computer errors;Assembly systems;Software systems;Voltage;Testing;Error correction;Registers;Hardware;Prototypes","data flow computing;program testing;reasoning about programs;software fault tolerance;type theory","type inference;soft-error fault tolerance prediction;software systems;voltage spikes;cosmic radiation;source level impact;soft errors;fault injection;black-box testing;high-level information;data flow structure;usage pattern;memory cells;untyped assembly representation;brake-by-wire controller","","","24","","","","","","IEEE","IEEE Conferences"
"Software Agents: Introduction and Application to Optimum 3G Network Planning [EM Programmer's Notebook]","A. Esposito; L. Tarricone; M. Zappatore","Dept. Innovation Eng., University of Salento, Via Monteroni 73100 Lecce, Italy; Dept. Innovation Eng., University of Salento, Via Monteroni 73100 Lecce, Italy; Dept. Innovation Eng., University of Salento, Via Monteroni 73100 Lecce, Italy","IEEE Antennas and Propagation Magazine","","2009","51","4","147","155","The continuous evolution of wireless networks, as well as the attention paid by public opinion to human exposure to electromagnetic fields radiated by base-station antennas, render the development of software tools to support optimum design and planning of wireless networks highly desirable. Although many tools are already available, open problems are still on the table, such as the efficient implementation of optimization methods to identify optimum locations and electrical parameters for base-station antennas. In this paper, the recent technology of software agents is adopted, in conjunction with genetic algorithms and parallel computing, in order to perform effective and efficient optimization of wireless networks. The paper introduces and discusses the suitability of these information technologies for attacking the problems addressed. The use of the proposed solution, based on software agents, was tested on real cases. Impressive results were achieved for both the accuracy and the performance attained, with the use of low-cost computing platforms and freeware tools.","1045-9243;1558-4143","","10.1109/MAP.2009.5338708","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5338708","Genetic algorithms;parallel processing;radio propagation;optimization methods;personal communication networks;land mobile radio cellular systems;land mobile radio equipment","Software agents;Application software;Wireless networks;Humans;Electromagnetic fields;Software tools;Optimization methods;Genetic algorithms;Parallel processing;Information technology","3G mobile communication;cellular radio;electromagnetic fields;genetic algorithms;grid computing;mobile antennas;parallel processing;software agents;telecommunication computing","software agent;optimum 3G network planning;wireless network design;electromagnetic fields radiation;base-station antenna;software tool;optimization method;genetic algorithm;parallel computing;freeware tool;land mobile radio cellular system;electrical parameter;grid computing","","4","23","","","","","","IEEE","IEEE Journals & Magazines"
"A Precise Chaotic Particle Swarm Optimization Algorithm based on Improved Tent Map","Y. He; J. Zhou; C. Li; J. Yang; Q. Li","NA; NA; NA; NA; NA","2008 Fourth International Conference on Natural Computation","","2008","7","","569","573","The target of this paper is to enhance the precision of existent chaotic particle swarm optimization algorithm in the nonlinear optimizations problems. For this aim, a one-dimensional chaotic search pattern using improved tent map is introduced. This new pattern can also be combined with particle swarm optimization algorithm. The novel hybrid algorithm is validated for several test functions and the experimental results demonstrate its efficiency and precision to obtain global optimal solution.","2157-9555;2157-9563","978-0-7695-3304","10.1109/ICNC.2008.588","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4668041","","Chaos;Particle swarm optimization;Logistics;Helium;Hydroelectric power generation;Testing;Mathematics;Physics computing;Application software;Stochastic processes","chaos;nonlinear programming;particle swarm optimisation;search problems","chaotic particle swarm optimization algorithm;tent map;nonlinear optimizations problems;1D chaotic search pattern","","7","15","","","","","","IEEE","IEEE Conferences"
"Simulation-based FDP & FCP analysis with queueing models","Q. P. Hu; R. Peng; S. H. Ng; H. Q. Wang","Department of Information Systems, City University of Hong Kong, China; Department of Industrial and Systems Engineering, National University of Singapore, Singapore; Department of Industrial and Systems Engineering, National University of Singapore, Singapore; Department of Information Systems, City University of Hong Kong, China","2008 IEEE International Conference on Industrial Engineering and Engineering Management","","2008","","","1577","1581","Continuous efforts have been devoted to software reliability modeling evolution over the past decades in order to adapt to the practical complex software testing environments. Many models have been proposed to describe the software fault related process. These software reliability growth models (SRGMs) have evolved from describing one fault detection process (FDP) into incorporating fault correction process (FCP) as well, in order to provide higher accuracy with more information. To provide mathematical tractability, models need to have closed form, with restrictive assumptions in a narrow sense. This in turn confines their capability for general applications. Alternatively, in this paper a general simulation based queueing modeling framework is proposed to describe FDP and FCP, with resource factors from practical software testing incorporated. Good simulation performance is observed with a numerical example. Furthermore, release time and debugger staffing issues are investigated with a revised cost model. The analysis is conducted through a simulation optimization approach.","2157-3611;2157-362X","978-1-4244-2629-4978-1-4244-2630","10.1109/IEEM.2008.4738137","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4738137","Software Reliability;Non-homogeneous Poisson Processes;Queueing Systems;Discrete Event Simulation","Analytical models;Queueing analysis;Software reliability;Software testing;Modeling;Discrete event simulation;Reliability engineering;Fault detection;Mathematical model;Application software","program debugging;program testing;queueing theory;software fault tolerance","queueing models;software testing;software fault related process;software reliability growth models;fault detection process;fault correction process;debugger staffing","","1","18","","","","","","IEEE","IEEE Conferences"
"Study and Applicaion of FTA Software System","S. Shang; L. Liu; S. Xue","College of Industrial Design & Information Engineering, Beijing Institute of Clothing Technology, Beijing, 100029, China. E-MAIL: shangshuy@163.com; College of Information Technology, Hebei University of Economics & Business, Shijiazhuang, 050011, China; College of Information Technology, Hebei University of Economics & Business, Shijiazhuang, 050011, China","2007 International Conference on Machine Learning and Cybernetics","","2007","3","","1299","1303","Fault tree analysis (FTA) is an effective technique to analyze the reliability of a complex system. Through selecting the reasonable top events, putting out the fault tree of the system,and carrying out quantitative analysis about the fault tree, the information of the system under study can be systematically understood. First, this paper studies how to calculate the occurrence probability of the top event and compute the importance degree of a basic event. Then this paper put forwards the realized arithmetic of FTA software system. Also, it analyzed and designed the class of fault tree, calculation module of minimal cut, graph drawing of tree, and calculation class of reliably characteristic. On the basis of the above study, the software system of Fault Tree Analysis is established. Making use of the fault tree analysis system, the reliability of the intelligent electrical apparatus is analyzed. The fault tree of intelligent electrical apparatus system is established according to its composing and the relating test data. It can find the minimal cut set, and gain the probability configuration importance degree. These studies are helpful for the reliability design of the intelligent electrical apparatus.","2160-133X;2160-1348","978-1-4244-0972-3978-1-4244-0973","10.1109/ICMLC.2007.4370345","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4370345","FTA;Reliability Calculation;Software System;System Analysis","US Department of Transportation;Software systems;Probability;Fault trees;Information analysis;Machine learning;Cybernetics;Educational institutions;Failure analysis;Logic","electrical engineering computing;electrical faults;fault trees;maintenance engineering;optimisation;probability","FTA software system;fault tree analysis technique;top event occurrence probability;basic event importance degree;intelligent electrical apparatus system reliability;minimal cut set;optimization design","","","5","","","","","","IEEE","IEEE Conferences"
"Wire Segment Length and Switch Box Co-Optimization for FPGA Architectures","K. Siozios; D. Soudris","VLSI Design and Testing Center, Department of Electrical and Computer Engineering, Democritus University of Thrace, 67100, Xanthi, Greece, email: ksiop@ee.duth.gr; VLSI Design and Testing Center, Department of Electrical and Computer Engineering, Democritus University of Thrace, 67100, Xanthi, Greece, email: dsoudris@ee.duth.gr","2006 International Conference on Field Programmable Logic and Applications","","2006","","","1","4","The novel methodology for designing a high-performance and low-energy FPGA interconnection architecture consisting of appropriate wire segments and multiple switch boxes is introduced. Depending on the localized performance and energy consumption requirements of each specific region of FPGA architecture, we derive a set of corresponding spatial routing information of the applications mapped onto reconfigurable device. In this paper, an interconnection network with segments L1&amp;L2 and 3 different switch box regions is used. The selection criterion for our approach is the minimal energytimesdelay product (EDP). The proposed methodology is fully-supported by the software tool called EX-VPR. With this interconnection architecture we achieved EDP reduction by 56%, performance increase by 47%, reduction in leakage power by 18%, reduction in total energy consumption by 9%, at the expense of increase of channel width by 15% compared to conventional FPGA architectures","1946-147X;1946-1488","1-4244-0312","10.1109/FPL.2006.311321","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4101083","","Switches;Wire;Field programmable gate arrays;Energy consumption;Computer architecture;Design methodology;Routing;Application software;Multiprocessor interconnection networks;Software tools","circuit optimisation;field programmable gate arrays;logic design;reconfigurable architectures","wire segment length;switch box co-optimization;field programmable gate arrays;FPGA interconnection architecture;wire segments;switch boxes;spatial routing information;reconfigurable device;switch box regions","","4","8","","","","","","IEEE","IEEE Conferences"
"Real-Coded Quantum Evolutionary Algorithm Based on Immune Theory for Multi-modal Optimization Problems","X. You; Y. Zhang; S. Liu","NA; NA; NA","2008 International Conference on Computer Science and Software Engineering","","2008","1","","403","406","A novel real-coded immune quantum evolutionary algorithm for multi-modal function optimization (MRIQEA) is proposed. By niching methods population is divided into subpopulations automatically, local search is carried by the immune mechanism, each subpopulation can obtain precise solutions, and then the population can maintain all optimal solutions. Because of the quantum evolutionary algorithm with intrinsic parallelism it can maintain quite nicely the population diversity than the classical evolutionary algorithm, because of the adaptive immune operator and real representation for the chromosome it can converge to all optimal solutions rapidly. The technique for improving the performance of MRIQEA has been described and its superiority is shown by some simulation experiments in this paper.","","978-0-7695-3336","10.1109/CSSE.2008.1344","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721772","quantum computing;multi-modal function;optimization;co-evolutionary strategy;real-coded chromosome","Quantum mechanics;Evolutionary computation;Immune system;Quantum computing;Biological cells;Educational institutions;Parallel processing;Testing;Computer science;Software engineering","artificial immune systems;evolutionary computation;quantum computing","real-coded quantum evolutionary algorithm;immune theory;multimodal function optimization problems;niching methods;adaptive immune operator;MRIQEA","","2","9","","","","","","IEEE","IEEE Conferences"
"Measuring Non-Functional Properties in Software Product Line for Product Derivation","N. Siegmund; M. Rosenmüller; M. Kuhlemann; C. Kästner; G. Saake","NA; NA; NA; NA; NA","2008 15th Asia-Pacific Software Engineering Conference","","2008","","","187","194","A software product line (SPL) enables stakeholders to derive different software products for a domain while providing a high degree of reuse of their code units. Software products are derived in a configuration process by composing different code units. The configuration process becomes complex if SPLs contain hundreds of features. In many cases, a stakeholder is not only interested in functional but also in non-functional properties of a desired product. Because SPLs can be used in different application scenarios alternative implementations of already existing functionality are developed to meet special non-functional requirements, like restricted binary size and performance guarantees. To enable these complex configurations we discuss and present techniques to measure non-functional properties of software modules and use these values to compute SPL configurations optimized to the users needs.","1530-1362;1530-1362","978-0-7695-3446","10.1109/APSEC.2008.45","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724547","Software Product Lines;Non-functional Properties;Product Derivation","Software measurement;Energy consumption;Sorting;Visualization;Software engineering;Application software;Testing;Time to market;Embedded system;Large-scale systems","software reusability","software product lines;product derivation;nonfunctional properties;binary size","","8","35","","","","","","IEEE","IEEE Conferences"
"A SCADA System for Water Potential Management of a Hydropower Plants Cascade","A. M. Tarta; G. Ungureanu; D. Capatana; F. Covaciu","Department of Computer Science Babe¿-Bolyai University. adriana@cs.ubbcluj.ro; IPA R&D Institute for Automation, Cluj Subsidiary. ipa@automation.ro; IPA R&D Institute for Automation, Cluj Subsidiary. ipa@automation.ro; IPA R&D Institute for Automation, Cluj Subsidiary. ipa@automation.ro","2006 IEEE International Conference on Automation, Quality and Testing, Robotics","","2006","1","","410","414","Hydropower plants exploitation optimization is a subject of interest everywhere. By optimizing the hydropower plants exploitation we refer to the use of water resources so that the power production is maximized, respecting, at the same time, the constraints imposed by the water consumer from the geographical region. The software system can successfully be used for these kind of goals. In this paper we present a SCADA system designed and developed in order to optimize the daily exploitation of the hydro plants cascade. The software application is funded by the Romanian Ministry of Education and Research, in the frame of MENER program","","1-4244-0360-X1-4244-0361","10.1109/AQTR.2006.254571","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4022893","","SCADA systems;Hydroelectric power generation;Water resources;Power system management;Constraint optimization;Production;Time factors;Software systems;Design optimization;Application software","control engineering computing;hydroelectric power stations;machine control;SCADA systems","SCADA system;water potential management;hydropower plant cascade;hydropower plant exploitation optimization;water resource;power production maximization;geographical water consumer;software application system","","2","10","","","","","","IEEE","IEEE Conferences"
"Optimization of the bluff body in vortex flowmeter","Juan Xing; Tao Zhang; Yue Hu","Tianjin University, 300072, China; Tianjin University, 300072, China; Tianjin University, 300072, China","2008 7th World Congress on Intelligent Control and Automation","","2008","","","9195","9198","In order to improve the intensity of the vortices at low flow velocity, a series of simulations have been performed for vortex flowmeters to optimize the trapezoid bluff body in terms of its geometrical parameters through FLUENT software. Comparative tests of the flow field characteristic are carried out between the bluff bodies with different rear body angles and parallel part lengths respectively and the results demonstrate that the strongest vortex signals are generated by the bluff body with its rear angle of 45deg and parallel part length of 4 mm. Then liquid flow calibrations have been conducted on the optimized vortex sensors and the results agree approximately with those simulations. The optimized vortex sensor has a lower limit of 0.25 m/s.","","978-1-4244-2113-8978-1-4244-2114","10.1109/WCICA.2008.4594386","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4594386","vortex flow sensor;numerical simulation;bluff body;geometric parameters","Sensors;Calibration;Software measurement;Computational fluid dynamics;Optimization;Laboratories;Fluid flow","flowmeters;optimisation;vortices","vortex flowmeter;bluff body optimization;trapezoid bluff body;FLUENT software","","","5","","","","","","IEEE","IEEE Conferences"
"Software Quality model Analysis Program","A. A. Hamada; M. N. Moustafa; H. I. Shaheen","EDS, 29 Emtedad Ramses st., Nasr city, Cairo 11471, Egypt; Ain-Shams University, Faculty of Engineering, Computer and Systems Department, 1 Elsarayat st., Abassia, Cairo 11571, Egypt; Ain-Shams University, Faculty of Engineering, Computer and Systems Department, 1 Elsarayat st., Abassia, Cairo 11571, Egypt","2008 International Conference on Computer Engineering & Systems","","2008","","","296","300","It is vital that data is obtained so that actions can be taken to improve the performance. Such improvement can be measured in terms of improved quality, increased customer satisfaction and decreased cost of quality. Different researchers have proposed software quality models to help measure the quality of software products. These models often include metrics for this purpose. Some of the classical and recent models are discussed and analyzed in this paper showing the points of strength and weakness of each model type. A new comprehensive model is proposed and analyzed. A complete solution is discussed through the paper to enable an effective and efficient use of the proposed model to help the development team in prioritizing the important metrics while developing the software products according to some inputs from the user and the objectives of the software being developed. The solution developed is called the quality model analysis program (QAP) and is a fuzzy system that weights the proposed model attributes according to certain rules. The solution enables software project managers to better utilize their resources and take specific actions to better improve the quality of the software produced.","","978-1-4244-2115-2978-1-4244-2116","10.1109/ICCES.2008.4773015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4773015","","Software quality;Software measurement;Costs;ISO standards;IEC standards;Software testing;Q factor;Performance analysis;Cities and towns;Data engineering","software metrics;software quality","software quality model analysis program;customer satisfaction;comprehensive model;software products;software project managers","","2","12","","","","","","IEEE","IEEE Conferences"
"Concurrency Issues in Automating RTS for Web Services","M. Ruth; S. Tu","University of New Orleans; University of New Orleans","IEEE International Conference on Web Services (ICWS 2007)","","2007","","","1142","1143","Regression testing (RT), testing software with previously used test cases, is a mainstream practice in software maintenance. Regression test selection (RTS) is to reduce the number of tests which need to be retested. Safe RTS techniques add the assurance that no modification-revealing test case will be left unselected. Several effective safe RTS techniques were developed for traditional applications, but none of them can be directly applied to Web services, even though there have been RT tools and techniques for Web services test-case generation, and ranking competing services. We have developed an approach to adapt Rothermel and Harrold's safe RTS technique to Web services. This approach was designed to be automated. In doing so, we have recognized a set of challenging issues that arise as a result of multiple concurrent modifications in distributed, autonomous, but still interconnected services. We believe not only these issues are common to any automated RTS approach, the needs for the solutions to these issues will also become more and more keen as composite Web services are getting more and more ubiquitous.","","0-7695-2924","10.1109/ICWS.2007.65","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4279719","","Concurrent computing;Web services;System testing;Software testing;Computer science;Software maintenance;Automatic testing;Production systems;Radio access networks;Protection","program testing;Web services","regression test selection;software testing;Web services","","7","1","","","","","","IEEE","IEEE Conferences"
"Electromagnetic optimization using Taguchi method: a case study of linear antenna array design","Wei-Chung Weng; Fan Yang; V. Demir; A. Elsherbeni","Dept. of Electr. Eng., Mississippi Univ., University, MS, USA; Dept. of Electr. Eng., Mississippi Univ., University, MS, USA; Dept. of Electr. Eng., Mississippi Univ., University, MS, USA; Dept. of Electr. Eng., Mississippi Univ., University, MS, USA","2006 IEEE Antennas and Propagation Society International Symposium","","2006","","","2063","2066","This paper introduces a novel electromagnetic optimization technique using Taguchi method. To illustrate this technique, a linear antenna array is optimized to realize a null control pattern and a sector beam pattern. It is found that Taguchi method is easy to implement and it converges to the desired patterns quickly. This method is a good candidate for optimizing EM applications","1522-3965;1947-1491","1-4244-0123","10.1109/APS.2006.1710987","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1710987","","Linear antenna arrays;Design optimization;Computer aided software engineering;Antenna radiation patterns;Phased arrays;Testing;Optimization methods;Chemical engineering;Mechanical engineering;Manufacturing","antenna radiation patterns;electromagnetic fields;linear antenna arrays;optimisation;Taguchi methods","electromagnetic optimization technique;Taguchi method;linear antenna array design;null control pattern;sector beam pattern;EM applications","","4","6","","","","","","IEEE","IEEE Conferences"
"A Hybrid Particle Swarm Optimization Algorithm for Multimodal Function Optimization","J. Gu; L. Lin; H. Wang","NA; NA; NA","2009 International Workshop on Intelligent Systems and Applications","","2009","","","1","4","Particle swarm optimization (PSO) has shown its good performance on numerical function problems. However, on some multimodal functions the PSO easily suffers from premature convergence because of the rapid decline in velocity. In this paper, a hybrid PSO algorithm, called HPSO, is proposed, which employs a modified velocity model to guarantee a non-zero velocity. In addition, a Cauchy mutation operator conducted on the global best particle is used for improving the global search ability of PSO. Experimental studies on a suite of multimodal functions with many local minima show that the HPSO outperforms the standard PSO, CEP, Gaussian swarm with Gaussian mutation (GPSO+GJ) and Gaussian swarm with Cauchy mutation (GPSO+CJ) on most test functions.","","978-1-4244-3894-5978-1-4244-3893","10.1109/IWISA.2009.5072627","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5072627","","Particle swarm optimization;Genetic mutations;Evolutionary computation;Equations;Software algorithms;Educational institutions;Geography;Computer science;Software engineering;Convergence","convergence;evolutionary computation;mathematical operators;particle swarm optimisation;search problems","hybrid particle swarm optimization algorithm;multimodal function optimization;numerical function problems;premature convergence;velocity model;nonzero velocity;Cauchy mutation operator;global search ability","","","7","","","","","","IEEE","IEEE Conferences"
"Software Fault Localization Based on Testing Requirement and Program Slice","J. Sun; Z. Li; J. Ni; F. Yin","Sichuan University, China; Sichuan Radio and TV University, China; Sichuan University, China; Sichuan University, China; Southwest University for Nationalities, China","2007 International Conference on Networking, Architecture, and Storage (NAS 2007)","","2007","","","168","176","A heuristic approach is proposed to locate a fault according to the priority. To a given test case wt, fault localization has to be proceeded when its output wrong. Firstly, four assistant test cases, one failed and three successful test cases, are selected out according to the biggest cardinality of Req(wt,t<sub>i</sub>), which stand for the common testing requirements both covered by wt and t<sub>i</sub>. Then, code prioritization methodology is put forward based on program slice technique. Dynamic slice technique is taken for wt and execution slice technique for four assistant test cases. Some dices are constructed with different priority which means the possibility of containing bug and is evaluated according to the occurrences in the selected slices. Thirdly, the key algorithm including two procedures, refining and augmenting, is followed here to fault localization based on priority. In the refining phase, the most suspicious codes am checked step by step; in the augmenting phase, more codes will be gradually considered on the basis of direct data dependency. At last, experimental studies are performed to illustrate the effectiveness of the technique.","","0-7695-2927","10.1109/NAS.2007.51","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4286423","","Software testing;Computer science;Sun;Educational institutions;TV;Failure analysis;Computer bugs;Societies","program slicing;program testing;software reliability","software fault localization;testing requirement;program slice;heuristic approach;dynamic slice technique;augmenting phase;direct data dependency","","2","9","","","","","","IEEE","IEEE Conferences"
"Teaching digital test with BIST analyzer","A. Jutman; A. Tsertov; A. Tsepurov; I. Aleksejev; R. Ubar; H. -. Wuttke","Department of Computer Engineering, Tallinn University of Technology, Raja 15, 12618, Estonia; Department of Computer Engineering, Tallinn University of Technology, Raja 15, 12618, Estonia; Department of Computer Engineering, Tallinn University of Technology, Raja 15, 12618, Estonia; Department of Computer Engineering, Tallinn University of Technology, Raja 15, 12618, Estonia; Department of Computer Engineering, Tallinn University of Technology, Raja 15, 12618, Estonia; Department of Computer Engineering, Tallinn University of Technology, Raja 15, 12618, Estonia","2008 19th EAEEIE Annual Conference","","2008","","","123","128","This paper describes a new software tool for high quality training/learning in the field of digital microelectronics. Its main purpose is to give insight into reliability and quality assurance technologies based on linear feedback shift registers (LFSR) and other pseudo-random pattern generators (PRPG). Various PRPG types are becoming the mainstream test generation solution used in built-in self-test (BIST) structures. Taking into account complex theoretical concepts behind the microelectronics self-testing (including data coding and compression, cryptography, field theory, linear programming) it is important to effectively educate engineers in this field. The software tool we present in this paper is aimed at facilitating this goal. Unlike other similar systems, this tool facilitates study of various test optimization problems, allows fault coverage analysis for different circuits and with different LFSR parameters. The main didactic aim of the tool is presenting complicated concepts in a comprehensive graphical and analytical way. The multi-platform JAVA runtime environment allows for easy usage of the tool both in the classroom and at home. The BIST Analyzer represents an integrated simulation, training, and research environment that supports both analytic and synthetic way of learning. Due to the above mentioned facts the tool provides a unique training platform to use in courses on electronic testing and design for testability.","","978-1-4244-2008-7978-1-4244-2009","10.1109/EAEEIE.2008.4610171","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4610171","","Built-in self-test;Circuit faults;Optimization;Polynomials;Training;Testing;Microelectronics","built-in self test;learning systems;shift registers;software tools;training","digital test;BIST analyzer;software tool;digital microelectronics;linear feedback shift registers;pseudo-random pattern generators;built-in self-test","","2","8","","","","","","IEEE","IEEE Conferences"
"Batch-Optimistic Test-Cases Generation Using Genetic Algorithms","A. A. Sofokleous; A. S. Andreou","NA; NA","19th IEEE International Conference on Tools with Artificial Intelligence(ICTAI 2007)","","2007","1","","157","164","This paper proposes a dynamic software testing framework, which is able to analyse the source code of a program, create the necessary data structures for automatic testing, such as control flow graphs, and generate a near to optimum set of test cases with reference to a test coverage criterion. The framework consists of two sub-systems: the first is a program analysis system that identifies the type of statements and the complexity of conditions, performs analysis of variables, extracts code paths and creates the control flow graph (CFG) of the program under testing. The second is a test system that uses the CFG for automatically generating test data based on evolutionary computing. The latter system utilises a specially designed genetic algorithm to produce the set of test cases satisfying the selected coverage criterion. The efficacy and performance of the proposed testing approach is assessed and validated using a variety of sample programs.","1082-3409;2375-0197","0-7695-3015-X978-0-7695-3015","10.1109/ICTAI.2007.113","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4410278","","Genetic algorithms;Automatic testing;System testing;Software testing;Flow graphs;Performance analysis;Data structures;Automatic generation control;Performance evaluation;Data mining","flow graphs;genetic algorithms;program diagnostics;program testing;source coding","dynamic software testing framework;program source code;control flow graphs;program analysis system;evolutionary computing;genetic algorithm","","1","22","","","","","","IEEE","IEEE Conferences"
"The Optimized Design of Rijndael Algorithm Based on SOPC","S. Xiao; Y. Chen; P. Luo","NA; NA; NA","2009 International Conference on Information and Multimedia Technology","","2009","","","384","387","Based on the analysis of the round transformation and key expansion, the advanced encryption standard (AES) algorithm is optimized through the look-up table. And then the optimized Rijndael algorithm based on SOPC (system on a programmable chip) is designed and implemented through software and hardware. According to the software design flow chart of the optimized Rijndael algorithm, the program design of the corresponding B table and key generator are completed. The testing results are: the highest working frequency is 147 MHz; the biggest data flow is 2382 Mbps. The design of the Rijndael algorithm based on ""NIOS II + FPGA"" can achieve a higher data processing speed while it occupies relatively low resources. The design has a big breakthrough compared to the traditional FPGA realization.","","978-1-4244-5383-2978-0-7695-3922","10.1109/ICIMT.2009.103","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381177","","Design optimization;Algorithm design and analysis;Software algorithms;Field programmable gate arrays;Cryptography;Table lookup;Hardware;Software design;Flowcharts;Testing","cryptography;field programmable gate arrays;logic design;system-on-chip;table lookup","optimized design;Rijndael algorithm;SOPC;round transformation;key expansion;advanced encryption standard algorithm;look-up table;system on a programmable chip;software design flow chart;program design;key generator;NIOS II;data processing speed;FPGA realization","","2","10","","","","","","IEEE","IEEE Conferences"
"A Methodology for Power-aware Pipelining via High-Level Performance Model Evaluations","L. A. D. Bathen; Y. Ahn; S. Pasricha; N. D. Dutt","NA; NA; NA; NA","2009 10th International Workshop on Microprocessor Test and Verification","","2009","","","19","24","Power is one of the major constraints considered during the design of embedded software. In order to reduce power consumption without sacrificing performance, software needs to be optimized in order to run as efficiently as possible on a given platform. When attempting to optimize the mapping of a piece of software on a multiprocessor system, designers often face the chicken-and-egg problem of whether to schedule tasks first, or do memory allocation first, as either step will affect the different optimization opportunities the other may provide. Because each optimization will affect the system's power consumption, it is critically important to be able to monitor the effects these transformations have. In this paper we present a methodology that allows designers to quickly evaluate the impact each code optimization will have in the system's power. Our exploration engine relies on SystemC-based power/performance models to quickly and accurately evaluate the dynamic power due to memory accesses as well as the expected CPU power consumption.","2332-5674;1550-4093","978-1-4244-6480-7978-1-4244-6479","10.1109/MTV.2009.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5460786","Design Space Exploration;Power Modeling;Software Optimizations;Chip-Multiprocessors","Pipeline processing;Energy consumption;Design optimization;Software design;Embedded software;Software performance;Multiprocessing systems;Monitoring;Design methodology;Engines","multiprocessing systems;optimisation;pipeline processing","power-aware pipelining;high-level performance model evaluation;embedded software;multiprocessor system;SystemC-based power model;CPU power consumption","","1","22","","","","","","IEEE","IEEE Conferences"
"Improved Quantum-Inspired Evolutionary Algorithm and Its Application to 3-SAT Problems","X. Feng; E. Blanzieri; Y. Liang","NA; NA; NA","2008 International Conference on Computer Science and Software Engineering","","2008","1","","333","336","An improved quantum-inspired evolutionary algorithm is presented in this paper. Quantum angle is adopted to present the quantum bit in the proposed algorithm. A novel quantum rotation gate strategy is adopted to adjust the direction of the quantum gate which is used to update the quantum population. The step size is adaptively adjusted rather than a fixed angle. Furthermore, the particle swarm optimization is added into the improved algorithm to accelerate the convergent speed and develop the local searching ability. To demonstrate the effectiveness and applicability of the proposed approach, several experiments are performed on the 3-SAT problems. The results show that it is feasible and effective to solve the 3-SAT problem using the proposed algorithm.","","978-0-7695-3336","10.1109/CSSE.2008.1512","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721755","quantum computation;quantum-inspired evolutionary","Evolutionary computation;Quantum computing;Computer science;Logic testing;Circuit testing;Software engineering;Application software;Particle swarm optimization;Acceleration;NP-hard problem","computability;evolutionary computation;particle swarm optimisation;quantum computing","quantum-inspired evolutionary algorithm;3-SAT problems;quantum angle;quantum bit;quantum rotation gate strategy;particle swarm optimization","","","16","","","","","","IEEE","IEEE Conferences"
"Load Distribution Optimization System Based on MSCGA","Y. Wang; Y. Yan; S. Qiao","NA; NA; NA","2009 Ninth International Conference on Hybrid Intelligent Systems","","2009","2","","144","148","This article using mutative scale chaos genetic algorithm (MSCGA) in load distribution calculation of hot strip mill finishing rolling unit, using Visual C++ 6.0 developed set computing system software platform. It can achieve test calculation (positive algorithm) of field experience (mechanism) load distribution reasonable, inverse algorithm amendment, MSCGA optimization (manually and automated), device and technical parameters on-line adjustment, etcetera. The system has good people-machine interface and provides easy operation. It has fast operation speed and easy to amend parameters. Seamless butt of scene software can implement.","","978-0-7695-3745","10.1109/HIS.2009.141","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5254438","","Chaos;Optimization methods;Genetic algorithms;Strips;Milling machines;Finishing;Genetic engineering;Distributed computing;System software;Job shop scheduling","C++ language;genetic algorithms;production engineering computing;rolling mills;software engineering;user interfaces","load distribution;optimization;MSCGA;mutative scale chaos genetic algorithm;hot strip mill finishing rolling unit;Visual C++ 6.0;people-machine interface;seamless butt;scene software","","1","5","","","","","","IEEE","IEEE Conferences"
"Study of the algorithm of auto wheel speed sensor testing based on LabVIEW","J. Tian; C. Gong; L. Ji; X. Chen","School of Electronic Engineering Huaihai Institute of Technology Lianyungang City, Jiangsu Province, China; School of Electronic Engineering Huaihai Institute of Technology Lianyungang City, Jiangsu Province, China; School of Electronic Engineering Huaihai Institute of Technology Lianyungang City, Jiangsu Province, China; Ganyu Power Supply Company Ganyu City, Jiangsu Province, China","2009 9th International Conference on Electronic Measurement & Instruments","","2009","","","2-908","2-913","Auto wheel speed sensor and target gear are the information sources and key components for auto electronic control system, and it is also one of the core technology in the research field of automotive electronics, it becomes the challenge for auto test sector to test the auto wheel speed sensor in high efficiency and high-precision. In order to complete the test high reliability for auto wheel speed sensor and gear, the testing algorithm of the current general popular parameters of duty cycle error and its defects are studied. The factors affect the accuracy and reliability of duty cycle error are analyzed. A new Pitch error algorithm of the maximum cycle and the minimum based on rising edge and falling edge is provided, and it reflects the signal distortion better and optimizes software algorithms. Module of parameter of the maximum and minimum cycle Pitch error measurement is designed based on LabVIEW, and has been successfully used in the auto wheel speed sensor test system.","","978-1-4244-3863-1978-1-4244-3864","10.1109/ICEMI.2009.5274417","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5274417","Auto Wheel speed sensor;Testing Algorithm;Duty cycle;Pitch error;LabVIEW;Wave missing","Wheels;Electronic equipment testing;Sensor systems;System testing;Gears;Software algorithms;Control systems;Automotive electronics;Error analysis;Distortion","automotive electronics;distortion;electric sensing devices;velocity measurement;virtual instrumentation;wheels","auto wheel speed sensor testing;LabVIEW;target gear;electronic control system;automotive electronics;auto test sector;duty cycle error;Pitch error algorithm;signal distortion;minimum cycle Pitch error measurement","","","5","","","","","","IEEE","IEEE Conferences"
"Kato: A Program Slicing Tool for Declarative Specifications","E. Uzuncaova; S. Khurshid","The University of Texas at Austin, USA; The University of Texas at Austin, USA","29th International Conference on Software Engineering (ICSE'07)","","2007","","","767","770","This paper presents Kato, a tool that implements a novel class of optimizations that are inspired by program slicing for imperative languages but are applicable to analyzable declarative languages, such as Alloy. Kato implements a novel algorithm for slicing declarative models written in Alloy and leverages its relational engine KodKodfor analysis. Given an Alloy model, Kato identifies a slice representing the model's core: a satisfying instance for the core can systematically be extended into a satisfying instance for the entire model, while unsatisfiability of the core implies unsatisfiability of the entire model. The experimental results show that for a variety of subject models Kato's slicing algorithm enables an order of magnitude speed-up over Alloy's default translation to SAT.","0270-5257;1558-1225","0-7695-2828","10.1109/ICSE.2007.47","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4222643","","Engines;Algorithm design and analysis;Optimizing compilers;Cost accounting;Software algorithms;Software systems;Automatic testing;Software testing;System testing;Buildings","formal specification;program slicing;software tools","Kato tool;program slicing;declarative specifications;imperative languages;declarative languages;Alloy model;KodKodfor analysis;SAT","","4","14","","","","","","IEEE","IEEE Conferences"
"Embedded Software Optimization for AVS-P7 Decoder Real-time Implementation on RISC Core","B. Lei; W. Jin; J. Hu; X. Zhang","NA; NA; NA; NA","IEEE Transactions on Consumer Electronics","","2007","53","3","1204","1210","AVS-P7 is the recent mobile video coding standard of China. Currently, ARM cores are widely used in mobile applications because of their low power consumption. In this paper, a scheme of the AVS-P7decoder real-time implementation on 32 bit MCU RISC processor ARM920T (S3C2440) is presented. The algorithm, redundancy, structure and memory optimization methods to implement AVS-P7 real-time are discussed in detail. The experiment results demonstrate the success of our optimization techniques and the real-time implementation. The ADS, MCPS, PSNR and simulation results show that the proposed AVS-P7 decoder can decode the QVGA image sequence in real-time with high image quality and has low complexity and less memory requirement. AVS conformance test result confirms the proposed AVS-P7 decoder full compliance with AVS. The proposed AVS-P7 decoder can be applied in many real-time applications like Mobile phone and IPTV in the third generation communication.","0098-3063;1558-4127","","10.1109/TCE.2007.4341605","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4341605","","Embedded software;Decoding;Reduced instruction set computing;Video coding;Application software;Energy consumption;Optimization methods;PSNR;Image sequences;Image quality","decoding;image coding;image sequences;reduced instruction set computing;software engineering","embedded software optimization;AVS-P7 decoder;real-time implementation;RISC core;low power consumption;ARM920T;S3C2440;memory optimization methods;PSNR;QVGA image sequence;image quality","","5","5","","","","","","IEEE","IEEE Journals & Magazines"
"Mitrion-C Application Development on SGI Altix 350/RC100","V. V. Kindratenko; R. J. Brunner; A. D. Myers","University of Illinois at Urbana-Champaign, USA; University of Illinois at Urbana-Champaign, USA; University of Illinois at Urbana-Champaign, USA","15th Annual IEEE Symposium on Field-Programmable Custom Computing Machines (FCCM 2007)","","2007","","","239","250","This paper provides an evaluation of SGIreg RASC^TM RC100 technology from a computational science software developer's perspective. A brute force implementation of a two-point angular correlation function is used as a test case application. The computational kernel of this test case algorithm is ported to the Mitrion-C programming language and compiled, targeting the RC100 hardware. We explore several code optimization techniques and report performance results for different designs. We conclude the paper with an analysis of this system based on our observations while implementing the test case. Overall, the hardware platform and software development tools were found to be satisfactory for accelerating computationally intensive applications, however, several system improvements are desirable.","","0-7695-2940-2978-0-7695-2940","10.1109/FCCM.2007.17","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4297260","","Hardware;Field programmable gate arrays;Application software;Computer applications;Testing;Kernel;Computer languages;Acceleration;Software tools;Microprocessors","C language;operating system kernels;optimising compilers;program testing;software engineering","Mitrion-C application development;SGI Altix 350/RC100;computational science software developer perspective;two-point angular correlation function;test case application;computational kernel;test case algorithm;Mitrion-C programming language;program compiler;code optimization technique","","6","8","","","","","","IEEE","IEEE Conferences"
"Automatic Test Generation for Model-Based Code Generators","S. V. Zelenov; D. V. Silakov; A. K. Petrenko; M. Conrad; I. Fey","NA; NA; NA; NA; NA","Second International Symposium on Leveraging Applications of Formal Methods, Verification and Validation (isola 2006)","","2006","","","75","81","This paper presents a novel testing approach for model-based design tools, termed GraphOTK, and applies it to the optimizing component of a code generator for Simulink and Stateflow models.","","978-0-7695-3071","10.1109/ISoLA.2006.70","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4463697","","Automatic testing;Embedded system;Design optimization;System testing;Automotive engineering;Application software;Documentation;Computer languages;Aerospace industry;Region 8","program testing;software engineering;software tools","automatic test generation;model-based code generators;model-based design tools;GraphOTK;Simulink;Stateflow models","","5","14","","","","","","IEEE","IEEE Conferences"
"A Research on General Software Architecture on RFID","L. Ye","NA","2008 Third International Conference on Convergence and Hybrid Information Technology","","2008","1","","1054","1057","Base on the requirements of RFID on identifying object, the paper researches on general software architecture on the application system. We use middleware and layer design to lower the coupling; use ORM technology to separate business and storing; use AOP technology to focus on business; use IOC technology to improve testing, use cleaning method that optimizes the overall accuracy adjusted cleaning costs. It will improve the using and deploy of application on RFID technology. And it will develop to be specialized and scalable.","","978-0-7695-3407","10.1109/ICCIT.2008.117","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4682172","RFID;software architecture;middle ware;layer design;ORm","Software architecture;Radiofrequency identification;Management training;Logistics;Marine technology;Application software;Cleaning;Vehicle driving;Production;Wireless sensor networks","middleware;radiofrequency identification;software architecture;telecommunication computing","general software architecture;RFID;radio frequency identification;middleware;layer design;ORM technology;AOP technology;IOC technology","","1","6","","","","","","IEEE","IEEE Conferences"
"CAD software development of throttle valves parameters of telescopic shock absorber based on speed characteristic","C. Zhou; L. Zhao; Z. Zheng","School of Traffic and Vehicle Engineering, Shandong University of Technology, Zibo 255049, China; School of Traffic and Vehicle Engineering, Shandong University of Technology, Zibo 255049, China; College of Information & Engineering, Zhengzhou University, 450052, China","2009 11th IEEE International Conference on Computer-Aided Design and Computer Graphics","","2009","","","524","527","By the relation of oil throttle pressure to flow volume and the deformation of throttle slice, the mathematics model and target function of valves parameters optimization design was built. Based on, the CAD soft ware of valves parameters of telescopic shock absorber was developed. A practical design with the CAD soft ware was given, the performance test was conducted for the telescopic shock absorber designed by CAD, and the tested values fit the desired ones well. The experiment results show that the model of optimization design is accurate, and the CAD soft ware is reliable.","","978-1-4244-3699-6978-1-4244-3701","10.1109/CADCG.2009.5246849","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5246849","","Programming;Valves;Shock absorbers;Design automation;Design optimization;Optical design;Testing;Petroleum;Mathematics;Deformable models","CAD;deformation;design engineering;mechanical engineering computing;optimisation;shock absorbers;valves","CAD software development;throttle valves parameters;telescopic shock absorber;throttle slice deformation;valves parameters optimization design","","","9","","","","","","IEEE","IEEE Conferences"
"A 65-nm Random and Systematic Yield Ramp Infrastructure Utilizing a Specialized Addressable Array With Integrated Analysis Software","M. Karthikeyan; S. Fox; W. Cote; G. Yeric; M. Hall; J. Garcia; B. Mitchell; E. Wolf; S. Agarwal","NA; NA; NA; NA; NA; NA; NA; NA; NA","IEEE Transactions on Semiconductor Manufacturing","","2008","21","2","161","168","This paper describes a yield learning infrastructure that has been developed and deployed to help rapidly ramp 65-nm random and systematic yield. This infrastructure consists of a 4-Mb addressable-array test circuit with &gt;8000 unique test structures along with customized software and automated analysis routines to distill the large datasets generated. Examples of the successful application of this methodology are provided.","0894-6507;1558-2345","","10.1109/TSM.2008.2000277","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4512058","Process monitoring;semiconductor defects;semiconductor device manufacture;yield optimization","Circuit testing;Probes;Integrated circuit yield;Automatic testing;Software testing;Monitoring;Semiconductor device manufacture;Optical arrays;Application software","integrated circuit testing;integrated circuit yield","random yield ramp;systematic yield ramp;integrated analysis software;yield learning infrastructure;addressable-array test circuit;automated analysis;size 65 nm;storage capacity 4 Mbit","","3","13","","","","","","IEEE","IEEE Journals & Magazines"
"Knowledge Management and Optimization Modeling (for Decision Making)","B. Crawford; C. Castro; E. Monfroy","NA; NA; NA","2009 Fourth International Conference on Computer Sciences and Convergence Information Technology","","2009","","","157","160","This paper is motivated by the need to address the innovative development of algorithms to solve combinatorial problems, generally issued from decision making. Because modeling and resolution of this kind of problems is a software engineering subject, then to find better ways of developing algorithms, solvers and metaheuristics is our interest too. Here, we fixed some concepts from knowledge management, creativity and new software engineering trends applied in our work.","","978-1-4244-5244-6978-0-7695-3896","10.1109/ICCIT.2009.85","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5367955","Knowledge Management;Knowledge Creation;Creativity;Optimization modeling.","Knowledge management;Decision making;Software engineering;Programming;Software algorithms;Collaborative work;Humans;Collaborative software;Information technology;Testing","decision making;knowledge management;optimisation;software engineering","knowledge management;optimization modeling;decision making;software engineering","","","20","","","","","","IEEE","IEEE Conferences"
"An investigation of the relationships between lines of code and defects","H. Zhang","School of Software, Tsinghua University Beijing 100084, China","2009 IEEE International Conference on Software Maintenance","","2009","","","274","283","It is always desirable to understand the quality of a software system based on static code metrics. In this paper, we analyze the relationships between lines of code (LOC) and defects (including both pre-release and post-release defects). We confirm the ranking ability of LOC discovered by Fenton and Ohlsson. Furthermore, we find that the ranking ability of LOC can be formally described using Weibull functions. We can use defect density values calculated from a small percentage of largest modules to predict the number of total defects accurately. We also find that, given LOC we can predict the number of defective components reasonably well using typical classification techniques. We perform an extensive experiment using the public Eclipse dataset, and replicate the study using the NASA dataset. Our results confirm that simple static code attributes such as LOC can be useful predictors of software quality.","1063-6773","978-1-4244-4897-5978-1-4244-4828","10.1109/ICSM.2009.5306304","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306304","","Lab-on-a-chip;Software quality;Predictive models;Software systems;NASA;Packaging machines;System testing;Laboratories;Density measurement;Software metrics","program diagnostics;software metrics;software quality","lines of code;software system quality;static code metrics;pre-release defects;post-release defects;Eclipse dataset","","36","21","","","","","","IEEE","IEEE Conferences"
"Handling Design Criticism","R. J. Wirfs-Brock","Wirfs-Brock Associates","IEEE Software","","2007","24","3","12","14","Design reviews are an essential part of any design process. However, taking the criticism that comes from such reviews can be hard. The word criticism even has a slightly negative connotation in our culture. But design criticism is invaluable, and effectively giving and receiving it are skills that every software designer needs to master. It can be difficult to filter out constructive arguments from the noise or to discern the reasoning behind offhand remarks. Knowing what tactic to take when someone criticizes your design can keep your creative design flowing and help you improve your ideas. This article gives a summary of some kinds of design criticism you might receive and how you might react","0740-7459;1937-4194","","10.1109/MS.2007.76","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4163018","design criticism;illogical arguments;judgments;aesthetics;design complexity","Voting;Art;Software design;Shape;Humans;Optimization;Concrete;Software testing","object-oriented methods;software engineering","software design criticism handling","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Test Sequence Generation from UML Sequence Diagrams","P. Samuel; A. T. Joseph","NA; NA","2008 Ninth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing","","2008","","","879","887","In this paper, we present an approach to generate test sequences from UML 2.0 sequence diagrams. Sequence diagrams are one of the most widely used UML models in the software industry. Although sequence diagrams are used for modeling the dynamic aspects of the system, they can also be used for model based testing. Existing work does not encompass certain important features of UML 2.0 sequence diagrams. Our work considers many of the novel features of UML 2.0 sequence diagrams like alt, loop, opt and break to generate test sequences. These areimportant features as far as testing is concerned. Our work begins with defining the important types of relationship that can exist between the messages. Based on the relationship between the messages, the message sequences are generated. Our work considers an important feature of UML 2.0 sequence diagrams called the dasiaExecution Occurrencepsila to generate message sequences. Next, an intermediate representation of the sequence diagram is built. This intermediate representation is called the Sequence Dependency Graph (SDG). The message sequences are incorporated into the SDG. Finally, we discuss a traversal algorithm to generate test sequences from SDG. Our method is fully automated and the test sequences generated can be used to check the correctness of the implementation under test.","","978-0-7695-3263","10.1109/SNPD.2008.100","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4617481","UML;Sequence Diagram;Test Sequence;Message Sequence","Unified modeling language;Object oriented modeling;Optimized production technology;Analytical models;Visualization;Testing;Silicon compounds","graph theory;program testing;Unified Modeling Language","test sequence generation;UML sequence diagrams;UML 2.0;UML model;model based testing;message sequences;execution occurrence;sequence dependency graph","","9","19","","","","","","IEEE","IEEE Conferences"
"Tool path optimization to regulate the cutting forces in pocket machining","Z. Yingjie; Z. Liujie","School of Mechanical Engineering, Xi'an Jiaotong University, China; School of Mechanical Engineering, Xi'an Jiaotong University, China","2009 4th IEEE Conference on Industrial Electronics and Applications","","2009","","","2764","2769","In this paper, a new method for optimization of cutting paths is proposed. In which the cutting forces and chatters concerned in the machining process are taken into account. The main objective of this work is to get a smooth variation in cutting resistance for the area where there is a sudden rise or decrease of cutting forces to avoid chatter vibration. Unlike the previously methods, the conventional tool paths are modified, and even additional tool paths are inserted according to the given maximal engagement angle to avoid the rapid changes of cutting forces. In our scheme, C-Bezier curve is applied to generate the trajectories of the additional tool paths based on the limit of variation in material removal rates induced on the cutting tool. The algorithm has been implemented for pocket milling, and simulation test was performed to verify the significance of proposed method.","2156-2318;2158-2297","978-1-4244-2799-4978-1-4244-2800","10.1109/ICIEA.2009.5138713","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5138713","2D pocketing;C-Bezier;matetial removal rates;cutting resistance","Machining;Manufacturing;Milling;Optimization methods;Mechanical engineering;Software prototyping;Vibrations;Cutting tools;Testing;Performance evaluation","computerised numerical control;cutting;cutting tools;optimisation","tool path optimization;cutting forces;pocket machining;machining process;C-Bezier curve;pocket milling;CNC machine tools","","2","7","","","","","","IEEE","IEEE Conferences"
"Enhancing the Efficiency of Regression Testing through Intelligent Agents","T. M. S. U. Salima; A. Askarunisha; N. Ramaraj","NA; NA; NA","International Conference on Computational Intelligence and Multimedia Applications (ICCIMA 2007)","","2007","1","","103","108","Software testing is indispensable for all software development. As all mature engineering disciplines need to have systematic testing methodologies, software testing is a very important subject of software engineering. In software development practice, testing accounts for as much as 50% of total development efforts. Testing can be manual, automated, or a combination of both.Manual testing is the process of executing the application and manually interacting with the application, specifying inputs and observing outputs. Manually testing the software is inefficient and costly. It is imperative to reduce the cost and improve the effectiveness of software testing by automating the testing process, which contains many testing related activities using various techniques and methods. In order to automate the process, we have to have some ways to generate oracles from the specification, and generate test cases to test the target software against the oracles to decide their correctness. Today we still don't have a full-scale system that has achieved this goal. In general, significant amount of human intervention is still needed in testing. The degree of automation remains at the automated test script level. This paper therefore provides a timely summary and enhancement of agent theory in software testing ,which motivates recent efforts in adapting concepts and methodologies for agent oriented software testing to complex system which has not previously done. Agent technologies facilitate the automated software testing by virtue of their high level decomposition, independency and parallel activation[4]. Usage of agent based regression testing reduces the complexity involved in prioritizing the testcases.With the ability of agents to act autonomously, monitoring code changes and generating test cases for the changed version of the code can be done dynamically. Agent-Oriented Software testing (AOST) is a nascent but active field of research . A comprehensive methodology that plays an essential role in Software testing must be robust but easy-to use. Moreover, it should provide a roadmap to guide engineers in creating Agent-based systems. The agent based regression testing(ABRT) proposed here is to offer a definition for encompassing to cover the regression testing phenomena based on agents, yet sufficiently tight that it can rule out complex systems that are clearly not agent based.","","0-7695-3050","10.1109/ICCIMA.2007.294","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4426561","","Intelligent agent;Software testing;Automatic testing;System testing;Programming;Application software;Software engineering;Costs;Humans;Automation","","","","5","15","","","","","","IEEE","IEEE Conferences"
"Applying CMMI and Strategy to ATE Development","S. T. Stevens","V I Engineering, Inc., 27300 Haggerty Rd., # F10, Farmington Hills, MI 48331. (248)489-1200, sstevens@viengineering.com","2006 IEEE Autotestcon","","2006","","","813","818","This paper provides a viewpoint of the capability maturity model integration (CMMI<sup>SM</sup> ) from the perspective of automated test equipment (ATE) development and test engineering. ATE development is a specialized segment of product development and shares many of the same issues. Requirements for the test equipment are very dependent on continually evolving product characteristics. Even with the best planning, lead times for ATE development are typically eroded by late changes to product requirements and designs, and eventually the critical path leads right through test! Without a solid process foundation, chaos ensues. The CMMI process models provide a framework for the integration of best practices in many disciplines. Portions of the systems engineering, software engineering, Integrated Product and Process Development and Supplier Sourcing models all offer important perspectives which affect ATE developers. This paper focuses on the CMMI processes and best practices which yield the greatest impact to test organizations and groups that provide ATE. The overall Test Strategy should help prioritize the process areas which deserve the most attention. Mature ATE organizations use a Balanced Scorecard approach to provide alignment with corporate and program level goals. Metrics monitor their progress against their corporate goals. At the program level, they apply a risk- driven approach to selectively apply resources that achieve the highest ROI for test dollars. From this business-oriented vantage point, organizations are likely to see increased efficiencies that will decrease overall system development costs by streamlining the testing component of their budgets.","1088-7725;1558-4550","1-4244-0052-X1-4244-0051","10.1109/AUTEST.2006.283769","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4062482","","Capability maturity model;Test equipment;Best practices;Samarium;Automatic testing;Product development;Path planning;Product design;Solids;Chaos","automatic test equipment;investment;software engineering;systems engineering","automated test equipment;ATE development;capability maturity model integration;test engineering;product requirements;systems engineering;software engineering;integrated product;process development;supplier sourcing models;test strategy;balanced scorecard;return on investment;business-oriented vantage point","","3","9","","","","","","IEEE","IEEE Conferences"
"An Improved Particle Swarm Optimization Algorithm and Its Application for Solving Traveling Salesman Problem","J. Zhang; W. Xiong","NA; NA","2009 WRI World Congress on Computer Science and Information Engineering","","2009","4","","612","616","An improved particle swarm optimization (IPSO) algorithm was proposed. In the basic particle swarm optimization (PSO) algorithm, the tentative behavior of individuals and the mutation of velocity have been introduced, according to the law of evolutionary process. Using the single node adjustment algorithm, each particle searches the neighbor area by itself at every generation after general steps. In the evolution, the particles can escape from the local optimum with the mutation of velocity. This kind of enhanced study behavior accords with the biological natural law even more, and helps to find the global optimum solution with great chance. For solving traveling salesman problem, numerical simulation results for the benchmark TSP problems shows the effectiveness and efficiency of the proposed method.","","978-0-7695-3507","10.1109/CSIE.2009.649","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5171068","PSO;IPSO;TSP","Particle swarm optimization;Traveling salesman problems;Computer science;Genetic mutations;Application software;Evolution (biology);Numerical simulation;Energy management;Information management;Benchmark testing","evolutionary computation;iterative methods;particle swarm optimisation;search problems;travelling salesman problems","improved particle swarm optimization algorithm;traveling salesman problem;IPSO algorithm;tentative behavior;velocity mutation;evolutionary process;single node adjustment algorithm;search problem;local optimum solution;biological natural law;global optimum solution;numerical simulation;TSP problem;velocity iterative formula","","3","8","","","","","","IEEE","IEEE Conferences"
"Localizing Software Faults Simultaneously","R. Abreu; P. Zoeteweij; A. J. C. v. Gemund","NA; NA; NA","2009 Ninth International Conference on Quality Software","","2009","","","367","376","Current automatic diagnosis techniques are predominantly of a statistical nature and, despite typical defect densities, do not explicitly consider multiple faults, as also demonstrated by the popularity of the single-fault Siemens set. We present a logic reasoning approach, called Zoltar-M(ultiple fault), that yields multiple-fault diagnoses, ranked in order of their probability. Although application of Zoltar-M to programs with many faults requires further research into heuristics to reduce computational complexity, theory as well as experiments on synthetic program models and two multiple-fault program versions from the Siemens set show that for multiple-fault programs this approach can outperform statistical techniques, notably spectrum-based fault localization (SFL). As a side-effect of this research, we present a new SFL variant, called Zoltar-S(ingle fault), that is provably optimal for single-fault programs, outperforming all other variants known to date.","1550-6002;2332-662X","978-1-4244-5913-1978-1-4244-5912-4978-0-7695-3828","10.1109/QSIC.2009.55","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381406","Software fault diagnosis;program spectra;statistical and reasoning approaches","Fault diagnosis;Logic;Embedded software;Probability;Computational complexity;Software quality;Software debugging;Testing;Mathematics;Computer science","computational complexity;probability;program diagnostics;software fault tolerance","logic reasoning approach;Zoltar-multiple fault;probability;computational complexity;synthetic program models;multiple-fault program versions;Siemens;spectrum-based fault localization;Zoltar-single fault;single-fault programs;software fault diagnosis","","6","20","","","","","","IEEE","IEEE Conferences"
"Efficient Formalism-Independent Monitoring of Parametric Properties","F. Chen; P. O. Meredith; D. Jin; G. Rosu","NA; NA; NA; NA","2009 IEEE/ACM International Conference on Automated Software Engineering","","2009","","","383","394","Parametric properties provide an effective and natural means to describe object-oriented system behaviors, where the parameters are typed by classes and bound to object instances at runtime. Efficient monitoring of parametric properties, in spite of increasingly growing interest due to applications such as testing and security, imposes a highly non-trivial challenge on monitoring approaches due to the potentially huge number of parameter instances. Existing solutions usually compromise their expressiveness for performance or vice versa. In this paper, we propose a generic, in terms of specification formalism, yet efficient, solution to monitoring parametric specifications. Our approach is based on a general algorithm for slicing parametric traces and makes use of static knowledge about the desired property to optimize monitoring. The needed knowledge is not specific to the underlying formalism and can be easily computed when generating monitoring code from the property. Our approach works with any specification formalism, providing better and extensible expressiveness. Also, a thorough evaluation shows that our technique outperforms other state-of-art techniques optimized for particular logics or properties.","1938-4300","978-1-4244-5259-0978-0-7695-3891","10.1109/ASE.2009.50","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5431757","Software Engineering;Monitoring;Runtime Verification","Condition monitoring;Java;Logic testing;Software engineering;Computerized monitoring;Runtime;Security;Programming profession;Mechanical factors;Software testing","formal specification;object-oriented methods;optimisation;program diagnostics;program testing","parametric properties;object-oriented system behavior;formal specification;parametric trace slicing;optimization;parametric specification monitoring;formalism-independent monitoring","","12","17","","","","","","IEEE","IEEE Conferences"
"Out-of-bounds array access fault model and automatic testing method study","C. Gao; M. Duan; L. Tan; Y. Gong","Beijing Graphic Institute, Beijing 100029, China; Beijing Graphic Institute, Beijing 100029, China; Beijing Graphic Institute, Beijing 100029, China; Network and Exchange Technology Country Key Laboratory, Beijing University of Posts and Telecommunications, Beijing 100876, China","Tsinghua Science and Technology","","2007","12","S1","14","19","Out-of-bounds array access(OOB) is one of the fault models commonly employed in the objectoriented programming language. At present, the technology of code insertion and optimization is widely used in the world to detect and fix this kind of fault. Although this method can examine some of the faults in OOB programs, it cannot test programs thoroughly, neither to find the faults correctly. The way of code insertion makes the test procedures so inefficient that the test becomes costly and time-consuming. This paper, uses a kind of special static test technology to realize the fault detection in OOB programs. We first establish the fault models in OOB program, and then develop an automatic test tool to detect the faults. Some experiments have exercised and the results show that the method proposed in the paper is efficient and feasible in practical applications.","1007-0214","","10.1016/S1007-0214(07)70077-9","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6074018","software testing;integer interval set;static analysis;out of bounds array access;syntax tree","Arrays;Indexes;Analytical models;Software;Automatic testing;Libraries","","","","","","","","","","","TUP","TUP Journals & Magazines"
"SoftSig: Software-Exposed Hardware Signatures for Code Analysis and Optimization","J. Tuck; W. Ahn; J. Torrellas; L. Ceze","North Carolina State University; University of Illinois, Urbana-Champaign; University of Illinois, Urbana-Champaign; University of Washington","IEEE Micro","","2009","29","1","84","95","Many code analysis techniques for optimization, debugging, and parallelization must perform runtime disambiguation of address sets. Hardware signatures support such operations efficiently and with low complexity. SoftSig exposes hardware signatures to software through instructions that control which addresses to collect and which to disambiguate against. The Memoise algorithm demonstrates SoftSig's versatility by detecting and eliminating redundant function calls.","0272-1732;1937-4143","","10.1109/MM.2009.15","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4796172","memory disambiguation;multicore architectures;runtime optimization","Hardware;Runtime;Debugging;Software algorithms;Monitoring;Performance analysis;Frequency;Costs;Proposals","logic testing;optimising compilers;parallel programming;program debugging;program diagnostics","software-exposed hardware signature;code analysis;code optimization;runtime disambiguation;code debugging;SoftSig","","1","12","","","","","","IEEE","IEEE Journals & Magazines"
"Functional Verification of RTL Designs driven by Mutation Testing metrics","Y. Serrestou; V. Beroulle; C. Robach","LCIS-INPG, Valence, France; LCIS-INPG, Valence, France; LCIS-INPG, Valence, France","10th Euromicro Conference on Digital System Design Architectures, Methods and Tools (DSD 2007)","","2007","","","222","227","The level of confidence in a VHDL description directly depends on the quality of its verification. This quality can be evaluated by mutation-based test, but the improvement of this quality requires tremendous efforts. In this paper, we propose a new approach that both qualifies and improves the functional verification process. First, we qualify test cases thanks to the mutation testing metrics: faults are injected in the design under verification (DUV) (making DUV's mutants) to check the capacity of test cases to detect theses mutants. Then, a heuristic is used to automatically improve IPs validation data. Experimental results obtained on RTL descriptions from ITC'99 benchmark show how efficient is our approach.","","0-7695-2978-X978-0-7695-2978","10.1109/DSD.2007.4341472","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4341472","","Genetic mutations;Qualifications;Fault detection;Automatic testing;Data engineering;Design engineering;Benchmark testing;Software performance;Design optimization;Signal analysis","automatic test pattern generation;design for testability;electronic engineering computing;hardware description languages;logic design;logic testing","RTL designs;VHDL description;mutation testing metrics;functional verification process;design under verification;intellectual properties validation data;RTL descriptions;ITC'99 benchmark;automatic test bench generation","","11","12","","","","","","IEEE","IEEE Conferences"
"Link Structure Ranking Algorithm for Trading Networks","A. Mirzal","NA","2009 International Conference on Complex, Intelligent and Software Intensive Systems","","2009","","","120","127","Ranking algorithms based on link structure of the network are well-known methods in Web search engines to improve the quality of the searches. The most famous ones are PageRank and HITS. PageRank uses probability of a random surfer to visit a page as the score of that page, and HITS instead of produces one score, proposes using two scores, authority and hub scores. In this paper, we introduce a new link structure ranking algorithm for trading network based on the differences between trading network and WWW network in the links addition process, a process that known to be the foundation of PageRank and HITS formulation. In the last section, we describe the using of proposed algorithm as a tool for network clustering in addition to its original function as a ranking method.","","978-1-4244-3569-2978-0-7695-3575","10.1109/CISIS.2009.27","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5066777","HITS;network clustering;PageRank;ranking algorithms;trading networks","Sparse matrices;World Wide Web;Testing;Software algorithms;Clustering algorithms;Internet;Intelligent networks;Intelligent structures;Competitive intelligence;Software quality","electronic trading;Internet;pattern clustering;probability;search engines","online trading network;link structure ranking algorithm;Web search engine;pagerank;probability;WWW network clustering;HITS formulation","","1","16","","","","","","IEEE","IEEE Conferences"
"Mutation Analysis for Security Tests Qualification","T. Mouelhi; Y. Le Traon; B. Baudry","GET ENST-Bretagne; GET ENST-Bretagne; IRISA- 35042 Rennes","Testing: Academic and Industrial Conference Practice and Research Techniques - MUTATION (TAICPART-MUTATION 2007)","","2007","","","233","242","In this paper, we study how mutation analysis can be adapted to qualify test cases aiming at testing a security policy. The objective is to make test cases efficient to reveal erroneous implementations of a security policy. The notion of security policy testing is studied and mutation operators are defined in relation with the security rules. To make the approach applicable in practice we discuss and empirically rank the security mutation operators from the most to the least difficult to kill. The empirical study is a library software, which is implemented with a typical 3-tier architecture.","","0-7695-2984-4978-0-7695-2984","10.1109/TAIC.PART.2007.21","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4344128","","Genetic mutations;Qualifications;Books;Data security;Access control;System testing;Least squares approximation;Permission;Software libraries;Computer architecture","authorisation;library automation;program testing","security mutation operators;access control test case qualification;security policy testing;LMS system software;library management system","","12","11","","","","","","IEEE","IEEE Conferences"
"Particle Swarm Based Meta-Heuristics for Function Optimization and Engineering Applications","M. Pant; R. Thangaraj; A. Abraham","NA; NA; NA","2008 7th Computer Information Systems and Industrial Management Applications","","2008","","","84","90","This paper evaluates the performance of three Particle Swarm Optimization (PSO) algorithms, namely attraction-repulsion based PSO (ATREPSO), Quadratic Interpolation based PSO (QIPSO) and Gaussian Mutation based PSO (GMPSO). Whereas all the algorithms are guided by the diversity of the population to search the global optimal solution of a given optimization problem, GMPSO uses the concept of mutation and QIPSO uses the reproduction operator to generate a new member of the swarm. We tested the variants of PSO on ten standard benchmark functions and compared the results with classical PSO algorithm. Also, the performance of all algorithms is tested on two engineering design problems. The numerical results show that all the algorithms outperform the classical particle swarm optimization by a remarkable difference.","","978-0-7695-3184","10.1109/CISIM.2008.33","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4557839","particle swarm optimization;nature inspired heuristics;Attraction-Repulsion based PSO;Quadratic Interpolation based PSO (QIPSO);Gaussian Mutation based PSO (GMPSO)","Particle swarm optimization;Genetic mutations;Ant colony optimization;Application software;Testing;Stochastic processes;Management information systems;Computer industry;Quality management;Technology management","interpolation;particle swarm optimisation","function optimization;particle swarm optimization algorithms;meta-heuristics;attraction-repulsion;quadratic interpolation;Gaussian mutation;global optimal solution;standard benchmark functions","","5","20","","","","","","IEEE","IEEE Conferences"
"Exploiting Concurrency in System-on-Chip Verification","J. Xu; C. Lim","The University of Adelaide, Adelaide, Australia 5000. Email: justinxu@eleceng.adelaide.edu.au; The University of Adelaide, Adelaide, Australia 5000. Email: cclim@eleceng.adelaide.edu.au","APCCAS 2006 - 2006 IEEE Asia Pacific Conference on Circuits and Systems","","2006","","","836","839","System-on-chip (SoC) design paradigm makes design verification a more time-consuming task. Therefore, for simulation-based methods, test quality is extremely important. This paper presents a method that increases the test quality by exploiting the concurrency in a system. The main idea is to generalize the elements of concurrency as transfers and then transform the system into a transfer-resource-graph. The graph can be traversed to produce high-quality tests. To further optimize the test quality in terms of concurrency, we are able to generate event-driven test-programs. This is made possible by modelling transfers as active building blocks","","1-4244-0386-31-4244-0387","10.1109/APCCAS.2006.342151","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4145523","Test Generation;System-on-Chip;Verification","Concurrent computing;System-on-a-chip;System testing;Automatic testing;Australia;Computer bugs;Software testing;Very large scale integration;Formal verification;Design methodology","automatic test pattern generation;integrated circuit design;system-on-chip","system-on-chip verification;design verification;test quality;system concurrency;transfer-resource-graph;event-driven test-programs;active building blocks;test generation","","2","11","","","","","","IEEE","IEEE Conferences"
"Aestimo: a feedback-directed optimization evaluation tool","P. Berube; J. N. Amaral","Dept. of Comput. Sci., Alberta Univ., Edmonton, Alta., Canada; Dept. of Comput. Sci., Alberta Univ., Edmonton, Alta., Canada","2006 IEEE International Symposium on Performance Analysis of Systems and Software","","2006","","","251","260","Published studies that use feedback-directed optimization (FDO) techniques use either a single input for both training and performance evaluation, or a single input for training and a single input for evaluation. Thus an important question is if the FDO results published in the literature are sensitive to the training and testing input selection. Aestimo is a new evaluation tool that uses a workload of inputs to evaluate the sensitivity of specific code transformations to the choice of inputs in the training and testing phases. Aestimo uses optimization logs to isolate the effects of individual code transformations. It incorporates metrics to determine the effect of training input selection on individual compiler decisions. Besides describing the structure of Aestimo, this paper presents a case study that uses SPEC CINT2000 benchmark programs with the Open Research Compiler (ORC) to investigate the effect of training/testing input selection on in-lining and if-conversion. The experimental results indicate that: (1) training input selection affects the compiler decisions made for these code transformation; (2) the choice of training/testing inputs can have a significant impact on measured performance.","","1-4244-0186","10.1109/ISPASS.2006.1620809","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1620809","","Optimizing compilers;Training data;Testing;Radio access networks;Statistics;Tiles;Program processors;Large Hadron Collider;Management training","software performance evaluation;optimising compilers","Aestimo;feedback-directed optimization evaluation tool;performance evaluation;code transformation;optimization logs;open research compiler","","6","25","","","","","","IEEE","IEEE Conferences"
"Overview of Debug Standardization Activities","B. Vermeulen; R. Kühnis; J. Rearick; N. Stollon; G. Swoboda","NXP Semiconductors; Nokia; AMD; HDL Dynamics; Texas Instruments","IEEE Design & Test of Computers","","2008","25","3","258","267","The semiconductor industry is disaggregated, with a complex web of suppliers and consumers. Standards help to facilitate and simplify the debug process. This article provides an overview of current standardization activity. One area in need of such standardization is that of on-chip debug processes and instruments. The debug area particularly exhibits limited commonality between different IP providers in terms of interfaces and methods for complex SoCs. The problem becomes even greater with more SoC integrators using diverse IP from different vendors, requiring an increasing range of debug, analysis, and optimization capabilities. This article describes the goals and ongoing activities of five debug standardization bodies: the Nexus 5001, MIPI (Mobile Industry Processor Interface) Test and Debug, IEEE PI149.7, IEEE P1687, and OCP-IP (Open Core Protocol International Partnership) Debug working groups.","0740-7475;1558-1918","","10.1109/MDT.2008.81","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4534167","debug standardization;silicon debug;software debug;performance analysis;debug interfaces;debugger tools","Standardization;Instruments;System testing;Hardware;Embedded software;Software debugging;Electronics industry;Embedded system;System-on-a-chip;Network-on-a-chip","circuit optimisation;computer debugging;IEEE standards;logic testing;standardisation;system-on-chip","debug standardization;semiconductor industry;system-on-chip;SoC;circuit optimization;Nexus 5001;MIPI Test and Debug;Mobile Industry Processor Interface;IEEE PI149.7;IEEE P1687;OCP-IP;Open Core Protocol International Partnership","","12","7","","","","","","IEEE","IEEE Journals & Magazines"
"Work in progress - implementation of a freshman level engineering analysis software","A. Miller; J. Oliva","Grand Valley State University School of Engineering, USA; L3Com-CPS, USA","2007 37th Annual Frontiers In Education Conference - Global Engineering: Knowledge Without Borders, Opportunities Without Passports","","2007","","","F1J-1","F1J-4","As part of an introductory engineering class at Grand Valley State University, freshman students are required to design, build, and test a simple support bracket with the objective of optimizing its strength to weight ratio. However, freshman students do not yet have the capability to predict the mechanical behavior of their designs. Commercial analysis software is available, but teaching the students to use the software is far beyond the scope of a freshman engineering course. As a result, the students utilize a trial and error approach to design, leaving the students a lasting misimpression of the engineering design process. Simple analysis software was written and implemented to predict the mechanical behavior of the student-designed parts with the intent of instilling a more professional design practice at the freshman level. This allows the students to modify their designs accordingly to improve performance. The software was designed specifically for the freshman design project in order to simplify use and to limit its applicability to other design problems. The software has now been implemented in the classroom. After one semester of use, statistical tests do not indicate a significant difference in design strength, but freshman students are becoming actively engaged in a professional engineering design process.","0190-5848;2377-634X","978-1-4244-1083-5978-1-4244-1084","10.1109/FIE.2007.4417806","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4417806","Design analysis;Finite element analysis;Freshman design;Design process","Design engineering;Process design;Engineering profession;Finite element methods;Educational institutions;Software testing;Design optimization;Software design;Engineering education;Systems engineering and theory","CAD;computer aided instruction;educational institutions;engineering education;finite element analysis;mechanical engineering computing;supports","freshman level engineering analysis software;Grand Valley State University;support bracket;strength to weight ratio;mechanical behavior;engineering design process;finite element analysis","","","","","","","","","IEEE","IEEE Conferences"
"A Multipurpose Code Coverage Tool for Java","R. Lingampally; A. Gupta; P. Jalote","Indian Institute of Technology, India; Indian Institute of Technology, India; Indian Institute of Technology, India","2007 40th Annual Hawaii International Conference on System Sciences (HICSS'07)","","2007","","","261b","261b","Most test coverage analyzers help in evaluating the effectiveness of testing by providing data on statement and branch coverage achieved during testing. If made available, the coverage information can be very useful for many other related activities, like, regression testing, test case prioritization, test-suite augmentation, test-suite minimization, etc. In this paper, we present a Java-based tool JavaCodeCoverage for test coverage reporting. It supports testing and related activities by recording the test coverage for various code-elements and updating the coverage information when the code being tested is modified. The tool maintains the test coverage information for a set of test cases on individual as well as test suite basis and provides effective visualization for the same. Open source database support of the tool makes it very useful for software testing research","1530-1605","0-7695-2755","10.1109/HICSS.2007.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4076910","","Java;Software testing;System testing;Performance evaluation;Visualization;Visual databases;Binary codes;Costs;Fault detection;Information analysis","Java;program testing","multipurpose code coverage tool;Java;branch coverage;software testing;regression testing;test case prioritization;test-suite augmentation;test-suite minimization;test coverage reporting;open source database","","6","32","","","","","","IEEE","IEEE Conferences"
"Learning to rank with voted multiple hyperplanes for documents retrieval","He-li Sun; Bo-qin Feng; Jian-bin Huang","Department of Computer Science & Technology, Xi'an Jiaotong Univ., 710049, China; Department of Computer Science & Technology, Xi'an Jiaotong Univ., 710049, China; School of Software, Xidian University, Xi'an 710071, China","2008 3rd International Conference on Intelligent System and Knowledge Engineering","","2008","1","","572","577","The central problem for many applications in Information retrieval is ranking. Learning to rank has been considered as a promising approach for addressing the issue. In this paper, we focus on applying learning to rank to document retrieval, particularly the approach of using multiple hyperplanes to perform the task. Ranking SVM (RSVM) is a typical method of learning to rank. We point out that although RSVM is advantageous, it still has shortcomings. RSVM employs a single hyperplane in the feature space as the model for ranking, which is too simple to tackle complex ranking problems. In this paper, we look at an alternative approach to RSVM, which we call ¿multiple vote ranker¿ (MVR), and make comparisons between the two approaches. MVR employs several base rankers and uses the vote strategy for final ranking. We study the performance of the two methods with respect to several evaluation criteria, and the experimental results on the OHSUMED dataset show that MVR outperforms RSVM, both in terms of quality of results and in terms of efficiency.","","978-1-4244-2196-1978-1-4244-2197","10.1109/ISKE.2008.4730996","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4730996","Learning to rank;Document retrieval;Ranking SVM;Multiple vote ranker","Information retrieval;Support vector machines;Voting;Machine learning;Training data;Support vector machine classification;Testing;Intelligent systems;Knowledge engineering;Sun","information retrieval;support vector machines","voted multiple hyperplanes;documents retrieval;information retrieval;ranking SVM;multiple vote ranker","","","21","","","","","","IEEE","IEEE Conferences"
"Fault Diagnosis on Board for Analog to Digital Converters","L. Ciani; M. Catelani; G. Iuculano","Università di Firenze, D.E.T., Via S. Marta 3, 50139, Firenze, Italy. tel.+39 055 4796392, fax +39 055494569. lorenzo.ciani@unifi.it; Università di Firenze, D.E.T., Via S. Marta 3, 50139, Firenze, Italy. tel.+39 055 4796392, fax +39 055494569. marcantonio.catelani@unifi.it; Università di Firenze, D.E.T., Via S. Marta 3, 50139, Firenze, Italy. tel.+39 055 4796392, fax +39 055494569. gaetano.iuculano@unifi.it","2007 IEEE Instrumentation & Measurement Technology Conference IMTC 2007","","2007","","","1","4","This paper describes a general purpose high reliable data acquisition system which allows A/D converter testing by histogram and two tone tests for the fault diagnosis on the same board. A reliability analysis has been carried out in order to optimize the project, the components choice and redundancy configuration. The software has been written in Matlab and LabVIEW, with an easy graphical user interface.","1091-5281","1-4244-1080-01-4244-0588","10.1109/IMTC.2007.379034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4258166","ADC testing"";""Data Acquisition"";""Histogram"";""Two tone test"";""Diagnosis","Fault diagnosis;Analog-digital conversion;Histograms;System testing;Data acquisition;Costs;Automatic testing;Software testing;Instrumentation and measurement;Redundancy","analogue-digital conversion;circuit reliability;circuit testing;data acquisition;fault diagnosis","fault diagnosis;analog-digital converters;reliable data acquisition;histogram;reliability analysis;redundancy configuration;Matlab;LabVIEW;graphical user interface;A/D converter testing","","1","10","","","","","","IEEE","IEEE Conferences"
"Incorporating Historic Knowledge into a Communication Library for Self-Optimizing High Performance Computing Applications","S. Feki; E. Gabriel","NA; NA","2008 Second IEEE International Conference on Self-Adaptive and Self-Organizing Systems","","2008","","","265","274","Emerging computing systems have a wide variety of hardware and software components influencing the performance of parallel applications, presenting end-users with a (nearly) unique execution environment on each parallel machine. One of the big challenges of High Performance Computing is therefore to develop portable and efficient codes for any execution environment. The Abstract Data and Communication Library (ADCL) is a self-optimizing runtime communication library aiming at providing the highest possible performance for application level communication operations. The library provides for a given communication pattern a large number of implementations and incorporates a runtime selection logic. This selection aims at adaptively choosing the best performing implementation on the current platform and for the given problem. In this paper, we present a recent enhancement to the library which introduces the capability of utilizing information from previous executions in order to minimize the overhead of the runtime selection logic which mainly stems from testing under performing implementations. We introduce the notion of similar problems by using a proximity measure for a given operation. The approach is evaluated for the n-dimensional neighborhood communication for two different network interconnects and for a large range of different problems.","1949-3673;1949-3681","978-0-7695-3404","10.1109/SASO.2008.47","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4663430","self-optimizing communication libraries;historic learning;proximity measures","High performance computing;Runtime library;Software libraries;Application software;Logic testing;Computer applications;Concurrent computing;Hardware;Software performance;Parallel machines","abstract data types;parallel processing;software libraries","historic knowledge;self-optimizing high performance computing;emerging computing systems;hardware components;software components;parallel application;execution environment;parallel machine;abstract data library;self-optimizing runtime communication library","","","17","","","","","","IEEE","IEEE Conferences"
"Development and Use of Small Addressable Arrays for Process Window Monitoring in 65nm Manufacturing","M. Karthikeyan; A. Gasasira; S. Fox; G. Yeric; M. Hall; J. Garcia; B. Mitchell; E. Wolf","IBM Systems and Technology Group, Hopewell Junction, NY, USA, karthikeyan@gus.ibm.com; IBM Systems and Technology Group, Hopewell Junction, NY, USA; IBM Systems and Technology Group, Hopewell Junction, NY, USA; IBM Systems and Technology Group, Hopewell Junction, NY, USA; Synopsys, Austin, TX; IBM Systems and Technology Group, Hopewell Junction, NY, USA; Synopsys, Austin, TX; IBM Systems and Technology Group, Hopewell Junction, NY, USA; Dallas, TX; IBM Systems and Technology Group, Hopewell Junction, NY, USA; Dallas, TX; IBM Systems and Technology Group, Hopewell Junction, NY, USA; Marlborough, MA, USA","2007 IEEE International Conference on Microelectronic Test Structures","","2007","","","135","139","In this paper we report on the development and use of two scribe-line compatible addressable array test structures in 65 nm technology for routine process window monitoring. One array was dedicated for front-end of line test structures, while a second consists exclusively of back-end test structures. Fast testing allows large-scale sampling of wafer lots in a manufacturing environment. Customized software is used to automate data analysis and calculate figures of merit that enable process and equipment performance to be tracked by process module. Examples of successful application of these arrays in identifying and addressing systematic yield detractors are provided.","1071-9032;2158-1029","1-4244-0780-X1-4244-0781","10.1109/ICMTS.2007.374470","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4252420","Process monitoring;Semiconductor defects;Semiconductor device manufacture;Yield optimization","Monitoring;Manufacturing processes;Circuit testing;System testing;USA Councils;Probes;Semiconductor device manufacture;Phased arrays;Read only memory;Microelectronics","arrays;integrated circuit design;integrated circuit manufacture;integrated circuit technology;integrated circuit testing;integrated circuit yield;large scale integration;process monitoring","scribe-line compatible addressable array test structures;process window monitoring;front-end test structures;back-end test structures;large-scale sampling;customized software;automate data analysis;systematic yield detractors;semiconductor defects;semiconductor device manufacturing environment;yield optimization;figures of merit calculation;size 65 nm","","5","8","","","","","","IEEE","IEEE Conferences"
"Ranking Attack-Prone Components with a Predictive Model","M. Gegick; L. Williams","NA; NA","2008 19th International Symposium on Software Reliability Engineering (ISSRE)","","2008","","","315","316","Limited resources preclude software engineers from finding and fixing all vulnerabilities in a software system. An early security risk analysis that ranks software components by probability of being attacked can provide an affordable means to prioritizing fortification efforts to the highest risk components. We created a predictive model using classification and regression trees and the following internal metrics: quantity of Klocwork static analysis warnings, file coupling, and quantity of changed and added lines of code. We validated the model against pre-release security testing failures on a large commercial telecommunications system. The model assigned a probability of attack to each file where upon ranking the probabilities in descending order we found that 72% of the attack-prone files are in the top 10% of the ranked files and 90% in the top 20% of the files.","1071-9458;2332-6549","978-0-7695-3405","10.1109/ISSRE.2008.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700353","Attack-prone","Predictive models;Security;Performance analysis;Input variables;Buffer overflow;Software systems;Risk analysis;Classification tree analysis;Regression tree analysis;Failure analysis","probability;program diagnostics;regression analysis;security of data;trees (mathematics)","attack-prone component;predictive model;security risk analysis;classification trees;regression trees;Klocwork static analysis warnings;file coupling","","2","4","","","","","","IEEE","IEEE Conferences"
"Symbolic Robustness Analysis","R. Majumdar; I. Saha","NA; NA","2009 30th IEEE Real-Time Systems Symposium","","2009","","","355","363","A key feature of control systems is robustness, the property that small perturbations in the system inputs cause only small changes in its outputs. Robustness is key to designing systems that work under uncertain or imprecise environments. While continuous control design algorithms can explicitly incorporate robustness as a design goal, it is not clear if robustness is maintained at the software implementation level of the controller: two ``close'' inputs can execute very different code paths which may potentially produce vastly different outputs. We present an algorithm and a tool to characterize the robustness of a control software implementation. Our algorithm is based on symbolic execution and non-linear optimization, and computes the maximum difference in program outputs over all program paths when a program input is perturbed. As a by-product, our algorithm generates a set of test vectors which demonstrate the worst-case deviations in outputs for small deviations in inputs. We have implemented our approach on top of the Splat test generation tool and we describe an evaluation of our implementation on two examples of automotive control code.","1052-8725","978-0-7695-3875","10.1109/RTSS.2009.17","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5368183","Symbolic execution;robustness analysis;continuity;software testing","Robustness;Robust control;Software algorithms;Software maintenance;Testing;Control systems;Control design;Algorithm design and analysis;Software tools;Automotive engineering","robust control;software engineering","continuous control design algorithms;robustness;control software implementation;symbolic execution;nonlinear optimization;Splat test generation tool;automotive control code","","13","17","","","","","","IEEE","IEEE Conferences"
"Data Mining Static Code Attributes to Learn Defect Predictors","T. Menzies; J. Greenwald; A. Frank","Lane Department of Computer Science and Electrical Engineering, West Virginia University, Morgantown, WV; Department of Computer Science, Portland State University, Portland, OR; Department of Computer Science, Portland State University, Portland, OR","IEEE Transactions on Software Engineering","","2007","33","1","2","13","The value of using static code attributes to learn defect predictors has been widely debated. Prior work has explored issues like the merits of ""McCabes versus Halstead versus lines of code counts"" for generating defect predictors. We show here that such debates are irrelevant since how the attributes are used to build predictors is much more important than which particular attributes are used. Also, contrary to prior pessimism, we show that such defect predictors are demonstrably useful and, on the data studied here, yield predictors with a mean probability of detection of 71 percent and mean false alarms rates of 25 percent. These predictors would be useful for prioritizing a resource-bound exploration of code that has yet to be inspected","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.256941","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4027145","Data mining detect prediction;McCabe;Halstead;artifical intelligence;empirical;naive Bayes.","Data mining;Bayesian methods;Artificial intelligence;Software testing;System testing;Learning systems;Art;Software quality;Software systems;Financial management","data mining;learning (artificial intelligence);program diagnostics;program testing;software quality","data mining;static code attributes;defect predictor learning;McCabes versus Halstead;lines of code counts;resource-bound exploration","","484","49","","","","","","IEEE","IEEE Journals & Magazines"
"An Optimized Breaking Index for the Boussinesq-Type Numerical Model","S. Li; H. He; J. Song","Institute of Oceanology, Chinese Academy of Sciences, China; Graduate School, Chinese Academy of Sciences, China; Institute of Oceanology, Chinese Academy of Sciences, China; Graduate School, Chinese Academy of Sciences, China; Institute of Oceanology, Chinese Academy of Sciences, China","Eighth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing (SNPD 2007)","","2007","3","","573","577","A breaking index appeared in the Boussinesq-type numerical model (Lynett, 2006) is replaced and tested. The new breaking index, which includes the water depth effect and is not sensitive to the spatial grid length, was first proposed by the Smith and Kraus (1991) and further optimized by many experimental data in this study. The numerical tests show that the modified Lynett's model with this new breaking index is more stable and efficient in the nearshore breaking point computations. Comparison with Dingemans (1994) experiment shows the validity of the modified Lynett's model in the nearshore wave breaking simulation.","","0-7695-2909-7978-0-7695-2909","10.1109/SNPD.2007.98","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4287918","","Numerical models;Nonlinear equations;Computational modeling;Water conservation;Software engineering;Artificial intelligence;Distributed computing;Oceans;Software testing;Pollution","ocean waves","optimized breaking index;Boussinesq-type numerical model;water depth effect;modified Lynett's model;nearshore breaking point computations;nearshore wave breaking simulation","","","14","","","","","","IEEE","IEEE Conferences"
"An Optimization Approach to Restart Tree Based on Mean Failure Frequencies","Y. Haizhi; W. Huiqiang; L. Ying","NA; NA; NA","First International Workshop on Knowledge Discovery and Data Mining (WKDD 2008)","","2008","","","644","648","Restart tree is the groundwork for microreboot and recursive recovery, while optimization of restart tree is a key problem in increasing efficiency of recursive recovery. Based on the research of related works, the optimization principle of restart tree is analyzed and a novel approach is proposed for optimizing restart tree. In this approach, mean failure frequencies of components are obtained by injecting exceptions during load testing, which could be used to compute and analyze the failure correlation degrees between components. The components with high failure correlation degrees are constructed as a restart group to realize the optimization of restart tree. The results of the case study show that this approach reduces the mean time to recovery of componentized distributed application and implements the optimization of restart tree.","","0-7695-3090-7978-0-7695-3090","10.1109/WKDD.2008.44","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4470476","","Frequency;Optimization methods;Application software;Data mining;Educational institutions;Computer science;Data engineering;Knowledge engineering;Testing;Failure analysis","distributed processing;system recovery","restart tree;microreboot;recursive recovery;optimization principle;mean failure frequency;load testing;failure correlation;distributed application","","","7","","","","","","IEEE","IEEE Conferences"
"Hard- and Software Modularity of the NOVA MPSoC Platform","C. Sauer; M. Gries; S. Dirk","Infineon Technologies, Communications Solutions, Munich, Germany, Christian.Sauer@infineon.com; Infineon Technologies, Communications Solutions, Munich, Germany, Matthias.Gries@infineon.com; Infineon Technologies, Communications Solutions, Munich, Germany","2007 Design, Automation & Test in Europe Conference & Exhibition","","2007","","","1","6","The network-optimized versatile architecture platform (NOVA) encapsulates embedded cores, tightly and loosely coupled coprocessors, on-chip memories, and I/O interfaces by special sockets that provide a common packet passing and communication infrastructure. To ease the programming of the heterogeneous multiprocessor target for the application developer, a component based framework is used for describing packet processing applications in a natural and productive way. Leveraging identical application and hardware communication semantics, code generators and off-the-shelf tool chains can automate the software implementation process. Using a prototype with four processing cores, the overhead of modularity and programmability for the platform was quantified","1530-1591;1558-1101","978-3-9810801-2","10.1109/DATE.2007.364441","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4211951","","Sockets;Network-on-a-chip;Application software;Coprocessors;Hardware;Cost function;Embedded software;Communications technology;Computer architecture;Software tools","hardware-software codesign;logic CAD;network-on-chip","hardware modularity;software modularity;NOVA MPSoC platform;network-optimized versatile architecture platform;common packet passing;heterogeneous multiprocessor;packet processing applications;software implementation process automation","","1","12","","","","","","IEEE","IEEE Conferences"
"Testing BPEL-based Web Service Composition Using High-level Petri Nets","W. Dong; H. Yu; Y. Zhang","Tsinghua University Beijing, China; Tsinghua University Beijing, China; Tsinghua University Beijing, China","2006 10th IEEE International Enterprise Distributed Object Computing Conference (EDOC'06)","","2006","","","441","444","This paper proposes a technique for analysis and testing BPEL-based Web service composition using high-level Petri nets. To illustrate how these compositions are verified, the relationships between BPEL-based Web service composition and high-level Petri nets is constructed. By analyzing the structure of Web service composition based on BPEL, the corresponding HPN is constructed. The dynamism and occurrence are presented in HPN with guard expression with coloured token. After translation, the equivalent HPN of the Web service composition based on BPEL can be verified on existing mature tool, and the related researches on HPN, e.g. testing coverage and reduction techniques that have been studied deeply, can be employed in testing of Web service composition based on BPEL, optimized test case can be generated based on the HPN translated. An example is provided to illustrate the translation ruled and the automatic verify progress","1541-7719","0-7695-2558","10.1109/EDOC.2006.59","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4031236","","Web services;Petri nets;Concurrent computing;Software testing;Computer science;Domain specific languages;Application software;Specification languages;Information systems;Information analysis","Petri nets;program diagnostics;program testing;program verification;Web services","BPEL-based Web service composition;high-level Petri nets;Web service composition analysis;Web service composition testing","","27","11","","","","","","IEEE","IEEE Conferences"
"Scrum and CMMI Level 5: The Magic Potion for Code Warriors","J. Sutherland; C. R. Jakobsen; K. Johnson","Patientkeeper Inc.; Systematic Software Engineering; AgileDigm Inc.","Agile 2007 (AGILE 2007)","","2007","","","272","278","Projects combining agile methods with CMMI<sup>1</sup> are more successful in producing higher quality software that more effectively meets customer needs at a faster pace. Systematic Software Engineering works at CMMI level 5 and uses lean Software Development as a driver for optimizing software processes. Early pilot projects at Systematic showed productivity on Scrum teams almost twice that of traditional teams. Other projects demonstrated a story based test driven approach to software development reduced defects found during final test by 40%. We assert that Scrum and CMMI together bring a more powerful combination of adaptability and predictability than either one alone and suggest how other companies can combine them.","","0-7695-2872","10.1109/AGILE.2007.52","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4293608","","Programming;Software engineering;Software testing;Medical services;Maintenance;Capability maturity model;Software quality;Productivity;Appraisal;Performance analysis","Capability Maturity Model;software quality","Scrum;CMMI level 5;software quality;systematic software engineering;software development;software process optimization","","16","6","","","","","","IEEE","IEEE Conferences"
"Ant Colony Optimization and its Application to Boolean Satisfiability for Digital VLSI Circuits","R. Sethuram; M. Parashar","Electrical and Computer Engineering Dept., Rutgers University, Piscataway, NJ 08854, USA. rajamani@caip.rutgers.edu; Electrical and Computer Engineering Dept., Rutgers University, Piscataway, NJ 08854, USA. parashar@caip.rutgers.edu","2006 International Conference on Advanced Computing and Communications","","2006","","","507","512","Ant colony optimization (ACO) [8] is a non-deterministic algorithm framework that mimics the foraging behavior of ants to solve difficult optimization problems. Several researchers have successfully applied ACO framework in different fields of engineering, but never in VLSI testing. In this paper, we first describe the basics of the ACO framework and ways to formulate different optimization problems within an ACO framework. We then present our own ACO algorithm to simultaneously solve multiple boolean SAT instances for digital VLSI circuits. Experiments conducted on scanned versions of ISCAS'89 benchmark circuits produced astonishing results. ACO framework for boolean satisifiability was found 200 times faster than spectral meta-heuristics [36] run in combinational mode. ACO framework has proven to be a promising optimization technique in large number of other fields. Since ACO can be used to solve different types of optimization and search problems, we believe that the concepts presented in this paper can open the gates for researchers solving different optimization problems that exist in VLSI testing more efficiently.","","1-4244-0715-X1-4244-0716","10.1109/ADCOM.2006.4289945","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4289945","","Ant colony optimization;Very large scale integration;Sequential analysis;Circuit testing;Search problems;Application software;Benchmark testing;Routing;Software testing;Insects","Boolean functions;computability;logic design;optimisation;VLSI","ant colony optimization;Boolean satisfiability;digital VLSI circuit;non-deterministic algorithm;foraging behavior;ISCAS'89 benchmark circuit;spectral meta-heuristic","","1","","","","","","","IEEE","IEEE Conferences"
"Generating High-Quality Random Numbers by Cellular Automata with PSO","Q. Wang; S. Yu; W. Ding; M. Leng","NA; NA; NA; NA","2008 Fourth International Conference on Natural Computation","","2008","7","","430","433","The random numbers are needed in variety of applications, yet finding good random number is a hard task. In this paper, a novel particle swarm cellular automata (PSCA) is proposed. Further, PSCA are applied in pseudorandom number generation. The produced random numbers pass all tests of diehard and Federal Information Processing Standards (FIPS) 140-2. The outputs have good performance in statistical tests. Randomness test results on our Pseudo Random Number Generators (PRNG) show that they are better than one-dimensional (1D) CA PRNGs and can be comparable to two-dimensional (2D) ones.","2157-9555;2157-9563","978-0-7695-3304","10.1109/ICNC.2008.560","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4668014","PSO;Cellular Automata;PRNG","Random number generation;Testing;Application software;Computational modeling;Hardware;Circuit simulation;Particle swarm optimization;Information processing;Cryptography;Mathematical analysis","cellular automata;particle swarm optimisation;random number generation;statistical testing","high quality random numbers;particle swarm cellular automata;pseudorandom number generation;Federal Information Processing Standards;statistical tests;randomness test;pseudo random number generators","","3","8","","","","","","IEEE","IEEE Conferences"
"Using ES Based Automated Software Clustering Approach to Achieve Consistent Decompositions","B. Khan; S. Sohail","NA; NA","2008 15th Asia-Pacific Software Engineering Conference","","2008","","","429","436","Effective life time of any software can be increased many folds by proper and up to date maintenance. Automated software module clustering is a method used by software professionals to recover high-level structure of the system by decomposing the system into smaller manageable subsystems, containing interdependent modules. Once the structure of the system is clear, the understanding of any system for proper maintenance can be achieved. We have proposed an automated clustering approach based on the principles of Evolution Strategies to search a large solution space consisting of modules and their relationships. Our approach tries to achieve near optimal decompositions consisting of independent subsystems, containing interdependent modules. We have compared our proposed approach with a widely used Genetic Algorithm based clustering technique and our approach worked better in all test cases. In this paper, we are highlighting one distinguishing feature of our approach: the consistency in results. For any optimization algorithm, exactly similar results in different executions of the algorithm on same data cannot be achieved. However, the results should remain in close proximity and should not change drastically. We have carried out a comparative study of our approach and the Genetic Algorithm based approach using a set of test systems. The results with our approach are always consistent than those produced by the Genetic Algorithm based approach.","1530-1362;1530-1362","978-0-7695-3446","10.1109/APSEC.2008.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724575","","Clustering algorithms;Genetic algorithms;Software maintenance;Partitioning algorithms;Software systems;Software engineering;Educational institutions;System testing;Algorithm design and analysis;Documentation","evolutionary computation;pattern clustering;software maintenance","automated software clustering;maintenance;evolution strategies;optimal decompositions;genetic algorithm based clustering","","1","21","","","","","","IEEE","IEEE Conferences"
"A Discrete PSO for Partitioning in VLSI Circuit","S. Peng; G. Chen; W. Guo","NA; NA; NA","2009 International Conference on Computational Intelligence and Software Engineering","","2009","","","1","4","Circuit partitioning is a critical step in the physical design of Very Large Scale Integrated (VLSI) circuits. The interest in finding an optimal partitioning especially in VLSI design has been a hot issue in recent years. In VLSI circuit partitioning, the problem of obtaining a minimum cut is of prime importance. In this paper, a discrete Particle Swarm Optimization (DPSO) algorithm is proposed for the optimization of VLSI intercomnection (netlist) bipartition. Meanwhile, the corresponding evaluation function and the operators of crossover and mutation are designed. The algorithm is implemented to test ISCAS89 benchmark circuits. Compared with the traditional genetic algorithm (GA) with the same evaluation function and the same genetic operators concerned above, partitioning results by DPSO algorithm is markedly improved.","","978-1-4244-4507","10.1109/CISE.2009.5364339","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5364339","","Very large scale integration;Partitioning algorithms;Iterative algorithms;Clustering algorithms;Particle swarm optimization;Circuit testing;Genetic algorithms;Convergence;Mathematics;Integrated circuit interconnections","circuit optimisation;integrated circuit design;VLSI","discrete PSO;VLSI circuit partitioning;very large scale integrated circuits;optimal partitioning;discrete particle swarm optimization;VLSI intercomnection bipartition;test ISCAS89 benchmark circuits","","2","16","","","","","","IEEE","IEEE Conferences"
"Uncertainty management in software engineering: Past, present, and future","H. Ibrahim; B. H. Far; A. Eberlein; Y. Daradkeh","University of Calgary, 2500 university Drive NW, AB, T2N 1N4, Canada; University of Calgary, 2500 university Drive NW, AB, T2N 1N4, Canada; Department of Computer Science & Engineering, American University of Sharjah, UAE; University of Calgary, 2500 university Drive NW, AB, T2N 1N4, Canada","2009 Canadian Conference on Electrical and Computer Engineering","","2009","","","7","12","Software development has significantly matured in the last decade. However, one of the critical challenges today is uncertainty inherent to every aspect of software development including requirement specifications, design, coding, and testing. In this paper, we propose a framework for uncertainty management in software engineering. The framework is used to model uncertainty inherent to software development activities and manage their consequences. The framework consists of four main phases: identification and prioritization, modeling and analysis, management and planning, and monitoring and evaluation. Commercial off-the-shelf (COTS)-based development is selected as an example to illustrate how the proposed framework is used in a simple but intuitive case study to represent uncertainty and manage its consequences.","0840-7789","978-1-4244-3509-8978-1-4244-3508","10.1109/CCECE.2009.5090081","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5090081","Uncertainty management;Software Engineering;COTS-Based Development","Uncertainty;Engineering management;Software engineering;Programming;Software development management;Software design;Monitoring;Decision making;Bayesian methods;Software systems","software development management;uncertainty handling","software engineering;uncertainty management;software development;model uncertainty;commercial off-the-shelf based development","","2","20","","","","","","IEEE","IEEE Conferences"
"Software development methods and usability: Perspectives from a survey in the software industry in Norway","B. Bygstad; G. Ghinea; E. Brevik","NA; NA; NA","Interacting with Computers","","2008","20","3","375","385","This paper investigates the relationship between software development methodologies and usability. The point of departure is the assumption that two important disciplines in software development, one of software development methods (SDMs) and one of usability work, are not integrated in industrial software projects.Building on previous research we investigate two questions; (1) Will software companies generally acknowledge the importance of usability, but not prioritise it in industrial projects? and (2) To what degree are software development methods and usability perceived by practitioners as being integrated? To this end a survey in the Norwegian IT industry was conducted. From a sample of 259 companies we received responses from 78 companies.In response to our first research question, our findings show that although there is a positive bias towards usability, the importance of usability testing is perceived to be much less than that of usability requirements. Given the strong time and cost pressures associated with the software industry, we believe that these results highlight that there is a gap between intention and reality. Regarding our second research question our survey revealed that companies perceive usability and software development methods to be integrated. This is in contrast to earlier research, which, somewhat pessimistically, has argued for the existence of two different cultures, one of software development and one of usability. The findings give hope for the future, in particular because the general use of system development methods are pragmatic and adaptable.","0953-5438;1873-7951","","10.1016/j.intcom.2007.12.001","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8149869","Software development methods;Usability;Software industry;Survey","","","","","6","","","","","","","OUP","OUP Journals & Magazines"
"GPU-based parallel particle swarm optimization","Y. Zhou; Y. Tan","Key Laboratory of Machine Perception and Intelligence (Peking University), Ministry of Education, and with Department of Machine Intelligence, School of Electronics Engineering and Computer Science, Peking University, Beijing 100871, China; Key Laboratory of Machine Perception and Intelligence (Peking University), Ministry of Education, and with Department of Machine Intelligence, School of Electronics Engineering and Computer Science, Peking University, Beijing 100871, China","2009 IEEE Congress on Evolutionary Computation","","2009","","","1493","1500","A novel parallel approach to run standard particle swarm optimization (SPSO) on Graphic Processing Unit (GPU) is presented in this paper. By using the general-purpose computing ability of GPU and based on the software platform of Compute Unified Device Architecture (CUDA) from NVIDIA, SPSO can be executed in parallel on GPU. Experiments are conducted by running SPSO both on GPU and CPU, respectively, to optimize four benchmark test functions. The running time of the SPSO based on GPU (GPU-SPSO) is greatly shortened compared to that of the SPSO on CPU (CPU-SPSO). Running speed of GPU-SPSO can be more than 11 times as fast as that of CPU-SPSO, with the same performance, compared to CPU-SPSO, GPU-SPSO shows special speed advantages on large swarm population applications and high dimensional problems, which can be widely used in real optimizing problems.","1089-778X;1941-0026","978-1-4244-2958-5978-1-4244-2959","10.1109/CEC.2009.4983119","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4983119","","Particle swarm optimization;Central Processing Unit;Graphics;Concurrent computing;Computer architecture;Ant colony optimization;Application software;Machine intelligence;Programming profession;Benchmark testing","computer graphics;parallel processing;particle swarm optimisation","graphic processing unit;particle swarm optimization;compute unified device architecture;software platform;benchmark test functions","","64","11","","","","","","IEEE","IEEE Conferences"
"Parameters Optimization of VSC-HVDC Control System Based on Simplex Algorithm","C. Zhao; X. Lu; G. Li","Member, IEEE, Key Laboratory of Power System Protection and Dynamic Security Monitoring and Control under Ministry of Education, North China Electric Power University, Baoding, 071003, Hebei Provinece, China. e-mail: chengyongzhao@ncepu.edu.cn; Key Laboratory of Power System Protection and Dynamic Security Monitoring and Control under Ministry of Education, North China Electric Power University, Baoding, 071003, Hebei Provinece, China; Student Member, IEEE, Key Laboratory of Power System Protection and Dynamic Security Monitoring and Control under Ministry of Education, North China Electric Power University, Baoding, 071003, Hebei Provinece, China","2007 IEEE Power Engineering Society General Meeting","","2007","","","1","7","The performance of VSC-HVDC system depends on the parameters of control system, and usually PI controllers have been used to adjust system to fulfill desired objectives. However, the optimization methods for PI controllers' parameters of VSC-HVDC system are very few up to now. A control strategy based on direct analytic expression for VSC- HVDC is presented and the corresponding control system is designed. Simplex algorithm and system objective function are adopted to optimize the PI parameters for single- and multi- objective VSC-HVDC system on the basis of the control strategy. Simulation results in PSCAD/EMTDC software testify the performance of VSC-HVDC control system with optimized PI parameters and show that the controllers with the optimized PI parameters can effectively control VSC-HVDC system. Advantages of the control system with optimized parameters, such as precise control, quickly responding time and strong robustness have been testified by step response.","1932-5517","1-4244-1296-X1-4244-1298","10.1109/PES.2007.386085","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4275851","VSC-HVDC;Simplex;direct analytic expression;parameters optimization","Control systems;System testing;Control system synthesis;Optimization methods;HVDC transmission;PSCAD;EMTDC;Software performance;Software testing;Robust control","HVDC power convertors;HVDC power transmission;PI control;power engineering computing","parameters optimization;VSC-HVDC control system;PI controllers parameters;system objective function;PSCAD-EMTDC software;voltage source convertors;high voltage DC transmission system","","14","16","","","","","","IEEE","IEEE Conferences"
"Towards heterogeneous microsystems design-for-test in a graduate student environment","P. A. Stokes; R. E. Mallard","CMC Microsystems, Kingston, Ontario, Canada; CMC Microsystems, Kingston, Ontario, Canada","2009 IEEE International Conference on Microelectronic Systems Education","","2009","","","81","84","Advances in the microfabrication of heterogeneous microsystems is enabling increasingly complex devices. Modeling, simulation and test methodologies are unable to keep pace. Custom test solutions require significant resources to implement and are often not reusable. Devices not performing as expected are difficult to diagnose. Design-for-testability techniques familiar to silicon microelectronics designers may offer solutions for validating and debugging designs. What is desirable is a system design and operational algorithm optimization in a rapid prototyping environment that incorporates design for testability considerations. The university research setting is particularly well suited for developing such an environment. In this paper, we review some of the generic tests performed on microsystems-based sensor systems by graduate students. Taking a research infrastructure perspective, we then propose improvements to proof of concept environments in universities to facilitate addition of design-for-test features into heterogeneous microsystems.","","978-1-4244-4407-6978-1-4244-4406","10.1109/MSE.2009.5270823","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5270823","","Design for testability;Testing;Prototypes;Application software;Software prototyping;Microfluidics;Manufacturing;Design optimization;Fabrication;Design methodology","microfabrication;microsensors","heterogeneous microsystem design-for-test;graduate student environment;microfabrication;generic test;microsystem-based sensor system","","","8","","","","","","IEEE","IEEE Conferences"
"Automatic Low Power Optimizations during ADL-driven ASIP Design","A. Chattopadhyay; D. Kammler; E. M. Witte; O. Schliebusch; H. Ishebabi; B. Geukes; R. Leupers; G. Ascheid; H. Meyr","Aachen University of Technology, Integrated Signal Processing Systems, 52056 Aachen, Germany, anupam@iss.rwth-aachen.de; Aachen University of Technology, Integrated Signal Processing Systems, 52056 Aachen, Germany; Aachen University of Technology, Integrated Signal Processing Systems, 52056 Aachen, Germany; Aachen University of Technology, Integrated Signal Processing Systems, 52056 Aachen, Germany; Aachen University of Technology, Integrated Signal Processing Systems, 52056 Aachen, Germany; Aachen University of Technology, Integrated Signal Processing Systems, 52056 Aachen, Germany; Aachen University of Technology, Integrated Signal Processing Systems, 52056 Aachen, Germany; Aachen University of Technology, Integrated Signal Processing Systems, 52056 Aachen, Germany; Aachen University of Technology, Integrated Signal Processing Systems, 52056 Aachen, Germany","2006 International Symposium on VLSI Design, Automation and Test","","2006","","","1","4","Increasing complexity of cutting-edge applications for future embedded systems demand even higher processor performance with a strong consideration for battery-life. Low power optimization techniques are, therefore, widely applied towards the development of modern application specific instruction-set processors (ASIPs). Architecture description languages (ADLs) offer the ASIP designers a quick and optimal design convergence by automatically generating the software tool-suite as well as the register transfer level (RTL) description of the processor. The automatically generated processor description is then subjected to the traditional RTL-based synthesis flow. Power-specific optimizations, often found in RTL-based commercial tools, cannot take the full advantage of the architectural knowledge embedded in the ADL description, resulting in sub-optimal power efficiency. In this paper, we address this issue by describing an efficient and universal technique of automatic insertion of gated clocks during the ADL-based ASIP design flow. Experiments with ASIP benchmarks show the dramatic impact of our approach by reducing power consumption up to 41% percent compared to naive RTL synthesis from ADL description, without any incurred overhead for area and speed","","1-4244-0179-81-4244-0180","10.1109/VDAT.2006.258140","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4027512","","Design optimization;Application specific processors;Embedded system;Application software;Architecture description languages;Convergence;Software tools;Registers;Clocks;Energy consumption","application specific integrated circuits;circuit optimisation;clocks;instruction sets;integrated circuit design;logic design;low-power electronics;microprocessor chips","automatic low power optimizations;ADL-driven ASIP design;embedded systems;battery-life;application specific instruction-set processors;architecture description languages;software tool-suite;register transfer level;RTL-based synthesis flow;gated clocks insertion","","4","15","","","","","","IEEE","IEEE Conferences"
"Determining the Fidelity of Hardware-In-the-Loop Simulation Coupling Systems","C. Koehler; A. Mayer; A. Herkersdorf","Infineon Technologies AG Am Campeon 1-12 85579 Neubiberg, Germany, christian.koehler@infineon.com; Infineon Technologies AG Am Campeon 1-12 85579 Neubiberg, Germany, albrecht.mayer@infineon.com; Munich University of Technology Arcisstr. 21 80290 Munich, Germany, herkersdorf@tum.de","2008 IEEE International Behavioral Modeling and Simulation Workshop","","2008","","","13","18","Hardware-in-the-Loop (HIL) simulation is a widely used concept for design, rapid prototyping, test and optimization of complex systems. The paper attempts to present a formal approach of determining the fidelity of HIL simulation coupling systems. This approach can help to design and optimize such systems.","2160-3804;2160-3812","978-1-4244-2896","10.1109/BMAS.2008.4751232","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4751232","HIL;Simulation;Modelling;Simulation Tools","Computational modeling;Hardware;Data acquisition;Design optimization;Computer simulation;Application software;Virtual prototyping;System testing;Analytical models;Numerical models","program testing;software prototyping","hardware-in-the-loop simulation coupling systems;rapid prototyping;complex systems optimization","","7","8","","","","","","IEEE","IEEE Conferences"
"Optimization of AVS Audio Decoder Implemented on RISC Core","B. Y. Lei; K. Lo; H. Lei","NA; NA; NA","2008 International Conference on Computer and Electrical Engineering","","2008","","","32","35","This paper proposes the software optimization for implementing AVS-P3 realtime on ARM 9. The optimization for embedded systems has been recognized as a key issue in improving cost as well as performance. In this work, we describe our experience in optimizing AVS audio decoder based on ARM9. We mainly optimize the inversed IntMDCT, CBC and dequantization modules. The test result shows that the decoding speed improves 10 times than the original AVS audio decoder decoding stereo wave signal with the 128 kbps bitrate and 48 KHz sampling rate. Meanwhile, the memory space is reduced from 130.78 KB to 59.91 KB, achieved 54.2% improvement.","","978-0-7695-3504","10.1109/ICCEE.2008.168","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4740941","","Decoding;Reduced instruction set computing;Streaming media;Embedded system;Portable media players;Optimization methods;Bit rate;Sampling methods;Broadband communication;Data structures","audio coding;decoding;embedded systems;optimisation;reduced instruction set computing;video coding","AVS audio decoder optimization;RISC core;software optimization;dequantization module;stereo wave signal;signal sampling;embedded system","","","11","","","","","","IEEE","IEEE Conferences"
"Optimization Strategy for SSVEP-Based BCI in Spelling Program Application","I. Sugiarto; B. Allison; A. Gräser","NA; NA; NA","2009 International Conference on Computer Engineering and Technology","","2009","1","","223","226","This paper describes an optimization strategy for steady state visual evoked potential (SSVEP)-based brain-computer interface (BCI), especially in a spelling program application. In this application, there are at least three components for implementing a complete BCI application: stimulator, signal processing, and application (spelling program). Ideally, those three components should run on different processing units in order to obtain optimum performance. But integrating those three components in one computer system also gives advantages: make it easier for the subject to concentrate and simplifies the system configuration. There are two main parts that need to be optimized: the flickering animation and the spelling system. When optimizing the flickering animation, we will focus on the display driver technology and programming aspects. The optimization of spelling system will be focused on the layout and representation of the letter matrix. We tested our program on several computers for the following parameters: frequency range, frequency resolution, and frequency stability. It can be concluded that using a computer monitor as the stimulator, the maximum synthesizable stimulator frequency is always half of its minimum refresh-rate, no matter which software technology is applied (DirectX or OpenGL). The maximum synthesizable frequency of up to 30 Hz with frequency resolution 0.11 Hz is achieved. We have tested our system on 106 subjects during CeBIT 2008 in Hannover, Germany. Mean accuracy for the spelling system is 92.5%. Therefore, the optimization strategy described here led to a stable and reliable system that performed effectively across most subjects without requiring extensive expert help or expensive hardware.","","978-0-7695-3521-0978-1-4244-3334","10.1109/ICCET.2009.189","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4769460","Brain-Computer Interface;EEG;SSVEP;spelling program","Application software;Frequency synthesizers;Animation;Computer displays;Steady-state;Brain computer interfaces;Signal processing;Stability;System testing;Hardware","brain-computer interfaces;computer animation;optimisation;spelling aids;visual evoked potentials","optimization;spelling program;steady state visual evoked potential;SSVEP;brain-computer interface;BCI;flickering animation;letter matrix;DirectX;OpenGL","","10","9","","","","","","IEEE","IEEE Conferences"
"Towards Resource Consumption-Aware Programming","A. Holzer; V. Januzaj; S. Kugele","NA; NA; NA","2009 Fourth International Conference on Software Engineering Advances","","2009","","","490","493","In order to check the fulfilment of non-functional requirements at an early system design and development stage, we provide a framework that facilitates the combination of platform-independent and platform-specific information in a query-based manner to calculate estimates for the resource consumption of the software under investigation at fine grained levels of code. Based on an already optimised intermediate representation of the source code, using a testing infrastructure for C code, we count the occurrence of instructions during program executions in a platform-independent manner. These instruction counters can be determined at program or function level. By combining these counters with cost information of a hardware platform we can provide resource consumption estimates. This allows the software developer to tailor the code steadily towards the non-functional characteristics of the software.","","978-1-4244-4779-4978-0-7695-3777","10.1109/ICSEA.2009.77","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5298783","Code Instrumentation;Embedded Systems;Execution Time","Costs;Hardware;Systems engineering and theory;Design engineering;Counting circuits;Embedded system;Automotive engineering;Software systems;Software engineering;Testing","query processing;software engineering;systems analysis","resource consumption aware programming;system design;platform-independent information;platform-specific information;system development;query-based manner;software resource consumption;program execution;platform-independent manner;instruction counters;nonfunctional requirements","","1","9","","","","","","IEEE","IEEE Conferences"
"The Optimization Research on Large-Diameter Longhole Blasting Parameters of Underground Mine Based on Artificial Neural Network","P. Dong; Z. Keping; L. Na; D. Hongwei; L. Kui; J. Fuliang","NA; NA; NA; NA; NA; NA","2009 Second International Conference on Intelligent Computation Technology and Automation","","2009","1","","419","422","This paper combines with Kafang's engineering practice of Xinshan mining area, makes crater tests, and then determines the blasting parameters under experimental conditions. Train the key stakeholders blasting parameters both at home and abroad based on the BP artificial neural network (ANN) model. On the basis that the best charge depth is 1.09 m which under the experimental conditions of blasting crater test. Conduct optimizing calculation of blasting parameters by using EasyNN-plus software. Through a comprehensive analysis of optimization ways and parameter error, recommend blasting parameters under experimental conditions: charge depth L=1.09 m, the best crater radius R<sub>j</sub>=0.77-0.79 m, the best crater volume V<sub>j</sub>=0.5-0.6 m<sup>3</sup>, and explosive consumption 1.0-1.1 kg/t.","","978-0-7695-3804","10.1109/ICICTA.2009.109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5287623","blasting parameters;optimization calculation;blasting test;artificial neural network(ANN);EasyNN-plus","Artificial neural networks;Testing;Explosives;Drilling;Ores;Tin;Mechanical factors;Artificial intelligence;Intelligent networks;Computer networks","explosions;mechanical engineering computing;mining;neural nets","large-diameter longhole blasting parameters;underground mine;Xinshan mining area;BP artificial neural network;blasting crater test","","1","13","","","","","","IEEE","IEEE Conferences"
"Towards Comprehensive Release Planning for Software Product Lines","M. I. Ullah; G. Ruhe","University of Calgary, Canada; University of Calgary, Canada","2006 International Workshop on Software Product Management (IWSPM'06 - RE'06 Workshop)","","2006","","","51","56","Release Planning (RP) plays an important role for the success of incremental product development. Proper planning includes consideration of stakeholder preferences, resources and their capacities, as well as product and business objectives. The complexity of this process is getting even larger when looking for releases of software product lines (SPL). SPL is considered as a viable and important software development paradigm allowing companies under certain conditions to realize order-of-magnitude improvements in time to market, cost, productivity, quality, and other business drivers. In this paper, we present ongoing research in the process to develop a comprehensive and formalized model for planning and optimizing releases for SPL. We have identified key issues that are unique to RP of SPL. Some of them are highlighted by an example modified from literature for illustrative purposes.","","0-7695-2714","10.1109/IWSPM.2006.9","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4022017","","Programming;Process planning;System testing;Computational intelligence;Competitive intelligence;Software testing;Laboratories;Software engineering;Product development;Capacity planning","","","","6","11","","","","","","IEEE","IEEE Conferences"
"A study on shape optimization of bus body structure based on stiffness sensitivity analysis","Zhang Xingwang; Tao Zhen","Nanchang Institute of Technology, Jiangxi Province 330099, China; Nanchang Institute of Technology, Jiangxi Province 330099, China","2009 IEEE 10th International Conference on Computer-Aided Industrial Design & Conceptual Design","","2009","","","1225","1229","The research of autobody lightweight technology is an important mainstream in the autobody design. In this paper A FEM model for the unitary body of a home-made bus is established .Then optimization design variables are determined through stiffness sensitivity analysis and by using finite element analysis software, an optimization is conducted, based on which a lightweighting scheme is put forward. The experiment indicated that after appropriate optimization the mass of the whole car body decrease 19.35 Kg and the stiffness of the autobody maintained invariable.","","978-1-4244-5266-8978-1-4244-5268","10.1109/CAIDCD.2009.5375216","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5375216","Bus Body;Finite Element Method;Stiffness;Sensitivity Analysis;Lightweighting","Shape;Sensitivity analysis;Design optimization;Optimization methods;Topology;Finite element methods;Arithmetic;Thin wall structures;Testing;Quadratic programming","automotive components;design engineering;elastic constants;finite element analysis;lightweight structures;optimisation;sensitivity analysis;shapes (structures)","autobody lightweight technology;bus body structure;shape optimization;stiffness sensitivity analysis;autobody design;FEM;finite element method;car body","","","22","","","","","","IEEE","IEEE Conferences"
"Efficient and effective random testing using the Voronoi diagram","T. Y. Chen; R. Merkel","Fac. of Inf. & Commun. Technol., Swinburne Univ. of Technol., Hawthorn, Vic., Australia; Fac. of Inf. & Commun. Technol., Swinburne Univ. of Technol., Hawthorn, Vic., Australia","Australian Software Engineering Conference (ASWEC'06)","","2006","","","6 pp.","299","Adaptive random testing (ART) is a method for improving the fault-finding effectiveness of random testing. Fixed-size candidate set ART is the most studied variant of this approach. However, existing implementations of FSCS-ART have had substantial selection overhead, with n test cases requiring O(n/sup 2/) time to generate. We describe the use of a geometric data structure known as the Voronoi diagram to reduce this overhead to no worse than O(n/spl radic/n) and, with further optimization, O(nlogn). We demonstrate experimentally that practical improvements in selection overhead can be gained using this improved implementation.","1530-0803;2377-5408","0-7695-2551","10.1109/ASWEC.2006.25","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1615063","","Software testing;Power capacitors;Subspace constraints;Australia;Computer errors;Data structures;Random sequences;Missiles;Error correction;Software engineering","program testing;computational complexity;computational geometry;data structures","adaptive random testing;fixed-size candidate set ART;Voronoi diagram;test cases;geometric data structure","","3","22","","","","","","IEEE","IEEE Conferences"
"A Comparison of Re-ranking Methods in Digital Libraries Using User Profiles","T. Van; M. Beigbeder","NA; NA","2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology","","2008","1","","751","754","In this paper we present a comparative study of re-ranking methods using user profiles in digital libraries which contain scientific articles. While most of present personalized search systems only use content-based methods to re-rank search results, in our work we use many different content-based and citation-based methods for this purpose. We also study many functions to combine scores computed by these methods. We conducted experiments on the test collection used in the INEX 2005 campaign to evaluate our strategies and received interesting results.","","978-0-7695-3496","10.1109/WIIAT.2008.206","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4740542","Digital Libraries;User Profiles;Personalized Search;Re-ranking","Software libraries;Search engines;Information retrieval;History;Intelligent agent;Asynchronous transfer mode;Testing;Monitoring;Content based retrieval;Computer science","content-based retrieval;digital libraries","reranking methods;digital libraries;user profiles;content-based methods;citation-based methods;INEX 2005 campaign","","","11","","","","","","IEEE","IEEE Conferences"
"Software Annotations for Power Optimization on Mobile Devices","R. Cornea; A. Nicolau; N. Dutt","Donald Bren School of Information and Computer Science University of California, Irvine, CA 92697-3425, radu@ics.uci.edu; NA; NA","Proceedings of the Design Automation & Test in Europe Conference","","2006","1","","1","6","Modern applications for mobile devices, such as multimedia video/audio, often exhibit a common behavior: they process streams of incoming data in a regular, predictable way. The runtime behavior of these applications can be accurately estimated most of the time by analyzing the data to be processed and annotating the stream with the information collected. We introduce a software annotation based approach to power optimization and demonstrate its application on a backlight adjustment technique for LCD displays during multimedia playback, for improved battery life and user experience. Results from analysis and simulation show that up to 65% of backlight power can be saved through our technique, with minimal or no visible quality degradation","1530-1591;1558-1101","3-9810801-1","10.1109/DATE.2006.244043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1656974","","Streaming media;Liquid crystal displays;Application software;Batteries;Personal digital assistants;Mobile computing;Transcoding;Cornea;Computer science;Runtime","liquid crystal displays;mobile radio;multimedia communication;portable computers;power consumption;software engineering","software annotations;power optimization;mobile devices;backlight adjustment technique;LCD displays;multimedia playback;battery life","","4","15","","","","","","IEEE","IEEE Conferences"
"On-Line Testing Technique for Artillery Body","B. Bai; Y. Hao","NA; NA","2009 International Conference on Computational Intelligence and Software Engineering","","2009","","","1","5","In the system the multi-parameters intelligent testing platform is the core. It had realized the intelligent process for measuring out data for the first time. The optimization result can get the conclusions of abrasion grade, mechanic damages, the disfigurements of material and the ablation degree. The on-line intelligent detecting and measuring platform system can widely be used in factories in which artilleries are produced or measured and in other fields relating to multi-parameter intelligent testing functions in deep holes. The system adopts the data fusion technology of multi-sensor to collect the data, and a series of omni-bearing detectors are used as the special purpose input device. The detectors are composed of a photoelectric sensor, a laser collimator, the testing-diameter equipment, the testing-thickness equipment, the optical system and the transmission device. They have fast rate of photoelectric conversion, low noise, high resolution power and fine signal evenness and are mainly used for gathering the various information.","","978-1-4244-4507","10.1109/CISE.2009.5363470","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5363470","","System testing;Intelligent sensors;Optical sensors;Optical noise;Intelligent systems;Time measurement;Production facilities;Detectors;Sensor systems;Laser fusion","military computing;photoelectric devices;sensor fusion;weapons","online testing technique;artillery body;multiparameters intelligent testing platform;abrasion grade;mechanic damages;intelligent detection;data fusion technology;multisensor technology;data collection;omnibearing detectors;photoelectric sensor;laser collimator;testing-diameter equipment;testing-thickness equipment;photoelectric conversion","","","4","","","","","","IEEE","IEEE Conferences"
"Automated Model Design Using Genetic Algorithms and Model Checking","R. Lefticaru; F. Ipate; C. Tudose","NA; NA; NA","2009 Fourth Balkan Conference in Informatics","","2009","","","79","84","In recent years there has been a growing interest in applying metaheuristic search algorithms in model-checking. On the other hand, model checking has been used far less in other software engineering activities, such as model design and software testing. In this paper we propose an automated model design strategy, by integrating genetic algorithms (used for model generation) with model checking (used to evaluate the fitness, which takes into account the satisfied/unsatisfied specifications). Genetic programming is the process of evolving computer programs, by using a fitness value determined by the program's ability to perform a given computational task. This evaluation is based on the output produced by the program for a set of training input samples. The consequence is that the evolved program can function well for the sample set used for training, but there is no guarantee that the program will behave properly for every possible input. Instead of training samples, in this paper we use a model checker, which verifies if the generated model satisfies the specifications. This approach is empirically evaluated for the generation of finite state-based models. Furthermore, the previous fitness function proposed in the literature, that takes into account only the number of unsatisfied specifications, presents plateaux and so does not offer a good guidance for the search. This paper proposes and evaluates the performance of a number of new fitness functions, which, by taking also into account the counterexamples provided by the model checker, improve the success rate of the genetic algorithm.","","978-0-7695-3783","10.1109/BCI.2009.15","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5359333","model design;genetic algorithms;model checking;fitness function","Algorithm design and analysis;Genetic algorithms;Software engineering;Software testing;Genetic programming;Mathematical model;Software design;Software algorithms;Particle swarm optimization;Ant colony optimization","complete computer programs;formal specification;formal verification;genetic algorithms;search problems;software performance evaluation","automated model design;genetic algorithms;model checking;metaheuristic search algorithms;software engineering activities;software testing;genetic programming;computer program evolution;finite state based models","","1","26","","","","","","IEEE","IEEE Conferences"
"Implementation Strategy of Several Key Issues in Intelligent Testing Paper Generation System","J. Luo; R. Zhang; Y. Jin","NA; NA; NA","2009 International Conference on Computational Intelligence and Software Engineering","","2009","","","1","4","Intelligent testing paper generation system is a matter of multi-objective optimization, and high-quality testing papers can be made by setting more detailed properties of examination questions. This paper designs some implementation strategies to satisfy users' testing requests, such as establishing knowledge points testing area and the proportions of each knowledge point as well as their requests of the percentage of each chapter, setting up max-min scores of examination question to adjust scores dynamically, and designing the exposure and the last using time records of questions to describe the utilization frequency of examination questions, etc.","","978-1-4244-4507","10.1109/CISE.2009.5363420","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5363420","","System testing;Sections;Databases;Knowledge management;Computer science;Content management;Mathematics;Frequency;Computer science education;Uncertainty","computer aided instruction;minimax techniques","intelligent testing paper generation system;multiobjective optimization;knowledge point;max-min score","","2","4","","","","","","IEEE","IEEE Conferences"
"Simulation for Constrainted Optimization of Inventory System by Using Arena and OptQuest","J. Wan; L. Li","NA; NA","2008 International Conference on Computer Science and Software Engineering","","2008","2","","202","205","We consider the simulation of constrained optimization problem, the (s, S) inventory system with stochastic lead-time and a service level constraint. We allow the orders to cross in time, which makes the problem more complicated. Bashyam and Fu (1998) first present this problem and obtained the answer by using perturbation analysis. Angun, Gurken, Hertog and Kleijnen (2006) studied the same question by using response surface method. The motivation for our work comes from the difference answers between them for the same model under the same situations. We establish the (s, S) inventory model by using Arean and find the estimators by OptQuest. In our conclusion, we give the true optimal estimator of (s*, S*) pairs estimated by Brute Force. Further, we prove that OptQuest can be used in solving the stochastic constrained optimization problem effectively. By testing estimatorspsila KKT conditions under large sample size procedure, we identify the best estimator.","","978-0-7695-3336","10.1109/CSSE.2008.1217","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722034","simulation;inventory system;arena;optimization","Constraint optimization;Computational modeling;Computer simulation;Technology management;Stochastic systems;Response surface methodology;Stochastic processes;Testing;Statistical analysis;Computer science","inventory management;optimisation;stochastic processes","stochastic constrainted optimization problem;inventory system;Arena;OptQuest;stochastic lead-time;service level constraint;Brute Force;stochastic constrained optimization","","1","21","","","","","","IEEE","IEEE Conferences"
"SAT-decoding in evolutionary algorithms for discrete constrained optimization problems","M. Lukasiewycz; M. Glass; C. Haubelt; J. Teich","Department of Computer Science 12, Hardware-Software-Co-Design, University of Erlangen-Nuremberg, Germany; Department of Computer Science 12, Hardware-Software-Co-Design, University of Erlangen-Nuremberg, Germany; Department of Computer Science 12, Hardware-Software-Co-Design, University of Erlangen-Nuremberg, Germany; Department of Computer Science 12, Hardware-Software-Co-Design, University of Erlangen-Nuremberg, Germany","2007 IEEE Congress on Evolutionary Computation","","2007","","","935","942","For complex optimization problems, several population-based heuristics like Multi-Objective Evolutionary Algorithms have been developed. These algorithms are aiming to deliver sufficiently good solutions in an acceptable time. However, for discrete problems that are restricted by several constraints it is mostly a hard problem to even find a single feasible solution. In these cases, the optimization heuristics typically perform poorly as they mainly focus on searching feasible solutions rather than optimizing the objectives. In this paper, we propose a novel methodology to obtain feasible solutions from constrained discrete problems in population- based optimization heuristics. At this juncture, the constraints have to be converted into the Prepositional Satisfiability Problem (SAT). Obtaining a feasible solution is done by the DPLL algorithm which is the core of most modern SAT solvers. It is shown in detail how this methodology is implemented in Multi-objective Evolutionary Algorithms. The SAT solver is used to obtain feasible solutions from the genetic encoded information on arbitrarily hard solvable problems where common methods like penalty functions or repair strategies are failing. Handmade test cases are used to compare various configurations of the SAT solver. On an industrial example, the proposed methodology is compared to common strategies which are used to obtain feasible solutions.","1089-778X;1941-0026","978-1-4244-1339-3978-1-4244-1340","10.1109/CEC.2007.4424570","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4424570","","Evolutionary computation;Constraint optimization;NP-complete problem;Iterative algorithms;Computer science;Glass;Genetics;Testing;Biological information theory;Evolution (biology)","evolutionary computation;genetic algorithms","SAT-decoding;evolutionary algorithms;discrete constrained optimization problems;multiobjective evolutionary algorithms;constrained discrete problems;population- based optimization heuristics;prepositional satisfiability problem;genetic encoded information;penalty functions;repair strategies","","12","26","","","","","","IEEE","IEEE Conferences"
"Lagrange multiplier selection for rate-distortion optimization in SVC","X. Li; P. Amon; A. Hutter; A. Kaup","Chair of Multimedia Communications and Signal Processing, University of Erlangen-Nuremberg, Germany; Siemens Corporate Technology, Information & Communications, Munich, Germany; Siemens Corporate Technology, Information & Communications, Munich, Germany; Chair of Multimedia Communications and Signal Processing, University of Erlangen-Nuremberg, Germany","2009 Picture Coding Symposium","","2009","","","1","4","The Lagrangian multiplier based rate-distortion optimization (RDO) has been widely employed in single layer video coding. During the development of scalable video coding (SVC) extension of H.264/AVC, it was directly applied in a multilayer scenario. However, such an application is not very efficient since the correlation between layers is not considered in the Lagrange multiplier selection. To improve the overall performance, in this paper a new selection algorithm is presented for RDO in SVC. Simulations show that the proposed method outperforms the recent SVC reference software. With a tiny computational cost, average gains of 0.22 dB and 0.35 dB were achieved in the tests of four-layer quality scalability and three-layer spatial scalability, respectively.","","978-1-4244-4593-6978-1-4244-4594","10.1109/PCS.2009.5167399","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5167399","SVC;Rate-Distortion Optimization;Lagrange Multiplier Selection","Lagrangian functions;Rate-distortion;Static VAr compensators;Video coding;Scalability;Automatic voltage control;Nonhomogeneous media;Application software;Computational modeling;Computational efficiency","optimisation;rate distortion theory;video coding","Lagrange multiplier selection;rate-distortion optimization;scalable video coding;SVC;single layer video coding;H.264/AVC;RDO;four-layer quality scalability;three-layer spatial scalability","","18","11","","","","","","IEEE","IEEE Conferences"
"Electrolyser Testing for Small Size RE/H2 Systems Development at IDRILAB","G. M. Tina; A. Moschetto; F. Cipiti","UNICT DIEES University of Catania - Electrical, Electronic and Systemic Department, Viale A. Doria, 5 - Catania, (Italy); UNICT DIEES University of Catania - Electrical, Electronic and Systemic Department, Viale A. Doria, 5 - Catania, (Italy); ECOENERGY - Hydrogen Systems Engineering, Via Etnea, 587 Catania, (Italy)","2007 International Conference on Clean Electrical Power","","2007","","","488","491","In order to develop renewable energy hydrogen-based storage systems, improving knowledge of low-cost conventional electrolysis technology is one of the main issues. This paper refers about initial testing activities carried out to assess the performances of a commercial standard alkaline-type electrolyser and its adequacy to be used in renewable hydrogen storage applications. From experimental data, electrolyser I-V characteristics have been found. Their knowledge allowed to start the development of a software tool, useful to optimize photovoltaic/electrolyser matching design. This work has been accomplished at IDRILAB, a research laboratory specifically established to test and develop renewable hydrogen systems, with the partnership of University of Catania and a private investor.","","1-4244-0631-51-4244-0632","10.1109/ICCEP.2007.384259","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4272430","Alkaline;Electrolysis;Hydrogen;Renewable Energy Storage","System testing;Hydrogen storage;Renewable energy resources;Energy storage;Electrochemical processes;Performance evaluation;Application software;Software tools;Design optimization;Photovoltaic systems","electrolysis;hydrogen storage","electrolyser testing;small-size RE/H<sub>2</sub> systems;IDRILAB;renewable energy hydrogen-based storage systems;alkaline-type electrolyser;I-V characteristics;photovoltaic-electrolyser matching design;H<sub>2</sub>","","","3","","","","","","IEEE","IEEE Conferences"
"Interactive Modelling of the Robotized Systems","A. Pisla; D. A. Vidrean; D. L. Pisla","The Technical University of Cluj-Napoca, 15 C. Daicoviciu, Cluj-Napoca, RO-400020, Romania. Fax: +40 264-430 461, e-mail: apisla@rdslink.ro; The Technical University of Cluj-Napoca, 15 C. Daicoviciu, Cluj-Napoca, RO-400020, Romania; The Technical University of Cluj-Napoca, 15 C. Daicoviciu, Cluj-Napoca, RO-400020, Romania","2006 IEEE International Conference on Automation, Quality and Testing, Robotics","","2006","2","","249","253","The paper deals with optimized facilities regarding robots, seen as very complex systems that within the design and modeling must consider long lists of restrictions. Therefore, product life management (PLM) is considered as an optimum solution for grouping all the restrictions under a single umbrella, having different specialized software packages for CAD/CAM/FEA/PDM","","1-4244-0360-X1-4244-0361","10.1109/AQTR.2006.254640","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4022963","","Grippers;Sensor systems and applications;Data processing;Robot programming;Mathematical model;Design optimization;Software packages;Design automation;Computer aided manufacturing;CADCAM","control engineering computing;robots","robotized system interactive modelling;robot optimized facilities;very complex systems;product life management;specialized software packages;CAD;CAM;FEA;PDM","","","6","","","","","","IEEE","IEEE Conferences"
"Statistical Testing of Off-line Comparative Subjective Evaluations for Optimizing Perceptual Conversational Quality in VoIP","B. Sat; B. W. Wah","NA; NA","2008 Tenth IEEE International Symposium on Multimedia","","2008","","","424","431","In this paper, we study the scheduling of off-line subjective tests for evaluating alternative parameter values of control schemes in real-time multimedia applications. These applications are characterized by multiple counteracting objective quality metrics (such as delay and signal quality) that can be affected by the control schemes. However, the tradeoffs among these metrics with respect to the subjective preferences of users are not defined. As a result, it is difficult to use the proper control value that leads to the best subjective quality without carrying out subjective tests. Since subjective tests are expensive to conduct and the number of possible control values and run-time conditions is prohibitively large, it is important that a minimum number of such tests be conducted, and that the results learned can be generalized to unseen conditions with statistical confidence. To this end, we study in this paper optimal algorithms for scheduling a sequence of subjective tests, while leaving the generalization of limited off-line subjective tests to guide the operations of the control schemes at run time to a future paper. Using an example application in the design of the play out scheduling (POS) algorithm for a two-party VoIP system, we study the accuracy and efficiency of conducting subjective tests simultaneously.","","978-0-7695-3454","10.1109/ISM.2008.116","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4741202","Subjective Tests;Comparative Evaluation;Mean Opinion Score;Real-time;multimedia communication;Statistical methods;VoIP;Conversational Quality","Statistical analysis;Communication system control;Delay;Control systems;Application software;Scheduling algorithm;System testing;IP networks;USA Councils;Processor scheduling","Internet telephony;multimedia communication;scheduling;statistical testing","statistical testing;off-line comparative subjective evaluations;perceptual conversational quality;VoIP;multiple counteracting objective quality metrics;statistical confidence;playout scheduling algorithm","","1","5","","","","","","IEEE","IEEE Conferences"
"A Writer Recognition approach Based on SVM","Z. Yin; P. Yin; F. Sun; H. Wu","Dept. of Computer Science and Technology, School of Software, State Key Lab of Intelligent Technology and Systems of Tsinghua University, Beijing 100084, P.R. China. Phone: +086-010-62796858, E-mail: zhying9999@163.com; Dept. of Computer Science and Technology, School of Software, State Key Lab of Intelligent Technology and Systems of Tsinghua University, Beijing 100084, P.R. China. Phone: +086-010-62796858, E-mail: ypp05@mails.tsinghua.edu.cn; Dept. of Computer Science and Technology, School of Software, State Key Lab of Intelligent Technology and Systems of Tsinghua University, Beijing 100084, P.R. China. Phone: +086-010-62796858, E-mail: fcsun@tsinghua.edu.cn; Dept. of Computer Science and Technology, School of Software, State Key Lab of Intelligent Technology and Systems of Tsinghua University, Beijing 100084, P.R. China. Phone: +086-010-62796858","The Proceedings of the Multiconference on "Computational Engineering in Systems Applications"","","2006","1","","581","586","A writer recognition approach based on SVM is proposed in this paper. Above all, the effect of SVM on the writer recognition is also analyzed under the condition of small samples and weak features, and then the improvement of its performance is validated by experiments. Futhermore, in terms of C-SVC, two parameter optimization schemes are put up based on collapse tests plus margin-based bounds and collapse tests plus compression bounds, respectively, their performance are compared, thus the generation performance of the classifier is enhanced","","7-302-13922-97-900718-14","10.1109/CESA.2006.4281720","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4281720","C-SVC;writer identification","Support vector machines;Systems engineering and theory;Testing;Support vector machine classification;Feature extraction;Image segmentation;Application software;Sun;Computer science;Intelligent systems","image recognition;optimisation;pattern classification;support vector machines","writer recognition;support vector machines;parameter optimization;collapse tests plus margin-based bounds;collapse tests plus compression bounds;classifier","","2","6","","","","","","IEEE","IEEE Conferences"
"Diagnositc Reasoner Model Production from Atlas Test Program Sets","R. McCroskey; M. Dietz","Honeywell International, 3660 Technology Dr, Minneapolis, MN 55418. (612)951-7750; Honeywell International, 3660 Technology Dr, Minneapolis, MN 55418. (612)951-7239","2006 IEEE Autotestcon","","2006","","","649","653","Model-based diagnostic reasoning provides improvements in fault isolation and repair time as well as cost savings provided by a reduction in false pull rates. To support legacy systems that use ATLAS test program sets for fault isolation and repair, Honeywell has developed a method for extracting diagnostic fault models from ATLAS source code. We will describe the content of the fault models and highlight model features that use data collected in a netcentric maintenance environment to optimize reasoner performance. We will then present a description of the method used to create bootstrap fault models, including a configurable ATLAS source code parser that uses regular text expressions to extract test and fault information from Test Program Sets and create fault models.","1088-7725;1558-4550","1-4244-0052-X1-4244-0051","10.1109/AUTEST.2006.283741","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4062454","","Production;Data mining;System testing;Isolation technology;Costs;Optimization methods;XML;Performance evaluation;Fault detection;Fault trees","diagnostic expert systems;software maintenance;XML","model-based diagnostic reasoning;fault isolation;false pull rates reduction;legacy systems;ATLAS test program sets;bootstrap fault models;configurable ATLAS source code parser","","1","2","","","","","","IEEE","IEEE Conferences"
"Optimizing Generation of Object Graphs in Java PathFinder","M. Gligoric; T. Gvero; S. Lauterburg; D. Marinov; S. Khurshid","NA; NA; NA; NA; NA","2009 International Conference on Software Testing Verification and Validation","","2009","","","51","60","Java PathFinder (JPF) is a popular model checker for Java programs. JPF was used to generate object graphs as test inputs for object-oriented programs. Specifically, JPF was used as an implementation engine for the Korat algorithm. Korat takes two inputs---a Java predicate that encodes properties of desired object graphs and a bound on the size of the graph---and generates all graphs (within the given bound) that satisfy the encoded properties. Korat uses a systematic search to explore the bounded state space of object graphs. Korat search was originally implemented in JPF using a simple instrumentation of the Java predicate. However, JPF is a general-purpose model checker and such direct implementation results in an unnecessarily slow search. We present our results on speeding up Korat search in JPF. The experiments on ten data structure subjects show that our modifications of JPF reduce the search time by over an order of magnitude.","2159-4848","978-1-4244-3775-7978-0-7695-3601","10.1109/ICST.2009.52","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4815337","","Java;Object oriented modeling;Automatic testing;Search engines;Software testing;State-space methods;Instruments;Data structures;Costs;Protocols","Java;object-oriented programming;program testing;program verification","Java PathFinder;object graphs;model checker;Java programs;object-oriented programs;Korat algorithm","","4","37","","","","","","IEEE","IEEE Conferences"
"Power-Constrained SOC Test Schedules through Utilization of Functional Buses","F. A. Hussin; T. Yoneda; H. Fujiwara; A. Orailoglu","Graduate School of Information Science, Nara Institute of Science and Technology, Kansai Science City, 630-0192, Japan. fawniz-h@is.naist.jp; Graduate School of Information Science, Nara Institute of Science and Technology, Kansai Science City, 630-0192, Japan. yoneda@is.naist.jp; Graduate School of Information Science, Nara Institute of Science and Technology, Kansai Science City, 630-0192, Japan. fujiwara@is.naist.jp; Computer Science and Engineering Department, University of California San Diego, La Jolla, CA 92093. alex@cs.ucsd.edu","2006 International Conference on Computer Design","","2006","","","230","236","In this paper, we are proposing a core-based test methodology that utilizes the functional bus for test stimuli and response transportation. An efficient algorithm for the generation of a complete test schedule that efficiently utilizes the functional bus under a power constraint is described. The test schedule is composed of a set of test vector delivery sequences in small chunks, denoted as packets. The utilization of small packet sizes optimizes the functional bus utilization. The experimental results show that the methodology is highly effective compared to previous approaches that do not use the functional bus. The strong results of the proposed approach are particularly highlighted when small bus widths are considered, an important consideration in current SOC designs where increasingly larger bus widths pose routing and reliability challenges.","1063-6404","978-0-7803-9706-4978-0-7803-9707","10.1109/ICCD.2006.4380822","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4380822","","Circuit testing;Automatic testing;Processor scheduling;Performance evaluation;Communication networks;Software testing;Built-in self-test;Information science;Cities and towns;Computer science","circuit testing;network synthesis;system-on-chip","power-constrained SOC test schedules;functional buses utilization;core-based test methodology;test vector delivery sequences;SOC designs","","7","17","","","","","","IEEE","IEEE Conferences"
"Keynote: Security Engineering: Developments and Directions","P. B. Thuraisingham","NA","2009 Third IEEE International Conference on Secure Software Integration and Reliability Improvement","","2009","","","2","3","Security Engineering is a critical component of systems engineering. When complex and large systems are put together, one needs to ensure that the systems are secure. Security engineering methodologies include gathering the security requirements, specifying the security policies, designing the security model, identifying the security critical components of the system design, security verification and validation and security testing. Before installation, one needs to develop a concept of operation (CONOPS) as well as carry out certification and accreditation. Much of the previous work in security engineering has focused on end to end security. That is, the organization needs to ensure that the applications, database systems, operating systems and networks have to be secure. In addition, one needs to ensure security when the subsystems are composed to form a larger system. More recently with open systems and the Web, secure system development is taking a whole new direction. The Office of the Deputy Assistant Secretary of Defense in the United States (Information and Identity Assurance) has stated that ""the Department of Defense's (DoD) policy, planning, and war fighting capabilities are heavily dependent on the information technology foundation provided by the Global Information Grid (GIG). However, the GIG was built for business efficiency instead of mission assurance against sophisticated adversaries who have demonstrated intent and proven their ability to use cyberspace as a tool for espionage and criminal theft of data. GIG mission assurance works to ensure the DoD is able to accomplish its critical missions when networks, services, or information are unavailable, degraded, or distrusted."" To meet the needs of mission assurance challenges, President's (George W. Bush) cyber plan (CNCI) has listed the area of developing multipronged approaches to supply chain risk management as one of the priorities. CNCI states that the reality of global supply chains presents significant challenges in thwarting counterfeit, or maliciously designed hardware and software products. To overcome such challenges and support successful mission assurance we need to design flexible and secure systems whose components may be untrusted or faulty. We need to achieve the secure operation of mission critical systems constructed from untrusted, semitrusted and fully trusted components for successful mission assurance. This keynote address will discuss the developments in security engineering from requirements, to policy to model to design to verification to testing as well as developing CONOPS and conducting certification and accreditation. System evaluation, usability and metrics related issues will also be discussed. Finally we will discuss the changes that have to be made to security engineering to support the next generation of secure systems for mission critical applications.","","978-0-7695-3758","10.1109/SSIRI.2009.74","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5325402","","Data security;Design engineering;Certification;Accreditation;Supply chains;Mission critical systems;Systems engineering and theory;System testing;Database systems;Operating systems","military computing;program testing;program verification;security of data;software metrics;software reusability","complex system;large system;security engineering methodology;security requirement;security policy;security verification;security validation;CONOPS system;operation concept;secure system development;DoD policy;war fighting capability;global information grid;GIG mission assurance;CNCI cyber plan;supply chain risk management;flexible system;mission critical system;system evaluation;system usability;system metric;database system;operating system","","1","","","","","","","IEEE","IEEE Conferences"
"Optimization of motion estimation in H.264/AVC encoder for video conference application","Rong Luo; Bin Chen","Dept. of Electronic Engineering/Tsinghua University/Beijing, 100084, China; Dept. of Electronic Engineering/Tsinghua University/Beijing, 100084, China","2009 International Conference on Communications, Circuits and Systems","","2009","","","537","539","The optimization of motion estimation in H.264/AVC encoder for video conference application is proposed. Firstly, an advanced diamond search algorithm is proposed. Then, for TMS320DM642's special architecture, a hierarchical coded optimization is proposed based on the intrinsic functions. The optimized encoder is tested on TMS320DM642 platform, which confirms that the CIF format sequence can be encoded at 40fps or so.","","978-1-4244-4886-9978-1-4244-4888","10.1109/ICCCAS.2009.5250484","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5250484","","Motion estimation;Automatic voltage control;Videoconference;Video compression;Motion analysis;Video coding;Testing;Application software;Electronic mail;Decoding","motion estimation;optimisation;search problems;sequences;teleconferencing;video coding;video communication","motion estimation optimization;H.264/AVC encoder;video conference application;diamond search algorithm;hierarchical coded optimization;intrinsic function;TMS320DM642 platform;CIF format sequence","","","11","","","","","","IEEE","IEEE Conferences"
"Towards automatic generation of test data using branch coverage","J. Chen; L. Yang","School of Information Science and Engineering, Central South University, Changsha 410083, China; School of Information Science and Engineering, Central South University, Changsha 410083, China","2009 4th International Conference on Computer Science & Education","","2009","","","921","925","By analyzing various methods of automatic generation of test data using branch coverage, their characteristics and disadvantages are discussed, a new algorithm for automatic generation of test data is proposed. Through constructing the new procedure flow chart, the algorithm optimizes the selection paths using Fibonacci law, and generates test data for assigned branch. When the branch predicates of the chosen path are linear representation, solve the linear restraint set directly to generate test data, otherwise determine that the path is inaccessible; When the branch predicate composing of nonlinear expression, linearize nonlinear function by using the divided difference approximate derivative to ensure the test data can easily generated through simple iteration, or conclude that path is inaccessible to a large extent. If the chosen path is to a large extent inaccessible or inaccessible, then a new path is selected, repeat the above process until the desired data obtained, if no new path was chosen, then the specified branch was inaccessible. Experiments show that the algorithm is feasible and valid.","","978-1-4244-3520-3978-1-4244-3521","10.1109/ICCSE.2009.5228264","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5228264","Branch coverage;Predicate function;Linear arithmetic representation","Automatic testing;Flowcharts;Computer science;Computer science education;Character generation;Software testing;Information science;Data engineering;Information analysis;Algorithm design and analysis","optimising compilers;program testing","automatic test data generation;branch coverage;flow chart;Fibonacci law;selection path optimisation;branch predicate;linear representation;nonlinear function;divided difference approximate derivative;iterative method","","1","12","","","","","","IEEE","IEEE Conferences"
"Automatic Processor Customization for Zero-Overhead Online Software Verification","H. Lu; A. Forin","NA; NA","IEEE Transactions on Very Large Scale Integration (VLSI) Systems","","2008","16","10","1346","1357","The PSL-to-Verilog (P2V) compiler can translate a set of assertions about a block-structured software program into a hardware design to be executed concurrently with the program. The assertions validate the correctness of the software program without altering the program's temporal behavior in any way, a result never previously achieved by any online model-checking system. Additionally, the techniques and implementations described apply to any general purpose program and the absence of execution overhead renders the system ideal for the verification and debugging of real-time systems. Assertions are expressed in a simple subset of the property specification language (PSL), an IEEE standard originally intended for the behavioral specification of hardware designs. The target execution system is the eMIPS processor, a dynamically self-extensible processor realized with a field-programmable gate array (FPGA). The system can concurrently execute and check multiple programs at a time. Assertions are compiled into eMIPS Extensions, which are loaded by the operating system software into a portion of the FPGA, and discarded once the program terminates. If an assertion is violated, the program receives an exception, otherwise, it executes fully unaware of its verifier. The software program is not modified in any way. It can be compiled separately with full optimizations and executes with or without the corresponding hardware checker. The P2V compiler, implemented in Python, generates code for the implementation of the eMIPS processor running on the Xilinx ML401 development board. It is currently used to verify software properties in areas such as testing, debugging, intrusion detection, and the behavioral verification of concurrent and real-time programs.","1063-8210;1557-9999","","10.1109/TVLSI.2008.2002047","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4629346","Assertion-based verification;eMIPS;monitor;online;PSL-to-Verilog (P2V);property specification language (PSL);temporal logic;Verilog","Hardware;Field programmable gate arrays;Program processors;Real time systems;Specification languages;Operating systems;Software systems;Software debugging;Software testing;Intrusion detection","field programmable gate arrays;formal specification;hardware-software codesign;IEEE standards;logic design;microprocessor chips;program compilers;specification languages","automatic processor customization;zero-overhead online software verification;PSL-to-Verilog compiler;block-structured software program;hardware design;online model-checking system;general purpose program;execution overhead;real-time systems;property specification language;IEEE standard;behavioral specification;target execution system;eMIPS processor;dynamically self-extensible processor;field-programmable gate array;eMIPS Extensions;operating system software;P2V compiler;Python;Xilinx ML401 development board;behavioral verification;concurrent programs;real-time programs","","6","20","","","","","","IEEE","IEEE Journals & Magazines"
"A Genetic Algorithm For A Bicriteria Flexible Job Shop Scheduling Problem","G. Vilcot; J. Billaut; C. Esswein","Volume Software, 20 rue Dublineau BP2630, 37026 Tours Cedex 1, France; Université François-Rabelais de Tours, Laboratoire d'Informatique, 64 avenue Jean Portalis, 37200 Tours, France. g.vilcot@volume-software.com, geoffrey.vilcot; Université François-Rabelais de Tours, Laboratoire d'Informatique, 64 avenue Jean Portalis, 37200 Tours, France. jean.billaut@univ-tours.fr; Université François-Rabelais de Tours, Laboratoire d'Informatique, 64 avenue Jean Portalis, 37200 Tours, France. carl.esswein@univ-tours.fr","2006 International Conference on Service Systems and Service Management","","2006","2","","1240","1244","The problem we consider in this paper is a flexible job shop scheduling problem. The aim is to minimize two criteria: the makespan and the maximum lateness and we are interested in finding an approximation of the Pareto front. The method that we propose is a genetic algorithm. The initial population is partially generated by using a tabu search algorithm which minimizes a linear combination of the two criteria. The method is tested on benchmark instances from literature","2161-1890;2161-1904","1-4244-0451-71-4244-0450","10.1109/ICSSSM.2006.320686","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4114668","Scheduling;Flexible Job Shop;Genetic algorithm;Bicriteria","Genetic algorithms;Job shop scheduling;Heuristic algorithms;Benchmark testing;Time measurement;Vectors;Optimization methods;Evolutionary computation;Fuzzy logic;Glass","benchmark testing;genetic algorithms;job shop scheduling;Pareto optimisation;search problems","genetic algorithm;flexible job shop scheduling problem;Pareto front;tabu search algorithm;benchmark instances","","2","11","","","","","","IEEE","IEEE Conferences"
"Research of Autonomic Software Based on Complexity","H. Zhang; H. Wang; H. Liu","NA; NA; NA","2008 4th International Conference on Wireless Communications, Networking and Mobile Computing","","2008","","","1","5","Autonomic computing (AC) is a new domain, based on AC theory to design autonomic system is not perfect and implement process is more difficult. Especially, there lack tools and models to test the autonomic system. According to above-mentioned reason, this paper mainly discusses dependability of autonomic software. We proposed a model to analyze autonomic maturity of software: First introduces software autonomic characteristics, autonomic manager; Second, summarizes the software complexity, relation between the autonomic and the complex; Third discusses problems of software autonomic and resolve technology. Finally analyses validness of the model. In the future, it is to be optimized in the application.","2161-9646;2161-9654","978-1-4244-2108-4978-1-4244-2107","10.1109/WiCom.2008.1321","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4679229","","Monitoring;Crisis management;Humans;Hardware;Educational institutions;Computer science;Agricultural engineering;Design engineering;System testing;Technology management","software fault tolerance","autonomic computing;autonomic system;software autonomic maturity;software autonomic characteristics;autonomic manager;software complexity","","","12","","","","","","IEEE","IEEE Conferences"
"Design of the Remote Control Software for Weather Radar Based on UDP","Y. She","NA","2009 WRI World Congress on Software Engineering","","2009","1","","150","153","At first, the paper describes the design demand of the remote control software for weather radar, then analyses the testing result of the antenna azimuth motion based on TCP/UDP data transmission, discusses the limitation of TCP protocol when data is transmitted with shorter size package in the remote control system, finally obtains conclusion that UDP protocol has better performance in real-time data transmission. In order to improve transmission reliability of the radar severs/client software, the characteristic of the radar data package based on UDP can be used to error control. By optimizing the program structure, the remote control software with transmission error control is realized in XDR-X Baud digital weather radar.","","978-0-7695-3570","10.1109/WCSE.2009.399","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5319004","weather radart;remote control;realtime;TCP;UDP","Meteorological radar;Motion control;Size control;Control systems;Radar antennas;Data communication;Protocols;Software packages;Packaging;Error correction","client-server systems;meteorological radar;radar antennas;radar computing;telecontrol;transport protocols","remote control software;antenna azimuth motion;TCP-UDP data transmission;transmission reliability;sever-client software;XDR-X Baud digital weather radar","","","5","","","","","","IEEE","IEEE Conferences"
"Optimization of a multi-band reconfigurable PIFA antenna","S. Loizeau; A. Sibille","ENSTA-ParisTech, 32 Bd VICTOR, 75739 cedex 15, France; ENSTA-ParisTech, 32 Bd VICTOR, 75739 cedex 15, France","2009 3rd European Conference on Antennas and Propagation","","2009","","","796","800","In this paper a frequency reconfigurable PIFA antenna is introduced and optimized. It features two microwave switches for a total of four configurations. Measurements are done on the antenna, which are then used to re-optimize the design.","2164-3342","978-1-4244-4753-4978-3-00-024573","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5067738","","Frequency;Switches;Antenna accessories;Insertion loss;Antenna measurements;Geometry;Dielectric substrates;Cognitive radio;Software radio;Pattern matching","antenna testing;microwave switches;multifrequency antennas;optimisation;planar inverted-F antennas","optimization;multiband reconfigurable PIFA antenna;microwave switches;planar inverted-F antennas","","1","13","","","","","","IEEE","IEEE Conferences"
"Prioritizing Software Inspection Results using Static Profiling","C. Boogerd; L. Moonen","Delft University of Technology, The Netherlands; Delft University of Technology, The Netherlands","2006 Sixth IEEE International Workshop on Source Code Analysis and Manipulation","","2006","","","149","160","Static software checking tools are useful as an additional automated software inspection step that can easily be integrated in the development cycle and assist in creating secure, reliable and high quality code. However, an often quoted disadvantage of these tools is that they generate an overly large number of warnings, including many false positives due to the approximate analysis techniques. This information overload effectively limits their usefulness. In this paper we present ELAN, a technique that helps the user prioritize the information generated by a software inspection tool, based on a demand-driven computation of the likelihood that execution reaches the locations for which warnings are reported. This analysis is orthogonal to other prioritization techniques known from literature, such as severity levels and statistical analysis to reduce false positives. We evaluate feasibility of our technique using a number of case studies and assess the quality of our predictions by comparing them to actual values obtained by dynamic profiling.","","0-7695-2353","10.1109/SCAM.2006.22","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4026864","","Inspection;Software quality;Software tools;Statistical analysis;Documentation;Costs;Software testing;Security;Embedded system;Economic forecasting","","","","18","41","","","","","","IEEE","IEEE Conferences"
"PSO Algorithm Combined with Neural Network Training Study","X. Cheng; D. Wang; K. Xie; J. Zhang","NA; NA; NA; NA","2009 International Conference on Computational Intelligence and Software Engineering","","2009","","","1","4","Neural network often is trained by multilayer feedforward neural network ago, but it may fall into local minimum point. In this article, swarm optimization particle is improved so that it can adapt to solve optimization problem of discrete variables. At the same time, introducing the crossover operation of genetic algorithm make it form hybrid particle swarm optimization. Then combining the method of neural network, weight training of neural network is transformed into function optimization. The error function is cited as the definition of particle fitness. Last, in the information filtering. The efficient is compared using the multilayer and particle swarm optimization.","","978-1-4244-4507","10.1109/CISE.2009.5367189","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5367189","","Neural networks;Multi-layer neural network;Feedforward neural networks;Particle swarm optimization;Feedforward systems;Genetic algorithms;Information filtering;Computer science;Optimization methods;Testing","feedforward neural nets;genetic algorithms;information filtering;particle swarm optimisation","particle swarm optimization;neural network training study;multilayer feedforward neural network;discrete variables;genetic algorithm;information filtering;particle fitness","","","12","","","","","","IEEE","IEEE Conferences"
"On Optimization of Miller-Rabin Primality Test on TI TMS320C54x Signal Processors","G. Dordevic; M. Markovic","Institute for manufacturing banknotes and coins NBS, Pionirska 2, 11030 Belgrade, Serbia, goran.djordjevic@nbs.yu; Banca Intesa ad Beograd, Milutina Milankovi¿a 1c, 11070 Belgrade, Serbia, mmarkovic@bancaintesabeograd.com","2007 14th International Workshop on Systems, Signals and Image Processing and 6th EURASIP Conference focused on Speech and Image Processing, Multimedia Communications and Services","","2007","","","229","232","In this paper, possibilities of realization of Miller-Rabin big number primality test on assembler of Texas Instruments digital signal processors of TMS320C54x family are considered. The importance of realization of the reliable Miller-Rabin primality test of very large numbers for the purpose of asymmetric RSA cryptographic algorithm is emphasized. An experimental analysis of the efficiency of Miller-Rabin test realization on appropriate signal processors of TMS320C54x family are presented. Possible optimization techniques for Miller-Rabin algorithm related to multiplication and modular reduction are evaluated. We modify the Karatsuba-Offman's algorithm and obtain a less recursive algorithm and applied it for the purpose of multiplication in the Miller-Rabin test. Obtained results justify the use of the application of coprocessor module on the basis on the considered signal processors with a hardware random number generator and the Miller-Rabin's algorithm for primality testing. Applying these modules, it could be achieved considerably higher level of the system security regarding to the software-only security systems.","","978-961-248-036-3978-961-248-029","10.1109/IWSSIP.2007.4381195","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4381195","Miller-Rabin's primality test;Digital signal processors;Karatsuba-Offman's algorithm;RSA algorithm;Chinese Remainder Theorem","Testing;Signal processing;Signal processing algorithms;Security;Assembly;Instruments;Digital signal processors;Cryptography;Signal analysis;Application software","coprocessors;cryptography;digital signal processing chips;random number generation","Miller-Rabin primality test;TI TMS320C54x signal processors;digital signal processors;asymmetric RSA cryptographic algorithm;coprocessor module;random number generator;software-only security systems","","","7","","","","","","IEEE","IEEE Conferences"
"Novel Adaptive Genetic Algorithm for Reactive Power Optimization of Power System","G. Yang; X. Liu","NA; NA","2009 International Conference on Computational Intelligence and Software Engineering","","2009","","","1","4","In order to avoid the premature convergence and improve convergence rate, a novel adaptive genetic algorithm for reactive power optimization is discussed in detail. In reproduction operator, the method of retaining optimal individual is used to ensure the convergence and at the same time, the competition method is also adopted to keep the better dispersal of all individuals. In Mutation operator, the mutation probability P<sub>m</sub> is improved based on adaptive genetic algorithm. When fitness of individuals in the population tends to be identical, P<sub>m</sub> can be adjusted to make bigger, and the local convergence can be avoided greatly. The algorithm has been applied to IEEE 30-bus testing system .The test shows that this algorithm is feasible and practical.","","978-1-4244-4507","10.1109/CISE.2009.5363700","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5363700","","Genetic algorithms;Reactive power;Power systems;Voltage;Mathematical model;Power generation;Genetic mutations;System testing;Power engineering and energy;Evolutionary computation","genetic algorithms;power systems;reactive power","adaptive genetic algorithm;reactive power optimization;power system;mutation operator;IEEE 30-bus testing system","","","9","","","","","","IEEE","IEEE Conferences"
"A New Method of Software Security Checking Based on Similar Feature Tree","J. Ren; L. Meng; C. Hu; K. Wang","NA; NA; NA; NA","2009 First International Conference on Information Science and Engineering","","2009","","","1553","1556","In order to optimize the fault feature database(FFD) and to improve the checking efficiency of software fault, in this paper, a novel method of software security checking based on similar feature tree(SFT) is proposed. All of fault feature patterns in FFD are considered as nodes of SFT. SFT is a special binary tree in which the left child of each node is a super-pattern of the node and the right child is a brother-pattern of the node. An improved K-modes clustering algorithm and association rules are used to construct SFT. According to the characteristics of association rules, if a fault feature which is obtained by program slicing from software procedure can't successfully matches to root of SFT, then it matches to the right child of the root. Otherwise the fault feature matches to the left child of the root. This process is iterated according to ""even left not right"" rule until leaf node in right sub-tree or no node can be successfully matched in left sub-tree. Finally, the checking result is given by SFT. Experimental results show that our method has higher efficiency of software fault checking.","2160-1283;2160-1291","978-1-4244-5728-1978-1-4244-4909-5978-0-7695-3887","10.1109/ICISE.2009.96","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5455083","","Clustering algorithms;Association rules;Computer bugs;Information science;Data engineering;Computer security;Spatial databases;Data security;Information security;Educational institutions","data mining;pattern clustering;program slicing;program testing;security of data;software fault tolerance","software security checking;similar feature tree;fault feature database;software fault checking;fault feature pattern;binary tree;k-modes clustering algorithm;association rule;program slicing","","","12","","","","","","IEEE","IEEE Conferences"
"Economic Dispatch with Environmental Considerations using Particle Swarm Optimization","M. R. AlRashidi; M. E. El-hawary","NA; NA","2006 Large Engineering Systems Conference on Power Engineering","","2006","","","41","46","This paper presents a particle swarm optimization (PSO) algorithm to solve an economic-emission dispatch problem (EED). This problem has been getting more attention recently due to the deregulation of the power industry and strict environmental regulations. It is formulated as a highly nonlinear constrained multiobjective optimization problem with conflicting objective functions. PSO algorithm is used to solve the formulated problem on two standard test systems, namely the 30-bus and 14-bus systems. Results obtained show that PSO algorithm outperformed most previously proposed algorithms used to solve the same EED problem. These algorithms included evolutionary algorithm, stochastic search technique, linear programming, and adaptive Hopfield neural network. PSO was able to find the Pareto optimal solution set for the multiobjective problem. In addition, PSO results were compared to LINGO software outcomes. Comparison results signify the effectiveness and robustness of PSO as a promising optimization tool for this specific problem","","1-4244-0556-41-4244-0557","10.1109/LESCPE.2006.280357","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4059363","Economic emission dispatch;economic cost dispatch;particle swarm;multiobjective optimization","Environmental economics;Particle swarm optimization;Power generation economics;Power industry;Constraint optimization;System testing;Evolutionary computation;Stochastic processes;Linear programming;Adaptive systems","environmental factors;evolutionary computation;linear programming;Pareto optimisation;power generation dispatch;power generation economics;stochastic programming","particle swarm optimization algorithm;economic-emission dispatch problem;power industry deregulation;environmental regulations;nonlinear constrained multiobjective optimization problem;evolutionary algorithm;stochastic search technique;linear programming;adaptive Hopfield neural network;Pareto optimal solution set","","12","22","","","","","","IEEE","IEEE Conferences"
"Applying Double Process Models for Verification in CMMI","L. Wenjie; L. Peng; Z. Weiming; Z. Bosheng","NA; NA; NA; NA","2008 International Conference on Computer Science and Software Engineering","","2008","2","","247","250","During the CMMI perform, quality assurance is realized by two aspects: monitor, control and review the development process of a product, and test product requirements. In order to insure the correctness and validity of the product quality assurance, this paper presents a product verification method which needs to build two process models: product development process model and product requirement process model by advanced modeling technology with visualization and simulation; standardize the description of the two processes by integrality check; validate and optimize them by simulation; and then acquire the review contents for each develop phase and system test use case by process slice technique.","","978-0-7695-3336","10.1109/CSSE.2008.467","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722045","","Capability maturity model;Product development;Computer science;Logic;Software engineering;Quality assurance;System testing;Quality control;Software tools;Educational institutions","Capability Maturity Model;formal verification;quality assurance;software quality","double process models;CMMI verification;test product requirements;product quality assurance;product development process model;product requirement process model","","","7","","","","","","IEEE","IEEE Conferences"
"Design of Workflow Working Time Server Supporting Highly Concurrent Request","G. Qin; J. Wang; T. Yuan; Q. Lv; X. Li","NA; NA; NA; NA; NA","2009 WRI World Congress on Software Engineering","","2009","4","","104","108","The working time is an important part in a workflow server. Clients always expect their requests could be responded in time. Calculating the working time according to the working calendar will result in the considerable delays in the requests. The more complex the calendar, the more obvious the delay. In recent years, many enterprise-scale workflows have been developed and many research works have been published. However, a workflow working time server which can support up to ten thousand of concurrent requests is still worth researching. In this paper, we define a working time calendar and design two algorithms for calculating the working time. One of the algorithms uses the method of feasible directions to calculate the next working time. Moreover, the paper proposes an optimizing calculating method for concurrent requests. The test results show that the algorithms are highly effective. The average executing time is 3.595 milliseconds, and the responding time of the concurrent request is independent of the number of request and timed tasks.","","978-0-7695-3570","10.1109/WCSE.2009.176","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5319512","working time algorithm;workflow;highly concurrent request","Calendars;Software engineering;Delay;Algorithm design and analysis;Optimization methods;Government;Engines;Software design;Educational institutions;Information technology","calenders;workflow management software","workflow working time server;highly concurrent request;working time calendar;enterprise-scale workflows;optimizing calculating method;request task;timed task","","","6","","","","","","IEEE","IEEE Conferences"
"Specification-Based Unit Testing of Publish/Subscribe Applications","A. Michlmayr; P. Fenkam; S. Dustdar","Vienna University of Technology; NA; NA","26th IEEE International Conference on Distributed Computing Systems Workshops (ICDCSW'06)","","2006","","","34","34","Testing remains the most applied verification method for software systems. Typically, the behavior of such systems is evaluated against their informal or formal specifications. In this paper, we consider an architecture-driven approach to software testing. We support the argument that, in many cases, the difficulties in testing can be alleviated by optimizing the test methodology to leverage the architecture of the application under test. To support this claim, we present a test framework for publish/subscribe applications. This paper evaluates our initial steps in this regard. We focus on the design of our framework, and illustrate how to accomplish unit testing of publish/subscribe applications against LTL specifications, considering a real-world application as example.","1545-0678","0-7695-2541","10.1109/ICDCSW.2006.103","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1648924","","Application software;Automatic testing;Java;Software testing;Software quality;System testing;Software systems;Formal specifications;Optimization methods;Computer architecture","","","","4","18","","","","","","IEEE","IEEE Conferences"
"A Hybrid Metaheuristic ACO-GA with an Application in Sports Competition Scheduling","H. Guangdong; L. Ping; W. Qun","China University of Geosciences, China; Beihang University, China; China University of Geosciences, China","Eighth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing (SNPD 2007)","","2007","3","","611","616","This paper presents a hybrid metaheuristic ACO-GA for the problem of sports competition scheduling (SCS). ACO-GA combines ant colony optimization (ACO) and genetic algorithms (GA). The procedures of ACO-GA are as follows. First, GA searches the solution space and generates activity lists to provide the initial population for ACO. Next, ACO is executed, when ACO terminates, the crossover and mutation operations of GA generate new population. ACO and GA search alternately and cooperatively in the solution space. Then we test ACO-GA with Oliver30 and att48. The results indicate that ACO-GA is an effective method. Finally this paper deals with SCS using ACO-GA.","","0-7695-2909-7978-0-7695-2909","10.1109/SNPD.2007.402","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4287925","","Genetic algorithms;Ant colony optimization;Scheduling;Geology;Genetic mutations;Feedback;Software engineering;Artificial intelligence;Distributed computing;Application software","optimisation;scheduling;sport","hybrid metaheuristic ACO-GA;sports competition scheduling;ant colony optimization;genetic algorithms;Oliver30;att48","","3","8","","","","","","IEEE","IEEE Conferences"
"Computer Based Emulation of Power Electronics Hardware","D. Majstorovic; Z. Pele; A. Kovacevic; N. Celanovic","NA; NA; NA; NA","2009 First IEEE Eastern European Conference on the Engineering of Computer Based Systems","","2009","","","56","64","This paper defines a highly optimized computer architecture and FPGA technology as the most feasible approach to satisfy the challenging requirements defined by the need to emulate power electronics hardware with sub-microsecond latency and sampling time. The proposed commercial off the shelf computational platforms and the accompanying software tools based on the industry standard software platform have the potential to bring qualitative improvements in the way how power electronics software is designed, how it is tested and how its performance is verified.","","978-1-4244-4677-3978-0-7695-3759","10.1109/ECBS-EERC.2009.20","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290990","Real time digital simulator;power electronics;FPGA.","Emulation;Power electronics;Hardware;Software tools;Software performance;Software quality;Electronic equipment testing;Computer architecture;Field programmable gate arrays;Delay","computer architecture;electronic engineering computing;field programmable gate arrays;power electronics;software architecture;software tools","power electronics hardware;computer based emulation;optimized computer architecture;FPGA technology;sub-microsecond latency;sampling time;shelf computational platforms;software tools;industry standard software platform;power electronics software","","6","20","","","","","","IEEE","IEEE Conferences"
"An industrial technology of test automation based on verified behavioral models of requirement specifications for telecommunication applications","S. Baranov; V. Kotlyarov; A. Letichevsky; V. M. Glushkov","Motorola Inc., USA; Motorola Inc., USA; Institute for Cybernetics, USA; Institute for Cybernetics, USA","IEEE EUROCON 2009","","2009","","","1272","1279","Applicability of formal methods in requirement capturing and analysis is determined to a great extent by how well the respective formalism matches the adopted engineering practice. Conventional notation of mathematical logic obviously has little chance to succeed as such formalization tool. A technology based on formalism of ldquobasic protocolsrdquo derived from the modeling languages UML/MSC which became a de facto standard in the current engineering practice is described in this paper. It allows for automated test generation for software which guarantees 100% coverage of thus formalized requirements. Suite optimization w.r.t. various criteria borrowed from practice is discussed, as well as potential directions for further research.","","978-1-4244-3860-0978-1-4244-3861","10.1109/EURCON.2009.5167801","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5167801","Requirement Formalization;Requirement Verification;Test Automation","Communication industry;Automatic testing;Automation;Software testing;Cybernetics;Logic;Protocols;Unified modeling language;Potential well;Natural languages","formal specification;program testing;Unified Modeling Language","test automation;verified behavioral models;requirement specifications;telecommunication applications;formal methods;mathematical logic;UML-MSC;automated test generation","","","9","","","","","","IEEE","IEEE Conferences"
"Architecture-aware Partial Order Reduction to Accelerate Model Checking of Networked Programs","C. Artho; W. Leungwattanakit; M. Hagiya; Y. Tanabe","NA; NA; NA; NA","2008 Ninth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing","","2008","","","807","813","Testing cannot cover all execution schedules in concurrent software. Model checking, however, is capable of verifying the outcome of all possible executions. It has been applied successfully to networked software, with all processes being analyzed in conjunction. Unfortunately, this approach does not scale very well. This paper presents a partial-order reduction through which a performance gain of up to 70% was achieved.","","978-0-7695-3263","10.1109/SNPD.2008.50","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4617470","Software model checking;model checking;partial-order reduction;optimization;centralization","Servers;Optimization;Java;Pediatrics;Software;Schedules;Computational modeling","software architecture","architecture-aware partial order reduction;model checking;networked programs;execution schedules;concurrent software;networked software","","","21","","","","","","IEEE","IEEE Conferences"
"Improved Iterative Object-Oriented Reengineering Process based on Dynamic Coupling Measures","J. Li; X. Yang; B. Xu; Y. Ding","PHD Student, College of Computer Science and Technology, Zhejiang University, Hangzhou, Zhejiang 310027 PRC. phone: 86-136-06807865, e-mail: lijuefeng@zju.edu.cn; Associate Professor, College of Computer Science and Technology, Zhejiang University, Hangzhou, Zhejiang 310027, PRC. e-mail: yangxh@zju.edu.cn; Zhejiang University, Hangzhou, Zhejiang 310027 PRC. College of Computer & Information Engineering, Zhejiang Gongshang University, Hangzhou, Zhejiang 310027 PRC. e-mail: bx93@hotmail.com; MS student, College of Computer Science and Technology, Zhejiang University, Hangzhou, Zhejiang 310027 PRC. e-mail: smile_ding@zju.edu.cn","2006 IEEE International Conference on Systems, Man and Cybernetics","","2006","3","","2209","2214","Reengineering from degenerated but in-use legacy systems to high-maintainable object-oriented (OO) software is becoming a hot research area in software engineering. The iterative reengineering process provide a gradually approach to replace the legacy system and satisfy the quality and functionally requirements. Meanwhile, based on the observed relationships between software coupling and external quality factors of OO system, a sophisticated and operable coupling measures framework is stated. This paper proposes an improved iterative reengineer process for achieving a low-coupling and high-maintainable OO system using dynamic software coupling measures. As an enhancement, a formal dynamic coupling evaluation process is discussed detailedly. Moreover, two possible methods are given for conducting software engineers to optimize reengineered system's external quality.","1062-922X","1-4244-0099-61-4244-0100","10.1109/ICSMC.2006.385189","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4274195","","Software measurement;Pollution measurement;Software quality;Partial response channels;Area measurement;Computer science;Q factor;Software systems;Computer architecture;Logic testing","object-oriented programming;software maintenance;software metrics;software quality","iterative object-oriented reengineering process;dynamic coupling measures;legacy systems;object-oriented software maintenance;software engineering;dynamic software coupling measures;formal dynamic coupling evaluation process","","","11","","","","","","IEEE","IEEE Conferences"
"Scrum and CMMI Level 5: The Magic Potion for Code Warriors","J. Sutherland; C. R. Jakobsen; K. Johnson","NA; NA; NA","Proceedings of the 41st Annual Hawaii International Conference on System Sciences (HICSS 2008)","","2008","","","466","466","Projects combining agile methods with CMMI are more successful in producing higher quality software that more effectively meets customer needs at a faster pace. Systematic software engineering works at CMMI level 5 and uses lean software development as a driver for optimizing software processes. Early pilot projects showed productivity on Scrum teams almost twice that of traditional teams. Other projects using a story-based test-driven approach to software development reduced defects in final test by 40%. We assert that Scrum and CMMI together bring a more powerful combination of adaptability and predictability than either one alone and suggest how other companies can combine them.","1530-1605","978-0-7695-3075","10.1109/HICSS.2008.384","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4439172","","Programming;Coordinate measuring machines;Software engineering;Capability maturity model;Software quality;Software testing;Uncertainty;Collaborative software;Productivity;Power system modeling","Capability Maturity Model;software quality","Capability Maturity Model;CMMI level 5;agile project methods;software quality;systematic software engineering;lean software development;software process optimization;Scrum project","","8","19","","","","","","IEEE","IEEE Conferences"
"C++ Optimizations for Mobile Applications","F. Chehimi; P. Coulton; R. Edwards","PhD student in the Department of Communication Systems at Lancaster University, Lancaster, LA1 4YW, UK. e-mail: f.chehimi@lancs.ac.uk; NA; NA","2006 IEEE International Symposium on Consumer Electronics","","2006","","","1","6","Mobile application development requires many techniques unfamiliar to the general PC developer due to the limitations presented by the mobile platforms. Unlike the PC environment where hardware capabilities can easily be upgraded to accommodate more complex applications, in the mobile environment the onus is on the ability of the developer to produce optimized software with minimal overhead to outcome the desired results. In this paper we define generic paradigms relating to memory and code optimizations for Symbian C++ applications on mobile phones. We illustrate through testing, on an actual device, the advantages that these techniques can produce and their importance for mobile application developers","0747-668X;2159-1423","1-4244-0216","10.1109/ISCE.2006.1689506","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1689506","Optimization;C++;mobile applications;Symbian OS","Application software;Testing;Mobile handsets;Hardware;Telephony;Mobile communication;Business;Programming profession;Optimizing compilers;Computer architecture","C++ language;mobile handsets;optimisation","Symbian C++ application;mobile application;memory optimization;code optimization;mobile phones","","5","12","","","","","","IEEE","IEEE Conferences"
"The optimization algorithm of reducing dimensional FFT for harmonics analysis of power system","M. Yong; W. Bolin","School of Electric Engineering, Hohai University, Nanjing, 210098, China; School of Electric Engineering, Hohai University. Nanjing, 210098, China","2009 International Conference on Sustainable Power Generation and Supply","","2009","","","1","4","In order to obtain higher precision, FFT (Fast Fourier Transform) algorithm requires that sampling points (N) in time-domain are largely enough, if the number of points increases, the amount of calculation will increase further more. For meeting the need of calculating speed, in this paper an optimization algorithm of reducing dimensional FFT that is suitable for harmonic analysis is proposed, the algorithm is based on characteristics that the number of points in frequency domain is far less than that of time domain for harmonics analysis of power system, and the application of windowed reducing dimensional FFT algorithm is also studied in the power system harmonic analysis. The harmonic measurement software of power system based on this algorithm has been tested through simulative experiment. The results show that the precision of optimization algorithm of reducing dimensional FFT is identical to that of standard FFT algorithm, but the amount of calculation and occupied memory in the computer of the former are much less than those of latter. So the presented algorithm greatly reduces the amount of calculation under the premise of ensuring accuracy.","2156-9681;2156-969X","978-1-4244-4934","10.1109/SUPERGEN.2009.5348321","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5348321","harmonics analysis;windowed;FFT;reduced dimensional FFT;power system","Harmonic analysis;Power system analysis computing;Power system harmonics;Power system simulation;Time domain analysis;Power system measurements;Fast Fourier transforms;Sampling methods;Frequency domain analysis;Application software","fast Fourier transforms;frequency-domain analysis;optimisation;power system harmonics","optimization algorithm;windowed-reducing dimensional FFT algorithm;power system harmonics analysis;fast Fourier transform algorithm;sampling point;frequency-domain analysis;harmonic measurement software","","","6","","","","","","IEEE","IEEE Conferences"
"Integration and Test Sequencing for Complex Systems","R. Boumen; I. S. M. de Jong; J. M. G. Mestrom; J. M. van de Mortel-Fronczak; J. E. Rooda","NA; NA; NA; NA; NA","IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans","","2009","39","1","177","187","The integration and test phase of complex manufacturing machines, like an ASML lithographic manufacturing system, is expensive and time consuming. The tests that can be performed at a certain point in time during the integration phase depend on the modules that are integrated and, therefore, on the preceding integration sequence. In this paper, we introduce a mathematical model to describe an overall integration and test sequencing problem, and we propose an algorithm to solve this problem. The method is a combination of integration sequencing and test sequencing. Furthermore, we introduce several strategies that determine when test phases should start. With a case study within the development of a software release that is used to control an ASML lithographic machine, we show that the described method and strategies can be used to solve real-life problems.","1083-4427;1558-2426","","10.1109/TSMCA.2008.2006374","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4717832","Integration;manufacturing machines;semiconductor industry;test sequencing;test strategy","System testing;Performance evaluation;Manufacturing systems;Semiconductor device testing;Time to market;Automatic testing;Optimization methods;Mathematical model;Semiconductor device manufacture;Electronics industry","electronic engineering computing;integrated circuit manufacture;integrated manufacturing systems;large-scale systems;lithography","complex manufacturing machine;test sequencing phase;integration sequencing phase;ASML lithographic manufacturing system;mathematical model","","7","12","","","","","","IEEE","IEEE Journals & Magazines"
"A Reliable Parallel Interval Global Optimization Algorithm Based on Mind Evolutionary Computation","Y. Lei; S. Chen","NA; NA","2009 Fourth ChinaGrid Annual Conference","","2009","","","205","209","In this paper, we investigate the parallel reliable computational model and propose a parallel interval evolutionary algorithm that integrates interval arithmetic and Mind Evolutionary Computation method. The major aim is to explorer the new parallel interval decomposition scheme can solve computation intensive problem and can determine the all optimal solution reliably. The proposed algorithm is experimentally testified on the ZiQiang 3000 cluster of Shanghai High Education Grid-e-Grid Computational Application Platform with a test suit containing 6 complex multi-modal function optimization benchmarks.","1949-131X;1949-1328","978-0-7695-3818","10.1109/ChinaGrid.2009.29","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5328922","Parallel reliable computing;Interval algorithm;mind evolutionary computation;global optimization","Evolutionary computation;Concurrent computing;Grid computing;Clustering algorithms;Testing;Algorithm design and analysis;Acceleration;Parallel algorithms;Optimization methods;Reliability engineering","evolutionary computation;grid computing;optimisation;parallel algorithms;software reliability","global optimization algorithm;mind evolutionary computation;parallel reliable computational model;evolutionary algorithm;interval arithmetic;parallel interval decomposition scheme;computation intensive problem;Shanghai High Education Grid-e-Grid Computational Application Platform;multimodal function optimization benchmarks;ZiQiang 3000 cluster","","2","8","","","","","","IEEE","IEEE Conferences"
"An efficient implementation of RSA for low cost microprocessors","A. Talal; M. A. Sobh; A. M. B. Eldin","Softlock Company; Computer and Systems, Engineering Dept. Ain Shams University; Computer and Systems Engineering Dept. Ain Shams University","2009 4th International Design and Test Workshop (IDT)","","2009","","","1","4","This paper describes an efficient implementation of RSA cryptographic operations that can be used through a wide range of processors starting from 8 bit low cost microprocessor up to 64 bit expensive processor. The proposed implementation method reduces the memory usage, and uses generic C code that can be used with minor modification in different word length processors. The final implementation is optimized to achieve faster execution over specific target processors. Various optimization techniques are studied, like code optimization, algorithm alternation and FPGA acceleration.","2162-0601;2162-061X","978-1-4244-5750-2978-1-4244-5748-9978-1-4244-5750","10.1109/IDT.2009.5404094","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5404094","RSA;Cryptography;Optimization;Profiling","Costs;Microprocessors;Public key cryptography;Testing;Elliptic curve cryptography;Security;Cryptographic protocols;Random number generation;Energy consumption;Application software","microprocessor chips;public key cryptography","low cost microprocessors;RSA cryptographic operations;generic C code;word length processors;optimization techniques;FPGA acceleration;code optimization;algorithm alternation","","","17","","","","","","IEEE","IEEE Conferences"
"Software efficient implementation of GMSK modem for an automatic identification system transceiver","M. G. Souissi; K. Grati; A. Ghazel; A. Kouki","Ecole supérieure de communication de Tunis, Tunisia; Ecole supérieure de communication de Tunis, Tunisia; Ecole supérieure de communication de Tunis, Tunisia; École de technologie supérieure de Montréal, Canada","2008 Canadian Conference on Electrical and Computer Engineering","","2008","","","000601","000606","A software implementation of a GMSK modem for maritime safety applications is proposed. The implementation targets a low cost 16 bit fixed point DSP architecture. Alternative architectures for the modulator and demodulator have been investigated and an optimal I/Q-based configuration was chosen. Several optimizations have been carried out in order to respect the limitations imposed by the fixed point DSP platform while satisfying the real time operation requirement. A hardware implementation is presented and validation tests show that the proposed software modem meets the system specifications in terms of communication data rates.","0840-7789","978-1-4244-1642-4978-1-4244-1643","10.1109/CCECE.2008.4564605","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4564605","AIS;DSP;optimized implementation;software solutions;embedded systems","Modems;Transceivers;Software safety;Digital signal processing;Computer architecture;Application software;Costs;Demodulation;Hardware;Software testing","marine communication;minimum shift keying;transceivers","software efficient implementation;GMSK modem;automatic identification system transceiver;maritime safety applications;fixed point DSP architecture;optimal I-Q-based configuration;communication data rates","","1","13","","","","","","IEEE","IEEE Conferences"
"Automotive electronics system, software, and local area network","S. Kawamura; Y. Furukawa","AutoNetworks Technologies, Ltd., Yokkaichi, Japan; Shibaura Institute of Technology, Saitama, Japan","Proceedings of the 4th International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS '06)","","2006","","","2","2","In this tutorial, an overview of automotive electronic systems and details of the development methodologies are presented. Automobiles were born to enhance human mobile performance. In early development stage, automotive engineers focused to strengthen automobile engine power. Afterwards, automobiles had enough function to drive faster than any animal, but they caused some social problems such as traffic accidents, environmental problems and traffic congestions. Automotive electronic technologies have been developed in order to solve these social problems. Roles of electronic technologies on automobile functional developments For the solution to safely, environment and traffic problems, various functions are necessary which could not be completed only by mechanical systems. In this section, roles of automobile electronic systems on countermeasures to the social problems are discussed. Vehicle motion control systems, power-train control systems, navigation systems, and advanced drive assist systems are introduced and automotive functions are defined. Design requirement for automotive electronic systems architecture Electronic systems composed basically of sensors, ECU's (Electronic Control Units), actuators and human interfaces. In early days, each electronic system was designed independently. Today's automobile has various functions which could be completed by multiple electric systems. Therefore, fundamental architecture of integrated electronic systems in an automobile is important to be designed in order to optimize the total function, cost and productivity. Design and development procedure of electronic systems and software In vehicle systems and software, required functions and complexity of products are increasing. In this situation, ECU suppliers are working with efficient development methodology to achieve the highest quality. Today most common development processes are still classical V shaped process, module design and C language programming. However, several new technologies such as UML design method are tried and some of them are adopted as the standard process. Automatic testing and simulation environment are also important for the development procedure. Automotive Local Area Network Recently more and more automotive equipment are controlled electronically and the number of ECU's is increasing. The number of wire harnesses is also increasing and many problems such as the increase of weight, lack of installation space and difficulty of handling are experienced. As the solution of these problems, multiplexing with automotive Local Area Network is important to secure high speed communication as well as to decrease the weight and volume of wire harnesses. We will review technologies of automotive Local Area Network from CAN and LIN, which are currently de facto standards, to FlexRay that is about to start being adopted.","","1-59593-370","10.1145/1176254.1176256","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4278480","automotive electronics","Automotive electronics;Software systems;Local area networks;Automobiles;Automotive engineering;Humans;Motion control;Control systems;Wire;Space technology","automotive electronics;engines;local area networks;motion control;velocity control","automotive electronic system;automotive electronic software;automotive local area network;automobile engine power;social problems;mechanical systems;vehicle motion control systems;power train control systems;navigation systems;advanced drive assist systems;electronic control units;human interfaces;C language programming;UML design method;wire harnesses","","","","","","","","","IEEE","IEEE Conferences"
"Optimized Design of Injection Mould for Mobile Phone Front Shell Based on CAE Technology","G. Huang; X. Li; X. Wu; J. Li","NA; NA; NA; NA","2009 International Joint Conference on Artificial Intelligence","","2009","","","648","650","There is a lot of limitations in the traditional process of mould design and manufacturing. With the development of the science and technology, especially in the field of the computer, CAE technology begins to be applied widely in the process of modern mould design and manufacturing. The results of CAE simulation analysis of injection molding can provide reliable and optimized reference data for mould design and manufacturing. Applying CAE simulation analysis technology of injection molding can not only increase the probability of success in mould test but also improve greatly the quality of mould design and manufacturing. In this paper, the injection molding process of mobile phone front shell is analyzed. The best position of gate is discussed by using the Moldflow software. The optimized design scheme of feed system is determined in view of the particular structure of the plastic part. Then the simulation flow analysis of injection molding is carried into execution. On the basis of CAE analysis of injection molding, the whole injection mould structure is designed and the working procedure of injection mould is stated.","","978-0-7695-3615","10.1109/JCAI.2009.218","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5159086","optimized design;injection mould;mobile phone front shell;CAE","Design optimization;Mobile handsets;Computer aided engineering;Injection molding;Analytical models;Manufacturing processes;Computer aided manufacturing;Computational modeling;Virtual manufacturing;Process design","CAD;computer aided engineering;digital simulation;injection moulding;mobile handsets;production engineering computing","optimized design;mobile phone front shell;mould manufacturing;CAE simulation analysis;injection molding;Moldflow software;feed system","","1","10","","","","","","IEEE","IEEE Conferences"
"Estimation of control logic for binary synthesis","M. Sangeetha; J. R. P. Perinbam; M. Kumaran","Research Scholar, Department of Electronics and Communication Engineering, College of Engineering, Guindy, Anna University; Professor, Department of Electronics and Communication Engineering, College of Engineering, Guindy, Anna University; Assistant Professor, Department of Computer Science & Engineering, Jaya Engineering College, Chennai","2008 International Conference on Signal Processing, Communications and Networking","","2008","","","454","457","The behavioral description is transformed into binary level representation. The assembly instructions are profiled by providing test vectors. The critical kernel of the profiled data is identified manually and partitioned into hardware. The partitioned software module is transformed into control data flow graph. The control logic of the control data flow graph for the software module partitioned to hardware can be estimated using behavioral network graph.","","978-1-4244-1923-4978-1-4244-1924","10.1109/ICSCN.2008.4447237","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4447237","","Logic;Hardware;Software performance;Educational institutions;Kernel;Optimizing compilers;Assembly;Testing;Flow graphs;Embedded system","data flow graphs;hardware-software codesign;instruction sets;logic partitioning","binary synthesis;assembly instructions;control logic estimation;binary level representation;test vectors;partitioned software module;control data flow graph;CAD tool","","1","7","","","","","","IEEE","IEEE Conferences"
"A Comparison and the Desireable Features of Version Control Tools","S. Candrlic; M. Pavlic; P. Poscic","University of Rijeka, Faculty of Arts and Sciences, Omladinska 14, Rijeka, Croatia. sanjac@ffri.hr; University of Rijeka, Faculty of Arts and Sciences, Omladinska 14, Rijeka, Croatia. mile.pavlic@ris.hr; University of Rijeka, Faculty of Arts and Sciences, Omladinska 14, Rijeka, Croatia. patrizia@ffri.hr","2007 29th International Conference on Information Technology Interfaces","","2007","","","121","126","The software development team is regularly faced with two major problems: access to previous software versions and simultaneous work of many programmers on the same source code at the same time. Some development teams solve these problems by using version control tools, while others opt for manual version control. This paper analyzes both approaches and compares them according to certain criteria.","1330-1012","953-7138-09-7953-7138-10","10.1109/ITI.2007.4283756","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4283756","version control;source code;team software development;version merging","Programming profession;Virtual reality;Software maintenance;Spatial databases;Embedded software;Software testing;Application software;Art;Optimized production technology;Error correction","configuration management;software development management","version control tool;software development team;software version;source code","","1","10","","","","","","IEEE","IEEE Conferences"
"The EMOO repository: a resource for doing research in evolutionary multiobjective optimization","C. A. Coello Coello","CINVESTAV-IPN, Mexico","IEEE Computational Intelligence Magazine","","2006","1","1","37","45","This article briefly describes the evolutionary multi-objective optimization (EMOO) repository, which has become much more than the simple list of bibliographic references that originated it. In its current state, the EMOO repository contains many Web resources, including Ph.D. theses, software, contact information of EMOO researchers and information about EMOO-related events. Such information has become a valuable source for students and researchers interested in this area.","1556-603X;1556-6048","","10.1109/MCI.2006.1597060","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1597060","","Mirrors;Web pages;Writing;Time factors;Europe;Uniform resource locators;Crystallization;Employment;Web page design;Software testing","evolutionary computation;optimisation;information systems;research and development;Web sites","EMOO repository;evolutionary multiobjective optimization;Web resources","","5","8","","","","","","IEEE","IEEE Journals & Magazines"
"Using Virtualization to Improve Software Rejuvenation","L. M. Silva; J. Alonso; P. Silva; J. Torres; A. Andrzejak","Univ. of Coimbra, Portugal; BSC-UPC, Barcelona, Spain; Univ. of Coimbra, Portugal; BSC-UPC, Barcelona, Spain; ZIB, Germany","Sixth IEEE International Symposium on Network Computing and Applications (NCA 2007)","","2007","","","33","44","In this paper, we present an approach for software rejuvenation based on automated self-healing techniques that can be easily applied to off-the-shelf Application Servers and Internet sites. Software aging and transient failures are detected through continuous monitoring of system data and performability metrics of the application server. If some anomalous behavior is identified the system triggers an automatic rejuvenation action. This self-healing scheme is meant to be the less disruptive as possible for the running service and to get a zero downtime for most of the cases. In our scheme, we exploit the usage of virtualization to optimize the self-recovery actions. The techniques described in this paper have been tested with a set of open-source Linux tools and the XEN virtualization middleware. We conducted an experimental study with two applications benchmarks (Tomcat/Axis and TPC-W). Our results demonstrate that virtualization can be extremely helpful for software rejuvenation and fail-over in the occurrence of transient application failures and software aging.","","0-7695-2922","10.1109/NCA.2007.53","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4276604","","Application software;Open source software;Web server;Aging;Internet;Software performance;Condition monitoring;Testing;Linux;Middleware","Internet;Linux;middleware;software maintenance","software rejuvenation;automated self-healing techniques;off-the-shelf application server;Internet sites;software aging;performability metrics;zero downtime;open-source Linux tools;XEN virtualization middleware;transient application failure","","12","43","","","","","","IEEE","IEEE Conferences"
"An Optimized Image Retrieval Method Based on Hierarchal Clustering and Genetic Algorithm","H. Min; S. Bo; X. Jianqing","NA; NA; NA","2009 International Forum on Information Technology and Applications","","2009","1","","747","749","Image search on Web is very familiar to various users, and improving the efficiency and accuracy of image search has become more and more a hotpot in this research field. For different commercial image engines use different retrieval techniques respectively, the coverage area and accuracy of each individual search engine await development. An improved method based on multi-optimization techniques of image retrieval is presented in the paper. On the base of relevance feed-back principle, the method does some work of the vectorization and weights adjusting to the images generated by commercial image engines, and then adopts hierarchal clustering and genetic algorithm techniques to optimize the results further. Finally, by developing a prototype of image retrieval engine based on the method presented and doing some tests, the advancing in accuracy of image retrievals of the method has been proved.","","978-0-7695-3600","10.1109/IFITA.2009.429","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5231764","Image retrieval;Search engine;Multi-Optimization;Relevance feed-back;Hierarchal clustering;Genetic Algorithm","Optimization methods;Image retrieval;Genetic algorithms;Search engines;Image databases;Information retrieval;Information technology;Application software;Image generation;Gaussian processes","genetic algorithms;image retrieval;Internet;search engines","optimized image retrieval method;hierarchal clustering;genetic algorithm;image search;Web;commercial image engines;multioptimization techniques;relevance feedback principle","","1","8","","","","","","IEEE","IEEE Conferences"
"Extending the JColibri Open Source Architecture for Managing High-Dimensional Data and Large Case Bases","A. Bottrighi; G. Leonardi; S. Montani; L. Portinale","NA; NA; NA; NA","2009 21st IEEE International Conference on Tools with Artificial Intelligence","","2009","","","269","275","CBR systems designers and developers' research can benefit from the availability of existing platforms, able to provide software design and implementation assistance. The JColibri platform, realized and maintained by the University of Madrid, is one of the most well known among such tools. In this work, we describe a couple of extensions we have provided to the core JColibri open source software. In particular, our extensions are meant to optimize case retrieval performances, in data-rich applications. Specifically, we focused our attention on treating (i) large case bases, in which retrieval time may become unacceptable, and (ii) cases with high-dimensional features - namely time series features - on which proper case representation and retrieval solutions need to be studied. The implemented code has been preliminarly tested, and it is now ready to be integrated with the JColibri code, and made available to the CBR research community. Additional extensions, always dealing with retrieval optimization, are foreseen as our future work.","1082-3409;2375-0197","978-1-4244-5619-2978-0-7695-3920","10.1109/ICTAI.2009.57","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5364971","Case-based reasoning;Jcolibri framework;Time series;Pivoting-based retrieval;high-dimensional data;large case bases","Artificial intelligence;Computer architecture;Open source software;Testing;Conference management;Software development management;Software design;Information retrieval;Application software;Design optimization","information retrieval;public domain software;software architecture","JColibri open source architecture;high-dimensional data management;large case bases;CBR systems;software design;JColibri platform;case retrieval performances","","","32","","","","","","IEEE","IEEE Conferences"
"An Ant Colony Optimization Algorithm for Solving the Multidimensional Knapsack Problems","J. Ji; Z. Huang; C. Liu; X. Liu; N. Zhong","NA; NA; NA; NA; NA","2007 IEEE/WIC/ACM International Conference on Intelligent Agent Technology (IAT'07)","","2007","","","10","16","Ant colony optimization (ACO) algorithm is a metaheuristic and stochastic search technology, which has been one of the effective tools for solving discrete optimization problems. However, there are two bottlenecks for large-scaled optimization problems: the ACO algorithm needs too much time to convergent and the solutions may not be really optimal. This paper proposes a novel ACO algorithm for the multidimensional knapsack problems (MKP), which employs a new pheromone diffusion model and a mutation scheme. First, in light of the preference to better solutions, the association distances among objects are mined in each iteration with top-k strategy. Then, a pheromone diffusion model based on info fountain of an object is established, which strengthens the collaborations among ants. Finally, an unique mutation scheme is applied to optimizing the evolution results of each step. The experimental results for the benchmark testing set of MKPs show that the proposed algorithm can not only get much more optimal solutions but also greatly enhance convergence speed.","","0-7695-3027-3978-0-7695-3027","10.1109/IAT.2007.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4407250","","Ant colony optimization;Multidimensional systems;Genetic mutations;Intelligent agent;Software algorithms;Benchmark testing;Educational institutions;Computer science;Laboratories;Software tools","knapsack problems;optimisation;search problems","ant colony optimization algorithm;multidimensional knapsack problems;metaheuristic;stochastic search technology;discrete optimization problems;pheromone diffusion model;mutation scheme","","5","14","","","","","","IEEE","IEEE Conferences"
"Control unit for a laboratory motor test bench for monitoring and controlling PMSM and induction motors","M. Ganchev","Arsenal Research, Giefinggasse 2, A-1210 Vienna, Austria","2007 European Conference on Power Electronics and Applications","","2007","","","1","8","The work presents a state-of-art control unit for pulse width modulated inverter-fed AC motors. The unit is suitable for a wide range of voltage source inverters with frequencies ranging from 1 KHz to up to 100 KHz. The hardware is characterized by a powerful floating point DSP, FPGA unit, asynchronous serial and IEEE 1394 communication interface, and 12 channels Analog/Digital interface with sample and conversion times together equaling 250 ns. The control software is managed by a specially designed real-time multitasking operating system. The operating system guarantees less than 300 ns time duration when jumping from one task to another upon internal or external event. The operating system can be adapted easily for arbitrary number of tasks with various prioritization levels and triggering events, and therefore suitable for interfacing hardware in the loop (HIL) simulation environments. The on-line interaction between the user and the running control software is implemented by a specially designed IEEE1394 driver for Windows XP and a graphical user interface (GUI). This allows graphical and numerical monitoring of software variables and their modification at will.","","978-92-75815-10","10.1109/EPE.2007.4417431","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4417431","Test bench;Real time processing;Real time simulation;Software;System Integration;Electrical Drive;DSP;Control of Drive;Fault Handling Strategy","Induction motors;Laboratories;Testing;Monitoring;Pulse width modulation inverters;Operating systems;AC motors;Communication system control;Hardware;Graphical user interfaces","computerised monitoring;control engineering computing;graphical user interfaces;induction motors;machine control;operating systems (computers);permanent magnet motors;power engineering computing;synchronous motors;testing","laboratory motor test bench;permanent magnet synchronous motor monitoring;permanent magnet synchronous motor control;induction motor;pulse width modulated inverter-fed AC motor;voltage source inverter;floating point DSP;FPGA unit;asynchronous serial communication;IEEE 1394 communication interface;analog-digital interface;control software;real-time multitasking operating system;hardware-in-the-loop simulation;Windows XP;graphical user interface;numerical monitoring;fault handling strategy;electrical drive;system integration;real time simulation;frequency 1 kHz to 100 kHz","","2","13","","","","","","IEEE","IEEE Conferences"
"Using MPE with Bayesian Network for Sub-optimization to Entropy-Based Methodology","B. Kuo; T. Hsieh; H. Wang","NA; NA; NA","2008 Eighth International Conference on Intelligent Systems Design and Applications","","2008","1","","381","386","Many researchers show that the properties of Bayesian network and information theory such as entropy between dichotomous concepts and test items generalize some common intuitions about item comparison, and provide principled foundational to design item-selection heuristics for adaptive testing in computer-assisted educational systems. But entropy-based heuristic methodology could be too time-consuming as interesting variables with high dimensions to perform in practical situations. Hence the goal of this paper is trying to modify entropy-based heuristic methodology as a new form using most probable explanation (MPE) with Bayesian network to overcome this problem and to hold considerably performance for constructing decision items tree for adaptive testing in computer-assisted educational systems. Experiment results show that the proposed new methodology, named MPE-entropy-based heuristic methodology, can reduce the time-complexity and lose little performance.","2164-7143;2164-7151","978-0-7695-3382","10.1109/ISDA.2008.108","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4696236","Bayesian network;Most Probable Explanation;Entropy","Bayesian methods;Computer bugs;Computer networks;System testing;Application software;Context modeling;Intelligent systems;Intelligent networks;Statistical analysis;Information theory","belief networks;decision trees;educational administrative data processing;entropy;explanation;optimisation;probability","MPE;Bayesian network;suboptimization;information theory;item-selection heuristics;item comparison;adaptive testing;computer-assisted educational system;entropy-based heuristic methodology;most probable explanation;decision item tree;time complexity;probability","","","14","","","","","","IEEE","IEEE Conferences"
"A Novel Clonal Selection Algorithm for Global Optimization Problems","X. Liu; L. Shi; R. Chen; H. Chen","NA; NA; NA; NA","2009 International Conference on Information Engineering and Computer Science","","2009","","","1","4","In order to increase the diversity of immune algorithm when solving high-dimensional global optimization problems, a novel clonal selection algorithm with randomized clonal expansion strategy(RCSA) is proposed. The main characteristic of RCSA is clonal expansion. In addition, a novel performance evaluation criterion is constructed in this paper, by which the performance of different population-based algorithms can be compared easily. In the experimental study, firstly we obtain an appropriate value of the ratio of clonal expansion through some traditional test functions. Next several conventional clonal selection algorithms are used to validate the performance of proposed RCSA. The experimental results of the RCSA are significantly better than that of the conventional CSAs.","2156-7379;2156-7387","978-1-4244-4994","10.1109/ICIECS.2009.5363636","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5363636","","Cloning;Genetic mutations;Machine learning algorithms;Artificial immune systems;Ant colony optimization;Immune system;Educational technology;Business;Testing;Application software","artificial immune systems","clonal selection algorithm;global optimization problem;immune algorithm;high-dimensional global optimization;randomized clonal expansion strategy;performance evaluation criterion;population based algorithm","","1","11","","","","","","IEEE","IEEE Conferences"
"Development and its Application of A Digital Design Software Based on Discrete Element Method","Y. Jian-qun; F. Hong; S. Yu-jing","School of Biological and Agricultural Engineering, Jilin University, 5988 Renmin Avenue, Changchun, 130025, China; College of Computer Science and Technology, Jilin University, 2699 Qianjin Avenue, Changchun, 130012, China; School of Biological and Agricultural Engineering, Jilin University, 5988 Renmin Avenue, Changchun, 130025, China","2008 15th International Conference on Mechatronics and Machine Vision in Practice","","2008","","","216","220","Mechanical contact among and between particles of granular material flow and machine operating parts exist extensively in industrial and agriculture processing. In order to simulate and analyze the performance of granular material processing machine with complex structures and combined motion, a novel method of the discrete element method based on the CAD model of machine parts (DEMBCAD) is proposed. Based on DEMBCAD, an integrated design and analysis software (LSY) is developed, which includes an embodied module in the CAD environment for building a machine model, and a DEM analysis platform. The architecture and function of the software are introduced in this paper. The DEM boundary elements of operating parts are constructed in CAD environment. Some design examples of DEMBCAD in agricultural engineering are presented to test the feasibility and performance of the LSY software. DEMBCAD provides a new method for developing new products, and the information from DEMBCAD simulations is helpful for design and parameter optimization of the granular material processing machines.","","978-1-4244-3779-5978-0-473-13532","10.1109/MMVIP.2008.4749536","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4749536","Digital design;optimal design;discrete element method;granular materials;CAD/CAE","Application software;Software design;Design automation;Materials processing;Agriculture;Analytical models;Performance analysis;Motion analysis;Buildings;Agricultural engineering","agricultural engineering;CAD;granular materials;mechanical contact;mechanical engineering computing","digital design software;discrete element method;mechanical contact;granular material flow;industrial processing;granular material processing machine;CAD model;DEMBCAD;integrated design software;analysis software;LSY;agricultural engineering;parameter optimization","","1","10","","","","","","IEEE","IEEE Conferences"
"Formal Study of Prioritized Service Compositions","C. Andres; G. Diaz; E. Martinez; Y. Zhang","NA; NA; NA; NA","2009 Fifth International Conference on Signal Image Technology and Internet Based Systems","","2009","","","355","362","This paper presents an enhanced derivation procedure to obtain a system of services, from a given choreography. In addition to the basic framework, we introduce several situations where nondeterminism appears and it is resolved by using a dynamic prioritized system. The priority policy is based on several parameters such as the request dispatching, the response time, the quality of the response, etc. These parameters are identified as resources used by a utility function, which determines the priority of each possible option in a nondeterministic choice.","","978-1-4244-5741-0978-1-4244-5740-3978-0-7695-3959","10.1109/SITIS.2009.63","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5633969","Testing;formal methods","Web services;Software reliability;Security;Software systems;Proposals","distributed processing;formal verification","prioritized service composition;nondeterminism;dynamic prioritized system;priority policy;utility function;formal method","","","8","","","","","","IEEE","IEEE Conferences"
"Theoretical and Empirical Analysis of a GPU Based Parallel Bayesian Optimization Algorithm","A. Munawar; M. Wahib; M. Munetomo; K. Akama","NA; NA; NA; NA","2009 International Conference on Parallel and Distributed Computing, Applications and Technologies","","2009","","","457","462","General purpose computing over graphical processing units (GPGPUs) is a huge shift of paradigm in parallel computing that promises a dramatic increase in performance. But GPGPUs also bring an unprecedented level of complexity in algorithmic design and software development. In this paper we describe the challenges and design choices involved in parallelization of Bayesian optimization algorithm (BOA) to solve complex combinatorial optimization problems over nVidia commodity graphics hardware using compute unified device architecture (CUDA). BOA is a well-known multivariate estimation of distribution algorithm (EDA) that incorporates methods for learning Bayesian network (BN). It then uses BN to sample new promising solutions. Our implementation is fully compatible with modern commodity GPUs and therefore we call it gBOA (BOA on GPU). In the results section, we show several numerical tests and performance measurements obtained by running gBOA over an nVidia Tesla C1060 GPU. We show that in the best case we can obtain a speedup of up to 13x.","2379-5352","978-0-7695-3914","10.1109/PDCAT.2009.32","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5372763","General Purpose computing over GPU (GPGPU);Estimation of Distribution Algorithms (EDAs)","Algorithm design and analysis;Bayesian methods;Concurrent computing;Design optimization;Parallel processing;Software algorithms;Software design;Programming;Graphics;Hardware","Bayes methods;combinatorial mathematics;computer graphic equipment;estimation theory;optimisation;parallel algorithms;parallel architectures","parallel Bayesian optimization algorithm;general purpose computing over graphical processing units;parallel computing;GPGPU;algorithmic design;software development;complex combinatorial optimization;nVidia commodity graphics hardware;compute unified device architecture;multivariate estimation;distribution algorithm;Bayesian network;commodity GPU;gBOA;nVidia Tesla C1060 GPU","","3","10","","","","","","IEEE","IEEE Conferences"
"Integration of Expert System and Integer Programming for Optimization of Strategic Planning","M. A. Azadeh; S. Sharifi; H. Izadbakhsh","Department of Industrial Engineering and Research Institute of Energy Management and Planning, Faculty of Engineering, University of Tehran, P.O. Box 11365-4563Iran. aazadeh@ut.ac.ir; ali@azadeh.com; Department of Industrial Engineering and Research Institute of Energy Management and Planning, Faculty of Engineering, University of Tehran, P.O. Box 11365-4563Iran; Department of Industrial Engineering and Research Institute of Energy Management and Planning, Faculty of Engineering, University of Tehran, P.O. Box 11365-4563Iran. izadbakhsh@engmail.ut.ac.ir","2006 4th IEEE International Conference on Industrial Informatics","","2006","","","966","973","This paper introduces a framework for design and optimization of strategic planning by integration of human-centered expert system and integer programming (IP). The development process of the expert system is facilitated through knowledge base, learning module, inference engine, description tools and usability testing and inspection. To achieve the above objective, all indigenous and exogenous factors affecting strategic planning are categorized and translated into rules in the expert system. Moreover, general strategies and their decision circumstances or criteria are given to the system The human-centered expert system is designed and tested for a medium-sized manufacturer of construction products. The IP model is introduced to optimize the recommended strategies among business units of large organization. The integration of expert system and IP for optimization of strategic planning is unique and new and has several unique advantages discussed in this paper. The approach of this paper was designed and tested for a medium-sized manufacturer of construction products and presents the advantages of integrated IP and expert system.","1935-4576;2378-363X","0-7803-9701-00-7803-9700","10.1109/INDIN.2006.275728","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4053520","Optimization;Strategic Planning;Expert System;Integer Programming","Expert systems;Linear programming;Strategic planning;Power system management;Risk management;Waste management;System testing;Medical expert systems;Usability;Information analysis","expert systems;inference mechanisms;integer programming;learning (artificial intelligence);program testing;small-to-medium enterprises;software reusability;strategic planning","integer programming;optimization;strategic planning;human-centered expert system;knowledge base;learning module;inference engine;description tools;usability testing;medium-sized manufacturer;construction products","","","115","","","","","","IEEE","IEEE Conferences"
"Use of the WFG Toolkit and PISA for Comparison of MOEAs","L. Bradstreet; L. Barone; L. While; S. Huband; P. Hingston","School of Computer Science & Software Engineering, The University of Western Australia, 35 Stirling Highway, Crawley, Australia, 6009, Email: lucas@csse.uwa.edu.au; School of Computer Science & Software Engineering, The University of Western Australia, 35 Stirling Highway, Crawley, Australia, 6009, Email: luigi@csse.uwa.edu.au; School of Computer Science & Software Engineering, The University of Western Australia, 35 Stirling Highway, Crawley, Australia, 6009, Email: lyndon@csse.uwa.edu.au; School of Computer Science & Software Engineering, The University of Western Australia, 35 Stirling Highway, Crawley, Australia, 6009, Email: huey@csse.uwa.edu.au; School of Computer & Information Science, Edith Cowan University, 2 Bradford Street, Mount Lawley, Australia, 6050, Email: p.hingston@ecu.edu.au","2007 IEEE Symposium on Computational Intelligence in Multi-Criteria Decision-Making","","2007","","","382","389","Understanding the behaviour of different optimisation algorithms is important in order to apply the best algorithm to a particular problem. The WFG toolkit was designed to aid this task for multi-objective evolutionary algorithms (MOEAs), offering an easily modifiable framework that allows practitioners the ability to test different features by ""plugging"" in different forms of transformations. In doing so, the WFG toolkit provides a set of problems that exhibit a variety of different characteristics. This paper presents a comparison between two state of the art MOEAs (NSGA-II and SPEA2) that exemplifies the unique capabilities of the WFG toolkit. By altering the control parameters or even the transformations that compose the WFG problems, we are able to explore the different types of problems where SPEA2 and NSGA-II each excel. Our results show that the performance of the two algorithms differ not only on the dimensionality of the problem, but also by properties such as the shape and size of the underlying Pareto surface. As such, the tunability of the WFG toolkit is key in allowing the easy exploration of these different features.","","1-4244-0702","10.1109/MCDM.2007.369117","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4223032","","Testing;Australia;Software algorithms;Evolutionary computation;Shape;Computational intelligence;Decision making;Computer science;Software engineering;Road transportation","evolutionary computation;Pareto optimisation","Walking Fish Group toolkit;PISA;optimisation algorithms;multiobjective evolutionary algorithms;NSGA-II;SPEA2;Pareto surface","","7","16","","","","","","IEEE","IEEE Conferences"
"Hybridization of particle swarm optimization with the K-Means algorithm for image classification","C. Hung; L. Wan","School of Computing and Software Engineering, Southern Polytechnic State University, Marietta, GA 30060 USA; School of Computing and Software Engineering, Southern Polytechnic State University, Marietta, GA 30060 USA","2009 IEEE Symposium on Computational Intelligence for Image Processing","","2009","","","60","64","The K-means algorithm is one of the widely used clustering algorithms in the image classification systems. However, the K-Means algorithm is easily trapped into the local optimal solutions. Several optimization techniques have been proposed to solve this problem such as genetic algorithms, simulated annealing and swarm intelligence. In this paper, we develop hybrid techniques using different particle swarm optimization (PSO) heuristics to optimize the k-means algorithm and examine the reliability of parametric values for different variants of PSO and k-means algorithms. These PSO heuristics include linear inertia reduction, constriction factor, and dynamic inertia and maximum velocity reduction. The performance of these hybridization of PSO and the k-means algorithms was tested on the image segmentation. These PSO heuristics can make the K-means algorithm more stable for finding better solutions and less dependent on the initial cluster centers based on the preliminary experimental results.","","978-1-4244-2760","10.1109/CIIP.2009.4937881","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4937881","","Particle swarm optimization;Classification algorithms;Image classification;Clustering algorithms;Genetic algorithms;Simulated annealing;Testing;Image segmentation;Helium;Stochastic processes","genetic algorithms;image classification;image segmentation;particle swarm optimisation;pattern clustering;simulated annealing","image classification systems;k-means algorithm;particle swarm optimization;clustering algorithms;genetic algorithms;simulated annealing;swarm intelligence;image segmentation","","2","26","","","","","","IEEE","IEEE Conferences"
"Applying a reusable framework for software selection","V. Maxville; J. Armarego; C. P. Lam","Edith Cowan University, Perth, WA, Australia; Murdoch University, Perth, WA, Australia; Edith Cowan University, Perth, WA, Australia","IET Software","","2009","3","5","369","380","With increasing use of component-based development (CBD), the process for selecting software from repositories is a critical concern for quality systems development. As support for developers blending in-house and third party software, the context-driven component evaluation (CdCE) process provides a three-phase approach to software selection: filtering to a short list, functional evaluation and ranking. The process was developed through iterative experimentation on real-world data. CdCE has tool support to generate classifier models, shortlists and test cases as artefacts that provide for a repeatable, transparent process that can be reused as the system evolves. Although developed for software component selection, the CdCE process framework can be easily modified for other selection tasks by substituting templates, tools, evaluation criteria and/or repositories. In this article the authors describe the CdCE process and its development, the CdCE framework as a reusable pattern for software selection and provide a case study where the process is applied.","1751-8806;1751-8814","","10.1049/iet-sen.2008.0096","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5273790","","","object-oriented programming;software quality;software reusability;software selection","software selection;component-based development;quality systems development;context-driven component evaluation;three-phase approach;iterative experimentation;reusable pattern framework","","","","","","","","","IET","IET Journals & Magazines"
"Efficient Software Architecture for IPSec Acceleration Using a Programmable Security Processor","J. Thoguluva; A. Raghunathan; S. T. Chakradhar","NEC Laboratories America, 4 Independence Way, Princeton, NJ 08540; Alphion Corporation; NEC Laboratories America, 4 Independence Way, Princeton, NJ 08540; NEC Laboratories America, 4 Independence Way, Princeton, NJ 08540","2008 Design, Automation and Test in Europe","","2008","","","1148","1153","Cryptographic accelerators and security processors are often used in embedded systems in order to enable enhanced security without significantly impacting performance or power consumption. However, realizing the performance promised by them requires the design of efficient software architectures for crypto offloading (offloading cryptographic operations from a host processor). In this paper, we describe an efficient software architecture for IPSec crypto offloading on a state-of-the-art mobile application processor system-on-chip (SoC) that includes a programmable security processor. We consider both user-space and kernel-space implementations of IPSec, compare their performance, and identify factors that limit the efficiency of crypto offloading. We describe two optimizations, called protocol-level crypto offloading and adaptive crypto offloading, which further improve the performance of IPSec by (i) offloading higher granularity computations to reduce the crypto offloading overheads, and (ii) using crypto offloading judiciously based on the trade-off between the savings in processing cycles vs. the overhead of communication with the security processor. We measure the performance of our implementation of IPSec crypto offloading using a commercial network protocol stack on the mobile application processor SoC, under a wide range of workloads. Our results indicate that efficient crypto offloading can result in application-level improvements of up to 10.6X in data rate and up to 5X in latency, enabling IPSec to be used for emerging high-bandwidth and interactive mobile applications.","1530-1591;1558-1101","978-3-9810801-3-1978-3-9810801-4","10.1109/DATE.2008.4484833","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4484833","","Software architecture;Acceleration;Cryptography;Power system security;Embedded system;Energy consumption;Software design;Application software;System-on-a-chip;High performance computing","cryptographic protocols;embedded systems;software architecture","software architecture;IPSec acceleration;programmable security processor;cryptographic accelerators;embedded systems;crypto offloading;mobile application processor system-on-chip;network protocol stack","","","20","","","","","","IEEE","IEEE Conferences"
"PANACEA Towards a Self-healing Development Framework","D. Breitgand; M. Goldstein; E. Henis; O. Shehory; Y. Weinsberg","Ibm Haifa Research Lab, Email: davidbr@il.ibm.com; IBM Haifa Research Lab, Email: maayang@il.ibm.com; IBM Haifa Research Lab, Email: ealan@il.ibm.com; IBM Haifa Research Lab, Email: onn@il.ibm.com; The Hebrew University Of Jerusalem, Email: wyaron@cs.huji.ac.il","2007 10th IFIP/IEEE International Symposium on Integrated Network Management","","2007","","","169","178","Self-healing capabilities allow software systems to overcome problems occurring during testing and run time, and thus improve overall system behavior. The PANACEA framework introduced in this paper provides a design methodology as well as ready-to-use healing elements aimed at enhancing software systems with self-healing capabilities both at design time and at run time. The PANACEA approach is based on inserting self- healing elements into the system at design and coding time, to be used later for healing at testing and run time. Specifically, the Panacea framework is based on inserting annotations into the system code at design and coding time, to later on serve as an interface for runtime monitoring, managing, configuring and healing of the annotated system components. The current embodiment of PANACEA includes several generic components that provide self-healing capabilities suited for a variety of application types. The PANACEA runtime environment automatically activates and invokes these components in order to optimize and heal the application. The PANACEA framework provides an innovative programming model that enables development of advanced self-healing applications. PANACEA introduces a paradigm shift in which software is made self-healing by design. This paradigm shift, however, is graceful since developers are not required to master neither new programming skills, nor languages. As our initial experiments demonstrate, PANACEA introduces a very small performance overhead, and scales well.","1573-0077","1-4244-0798-21-4244-0799","10.1109/INM.2007.374781","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4258533","","Web server;Humans;Monitoring;Automatic testing;System testing;Network servers;Databases;Application software;Software systems;Computer architecture","program testing;software engineering","PANACEA;self-healing development framework;software testing;runtime monitoring;innovative programming model","","10","24","","","","","","IEEE","IEEE Conferences"
"Autoreclosure in extra high voltage lines using Taguchi’s method and optimized neural networks","D. Z. F.; K. S. Rama Rao; T. M. Baloch","Universiti Teknologi PETRONAS, Department of Electrical and Electronics Engineering, 31750 Tronoh, Malaysia; Universiti Teknologi PETRONAS, Department of Electrical and Electronics Engineering, 31750 Tronoh, Malaysia; Universiti Teknologi PETRONAS, Department of Electrical and Electronics Engineering, 31750 Tronoh, Malaysia","2008 IEEE Canada Electric Power Conference","","2008","","","1","7","This paper presents a method to discriminate the temporary faults from the permanent ones in an extra high voltage (EHV) transmission line so that improper reclosing of the line onto a fault is avoided. The fault identification prior to reclosing is based on optimized artificial neural network associated with standard error back-propagation, Levenberg Marquardt algorithm and resilient back-propagation training algorithms together with Taguchipsilas method. The algorithms are developed using MATLABtrade software. A range of faults are simulated on EHV modeled transmission line using SimPowerSytemstrade, and the spectra of the fault data are analyzed using fast Fourier transform which facilitates extraction of distinct features of each type of fault. For both training and testing purposes, the neural network is fed with the normalized energies of the DC component, the fundamental and the first four harmonics of the faulted voltages. The developed algorithm is effectively trained, verified and validated with a set of training, dedicated testing and validation data respectively.","","978-1-4244-2894-6978-1-4244-2895","10.1109/EPC.2008.4763325","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4763325","Autoreclosure;transmission line faults;EHV transmission;artificial neural networks;Levenberg Marquardt algorithm;back-propagation algorithm;Taguchi’s method","Voltage;Optimization methods;Neural networks;Artificial neural networks;Transmission lines;Testing;Fault diagnosis;Software algorithms;Computer languages;Analytical models","backpropagation;fast Fourier transforms;mathematics computing;neural nets;optimisation;power engineering computing;power transmission faults;power transmission lines;Taguchi methods","extra high voltage line auto reclosure;Taguchi method;optimized artificial neural network;EHV transmission line fault;Levenberg Marquardt algorithm;resilient back-propagation training algorithm;MATLAB software;SimPowerSytems software;fast Fourier transform;feature extraction;DC component normalized energy;faulted voltage harmonics","","","22","","","","","","IEEE","IEEE Conferences"
"PerDaCol and PerfAnalysis - A Tool Set for Performance Measurement Data Collection and Evaluation of Real-Time Communication Systems","A. Erdmann; D. Weber","NA; NA","2009 Sixth International Conference on the Quantitative Evaluation of Systems","","2009","","","213","214","This contribution describes a tool set which has been implemented over the last years within Nokia Siemens Networks (NSN) in order to help analyzing and optimizing various product lines of many of today's NSN large-scale telecommunication systems platforms which are based on Unix-like software architecture. The paper and the tool presentation gives an overview of the general implementation concepts of the tool and outlines the advantages of the tool set by means of various examples from a practical point of view.","","978-0-7695-3808","10.1109/QEST.2009.46","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290834","performance measurement;IMS;data collection;evaluation;unix-like software architecture","Real time systems;System testing;Performance analysis;Phase measurement;Large-scale systems;Software architecture;Statistics;Hardware;Multimedia systems;Telecommunication traffic","software architecture;software performance evaluation","performance measurement data collection;real-time communication systems;Nokia Siemens Networks;large-scale telecommunication systems;Unix-like software architecture","","","4","","","","","","IEEE","IEEE Conferences"
"Design optimization and stamper fabrication of light guiding plates using silicon based micro-features","J. Yu; Pei-Kai Hsu","National Kaohsiung First University of Science and Technology, 2 Jhuoyue Rd., Nanzih, 811, Taiwan, R.O.C.; National Kaohsiung First University of Science and Technology, 2 Jhuoyue Rd., Nanzih, 811, Taiwan, R.O.C.","2009 Symposium on Design, Test, Integration & Packaging of MEMS/MOEMS","","2009","","","202","207","This study applies a novel fabrication process of molding stamper that combines anisotropic wet etching of silicon-on-insulator (SOI) wafers with electroforming to produce precision stampers. Micron-size features, such as trapezoidal grooves and prisms, can be accurately fabricated and distributed. Because the feature geometry and the distribution can be accurately realized using the proposed scheme, the design optimization of light guide plate (LGP) become realistic. By observing the illumination characteristics of LED edge-lit LGP, the distribution pattern of the LGP is transformed into the parameter design of 7 anchor spacing and the spacing modulation amplitude. This study manipulates the distribution parameters using the fuzzy optimization to obtain a LGP design with high illumination uniformity. The design of a LGP of 3.5 inch using LED light source is used as an illustrated example. The optical software, TracePro is applied to simulate the luminance performance. The optimization converges quickly and provides the optimum design with an average brightness of 2266 (nit) and uniformity of 90% without any use of diffusive sheets. The application demonstrates the feasibility and effectiveness of the proposed scheme.","","978-1-4244-3874","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4919539","","Design optimization;Fabrication;Lighting;Light emitting diodes;Anisotropic magnetoresistance;Wet etching;Silicon on insulator technology;Geometry;Amplitude modulation;Light sources","electroforming;etching;light emitting diodes;micromechanical devices;moulding;optimisation;silicon-on-insulator","design optimization;stamper fabrication;light guiding plates;silicon based microfeatures;molding stamper;anisotropic wet etching;silicon-on-insulator wafers;electroforming;trapezoidal grooves;prisms;LED edge-lit LGP;light emitting diodes;spacing modulation amplitude;fuzzy optimization;LED light source;optical software;TracePro;luminance performance;brightness;diffusive sheets;size 3.5 inch","","","9","","","","","","IEEE","IEEE Conferences"
"Optimizing Logarithmic Arithmetic on FPGAs","H. Fu; O. Mencer; W. Luk","Imperial College London, UK; Imperial College London, UK; Imperial College London, UK","15th Annual IEEE Symposium on Field-Programmable Custom Computing Machines (FCCM 2007)","","2007","","","163","172","This paper proposes optimizations of the methods and parameters used in both mathematical approximation and hardware design for logarithmic number system (LNS) arithmetic. First, we introduce a general polynomial approximation approach with an adaptive divide-in-halves segmentation method for evaluation of LNS arithmetic functions. Second, we develop a library generator that automatically generates optimized LNS arithmetic units with a wide bit-width range from 21 to 64 bits, to support LNS application development and design exploration. The basic arithmetic units are tested on practical FPGA boards as well as software simulation. When compared with existing LNS designs, our generated units provide in most cases 6% to 37% reduction in area and 20% to 50% reduction in latency. The key challenge for LNS remains on the application level. We show the performance of LNS versus floating-point for realistic applications: digital sine/cosine waveform generator, matrix multiplication and radiative Monte Carlo simulation. Our infrastructure for fast prototyping LNS FPGA applications allows us to efficiently study LNS number representation and its tradeoffs in speed and size when compared with floating-point designs.","","0-7695-2940-2978-0-7695-2940","10.1109/FCCM.2007.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4297253","","Arithmetic;Field programmable gate arrays;Application software;Design optimization;Hardware;Polynomials;Software libraries;Software testing;Delay;Signal generators","digital arithmetic;electronic engineering computing;field programmable gate arrays;logic design;optimisation;polynomial approximation","logarithmic arithmetic optimization;field programmable gate arrays;FPGA;mathematical approximation;hardware design;logarithmic arithmetic number system;polynomial approximation approach;adaptive divide-in-halves segmentation method;library generator","","5","19","","","","","","IEEE","IEEE Conferences"
"An Interprocedural Code Optimization Technique for Network Processors Using Hardware Multi-Threading Support","H. Scharwaechter; M. Hohenauer; R. Leupers; G. Ascheid; H. Meyr","Integrated Signal Processing Systems RWTH Aachen University Aachen, Germany; NA; NA; NA; NA","Proceedings of the Design Automation & Test in Europe Conference","","2006","1","","1","6","Sophisticated C compiler support for network processors (NPUs) is required to improve their usability and consequently, their acceptance in system design. Nonetheless, high-level code compilation always introduces overhead, regarding code size and performance compared to handwritten assembly code. This overhead result partially from high-level function calls that usually introduce memory accesses in order to save and reload registers contents. A key feature of many NPU architectures is hardware multithreading support, in the form of separate register files, for fast context switching between different application tasks. In this paper, a new NPU code optimization technique to use such HW contexts is presented that minimizes the overhead for saving and reloading register contents for function calls via the runtime stack. The feasibility and the performance gain of this technique are demonstrated for the Infineon Technologies PP32 NPU architecture and typical network application kernels","1530-1591;1558-1101","3-9810801-1","10.1109/DATE.2006.243808","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1657021","","Hardware;Registers;Yarn;Switches;Assembly;Application software;Computer architecture;High level languages;Costs;Delay","circuit optimisation;hardware-software codesign;logic design;multi-threading;operating system kernels;program compilers;system-on-chip","interprocedural code optimization;network processors;hardware multithreading support;C compiler;high-level code compilation;assembly code;memory access;register files;context switching;NPU code optimization;network application kernels","","1","13","","","","","","IEEE","IEEE Conferences"
"Detection of Bugs by Compiler Optimizer Using Macro Expansion of Functions","D. T. V. R. Rao","Infosys Technologies Ltd., India","5th ACIS International Conference on Software Engineering Research, Management & Applications (SERA 2007)","","2007","","","855","862","A new static analysis based approach is proposed to detect interface bugs in software. Unlike existing static analyses, which suggest new tools, this approach does not suggest a new tool, but leverages the optimizer which is part of the compiler already used by programmers. To facilitate the optimizer in detecting the interface bugs of a function, a macro is created which encodes the conditions to be checked for the function arguments. The approach is found to be effective when applied on two already well-tested commercial software systems, where it detected more than 50 bugs.","","978-0-7695-2867-00-7695-2867","10.1109/SERA.2007.78","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4297025","","Computer bugs;Optimizing compilers;Program processors;Programming profession;Assembly;Software engineering;Conference management;Engineering management;Application software;Cities and towns","program compilers;program debugging","bugs detection;compiler optimizer;functions macro expansion;static analysis based approach;interface bugs;commercial software systems","","","20","","","","","","IEEE","IEEE Conferences"
"Time Sensitive Ranking with Application to Publication Search","X. Li; B. Liu; P. Yu","NA; NA; NA","2008 Eighth IEEE International Conference on Data Mining","","2008","","","893","898","Link-based ranking has contributed significantly to the success of Web search. PageRank and HITS are the best known link-based ranking algorithms. These algorithms do not consider an important dimension, the temporal dimension. They favor older pages because these pages have many in-links accumulated over time. Bringing new and quality pages to the users is important because most users want the latest information. Existing remedies to PageRank are mostly heuristic approaches. This paper investigates the temporal aspect of ranking with application to publication search, and proposes a principled method based on the stationary probability distribution of the Markov chain. The proposed techniques are evaluated empirically using a large collection of high energy particle physics publication. The results show that the proposed methods are highly effective.","1550-4786;2374-8486","978-0-7695-3502","10.1109/ICDM.2008.155","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4781197","time sensitive;publication;ranking;search","Web pages;Web search;Probability distribution;Search engines;Data mining;Computer science;Testing;Software libraries;Performance evaluation;Social network services","information retrieval;Internet;Markov processes;probability;publishing;search engines","time sensitive ranking;publication search;link-based ranking;Web search;PageRank;HITS;stationary probability distribution;Markov chain","","5","15","","","","","","IEEE","IEEE Conferences"
"Cantag: an open source software toolkit for designing and deploying marker-based vision systems","A. C. Rice; A. R. Beresford; R. K. Harle","Comput. Lab., Cambridge Univ., UK; Comput. Lab., Cambridge Univ., UK; Comput. Lab., Cambridge Univ., UK","Fourth Annual IEEE International Conference on Pervasive Computing and Communications (PERCOM'06)","","2006","","","10 pp.","21","This paper presents Cantag, an open source software toolkit for building marker-based vision (MBV) systems that can identify and accurately locate printed markers in three dimensions. The extensibility of the system makes it ideal for dynamic location and poses determination in pervasive computing systems. Unlike prior MBV systems, Cantag supports multiple fiducial shapes, payload types, data sizes and image processing algorithms in one framework. It allows the application writer to generate a custom tag design and associated optimised executable for any given application. The system includes a test harness which can be used to quantify, compare and contrast the performance of different designs. This paper explores the design space of tags within the Cantag system, and describes the design parameters and performance characteristics which an application writer can use to select the best tag system for any given scenario. It presents quantitative analysis of different markers and processing algorithms, which are compared fairly for the first time","","0-7695-2518","10.1109/PERCOM.2006.13","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1604788","","Open source software;Software design;Machine vision;Space exploration;Buildings;Pervasive computing;Shape;Payloads;Image processing;Design optimization","augmented reality;computer vision;public domain software;software tools;ubiquitous computing","Cantag system;open source software toolkit;marker-based vision system;pervasive computing system;image processing;custom tag design;quantitative analysis","","12","17","","","","","","IEEE","IEEE Conferences"
"Research of Optimizing Device Description Technology Based on XML in EPA","S. Yuanzhong; T. Chenggong; L. Quan; P. Xiaohong","NA; NA; NA; NA","2009 Second International Symposium on Electronic Commerce and Security","","2009","1","","561","564","In order to enhance the interoperability of device in EPA industry network control system, the characteristics of XDDL, the application flow and functions of EPA device description were discussed. The latest methods of XML parsing and the utilization efficiency of the device description data after being parsed were analyzed. A kind of parsing method and a double buffers technology used for device description data application were proposed. To evaluate the optimizing solution, the parsing methods were tested and compared. The test results indicated that the processing efficiency of EPA data parsing and application was improved. On the basis, the applying prospect of the optimizing solutions in CIMS and field device system was discussed.","","978-0-7695-3643","10.1109/ISECS.2009.206","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5209876","Device Description;Interoperability;XML parsing;CIMS;EPA","XML;Industrial control;Electrical equipment industry;Control systems;Application software;Optimization methods;Testing;Computer integrated manufacturing;Manufacturing automation;Automatic control","automation;buffer storage;industrial control;local area networks;open systems;XML","device description technology;device interoperability;Ethernet for plant automation industry network control system;XDDL;application flow;XML parsing;double buffers technology;optimizing solution;CIMS;field device system","","","5","","","","","","IEEE","IEEE Conferences"
"JIRiSS - an Eclipse plug-in for Source Code Exploration","D. Poshyvanyk; A. Marcus; Yubo Dong","Wayne State University; NA; NA","14th IEEE International Conference on Program Comprehension (ICPC'06)","","2006","","","252","255","JIRiSS (information retrieval based software search for Java) is a software exploration tool that uses an indexing engine based on an information retrieval method. JIRiSS is implemented as a plug-in for Eclipse and it allows the user to search Java source code for the implementation of concepts formulated as natural language queries. The results of the query are presented as a ranked list of software methods or classes, ordered by the similarity to the user query. In addition to that, JIRiSS includes other advanced features like automatically generated software vocabulary, advanced query formulation options including spell-checking as well as fragment-based search","1092-8138","0-7695-2601","10.1109/ICPC.2006.32","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1631130","","Software tools;Information retrieval;Java;Indexing;Natural languages;Documentation;Search engines;Software systems;Computer science;Vocabulary","information retrieval;Java;program testing;software tools","JIRiSS;Eclipse;source code exploration;information retrieval method;software search;Java;software exploration tool;natural language queries;software vocabulary;query formulation;fragment-based search","","18","14","","","","","","IEEE","IEEE Conferences"
"GRANADA validation of optimized Multiple Gate Delay structures for Galileo SinBOC(1,1) signal tracking","X. Hu; E. S. Lohan","Institute of Communications Engineering, Tampere University of Technology, P.O.Box 553, FIN-33101, Finland; xuan.hu@tut.fi; Institute of Communications Engineering, Tampere University of Technology, P.O.Box 553, FIN-33101, Finland; lena-simona.lohan@tut.fi","2007 7th International Conference on ITS Telecommunications","","2007","","","1","5","Multipath is an issue of paramount importance in the GNSS context, and the dominant error source for the Delay Lock Loop (DLL) used for code tracking. This paper introduces an optimized Multiple Gate Delay (MGD) code tracking implementation in the modified GRANADA Bit-true Software Receiver Simulator. Then the tracking performance of the optimized MGD structure is presented via GRANADA simulation results. This tracking performance is also compared with the tracking performance of High Resolution Correlator (HRC) and of the narrow correlator or narrow Early-Minus-Late (nEML) structures. The tracking performance criteria include Multipath Error Envelopes (MEEs) and Root Mean Square Error (RMSE) for both multipath static channels and fading channels. It is shown that the optimized MGD structure has a better performance than nEML and HRC in the multipath channels, and that the GRANADA Bit-true Software Receiver Simulator is a useful tool for testing the performance.","","1-4244-1177-71-4244-1178","10.1109/ITST.2007.4295874","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4295874","","Delay;Tracking loops;Correlators;Satellite navigation systems;Root mean square;Fading;Multipath channels;Software performance;Software tools;Software testing","fading channels;multipath channels;satellite navigation;telecommunication computing","GRANADA validation;multiple gate delay structures;Galileo SinBOC signal tracking;GNSS;delay lock loop;code tracking;bit-true software receiver simulator;high resolution correlator;early-minus-late structures;multipath error envelopes;root mean square error;multipath static channels;fading channels","","2","16","","","","","","IEEE","IEEE Conferences"
"A Multi-Objective Portfolio Selection Formulation of Corporate Social Responsibility and Optimization Algorithms","Y. Qi; X. Peng; J. Liu","NA; NA; NA","2009 International Conference on Computational Intelligence and Software Engineering","","2009","","","1","4","There is a rapidly growing interest in corporate social responsibility (CSR), especially in the current global financial crisis. We briefly review the development of CSR and point out in China the research is quite frequently immature and heavily qualitative. To facilitate the development, we propose a quantitative way to measure CSR and formulate CSR in a multiple objective portfolio selection model as an extension of traditional Markowitz' portfolio selection model. Based on Chinese stocks by industries, we calculate the stocks' CSR measurements and perform model optimization by computing the efficient portfolios. Comparing with major indexes of Chinese stock markets as benchmarks by t-tests, some efficient portfolios outperform the indexes and therefore can justify our approach. Moreover, our methodology stems from established measurements of CSR and latest enrichments of programming methodology and finance and therefore can be applicable to CSR research worldwide. At the early stage of research by the intersection of programming methodology and finance, more important and inspiring questions are raised than answered. Consequently, as the frontier is being pushed, definitely being counted are more promising results.","","978-1-4244-4507","10.1109/CISE.2009.5364822","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5364822","","Portfolios;Finance;Investments;Financial management;Computer industry;Stock markets;Programming profession;Government;Europe;History","optimisation;stock markets","multi-objective portfolio selection formulation;corporate social responsibility;optimization algorithms;Markowitz portfolio selection model;Chinese stock markets","","","19","","","","","","IEEE","IEEE Conferences"
"An Optimized System for Multiple Sequence Alignment","C. Yilmaz; M. Gök","NA; NA","2009 International Conference on Reconfigurable Computing and FPGAs","","2009","","","178","182","Multiple sequence alignment (MSA) is one of the essential operations for identifying functional and structural relations among proteins. The execution of an MSA algorithm requires high-performance platforms. This paper presents a hardware system that speeds up the popular MSA software ClustalW. The proposed design performs the computation of the most time consuming step of the ClustalW. Test results show that the proposed hardware increases the performance of this step up to 85 times.","2325-6532","978-1-4244-5293-4978-0-7695-3917","10.1109/ReConFig.2009.82","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5382048","Multiple Sequence Alignment;ClustalW;Hardware","Sequences;Hardware;System testing;Phylogeny;Algorithm design and analysis;Heuristic algorithms;Field programmable gate arrays;Protein engineering;Design methodology;Engines","biology computing;field programmable gate arrays;molecular biophysics;molecular configurations;optimisation;proteins","optimized system;multiple sequence alignment;proteins;functional-structural relation;hardware system;MSA software ClustalW;FPGA implementations","","2","14","","","","","","IEEE","IEEE Conferences"
"Biogas production from banana stem waste: optimisation of 10 l sequencing batch reactor","N. Zainol; J. Salihon; R. Abdul-Rahman","Faculty of Chemical and Natural Resources Engineering, Universiti Malaysia Pahang, Locked Bag 12, 25000, Kuantan, Malaysia; Faculty of Chemical and Natural Resources Engineering, Universiti Malaysia Pahang, Locked Bag 12, 25000, Kuantan, Malaysia; Department of Chemical and Process Engineering, Faculty of Engineering, Universiti Kebangsaan Malaysia, 43600 UKM Bangi, Selangor, Malaysia","2008 IEEE International Conference on Sustainable Energy Technologies","","2008","","","357","359","The performance of biogas production using banana stem waste in anaerobic system was investigated. Mixed culture for this study was from banana plantation soil and acclimatized in anaerobic condition. The performance was tested under the conditions of various temperatures (26degC-40degC), organic loading rates (OLR) (0.4 g TS/l.d-2 gTS/l.d), and hydraulic retention times (HRT) (3 d-20 d). Conditions for temperature, OLR and HRT in this study was based on the best range obtained from literature review The optimization test was done using design-expert software version 6.0.4 was utilized in this research to design the experiments and optimize the system. It was found that for maximum biogas yield the levels of the variables are as follows: temperature = 35.8degC, OLR = 1.42 gTS/l.d and HRT = 11.7 d. Experiments conducted with these optimised levels of variables gave an average biogas yield of 1.95 l/g COD. Factor analysis to both system discovered that all factors studied (OLR, HRT and temperature) gave significant effect to biogas production process.","2165-4387;2165-4395","978-1-4244-1887-9978-1-4244-1888","10.1109/ICSET.2008.4747032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4747032","","Inductors;Wastewater treatment;Temperature;Filters;Chemical engineering;Batch production systems;Design optimization;Fluidization;Energy consumption;Power engineering and energy","biofuel;bioreactors;waste recovery","banana stem waste;batch reactor sequencing;banana plantation soil;anaerobic condition;organic loading rates;hydraulic retention times;optimization test;design-expert software version;biogas production process;temperature 26 degC to 40 degC","","","18","","","","","","IEEE","IEEE Conferences"
"Genetic Algorithm Based Optimization for AdaBoost","Z. Dezhen; Y. Kai","NA; NA","2008 International Conference on Computer Science and Software Engineering","","2008","1","","1044","1047","AdaBoost was proposed as an efficient algorithm of the ensemble learning field, it selects a set of weak classifiers and combines them into a final strong classifier. However, conventional AdaBoost is a sequential forward search procedure using the greedy selection strategy, redundancy can not be avoided. We proposed a post optimization procedure for the found classifiers and their coefficients based on genetic algorithm, which removes the redundancy classifiers and leads to shorter final classifiers and a speedup of classification. Our algorithm is tested on the UCI benchmark data sets, fewer weak classifiers and faster classification compared with conventional AdaBoost algorithm is experienced.","","978-0-7695-3336","10.1109/CSSE.2008.1040","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721931","AdaBoost;genetic algorithm;weak classifier;strong classifier","Genetic algorithms;Boosting;Machine learning algorithms;Machine learning;Computer science;Software engineering;Genetic engineering;Educational institutions;Software algorithms;Classification algorithms","genetic algorithms;learning (artificial intelligence);pattern classification","genetic algorithm;sequential forward search procedure;greedy selection strategy;post optimization procedure;AdaBoost algorithm","","4","7","","","","","","IEEE","IEEE Conferences"
"A Multi-Layer Cloud State Space Model and Its Application","A. Wu; Y. Zhao; Z. Ma; G. Zeng; X. Tu","NA; NA; NA; NA; NA","2009 International Conference on Computational Intelligence and Software Engineering","","2009","","","1","4","Based on multi-layer state space model, the paper proposes a five-layer energy information hierarchical model, and presents a rank normal cloud conversion model to complete the transforming between energy grade and value. To meet the demands of the research, the rank conversion cloud includes two parts, the single energy rank conversion cloud and the integrated energy rank conversion cloud. Finally, the experiment shows that the model can better solve uncertainty problems of the fuzziness brought by grading and randomness along with investigation, and its practicality is better.","","978-1-4244-4507","10.1109/CISE.2009.5365562","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5365562","","Clouds;State-space methods;Explosions;Automatic testing;Artificial intelligence;Graphical user interfaces;Physics;Arithmetic;Application software;Electronic mail","fuzzy set theory;information theory;state-space methods;uncertain systems","multilayer cloud state space model;five-layer energy information hierarchical model;rank normal cloud conversion model;energy grade;single energy rank conversion cloud;integrated energy rank conversion cloud;uncertainty problem;fuzziness","","","7","","","","","","IEEE","IEEE Conferences"
"A distributed and integrated platform for missile’s parallel design","Fei Xiao; Zhe Gan; Min Chen; Weihua Zhang","College of Aerospace and Materials Engineering, National University of Defense Technology, Changsha, Hunan Province, China; College of Electronic Science and Engineering, National University of Defense Technology, Changsha, Hunan Province, China; College of Aerospace and Materials Engineering, National University of Defense Technology, Changsha, Hunan Province, China; College of Aerospace and Materials Engineering, National University of Defense Technology, Changsha, Hunan Province, China","2008 International Conference on Information and Automation","","2008","","","1681","1684","A distributed and integrated framework for missile design and manufacturing is developed in this study, using multidisciplinary optimization and workflow technology. The framework contains technologies of application integration, information integration and process integration. It can reduce the complexity of data exchange among disciplines, shorten the product development cycle and cut down the cost in product development. Users can customize system composing, design flow and data connection. In addition, the platform support engineers of optimization design and distributed design to work together. An example in this paper shows that the framework has been tested in missile design.","","978-1-4244-2183-1978-1-4244-2184","10.1109/ICINFA.2008.4608275","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4608275","Multidisciplinary Design Optimization;Distributed;Integrated Design;Workflow;Missile","Missiles;Optimization;Process control;Software;Design optimization;Data models;Collaboration","CAD;defence industry;distributed processing;military computing;missiles;optimisation;product development;production engineering computing","distributed platform;integrated platform;missile parallel design;missile manufacturing;multidisciplinary optimization;workflow technology;process integration;data exchange;product development cycle","","1","8","","","","","","IEEE","IEEE Conferences"
"Comparison of Scheduling Strategies via a Statistical Analysis in an Industrial Context","D. Duvivier; O. Roux; V. Dhaevers; A. Lazraq","Laboratoire d'Informatique du Littoral, Universite du Littoral Cote d'Opale, BP719, F-62228 Calais, France. david.duvivier@lil.univ-littoral.fr; Catholic University of Mons, 151 Chaussée de Binche, B-7000 Mons, Belgium. roux@fucam.ac.be; Catholic University of Mons, 151 Chaussée de Binche, B-7000 Mons, Belgium. dhaevers@fucam.ac.be; ENIM, Rabat, Morocco. lazraq@enim.ac.ma","2006 International Conference on Service Systems and Service Management","","2006","1","","216","221","This paper deals with multicriteria decision-making applied to discrete-continuous scheduling problems. The need to quickly generate good compromises between conflicting objectives requires the implementation of fast multicriteria scheduling strategies based on simulation and optimization methods. The purpose of this paper is to compare the solutions of five scheduling strategies so as to select the most adequate strategy when considering the decision-maker's preferences. The solutions are ranked via a lexicographical sort or a multicriteria analysis and validated by standard non-parametric statistical tests. This study synthesizes the results of the application of several scheduling strategies to a real-life, highly constrained industrial problem","2161-1890;2161-1904","1-4244-0451-71-4244-0450","10.1109/ICSSSM.2006.320615","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4114435","Decision making;multicriteria optimization;computer aided manufacturing;simulation;statistical analysis","Job shop scheduling;Statistical analysis;Testing;Processor scheduling;Decision making;Computational modeling;Optimization methods;Application software;Manufacturing industries;Computer aided manufacturing","decision making;optimisation;scheduling;statistical testing;strategic planning","discrete-continuous scheduling strategies;statistical testing;industrial context;multicriteria decision-making;optimization method;lexicographical sorting","","","13","","","","","","IEEE","IEEE Conferences"
"An Optimized Model for Blasting Parameters in Underground Mines' Deep-Hole Caving Based on Rough Set and Artificial Neural Network","F. Jiang; K. Zhou; H. Deng; X. Li; Y. Zhong","NA; NA; NA; NA; NA","2009 Second International Symposium on Computational Intelligence and Design","","2009","1","","459","462","For better predicting and optimizing the blasting parameters in underground deep-hole mining, 16 groups of deep-hole blasting parameters are collected and collated, combining rough set and artificial neuron network theory, an optimized model for basting parameters in underground mines' long-hole caving based on rough set and artificial neural network is set up. Adopting the rough set software for data reduction, then using the reduced data and raw data as the inputs of the ANN software, the predictions have completed. The input attributes of the ANN model are 6, the RS - ANN model input attributes are 5, both training samples are 12, both forecast samples are 3, the former average prediction accuracy is 0.91 ~ 13.7%, the latter is 0.12 ~ 7.97%. This study shows that rough set is effective in data reduction while retaining key information; the predicted results of RS - ANN model coincide with the actual situation, and the overall accuracy increased by more.","","978-0-7695-3865","10.1109/ISCID.2009.122","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5368964","deep-hole caving;blasting parameters;rough set;artificial neural network;prediction and optimization","Artificial neural networks;Design optimization;Predictive models;Optimization methods;Testing;Drilling;Safety;Design engineering;Genetic algorithms;Knowledge representation","data reduction;mining;neural nets;optimisation;rough set theory","blasting parameters;underground deep-hole mining;rough set;artificial neural network;optimization;data reduction","","2","11","","","","","","IEEE","IEEE Conferences"
"Design Optimisation of Electromagnetic Devices Using Continuum Design Sensitivity Analysis Combined with Comercial EM Software","D. Kim; J. K. Sykulski; D. A. Lowther","Kyungpook National University, South Korea; University of Southampton, United Kingdom; McGill University, Canada","6th International Conference on Computational Electromagnetics","","2006","","","1","3","This paper deals with two kinds of optimisation problems, relevant to the optimised source distribution and the shape optimum design, using Continuum Design Sensitivity Analysis (CDSA) in combination with standard electromagnetic (EM) software. Fast convergence and compatibility with existing EM software are the distinctive features of the proposed implementation. In order to verify the advantages and also to facilitate understanding of the method itself, two design optimisation problems have been tested: one is an MRI design problem related to finding an optimal permanent magnet distribution and the other is a pole shape design problem of a BLDC motor for reducing cogging torque, using both 2D and 3D models.","","978-3-8007-2957","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5754946","","Educational institutions;Software","","","","","","","","","","","VDE","VDE Conferences"
"Effectiveness of Software Solutions in Reducing Errors Due to Multi-Path in Spherical near Field Measurements","M. Giles; S. Mishra","Canadian Space Agency, David Florida Laboratory, Ottawa, Canada; Canadian Space Agency, David Florida Laboratory, Ottawa, Canada. shantnu.mishra@space.gc.ca","The Second European Conference on Antennas and Propagation, EuCAP 2007","","2007","","","1","1","Summary form only given. The presence of multi-path reflections is usually a source of significant error in low frequency spherical near field measurements as the test antennas typically have low gain and the cost of lining the anechoic chamber with optimal low reflectivity is prohibitive. Some earlier papers have discussed the effects of there errors on far field patterns and mitigation of some of the errors using hardware solutions and by range optimization. Many of the commercially available software suits for near-field to far field conversion provide algorithms and utilities for reducing the errors due to multi-path. Some examples of such software solutions include MARS add-on for Near Field Systems Inc. (NSI), IsoFilter technique from MI Technologies, and Spherical mode filtering routines in CASAMS and TICRA SNIFTD software. These techniques generally require either over sampling of measured data or mounting the antenna such that the phase centre is displaced with respect to the centre of measurement thereby acquiring data on a sphere larger than the minimum sphere originating at the phase centre. The error reduction algorithms then estimate and filter out the contribution due to multi- path This paper presents the results of a study undertaken to determine the effectiveness of some of these algorithms in reducing the multi-path errors in a low frequency measurement facility. A variety of antennas such as horn antennas, log-periodic, helical and phased arrays were measured in a spherical near field facility housed in a sub-optimally lined anechoic chamber. The far field data with and without the software correction was compared to the expected behaviour of the antenna calculated using numerical techniques. The results of these measurements showing the effectiveness and limitations of the techniques studied will be presented. Practical considerations in reaching optimal solutions will be discussed.","0537-9989","978-0-86341-842","10.1049/ic.2007.0918","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4459041","","","anechoic chambers (electromagnetic);antenna testing;electrical engineering computing;electromagnetic wave reflection","multipath reflections;spherical near field measurement errors;antenna testing;MARS add-on;IsoFilter technique;MI Technologies;Spherical mode filtering routines;CASAMS;TICRA SNIFTD software;over sampling;suboptimally lined anechoic chamber","","","","","","","","","IET","IET Conferences"
"Compositive Test of a Platform Architecture for E-Government","Z. Zhang; L. Li","NA; NA","2008 4th International Conference on Wireless Communications, Networking and Mobile Computing","","2008","","","1","3","In our country, application of e-government based on the homemade platform is seldom, government support homemade platform application positively all along, But the current problem is that the homemade software and hardware is not mature, they are not compatible each other, this paper passed through combines, test, optimization repeatedly, finally has formed an e-government application platform based on Linux operation system, OpenBASE database, Tongweb middleware, and Shuguang server, and gives the key integration technology, compare the result of cut-and-try run with the current popular web technical indicates that this platform is practical and feasible. It will provide the reference to our country's e-government construction, at the same time save the cost greatly.","2161-9646;2161-9654","978-1-4244-2108-4978-1-4244-2107","10.1109/WiCom.2008.2179","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4680368","","Electronic government;Application software;Computer architecture;Hardware;Software testing;System testing;Linux;Databases;Middleware;Costs","government data processing;Linux;middleware;relational databases","homemade platform;homemade software;homemade hardware;E-government;Linux operation system;OpenBASE database;Tongweb middleware;Shuguang server;relational databases","","","4","","","","","","IEEE","IEEE Conferences"
"Avoiding Irrelevant and Misleading Information When Estimating Development Effort","M. Jørgensen; S. Grimstad","Simula Research Laboratory; Simula Research Laboratory","IEEE Software","","2008","25","3","78","83","Software projects average about 30 percent accuracy in effort estimation.1 Expecting highly accurate effort estimates might be unrealistic because software development projects are inherently uncertain. Nevertheless, software professionals' tendency toward overly optimistic estimates and their high level of estimation inconsistency suggest potential for improving effort estimation processes.","0740-7459;1937-4194","","10.1109/MS.2008.57","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4497769","software development;effort estimation;estimation irrelevant information","Programming;Testing;Costs;Laboratories;Statistical analysis;Statistical distributions;Recruitment","software cost estimation;software development management","software development project;software cost estimation","","19","12","","","","","","IEEE","IEEE Journals & Magazines"
"Hybrid Particle Swarm Optimization Approach for Solving the Discrete OPF Problem Considering the Valve Loading Effects","M. R. AlRashidi; M. E. El-Hawary","NA; NA","IEEE Transactions on Power Systems","","2007","22","4","2030","2038","This paper presents a hybrid particle swarm optimization algorithm (HPSO) as a modern optimization tool to solve the discrete optimal power flow (OPF) problem that has both discrete and continuous optimization variables. The problem is classified as constrained mixed integer nonlinear programming with multimodal characteristics. The objective functions considered are the system real power losses, fuel cost, and the gaseous emissions of the generating units. Two different types of fuel cost functions are considered in this study, namely the conventional quadratic function and the augmented quadratic function to introduce more accurate modeling that incorporates the valve loading effects. The latter model presents nondifferentiable and nonconvex regions that challenge most gradient-based optimization algorithms. The proposed algorithm makes use of the PSO, known for its global searching capabilities, to allocate the optimal control settings while Newton-Raphson algorithm minimizes the mismatch of the power flow equations. A hybrid inequality constraint handling mechanism that preserves only feasible solutions without the need to augment the original objective function is incorporated in the proposed approach. To demonstrate its robustness, the proposed algorithm was tested on the IEEE 30-bus system with six generating units. Several cases were investigated to test and validate the consistency of detecting optimal or near optimal solution for each objective. Results are compared to solutions obtained of MATPOWER software outcomes that employs sequential quadratic programming algorithm to solve the OPF. The impact of the proposed inequality constraint handling method in improving the HPSO performance is illustrated. Furthermore, a study of HPSO parameters tuning with regard to the OPF problem is presented and analyzed.","0885-8950;1558-0679","","10.1109/TPWRS.2007.907375","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4349053","Emission;optimal power flow (OPF);particle swarm optimization;power system operation","Particle swarm optimization;Valves;Load flow;Fuels;Cost function;Power system modeling;Power generation;Optimal control;Equations;Robustness","gradient methods;integer programming;load flow;load regulation;Newton-Raphson method;optimal control;particle swarm optimisation;quadratic programming","hybrid particle swarm optimization approach;HPSO;discrete optimal power flow problem;OPF;valve loading effects;constrained mixed integer nonlinear programming;multimodal characteristics;power losses;fuel cost functions;gaseous emissions;generating units;gradient-based optimization algorithm;global searching capabilities;optimal control settings;Newton-Raphson algorithm;hybrid inequality constraint handling mechanism;IEEE 30-bus system;MATPOWER software;sequential quadratic programming algorithm;inequality constraint handling method","","137","30","","","","","","IEEE","IEEE Journals & Magazines"
"Optimisation of wirebond interconnects by automated parameter variation","S. Martens; J. Wilde; E. Zukowski; F. Volklein; M. Ledermann","Infineon Technologies AG, Regensburg, Germany; University of Freiburg, IMTEK, Institute of Microsystem Technology, Assembly and Packaging, Germany; University of Freiburg, IMTEK, Institute of Microsystem Technology, Assembly and Packaging, Germany; University of Applied Sciences Wiesbaden, IMtech, Institute of Microtechnologies, Germany; Robert Bosch GmbH, Stuttgart, Germany","EuroSimE 2008 - International Conference on Thermal, Mechanical and Multi-Physics Simulation and Experiments in Microelectronics and Micro-Systems","","2008","","","1","4","A numerical optimisation strategy for interconnections in electronic packaging is demonstrated. The method is based on a toolbox for the parametric generation of finite- element models of package types such as Chip Scale Package (CSP), Micro Lead Package (MLP) or Ball Grid Array (BGA). The novelty of this work is the combination of this modeling toolbox with an optimisation software for automatic parameter variation. Resulting in a convenient tool to investigate the influence of geometry on the relevant quality characteristics of the device. Users can set the parameters to be varied, the ranges of parameter variation and the number of iterations. The optimisation software automatically generates the parameter sets depending on the number of iterations. The generation of a finite-element model for each parameter set, the meshing and the implementation of the required material properties are also automated by the toolbox. Thereafter, the simulation of the desired load conditions results in quality characteristics such as the maximum mechanical stress for each set. After completion of all iterations, the optimisation software provides a user interface for statistical analysis and graphic visualisation of the results. The wirebond geometry is also included in the toolbox. Influence on maximum mechanical stress and fatigue properties under thermal loads is examined during this study. As an example, the effect of the bonding tool geometry on the locations and the value of the maximum mechanical stress in the wirebond material during thermal shocking is determined. This combination of parametric finite-element model generation and automatic parameter variation represents a powerful tool for design automation in packaging technology and product development. The effects of several geometrical parameters on the thermal and mechanical behaviour of packaging interconnects can be predicted. In a virtual product-development process, time- and cost-intensive prototyping and testing can thus be reduced.","","978-1-4244-2127-5978-1-4244-2128","10.1109/ESIME.2008.4525044","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4525044","","Electronics packaging;Chip scale packaging;Geometry;Finite element methods;Thermal stresses;Mesh generation;Lead;Software tools;Material properties;User interfaces","electronic design automation;electronics packaging;fatigue cracks;finite element analysis;integrated circuit bonding;integrated circuit interconnections;lead bonding;optimisation;product development","wirebond interconnect optimisation;automated parameter variation;electronic packaging;toolbox modeling;finite-element model;geometry;mechanical stress;fatigue properties;thermal load;product development;design automation","","","8","","","","","","IEEE","IEEE Conferences"
"Research on Optimized Arithmetic of Endless Rolling Multi-drive System","H. Wang; L. Li; X. Liu; Y. Zang","NA; NA; NA; NA","2009 Second International Conference on Intelligent Networks and Intelligent Systems","","2009","","","570","573","Endless rolling multi-drive control system is the position servo control system whose information flow is based on Fieldbus, actuators AC motors, control and amplification devices frequency converters. According to experimental method to establish mathematical models of endless rolling simulation test platform, considering some of the special requirements, the LQR control algorithm based on fuzzy control has been proposed. This arithmetic is simulated by Matlab/Simulink software. Meanwhile, the experiment to test the method has also been provided by STEP7 programming software. The experiment result shows that this method is effective.","","978-1-4244-5557-7978-0-7695-3852","10.1109/ICINIS.2009.150","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5366075","endless rolling;fuzzy;LQR","Arithmetic;Control systems;Servosystems;Field buses;Actuators;AC motors;Frequency conversion;Mathematical model;Testing;Fuzzy control","AC motor drives;fuzzy control;linear quadratic control;machine control;rolling;servomechanisms","optimized arithmetic;endless rolling multidrive control system;position servo control system;information flow;fieldbus;actuators AC motors;amplification devices frequency converters;endless rolling simulation test platform;LQR control algorithm;fuzzy control;Matlab/Simulink software;STEP7 programming software","","","10","","","","","","IEEE","IEEE Conferences"
"Building an Experimental Infrastructure for B3G Testing Using an Event-based Distributed System","G. Kormentzas","NCSR "Demokritos", Institute of Informatics and Telecommunications, Athens, Greece. gkorm@iit.demokritos.gr","2007 16th IST Mobile and Wireless Communications Summit","","2007","","","1","6","Wireless networking segments of different technologies are going to coexist and interoperate in the context of a future IP next generation network. This coexistence and interoperation creates a number of open research and standardization issues making the necessity of testing an important key enabler towards the transmission of existing wireless networks to the next generation ones. A de facto requirement for reliable B3G testing is the formation of big infrastructures that provide advance testing facilities incorporating a number of hardware and software components, clusters, tetsbeds. The paper discusses the formation of such a big testing infrastructure called virtual distributed testbed (VDT) that offers the capability for advance B3G radio resource management testing, especially algorithms concerning both cross-layer and cross-system optimization. As VDT is structured around an event-based logic, its macroscopic picture resembles to an event-based distributed system. Building on this perspective, the paper exhibits the VDT features exploiting relevant scientific literature. All these features are going to be implemented in the context of the EC-funded UNITE project. The outcome of this project targets to be an event-based virtual distributed testbed that is going to be used for exhaustive B3G cross-layer/cross-system testing.","2167-1753;2167177X","1-4244-1662-0963-8111-66","10.1109/ISTMWC.2007.4299328","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4299328","","System testing;Next generation networking;Software testing;Standardization;Wireless networks;Hardware;Resource management;Clustering algorithms;Logic;Buildings","3G mobile communication;groupware;open systems;optimisation;telecommunication computing;telecommunication network management","event-based distributed system;wireless networking segments;virtual distributed testbed;B3G radio resource management testing;cross-system optimization;EC-funded UNITE project","","","17","","","","","","IEEE","IEEE Conferences"
"Build-and-Test Workloads for Grid Middleware: Problem, Analysis, and Applications","A. Iosup; D. Epema; P. Couvares; A. Karp; M. Livny","Fac. EEMCS, Delft Univ. of Technol., Delft; Fac. EEMCS, Delft Univ. of Technol., Delft; NA; NA; NA","Seventh IEEE International Symposium on Cluster Computing and the Grid (CCGrid '07)","","2007","","","205","213","The Grid promise is starting to materialize today: large- scale multi-site infrastructures have grown to assist the work of scientists from all around the world. This tremendous growth can be sustained and continued only through a higher quality of the middleware, in terms of deployability and of correct functionality. A potential solution to this problem is the adoption of industry practices regarding middleware building and testing. However, it is unclear what good build-and-test environments for grid middleware should look like, and how to use them efficiently. In this work we address both these problems. First, we study the characteristics of the NMI build-and-test environment, which handles millions of testing tasks annually, for major Grid middleware such as Condor, Globus, VDT, and gLite. Through the analysis of a system-wide trace covering the past two years we find the main characteristics of the workload, as well as the performance of the system under load. Second, we propose mechanisms for more efficient test management and operation, and for resource provisioning and evaluation. Notably, we propose a generic test optimization technique that reduces the test time by 95%, while achieving 93% of the maximum accuracy, under real conditions.","","0-7695-2833","10.1109/CCGRID.2007.29","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4215383","","Middleware;Testing;Grid computing;Large-scale systems;Materials science and technology;Performance analysis;Packaging;Application software;Computer science;Resource management","grid computing;middleware;optimisation","build-and-test workloads;grid middleware;middleware building;middleware testing;NMI;Condor;Globus;VDT;gLite;generic test optimization technique","","2","24","","","","","","IEEE","IEEE Conferences"
"Scheduling Product Line Features for Effective Roadmapping","J. Savolainen; J. Kuusela","NA; NA","2008 15th Asia-Pacific Software Engineering Conference","","2008","","","195","202","Large industrial product lines may produce tens of thousands of variants each year. Each variant typically contains both reusable assets as well as product specific code created by different organizational units. To produce this vast number of variants the organizational resources must be used efficiently. For roadmapping this means an ability to schedule production of reusable assets so that all variants can be completed according to their requirements. When aiming for centralized variability management, roadmapping requires effective management of product line feature dependences and priorities. In this paper, we first introduce the problems haunting feature roadmapping in industrial product lines. Then we investigate how these problems can be solved using a novel approach for organizing product lines based on our practical experiences. Finally, we discuss our experiences and compare our approach with results by other researchers.","1530-1362;1530-1362","978-0-7695-3446","10.1109/APSEC.2008.21","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724548","Product line;reuse;roadmapping;feature modelling","Job shop scheduling;Software engineering;Computer industry;Production;Organizing;Testing;Technology management;Pricing;Portfolios","formal specification;product development;scheduling;software development management;software maintenance;software reusability","software product line feature dependency management;reusable asset;product specific code;organizational resource;requirements scheduling;centralized variability management;product line roadmapping;software product line evolution","","","13","","","","","","IEEE","IEEE Conferences"
"Automated Web Performance Analysis","M. Pinzger","NA","2008 23rd IEEE/ACM International Conference on Automated Software Engineering","","2008","","","513","516","Performance is a key feature in many systems nowadays. There are several tools on the market that ensure and test for adequate performance. They, can be divided into simulation tools and monitoring tools. But only a few automatise and combine both approaches. This paper describes a system capable of automatically creating a web performance simulation and conducting trend analysis of the system under test (SUT). To achieve this the system requires input information, like Monitoring Points and Static-Information about the SUT. The system monitors and analyses the SUT and based on this information generates a simulation model of the system. The simulation model is refinded stepwise e.g. by adding or removing connections between the model components or adjusting the parameters until the aimed accuracy is achieved. With the help of the simulation model a prediction module creates an analysis of the SUT, and thereby can give as much information about the current state of the system and potential trends as possible. This predictive information can be used for pro-active server tuning or other performance optimisations. The focus of my PhD thesis is on the adjustment and prediction part of the system described here. For all other parts, already existing tools and techniques will be used where possible. This initial paper outlines the complete system.","1938-4300","978-1-4244-2188-6978-1-4244-2187","10.1109/ASE.2008.95","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4639388","","Monitoring;Analytical models;Predictive models;Software;Data models;Computational modeling;Complexity theory","Internet;program testing;software performance evaluation;system monitoring","automated Web performance analysis;simulation tools;monitoring tools","","2","13","","","","","","IEEE","IEEE Conferences"
"Modeling and Capability Assessment of Maintenance System Based on Agent","J. Zhao; H. Qi; Y. Zhang","NA; NA; NA","2008 International Seminar on Business and Information Management","","2008","2","","211","215","In order to evaluate capability of maintenance system, we have defined the maintenance support plan of warship equipment, built the substance modeling of maintenance system, abstracted the network plan of maintenance, established the optimization target of plan and the indexes of capability assessment, built two tier modeling and the algorithm of sea base and factory based on agent. The test have proved the validity of modeling and algorithm, it can provide a science approach for command man making a decision of equipment support.","","978-0-7695-3560","10.1109/ISBIM.2008.136","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5116458","Agent;Warship maintenance system;evaluate;optimization;heuristic regulation","Artificial intelligence;Solid modeling;Stress;Competitive intelligence;Computational modeling;Seminars;Information management;Production facilities;Testing;Inductors","maintenance engineering;military computing;military equipment;optimisation;software agents","maintenance system;agent;maintenance support plan;warship equipment;optimization;target of plan","","","10","","","","","","IEEE","IEEE Conferences"
"Dynamic Voltage Scaling of Supply and Body Bias Exploiting Software Runtime Distribution","S. Hong; S. Yoo; B. Bin; K. Choi; S. Eo; T. Kim","EE Department, Stanford University; Samsung Electronics; Samsung Electronics; Samsung Electronics; Samsung Electronics; EECS, Seoul National University","2008 Design, Automation and Test in Europe","","2008","","","242","247","This paper presents a method of dynamic voltage scaling (DVS) that tackles both switching and leakage power with combined V<sub>dd</sub>/V<sub>bs</sub>scaling and gives minimum average energy consumption exploiting the runtime distribution of software execution. We present a mathematical formulation of the DVS problem and an efficient numerical solution. Experimental results show that the presented method shows up to 44% further reduction in energy consumption compared with existing methods. Especially, when the leakage power consumption is significant, i.e. when temperature is high, the presented method is proven to be the most effective.","1530-1591;1558-1101","978-3-9810801-3-1978-3-9810801-4","10.1109/DATE.2008.4484693","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4484693","","Dynamic voltage scaling;Runtime;Voltage control;Energy consumption;Statistical analysis;Frequency estimation;Temperature;Optimization methods;Software performance;Constraint optimization","logic design;microprocessor chips","dynamic voltage scaling;software runtime distribution;switching power;software execution;energy consumption;leakage power consumption","","14","14","","","","","","IEEE","IEEE Conferences"
"Model Checking Software at Compile Time","A. Fehnker; R. Huuck; P. Jayet; M. Lussenburg; F. Rauch","University of New South Wales, Australia; University of New South Wales, Australia; University of New South Wales, Australia; University of New South Wales, Australia; University of New South Wales, Australia","First Joint IEEE/IFIP Symposium on Theoretical Aspects of Software Engineering (TASE '07)","","2007","","","45","56","Software has been under scrutiny by the verification community from various angles in the recent past. There are two major algorithmic approaches to ensure the correctness of and to eliminate bugs from such systems: software model checking and static analysis. Those approaches are typically complementary. In this paper we use a model checking approach to solve static analysis problems. This not only avoids the scalability and abstraction issues typically associated with model checking, it allows for specifying new properties in a concise and elegant way, scales well to large code bases, and the built-in optimizations of modern model checkers enable scalability also in terms of numbers of properties to be checked. In particular, we present Goanna, the first C/C++ static source code analyzer using the off-the-shelfmodel checker NuSMV, and we demonstrate Goanna's suitability for developer machines by evaluating its run-time performance, memory consumption and scalability using the source code of OpenSSL as a test bed.","","0-7695-2856-2978-0-7695-2856","10.1109/TASE.2007.34","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4239949","","Scalability;Runtime;Australia;Computer bugs;Algorithm design and analysis;Encoding;Software algorithms;Software systems;Performance analysis;Software testing","formal specification;program compilers;program debugging;program diagnostics;program verification","software model checking;static analysis;bug elimination;formal specification;program compiler","","10","28","","","","","","IEEE","IEEE Conferences"
"Improved Imperialist Competitive Algorithm for Constrained Optimization","Y. Zhang; Y. Wang; C. Peng","NA; NA; NA","2009 International Forum on Computer Science-Technology and Applications","","2009","1","","204","207","This paper introduces an improved evolutionary algorithm based on the imperialist competitive algorithm. The original approach in the imperialist competitive algorithm has difficulty in implement practically with the increase of the dimension of the search spaces, as the ambiguous definition of the ¿random angle¿ in the process of optimization. Compare to the original algorithm, the proposed approach based on the concept of small probability perturbation has more simplicity to be implemented, especially in solving high-dimensional optimization problems. Furthermore, the present algorithm has been extended to constrained optimization problem, using a classical penalty technique to handle constraints. Several numerical optimization examples are tested by applying the proposed algorithm, and the results show its applicability and flexibility in dealing with different types of optimization problems.","","978-1-4244-5423-5978-1-4244-5422-8978-0-7695-3930","10.1109/IFCSTA.2009.57","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5385096","Improved Imperialist Competitive Algorithm;Constrained Optimization;Evolutionary Algorithm;Penalty Technique","Constraint optimization;Evolutionary computation;Costs;Space technology;Testing;Adaptive arrays;Chemical industry;Computer applications;Application software;Automation","competitive algorithms;constraint handling;evolutionary computation;probability","imperialist competitive algorithm;constrained optimization;evolutionary algorithm;random angle;probability perturbation;high-dimensional optimization problem;classical penalty technique;constraint handling;numerical optimization","","12","25","","","","","","IEEE","IEEE Conferences"
"A decision support tool to optimize scheduling of IT changes","R. Reboucas; J. Sauve; A. Moura; C. Bartolini; D. Trastour","Department of Computing Systems, Federal University of Campina Grande, Campina Grande, Brazil. rodrigor@dsc.ufcg.edu.br; Department of Computing Systems, Federal University of Campina Grande, Campina Grande, Brazil. jacques@dsc.ufcg.edu.br; Department of Computing Systems, Federal University of Campina Grande, Campina Grande, Brazil. antao@dsc.ufcg.edu.br; HP Laboratories Palo Alto, USA. claudio.bartolini@hp.com; HP Laboratories Bristol, UK. david.trastour@hp.com","2007 10th IFIP/IEEE International Symposium on Integrated Network Management","","2007","","","343","352","Change management is one of the most critical processes in IT management. Some of the reasons are the sheer number of changes and the difficulty of evaluating the impact of changes on the IT services being provided. Through carrying out a survey with IT managers and practitioners, we have found that, among the activities performed during change management, change scheduling (allocating changes to change windows) is the most problematic one. In this paper we solve the change scheduling problem by using a business-driven approach that evaluates the impact of a change schedule in terms of the financial loss imposed on the service provider. Toward this aim, we model the impact of SLA violations when the implementation of changes is done after their deadline. A change scheduling optimization problem is then formalized and its solution is applied to a typical scenario. The results show that optimizing the scheduling of changes can result in significant savings to an IT support organization.","1573-0077","1-4244-0798-21-4244-0799","10.1109/INM.2007.374799","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4258551","change management;business-driven IT management;service level management;Information Technology Infrastructure Library (ITIL);business metrics;modeling;performance evaluation;business impact;decision and negotiation support tools","Scheduling;Technology management;Laboratories;Software tools;Libraries;Quality management;Software testing;Software prototyping;Optimization methods;Solids","decision support systems;DP management;management of change;optimisation;scheduling","decision support tool;change scheduling optimization;IT management;change management;SLA violations","","22","21","","","","","","IEEE","IEEE Conferences"
"Competency-Based Intelligent Curriculum Sequencing: Comparing Two Evolutionary Approaches","L. de-Marcos; R. Barchino; J. Martínez; J. Gutiérrez; J. Hilera","NA; NA; NA; NA; NA","2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology","","2008","3","","339","342","The process of creating e-learning contents using reusable learning objects (LOs) can be broken down in two sub-processes: LOs finding and LO sequencing. Although semiautomatic tools that aid in the finding process exits, sequencing is usually performed by instructors, who create courses targeting generic profiles rather than personalized materials. This paper proposes an evolutionary approach to automate this latter problem while, simultaneously, encourages reusability and interoperability by promoting standards employment. A model that enables automated curriculum sequencing is proposed. By means of interoperable competency records and LO metadata, the sequencing problem is turn into a constraint satisfaction problem. Particle swarm optimization (PSO) and genetic algorithm (GA) agents are designed, built and tested in real and simulated scenarios. Results show both approaches succeed in all test cases, and that they handle reasonably computational complexity inherent to this problem, but PSO approach outperforms GA.","","978-0-7695-3496","10.1109/WIIAT.2008.279","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4740793","Learning Object;Competency;Sequencing;Swarm Intelligence;Genetic Algorithm","Competitive intelligence;Electronic learning;Intelligent agent;Testing;Particle swarm optimization;Genetic algorithms;Adaptive systems;Costs;Artificial intelligence;Computer science","computational complexity;constraint theory;educational courses;genetic algorithms;intelligent tutoring systems;meta data;object-oriented programming;open systems;particle swarm optimisation;software reusability","competency-based intelligent curriculum sequencing;e-learning;reusable learning object;educational course;evolutionary approach;interoperability;metadata;constraint satisfaction;particle swarm optimization;genetic algorithm;computational complexity","","1","15","","","","","","IEEE","IEEE Conferences"
"Slack-based global multiprocessor scheduling of aperiodic tasks in parallel embedded real-time systems","L. Lundberg; H. Lennerstad","Department of Systems and Software, School of Engineering, Blekinge Institute of Technology, 372 25 Ronneby, Sweden; Department of Systems and Software, School of Engineering, Blekinge Institute of Technology, 372 25 Ronneby, Sweden","2008 IEEE/ACS International Conference on Computer Systems and Applications","","2008","","","465","472","We provide a constant time schedulability test and priority assignment algorithm for an on-line multiprocessor server handling aperiodic tasks. Dhall's effect is avoided by dividing tasks in two priority classes based on their utilization: heavy and light. The improvement in this paper is due to assigning priority of light tasks based on slack - not on deadlines. We prove that if the load on the multiprocessor stays below (3 - radic5)/2 ap 38.197%, the server can accept an incoming aperiodic task and guarantee that the deadlines of all accepted tasks will be met. This is better than the current state-of- the-art algorithm where the priorities of light tasks are based on deadlines (the corresponding bound is in that case 35.425%).","2161-5322;2161-5330","978-1-4244-1967-8978-1-4244-1968","10.1109/AICCSA.2008.4493574","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4493574","","Processor scheduling;Real time systems;Scheduling algorithm;Delay;Embedded software;Software systems;Software testing;System testing;Software algorithms;Admission control","embedded systems;multiprocessing systems;processor scheduling","slack-based global multiprocessor scheduling;aperiodic tasks;parallel embedded real-time systems;time schedulability test;priority assignment algorithm","","1","10","","","","","","IEEE","IEEE Conferences"
"Mapping the Physical Layer of Radio Standards to Multiprocessor Architectures","C. Grassmann; M. Richter; M. Sauermann","Infineon Technologies AG COM PS CE; Siemens CT PP 2; Infineon Technologies AG COM PS CE","2007 Design, Automation & Test in Europe Conference & Exhibition","","2007","","","1","6","We are concerned with the software implementation of baseband processing for the physical layer of radio standards (""software defined radio - SDR""). Given the constraints for mobile terminals with respect to power consumption, chip area and performance, nonstandard architectures without compiler support are the targets a SDR implementation has to face. For this domain we present a way to safely move from a functional model to the assembly level in order to come to a tested multithreaded optimized implementation in manageable time. We carried out this program for the standards WLAN IEEE 802.11b and 3GPP WCDMA exploiting various levels of parallelism: thread level parallelism (""MIMD""), data level parallelism (""SIMD"") and instruction level parallelism (""VLIW""). We came up with a software implementation running in real time on Infineon's programmable multiple SIMD core (MuSIC) processor","1530-1591;1558-1101","978-3-9810801-2","10.1109/DATE.2007.364496","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4212006","","Physical layer;Computer architecture;Baseband;Software standards;Software radio;Energy consumption;Assembly;Testing;Wireless LAN;Multiaccess communication","code division multiple access;IEEE standards;multiprocessing systems;software radio","physical layer;radio standards;multiprocessor architectures;software implementation;baseband processing;software defined radio;WLAN IEEE 802.11b;3GPP WCDMA;thread level parallelism;data level parallelism;instruction level parallelism;programmable multiple SIMD core processor","","3","9","","","","","","IEEE","IEEE Conferences"
"Cache miss reduction through hardware-assisted loop optimization","Kang Zhao; Jinian Bian; Chenqian Jiang; Sheqin Dong; Satoshi Goto","Dept. of Computer Science and Technology, Tsinghua University, Beijing, China; Dept. of Computer Science and Technology, Tsinghua University, Beijing, China; Dept. of Computer Science and Technology, Tsinghua University, Beijing, China; Dept. of Computer Science and Technology, Tsinghua University, Beijing, China; Graduate School of Information, Production and Systems, Waseda University, Japan","2008 12th International Conference on Computer Supported Cooperative Work in Design","","2008","","","129","134","To reduce the miss rate of the instruction cache, a hardware-assisted loop optimization method is proposed in this paper. This method utilizes the hardware/software co-design strategy on the behavior level. Especially, this method is equipped with the specific instruction set to limit the cache misses, which can be viewed as a set of hardware for special purposes. Then based on the specific instruction set, a scheduling process is integrated which reduces the cache miss rate through the code transformation. Finally, a set of benchmarks from MediaBench1.0 are tested on the SimpleScalar platform to assist the proposed method. The final experiments indicate that 26% enhancement can be obtained for the cache miss reduction, where the specific instruction generation and the scheduling processes contribute about 23% and 3% respectively.","","978-1-4244-1650-9978-1-4244-1651","10.1109/CSCWD.2008.4536969","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4536969","Cache Miss;Hardware/software co-design;CSCW design;ASIP","Hardware;Application specific processors;Equations;Optimization methods;Microprocessors;Optimized production technology;Analytical models;Power system modeling;Computer science;Production systems","hardware-software codesign;instruction sets","hardware-assisted loop optimization;instruction cache;hardware/software codesign strategy;instruction set;scheduling process;cache miss reduction","","","14","","","","","","IEEE","IEEE Conferences"
"Automatic fault detection and diagnosis in complex software systems by information-theoretic monitoring","M. Jiang; M. A. Munawar; T. Reidemeister; P. A. S. Ward","Shoshin Distributed Systems Group, E&CE Department, University of Waterloo, Ontario N2L 3G1, Canada; Shoshin Distributed Systems Group, E&CE Department, University of Waterloo, Ontario N2L 3G1, Canada; Shoshin Distributed Systems Group, E&CE Department, University of Waterloo, Ontario N2L 3G1, Canada; Shoshin Distributed Systems Group, E&CE Department, University of Waterloo, Ontario N2L 3G1, Canada","2009 IEEE/IFIP International Conference on Dependable Systems & Networks","","2009","","","285","294","Management metrics of complex software systems exhibit stable correlations which can enable fault detection and diagnosis. Current approaches use specific analytic forms, typically linear, for modeling correlations. In this paper we use normalized mutual information as a similarity measure to identify clusters of correlated metrics, without knowing the specific form. We show how we can apply the Wilcoxon rank-sum test to identify anomalous behaviour. We present two diagnosis algorithms to locate faulty components: RatioScore, based on the Jaccard coefficient, and SigScore, which incorporates knowledge of component dependencies. We evaluate our mechanisms in the context of a complex enterprise application. Through fault injection experiments, we show that we can detect 17 out of 22 faults without any false positives. We diagnose the faulty component in the top five anomaly scores 7 times out of 17 using SigScore, which is 40% better than when system structure is ignored.","1530-0889;2158-3927","978-1-4244-4422-9978-1-4244-4421","10.1109/DSN.2009.5270324","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5270324","self-managing systems;fault detection and diagnosis;information theory;statistical techniques","Fault detection;Fault diagnosis;Software systems;Computerized monitoring;Entropy;Predictive models;Automatic testing;Clustering algorithms;Information theory;Fault location","fault diagnosis;fault tolerant computing;information theory;software maintenance;statistical analysis","automatic fault detection system;complex software system;information theoretic monitoring;management metrics;fault diagnosis;normalized mutual information;Wilcoxon rank-sum test;anomalous behaviour identification;RatioScore component;Jaccard coefficient;SigScore component;complex enterprise application","","18","19","","","","","","IEEE","IEEE Conferences"
"Automated Trace Analysis of Discrete-Event System Models","P. Kemper; C. Tepper","College of William and Mary, Williamsburg; ITGAIN Consulting, Hanover","IEEE Transactions on Software Engineering","","2009","35","2","195","208","In this paper, we describe a novel technique that helps a modeler gain insight into the dynamic behavior of a complex stochastic discrete event simulation model based on trace analysis. We propose algorithms to distinguish progressive from repetitive behavior in a trace and to extract a minimal progressive fragment of a trace. The implied combinatorial optimization problem for trace reduction is solved in linear time with dynamic programming. We present and compare several approximate and one exact solution method. Information on the reduction operation as well as the reduced trace itself helps a modeler to recognize the presence of certain errors and to identify their cause. We track down a subtle modeling error in a dependability model of a multi-class server system to illustrate the effectiveness of our approach in revealing the cause of an observed effect. The proposed technique has been implemented and integrated in Traviando, a trace analyzer to debug stochastic simulation models.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.75","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4620122","Testing and Debugging;Simulation;Trace analysis;Cycle reduction;Testing and Debugging;Simulation;Trace analysis;Cycle reduction","Discrete event systems;Discrete event simulation;Context modeling;Communication system software;Software performance;Automatic control;Control systems;Stochastic systems;Debugging;Data mining","combinatorial mathematics;discrete event simulation;dynamic programming;program diagnostics","automated trace analysis;discrete event system models;dynamic behavior;complex stochastic discrete event simulation model;minimal progressive fragment;combinatorial optimization problem;trace reduction;linear time;dynamic programming;reduction operation;modeling error;dependability model;multiclass server system;Traviando;trace analyzer;stochastic simulation model","","12","22","","","","","","IEEE","IEEE Journals & Magazines"
"Acquisition Times in Magnetic Resonance Imaging: Optimization in Clinical Use","G. Andria; F. Attivissimo; G. Cavone; A. M. L. Lanzolla","NA; NA; NA; NA","IEEE Transactions on Instrumentation and Measurement","","2009","58","9","3140","3148","Nuclear magnetic resonance imaging is a routine clinical system used for whole-body patient scanning that provides 3D images. Recent technological innovations have encouraged the use of this technology for noninvasive coronary, heart, and chest investigation or for research applications, but the image quality of this technique depends on several factors. Some parameters are linked to the apparatus designed to acquire the magnetic resonance image, whereas others can be controlled by the user. In this paper, the authors analyze the software-controlled magnetic-resonance-imaging parameters to reduce the health examination acquisition time by assuring a good quality of the images. This objective is of fundamental importance to both increase the number of clinical tests produced with this equipment and to reduce the radiation doses in the patients. For this purpose, the parameters that influence the time acquisition and the signal-to-noise ratio were investigated, and a software platform for optimizing the imaging acquisition time was developed.","0018-9456;1557-9662","","10.1109/TIM.2009.2016888","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5196790","Biological tissues;biomedical signal analysis;image segmentation;magnetic resonance imaging (MRI);optimization methods;simulation;time-optimal control","Magnetic resonance imaging;Technological innovation;Heart;Application software;Image quality;Magnetic resonance;Image analysis;Magnetic analysis;Testing;Signal to noise ratio","biomedical equipment;biomedical MRI;medical computing;optimisation","magnetic resonance imaging;health examination acquisition time;clinical system;whole-body patient scanning;optimization;image quality;software-controlled magnetic-resonance-imaging parameter;signal-to-noise ratio;biomedical equipment","","16","36","","","","","","IEEE","IEEE Journals & Magazines"
"Design of Examination Paper Generating System from Item Bank by Using Genetic Algorithm","W. Huang; Z. Wang","NA; NA","2008 International Conference on Computer Science and Software Engineering","","2008","5","","1323","1325","Based on the activity for building the foundation and application of computer, a common used automatic examination paper generating system is designed. This paper introduces how to design the database of item bank and establish the mathematic model of examination paper generating system, and discusses the method of finding the solution. To solve the model, the multiple objectives are translated into single objective by using genetic algorithm.","","978-0-7695-3336","10.1109/CSSE.2008.47","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4723153","Item bank;Automatic Generating test paper;Genetic Algorithm;Multi-Objective Optimization","Algorithm design and analysis;Genetic algorithms;Computer science;Paper technology;Buildings;Application software;Computer applications;Software engineering;Software design;Educational institutions","database management systems;document handling;educational administrative data processing;genetic algorithms","item bank;genetic algorithm;automatic examination paper generating system;database;mathematic model","","7","6","","","","","","IEEE","IEEE Conferences"
"Function Inlining Algorithm for Program Analysis","T. Wang; X. Su; P. Ma","NA; NA; NA","2009 International Conference on Computational Intelligence and Software Engineering","","2009","","","1","4","Most of existing inlining algorithms are used in optimizing compilers and are not suitable for program analysis. Therefore, an inlining algorithm based on program dependence graph is proposed. It uses simple function call tree to determine the sequence of inlining and adopts program dependence graph as the intermediate representation for a program. Inline expansion is performed on program dependence graphs, and a single program dependence graph without call node is produced in the end, so that the original program is transformed into a semantically equivalent program that is free of function invocation. This algorithm has already been applied to the code normalization process of an automatic grading system of student programs and a similar code detection system. Test results show that it can improve the variation removal rate of the code normalization and facilitate program analysis.","","978-1-4244-4507","10.1109/CISE.2009.5364393","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5364393","","Algorithm design and analysis;Optimizing compilers;Tree graphs;Computer science;Testing;Software maintenance;Functional programming;Impedance","program diagnostics;trees (mathematics)","function inlining algorithm;program analysis;optimizing compiler;program dependence graph;function call tree","","","13","","","","","","IEEE","IEEE Conferences"
"Application of flow field simulation technique to the study of exhaust noise of car","Lu Lirong; Jin Xiaoxiong; Peng Wei; He Wei","College of Automotive Engineering, Tongji University, Shanghai, China; College of Automotive Engineering, Tongji University, Shanghai, China; College of Automotive Engineering, Tongji University, Shanghai, China; College of Automotive Engineering, Tongji University, Shanghai, China","2008 IEEE Vehicle Power and Propulsion Conference","","2008","","","1","5","A new method is developed to solve the exhaust noise problem. Aiming at studying on the exhaust noise of car, firstly initial analysis is made on the flow field of exhaust muffler of car by Computational Fluid Dynamics (CFD) software Fluent in this paper. Then structure optimization solution is brought forward and simulation analysis is made on the solution. The CFD method and experimental results show good agreement through test, which proves that the optimization solution is effective. The noise level of the end of exhaust muffler is decreased. Thus it improves the overall acoustic performance.","1938-8756","978-1-4244-1848-0978-1-4244-1849","10.1109/VPPC.2008.4677661","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4677661","Exhaust Noise;CFD;Flow Field;Big Auxiliary Muffler;Noise Test","Exhaust systems;Acoustic noise;Computational fluid dynamics;Solid modeling;Boundary conditions;Educational institutions;Automotive engineering;Acoustic testing;Vehicles;Engines","automotive components;computational fluid dynamics;exhaust systems;flow simulation;optimisation;silencers","flow field simulation technique;exhaust noise;car;exhaust muffler;computational fluid dynamics;CFD software Fluent;structure optimization solution;acoustic performance","","1","10","","","","","","IEEE","IEEE Conferences"
"Training Bao Game-Playing Agents using Coevolutionary Particle Swarm Optimization","J. Conradie; A. P. Engelbrecht","Department of Computer Science, University of Pretoria, Pretoria, South Africa; Department of Computer Science, University of Pretoria, Pretoria, South Africa, engel@cs.up.ac.za","2006 IEEE Symposium on Computational Intelligence and Games","","2006","","","67","74","Bao, an African board game of the Mancala family, is a complex two-player game with a very large search space and complex rule set. The success of game tree approaches to create game-playing agents rests heavily on the usually handcrafted, static evaluation function. One of the first steps towards using a game tree is to design an appropriate, efficient evaluation function. This paper investigates the effectiveness of a revolutionary particle swarm optimization (PSO) approach to evolve the evaluation function for the game of Bao. This approach uses a PSO algorithm to evolve a neural network as evaluation function, using an unsupervised, competitive learning approach. The revolutionary approach to evolving game-playing agents assumes no prior knowledge of game strategies. The only domain specific information used by the model are the rules of the game, and the outcomes of games played. The performance of the evolved game-playing agents is compared to a game tree-based agent using a handcrafted evaluation function, as well as a player that makes random moves. Results show that the coevolutionary PSO approach succeeded in learning playing strategies for Bao","2325-4270;2325-4289","1-4244-0464","10.1109/CIG.2006.311683","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4100110","Bao;Particle swarm optimization;coevolution","Particle swarm optimization;Neural networks;Computer science;Africa;Humans;Testing;Genetic programming","computer games;evolutionary computation;games of skill;multi-agent systems;neural nets;particle swarm optimisation;software agents;trees (mathematics);unsupervised learning","Bao game-playing agent training;coevolutionary particle swarm optimization;Mancala African board game;two-player game;game tree;evaluation function design;neural network evolution;unsupervised competitive learning;multiagent systems","","8","24","","","","","","IEEE","IEEE Conferences"
"Analysis and optimization of fault-tolerant embedded systems with hardened processors","V. Izosimov; I. Polian; P. Pop; P. Eles; Z. Peng","Dept. of Computer and Inform. Science, Linköping University, SE-581 83, Sweden; Institute for Computer Science, Albert-Ludwigs-University of Freiburg, D-79110 Freiburg im Breisgau, Germany; Dept. of Informatics and Math. Modelling, Technical University of Denmark, DK-2800 Kongens Lyngby, Denmark; Dept. of Computer and Inform. Science, Linköping University, SE-581 83, Sweden; Dept. of Computer and Inform. Science, Linköping University, SE-581 83, Sweden","2009 Design, Automation & Test in Europe Conference & Exhibition","","2009","","","682","687","In this paper we propose an approach to the design optimization of fault-tolerant hard real-time embedded systems, which combines hardware and software fault tolerance techniques. We trade-off between selective hardening in hardware and process re-execution in software to provide the required levels of fault tolerance against transient faults with the lowest-possible system costs. We propose a system failure probability (SFP) analysis that connects the hardening level with the maximum number of re-executions in software. We present design optimization heuristics, to select the fault-tolerant architecture and decide process mapping such that the system cost is minimized, deadlines are satisfied, and the reliability requirements are fulfilled.","1530-1591;1558-1101","978-1-4244-3781-8978-3-9810-8015","10.1109/DATE.2009.5090752","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5090752","","Fault tolerant systems;Embedded system;Circuit faults;Costs;Radiation hardening;Hardware;Fault tolerance;Real time systems;Error analysis;Design optimization","embedded systems;probability;safety-critical software;software fault tolerance","hardened processors;fault-tolerant hard real-time embedded systems;selective hardening;transient faults;system failure probability analysis;fault-tolerant architecture;decide process mapping","","20","24","","","","","","IEEE","IEEE Conferences"
"Graphics optimization for J2ME compatible mobile phones","I. Valdin","Motorola Corporation, Motorola GSG-Russia, 12 Sedova Street, St. Petersburg, 192019, Russia. e-mail: Igor.Valdin@motorola.com","2006 IEEE International Symposium on Consumer Electronics","","2006","","","1","4","Nowadays the high-resolution color screen becomes an inseparable component of modern mobile phones. Popular games, movies and 3D graphics capabilities require effective image rendering which is a task for both hardware and software engineers. The article is dedicated to the acceleration of low level graphics for J2ME based applications. It covers several aspects: algorithmic optimizations, use of embedded hardware accelerators, graphics performance measurement. Some high speed 2D graphics algorithms and most popular J2ME graphics benchmarks are considered. The issues related to the use of hardware graphics accelerators are discussed. And finally, practical results of low-level graphics optimization on Motorola phones are shown","0747-668X;2159-1423","1-4244-0216","10.1109/ISCE.2006.1689462","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1689462","graphics optimization;j2me;mobile phones;benchmarks","Graphics;Mobile handsets;Hardware;Acceleration;Java;Rendering (computer graphics);Manufacturing;Displays;Engineering drawings;Motion pictures","benchmark testing;image resolution;mobile handsets;rendering (computer graphics)","2D graphics optimization;J2ME compatible mobile phones;high-resolution color screen;image rendering;embedded hardware accelerator;graphics performance measurement;benchmarks;Motorola phones","","","11","","","","","","IEEE","IEEE Conferences"
"Evolutionary multi-objective optimization for generating artificial creature’s personality","Chi-Ho Lee; Kang-Hee Lee; Jong-Hwan Kim","Department of Electrical Engineering and Computer Science Korea Advanced Institute of Science and Technology (KAIST) 373-1, Guseong-Dong, Yuseong-Gu, Daejeon, 305-701, Republic of Korea; Application Technology Lab., Telecommunication R&D Center Telecommunication Network Business Samsung Electronics Co., Ltd. Korea; Department of Electrical Engineering and Computer Science Korea Advanced Institute of Science and Technology (KAIST) 373-1, Guseong-Dong, Yuseong-Gu, Daejeon, 305-701, Republic of Korea","2007 IEEE Congress on Evolutionary Computation","","2007","","","2450","2455","This paper proposes the evolutionary generation of an artificial creature's personality by using the concept of multi-objective optimization. The artificial creature has its own genome and in which each chromosome consists of many genes that contribute to defining its personality. The large number of genes allows for a highly complex system, however it becomes increasingly difficult and time-consuming to ensure reliability, variability and consistency for the artificial creature's personality while manually assigning gene values for the individual genome. Moreover, there needs user's preference to obtain artificial creature's personality by using evolutionary generation. Preference is strongly depend on each user and most of them would have difficulty to define their preference as a fitness function. To solve this problem, this paper proposes multi-objective generating process of an artificial creature's personality. Genome set is evolved by applying strength Pareto evolutionary algorithm (SPEA). To facilitate the individuality of generated artificial creature, complement of (1-k) dominance and pruning method considering deviation are proposed. Obtained genomes are tested by using an artificial creature, Rity in the virtual 3D world created in a PC.","1089-778X;1941-0026","978-1-4244-1339-3978-1-4244-1340","10.1109/CEC.2007.4424778","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4424778","","Genomics;Bioinformatics;Biological cells;Evolutionary computation;Humans;Application software;Research and development;Software testing;Autonomous agents;Robot sensing systems","artificial life;evolutionary computation;Pareto optimisation;software agents;virtual reality","evolutionary multiobjective optimization;artificial creature personality generation;evolutionary generation;artificial creature genome;highly complex system;strength Pareto evolutionary algorithm;Rity;virtual 3D world","","3","13","","","","","","IEEE","IEEE Conferences"
"Bridging RTL and gate: correlating different levels of abstraction for design debugging","Eric Cheung; Xi Chen; Furshing Tsai; Yu-Chin Hsu; Harry Hsieh","University of California, 92521, USA; Novas Software, Inc. San Jose, California 95110, USA; Novas Software, Inc. San Jose, California 95110, USA; Novas Software, Inc. San Jose, California 95110, USA; University of California, 92521, USA","2007 IEEE International High Level Design Validation and Test Workshop","","2007","","","73","80","In order to help designers debug and verify a Gate-Level design that is generated from a Register-Transfer-Level (RTL) reference model, it is important to bridge the knowledge gap between the two levels of abstraction. In this paper, we present a comprehensive approach to establish correspondence of design objects between a Gate-Level implementation and its golden reference model specified at RTL. We consider both common logic synthesis transformations and advanced logic optimizations that are applied in the generation of the Gate-Level implementation, while not being restricted to any specific synthesis tool. Our approach integrates a set of techniques to compare the similarities in names, structures, and functions between the Gate-Level implementation and the RTL counterpart We use large industrial designs to demonstrate the effectiveness of our approach and show how our design correlation tool can help designers solve their problems such as Engineering Change Order, Timing Closure, and Emulation Visualization.","1552-6674","978-1-4244-1480","10.1109/HLDVT.2007.4392790","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4392790","","Timing;Signal design;Signal processing;Emulation;Logic;Visualization;Signal synthesis;Software debugging;Bridges;Design engineering","high level synthesis;logic testing","gate-level design debugging;gate-level design verification;register-transfer-level model;RTL reference model;logic synthesis transformations;logic optimizations","","","12","","","","","","IEEE","IEEE Conferences"
"Embedding Smart Buffers for Window Operations in a Stream-Oriented C-to-VHDL Compiler","F. Diet; E. H. D'Hollander; K. Beyls; H. Devos","NA; NA; NA; NA","4th IEEE International Symposium on Electronic Design, Test and Applications (delta 2008)","","2008","","","142","147","Important classes of algorithms which can benefit from the advantages of C-to-VHDL compiling are window operations. These execute a number of instructions on a large amount of array data. Since arrays are usually translated into FPGA block memory structures, it is important to minimize the required number of block memory accesses. Recently, a smart buffer has been introduced, in which a number of past and present array elements can be temporarily stored to be reused over a number of different loop nest iterations. In this paper, the smart buffer approach is analysed for use in the stream- oriented Impulse-C compiler. Experimental automatic generation of VHDL code for this buffer is described. The smart buffer is then linked with the VHDL code generated by the Impulse-C compiler to obtain data efficient designs.","","978-0-7695-3110","10.1109/DELTA.2008.111","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4459528","FPGA;High-performance computing;C-to-VHDL compiling;Impulse-C;Smart buffer","Hardware;Field programmable gate arrays;Electronic equipment testing;Optimizing compilers;System testing;Information systems;Buffer storage;Software testing;Software algorithms;Design methodology","embedded systems;field programmable gate arrays;hardware description languages;hardware-software codesign;program compilers","smart buffer;window operations;stream-oriented C-to-VHDL compiler;FPGA block memory structure;stream-oriented impulse-C compiler;VHDL automatic code generation","","","11","","","","","","IEEE","IEEE Conferences"
"An Optimized Strategy of Service Negotiation","T. Sun; S. Li; Q. Zhu","Chongqing University, China; Chongqing University, China; Chongqing University, China","2006 Second IEEE International Symposium on Service-Oriented System Engineering (SOSE'06)","","2006","","","210","214","In service-oriented computing, negotiation can help service provider and consumer reaching an agreement between services. This paper discusses the optimization of utility in service negotiation. In one-to-one negotiation with multiple issues, two parties can reach the final equilibrium point using the concession negotiation strategies. Using traditional concession strategies, each party cares only about the maximization of its own utility rather than of the joint utility. In many applications, this does not ensure each party can reach the maximum point of utility. We propose an improved concession strategy in this paper, which allows further optimization after reaching the preliminary results of negotiation, so that the joint utility between two parties can be improved. This strategy can be used in the applications that need dynamic finding, evaluating, selecting, and binding of services, which helps service provider and consumer reaching an agreement. The results of simulation show that the strategy of joint utility optimization is effective","","0-7695-2726","10.1109/SOSE.2006.8","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4027140","","Distributed computing;Collaboration;Systems engineering and theory;Service oriented architecture;Sun;Educational institutions;Computer science;Testing;Quality of service;Pricing","software architecture;software development management","service negotiation;service-oriented computing;service provider;service consumer;concession negotiation;joint utility optimization","","4","12","","","","","","IEEE","IEEE Conferences"
"Using citations for ranking in digital libraries","P. Ingwersen; B. Larsen","Royal School of Library and Information Science, Denmark; Royal School of Library and Information Science, Denmark","Proceedings of the 6th ACM/IEEE-CS Joint Conference on Digital Libraries (JCDL '06)","","2006","","","370","370","Citations drawn from the list of references of scholarly documents may have many uses in digital libraries, including reference linking. In this paper, we present the main results from an experiment designed to exploit the potential power of bibliographical citations for information retrieval in digital libraries","","1-59593-354","10.1145/1141753.1141865","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4119185","citation indexing;information retrieval","Software libraries;Testing;Information retrieval;Indexing;Joining processes;Search engines;Information science;Databases;Bibliometrics;Frequency","citation analysis;digital libraries;information retrieval;search engines","digital library ranking;scholarly documents;reference linking;bibliographical citations;information retrieval","","","6","","","","","","IEEE","IEEE Conferences"
"A method based on Kolmogorov complexity to improve the efficiency of strategy optimization with limited memory space","Qing-Shan Jia; Qian-Chuan Zhao; Yu-Chi Ho","Dept. of Autom., Tsinghua Univ., Beijing, China; Dept. of Autom., Tsinghua Univ., Beijing, China; Dept. of Autom., Tsinghua Univ., Beijing, China","2006 American Control Conference","","2006","","","6 pp.","","The pervasive application of digital computer in control and optimization techniques forces us to consider the constraint of limited memory space when dealing with large scale practical systems. As an example, we consider the famous Witsenhausen counterexample with the new constraint of limited memory space in this paper. The main difficulty is how to sample strategies that can be stored in the given memory space efficiently. The concept of Kolmogorov complexity measures the minimal memory space to store a strategy (i.e., simple strategies), but is incomputable. To overcome this difficulty, we propose a method based on ordered binary decision diagram to sample only simple strategies. Besides the high sampling efficiency which is demonstrated by numerical testing, the proposed sampling method can be easily combined with optimization algorithms and performance evaluation techniques. As an example, we show how to combine ordinal optimization, numerical integration, and the proposed sampling method to solve the Witsenhausen problem with the constraint of limited memory space. We hope this work can shed some insights to computer-based optimization problems with memory space constraint in a more general situation","0743-1619;2378-5861","1-4244-0209-31-4244-0210","10.1109/ACC.2006.1657194","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1657194","","Optimization methods;Memory management;Constraint optimization;Sampling methods;Application software;Computer applications;Pervasive computing;Digital control;Force control;Control systems","binary decision diagrams;computational complexity;integration;optimisation;sampling methods;storage management","Kolmogorov complexity;strategy optimization;ordered binary decision diagram;sampling method;performance evaluation;ordinal optimization;numerical integration;Witsenhausen problem;limited memory space","","1","16","","","","","","IEEE","IEEE Conferences"
"Research on DCW-PSO Algorithm and Its Application in Intelligent Transportation Systems","W. Li; K. Zhu","NA; NA","2009 Fifth International Conference on Natural Computation","","2009","4","","393","397","Based on the urban traffic network's real-time characteristic, a particle swarm optimization with dynamically changing weight (DCW-PSO) is adopted in order to search the optimal path quickly and efficiently. This algorithm brings in the factors of evolution speed and aggregation degree, based on which the weight is changed dynamically in each iteration process. Finally, the DCW-PSO proposed in this paper and the algorithm of particle swarm optimization with linearly decreasing weight (LCD-PSO) are tested separately in the process of searching the optimal path, and the conclusion is drew out that DCW-PSO is more suitable to search the optimal path in intelligent transportation systems.","2157-9555;2157-9563","978-0-7695-3736","10.1109/ICNC.2009.301","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5363814","particle swarm optimization;intelligent transportation systems;dynamically changing weight","Intelligent transportation systems;Particle swarm optimization;Telecommunication traffic;Laboratories;Roads;Application software;Computer vision;Educational technology;Computer science education;Real time systems","particle swarm optimisation;traffic engineering computing","particle swarm optimization;dynamically changing weight;intelligent transportation systems;linearly decreasing weight;urban traffic network real-time characteristic","","","8","","","","","","IEEE","IEEE Conferences"
"Hardware aging-based software metering","F. Dabiri; M. Potkonjak","Computer Science Department, University of California Los Angeles, USA; Computer Science Department, University of California Los Angeles, USA","2009 Design, Automation & Test in Europe Conference & Exhibition","","2009","","","460","465","Reliable and verifiable hardware, software and content usage metering (HSCM) are of primary importance for wide segments of e-commerce including intellectual property and digital rights management. We have developed the first HSCM technique that employs intrinsic aging properties of components in modern and pending integrated circuits (ICs) to create the first self-enforceable HSCM approach. There are variety of hardware aging techniques that range from electro-migration in wires to slow-down of crystal-based clocks. We focus on transistor aging due to negative bias temperature instability (NBTI) effects where the delay of gates increases proportionally to usage times. We address the problem of how we can measure the amount of time a particular licensed software (LS) is used by designing an aging circuitry and exposing it to the unique inputs associated with each LS. If a particular LS is used longer than specified, it automatically disables itself. Our novel HSCM technique uses a multi-stage optimization problem of computing the delays of gates, their aging degradation factors, and finally LS usage using convex programming. The experimental results show not just viability of the technique but also surprisingly high accuracy in the presence of measurement noise and imperfect aging models. HSCM can be used for many other business and engineering applications such as power minimization, software evaluation, and processor design.","1530-1591;1558-1101","978-1-4244-3781-8978-3-9810-8015","10.1109/DATE.2009.5090709","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5090709","","Hardware;Aging;Integrated circuit reliability;Intellectual property;Content management;Wires;Clocks;Negative bias temperature instability;Niobium compounds;Titanium compounds","ageing;computer crime;convex programming;software engineering","hardware aging-based;software metering;content usage metering;e-commerce;intellectual property;digital rights management;negative bias temperature instability;licensed software;multistage optimization problem;aging degradation factors;convex programming","","11","12","","","","","","IEEE","IEEE Conferences"
"Internet traffic management based on AMCC network processor","Xiaona Niu; Yunfei Guo; Jin Zhang; Chao Wang","National Digital Switching System Engineering & Technology Research Center(NDSC), No.7, JianXue Street, Zhengzhou, China 450002; National Digital Switching System Engineering & Technology Research Center(NDSC), No.7, JianXue Street, Zhengzhou, China 450002; National Digital Switching System Engineering & Technology Research Center(NDSC), No.7, JianXue Street, Zhengzhou, China 450002; National Digital Switching System Engineering & Technology Research Center(NDSC), No.7, JianXue Street, Zhengzhou, China 450002","2008 11th IEEE International Conference on Communication Technology","","2008","","","533","536","The remarkable and continuous increase in link speed and application variety of Internet calls for new QoS provision technology that is both efficient and flexible. Being a kind of application-specific processor optimized for network computation, network processor (NP) combines the high performance of hardware and the flexibility of software, which exactly meets the demands of nowadays Internet traffic management and QoS provision. This paper originally presents an efficient traffic management mechanism based on AMCC network processor. The data-plane software architecture, the manipulation process, especially some key components such as accurate traffic classification, flexible access control and three-step scheduling are discussed in detail. We implement the traffic management mechanism based on NP3450 network processor and test it using Spirent AX/4000 broadband test system. The test results show that our proposal can efficiently provide QoS guarantee for various traffic category on OC-48 links.","","978-1-4244-2250-0978-1-4244-2251","10.1109/ICCT.2008.4716108","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4716108","traffic management;network processor;AMCC;data-plane","IP networks;Telecommunication traffic;Communication system traffic control;Computer network management;System testing;Application software;Internet;Application specific processors;Computer networks;High performance computing","access control;Internet;microprocessor chips;quality of service;software architecture;telecommunication network management;telecommunication traffic","Internet traffic management;AMCC network processor;QoS provision technology;application-specific processor;network computation;data-plane software architecture;access control;three-step scheduling;NP3450 network processor;Spirent AX/4000 broadband test;OC-48 links","","1","9","","","","","","IEEE","IEEE Conferences"
"Models of Circuits and Their Elements for Functional Decomposition and Verification at the Stage of Computer Systems' PC Boards Design","B. Al-Zabi; A. Kernytskyy; S. Tkatchenko","CAD/CAM Department, Lviv Polytechnic National University, 12, S. Bandery Str., Lviv, 79013, UKRAINE; CAD/CAM Department, Lviv Polytechnic National University, 12, S. Bandery Str., Lviv, 79013, UKRAINE, E-mail: akern@dr.com; CAD/CAM Department, Lviv Polytechnic National University, 12, S. Bandery Str., Lviv, 79013, UKRAINE","2007 9th International Conference - The Experience of Designing and Applications of CAD Systems in Microelectronics","","2007","","","286","287","The possibilities of application of theoretical and graph models of circuits when solving tasks of optimization and verification in planning of the computer systems are considered. The use of the nets weighed at vertexes is offered.","","966-533-587","10.1109/CADSM.2007.4297552","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4297552","circuit;functional verification;computer systems;graphs","Circuit topology;Heuristic algorithms;Application software;Design optimization;Set theory;Graph theory;Polynomials;Programming;Computer aided manufacturing;CADCAM","circuit layout CAD;circuit optimisation;formal verification;graph theory;printed circuit layout;printed circuit testing","circuit models;functional lay-decomposition;computer systems' PC boards design;graph models;circuit optimization","","","2","","","","","","IEEE","IEEE Conferences"
"Performance impact analysis with KPP using application response measurement in E-government systems","N. Yoo","DoD/HA, Falls Church, Virginia, USA","2009 IEEE International Conference on Software Maintenance","","2009","","","503","506","In this paper, the performance impact analysis of e-government systems with key performance parameters is being considered. Meaningful impact analysis in sustained government systems is required for considering non-functional requirements and functional requirements. Performance requirements are a critical component of non-functional areas. For example, if a new system change is set to the system, the impact in terms of the response time must be implemented in each sub-system. In this paper, an XML-based framework can be used to analyze performance impacts on sub-systems and can provide a scheme to enhance impact analysis by performance monitoring using application response measurement. Through a health system example as a case study, a performance requirement model to describe extended trees and adapting analysis result of performance monitoring using application response measurement and XML tree representation are addressed. This paper also proposes a scheme for prioritized processing and an algorithm for effectively enhancing impact analysis in a timely fashion.","1063-6773","978-1-4244-4897-5978-1-4244-4828","10.1109/ICSM.2009.5306282","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306282","","Performance analysis;Electronic government;Systems engineering and theory;Delay;Monitoring;XML;Reliability engineering;Time measurement;System testing;Algorithm design and analysis","government data processing;software performance evaluation;tree data structures;XML","performance impact analysis;application response measurement;e-government systems;XML-based framework;performance monitoring;extended trees;prioritized processing;key performance parameter","","","5","","","","","","IEEE","IEEE Conferences"
"An Approach of Workflow Optimization of Global Supply Chain Based on Object Oriented Petri Net Paper Title","B. Xian-hua; X. Wei","NA; NA","2009 International Conference on Information Technology and Computer Science","","2009","1","","167","170","This paper discussed the workflow optimization of global supply chain, different from the past studies based on game theory. We setup the object oriented Petri net to model the global supply chain, and gave the modeling steps and activity test algorithm. Base on the I/O of OPN tool, we designed the service evolution strategy by using SOA framework. The study also gives the steps of SOA services to the response of request from kinds of customers.","","978-0-7695-3688","10.1109/ITCS.2009.41","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5190042","supply chain;Petri net;workflow","Supply chains;Production;Raw materials;Partial response channels;Object oriented modeling;Service oriented architecture;Chemical industry;Fires;Information technology;Computer science","game theory;Petri nets;software architecture;supply chain management;Web services;workflow management software","workflow optimization;global supply chain;object oriented Petri net paper title;game theory;modeling steps;activity test algorithm;service evolution strategy;service oriented architecture","","","11","","","","","","IEEE","IEEE Conferences"
"Adaptive Test Question Selection for Web-Based Educational System","O. Vozár; M. Bieliková","NA; NA","2008 Third International Workshop on Semantic Media Adaptation and Personalization","","2008","","","164","169","In this paper we present a method proposed to select test questions adapting to individual needs of students in the context of Web-based educational system. It functions as a combination of three particular methods. First one is based on course structure and focuses on the selection of the most appropriate topic for learning, second uses the Item Response Theory to select k-best questions with adequate difficulty for particular learner and the last is based on usage history and prioritizes questions according to specific strategies, e.g. to filter out the questions that was recently asked. We describe how these methods evaluate user answers to gather information concerning their characteristics for more precise selection of further questions. We evaluated proposed method within our Web-based system called Flip on domain of functional programming.","","978-0-7695-3444","10.1109/SMAP.2008.15","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724867","adaptive test question selection;web-based educational system;item response theory;lerning programming","System testing;Adaptive systems;Feedback;Software testing;Informatics;History;Software engineering;Information technology;Educational technology;Filtering theory","courseware;Internet","Web-based educational system;adaptive test question selection;course structure;item response theory;functional programming;k-best question selection","","4","9","","","","","","IEEE","IEEE Conferences"
"A High Speed DMA Transaction Method for PCI Express Devices","Y. Peng; B. Li; D. Liu; X. Peng","NA; NA; NA; NA","2009 IEEE Circuits and Systems International Conference on Testing and Diagnosis","","2009","","","1","4","PCI Express is being more and more widely deployed for its attractive bandwidth in virtual instrument design. However, very few PCI express transaction methods can be found in literature. So, this paper presents a novel PCI Express DMA transaction method based on bridge chip PEX 8311. Furthermore, we propose a new method to optimize PCI Express DMA Transaction through improving both bus-efficiency and DMA-efficiency. A novel FSM design to respond data and address cycles on PCI Express bus is introduced, and a continuous data burst is realized, which greatly promote bus-efficiency. As the foundation of the whole design, a successful 2.5G PCI Express interface design is presented. Then, to make a whole solution, in software design, WDM driver framework and three successful DMA optimizing options for PCI Express devices are presented to improve DMA-efficiency. At last, a FSM-based test to data transaction speed is proposed. Experiments show that method discussed here can reach a maximum DMA WRITE speed up to 166 MBytes/s and DMA READ speed up to 136 MBytes/s. Both of them exceed PCI theoretical maximum speed (133MBytes/s). In fact, this paper provides not noly a PCI Express example, but also PCI Express interface solution and DMA transaction method which can be directly extended into high speed PXI-Express applications and so forth. That is quite meaningful to next generation instrumentation.","2324-8475;2324-8491","978-1-4244-2587","10.1109/CAS-ICTD.2009.4960747","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4960747","","Bandwidth;Optimization methods;Instruments;Wavelength division multiplexing;Computer peripherals;Read-write memory;Automatic testing;Automatic control;Bridges;Software design","file organisation;peripheral interfaces;storage management chips;transaction processing","high speed DMA transaction method;PCI express devices;virtual instrument design;bridge chip PEX 8311;FSM design;direct memory access;byte rate 166 MByte/s;byte rate 136 MByte/s;byte rate 133 MByte/s","","6","7","","","","","","IEEE","IEEE Conferences"
"Numerical Simulation and Experiment of Tubular Pump Device Model","D. Zhang; W. Shi; B. Chen; X. Guan","NA; NA; NA; NA","2009 International Conference on Computational Intelligence and Software Engineering","","2009","","","1","4","The tubular pump device model including rotor, stator and inlet and outlet passages was simulated based on SIMPLEC arithmetic and standard k-¿ RNG turbulence model. The match of rotor and stator in tubular pump was optimized. The numerical results show that the device model has a steady flow field and high efficiency when the distance between rotor and stator is 0.11D<sub>2</sub>. With the distance increasing or decreasing, vortex zone appears near the vane exit, which results in unsteady flow field and efficiency decrease. Streamlines show that the remaining circulation still exists at stator outlet with spiral flow pattern. The optimized device model was manufactured and tested. The test results show that the efficiency of the optimized device model reaches 79.05% at blade adjustable angle of -2°, which could meet the design requirements.","","978-1-4244-4507","10.1109/CISE.2009.5363417","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5363417","","Numerical simulation;Blades;Impellers;Stators;Design optimization;Spirals;Testing;Gold;Lakes;Navier-Stokes equations","mechanical engineering;numerical analysis;optimisation;pumps;rotors;stators","numerical simulation;tubular pump device model;SIMPLEC arithmetic;k-¿ RNG turbulence model;rotor;stator","","","10","","","","","","IEEE","IEEE Conferences"
"Assessment of a Multi-Strategy Classifier for an Embedded Software System","Kehan Gao; Yudong Xiao; T. M. Khoshgoftaar; Kehan Gao; Yudong Xiao; T. M. Khoshgoftaar","NA; Florida Atlantic Univ., Boca Raton, FL; Florida Atlantic Univ., Boca Raton, FL; NA; Florida Atlantic Univ., Boca Raton, FL; Florida Atlantic Univ., Boca Raton, FL","2006 18th IEEE International Conference on Tools with Artificial Intelligence (ICTAI'06)","","2006","","","651","658","In this paper, a new classification model, RB2CBL, is proposed. Its structure and methodology are described. By cascading a rule-based (RB) model with two case-based learning (CBL) models, RB2CBL possesses the merits of both RB model and CBL model and restrains their drawbacks. In the RB2CBL model, the parameter optimization of the CBL models is essential, and the embedded genetic algorithm optimizer is used. In our case study, a dataset collected from initial releases of two large, Windowscopy-based embedded system applications, which were used primarily for customizing the configuration of wireless telecommunications products, is processed to investigate and evaluate the models. The results show that, by suitably choosing accuracy settings of the RB model, RB2CBL model outperforms the RB model alone without overfitting. In practice, the RB2CBL model effectively reduced the misclassification rates and improved prediction accuracy for the embedded software system","1082-3409;2375-0197","0-7695-2728","10.1109/ICTAI.2006.35","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4031956","rule-based model;case-based learning;genetic algorithm;multi-strategy classifier;software quality classification","Embedded software;Decision trees;Error analysis;Noise level;Genetic algorithms;Predictive models;Software quality;Classification tree analysis;System testing;Training data","embedded systems;genetic algorithms;knowledge based systems;learning (artificial intelligence);pattern classification","multistrategy classifier;embedded software system;classification model;RB2CBL;rule-based model;case-based learning models;genetic algorithm optimizer;wireless telecommunications products","","1","26","","","","","","IEEE","IEEE Conferences"
"Digital power estimation flow combining academic and industrial tools","R. Mehrotra; T. English; K. L. Man; E. Popovici; M. P. Schellekens","Centre for Efficiency-Oriented Languages (CEOL) and Department of Microelectronic Engineering, University College Cork (UCC), Ireland; Centre for Efficiency-Oriented Languages (CEOL) and Department of Microelectronic Engineering, University College Cork (UCC), Ireland; Centre for Efficiency-Oriented Languages (CEOL) and Department of Microelectronic Engineering, University College Cork (UCC), Ireland; Centre for Efficiency-Oriented Languages (CEOL) and Department of Microelectronic Engineering, University College Cork (UCC), Ireland; Centre for Efficiency-Oriented Languages (CEOL) and Department of Microelectronic Engineering, University College Cork (UCC), Ireland","2008 International SoC Design Conference","","2008","02","","II-89","II-92","We analyse how a power estimation and optimisation design flow from the academic domain may be integrated into an industrial design flow and how this integration creates an open-source environment in which new techniques can be developed and compared. In this integrated design flow, new tools are introduced and several practical issues such as technology libraries and the use of simulation data to derive realistic power estimation results are addressed. Experimental results (enabling power saving of over 40%) demonstrate the applicability and effectiveness of the integration of the two design flows. The integrated design flow provides a solid platform for future research in logic synthesis, power estimation and power optimisation.","","978-1-4244-2598-3978-1-4244-2599","10.1109/SOCDC.2008.4815691","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4815691","","Logic design;Design optimization;Hardware design languages;Libraries;Electronic design automation and methodology;Logic testing;Open source software;Solids;Logic programming;System testing","circuit CAD;logic CAD;public domain software","digital power estimation flow;academic tools;industrial tools;optimisation design flow;open-source environment;technology libraries","","2","14","","","","","","IEEE","IEEE Conferences"
"Simulation and Optimization for the Airport Passenger Flow","Y. Ju; A. Wang; H. Che","NA; NA; NA","2007 International Conference on Wireless Communications, Networking and Mobile Computing","","2007","","","6605","6608","Airport is an ideal application area of simulation. Passenger flow is very important for the airport waiting room management and operation. A simulation model is developed to analyse the airport waiting room using commercially available simulation software Arena 9.01 in this paper. The simulation model can be used to gain insights into the presence of bottlenecks and their causes, and to evaluate the key performance measures of the system. Through optimization, different resources can be reassigned. Therefore, it provides the basis for increasing utilization of facilities and improving service level.","2161-9646;2161-9654","1-4244-1311-71-4244-1312-5978-1-4244-1311-9978-1-4244-1312","10.1109/WICOM.2007.1621","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4341396","","Airports;Analytical models;Air traffic control;Traffic control;Educational technology;Throughput;Automation;System testing;Technology management;Application software","airports;optimisation;transportation","airport passenger flow;simulation model;airport waiting room","","5","5","","","","","","IEEE","IEEE Conferences"
"Application on job-shop scheduling with Genetic Algorithm based on the mixed strategy","Liang Xu; Wang Shuang; Huang Ming","Software Technology Institute, Dalian Jiao Tong University, 116028, China; Software Technology Institute, Dalian Jiao Tong University, 116028, China; Software Technology Institute, Dalian Jiao Tong University, 116028, China","2009 Chinese Control and Decision Conference","","2009","","","2007","2009","Adaptive genetic algorithm for solving job-shop scheduling problems has the defects of the slow convergence speed on the early stage and it is easy to trap into local optimal solutions, this paper introduces a time operator depending on the time evolution to solve this problem. Its purpose is to overcome the defect of adaptive genetic algorithm whose crossover and mutation probability can not make a corresponding adjustment with evolutionary process. Algorithm's structure is hierarchical, scheduling problems can be fully demonstrated the characteristics by using this strategy, not only improve the convergence rate but also maintain the diversity of the population, furthermore avoid premature. The population in the same layer evolve with two goals-time optimal and cost optimal at the same time, the basic genetic algorithm is applied between layers. The improved algorithm was tested by Muth and Thompson benchmarks, the results show that the optimized algorithm is highly efficient and improves both the quality of solutions and speed of convergence.","1948-9439;1948-9447","978-1-4244-2722-2978-1-4244-2723","10.1109/CCDC.2009.5191650","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5191650","Adaptive;Time operator;Hierarchic structure","Genetic algorithms;Genetic mutations;Cost function;Convergence;Algorithm design and analysis;Application software;Scheduling algorithm;Benchmark testing;Design optimization","genetic algorithms;job shop scheduling;probability","job-shop scheduling;adaptive genetic algorithm;time operator;time evolution;crossover probability;mutation probability;convergence rate;time optimal;cost optimal","","","8","","","","","","IEEE","IEEE Conferences"
"Applications of Performance Benchmarking to the Development of Signal Processing Systems Based on Personal Computer Technology","R. Inkol; C. Wilson; M. Eidus","DRDC Ottawa, Department of National Defence, Ottawa, Ontario K1A 0Z4. email: robert.inkol@drdc-rddc.gc.ca; DRDC Ottawa, Department of National Defence, Ottawa, Ontario K1A 0Z4. collin.wilson@drdc-rddc.gc.ca; Vantage Point International, 400 March Rd, Suite 210, Ottawa, Ontario K2K 3H4. email: meidus@vantpoint.com","2006 Canadian Conference on Electrical and Computer Engineering","","2006","","","41","45","Highly optimized libraries of vector and matrix math functions, such as the Intel integrated performance primitive (IPP) libraries, can be used to quickly and economically construct high performance signal processing systems built up around personal computer technology. This paper describes a portable benchmarking software suite that has been developed for benchmarking the performance obtainable with IPP implementations of common signal processing algorithms, including the fast Fourier transform (FFT) and finite impulse response (FIR) filter. The results presented provide useful insights into system design choices concerning algorithms, software architecture and processors","0840-7789","1-4244-0038-41-4244-0038","10.1109/CCECE.2006.277339","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4055086","Signal processing;Single Instruction Multiple Data;Fast Fourier Transform;multirate filter;benchmark","Application software;Signal processing;Microcomputers;Signal processing algorithms;Software libraries;Finite impulse response filter;Software performance;Fast Fourier transforms;Software algorithms;Software architecture","benchmark testing;fast Fourier transforms;FIR filters;mathematics computing;microcomputer applications;signal processing;software libraries","benchmarking;signal processing system;personal computer technology;Intel integrated performance primitive library;fast Fourier transform;finite impulse response filter","","1","7","","","","","","IEEE","IEEE Conferences"
"Enhancing Business Process Management with Simulation Optimization","J. April; M. Better; F. Glover; J. Kelly; M. Laguna","OptTek Systems, Inc., 1919 Seventh Street, Boulder, CO 80302, U.S.A. email april@OptTek.com; OptTek Systems, Inc., 1919 Seventh Street, Boulder, CO 80302, U.S.A. email better@OptTek.com; OptTek Systems, Inc., 1919 Seventh Street, Boulder, CO 80302, U.S.A. email glover@OptTek.com; OptTek Systems, Inc., 1919 Seventh Street, Boulder, CO 80302, U.S.A. email kelly@OptTek.com; OptTek Systems, Inc., 1919 Seventh Street, Boulder, CO 80302, U.S.A. e-mail: laguna@OptTek.com","Proceedings of the 2006 Winter Simulation Conference","","2006","","","642","649","A growing number of business process management software vendors are offering simulation capabilities to extend their modeling functions and enhance their analytical proficiencies. Simulation is promoted to enable examination and testing of decisions prior to actually making them in the ""real"" environment. In this paper, we illustrate how to optimize simulation models, by presenting two examples of simulation optimization using OptQuestreg. In the first case, we construct a discrete event simulation model of a hospital emergency room to determine a configuration of resources that results in the shortest average cycle time for patients. In the second case, we develop a simulation model to minimize staffing levels for personal claims processing in an insurance company. We then summarize some of the most relevant approaches that have been developed for the purpose of optimizing simulated systems and conclude with a metaheuristic black box approach that leads the field of practical applications","0891-7736;1558-4305","1-4244-0501-71-4244-0500","10.1109/WSC.2006.323141","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4117665","","Analytical models;Hospitals;Application software;Testing;Discrete event simulation;Insurance;Optimization methods;Virtual manufacturing;Companies;Software development management","business data processing;discrete event simulation;DP industry;insurance;medical administrative data processing","simulation optimization;business process management software vendors;OptQuest;discrete event simulation;hospital emergency room;shortest average cycle time;personal claims processing;insurance company","","14","8","","","","","","IEEE","IEEE Conferences"
"Optimizing the selection of representative configurations in verification of evolving product lines of distributed embedded systems","K. D. Scheidemann","BMW Car IT GmbH, Munchen, Germany","10th International Software Product Line Conference (SPLC'06)","","2006","","","75","84","Electronics and computer science play a more and more prominent role in automotive technology. In the future the prevalence of those new technologies and the customers' demand for individuality leads to tremendously large configuration spaces of vehicle control systems. To cope with the resulting complexity in verification, new strategies need to be explored. One likely future challenge is to determine a set of vehicle configurations, such that the successful verification of this small set implies the correctness of the entire product family. This paper presents a method to address this task, based on exploiting communalities in architecture and requirements. We introduce efficient algorithms with provable quality guarantees for the optimization problems of choosing the minimum set of configurations necessary to verify all possible configurations and choosing the best k configurations to maximize the verification coverage of the entire product family. We discuss extensions of our method which allow requirement priorities and the consideration of configuration costs, and present a technique for automatically determining communalities in architecture and requirements which can be exploited by our optimization methods. We demonstrate the effectiveness of our method on an indicator light system product family. In this example a configuration reduction by 60% can be achieved","","0-7695-2599","10.1109/SPLINE.2006.1691579","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1691579","","Embedded system;Space technology;Control systems;Space vehicles;System testing;Automotive engineering;Computer architecture;Production;Hardware;Computer science","automobiles;automotive electronics;configuration management;control engineering computing;embedded systems;program testing;program verification","representative configurations;evolving product lines;distributed embedded system;automotive technology;vehicle control system;vehicle configurations;verification coverage","","4","15","","","","","","IEEE","IEEE Conferences"
"Multiple Weighted Objectives Heuristic for the Redundancy Allocation Problem","D. W. Coit; A. Konak","NA; NA","IEEE Transactions on Reliability","","2006","55","3","551","558","A new heuristic is proposed and tested for system reliability optimization. The multiple weighted objective heuristic is based on a transformation of the problem into a multiple objective optimization problem, and then ultimately, transformation into a different single objective problem. The multiple objectives are to simultaneously maximize the reliability of each individual subsystem. This is a logical approach because system reliability is the product of the subsystem reliabilities, so if they are maximized, the system reliability will also be high. This new formulation and associated heuristic are then based on solving a sequence of linear programming problems. It is one of the very few optimization approaches that allow for linear programming algorithms and software to be used for the redundancy allocation problem when mixing of functionally equivalent components is allowed. Thus, it represents an efficient solution method that relies on readily available optimization tools. The heuristic is tested on many example problems, and compared to competing solution approaches. Overall, the heuristic performance is observed to be very good on the tested problem, and superior to the max-min heuristic regarding both efficiency, and performance","0018-9529;1558-1721","","10.1109/TR.2006.879654","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1688092","Multiple objective optimization;redundancy allocation;system reliability","Redundancy;Reliability;Linear programming;Genetic algorithms;Costs;Mathematical programming;Dynamic programming;System testing;Software algorithms;Optimization methods","linear programming;optimisation;redundancy;reliability theory","multiple weighted objective heuristic;redundancy allocation problem;system reliability optimization;multiple objective optimization problem;single objective problem;subsystem reliability;linear programming problems","","46","23","","","","","","IEEE","IEEE Journals & Magazines"
"ArF photoresist etching behavior evaluation","Martin Yang; Helios Kim; Fumitake Mieno","SMIC Shanghai, China; SMIC Shanghai, China; SMIC Shanghai, China","2007 International Symposium on Semiconductor Manufacturing","","2007","","","1","3","The transition of photoresist from KrF photoresist to ArF photoresist poses new challenges for etching process, especially for dielectric etching. In this article we design two types of dielectric etching applications, hole (contact) etching and LS (line space) etching. SAS software is employed for DOE (design of experiment) analysis of hole etching process optimization, best condition is derived and confirmed by experiment . To address LER, which is a persistent issue in LS application, mechanism is proposed and LER is successfully solved by new process.","1523-553X","978-1-4244-1141-2978-1-4244-1142","10.1109/ISSM.2007.4446872","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4446872","","Resists;Etching;Application software;US Department of Energy;Synthetic aperture sonar;Analysis of variance;Dielectrics;Semiconductor process modeling;Software testing;Plasma applications","argon compounds;design of experiments;etching;optimisation;photoresists","photoresist etching behavior;dielectric etching;hole contact etching;line space etching;SAS software;design of experiment analysis;etching process optimization","","","3","","","","","","IEEE","IEEE Conferences"
"Tomography System to Acquire 3D Images of Cells in Laminar Flow: Software Architecture","G. F. Abate; F. Bavaro; G. Castello; P. Daponte; D. Grimaldi; G. Guglielmelli; U. Martinelli; F. Mauro; S. Moisa; M. Napolitano; S. Rapuano; P. Scerbo","NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA","IEEE International Workshop on Medical Measurement and Applications, 2006. MeMea 2006.","","2006","","","74","79","In the framework of the project FIRB 2001 the Italian Ministry of Education, University and Research (MIUR) has supported the research project RBAUOIZMZ5 aimed to produce a tomography system able to acquire 3D images of cells in laminar flow. The innovative aspects of the new image flow cytometer (IFC) designed and prototyped under this project regard to both the hardware and the software architecture. In particular, it is able to automatically identify micronuclei on the acquired images of flowing lymphocytes. Therefore, the new IFC overcomes the characteristics of the conventional ones. The paper focuses on the software innovations regarding to (i) the digital signal processing procedure, (ii) the optimized methodology for classifying the different cell types, (iii) the programmable gain of each input channel, and (iv) the image preprocessing to detect the micronucleus in the acquired image of the human lymphocytes. In order to highlight the improved performance of the IFC software architecture, results of preliminary experimental tests are shown","","1-4244-0253","10.1109/MEMEA.2006.1644464","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1644464","","Tomography;Software architecture;Software prototyping;Hardware;Technological innovation;Digital signal processing;Optimization methods;Data preprocessing;Humans;Software testing","biomedical measurement;cellular biophysics;laminar flow;medical image processing;optical tomography","tomography system;3D cell image;laminar flow;software architecture;image flow cytometer;micronuclei identification;lymphocyte;digital signal processing procedure;programmable gain;image preprocessing;image detection","","8","26","","","","","","IEEE","IEEE Conferences"
"A generalized scheduling approach for dynamic dataflow applications","W. Plishker; N. Sane; S. S. Bhattacharyya","Electrical and Computer Engineering Department, University of Maryland, College Park, USA; Electrical and Computer Engineering Department, University of Maryland, College Park, USA; Electrical and Computer Engineering Department, University of Maryland, College Park, USA","2009 Design, Automation & Test in Europe Conference & Exhibition","","2009","","","111","116","For a number of years, dataflow concepts have provided designers of digital signal processing systems with environments capable of expressing high-level software architectures as well as low-level, performance-oriented kernels. But analysis of system-level trade-offs has been inhibited by the diversity of models and the dynamic nature of modern dataflow applications. To facilitate design space exploration for software implementations of heterogeneous dataflow applications, developers need tools capable of deeply analyzing and optimizing the application. To this end, we present a new scheduling approach that leverages a recently proposed general model of dynamic dataflow called core functional dataflow (CFDF). CFDF supports high-level application descriptions with multiple models of dataflow by structuring actors with sets of modes that represent fixed behaviors. In this work we show that by decomposing a dynamic dataflow graph as directed by its modes, we can derive a set of static dataflow graphs that interact dynamically. This enables designers to readily experiment with existing dataflow model specific scheduling techniques to all or some parts of the application while applying custom schedulers to others. We demonstrate this generalized dataflow scheduling method on dynamic mixed-model applications and show that run-time and buffer sizes significantly improve compared to a baseline dynamic dataflow scheduler and simulator.","1530-1591;1558-1101","978-1-4244-3781-8978-3-9810-8015","10.1109/DATE.2009.5090642","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5090642","","Dynamic scheduling;Application software;Signal design;Digital signal processing;Software architecture;Kernel;Space exploration;Software tools;Design optimization;Runtime","digital signal processing chips;embedded systems;hardware-software codesign;parallel processing;scheduling","generalized scheduling;dynamic dataflow applications;digital signal processing;space exploration;software implementation;heterogeneous dataflow application;core functional dataflow;static dataflow graph;dynamic mixed model","","17","19","","","","","","IEEE","IEEE Conferences"
"A High-Accuracy Parameter Estimation PSO Algorithm","Y. Yin; L. Sun; M. Han","NA; NA; NA","2008 International Conference on Embedded Software and Systems Symposia","","2008","","","7","12","A preferable value for parameters proved to be crucial in enhancing the performance and efficiency of particle swarm optimization (PSO) algorithm. To provide good solution for reasonable choice of parameter values within fairly wide range for particle swarm optimization, this paper presents a novel parameter optimizing configuration strategy based on multi-order rhombus thought (MRT), which depends on the optimization function to adaptively configure the most suitable set of parameters. With the divergent-concentrate-redivigent-reconcentrate nature of MRT, parameters are gradually optimized by the rhombus thought process as feedback information of the evolutionary process. Compared with other main improved methods, the computation procedures of MRTPSO algorithm are discussed, and numerical experiments based on typical benchmarks are given to illustrate the better convergence characteristic and shorter executing time of MRTPSO algorithm.","","978-0-7695-3288","10.1109/ICESS.Symposia.2008.8","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4627122","","Optimization;Heuristic algorithms;Benchmark testing;Particle swarm optimization;Convergence;Accuracy;Mathematical model","parameter estimation;particle swarm optimisation","high-accuracy parameter estimation;PSO algorithm;particle swarm optimization;parameter optimizing configuration strategy;multi-order rhombus thought;divergent-concentrate-redivigent-reconcentrate nature;evolutionary process;MRTPSO algorithm","","1","13","","","","","","IEEE","IEEE Conferences"
"Long-term Performance Bottleneck Analysis and Prediction","F. Gao; S. Sair","Department of Electrical and Computer Engineering, North Carolina State University, Raleigh, NC 27695. Email: fgao@ncsu.edu; Department of Electrical and Computer Engineering, North Carolina State University, Raleigh, NC 27695. Email: ssair@ncsu.edu","2006 International Conference on Computer Design","","2006","","","3","9","Identifying performance bottlenecks is important for microarchitects and application developers to produce high performance microprocessor designs and application software. Many techniques are used for this purpose, including simulation, software profiling and hardware event counters. Recently long-term program behavior has been getting more attention from researchers because of its potential applications in system-level, as well as program-level optimizations. In this paper, we study performance bottlenecks from a long-term program behavior viewpoint by classifying dynamic program execution into bottleneck phases -the portions of execution that have similar performance bottlenecks. We propose an event counter based performance model that can accurately estimate the performance cost for critical system events. Based on this model, we propose the bottleneck vector as the basis of long-term performance bottleneck analysis and a runtime bottleneck phase tracking scheme. In addition, three bottleneck phase prediction schemes are studied. Finally, we present an application of our performance bottleneck analysis model -an adaptive value predictor, which improves average performance by 7% when compared to the original value predictor design.","1063-6404","978-0-7803-9706-4978-0-7803-9707","10.1109/ICCD.2006.4380786","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4380786","","Performance analysis;Application software;Counting circuits;Microprocessors;Software design;Discrete event simulation;Hardware;Costs;Runtime;Predictive models","optimisation;program testing;software performance evaluation","long-term performance bottleneck analysis;microprocessor design;dynamic program execution;runtime bottleneck phase tracking scheme;adaptive value predictor;program-behaviour optimization;application software;event counter based performance model","","1","18","","","","","","IEEE","IEEE Conferences"
"Benchmarking and optimisation of Simulink code using Real-Time Workshop and Embedded Coder for inverter and microgrid control applications","A. J. Roscoe; S. M. Blair; G. M. Burt","University of Strathclyde, UK; University of Strathclyde, UK; University of Strathclyde, UK","2009 44th International Universities Power Engineering Conference (UPEC)","","2009","","","1","5","When creating software for a new power systems control or protection device, the use of auto-generated C code via MATLAB Simulink Real-Time Workshop and Embedded Coder toolboxes can be a sensible alternative to hand written C code. This approach offers the benefits of a simulation environment, platform independence and robust code. This paper briefly summarises recent experiences with this coding process including the pros and cons of such an approach. Extensive benchmarking activities are presented, together with descriptions of simple (but non-obvious) optimisations made as a result of the benchmarking. Examples include replacement of certain Simulink blocks with seemingly more complex blocks which execute faster. ¿S functions¿ are also designed for certain key algorithms. These must be fully ¿in-lined¿ to obtain the best speed performance. Together, these optimisations can lead to an increase in execution speed of more than 1.4× in a large piece of auto-generated C code. An example is presented, which carries out Fourier analysis of 3 signals at a common (variable) frequency. The overall speed improvement relative to the baseline is 2.3×, of which more than 1.4× is due to non-obvious improvements resulting from benchmarking activities. Such execution speed improvements allow higher frame rates or larger algorithms within inverters, drives, protection and control applications.","","978-0-947649-44-9978-1-4244-6823-2978-0-947649-45","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5429517","Power system measurements;Power system control;Power system protection;Motor drives;Inverters;Power electronics","Inverters;Application software;Embedded software;Software tools;Power system control;Power system protection;MATLAB;Real time systems;Power system simulation;Robustness","benchmark testing;invertors;motor drives;power system analysis computing;power system control;power system measurement;power system protection","Simulink code;inverter application;microgrid control application;power systems control;coding process;auto-generated C code;real-time workshop;embedded coder;Fourier analysis","","3","9","","","","","","IEEE","IEEE Conferences"
"A compiler-guided approach for reducing disk power consumption by exploiting disk access locality","Seung Woo Son; Guangyu Chen; M. Kandemir","Dept. of Comput. Sci. & Eng., Pennsylvania State Univ., University Park, PA, USA; Dept. of Comput. Sci. & Eng., Pennsylvania State Univ., University Park, PA, USA; Dept. of Comput. Sci. & Eng., Pennsylvania State Univ., University Park, PA, USA","International Symposium on Code Generation and Optimization (CGO'06)","","2006","","","11 pp.","268","Power consumption of large servers and clusters has recently been a popular research topic, since this issue is important from both technical and environmental viewpoints. The prior research proposed disk power management as one of the important ways of reducing overall power of a large system and considered both hardware-based and software-guided disk power reduction schemes. One of the common characteristics of the previously proposed approaches to disk power reduction is that they work with a given disk access pattern. In comparison, the goal of the approach proposed in this paper is to restructure application code using an optimizing compiler so that disk idle periods are lengthened. This in turn allows the underlying disk power management scheme to be more effective since such schemes usually prefer the long idle periods over the short ones. Our approach targets at large scientific applications that operate on disk-resident arrays using nested loops and exhibit regular data access patterns. To test the effectiveness of the proposed approach, we implemented it within an optimizing compiler and performed experiments with six data-intensive applications that manipulate disk-resident data. Our experimental analysis shows that the proposed approach is very successful in practice and reduces the total disk energy consumption on average by 18.17%, as compared to an execution without any disk power management, and by 11.55%, as compared to an execution that employs disks with low-power capabilities without our code restructuring approach.","","0-7695-2499","10.1109/CGO.2006.2","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1611546","","Energy consumption;Energy management;Power system management;Hardware;Application software;Optimizing compilers;Computer science;Power engineering and energy;Testing;Performance evaluation","optimising compilers;program control structures;power consumption;disc storage;storage management","disk power consumption;disk access locality;disk power management;disk power reduction;restructure application code;optimizing compiler;disk-resident arrays;nested loops;data access pattern;disk energy consumption","","","30","","","","","","IEEE","IEEE Conferences"
"Design & fabrication of turnstile antenna with feed network optimization for Leo satellites","K. Keyghobad; J. M. Baabuei; T. Heydari","Islamic Azad University, South Tehran Branch, Ukraine; Islamic Azad University, South Tehran Branch, Ukraine; Islamic Azad University, South Tehran Branch, Ukraine","2007 6th International Conference on Antenna Theory and Techniques","","2007","","","286","288","In this paper design of a turnstile in UHF band (465 MHz) is carried out. Optimum values for its main physical parameters such as monopole lengths, monopoles distance and angles are obtained by using HFSSV10 software. Antenna feed network is designed with microstrip lines to achieve in to the 90 degrees phase difference between adjacent monopoles and small size, simultaneously. Obtained scattering parameters from antenna design are used with microstrip feed network in microwave office software to achieve the desired VSWR characteristic of antenna at 465 MHz. In next step, the line and stub lengths are optimized in order to achieve a return loss better than 20 dB in operation frequency. Variable lumped capacitors are used in designed feed network for frequency fine tuning. The designed antenna and feed network are fabricated and tested. The diagrams represent a very good agreement between simulation and measurement.","","978-1-4244-1584","10.1109/ICATT.2007.4425187","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4425187","LEO satellite;antenna;turnstile;VSWR","Fabrication;Satellite antennas;Antenna feeds;Design optimization;Low earth orbit satellites;Microstrip antennas;Microwave antennas;Frequency;Scattering parameters;Capacitors","antenna feeds;artificial satellites;electrical engineering computing;microstrip lines;UHF antennas","turnstile antenna;feed network optimization;LEO satellites;HFSSV10 software;microstrip lines;microstrip feed network;microwave office software;VSWR characteristic;variable lumped capacitors","","","2","","","","","","IEEE","IEEE Conferences"
"Development of the Adaboost-SVM Model for the Classification of Estrogen Receptor-B Ligands","C. Zhou; Y. Zhang","NA; NA","2009 Fourth International Conference on Innovative Computing, Information and Control (ICICIC)","","2009","","","1232","1235","A new QSAR model for the classification of estrogen receptor-ß (ERß) selective ligand has been developed with adaptive boosting (Adaboost) and support vector machine (SVM). Compound structures were drawn in Molinspiration WebME Editor and imported into the E-Dragon 1.0 software to calculate seven categories descriptors. The selection of variables for each descriptor was performed with particle swarm optimization (PSO). On a known compound data set, mathematical model was obtained by AdaBoost using SVM as the base classifier. Among all descriptors in the model, the RDF descriptor exhibited the highest accuracy in the predictions, which contained five variables. By comparing with previous study, the AdaBoost-SVM model improved the prediction accuracy of the training set and the test set to 100.0% and 92.3%, up from 92.4% and 88.5% when only SVM was applied. The results indicate that the combination of Adaboost- SVM and PSO gives a powerful tool for QSAR studies and classification investigations.","","978-1-4244-5544-7978-1-4244-5543-0978-0-7695-3873","10.1109/ICICIC.2009.176","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5412206","","Erbium;Support vector machines;Support vector machine classification;Testing;Chemicals;Biological information theory;Biochemistry;Particle swarm optimization;Predictive models;Accuracy","biology computing;particle swarm optimisation;pattern classification;QSAR;support vector machines","adaptive boosting;support vector machine;SVM;QSAR model;particle swarm optimization;Molinspiration WebME Editor;E-Dragon 1.0 software;selective ligand;data set;AdaBoost;estrogen receptor-ß ligand;classification","","","12","","","","","","IEEE","IEEE Conferences"
"A Model to Handle Issues in Office Activities through a Software Application","J. B. P. Victorino; M. L. G. Barrios; S. O. Limon","NA; NA; NA","2009 International Conference on Electrical, Communications, and Computers","","2009","","","235","239","Usually private or governmental offices perform activities in which their processes are focused on giving support to mechanize their operations. In this article we propose to have offices with a high performance in handle issues. To make an efficient infrastructure of process in offices, we need to optimize the handling of issues. Bosses could handle different issues at the same time. The handling of issues gives the boss the possibility to increase his performance. The boss would attend his different tasks by a software program in intranet, and the secretary would chat with her boss to solve issues. This process was implemented in Java and was tested in a small school office.","","978-0-7695-3587","10.1109/CONIELECOMP.2009.27","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5163924","Office issues;intranet in offices;increase office performance;issues","Application software;Performance analysis;Proposals;Java;Testing;Educational institutions;Graphics","intranets;Java","office activities;software application;intranet;Java;office issues","","","7","","","","","","IEEE","IEEE Conferences"
"Teaching research in the laboratory using diagnosis environment for digital systems","S. Kostin; R. Ubar; J. Raik; M. Aarna; M. Brik; H. -. Wuttke","Department of Computer Engineering, Tallinn University of Technology, Raja 15, 12618, Estonia; Department of Computer Engineering, Tallinn University of Technology, Raja 15, 12618, Estonia; Department of Computer Engineering, Tallinn University of Technology, Raja 15, 12618, Estonia; Department of Computer Engineering, Tallinn University of Technology, Raja 15, 12618, Estonia; Department of Computer Engineering, Tallinn University of Technology, Raja 15, 12618, Estonia; Faculty of Computer Science and Automation, Ilmenau Technical University, PO 10 0565, 98684, Germany","2009 EAEEIE Annual Conference","","2009","","","1","4","In this work, we propose a multifunctional e-learning environment with remote access for learning, getting hands-on experience, and carrying out laboratory research in developing optimized procedures for locating faults in complex electronic systems. It is a combination of a collection of software tools which simulate a system under diagnosis, emulate a pool of different strategies, methods, and algorithms of diagnostic reasoning and fault location, and allow to experiment with different embedded self-diagnosing architectures. Hands-on experiments target research teaching issues. The interactive character of the environment makes experiments attractive and helps to raise the students' curiosity.","","978-1-4244-5385-6978-1-4244-5386","10.1109/EAEEIE.2009.5335462","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5335462","","Education;Digital systems;Fault diagnosis;Circuit faults;System testing;Electronic learning;Software tools;Software algorithms;Circuit testing;Built-in self-test","computer aided instruction;electronic engineering education;fault location;laboratory techniques;software tools","laboratory research;diagnosis environment;digital systems;multifunctional e-learning environment;remote access;complex electronic systems;software tools;diagnostic reasoning;fault location;embedded self-diagnosing architectures","","","4","","","","","","IEEE","IEEE Conferences"
"Multicore Programming Techniques for High-Performance ATE","M. Ravindran; J. Meisel","NA; NA","IEEE Instrumentation & Measurement Magazine","","2009","12","4","26","29","This paper examined how recent innovations in processor technology are pushing the limits for ATE applications. Various multicore programming techniques were discussed including task parallelism, data parallelism, and pipelining. In addition, an example of optimizing complex analysis was covered. The benefits of adopting multicore technology and parallel software architectures include a reduction in overall test time, more sophisticated simulation approaches, and the ability to analyze complex systems.","1094-6969;1941-0123","","10.1109/MIM.2009.5277931","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5277931","","Multicore processing;Testing;Parallel processing;Application software;Clocks;Central Processing Unit;Microprocessors;Parallel programming;Programming profession","multiprocessing systems;parallel programming;pipeline processing;software architecture","multicore programming techniques;ATE;processor technology;task parallelism;data parallelism;pipelining;optimizing complex analysis;parallel software architectures;sophisticated simulation approaches","","","6","","","","","","IEEE","IEEE Journals & Magazines"
"Integration of Hardware and Software for an Optimized Appliance Drive","J. Stafford; A. Mohammed","NA; NA","2007 IEEE Industry Applications Annual Meeting","","2007","","","1064","1069","The benefits of variable speed operations are well known for appliance applications. However, cost sensitive applications like appliances avoided continuously variable speed drives to reduce overall system cost. Several factors are rapidly changing this scenario. The prices of DSP controllers have dropped significantly. Moreover, DSP controllers now integrate a variety of sophisticated power electronics peripherals (general-purpose timers, PWM generators, analog-to-digital converters etc.). This integration of DSP core with a set of useful power electronics peripherals significantly simplifies the design process. At the same time, the cost sensitive drives like appliances are starting to utilize variable speed drives to maximize the system efficiency. This paper discusses an implementation where a continuously variable speed operation is provided for a cost sensitive application by using a single low cost DSP controller. This paper shows how to integrate ""C"" programming techniques for most optimized usage of on-chip memory (RAM & Flash) as well as peripherals to implement a field oriented controlled drive. In addition, the paper discusses how to integrate additional test routines to support standards like IEC-60730.","0197-2618","978-1-4244-1259-4978-1-4244-1260","10.1109/07IAS.2007.165","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4347914","","Hardware;Home appliances;Costs;Digital signal processing;Application software;Variable speed drives;Electric variables control;Power electronics;Pulse width modulation;Pulse width modulation converters","digital signal processing chips;power electronics;variable speed drives","variable speed drive;DSP controllers;power electronics;cost sensitive drives;field oriented controlled drive;C-programming","","","8","","","","","","IEEE","IEEE Conferences"
"Optimization of FPGA design and implementation of timing recovery in DVB-S2","Wang Xin; Zuyao Ni","School of Aerospace, Tsinghua University, Beijing 100084, China; School of Aerospace, Tsinghua University, Beijing 100084, China","2008 International Conference on Communications, Circuits and Systems","","2008","","","1265","1269","The design of timing recovery in DVB-S2 demodulator is a challenging issue, especially with the modulation scheme of high order amplitude and phase shift keying (APSK) and the very low signal to noise ratio (SNR) near Shannon limits. This paper presents an FPGA implementation scheme which meets the requirements of DVB-S2 system. Based on some comparison and analysis, Gardner algorithm is adopted as detector for its insensitiveness to carrier offset and modulation types, and cubic interpolator is employed for its high accuracy. Controller is also designed to restrain cycle slips and the implementation of loop filter is optimized to reduce truncation noise. The design is implemented in Altera Stratix II EP2S180 FPGA, working with other components of the demodulator. Software simulation results show that the performance loss introduced by timing recovery is negligible for all DVB-S2 modulation types, and the circuit provides reasonable symbol synchronization even when the signal to noise ratio (Eb/No) equals to -2 dB in FPGA tests.","","978-1-4244-2063-6978-1-4244-2064","10.1109/ICCCAS.2008.4657997","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4657997","","Timing;Modulation;Digital video broadcasting;Demodulation;Field programmable gate arrays;Synchronization;Finite impulse response filter","amplitude shift keying;demodulators;field programmable gate arrays;optimisation;synchronisation","FPGA design;optimization;timing recovery;DVB-S2 demodulator;amplitude and phase shift keying;Shannon limits;Gardner algorithm;cubic interpolator;loop filter;Altera Stratix II EP2S180;software simulation;symbol synchronization","","","10","","","","","","IEEE","IEEE Conferences"
"Fast soft error rate computing technique based on state probability propagating","Sheng Weiguang; Xiao Liyi; Mao Zhigang","Micro Electronic Center, Harbin Institute of Technology, China; Micro Electronic Center, Harbin Institute of Technology, China; Micro Electronic Center, Harbin Institute of Technology, China","2009 4th IEEE Conference on Industrial Electronics and Applications","","2009","","","734","738","Fast soft error sensitivity characterization technique is essential for the soft error tolerance optimization of modern VLSI circuits. In this paper, an efficient soft error evaluation technique based on syntax analysis and state probability propagating technique is developed, which can automatically analyze the soft error rate of combinational logic circuits and the combinational part of sequential circuits in Verilog synthesized netlist within a few seconds. We implemented the idea in a software tool called HSECT-ANLY, which use Verilog syntax analysis to automate the soft error rate evaluation procedure and state propagating technique to speed up the analyzation process. By using HSECT-ANLY, experiments are carried out on some ISCAS'85 and ISCAS'89 benchmark circuits implemented with TSMC 0.18 mum technology and results are obtained. The result comparison with the traditional test vector propagating technique shows that the introduced method is much faster (2-3 magnitudes speeding up) with some accuracy losses, and be very suitable for the reliability optimization as the sub-algorithm of the optimization algorithms such as genetic algorithms to evaluate the fitness (soft error rate) rapidly.","2156-2318;2158-2297","978-1-4244-2799-4978-1-4244-2800","10.1109/ICIEA.2009.5138302","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5138302","Soft error;combinational logic circuit;syntax analysis;state probability propagating;reliability optimization","Error analysis;Hardware design languages;Optimization methods;Very large scale integration;Combinational circuits;Sequential circuits;Circuit synthesis;Software tools;Benchmark testing;Circuit testing","hardware description languages;logic circuits;optimisation;probability;VLSI","fast soft error rate computing technique;VLSI circuits;syntax analysis;state probability propagating technique;combinational logic circuits;sequential circuits;Verilog;software tool;HSECT-ANLY;genetic algorithms","","","19","","","","","","IEEE","IEEE Conferences"
"Study a New Mesh Simplification Method in Reverse Engineering","H. Zhao; Y. Yang; G. Xie","NA; NA; NA","2009 Second International Symposium on Computational Intelligence and Design","","2009","2","","320","323","This paper introduces a new method of surface simplification based on calculating changing rate of surface. We apply the adapted Butterfly subdivision method to determine the new vertex positions of simplified mesh, making the new vertices lie on an optimized limiting surface. We test our method on the rabbit mesh, and it is very effective to hold geometric features.","","978-0-7695-3865","10.1109/ISCID.2009.226","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5368465","","Reverse engineering;Computer graphics;Application software;Computational intelligence;Paper technology;Civil engineering;Optimization methods;Testing;Rabbits;Size measurement","mesh generation;optimisation;reverse engineering;surface reconstruction","mesh simplification method;reverse engineering;surface changing rate;adapted Butterfly subdivision method;optimized limiting surface;rabbit mesh","","","18","","","","","","IEEE","IEEE Conferences"
"Model-based synthesis and optimization of static multi-rate image processing algorithms","J. Keinert; H. Dutta; F. Hannig; C. Haubelt; J. Teich","Fraunhofer IIS, Digital Cinema Department, Erlangen, Germany; Hardware/Software Co-Design, Department of Computer Science, University of Erlangen-Nuremberg, Germany; Hardware/Software Co-Design, Department of Computer Science, University of Erlangen-Nuremberg, Germany; Hardware/Software Co-Design, Department of Computer Science, University of Erlangen-Nuremberg, Germany; Hardware/Software Co-Design, Department of Computer Science, University of Erlangen-Nuremberg, Germany","2009 Design, Automation & Test in Europe Conference & Exhibition","","2009","","","135","140","High computational effort in modern image processing applications like medical imaging or high-resolution video processing often demands for massively parallel special purpose architectures in form of FPGAs or ASICs. However, their efficient implementation is still a challenge, as the design complexity causes exploding development times and costs. This paper presents a new design flow which permits to specify, analyze, and synthesize complex image processing algorithms. A novel buffer requirement analysis allows exploiting possible tradeoffs between required communication memory and computational logic for multi-rate applications. The derived schedule and buffer results are taken into account for resource optimized synthesis of the required hardware accelerators. Application to a multi-resolution filter shows that buffer analysis is possible in less than one second and that scheduling alternatives influence the required communication memory by up to 24% and the computational resources by up to 16%.","1530-1591;1558-1101","978-1-4244-3781-8978-3-9810-8015","10.1109/DATE.2009.5090646","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5090646","","Image processing;Algorithm design and analysis;Processor scheduling;Concurrent computing;Biomedical imaging;Computer architecture;Field programmable gate arrays;Costs;Image analysis;Logic","application specific integrated circuits;buffer layers;field programmable gate arrays;image processing;logic circuits;scheduling","medical imaging;FPGA;ASIC;complex image processing algorithms;buffer requirement analysis;communication memory;computational logic;hardware accelerators;multiresolution filter","","2","18","","","","","","IEEE","IEEE Conferences"
"Multi-strategy Integration for Actionable Trading Agents","L. Cao","NA","2007 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology - Workshops","","2007","","","487","490","Trading agents are very useful for developing and back-testing quality trading strategies to support smart trading actions in the market. However, the existing trading agent research mainly focuses on simple and simulated strategies. As a result, there exists a big gap between academia and business when the developed trading agents are deployed in the real life. Therefore, the actionable capability of developed trading agents is often very limited. In this paper, we introduce approaches for optimizing and integrating multiple classes of strategies for trading agents. Five categories of trading strategies, including 36 types of trading strategies are trained and tested. A strategy integration and optimization approach is proposed to identify golden trading strategy in each category, and finally recommend positions associated with these golden strategies to trading agents. Test in five international markets on ten years of data respectively has shown that the final strategies recommended to trading agents can lead to high benefits while low costs. Concurrent execution of positions recommended by all golden strategies can greatly enhance performance.","","0-7695-3028","10.1109/WI-IATW.2007.13","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4427634","trading agenttrading strategyoptimizationintegration","Intelligent agent;Testing;Position measurement;Conferences;Information technology;Australia;Decision making;Cost function;Software agents;Supply chain management","electronic commerce;software agents","multistrategy integration;actionable trading agents;back-testing quality trading strategies;smart trading actions;optimization approach;golden trading strategy","","1","6","","","","","","IEEE","IEEE Conferences"
"Source-Level Timing Annotation and Simulation for a Heterogeneous Multiprocessor","T. Meyerowitz; A. Sangiovanni-Vincentelli; M. Sauermann; D. Langen","University of California at Berkeley, Berkeley, California, USA. tcm@eecs.berkeley.edu; University of California at Berkeley, Berkeley, California, USA. alberto@eecs.berkeley.edu; Infineon Technologies, Munich, Germany. Mirko.Sauermann@infineon.com; Infineon Technologies, Munich, Germany. Dominik.Langen@infineon.com","2008 Design, Automation and Test in Europe","","2008","","","276","279","A generic and retargetable tool flow is presented that enables the export of timing data from software running on a cycle-accurate Virtual Prototype (VP) to a concurrent functional simulator. First, an annotation framework takes information gathered from running an application on the VP and automatically annotates the line-level delays back to the original source code. Then, a SystemC-based timed functional simulator runs the annotated source code much faster than the VP while preserving timing accuracy. This simulator is API-compatible with the multiprocessor's operating system. Therefore, it can compile and run unmodified applications on the host PC. This flow has been implemented for MuSIC (Multiple SIMD Cores) [6], a heterogeneous multiprocessor developed at Infineon to support Software Defined Radio (SDR). When compared with an optimized cycle-accurate VP of MuSIC on a variety of tests, including a multiprocessor JPEG encoder, the accuracy is within 20%, with speedups from 10x to 1000x.","1530-1591;1558-1101","978-3-9810801-3-1978-3-9810801-4","10.1109/DATE.2008.4484897","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4484897","","Timing;Application software;Software tools;Software prototyping;Virtual prototyping;Delay lines;Accuracy;Operating systems;Software radio;Testing","concurrency control;digital simulation;hardware description languages;multiprocessing systems;operating systems (computers);software prototyping;system monitoring;virtual prototyping","source-level timing annotation;heterogeneous multiprocessor;operating system;cycle-accurate virtual prototype;source code;SystemC-based timed functional simulator;API","","15","10","","","","","","IEEE","IEEE Conferences"
"Trends and trade-offs in designing highly robust throughput on chip communication network","M. Coppola","STMicroelectronics, Grenoble, France","12th IEEE International On-Line Testing Symposium (IOLTS'06)","","2006","","","1 pp.","","Summary form only given. Moore's law predicts that soon it will be possible to integrate billions of transistors on a single chip. Currently on-chip communication for multiprocessor system-on-chip (MPSoC) is realized using buses such as AMBA, STbus, and IBM's Core-connect. On-chip buses are not fundamentally different than computer buses, except that they are designed and optimized to operate entirely within a single chip meaning that wider buses are possible, and there is no constraint to the number of pins. These buses typically include a number of address and data wires, bi-directional signaling for management, and a complex arbitration policy. Since wire delay becomes more critical than computation delay, bus implementations for future SoC will be increasingly hard. For example, placement and routing for on-chip buses is extremely complex due to the large number of wires. Thus, a SoC bus may occupy an area comparable to a processing element. Current electronic design methodology is not able to handle the escalating design complexity and decreased time-to-market requirements for embedded applications in MPSoC, due to extensive hardware/software partitioning and design space exploration. Network-on-chip simplifies the overall MPSoC design and provides services to various computing resources, such as general and specialized processing elements, storage devices, reconfigurable FPGAs, or application-specific IP blocks, through a standard, structured, functionally correct, robust, scalable, and efficient intra-chip communication and synchronization infrastructure. In this talk we explain why scalable on-chip communication networks, such as networks-on-chip (NoC) are today not only attractive for system on chip design but they possess some features for designing robust systems","1942-9398;1942-9401","0-7695-2620","10.1109/IOLTS.2006.64","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1655521","","Robustness;Throughput;Network-on-a-chip;Communication networks;System-on-a-chip;Wires;Delay;Moore's Law;Multiprocessing systems;Data buses","hardware-software codesign;integrated circuit design;integrated circuit interconnections;network routing;network-on-chip","Moores law;multiprocessor system-on-chip;on-chip buses;data wires;bi-directional signaling;wire delay;integrated circuit placement;integrated circuit routing;electronic design methodology;hardware/software partitioning;design space exploration;network-on-chip;on-chip communication networks","","","","","","","","","IEEE","IEEE Conferences"
"Composing Highly Reliable Service-Oriented Applications Adaptively","W. Tsai; J. Elston; Y. Chen","NA; NA; NA","2008 IEEE International Symposium on Service-Oriented System Engineering","","2008","","","115","122","This paper proposes dynamic and adaptive reliability methods that can be integrated into a collaborative composition framework with dynamic verification and adaptive reliability modeling. The composition process follows a template-based approach with ontology systems where a variety of items can be reused including services, workflows, application templates, and test cases/scripts. Items submitted to publication are subjected to test and evaluation before they can be published and reused. Test cases can also be ranked based on their effectiveness and test cases will be continuously re-ranked based on recent testing results and reliability modeling. Composed applications will be tested and evaluated before deployment, and even during execution, applications will be monitored so that their reliability can be continuously updated. This paper proposed several adaptive mechanisms to estimate software reliability and update test case effectiveness. They are illustrated by examples.","","978-0-7695-3499","10.1109/SOSE.2008.50","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4730473","Service-Oriented Architecture;Dynamic Reliability Modeling","Application software;Collaborative work;Software testing;Service oriented architecture;Runtime;Quality of service;Condition monitoring;Reliability engineering;Systems engineering and theory;USA Councils","ontologies (artificial intelligence);program verification;software architecture;software reliability","service-oriented applications;adaptive reliability;dynamic verification;ontology systems;software reliability","","1","26","","","","","","IEEE","IEEE Conferences"
"Mathematical-Based Benchmarking: To Predict ANT Exploration Time-Series Dataset","P. Saeedi; S. A. Sorensen","NA; NA","2009 International Conference on Computational Intelligence and Software Engineering","","2009","","","1","4","Exploration robots are sent inside collapsed building in search of trapped victims. Once inside, they have to explore the entire maze of obstacles and debris as quickly as possible without missing any accessible area by means of the search algorithm. Comparing the performances of emerging search algorithms against known benchmarks, inside realistic simulated fields is crucial for fast victim localisation. In this paper we discuss how to apply the ridge regression in order to obtain the coefficient of relationship between the ""ANT victim discovery time step"" and the ""search field complexity"". This estimated coefficient allows us to predict the victim discovery time step of ANT exploration algorithm inside various generated search fields.","","978-1-4244-4507","10.1109/CISE.2009.5366677","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5366677","","NIST;Testing;Computer science;Educational institutions;Performance evaluation;Navigation;Fractals;Buildings;Mobile robots;Robot sensing systems","benchmark testing;optimisation;path planning;robots;time series","ANT exploration prediction;time series prediction;exploration robots;search algorithm;benchmarking","","","13","","","","","","IEEE","IEEE Conferences"
"""It's Not the Pants, it's the People in the Pants"" Learnings from the Gap Agile Transformation What Worked, How We Did it, and What Still Puzzles Us","D. Goodman; M. Elbaz","NA; NA","Agile 2008 Conference","","2008","","","112","115","After 7 years of traditional IT delivery, Gap Inc.Direct decided to adopt Agile. This experience report discusses three key factors that contributed to our successful (and ongoing) Agile transformation: 1. Ambitious Pilot Project, 2. Massive Investment in Continuous Integration, 3. Rethinking our Assets. The choices we made might seem risky and even counter-intuitive, but understanding them could help other organizations consider different points of view and priorities as they embark on the transition to Agile. Additionally, we will identify ongoing challenges and what is left in our transformation backlog.","","978-0-7695-3321","10.1109/Agile.2008.87","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4599459","Gap;Agile Transformation;Continuos Integration;Test Automation;Iterative Development","Business;Organizations;Programming;Testing;Software;Writing;Project management","project management;software engineering;software management","Gap Inc;agile transformation;IT delivery;ambitious pilot project;massive investment;continuous integration;assets rethinking","","4","","","","","","","IEEE","IEEE Conferences"
"Barrier Optimization for OpenMP Program","H. Ma; R. Zhao; X. Gao; Y. Zhang","NA; NA; NA; NA","2009 10th ACIS International Conference on Software Engineering, Artificial Intelligences, Networking and Parallel/Distributed Computing","","2009","","","495","500","Barrier construct in OpenMP program is a directive used to remove race code before continuing. Each thread waits until all of the other threads of a team have reached the barrier region. And data dependence is a technology to find whether two statements can be run in parallel. This paper presents two ways to optimize the OpenMP programa barrier. The first one is to remove redundant barrier with data dependence information, that is, if all the statements across a barrier have no data dependence, then the barrier can be safely removed. And the second one is to reduce the cost of barrier. The paper gives an implementation of another form of parallelism- DOACROSS and a new form of OpenMP barrier-region barrier, in which both are synchronized with busy-waiting. Experimental results show that the performance of the optimized OpenMP program is improved.","","978-0-7695-3642","10.1109/SNPD.2009.16","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5286621","","Yarn;Costs;Testing;Parallel programming;Programming profession;Software engineering;Artificial intelligence;Intelligent networks;Distributed computing;Information science","application program interfaces;optimisation;parallel programming;synchronisation","barrier optimization;OpenMP program;data dependence technology;DOACROSS parallelism;OpenMP barrier-region barrier;busy-waiting synchronization;barrier construct","","","19","","","","","","IEEE","IEEE Conferences"
"Gradient ascent paired-comparison subjective quality testing","S. Voran; A. Catellier","Insitute for Telecommunication Sciences, 325 Broadway, Boulder, Colorado, USA 80305; Insitute for Telecommunication Sciences, 325 Broadway, Boulder, Colorado, USA 80305","2009 International Workshop on Quality of Multimedia Experience","","2009","","","133","138","Subjective testing is the most direct means of assessing audio, video, and multimedia quality as experienced by users and maximizing the information gathered while minimizing the number of trials is an important goal. We propose gradient ascent subjective testing (GAST) as an efficient way to locate optimizing sets of coding or transmission parameter values. GAST combines gradient ascent optimization techniques with paired-comparison subjective test trials to efficiently locate parameter values that maximize perceived quality. We used GAST to search a two-dimensional parameter space for the known region of maximal audio quality as proof-of-concept. That point was accurately located and we estimate that conventional testing would have required at least 27 times as many trials to generate the same results.","","978-1-4244-4370","10.1109/QOMEX.2009.5246964","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5246964","Audio Quality;Gradient Ascent;Golden Section Line Search;Multimedia Quality;Subjective Testing;Video Quality","Switches;Humans;Software quality;Audio recording;Protocols;Software testing;Quality assessment;Redundancy;Robustness;Error correction codes","gradient methods;multimedia systems","audio assesment;video assesment;multimedia quality;information gathering;gradient ascent subjective testing;two-dimensional parameter space;proof-of-concept","","4","6","","","","","","IEEE","IEEE Conferences"
"Comparison between Particle Swarm Optimization, Differential Evolution and Multi-Parents Crossover","X. Xu; Y. Li","NA; NA","2007 International Conference on Computational Intelligence and Security (CIS 2007)","","2007","","","124","127","Particle swarm optimization (PSO), differential evolu- tion (DE) and multi-parents crossover (MPC) are the evo- lutionary computation paradigms, all of which have shown superior performance on complex non-linear function op- timization problems. This paper detects the underlying re- lationship between them and then qualitatively proves that these heuristic approaches from different theoretical prin- ciples are consistent in form. Comparison experiments in- volving eight test functions well studied in the evolutionary optimization literature are used to highlight some perfor- mance differences between the techniques. The results from our study show that DE generally outperforms the other al- gorithms.","","0-7695-3072-9978-0-7695-3072","10.1109/CIS.2007.37","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4415315","","Particle swarm optimization;Partitioning algorithms;Software engineering;Evolutionary computation;Testing;Heuristic algorithms;Genetics;Algorithm design and analysis;Computational intelligence;Security","","","","6","11","","","","","","IEEE","IEEE Conferences"
"Air Reverse Circulation Bit Internal Fluid Simulation Based on CFD","S. Hao; H. Huang; K. Yin","NA; NA; NA","2009 International Conference on Information Technology and Computer Science","","2009","2","","86","89","The article instructs the work principle of the injector device and its application in the reverse-circulation sampling drilling bit. Then use the fluent fluid engineering emulator software to simulate the internal fluid territory of the injector when the obliquity is 15, 30,45,60,75 and 90 degree. The pressure distributing cloud chart and the corresponding numerical value has been gotten. An injector with 6 degrees according to the conclusion from fluent was designed to experiment in the laboratory. Compare the two values we found that they are coincident essentially. It shows that using emulator technology to design optimization is credible and simple. The cost is low and the design cycle is short compared with the method through experience and test.","","978-0-7695-3688","10.1109/ITCS.2009.156","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5190188","air reverse circulation;bit;injector hole;optimization","Computational fluid dynamics;Instruments;Drilling;Design optimization;Sampling methods;Information technology;Computer science;Computational modeling;Computer simulation;Application software","computational fluid dynamics;drilling;drilling machines;lubricants;mechanical engineering computing","CFD;injector device;reverse-circulation sampling drilling bit;air reverse circulation bit;internal fluid simulation;fluent fluid engineering emulator software;pressure distributing cloud chart;design optimization","","","8","","","","","","IEEE","IEEE Conferences"
"Joint Optimization for Power Loss Reduction in Distribution Systems","D. Zhang; Z. Fu; L. Zhang","NA; NA; NA","IEEE Transactions on Power Systems","","2008","23","1","161","169","In distribution systems, network reconfiguration and capacitor control, generally, are used to reduce real power losses and to improve voltage profiles. Since both capacitor control and network reconfiguration belong to the complicated combinatorial optimization problems, it is hard to combine them efficiently for better optimization results. In this paper, a joint optimization algorithm of combining network reconfiguration and capacitor control is proposed for loss reduction in distribution systems. To achieve high performance and high efficiency of the proposed algorithm, an improved adaptive genetic algorithm (IAGA) is developed to optimize capacitor switching, and a simplified branch exchange algorithm is developed to find the optimal network structure for each genetic instance at each iteration of capacitor optimization algorithm. The solution algorithm has been implemented into a software package and tested on a 119-bus distribution system with very promising results.","0885-8950;1558-0679","","10.1109/TPWRS.2007.913300","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4435956","Capacitor control;genetic algorithm;joint optimization;loss minimum;network reconfiguration","Switched capacitor networks;Control systems;Genetic algorithms;Simulated annealing;Voltage control;Optimization methods;Automatic control;Artificial intelligence;Software algorithms;Software packages","capacitor switching;distribution networks;genetic algorithms","power loss reduction;distribution systems;capacitor control;voltage profiles;combinatorial optimization;adaptive genetic algorithm;capacitor switching;branch exchange algorithm","","51","22","","","","","","IEEE","IEEE Journals & Magazines"
"Evaluating Different Ranking Functions for Context-Based Literature Search","N. Ratprasartporn; S. Bani-Ahmad; A. Cakmak; J. Po; G. Ozsoyoglu","Department of Electrical Engineering and Computer Science, Case Western Reserve University, Cleveland, Ohio 44106. nattakarn@case.edu; Department of Electrical Engineering and Computer Science, Case Western Reserve University, Cleveland, Ohio 44106. sulieman@case.edu; Department of Electrical Engineering and Computer Science, Case Western Reserve University, Cleveland, Ohio 44106. cakmak@case.edu; Department of Electrical Engineering and Computer Science, Case Western Reserve University, Cleveland, Ohio 44106. jlp25@case.edu; Department of Electrical Engineering and Computer Science, Case Western Reserve University, Cleveland, Ohio 44106. tekin@case.edu","2007 IEEE 23rd International Conference on Data Engineering Workshop","","2007","","","261","268","Context-based literature digital library search is a new search paradigm that creates an effective ranking of query outputs by controlling query output topic diversity. We define contexts as pre-specified ontology-based terms and locate the paper set of a context based on semantic properties of the context (ontology) term. In order to provide a comparative assessment of papers in a context and effectively rank papers returned as search outputs, prestige scores are attached to all papers with respect to their assigned contexts. In this paper, we present three different prestige score (ranking) functions for the context-based environment, namely, citation-based, text-based, and pattern-based score functions. Using biomedical publications as the test case and Gene Ontology as the context hierarchy, we have evaluated the proposed ranking functions in terms of their accuracy and separability. We have found that text-based and pattern-based score functions yield better accuracy and separability than citation-based score functions.","","978-1-4244-0831-3978-1-4244-0832","10.1109/ICDEW.2007.4401002","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4401002","","Ontologies;Software libraries;Testing;Intrusion detection;Portals;Search engines","citation analysis;digital libraries;information filtering;ontologies (artificial intelligence);text analysis","ranking function;context-based literature search;digital library search;query output ranking;ontology-based terms;semantic properties;prestige score;citation-based score function;text-based score function;pattern-based score function;biomedical publication;gene ontology;context hierarchy","","","27","","","","","","IEEE","IEEE Conferences"
"Improved Ant Colony Algorithm and its Applications in TSP","X. Song; B. Li; H. Yang","Hebei Polytechnic University, China; Tangshan college, China; Hebei Polytechnic University, China","Sixth International Conference on Intelligent Systems Design and Applications","","2006","2","","1145","1148","In the fields of ant colony optimization (ACO), models of collective intelligence of ants are transformed into useful optimization techniques. A kind of improved ACO (named PMACO) approach for traveling salesman problems (TSP) is presented. Aimed at the disadvantages existed in ACO, several new betterments are proposed and evaluated. In particular, the option that an ant hunts for the next step, the use of a combination of two kinds of pheromone evaluation models, the change of amount in the ant colony during the run of the algorithm, and the mutation of pheromone are studied. We tested ACO algorithm on a set of benchmark problems from the traveling salesman problem library. It performed better than the original and the other improved ACO algorithms","2164-7143;2164-7151","0-7695-2528","10.1109/ISDA.2006.253773","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021825","Ant colony optimization;Traveling Salesman Problem","Cities and towns;Ant colony optimization;Genetic mutations;Traveling salesman problems;Educational institutions;Application software;Automatic control;Continuing education;Benchmark testing;Libraries","graph theory;travelling salesman problems","traveling salesman problems;pherome mutation ACO algorithm;ant colony optimization","","18","5","","","","","","IEEE","IEEE Conferences"
"An agent-based approach for automating the disturbance handling for flexible manufacturing systems","I. Badr; P. Göhner","Universität Stuttgart, Institute of Industrial, Automation and Software Engineering (IAS), Pfaffenwaldring 47, 70550, Germany; Universität Stuttgart, Institute of Industrial, Automation and Software Engineering (IAS), Pfaffenwaldring 47, 70550, Germany","2009 IEEE Conference on Emerging Technologies & Factory Automation","","2009","","","1","4","In this paper, an agent-based approach for automating the disturbance handling in flexible manufacturing systems is proposed. This approach builds on a model that captures the environmental context of disturbances to derive their expected impact. Distributed hierarchical optimization is employed to update the generated schedule under consideration of efficiency and stability. The proposed approach has been realized and tested on a model of a real flexible manufacturing system. Test results have shown a very good potential for the proposed approach in automating disturbance handling by keeping small deviations from the original schedule and achieving high efficiency measures with good reactivity.","1946-0740;1946-0759","978-1-4244-2727-7978-1-4244-2728","10.1109/ETFA.2009.5347230","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5347230","","Flexible manufacturing systems;Job shop scheduling;Manufacturing automation;Stability;Virtual manufacturing;Computer industry;Manufacturing industries;Software engineering;Electric breakdown;Context modeling","distributed processing;flexible manufacturing systems;software agents","agent-based approach;disturbance handling;flexible manufacturing system;distributed hierarchical optimization","","6","6","","","","","","IEEE","IEEE Conferences"
"Knowledge discovery in high-dimensional data: case studies and a user survey for the rank-by-feature framework","Jinwook Seo; B. Shneiderman","Children's Res. Inst., Washington, DC, USA; NA","IEEE Transactions on Visualization and Computer Graphics","","2006","12","3","311","322","Knowledge discovery in high-dimensional data is a challenging enterprise, but new visual analytic tools appear to offer users remarkable powers if they are ready to learn new concepts and interfaces. Our three-year effort to develop versions of the hierarchical clustering explorer (HCE) began with building an interactive tool for exploring clustering results. It expanded, based on user needs, to include other potent analytic and visualization tools for multivariate data, especially the rank-by-feature framework. Our own successes using HCE provided some testimonial evidence of its utility, but we felt it necessary to get beyond our subjective impressions. This paper presents an evaluation of the hierarchical clustering explorer (HCE) using three case studies and an e-mail user survey (n=57) to focus on skill acquisition with the novel concepts and interface for the rank-by-feature framework. Knowledgeable and motivated users in diverse fields provided multiple perspectives that refined our understanding of strengths and weaknesses. A user survey confirmed the benefits of HCE, but gave less guidance about improvements. Both evaluations suggested improved training methods","1077-2626;1941-0506;2160-9306","","10.1109/TVCG.2006.50","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1608018","Information visualization evaluation;case study;user survey;rank-by-feature framework;hierarchical clustering explorer.","Computer aided software engineering;Data visualization;Histograms;Scattering;Data analysis;Computer Society;Visual analytics;Testing;Genomics","data mining;data visualisation;database management systems;interactive systems;user interfaces","knowledge discovery;high-dimensional data;rank-by-feature framework;hierarchical clustering explorer;interactive tool;visualization tools;e-mail user survey;visual analytic tools;user interface;multivariate data","Algorithms;Artificial Intelligence;Cluster Analysis;Computer Graphics;Computer Simulation;Imaging, Three-Dimensional;Models, Theoretical;Oligonucleotide Array Sequence Analysis;User-Computer Interface","34","27","","","","","","IEEE","IEEE Journals & Magazines"
"Faster Exploration of High Level Design Alternatives using UML for Better Partitions","W. Ahmed; D. Myers","Curtin University of Technology Sarawak Campus, Malaysia waseem@curtin.edu.my; NA","Proceedings of the Design Automation & Test in Europe Conference","","2006","1","","1","2","Partitioning is a time consuming and computationally complex optimization problem in the codesign of hardware software systems. The stringent time-to-market requirements have resulted in truncating this step resulting in sub-optimal solutions being offered to consumers. To obtain the true global minima, which translates to finding the best solution available, a new methodology is needed that can achieve this goal in a minimal time. An approach is presented that forms a basis for design space exploration from a partitioning perspective using UML2.0","1530-1591;1558-1101","3-9810801-1","10.1109/DATE.2006.243971","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1656953","","Unified modeling language;Partitioning algorithms;Algorithm design and analysis;Cost function;Hardware;Time to market;Space exploration;Real time systems;Embedded system;Australia","hardware-software codesign;Unified Modeling Language","high level design;partitioning;complex optimization problem;hardware software codesign systems;UML2.0","","","4","","","","","","IEEE","IEEE Conferences"
"SimSoC: A SystemC TLM integrated ISS for full system simulation","C. Helmstetter; V. Joloboff","INRIA, LIAMA, 95 Zhongguancun East Road, Haidian District, 100080, Beijing, CHINA; INRIA, LIAMA, 95 Zhongguancun East Road, Haidian District, 100080, Beijing, CHINA","APCCAS 2008 - 2008 IEEE Asia Pacific Conference on Circuits and Systems","","2008","","","1759","1762","The development of embedded systems requires the development of increasingly complex software and hardware platforms. Full system simulation makes it possible to run the exact binary embedded software including the operating system on a totally simulated hardware platform. Whereas most simulation environments do not support full system simulation, or do not use any hardware modeling techniques, or have combined different types of technology, SimSoC is developing a full system simulation architecture with an integrated approach relying only upon SystemC hardware modeling and transaction-level modeling abstractions (TLM) for communications. To simulate processors at reasonably high speed, SimSoC integrates instruction set simulators (ISS) as SystemC modules with TLM interfaces to the other platform components. The ISSpsilas use a variant approach of dynamic translation to run binary code. The dynamic translator uses pre-compiled code that consists of specialized functions for instruction execution, using partial evaluation techniques. It is generated by a configurable code generator, which makes it possible to tune the generated code to optimize simulation speed for the target software application.","","978-1-4244-2341-5978-1-4244-2342","10.1109/APCCAS.2008.4746381","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4746381","","Embedded software;Computational modeling;Open source software;Operating systems;Hardware design languages;Software testing;Embedded system;Application software;Kernel;Computer simulation","embedded systems;hardware-software codesign;instruction sets;system-on-chip","SimSoC;SystemC TLM integrated ISS;full system simulation;embedded systems;binary embedded software;hardware modeling techniques;SystemC hardware modeling;transaction-level modeling abstractions;instruction set simulators;configurable code generator;partial evaluation techniques;target software application;binary codes;complex software/hardware platforms;dynamic translator","","8","17","","","","","","IEEE","IEEE Conferences"
"Multi - Layered Best Basis Image Compression","D. M. Yadav; D. S. Bormane","NA; NA","International Conference on Computational Intelligence and Multimedia Applications (ICCIMA 2007)","","2007","3","","79","83","Software testing is indispensable for all software development. As all mature engineering disciplines need to have systematic testing methodologies, software testing is a very important subject of software engineering. In software development practice, testing accounts for as much as 50% of total development efforts. Testing can be manual, automated, or a combination of both. Manual testing is the process of executing the application and manually interacting with the application, specifying inputs and observing outputs. Manually testing the software is inefficient and costly. It is imperative to reduce the cost and improve the effectiveness of software testing by automating the testing process, which contains many testing related activities using various techniques and methods. In order to automate the process, we have to have some ways to generate oracles from the specification, and generate test cases to test the target software against the oracles to decide their correctness. Today we still don't have a full-scale system that has achieved this goal. In general, significant amount of human intervention is still needed in testing. The degree of automation remains at the automated test script level. This paper therefore provides a timely summary and enhancement of agent theory in software testing ,which motivates recent efforts in adapting concepts and methodologies for agent oriented software testing to complex system which has not previously done. Agent technologies facilitate the automated software testing by virtue of their high level decomposition, independency and parallel activation. Usage of agent based regression testing reduces the complexity involved in prioritizing the test cases.With the ability of agents to act autonomously, monitoring code changes and generating test cases for the changed version of the code can be done dynamically. Agent-oriented software testing (AOST) is a nascent but active field of research . A comprehensive methodology that plays an essential role in software testing must be robust but easy-to use. Moreover, it should provide a roadmap to guide engineers in creating agent-based systems. The agent based regression testing (ABRT) proposed here is to offer a definition for encompassing to cover the regression testing phenomena based on agents, yet sufficiently tight that it can rule out complex systems that are clearly not agent based.","","0-7695-3050","10.1109/ICCIMA.2007.293","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4426345","","Image coding;Software testing;Automatic testing;System testing;Programming;Application software;Software engineering;Costs;Humans;Automation","approximation theory;data compression;image coding;image representation;image sequences;image texture;wavelet transforms","multi layered best basis image compression;multilayered image representation technique;image encoding;image residual sequence;image textured pattern-brushlet;localized oriented textured feature;coarse main approximation method;wavelet packet bases","","","12","","","","","","IEEE","IEEE Conferences"
"FACSE: A framework for architecture and compilation space exploration","S. Niar; N. Inglart; M. Chaker; S. Hanafi; N. Benameur","University of Valenciennes, Le Mont Houy, 59313 Cedex 9, France; University of Valenciennes, Le Mont Houy, 59313 Cedex 9, France; Ecole Nationale d¿Ingénieurs de Sfax, B.P w.3038, Tunisia; University of Valenciennes, Le Mont Houy, 59313 Cedex 9, France; University of Valenciennes, Le Mont Houy, 59313 Cedex 9, France","2007 International Conference on Design & Technology of Integrated Systems in Nanoscale Era","","2007","","","249","252","To exploit microelectronics progresses for next processor generations, it is crucial to design new tools allowing a better adequacy between architectures and applications. The Facse project, which we present in this paper, strives to achieve this objective. As the two aspects of the system (hardware and software) are taken into account simultaneously, it is possible to increase the performances significantly. Facse is based on the use of adapted heuristics to guide search towards the most promising architecture and compiler optimization configurations in one side and rapid evaluation methods of these configurations on the other side.","","978-1-4244-1277-8978-1-4244-1278","10.1109/DTIS.2007.4449531","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4449531","Architecture;processors;simulation;optimization;exploration","Space exploration;Computer architecture;Application software;Testing;Program processors;Transistors;Network-on-a-chip;Microelectronics;Hardware;Software performance","hardware-software codesign;heuristic programming;microprocessor chips;optimising compilers","FACSE;compilation space exploration;next processor generations;adapted heuristics;compiler optimization;rapid evaluation methods","","","12","","","","","","IEEE","IEEE Conferences"
"Policy-Based Network Management in Home Area Networks: Interim Test Results","A. I. Rana; M. O Foghlu","NA; NA","2009 3rd International Conference on New Technologies, Mobility and Security","","2009","","","1","3","This paper argues that Home Area Networks (HANs) are a good candidate for advanced network management automation techniques, such as Policy-Based Network Management (PBNM). What is proposed is a simple use of policy based network management to introduce some level of Quality of Service (QoS) and Security management in the HAN, whilst hiding this complexity from the home user. In this paper we have presented the interim test results of our research experiments (based on a scenario) using the HAN testbed. After using policies to prioritize different traffic, packet loss decreased to 30% and VoIP quality improved dramatically without employing any intelligent bandwidth allocation technique.","2157-4952;2157-4960","978-1-4244-6273-5978-1-4244-4765-7978-1-4244-6272","10.1109/NTMS.2009.5384722","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5384722","","Testing;Home automation;Telecommunication traffic;Energy management;Quality management;Quality of service;Internet;Engineering management;Software systems;Channel allocation","computer network management;computer network security;home computing;quality of service","policy-based network management;home area networks;interim test results;advanced network management automation;quality of service;security management","","6","7","","","","","","IEEE","IEEE Conferences"
"An incremental and optimized learning method for the automatic classification of protein crystal images","G. Xu; C. Chiu; E. D. Angelini; A. F. Laine","Department of Biomedical Engineering, Columbia University, NY, USA; Department of Biomedical Engineering, Columbia University, NY, USA; Departement Traitement du Signal et des Images (TSI), Paris, France; Department of Biomedical Engineering, Columbia University, NY, USA","2006 International Conference of the IEEE Engineering in Medicine and Biology Society","","2006","Supplement","","6526","6529","Protein production has experienced great advances in recent years. In particular, high throughput protein production, coupled with the use of robotics, outputs thousands of mixtures in micro-array wells. To detect the presence of protein crystal formation, images of these wells are acquired regularly using robotic cameras. Traditionally, a crystallographer would manually process each image - identifying the wells that resulted in protein crystal formation. This manual inspection process is slow and given the high rate of mixture output, it has become near impossible for crystallographers keep up. Our aim is to create an automated method of detecting which wells have crystals and which ones do not. We make use of a neural network that is trained based on manually classified ground truth data. After it is trained, the automatic classifier would give a binary output - a value of one for the detection of crystals and precipitates in images and a value of zero otherwise. In our previous papesr, the core methods of using multi-scale Laplacian image representation to extract image features and the implementation of the neural network classifier were discussed. Here we present a new, optimized approach to training the neural network and results from a large-scale test. We claim that the neural network can be better trained if the training image dataset is optimized in the sense that ambiguous images are removed during the initial training processes. Incremental training is implemented so that the network can be improved as more data becomes available. From initial results with training based on a 6,000 optimized image dataset, the accuracy of the improved classifier approaches 95% in identifying a wide array of images.","1557-170X","1-4244-0032-51-4244-0033","10.1109/IEMBS.2006.260870","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4030589","","Optimization methods;Learning systems;Crystals;Production;Neural networks;Protein engineering;Crystallography;Robot vision systems;Cameras;Inspection","","","Algorithms;Artificial Intelligence;Crystallization;Proteins;Software","2","3","","","","","","IEEE","IEEE Conferences"
"A Hybrid Multi-objective Evolutionary Algorithm and Its Application in Component-based Product Design","X. Zheng; H. Duan; H. Liu","Shandong Normal University, China; Shandong Normal University, China; Shandong Normal University, China","Eighth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing (SNPD 2007)","","2007","1","","570","575","Component-based product design usually appears as a multi-objective optimization problem (MOP). Traditional methods solving MOPs are robust and have proven their effectiveness in handling many classes of optimization problems. However, such techniques can encounter difficulties such as getting trapped in local minima, increasing computational complexity, and not being applicable to certain classes of objective functions. Multi-Objective Evolutionary Algorithms (MOEAs) can overcome these disadvantages and have shown great potentials to solve MOPs. In this paper, an h-MOEA is proposed by employing effective strategies from evolutionary computation, which is suitable for solving the MOP in design optimization and can generate more diverse solutions in an accepted time span. Then, the effectiveness and correctness of h-MOEA is verified using several popular benchmark functions. Also, a prototype is developed and used in component-based product design optimization. Finally, the optimization results of a product design case are shown in detail.","","0-7695-2909-7978-0-7695-2909","10.1109/SNPD.2007.81","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4287572","Component-based product design;MOP;MOEA;Diverse solutions","Evolutionary computation;Product design;Design optimization;Genetic algorithms;Robustness;Optimization methods;Computational complexity;Mathematical programming;Benchmark testing;Software engineering","computational complexity;evolutionary computation;product design","hybrid multiobjective evolutionary algorithm;component-based product design;multiobjective optimization problem;computational complexity;h-MOEA;design optimization","","","14","","","","","","IEEE","IEEE Conferences"
"A Novel Multi-objective Optimization Algorithm Based on Artificial Immune System","L. Chun-Hua; Z. Xin-Jan; H. Wan-Qi; C. Guang-Yi","NA; NA; NA; NA","2009 Fifth International Conference on Natural Computation","","2009","4","","569","574","The traditional evolutionary algorithm (EA) for solving the multi-objective optimization problem (MOP) is difficult to accelerate convergence and keep the diversity of the achieved Pareto optimal solutions. A novel EA, i.e., immune multi-objective optimization algorithm (IMOA), is proposed to solve the MOP in this paper. The special evolutional mechanism of the artificial immune system (AIS) prevents the prematurity and quickens the convergence of optimization. The method combined by the random weighted method and the adaptive weighted method guarantee the acquired solutions to distribute on the Pareto front uniformly and widely. An external set for storing the Pareto optimal solutions is built up and updated by a novel approach. By graphical presentation and examination of selected performance metrics on two difficult test functions, the proposed IMOA is found to outperform four other algorithms in terms of finding a diverse set of solutions and converging near the true Pareto front.","2157-9555;2157-9563","978-0-7695-3736","10.1109/ICNC.2009.285","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5365953","Artificial immune system;Multi-objective optimization;Pareto optimal solutions","Artificial immune systems;Pareto optimization;Evolutionary computation;Acceleration;Application software;Fuel cells;Sorting;Genetic algorithms;Heuristic algorithms;Computational modeling","artificial immune systems;convergence;evolutionary computation;Pareto optimisation","artificial immune system;evolutionary algorithm;convergence acceleration;Pareto optimal solutions;immune multiobjective optimization algorithm;random weighted method;adaptive weighted method;Pareto front;graphical presentation","","2","8","","","","","","IEEE","IEEE Conferences"
"Testing the Object-Oriented Programs Using a Multi-Stage Genetic Algorithm","A. S. Ghiduk","NA","2009 2nd International Conference on Computer Science and its Applications","","2009","","","1","6","","2159-7030;2159-7049","978-1-4244-4945","10.1109/CSA.2009.5404279","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5404279","","Genetic algorithms;Biological cells;Automatic testing;Object oriented modeling;Encoding;Decoding;Java;Software testing;Optimization methods;Genetic mutations","","","","","12","","","","","","IEEE","IEEE Conferences"
"The JAMES II Framework for Modeling and Simulation","J. Himmelspach; A. M. Uhrmacher","NA; NA","2009 International Workshop on High Performance Computational Systems Biology","","2009","","","101","102","JAMES II is a general and open framework based on the ldquoPlug'n simulaterdquo concept, which enables developers to integrate their modeling and simulation methodological ideas into, and to create their applications upon an existing framework.This concept together with currently more than 400 plug-ins,and an explicit representation and storage of experiments ease developing modeling and simulation methods and contribute to a systematic experimental evaluation of methods. Modelers benefit from the flexibility that the framework provides with respect to modeling, simulation, and analysis methods, supporting effective and efficient simulation studies, and the well tested methods add to the credibility of the results achieved.","","978-0-7695-3809","10.1109/HiBi.2009.20","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5298693","Experimental Algorithmics;Performance Evaluation;Framework;Modeling;Simulation;Plug-Ins;Distributed Simulation","Biological system modeling;Computational modeling;Analytical models;Computer simulation;Computational systems biology;Application software;Optimization methods;Partitioning algorithms;Computer science;Testing","add-on boards;digital simulation;distributed processing;software architecture","JAMES II framework;modeling;plug-in simulate;experimental algorithmics;distributed simulation;software architecture","","2","3","","","","","","IEEE","IEEE Conferences"
"Capacity Optimization for 3G Wireless Networks","B. L. Horton; N. Grumman; R. F. Smith","Combat Syst. Group of Northrop Grumman, New Orleans; NA; NA","2008 40th Southeastern Symposium on System Theory (SSST)","","2008","","","164","168","This paper evaluates code division multiple access evolution data optimized (CDMA2000 1x EV-DO) and the universal mobile telecommunication system (UMTS) with regard to their ability to support simultaneous user equipment requests. The question of which of these third generation (3G) technologies is best was determined by analyzing their second generation (2G) core network technology scalability and extensibility and their ability to provide third generation cellular functionality. The core network (CN) architecture, which provides interconnectivity with fixed services such as the Internet and the public switched telephone network, serves as the platform for evolution to 3G performance. The performance results of a CDMA2000 lx EV-DO and a UMTS network were designed and analyzed using the test Ericsson mobile system (TEMS) CellPlanner simulation software. Simulation results were used to determine the accuracy of coverage predictions, the amount of traffic each system could support, and the number of simultaneous users that could be supported by each system.","0094-2898;2161-8135","978-1-4244-1806-0978-1-4244-1807","10.1109/SSST.2008.4480212","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4480212","","Wireless networks;3G mobile communication;Multiaccess communication;Scalability;Cellular networks;IP networks;Web and internet services;Internet telephony;Performance analysis;Software testing","3G mobile communication;code division multiple access","3G wireless networks;code division multiple access evolution;universal mobile telecommunication system;second generation core network;Internet;public switched telephone network;core network architecture;third generation cellular functionality;test Ericsson mobile system","","","15","","","","","","IEEE","IEEE Conferences"
"Optimizing Near-ML MIMO Detector for SDR Baseband on Parallel Programmable Architectures","M. Li; B. Bougard; W. Xu; D. Novo; L. Van Der Perre; F. Catthoor","Nomadic Embedded System Division, IMEC, Leuven, Belgium. Email: limin@imec.be; Nomadic Embedded System Division, IMEC, Leuven, Belgium. Email: bougardb@imec.be; Department of EE, Caltech, CA, USA. Email: weiyu@caltech.edu; Nomadic Embedded System Division, IMEC, Leuven, Belgium. Email: novo@imec.be; Nomadic Embedded System Division, IMEC, Leuven, Belgium. Email: vdperre@imec.be; Nomadic Embedded System Division, IMEC, Leuven, Belgium. Email: catthoor@imec.be","2008 Design, Automation and Test in Europe","","2008","","","444","449","ML and near-ML MIMO detectors have attracted a lot of interest in recent years. However, almost all the reported implementations are delivered in ASICs or FPGAs. Our contribution is optimizing the near-ML MIMO detector for parallel programmable architectures, such as those with ILP and DLP features. In the proposed SSFE (selective spanning with fast enumeration), architecture-friendliness is explicitly introduced from the very beginning of the design flow. Importantly, high level algorithmic transformations make the dataflow pattern and structure fit architecture-characteristics very well. We enable abundant vector-parallelism with highly regular and deterministic dataflow in the SSFE; memory rearrangements, shuffling and non-predictable dynamism are all elaborately excluded. Hence, the SSFE can be easily parallelized and efficiently mapped onto ILP and DLP architectures. Furthermore, to fine-tune the SSFE on parallel architectures, extensive pre-compiler transformations are applied with the help of the application-level information. These optimize not only computation-operations but also address-generations and memory-accesses. Experiments show that the SSFE brings very efficient resource-utilizations on real-life VLIW architectures. Specifically, with the SSFE the percentage of NOPs instructions on VLIW is below 1%, even better than that achieved by the software-pipelined FFT. To the best of our knowledge, this is the first reported work about comprehensive optimizations of near-ML MIMO detectors for parallel programmable architectures.","1530-1591;1558-1101","978-3-9810801-3-1978-3-9810801-4","10.1109/DATE.2008.4484721","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4484721","","MIMO;Detectors;Baseband;Signal processing algorithms;Computer architecture;VLIW;Field programmable gate arrays;Embedded system;Pipeline processing;Parallel programming","application specific integrated circuits;data flow computing;field programmable gate arrays;maximum likelihood detection;MIMO communication;parallel architectures;software radio","near-ML MIMO detector;SDR baseband;parallel programmable architectures;ASIC;FPGA;selective spanning with fast enumeration;high level algorithmic transformations;parallel architectures;extensive pre-compiler transformations;application-level information;computation-operations optimization;address-generations optimization;memory-accesses optimization;resource utilizations;VLIW architectures","","11","18","","","","","","IEEE","IEEE Conferences"
"Characterization of SPEC CPU2006 and SPEC OMP2001: Regression Models and their Transferability","E. Ould-Ahmed-Vall; K. A. Doshi; C. Yount; J. Woodlee","Intel Corporation, 5000 W Chandler Blvd., Chandler, AZ 85226, elmoustapha.ould-ahmed-vall@intel.com; Intel Corporation, 5000 W Chandler Blvd., Chandler, AZ 85226, kshitij.a.doshi@intel.com; Intel Corporation, 5000 W Chandler Blvd., Chandler, AZ 85226, chuck.yount@intel.com; Intel Corporation, 5000 W Chandler Blvd., Chandler, AZ 85226, jim.woodlee@intel.com","ISPASS 2008 - IEEE International Symposium on Performance Analysis of Systems and software","","2008","","","179","190","Analysis of workload execution and identification of software and hardware performance barriers provide critical engineering benefits; these include guidance on software optimization, hardware design tradeoffs, configuration tuning, and comparative assessments for platform selection. This paper uses Model trees to build statistical regression models for the SPEC1 CPU2006 and the SPEC OMP2001 suites. These models link performance to key microarchitectural events. The models provide detailed recipes for identifying the key performance factors for each suite and for determining the contribution of each factor to performance. The paper discusses how the models can be used to understand the behaviors of the two suites on a modern processor. These models are applied to obtain a detailed performance characterization of each benchmark suite and its member workloads and to identify the commonalities and distinctions among the performance factors that affect each of the member workloads within the two suites. This paper also addresses the issue of model transferability. It explores the question: How useful is an existing performance model (built on a given suite of workloads) to study the performance of different workloads or suites of workloads? A performance model built using data from workload suite P is considered transferable to workload suite Q if it can be used to accurately study the performance of workload suite Q. Statistical methodologies to assess model transferability are discussed. In particular, the paper explores the use of two-sample hypothesis tests and prediction accuracy analysis techniques to assess model transferability. It is found that a model trained using only 10% of the SPEC CPU2006 data is transferable to the remaining data. This finding holds also for SPEC OMP2001. In contrast, it is found that the SPEC CPU2006 model is not transferable to SPEC OMP2001 and vice versa.","","978-1-4244-2232-6978-1-4244-2233","10.1109/ISPASS.2008.4510750","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4510750","","Software performance;Hardware;Performance analysis;Design engineering;Design optimization;Software design;Regression tree analysis;Microarchitecture;Statistical analysis;Testing","mathematics computing;regression analysis;software performance evaluation","SPEC CPU2006;SPEC OMP2001;workload execution;workload identification;statistical regression models;microarchitectural events;model transferability","","6","25","","","","","","IEEE","IEEE Conferences"
"Folded cascode OTA design for wide band applications","H. Daoud; S. Ben Salem; S. Zouari; M. Loulou","NA; NA; NA; NA","International Conference on Design and Test of Integrated Systems in Nanoscale Technology, 2006. DTIS 2006.","","2006","","","437","440","This paper deals with design and optimization of a folded cascode operational transconductance amplifier. First, a detailed description of an optimum OTA topology is done in order to optimize MOS transistor sizing. Second, the design of folded cascode OTA, which works for frequencies that lead to a base band circuit design for RF application, is based on transistor sizing methodology. Third, folded cascode OTAs generally find several applications that are well developed. Simulation results are performed using SPICE software and BSIM3V3 model for CMOS 0.35mum process, show that the designed folded cascode OTA has a 85dB DC gain and provides a gain bandwidth product of around 332MHz","","0-7803-9726","10.1109/DTIS.2006.1708674","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1708674","","Wideband;Application software;Performance gain;Design optimization;Transconductance;Operational amplifiers;Radiofrequency amplifiers;Circuit topology;MOSFETs;Frequency","CMOS integrated circuits;integrated circuit design;operational amplifiers;wideband amplifiers","folded cascode OTA design;operational transconductance amplifier;wide band applications;MOS transistor sizing;base band circuit design;RF application;transistor sizing methodology;SPICE software;BSIM3V3 model;CMOS process;CMOS IC design;85 dB;0.35 micron","","3","6","","","","","","IEEE","IEEE Conferences"
"Simulation of a Smart Grid City with Software Agents","S. Karnouskos; T. N. d. Holanda","NA; NA","2009 Third UKSim European Symposium on Computer Modeling and Simulation","","2009","","","424","429","In the future smart city, new information and communication technologies will enable a better management of the available resources. The future smart grid infrastructure is emerging as a complex system where fine-grained monitoring and control of energy generating and/or consuming entities within the electricity network is possible. This will result to better approaches that will boost energy efficiency. A simulation of a dynamic ecosystem such as the smart city, will enable us to test new concepts and resource-optimization approaches. Therefore we have analyzed, designed, and build a simulator based on software agents that attempts to create the dynamic behavior of a smart city. It simulates discrete heterogeneous devices that consume and/or produce energy, that are able to act autonomously and collaborate. The behavior of these devices and their groupings e.g. smart houses, has been modeled in order to map as near as possible the real behavior patterns of the respective physical objects.","","978-1-4244-5345-0978-0-7695-3886","10.1109/EMS.2009.53","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5358746","Simulation software;energy measurement;mobile agents;web services","Smart grids;Cities and towns;Software agents;Communications technology;Resource management;Technology management;Monitoring;Communication system control;Control systems;Mesh generation","electricity supply industry;power consumption;power engineering computing;power markets;power system measurement;software agents","smart grid city;software agent;information and communication technology;smart grid infrastructure;complex system;fine-grained monitoring;electricity network;energy efficiency;dynamic ecosystem;resource optimization;dynamic behavior;discrete heterogeneous device;energy consumption;smart house","","71","12","","","","","","IEEE","IEEE Conferences"
"Effective Fault Localization using Code Coverage","W. E. Wong; Y. Qi; L. Zhao; K. Cai","University of Texas at Dallas; University of Texas at Dallas; University of Texas at Dallas; Beijing University of Aeronautics","31st Annual International Computer Software and Applications Conference (COMPSAC 2007)","","2007","1","","449","456","Localizing a bug in a program can be a complex and time- consuming process. In this paper we propose a code coverage-based fault localization method to prioritize suspicious code in terms of its likelihood of containing program bugs. Code with a higher risk should be examined before that with a lower risk, as the former is more suspicious (i.e., more likely to contain program bugs) than the latter. We also answer a very important question: how can each additional test case that executes the program successfully help locate program bugs? We propose that with respect to a piece of code, the aid introduced by the first successful test that executes it in computing its likelihood of containing a bug is larger than or equal to that of the second successful test that executes it, which is larger than or equal to that of the third successful test that executes it, etc. A tool, chiDebug, was implemented to automate the computation of the risk of the code and the subsequent prioritization of suspicious code for locating program bugs. A case study using the Siemens suite was also conducted. Data collected from our study support the proposal described above. They also indicate that our method (in particular Heuristics III (c), (d), and (e)) can effectively reduce the search domain for locating program bugs.","0730-3157","0-7695-2870","10.1109/COMPSAC.2007.109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4291037","","Computer bugs;Proposals;Computer science;Automatic testing;Data visualization;Software testing;Software debugging;Software design;Java;Application software","codes;fault tolerant computing;program debugging","program bug localization;code coverage-based fault localization;suspicious code;program bug location;Siemens suite;search domain","","55","22","","","","","","IEEE","IEEE Conferences"
"A Power Measuring Technique for Built-in Test Purposes","V. Konstantakos; T. Laopoulos","Electronics Laboratory, Physics Department, Aristotle University of Thessaloniki, Thessaloniki, 54124, Greece. Phone: +30 2310 998033, Fax: +30 2310 998018, email: vkonstad@auth.gr; Electronics Laboratory, Physics Department, Aristotle University of Thessaloniki, Thessaloniki, 54124, Greece. Phone: +30 2310 998033, Fax: +30 2310 998018","2006 IEEE Instrumentation and Measurement Technology Conference Proceedings","","2006","","","90","95","A built-in configuration for monitoring the good operating condition of an instrumentation system (with analog and digital circuits) in real-time, is presented in this work. The approach is based on a current monitoring circuit which records the changes in the power supply current of the system without practically interfering the operation of the system and sums it within a specific time period. The comparison of the resulting power consumption measurement for hardware modules while executing certain tasks (defined in time by the software) with already known values may indicate the good operating condition of the system. This methodology may also provide information useful to power optimization techniques","1091-5281","0-7803-9360-00-7803-9359","10.1109/IMTC.2006.328290","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4124283","built-in testing;built-in integrated sensor;embedded systems;current measurement;power estimation","Power measurement;Built-in self-test;Condition monitoring;Instruments;Digital circuits;Real time systems;Current supplies;Power supplies;Energy consumption;Software measurement","built-in self test;electric current measurement;power measurement;power supply circuits","built-in test;built-in configuration;instrumentation system;analog circuits;digital circuits;current monitoring circuit;power supply current;power consumption measurement;current measurement","","2","13","","","","","","IEEE","IEEE Conferences"
"Security Checkpoint Optimizer Simulation Tool For Passenger Screening Prototyping","D. Wilson; R. Pryor; S. A. So; E. K. Roe","Transportation Security Administration WJHTC, Building 315 Atlantic City International Airport Atlantic City, NJ 08405, U.S.A.; Transportation Security Administration 701 South 12th Street Arlington, VA 22202, U.S.A.; Northrop Grumman Corp. 15010 Conference Center Dr. Chantilly, VA 20151, U.S.A.; Northrop Grumman Corp. 15010 Conference Center Dr. Chantilly, VA 20151, U.S.A.","2007 Winter Simulation Conference","","2007","","","2384","2384","Summary form only given. Northrop Grumman in support of Transportation Security Administration (TSA), part of the U.S. Department of Homeland Security (DHS), began development of the Security Checkpoint Optimizer (SCO) discrete-event simulation software in 2001. Since then, a number of security screening simulation models have been developed in support of DHS Science and Technology Branch and TSA studies. In 2007, Northrop Grumman was tasked by the TSA Office of Security Technologies to develop a prototype system demonstrating connectivity of screening equipment with an enterprise screening control and monitoring system. To support the prototype, SCO was enhanced with network capabilities to enable device emulation. The SCO simulation software was able to act as a ""stand in"" for screening devices, which are not currently network-capable. The initial prototype showed the feasibility of such an interconnected system, and will serve as a basis for future testing. The study also produced valuable insights to facilitate actual implementation.","0891-7736;1558-4305","978-1-4244-1305-8978-1-4244-1306","10.1109/WSC.2007.4419899","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4419899","","Virtual prototyping;National security;Software prototyping;Prototypes;Transportation;Terrorism;Discrete event simulation;Control systems;Monitoring;Emulation","discrete event simulation;monitoring;security","security checkpoint optimizer simulation tool;passenger screening prototyping;Northrop Grumman;Transportation Security Administration;US Department of Homeland Security;discrete-event simulation software;security screening simulation model;screening equipment;enterprise screening control;monitoring system;network capabilities;device emulation","","","","","","","","","IEEE","IEEE Conferences"
"Continuous Variable Neighbourhood Search Algorithm Based on Evolutionary Metaheuristic Components: A Scalability Test","C. García-Martínez; M. Lozano","NA; NA","2009 Ninth International Conference on Intelligent Systems Design and Applications","","2009","","","1074","1079","Variable neighbourhood search is a metaheuristic combining three components: generation, improvement, and shaking components. In this paper, we describe a continuous variable neighbourhood search algorithm based on three specialised evolutionary algorithms, which play the role of each aforementioned component: 1) an EA specialised in generating a good starting point as generation component, 2) an EA specialised in exploiting local information as improvement component, 3) and another EA specialised in providing local diversity as shaking component. We adopt the experimental framework proposed for the Special session on evolutionary algorithms and other metaheuristics for continuous optimization problems - a scalability test, for the ISDA'09 conference, to test the ability of the model of being scalable for high-dimensional problems.","2164-7143;2164-7151","978-1-4244-4735-0978-0-7695-3872","10.1109/ISDA.2009.68","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5364190","Variable Neighbourhood Search;Specialised Evolutionary Algorithms;Continuous Optimization;Scalability;Evolutionary Metaheuristic Components;Hybrid Metaheuristics","Scalability;Evolutionary computation;Artificial intelligence;Iterative algorithms;Intelligent systems;System analysis and design;Application software;System testing;Numerical analysis;Computer science","evolutionary computation;optimisation;search problems","continuous variable neighbourhood search algorithm;evolutionary metaheuristic components;scalability test;generation component;improvement component;shaking component","","","23","","","","","","IEEE","IEEE Conferences"
"Two Case Studies of User Experience Design and Agile Development","M. Najafi; L. Toyoshiba","NA; NA","Agile 2008 Conference","","2008","","","531","536","How can user experience design (UED) practices be leveraged in agile development to improve product usability? UED practices identify the needs and goals of the user through user research and testing. By incorporating UED in agile development, user research and testing can be utilized to prioritize features in the product backlog and to iteratively refine designs to achieve better usability. Furthermore, integrating UED and Agile processes can be accomplished with little or no impact on release schedules. The cases studies presented in this paper describe two examples of UED and agile integration at VeriSign.","","978-0-7695-3321","10.1109/Agile.2008.67","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4599534","Agile;User Experience;VeriSign;Usability;User Experience Design;UED;Sprint;Scrum;security","Testing;Web sites;Usability;Book reviews;Planning;Schedules;Security","program testing;software development management;software engineering;software performance evaluation","user experience design;agile development;product usability;VeriSign","","14","7","","","","","","IEEE","IEEE Conferences"
"Algorithms for Maximum Satisfiability using Unsatisfiable Cores","J. Marques-Silva; J. Planes","University of Southampton, Electronics & Computer Science, Southampton, UK. jpms@ecs.soton.ac.uk; University of Southampton, Electronics & Computer Science, Southampton, UK. jp3@ecs.soton.ac.uk","2008 Design, Automation and Test in Europe","","2008","","","408","413","Many decision and optimization problems in electronic design automation (EDA) can be solved with Boolean satisfiability (SAT). Moreover, well-known extensions of SAT also find application in EDA, including pseudo-Boolean optimization, quantified Boolean formulas, multi-valued SAT and, more recently, Maximum Satisfiability (MaxSAT). Algorithms for MaxSAT are still fairly inefficient in industrial settings, in part because the most effective SAT techniques cannot be easily extended to MaxSAT. This paper proposes a novel algorithm for MaxSAT that improves existing state of the art solvers by orders of magnitude on industrial benchmarks. The new algorithm exploits modern SAT solvers, being based on the identification of unsatisfiable subformulas. Moreover, the new algorithm provides additional insights between unsatisfiable subformulas and the maximum satisfiability problem.","1530-1591;1558-1101","978-3-9810801-3-1978-3-9810801-4","10.1109/DATE.2008.4484715","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4484715","","Electronic design automation and methodology;Inference algorithms;Computer science;Design optimization;Application software;Industrial relations;Debugging;Encoding;Circuits;Data structures","Boolean functions;circuit optimisation;electronic design automation;optimisation","electronic design automation;pseudo-Boolean optimization problems;Boolean satisfiability;maximum satisfiability algorithms;MaxSAT","","8","27","","","","","","IEEE","IEEE Conferences"
"Impact of source code optimizations on power consumption of embedded systems","D. A. Ortiz; N. G. Santiago","Electrical and Computer Engineering Department, University of Puerto Rico, Mayagüez Campus, PR 00681-9042, Puerto Rico; Electrical and Computer Engineering Department, University of Puerto Rico, Mayagüez Campus, PR 00681-9042, Puerto Rico","2008 Joint 6th International IEEE Northeast Workshop on Circuits and Systems and TAISA Conference","","2008","","","133","136","Power consumption is an important constraint in the design of battery-operated embedded systems. Minimizing power dissipation may be handled in terms of hardware or software optimizations. Source code-level optimization techniques have been used as an alternative to achieve low power consumption when programming embedded systems. However these techniques should be analyzed with statistical sound methods in order to reach strong conclusions about their real impact on power consumption. In this work, source code optimizations were applied on a set of representative benchmarks for embedded processors (MiBench) to analyze whether the techniques have or not an effect on power dissipation of a set of microprocessor based platforms. Design of experiments techniques (DOE) and analysis of variance (ANOVA) were used to achieve statistical sound conclusions. Results showed that not all optimizations have a significant effect on power consumption, moreover some techniques depend on the target platform where they are run.","","978-1-4244-2331-6978-1-4244-2332","10.1109/NEWCAS.2008.4606339","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4606339","","Optimization;Power demand;Benchmark testing;Embedded system;Software;Program processors;Analysis of variance","design of experiments;embedded systems;low-power electronics;microprocessor chips;optimisation;power aware computing","source code-level optimization techniques;embedded system power consumption;battery-operated embedded system design;power dissipation minimization;programming embedded system;statistical sound methods;MiBench;representative benchmarks;microprocessor based platforms;design-of-experiments techniques;analysis-of-variance;ANOVA;low power consumption","","9","18","","","","","","IEEE","IEEE Conferences"
"A Kernel-based Discrimination Framework for Solving Hypothesis Testing Problems with Application to Speaker Verification","Yi-Hsiang Chao; Wei-Ho Tsai; Hsin-Min Wang; Ruei-Chuan Chang","Academia Sinica, Taipei, Taiwan; NA; NA; NA","18th International Conference on Pattern Recognition (ICPR'06)","","2006","4","","229","232","Real-word applications often involve a binary hypothesis testing problem with one of the two hypotheses ill-defined and hard to be characterized precisely by a single measure. In this paper, we develop a framework that integrates multiple hypothesis testing measures into a unified decision basis, and apply kernel-based classification techniques, namely, kernel Fisher discriminant (KFD) and support vector machine (SVM), to optimize the integration. Experiments conducted on speaker verification demonstrate the superiority of our approaches over the predominant approaches","1051-4651","0-7695-2521","10.1109/ICPR.2006.89","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1699822","","Testing;Support vector machines;Application software;Speech;Solid modeling;Kernel;Support vector machine classification;Loss measurement;Chaos;Information science","heuristic programming;pattern classification;speaker recognition;support vector machines","kernel-based discrimination;hypothesis testing problem;speaker verification;kernel-based classification;kernel Fisher discriminant;support vector machine","","2","12","","","","","","IEEE","IEEE Conferences"
"Integrating multiuser dynamic OFDMA into IEEE 802.11a and prototyping it on a real-time software-defined radio testbed","H. S. Lichte; S. Valentin; F. Eitzen; M. Stege; C. Unger; H. Karl","University of Paderborn, Warburger Str. 100, 33098 Paderborn, Germany. hermann.lichte@upb.de, Phone: +49 5251.60-5374, Fax: +49 5251.60-5377; University of Paderborn, Warburger Str. 100, 33098 Paderborn, Germany. stefanv@upb.de, Phone: +49 5251.60-5374, Fax: +49 5251.60-5377; University of Paderborn, Warburger Str. 100, 33098 Paderborn, Germany. folk@upb.de, Phone: +49 5251.60-5374, Fax: +49 5251.60-5377; Signalion GmbH, Sudhausweg 5, 01099 Dresden, Germany. matthias.stege@signalion.com, Phone: +49 351.206931-0, Fax: +49 351.206931-11; Signalion GmbH, Sudhausweg 5, 01099 Dresden, Germany. carsten.unger@signalion.com, Phone: +49 351.206931-0, Fax: +49 351.206931-11; University of Paderborn, Warburger Str. 100, 33098 Paderborn, Germany. holger.karl@upb.de, Phone: +49 5251.60-5374, Fax: +49 5251.60-5377","2007 3rd International Conference on Testbeds and Research Infrastructure for the Development of Networks and Communities","","2007","","","1","9","Multiuser dynamic orthogonal frequency division multiple access (OFDMA) can achieve high downlink capacities in future wireless networks by optimizing the subcarrier allocation for each user. When it comes to the integration into current wireless local area network (WLAN) standards, dynamic OFDMA raises several implementation issues which are neglected in theoretical papers. Putting this emerging approach into practice requires to treat these issues accordingly and to demonstrate the feasibility of the system design. In this paper, we propose a dynamic OFDMA integration for the physical layer of the widespread IEEE 802.11a standard. To test our implementation and demonstrate its practical relevance we use a pragmatic approach: We prototype multiuser dynamic OFDMA on a real-time software-defined radio testbed for WLANs. We discuss details of our implementation and provide measurements showing that it does not introduce significant overhead into the IEEE 802.11a system at high subcarrier allocation quality. We particularly focus on the problems of our integration as well as the concepts and limitations of the used testbed.","","978-1-4244-0738-5978-1-4244-0739","10.1109/TRIDENTCOM.2007.4444671","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4444671","","Software prototyping;Software testing;OFDM;Wireless LAN;Frequency conversion;Downlink;Physical layer;Delay;Dynamic scheduling;Processor scheduling","frequency division multiple access;software radio;wireless LAN","multiuser dynamic OFDMA;IEEE 802.11a;real-time software-defined radio testbed;orthogonal frequency division multiple access;wireless networks;subcarrier allocation;wireless local area network standards;WLAN standards","","6","20","","","","","","IEEE","IEEE Conferences"
"Complete hardware / software solution for implementing the control of the electrical machines with programmable logic circuits","D. Mic; S. Oniga; E. Micu; C. Lung","North University of Baia Mare/Electronics and Computers Department, Romania; North University of Baia Mare/Electronics and Computers Department, Romania; Transilvania University of Bra¿ov/ Automatics Department, Romania; North University of Baia Mare/Electronics and Computers Department, Romania","2008 11th International Conference on Optimization of Electrical and Electronic Equipment","","2008","","","107","114","In this paper authors propose a rapid and new method to design and optimize the electric motor control using field programmable gate array (FPGA). The elaborated designing algorithm takes advantage of the unification of ISE Xilinx digital designing environment with Matlab/Simulink simulation and testing environment, through system generator toolbox. This results in a faster and efficient design flow. A software and hardware operational system was realized using programmable logic circuits to control a brushless DC (BLDC) motor. The system is able to operate independently or to communicate bidirectional and in real time with Matlab/Simulink environment. A hierarchical control system design method is achieved too, based on parameterized models; system generator toolbox library is extended with specific electric motors control blocks (PWM, PID, commutation blocks, etc).","","978-1-4244-1544-1978-1-4244-1545","10.1109/OPTIM.2008.4602465","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4602465","","Hardware;Control systems;Algorithm design and analysis;Field programmable gate arrays;Testing;Software;Generators","brushless DC motors;commutation;control engineering computing;design engineering;electric machine analysis computing;field programmable gate arrays;machine control;programmable circuits;programmable logic devices;three-term control","hardware-software solution;electrical machines;programmable logic circuits;field programmable gate array;FPGA;ISE Xilinx digital designing environment;Matlab-Simulink simulation;system generator toolbox;brushless DC motor;Matlab-Simulink environment;hierarchical control system design method;system generator toolbox library;PWM;PID;commutation blocks","","5","14","","","","","","IEEE","IEEE Conferences"
"System Performance Optimization Methodology for Infineon's 32-Bit Automotive Microcontroller Architecture","A. Mayer; F. Hellwig","Infineon Technologies, Am Campeon 1-12, 85579 Neubiberg, Germany. albrecht.mayer@infineon.com; Infineon Technologies, Am Campeon 1-12, 85579 Neubiberg, Germany. frank.hellwig@infineon.com","2008 Design, Automation and Test in Europe","","2008","","","962","966","Microcontrollers are the core part of automotive Electronic Control Units (ECUs). A significant investment of the ECU manufacturers and even their customers is linked to the specified microcontroller family. To preserve this investment it is required to continuously design new generations of the microcontroller with hardware and software compatibility but higher system performance and/or lower cost. The challenge for the microcontroller manufacturer is to get the relevant inputs for improving the system performance, since a microcontroller is used by many customers in many different applications. For Infmeon's latest TriCorereg based 32-bit microcontroller product line, the required statistical data is gathered by using the trace features of the Emulation Device (ED). Infineon's customers use EDs in their unchanged target system and application environment. With an analytical methodology and based on this statistical data, the performance improvements of different SoC architecture and implementation options can be quantified. This allows an objective assessment of improvement options by comparing their performance cost ratios.","1530-1591;1558-1101","978-3-9810801-3-1978-3-9810801-4","10.1109/DATE.2008.4484805","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4484805","","System performance;Optimization methods;Automotive engineering;Microcontrollers;Investments;Manufacturing;Costs;Computer architecture;Automotive electronics;Hardware","automotive electronics;microcontrollers;system-on-chip","automotive microcontroller architecture;automotive electronic control units;Infineon;TriCore;emulation device;SoC;word length 32 bit","","5","5","","","","","","IEEE","IEEE Conferences"
"PSO approach to preview tracking control systems","N. Birla; A. Swarup","Research Scholar, Department of Electrical Engineering, National Institute of Technology, Kurukshetra, Haryana, India; Dean, Galgotia College of Engineering and Technology, Greater Noida, U.P., India., (On Leave from National Institute of Technology, Kurukshetra)","TENCON 2009 - 2009 IEEE Region 10 Conference","","2009","","","1","6","Preview Control is a field well suited for application to systems that have reference signals known a priori. The use of advance knowledge of reference signal can improve the tracking quality of the concerned control system. The classical solution to the Preview Control problem is obtained using the Algebraic Riccati Equation. The solution obtained is good but it is not optimal and has a scope of improvement. This paper presents a novel method of design of H¿ Preview Controller using Particle Swarm Optimization (PSO) technique. The procedure is based on improving the performance by minimizing the objective function i.e. IAE (Integral of Absolute Error). The procedure is tested for two systems - Altitude Control System of second order and an industrial system of fifth order using MATLAB software environment. The results show that the solutions of PSO based technique are better in terms of the control characteristics of transient response of the system for various preview lengths, stability and also in terms of the objective function Integral of Absolute Error (IAE).","2159-3442;2159-3450","978-1-4244-4546-2978-1-4244-4547","10.1109/TENCON.2009.5395843","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5395843","Preview Control;H∞ Control;Particle Swarm Optimization","Control systems;Riccati equations;Integral equations;Design methodology;Particle swarm optimization;Software testing;System testing;Computer industry;Electrical equipment industry;Industrial control","H<sup>¿</sup> control;particle swarm optimisation;predictive control;Riccati equations","tracking control;H¿ preview control;particle swarm optimization;IAE objective function;integral of absolute error;algebraic Riccati equation;altitude control system;industrial system","","1","15","","","","","","IEEE","IEEE Conferences"
"A design flow for configurable embedded processors based on optimized instruction set extension synthesis","R. Leupers; K. Karuri; S. Kraemer; M. Pandey","Inst. for Integrated Signal Process. Syst., KWTH Aachen Univ., Germany; Inst. for Integrated Signal Process. Syst., KWTH Aachen Univ., Germany; Inst. for Integrated Signal Process. Syst., KWTH Aachen Univ., Germany; Inst. for Integrated Signal Process. Syst., KWTH Aachen Univ., Germany","Proceedings of the Design Automation & Test in Europe Conference","","2006","1","","6 pp.","","Design tools for application specific instruction set processors (ASIPs) are an important discipline in systems-level design for wireless communications and other embedded application areas. Some ASIPs are still designed completely from scratch to meet extreme efficiency demands. However, there is also a trend, towards use of partially predefined, configurable RISC-like embedded processor cores that can be quickly tuned to given applications by means of instruction set extension (ISE) techniques. While the problem of optimized ISE synthesis has been studied, well from a theoretical perspective, there are still few approaches to an overall HW/SW design flow for configurable cores that take all real-life constraints into account In this paper, we therefore present a novel procedure for automated ISE synthesis that accommodates both user-specified and processor-specific constraints in a flexible wary and that produces value optimized ISE solutions in-short time. Driven by an advanced application C code analysis/profiling frontend, the ISE synthesis core algorithm is embedded into a complete, design flow, where the backend is formed by a state-of-the-art industrial tool for processor configuration, ISE IIW synthesis, and SW tool retargeting. The proposed, design flow, including ISE synthesis, is demonstrated, via several benchmarks for the MIPS CorExtend configurable RISC processor platform","1530-1591;1558-1101","3-9810801-1","10.1109/DATE.2006.243972","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1656954","","Design optimization;Application specific processors;Computational Intelligence Society;Constraint optimization;Signal synthesis;Signal design;Algorithm design and analysis;Process design;Instruction sets;Signal processing","computer architecture;embedded systems;hardware-software codesign;instruction sets;system-on-chip","application specific instruction set processors;systems-level design;wireless communications;RISC;embedded processor cores;instruction set extension techniques;HW/SW design flow;configurable cores;automated ISE synthesis;user-specified constraints;processor-specific constraints;C code analysis;industrial tool;processor configuration;ISE IIW synthesis;SW tool retargeting;MIPS CorExtend platform","","21","20","","","","","","IEEE","IEEE Conferences"
"Improved Pattern Matching to Find DNA Patterns","L. Dudas","Department of Information Engineering, University of Miskolc, Hungary. iitdl@uni-miskolc.hu","2006 IEEE International Conference on Automation, Quality and Testing, Robotics","","2006","2","","345","349","The process of finding given patterns in DNA sequences is widely used in modern biological sciences. This paper shows an algorithmic improvement for exact pattern matching introducing a new heuristic. For implementation the author created an application that uses three well-known heuristics to ensure the O(n) worst case time, the O(n log<sub>sigma</sub>(m)/m) average case time and the O(n/m) best case time of searching for an m length pattern in an n length text that use a sigma letter alphabet. This application served as a testbed for the new H4 heuristic. The novelty is in optimization of the direction of text window movement in the preprocessing phase. The idea takes advantages of RAM based searching: usually all the text resides in today's gigabyte memory so the opposite direction of searching window moving requires the same time as the usual. A new function predicts the better moving direction in preprocessing time, based on the unsymmetrical property of pattern. Tests proved that this heuristic may result in fewer jumps and tested characters in the search phase of pattern matching","","1-4244-0360-X1-4244-0361","10.1109/AQTR.2006.254657","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4022980","","Pattern matching;DNA;Testing;Hydrogen;Sequences;Software algorithms;Biology;Biological information theory;Read-write memory;Random access memory","biology computing;computational complexity;DNA;genetics;pattern matching;random-access storage","pattern matching;DNA patterns;computational complexity;RAM based searching","","","8","","","","","","IEEE","IEEE Conferences"
"Energy Evaluation of Software Implementations of Block Ciphers under Memory Constraints","J. Grossschadl; S. Tillich; C. Rechberger; M. Hofmann; M. Medwed","Graz University of Technology, Institute for Applied Information Processing and Communications, Inffeldgasse 16a, A-8010 Graz, Austria, jgrosz@iaik.tugraz.at; Graz University of Technology, Institute for Applied Information Processing and Communications, Inffeldgasse 16a, A-8010 Graz, Austria, stillich@iaik.tugraz.at; Graz University of Technology, Institute for Applied Information Processing and Communications, Inffeldgasse 16a, A-8010 Graz, Austria, chrech@iaik.tugraz.at; Graz University of Technology, Institute for Applied Information Processing and Communications, Inffeldgasse 16a, A-8010 Graz, Austria, mhofmann@sbox.tugraz.at; Graz University of Technology, Institute for Applied Information Processing and Communications, Inffeldgasse 16a, A-8010 Graz, Austria, koermy@sbox.tugraz.at","2007 Design, Automation & Test in Europe Conference & Exhibition","","2007","","","1","6","Software implementations of modern block ciphers often require large lookup tables along with code size increasing optimizations like loop unrolling to reach peak performance on general-purpose processors. Therefore, block ciphers are difficult to implement efficiently on embedded devices like cell phones or sensor nodes where run-time memory and program ROM are scarce resources. In this paper, the performance, energy consumption, runtime memory requirements, and code size of the five block ciphers RC6, Rijndael, Serpent, Twofish, and XTEA on the StrongARM SA-1100 processor was analyzed and compared. Most previous evaluations of block ciphers considered performance as the sole metric of interest and did not care about memory requirements or code size. In contrast to previous work, this study of the performance and energy characteristics of block ciphers has been conducted with ""lightweight"" implementations which restrict the size of lookup tables to 1 kB and also impose constraints on the code size. The author found that Rijndael and RC6 can be well optimized for high performance and energy efficiency, while at the same time meeting the demand for low memory (RAM and ROM) footprint. In addition, the impact of key expansion and modes of operation on the overall performance and energy consumption of each block cipher was discussed. Simulation results show that RC6 is the most energy-efficient block cipher under memory constraints and thus the best choice for resource-restricted devices","1530-1591;1558-1101","978-3-9810801-2","10.1109/DATE.2007.364443","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4211953","Lightweight cryptography;symmetric cipher;energy optimization;memory footprint;code size reduction","Memory management;Table lookup;Runtime;Read only memory;Energy consumption;Energy efficiency;Random access memory;Software performance;Cellular phones;Performance analysis","cryptography;embedded systems;energy consumption;microprocessor chips;semiconductor storage","energy evaluation;block cipher software implementation;memory constraints;runtime memory requirements;RC6;Rijndael;Serpent;Twofish;XTEA;StrongARM SA-1100 processor;lightweight implementations;lightweight cryptography;symmetric cipher;memory footprint;code size reduction;1 kByte","","5","19","","","","","","IEEE","IEEE Conferences"
"Fast algorithms for the computation of Ranklets","F. Smeraldi","Queen Mary University of London, School of Electronic Engineering and Computer Science, Mile End Road, London E1 4NS, UK","2009 16th IEEE International Conference on Image Processing (ICIP)","","2009","","","3969","3972","Ranklets are orientation selective rank features with applications to tracking, face detection, texture and medical imaging. We introduce efficient algorithms that reduce their computational complexity from O(N log N) to O(¿N + k), where N is the area of the filter. Timing tests show a speedup of one order of magnitude for typical usage, which should make Ranklets attractive for real-time applications.","1522-4880;1522-4880;2381-8549","978-1-4244-5654-3978-1-4244-5653-6978-1-4244-5655","10.1109/ICIP.2009.5413803","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5413803","Ranklets;Rank features;Orientation decomposition;Wilcoxon statistics;Distribution counting sort","Brightness;Statistics;Application software;Face detection;Filters;Timing;Biomedical engineering;Computer science;Biomedical imaging;Testing","computational complexity;wavelet transforms","ranklets;orientation selective rank feature","","","18","","","","","","IEEE","IEEE Conferences"
"Signature Evaluation for Thermal Infrared Countermine and IED Detection Systems","J. Peters; S. Howington; O. Eslinger; J. Fairley; J. Ballard; R. Goodson; V. Carpenter","NA; NA; NA; NA; NA; NA; NA","2008 DoD HPCMP Users Group Conference","","2008","","","349","353","The countermine computational testbed is a coupled system of parallelized computer codes and associated software tools that can simulate images from remote sensors operating in the visible and infrared portions of the electromagnetic spectrum. The testbed is used to explore thermal and hydrologic process interaction at a fine scale to understand the contrast between target and background. The testbed provides an opportunity to exploit knowledge of the ground state (soil moisture, soil, or vegetation temperature) and material properties (hydraulic and thermal), to optimize sensor selection and time of flight, and to improve automated target recognition algorithms (ATRs) by limiting false alarms. For example, the testbed provided simulated data to provide field guidance on sensor performance taking into account weather events. The use of simulated data to improve ATR performance is described in detail.","","978-0-7695-3515-9978-1-4244-3323","10.1109/DoD.HPCMP.UGC.2008.87","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4755889","","Infrared detectors;Concurrent computing;Optical computing;Temperature sensors;Software testing;System testing;Electromagnetic coupling;Software tools;Computational modeling;Computer simulation","infrared imaging;landmine detection;object detection;remote sensing;software tools","signature evaluation;thermal infrared countermine;IED detection systems;countermine computational;parallelized computer codes;software tools;remote sensors;electromagnetic spectrum;automated target recognition algorithms","","1","3","","","","","","IEEE","IEEE Conferences"
"Componentizing hardware/software interface design","Kecheng Hao; Fei Xie","Department of Computer Science, Portland State University, OR 97207, USA; Department of Computer Science, Portland State University, OR 97207, USA","2009 Design, Automation & Test in Europe Conference & Exhibition","","2009","","","232","237","Building highly optimized embedded systems demands hardware/software (HW/SW) co-design. A key challenge in co-design is the design of HW/SW interfaces, which is often a design bottleneck. We propose a novel approach to HW/SW interface design based on the concept of bridge component. Bridge components fill the HW/SW semantic gap by propagating events across the HW/SW boundary and raise the abstraction level for designing HW/SW interfaces by abstracting processors, buses, embedded OS, etc. of embedded system platforms. Bridge components are specified in platform-specific bridge specification languages (BSLs) and compiled by the BSL compilers for simulation and deployment.We have applied our approach to two different embedded system platforms. Case studies have shown that bridge components greatly simplify component-based co-design of embedded systems and system simulation speed can be improved three orders of magnitude by simulating bridge components on the transaction level.","1530-1591;1558-1101","978-1-4244-3781-8978-3-9810-8015","10.1109/DATE.2009.5090663","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5090663","","Hardware;Software design;Bridges;Embedded system;Computational modeling;Specification languages;Acceleration;Sensor systems;Computer science;Computer interfaces","embedded systems;hardware-software codesign;object-oriented programming;program compilers;specification languages","hardware-software interface design;bridge component-based co-design;embedded system;bridge specification language;BSL compiler","","2","17","","","","","","IEEE","IEEE Conferences"
"An Efficient Algorithm for Clustering Search Engine Results","H. Zhang; B. Pang; K. Xie; H. Wu","National Laboratory of Software Development Environment, Beihang University, Beijing 100083, China. hzhang@nlsde.buaa.edu.cn; National Laboratory of Software Development Environment, Beihang University, Beijing 100083, China. pangbin@nlsde.buaa.edu.cn; National Laboratory of Software Development Environment, Beihang University, Beijing 100083, China. xieke@nlsde.buaa.edu.cn; National Laboratory of Software Development Environment, Beihang University, Beijing 100083, China. wuhui@nlsde.buaa.edu.cn","2006 International Conference on Computational Intelligence and Security","","2006","2","","1429","1434","With the increasing number of Web documents in the Internet, the most popular keyword-matching-based search engines, such as Google, often return a long list of search results ranked based on their relevance and importance to the query. To cluster the search engine results can help users find the results in several clustered collections, so it is easy to locate the valuable search results that the users really needed. In this paper, we propose a new key-feature clustering (KFC) algorithm which firstly extracts the significant keywords from the results as key features and cluster them, then clusters the documents based on these clustered key features. At last, the paper presents and analyzes the results from experiments we conducted to test and validate the algorithm","","1-4244-0604-81-4244-0605","10.1109/ICCIAS.2006.295296","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4076202","","Clustering algorithms;Search engines;Frequency;Algorithm design and analysis;Internet;Flowcharts;Software algorithms;Programming;Testing;Web pages","document handling;Internet;pattern clustering;relevance feedback;search engines","search engine result clustering;Web documents;Internet;keyword matching;Google;key-feature clustering","","1","11","","","","","","IEEE","IEEE Conferences"
"Collaborative architecture framework for the design & manufacturing of medical devices","C. Aguwa; L. Monplaisir","Industrial & Manufacturing Engineering, Wayne State University, Detroit, MI ¿ USA; Industrial & Manufacturing Engineering, Wayne State University, Detroit, MI ¿ USA","PICMET '09 - 2009 Portland International Conference on Management of Engineering & Technology","","2009","","","2246","2249","The purpose of this project is to develop a modular architecture framework for the design and manufacture of medical devices. This modular framework aims to incorporate design variables and criteria that are unique to the medical domain to facilitate reliable operation, easier maintenance, and faster product development time. Central to this research effort is the need for inputs from range of stakeholders. The specific goals for this effort are: to determine design criteria by collaborating with users and manufacturers of medical equipment and literature search; to translate user inputs to specific design targets; to develop a preliminary modular design framework using multi criteria optimization methods; to test preliminary modular architecture using a simple medical device such as a glucometer. The importance of the research with respect to its application in the medical arena can be very significant. With the product interaction with humans, both on the manufacturing level, and the user level, the issue of safety is paramount. Some of the other significant contributions are in the improvement of the following: product quality and reliability; product life cycle issues; an enabler for the medical community.","2159-5100;2159-5119","978-1-890843-20","10.1109/PICMET.2009.5261849","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5261849","","Collaboration;Costs;Medical tests;Maintenance;Manufacturing industries;Biomedical engineering;Product development;Product design;Assembly;Testing","biomedical equipment;groupware;manufacturing systems;optimisation;product life cycle management;software architecture","collaborative architecture;medical devices design;medical devices manufacturing;medical equipment;literature search;multicriteria optimization;product interaction;product quality;product reliability;product life cycle","","1","23","","","","","","IEEE","IEEE Conferences"
"S.T.R.E.S.S. : Stress Testing and Reverse Engineering for System Security","M. Rosi; L. Maccari; R. Fantacci","NA; NA; NA","2007 IEEE International Conference on Communications","","2007","","","1429","1434","In modern wireless networks the functions included into layer II have to deal with complex problems, such as security and access control, that were previously demanded to upper layers. This growing complexity led some vendors to implement layer II primitives directly in software, e.g. IEEE 802.111 has been largely distributed as a software patch to be used with legacy 802.11b/g hardware. In any extremely complex software the likelihood of committing errors during the implementation raises, and it is well known that software bugs can lead to instability of the system and possibly to security vulnerability. Software bugs are the most common cause of successful attacks against any kind of network and represent a real plague for system administrators. Stress test is a widely used methodology to find and eliminate software bugs. In this paper we present a platform to perform a stress test of generic network protocols implementations but especially optimized for Layer II stress tests, that present specific problems. With our approach a generic network protocol described with ABNF language can be tested transmitting arbitrary frame sequences and interpreting the responses to verify consistence with the communication standard used. Our platform can interact dynamically with the tested machine (an access point, a router etc.) to verify its robustness and its compliance with the standard. Experiments confirmed the validity of our approach both as a stress test technique for system under development and as a reverse engineering technique for interaction with closed source system.","1550-3607;1938-1883","1-4244-0353","10.1109/ICC.2007.240","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4288911","","Stress;System testing;Reverse engineering;Security;Computer bugs;Wireless networks;Access control;Hardware;Software testing;Performance evaluation","computer networks;protocols;radiocommunication;reverse engineering;telecommunication security","stress testing;reverse engineering;system security;wireless networks;generic network protocol;closed source system","","","10","","","","","","IEEE","IEEE Conferences"
"A nonparametric statistical approach for stereo correspondence","S. Candemir; Y. S. Akgul","GIT Vision Lab, Department Of Computer Engineering, Gebze Institute Of Technology, Kocaeli 41400, Turkey; GIT Vision Lab, Department Of Computer Engineering, Gebze Institute Of Technology, Kocaeli 41400, Turkey","2007 22nd international symposium on computer and information sciences","","2007","","","1","6","This paper introduces a novel non-parametric statistical metric that can decide if the recovered set of parameters from a computer vision optimization process can actually be considered as a statistically significant solution. The level of significance can be used as a quality metric of the solution which makes it possible (i) to compare the solutions obtained using different optimization methods, and also (ii) to set intuitive thresholds on the acceptance criteria. We chose the stereo correspondence optimization methods as the initial test bed for the new technique. We compare and combine the results of sum of squared differences (SSD) and sum of absolute differences (SAD) methods for stereo correspondence. We validated our claims by running experiments on standard stereo pairs with ground truth. The results showed that the introduced ideas actually work very well and they can be used to improve the optimization results from different sources.","","978-1-4244-1363-8978-1-4244-1364","10.1109/ISCIS.2007.4456866","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4456866","","Computer vision;Stereo vision;Design optimization;Optimization methods;Testing;Image sequences;Reliability engineering;Application software;Solid modeling;Parameter extraction","optimisation;statistical analysis;stereo image processing","stereo correspondence optimization;nonparametric statistical metric;computer vision optimization;sum-of-squared differences method;sum-of-absolute differences method","","1","11","","","","","","IEEE","IEEE Conferences"
"GoalDebug: A Spreadsheet Debugger for End Users","R. Abraham; M. Erwig","Oregon State University, USA; Oregon State University, USA","29th International Conference on Software Engineering (ICSE'07)","","2007","","","251","260","We present a spreadsheet debugger targeted at end users. Whenever the computed output of a cell is incorrect, the user can supply an expected value for a cell, which is employed by the system to generate a list of change suggestions for formulas that, when applied, would result in the user-specified output. The change suggestions are ranked using a set of heuristics. In previous work, we had presented the system as a proof of concept. In this paper, we describe a systematic evaluation of the effectiveness of inferred change suggestions and the employed ranking heuristics. Based on the results of the evaluation we have extended both, the change inference process and the ranking of suggestions. An evaluation of the improved system shows that change inference process and the ranking heuristics have both been substantially improved and that the system performs effectively.","0270-5257;1558-1225","0-7695-2828","10.1109/ICSE.2007.39","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4222587","","Programming profession;Error correction;Performance evaluation;Automatic testing;Fault diagnosis;NIST;Software debugging;Software testing","program debugging;user centred design","GoalDebug;spreadsheet debugger;end users;user-specified output;ranking heuristics;change inference process","","29","28","","","","","","IEEE","IEEE Conferences"
"Effect of Thermal strain in helical slow-wave circuit on TWT Cold-test characteristics","L. Yao; Z. Yang; Z. Huo; X. Zhu; B. Li","Institute of High Energy Electronics, University of Electronic Science and Technology of China, NO.4, Section 2, North Jianshe Road, Chengdu, P.R. China 610054, Email: ylm@uestc.edu.cn, Tel: 86-28-8320-3371, Fax: 86-28-8320-3371; Institute of High Energy Electronics, University of Electronic Science and Technology of China, NO.4, Section 2, North Jianshe Road, Chengdu, P.R. China 610054; Institute of High Energy Electronics, University of Electronic Science and Technology of China, NO.4, Section 2, North Jianshe Road, Chengdu, P.R. China 610054; Institute of High Energy Electronics, University of Electronic Science and Technology of China, NO.4, Section 2, North Jianshe Road, Chengdu, P.R. China 610054; Institute of High Energy Electronics, University of Electronic Science and Technology of China, NO.4, Section 2, North Jianshe Road, Chengdu, P.R. China 610054","2007 IEEE International Vacuum Electronics Conference","","2007","","","1","2","The heat transfer analysis presented in this study deals with the thermal performance of the helix slow-wave circuit for TWT. The current paper estimate the temperature of helix accurately through finite element software ANSYS. The expansion of rods makes the helix concave or convex at different position. Thermal strain made the radius of the tape increased overall, so the phase velocity decreases as the temperature of helix gets higher. But the impedance keeps almost the same value as the temperature increasing. Based on this kind of information, manufacturers can adjust and optimize the designs of devices before fabrication.","","1-4244-0633","10.1109/IVELEC.2007.4283237","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4283237","","Capacitive sensors;Circuits;Temperature;Heat transfer;Performance analysis;Finite element methods;Impedance;Manufacturing;Design optimization;Fabrication","finite element analysis;heat transfer;slow wave structures","thermal strain effect;helical slow-wave circuit;TWT cold-test characteristics;heat transfer analysis;phase velocity;finite element software ANSYS","","3","3","","","","","","IEEE","IEEE Conferences"
"Commissioning of the ATLAS reconstruction software with first data","W. Lampl","NA","2009 IEEE Nuclear Science Symposium Conference Record (NSS/MIC)","","2009","","","1179","1182","Looking towards first LHC collisions, the ATLAS detector is being commissioned using all types of physics data available: cosmic rays and events produced during a few days of LHC single beam operations. In addition to putting in place the trigger and data acquisition chains, commissioning of the full software chain is a main goal. This is interesting not only to ensure that the reconstruction, and monitoring chains are ready to deal with LHC physics data, but also to understand the detector performance in view of achieving the physics requirements. The recorded data have allowed us to study the ATLAS detector in terms of efficiencies, resolutions, channel integrity, alignment and calibrations. They have also allowed us to test and optimize the sub-systems reconstruction as well as some combined algorithms, such as combined tracking tools and different muon identification algorithms. The status of the integration of the complete software chain will be presented as well as the data analysis results.","1082-3654;1082-3654","978-1-4244-3961-4978-1-4244-3962","10.1109/NSSMIC.2009.5402401","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5402401","ATLAS;Reconstruction;Commissioning","Large Hadron Collider;Physics;Detectors;Event detection;Cosmic rays;Data acquisition;Monitoring;Calibration;Testing;Mesons","data acquisition;position sensitive particle detectors","ATLAS reconstruction software;Large Hadron Collider collisions;ATLAS detector;cosmic rays;LHC single beam operations;data acquisition chains;software chain;reconstruction software;monitoring chains;detector performance;recorded data;muon identification algorithms","","","2","","","","","","IEEE","IEEE Conferences"
"Runtime Optimization of Application Level Communication Patterns","E. Gabriel; S. Huang","Department of Computer Science, University of Houston, Houston, TX, USA. gabriel@cs.uh.edu; Department of Computer Science, University of Houston, Houston, TX, USA. shhuang@cs.uh.edu","2007 IEEE International Parallel and Distributed Processing Symposium","","2007","","","1","8","This paper introduces the Abstract Data and Communication Library (ADCL). ADCL is an application level communication library aiming at providing the highest possible performance for application level communication operations on a given execution environment. The library provides for each communication pattern a large number of implementations and incorporates a runtime selection logic in order to choose the implementation leading to the highest performance of the application on the current platform. Two different runtime selection algorithms are currently available within ADCL: the library can either apply a brute force search strategy which tests all available implementations of a given communication pattern; alternatively, a heuristic relying on attributes characterizing an implementation has been developed in order to speed up the runtime decision procedure. The paper also evaluates the performance of a finite difference code using ADCL on an AMD Opteron cluster using InfiniBand and Gigabit Ethernet interconnects.","1530-2075","1-4244-0909-81-4244-0910","10.1109/IPDPS.2007.370406","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4228134","","Application software;Runtime library;Ethernet networks;Software performance;Equations;Testing;Finite difference methods;Hardware;Communication system software;Iterative algorithms","parallel programming;program compilers;software libraries","runtime optimization;application level communication patterns;Abstract Data and Communication Library;ADCL application level communication library;runtime selection logic;AMD Opteron cluster;InfiniBand;Gigabit Ethernet interconnects","","4","11","","","","","","IEEE","IEEE Conferences"
"Comparative Performance Analysis of CAC Reward Optimization Algorithms in Wireless Networks","O. Yilmaz; I. Chen","NA; NA","2009 International Conference on Advanced Information Networking and Applications","","2009","","","463","470","In this paper, we perform a comparative analysis of a set of call admission control (CAC) algorithms designed for servicing multiple priority classes in wireless networks with quality of service (QoS) guarantees. We evaluate the performance of partitioning, threshold-based, spillover, and elastic-threshold CAC algorithms in terms of the maximum number of mobile users the system is able to support with QoS satisfaction while maximizing the ""reward"" obtainable from servicing multiple priority classes with distinct QoS requirements. We compare these algorithms thoroughly with test cases generated through a combination of user workload, mobility, location, and population. We verify analytical results via simulation validation using real mobility trace data to model user mobility. We also analyze the tradeoff between solution optimality vs. solution efficiency in designing CAC algorithms for reward optimization and QoS satisfaction when servicing multiple service classes with distinct QoS requirements in wireless networks.","1550-445X;2332-5658","978-1-4244-4000-9978-0-7695-3638","10.1109/AINA.2009.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5076235","Admission control;QoS guarantees;performance analysis;reward optimization;mobile networks.","Performance analysis;Wireless networks;Quality of service;Partitioning algorithms;Algorithm design and analysis;Design optimization;Call admission control;Analytical models;Bandwidth;Application software","mobile radio;optimisation;quality of service;telecommunication congestion control","CAC reward optimization algorithm;performance analysis;call admission control;quality of service;QoS;mobile user;wireless network","","","17","","","","","","IEEE","IEEE Conferences"
"Persistent Code Caching: Exploiting Code Reuse Across Executions and Applications","V. J. Reddi; D. Connors; R. Cohn; M. D. Smith","Harvard University; University of Colorado; Intel Corporation; Harvard University","International Symposium on Code Generation and Optimization (CGO'07)","","2007","","","74","88","Run-time compilation systems are challenged with the task of translating a program's instruction stream while maintaining low overhead. While software managed code caches are utilized to amortize translation costs, they are ineffective for programs with short run times or large amounts of cold code. Such program characteristics are prevalent in real-life computing environments, ranging from graphical user interface (GUI) programs to large-scale applications such as database management systems. Persistent code caching addresses these issues. It is described and evaluated in an industry-strength dynamic binary instrumentation system - Pin. The proposed approach improves the intra-execution model of code reuse by storing and reusing translations across executions, thereby achieving inter-execution persistence. Dynamically linked programs leverage inter-application persistence by using persistent translations of library code generated by other programs. New translations discovered across executions are automatically accumulated into the persistent code caches, thereby improving performance over time. Inter-execution persistence improves the performance of GUI applications by nearly 90%, while inter-application persistence achieves a 59% improvement. In more specialized uses, the SPEC2K INT benchmark suite experiences a 26% improvement under dynamic binary instrumentation. Finally, a 400% speedup is achieved in translating the Oracle database in a regression testing environment","","0-7695-2764","10.1109/CGO.2007.29","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4145106","","Graphical user interfaces;Instruments;Virtual manufacturing;Costs;Databases;Application software;Libraries;Runtime;Computer interfaces;Testing","cache storage;program compilers;program interpreters;software reusability","persistent code caching;code reuse;run-time compilation;program instruction stream;software managed code caches;graphical user interface;large-scale applications;Pin dynamic binary instrumentation system;Oracle database;regression testing environment","","4","35","","","","","","IEEE","IEEE Conferences"
"Development of a Fuel Cell Hybrid Low-Speed Electric Vehicle Testing Facility","S. Tezcan; Z. Dong; A. Suleman","Northwest Conveyer Ltd, Vancouver, Canada.; Professor, Mechanical Engineering, University of Victoria, Victoria, British Columbia, V8W3P6, Canada, phone: 250-721-8900; fax: 250-721-6051; e-mail: zdong@me.uvic.ca; Professor, Mechanical Engineering, University of Victoria, Victoria, British Columbia, V8W3P6, Canada","2006 2nd IEEE/ASME International Conference on Mechatronics and Embedded Systems and Applications","","2006","","","1","8","A dynamometer system for testing fuel cell hybrid low-speed electric vehicles (FCHLSEVs) is developed in this work. The system is designed to assist in the analysis and development of FCHLSEVs as well as in the validation of current vehicle performance modeling methods. It will be used as a design and evaluation tool for various vehicular fuel cell power systems and low-speed electric vehicles. The system consists of: roller units, eddy-current absorber and controller, a custom platform for housing, custom connection and adjustment parts, data acquisition electronics and software, and research software. The paper presents the dynamometer system design; the mechanics model of low-speed vehicles and their testing methods that is used in the design and optimization of FCHLSEV, and the operation of the dynamometer system; the designed dynamometer platform and its variations for testing of various FCHLSEVs, including fuel cell battery hybrid electric scooters and utility vehicles. The work provides a unique research tool for developing the mathematical models and carrying out the design optimization of FCHLSEVs","","0-7803-9721","10.1109/MESA.2006.296977","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4077804","dynamometer;fuel cells;hybrid vehicle;low-speed electric vehicles","Fuel cells;Hybrid electric vehicles;System testing;Power system modeling;Hybrid power systems;Battery powered vehicles;Fuel cell vehicles;Design optimization;Performance analysis;Power system analysis computing","battery powered vehicles;dynamometers;fuel cell vehicles;hybrid electric vehicles;testing","fuel cell hybrid low-speed electric vehicle;testing facility;dynamometer system;vehicle performance modeling methods;vehicular fuel cell power systems;roller units;eddy-current absorber;custom connection;adjustment parts;data acquisition electronics;fuel cell battery;hybrid electric scooters;utility vehicles","","1","20","","","","","","IEEE","IEEE Conferences"
"Rate Distortion Optimization for H.264 Interframe Coding: A General Framework and Algorithms","E. Yang; X. Yu","NA; NA","IEEE Transactions on Image Processing","","2007","16","7","1774","1784","Rate distortion (RD) optimization for H.264 interframe coding with complete baseline decoding compatibility is investigated on a frame basis. Using soft decision quantization (SDQ) rather than the standard hard decision quantization, we first establish a general framework in which motion estimation, quantization, and entropy coding (in H.264) for the current frame can be jointly designed to minimize a true RD cost given previously coded reference frames. We then propose three RD optimization algorithms-a graph-based algorithm for near optimal SDQ in H.264 baseline encoding given motion estimation and quantization step sizes, an algorithm for near optimal residual coding in H.264 baseline encoding given motion estimation, and an iterative overall algorithm to optimize H.264 baseline encoding for each individual frame given previously coded reference frames-with them embedded in the indicated order. The graph-based algorithm for near optimal SDQ is the core; given motion estimation and quantization step sizes, it is guaranteed to perform optimal SDQ if the weak adjacent block dependency utilized in the context adaptive variable length coding of H.264 is ignored for optimization. The proposed algorithms have been implemented based on the reference encoder JM82 of H.264 with complete compatibility to the baseline profile. Experiments show that for a set of typical video testing sequences, the graph-based algorithm for near optimal SDQ, the algorithm for near optimal residual coding, and the overall algorithm achieve on average, 6%, 8%, and 12%, respectively, rate reduction at the same PSNR (ranging from 30 to 38 dB) when compared with the RD optimization method implemented in the H.264 reference software.","1057-7149;1941-0042","","10.1109/TIP.2007.896685","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4237212","Fixed-slope lossy compression;H.264 hybrid coding;rate distortion (RD) optimization;soft decision quantization (SDQ)","Rate-distortion;Quantization;Iterative algorithms;Motion estimation;Encoding;Software algorithms;Decoding;Code standards;Entropy coding;Costs","entropy codes;motion estimation;quantisation (signal);residue codes;video coding","rate distortion optimization;interframe coding;soft decision quantization;hard decision quantization;motion estimation;entropy coding;graph-based algorithm;baseline encoding;residual coding;JM82;video testing sequences;H.264 reference software","Algorithms;Artifacts;Computer Graphics;Data Compression;Data Compression;Documentation;Image Enhancement;Image Interpretation, Computer-Assisted;Internationality;Multimedia;Numerical Analysis, Computer-Assisted;Signal Processing, Computer-Assisted;Video Recording","67","31","","","","","","IEEE","IEEE Journals & Magazines"
"Ants and Multiple Knapsack Problem","U. Boryczka","University of Silesia, Poland","6th International Conference on Computer Information Systems and Industrial Management Applications (CISIM'07)","","2007","","","149","154","In this paper a new optimization algorithm based on ant colony metaphor (ACO)and a new approach for the Multiple Knapsack Problem is presented. The MKP is the problem of assigning a subset of n items to m distinct knapsacks, such that the total profit sum of the selected items is maximized, without exceeding the capacity of each of the knapsacks. The problem has several difficulties in adaptation as well as the trail representation of the solutions of MKP or a dynamically changed heuristic function applied in this approach. Presented results show the power of the ACO approach for solving this type of subset problems.","","0-7695-2894","10.1109/CISIM.2007.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4273512","ant colony optimization;multiple knapsack problem;combinatorial optimization.","Ant colony optimization;Chemicals;Computer science;Testing;Management information systems;Computer industry;Conference management;Application software;Feedback;Distributed computing","knapsack problems;optimisation","multiple knapsack problem;ant colony optimization;heuristic function","","6","12","","","","","","IEEE","IEEE Conferences"
"Optimized Restoration of Unbalanced<?Pub _bookmark="""" Command=""here""?>Distribution Systems","S. Khushalani; J. M. Solanki; N. N. Schulz","NA; NA; NA","IEEE Transactions on Power Systems","","2007","22","2","624","630","A novel formulation for service restoration algorithm for unbalanced three phase distribution systems is described. This problem is a constrained multiobjective optimization formulated as a mixed integer non-linear programming problem. A comparison of the solutions with and without switch pairs has been made. The formulation was first validated using already developed three-phase unbalanced power flow software. The three-phase unbalanced power flow equations were embedded in the formulation, and hence separate calculations were not needed. Simulation results are presented for modified IEEE 13-node and IEEE 37-node test cases","0885-8950;1558-0679","","10.1109/TPWRS.2007.894866","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4162603","Load flow analysis;optimization methods;power distribution;power distribution control;power system restoration;shipboard power system","Power system restoration;Switches;Load flow;Equations;Power system simulation;Power distribution;Power system control;Power system analysis computing;Power system reliability;Load management","integer programming;load flow;nonlinear programming;power distribution;power system faults;power system restoration;power system simulation","service restoration optimization;unbalanced three-phase distribution systems;constrained multiobjective optimization;mixed integer nonlinear programming problem;three-phase unbalanced power flow software;IEEE 13-node tests;IEEE 37-node tests","","39","20","","","","","","IEEE","IEEE Journals & Magazines"
"An Evolutionary Particle Swarm Optimization algorithm for data clustering","S. Alam; G. Dobbie; P. Riddle","Department of Computer Science, the University of Auckland, Private Bag 92019, New Zealand; Department of Computer Science, the University of Auckland, Private Bag 92019, New Zealand; Department of Computer Science, the University of Auckland, Private Bag 92019, New Zealand","2008 IEEE Swarm Intelligence Symposium","","2008","","","1","6","Clustering is an important data mining task and has been explored extensively by a number of researchers for different application areas such as finding similarities in images, text data and bio-informatics data. Various optimization techniques have been proposed to improve the performance of clustering algorithms. In this paper we propose a novel algorithm for clustering that we call evolutionary particle swarm optimization (EPSO)-clustering algorithm which is based on PSO. The proposed algorithm is based on the evolution of swarm generations where the particles are initially uniformly distributed in the input data space and after a specified number of iterations; a new generation of the swarm evolves. The swarm tries to dynamically adjust itself after each generation to optimal positions. The paper describes the new algorithm the initial implementation and presents tests performed on real clustering benchmark data. The proposed method is compared with k-means clustering- a benchmark clustering technique and simple particle swarm clustering algorithm. The results show that the algorithm is efficient and produces compact clusters.","","978-1-4244-2704-8978-1-4244-2705","10.1109/SIS.2008.4668294","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4668294","","Particle swarm optimization;Clustering algorithms;Partitioning algorithms;Data mining;Benchmark testing;USA Councils;Computer science;Application software;Bioinformatics;Performance evaluation","data mining;evolutionary computation;particle swarm optimisation;pattern clustering","data clustering;evolutionary particle swarm optimization algorithm;data mining;swarm generation evolution;iteration method","","27","13","","","","","","IEEE","IEEE Conferences"
"A novel analogous simulation method with applications on 3G mobile system planning and optimization","Ren Gang; Chen Binghai","Sch. of Electron. Eng., Sichuan Univ., Chengdu, China; NA","2006 8th International Conference Advanced Communication Technology","","2006","2","","6 pp.","1320","The authors first introduce the background of 3G mobile communication systems and its simulation, after that, a new analogous simulation method is introduced to improve the authentication of computer software simulation result. The new simulation method is then compared with traditional simulation method and is further illustrated with two 3G mobile communication system applications. Finally, the authors analyze the performance of this new simulation method and provide a comparison between the analogous simulation result they derived from the simulation example on 2.5G mobile communication system and the test result of an actual 3G experimental system, which was constructed in Nanchong, Sichuan province, China. The simulation system also contains a Web based platform enable operating over the user friendly interface","","89-5519-129","10.1109/ICACT.2006.206213","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1625818","Simulation;Wireless Communications;Modeling;3G;Communication Software","Optimization methods;Computational modeling;3G mobile communication;Computer simulation;Analytical models;Application software;Authentication;Communication system software;Performance analysis;System testing","3G mobile communication;telecommunication computing","3G mobile system planning;analogous simulation method;3G experimental system;Web based platform","","","17","","","","","","IEEE","IEEE Conferences"
"A Novel Search Biases Selection Strategy for Constrained Evolutionary Optimization","Min Zhang; Huantong Geng; Wenjian Luo; Linfeng Huang; Xufa Wang","Nature Inspired Computation and Applications Laboratory, Department of Computer Science and Technology, University of Science and Technology of China (phone: 86-0551-3602824-5; email: zhangmin@mail.ustc.edu.cn); NA; NA; NA; NA","2006 IEEE International Conference on Evolutionary Computation","","2006","","","1845","1850","The issues of the search biases selection based on stochastic ranking are pointed out by an example with three possible outputs and are also demonstrated by an experiment designed here. In order to improve the explicit search biases ability in feasible regions, three conditions for explicit search biases are presented and a novel search biases selection strategy with stochastic ranking is proposed in this paper. This strategy is applied to our new algorithm based on ES (evolution strategy). The new algorithm has been tested on 13 common benchmark functions and the experimental results have demonstrated that to some extent the convergence speed, the numerical accuracy and stability of best solutions are improved.","1089-778X;1941-0026","0-7803-9487","10.1109/CEC.2006.1688531","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1688531","","Constraint optimization;Stochastic processes;Computer applications;Application software;Laboratories;Computer science;Benchmark testing;Convergence of numerical methods;Numerical stability;Evolutionary computation","constraint handling;evolutionary computation;search problems;stochastic processes","search biases selection strategy;constrained evolutionary optimization;stochastic ranking","","1","15","","","","","","IEEE","IEEE Conferences"
"Dynamics model research of centrifugal barrel surface finishing based on 3D Discrete Element Method","Song Chun-hua; Yang Shi-chun","College of Computer Software Engineering, Taiyuan University of Technology, China; College of Mechanical Engineering, Taiyuan University of Technology, China","2009 4th IEEE Conference on Industrial Electronics and Applications","","2009","","","942","946","Dynamics model on abrasive particles and workpieces during the centrifugal barrel surface finishing process was built by Discrete Element Method (DEM). The motion status of abrasive particles and workpieces was visually simulated and some important process parameters were numerically studied. The influence of planetary transmission ratio on finishing efficiency was analyzed, and finishing efficiency was the highest as the transmission ratio is -1. Finally, the conclusion was validated by experiments. Such economical and precise method could replace lots of finishing tests to optimize process parameters, which would provide theoretic basis for experimental research and efficient means for the finishing mechanism analysis of centrifugal barrel finishing.","2156-2318;2158-2297","978-1-4244-2799-4978-1-4244-2800","10.1109/ICIEA.2009.5138339","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5138339","Centrifugal Barrel Finishing;Discrete Element Method;Dynamics Model;Simulation","Surface finishing;Abrasives;Educational institutions;Process design;Software engineering;Mechanical engineering;Testing;Optimization methods;Deburring;Design optimization","abrasives;finite element analysis;surface finishing","centrifugal barrel surface finishing;3D discrete element method;abrasive particles;planetary transmission;finishing efficiency;finishing mechanism analysis","","","10","","","","","","IEEE","IEEE Conferences"
"High-speed data-injection for data-flow verification at LHCb","O. Callot; M. Frank; J. Garnier; C. Gaspar; G. Liu; N. Neufeld; A. C. Smith; D. Sonnick; A. S. Varela","LAL, France; CERN, Switzerland; CERN, Switzerland; CERN, Switzerland; INFN, Italy; CERN, Switzerland; CERN, Switzerland; CERN, Switzerland; CERN, Switzerland","2009 16th IEEE-NPSS Real Time Conference","","2009","","","144","149","The High Level Trigger (HLT) and Data Acquisition System select about 2 kHz of events out of the 40 MHz of beam crossings. The selected events are consolidated into files on an onsite storage and then sent to permanent storage for subsequent analysis on the Grid. For local and full-chain tests a method to exercise the data-flow through the HLT when there are no actual data needed. In order to test the system as much as possible under identical conditions as for data-taking the solution would be to inject data at the input of the HLT at a minimum rate of 2 kHz. This is done via a software implementation of the trigger system which sends data to the HLT. The application has to simulate that the data it sends come from a real LHCb readout-boards. Data can come from several input streams, which are selected according to probabilities or frequencies. Therefore the emulator offers runs which are more than identical data-flows coming from a sequence on tape, but physics-like pseudo-indeterministic data-flow, including lumi events and likely interesting b-quark events. Both simulation data and previously recorded real data can be re-played through the system in this manner. As the data rate is high (100 MB/s), care has been taken to optimize the emulator for throughput from the Storage Area Network. The emulator can be run in stand-alone mode but is even more interesting that it can emulate any partition of LHCb in parallel with the real hardware partition. In this mode it is fully integrated into the standard run-control. The architecture, implementation, and performance results of the emulator and full tests will be presented. This emulator is a key piece in the ongoing data-challenges in LHCb. Results from these Full System Integration Tests (FEST) will be presented, which helped to verify and benchmark the entire LHCb data-flow.","","978-1-4244-4454-0978-1-4244-4455","10.1109/RTC.2009.5322115","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5322115","High Level Trigger;Data Acquisition;tests;emulator;simulation;benchmark","Data acquisition;System testing;Detectors;Large Hadron Collider;Benchmark testing;Real time systems;Application software;Frequency;Throughput;Storage area networks","data acquisition;data flow analysis;grid computing;nuclear electronics;program verification;trigger circuits","high-speed data-injection;data-flow verification;high level trigger;HLT;data acquisition system;beam crossings;full-chain tests;software implementation;LHCb readout-boards;identical data-flows;b-quark events;storage area network;stand-alone mode;real hardware partition;full system integration tests;FEST;LHCb data-flow;Grid;physics-like pseudo-indeterministic data-flow;Large Hadron Collider","","1","18","","","","","","IEEE","IEEE Conferences"
"Grid Technology Support Information and Application Service Interoperability","C. Degang; W. Yujuan; C. Wenyuan","NA; NA; NA","2009 International Conference on Interoperability for Enterprise Software and Applications China","","2009","","","114","119","Developing the technology and infrastructure which can enable efficient sharing of distributed resources and cooperative work on the scale of Internet is of significance for industrial application and investigation, especially for the complicated computation. BRIDGE project developed a Grid platform to realize the share without disclosing all details. Now, the platform has been proved by a testing that addresses an application in aviation industry.","","978-0-7695-3652","10.1109/I-ESA.2009.39","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5260853","Grid Technology;Interoperability;Optimization","Bridges;Application software;Aerospace industry;Computer industry;Collaborative work;Testing;Europe;Space technology;Grid computing;Virtual manufacturing","grid computing;Internet;open systems","grid technology support information;application service interoperability;distributed resources;cooperative work;Internet;BRIDGE project;aviation industry","","","11","","","","","","IEEE","IEEE Conferences"
"Optimizing non-blocking collective operations for infiniband","T. Hoefler; A. Lumsdaine","Open Systems Lab, Indiana University, Bloomington 47405, USA; Open Systems Lab, Indiana University, Bloomington 47405, USA","2008 IEEE International Symposium on Parallel and Distributed Processing","","2008","","","1","8","Non-blocking collective operations have recently been shown to be a promising complementary approach for overlapping communication and computation in parallel applications. However, in order to maximize the performance and usability of these operations it is important that they progress concurrently with the application without introducing CPU overhead and without requiring explicit user intervention. While studying non- blocking collective operations in the context of our portable library (libNBC), we found that most MPI implementations do not sufficiently support overlap over the InfiniBand network. To address this issue, we developed a low-level communication layer for libNBC based on the Open Fabrics InfiniBand verbs API. With this layer we are able to achieve high degrees of overlap without the need to explicitly progress the communication operations. We show that the communication overhead of parallel application kernels can be reduced up to 92% while not requiring user intervention to make progress.","1530-2075","978-1-4244-1693-6978-1-4244-1694","10.1109/IPDPS.2008.4536138","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4536138","","Niobium compounds;Testing;Open systems;Open source software;Software libraries;Computer architecture;Computer applications;Concurrent computing;Usability;Context","message passing;software libraries;software packages","nonblocking collective operations;overlapping communication;portable library;MPI implementations;InfiniBand network;parallel application kernels","","11","36","","","","","","IEEE","IEEE Conferences"
"Two analytical approaches for optimal placement of distributed generation unit in power systems","S. Ghosh; S. P. Ghoshal; S. Ghosh","Department of Electrical Engineering, Dr. B. C. Roy Engineering College, Durgapur, INDIA; Department of Electrical Engineering, National Institute Of Technology, Durgapur, INDIA; Department of Electrical Engineering, National Institute Of Technology, Durgapur, INDIA","2009 International Conference on Power Systems","","2009","","","1","6","With ever-increasing demand of electricity consumption and increasing open access particularly in restructured environment, transmission line congestion is quite frequent. For alleviating congestion, proper locations of DG are important. This paper focuses a comparative study between a novel bus reduction approach and modified load flow with optimization for determination of the optimal placement of single distributed generator in power systems. The objective of optimal placement of DG is aimed at ¿reduction in power losses¿. The two approaches are tested on IEEE 6 bus, IEEE 14 bus and IEEE 30 bus test systems and verified by Power World Simulator software. The voltage profile of the whole system after placement of DG at optimal location is also focused here.","","978-1-4244-4330-7978-1-4244-4331","10.1109/ICPWS.2009.5442750","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5442750","Analytical method;Distributed Generation (DG);Power loss;Voltage profile","Distributed control;Power system analysis computing;Power system simulation;Software testing;System testing;Energy consumption;Power transmission lines;Load flow;Distributed power generation;Power generation","distributed power generation;load flow;power systems;power transmission lines","distributed generation unit;electricity consumption;power systems;optimal placement analytical approach;transmission line congestion;bus reduction approach;modified load flow;power loss reduction;IEEE 6 bus test systems;IEEE 14 bus test systems;IEEE 30 bus test systems;power world Simulator software","","5","18","","","","","","IEEE","IEEE Conferences"
"Channel-optimized quantizers for decentralized detection in sensor networks","Bin Liu; Biao Chen","Dept. of Electr. Eng. & Comput.Sci., Syracuse Univ., NY, USA; Dept. of Electr. Eng. & Comput.Sci., Syracuse Univ., NY, USA","IEEE Transactions on Information Theory","","2006","52","7","3349","3358","Motivated by the delay and resource constraints omnipresent in most wireless sensor network applications, we design channel-optimized scalar quantizers for a canonical decentralized detection system. Aimed at minimizing the error probability of the fusion center output, we first establish the optimality of monotone likelihood ratio partition of the observation space for the local quantizer design. We then devise an iterative algorithm to construct distributed quantizers that are person-by-person optimal. The channel-optimized approach is shown to offer better performance compared with various alternatives. It also exhibits inherent adaptivity in resource (bit) allocation in response to varying channel conditions.","0018-9448;1557-9654","","10.1109/TIT.2006.876350","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1650382","Channel-optimized quantizer;distributed detection;distributed signal processing;joint source-channel coding;sensor networks","Intelligent networks;Wireless sensor networks;Iterative algorithms;Testing;Sensor fusion;Error probability;Delay;Application software;Sensor systems and applications;Quantization","wireless sensor networks;error statistics;iterative methods;resource allocation;sensor fusion;signal detection;optimisation;quantisation (signal);telecommunication channels","wireless sensor network application;channel-optimized scalar quantizer;decentralized detection system;error probability;fusion center;iterative algorithm;resource allocation","","45","19","","","","","","IEEE","IEEE Journals & Magazines"
"Using Higher Levels of Abstraction for Solving Optimization Problems by Boolean Satisfiability","R. Wille; D. Große; M. Soeken; R. Drechsler","NA; NA; NA; NA","2008 IEEE Computer Society Annual Symposium on VLSI","","2008","","","411","416","Optimization problems can be solved using Boolean satisfiability by mapping them to a sequence of decision problems. Therefore, in the last years several encodings have been developed. Independently, also new solvers have been introduced lifting Boolean satisfiability to higher levels of abstraction, e.g. SAT modulo theories (SMT) solvers and word level solvers. Both support bit-vector logic and thus allow more compact encodings of the problems. In this paper we investigate the efficiency of these new solver paradigms applied to optimization problems. We show for two case studies - graph coloring and exact synthesis of reversible logic - that the resulting problem instances can be reduced with respect to the size. In addition for the synthesis problem significant run-time improvements can be achieved.","2159-3469;2159-3477","978-0-7695-3170-0978-0-7695-3291","10.1109/ISVLSI.2008.82","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556830","Boolean Satisfiability;Optimization Problems;SMT;Bit-vector Logic;Word Level","Encoding;Surface-mount technology;Runtime;Application software;Computer Society;Very large scale integration;Computer science;Circuit testing;Logic gates;Art","Boolean algebra;computability;optimisation","abstraction;optimization problem;Boolean satisfiability;decision problem;bit-vector logic;compact encoding;graph coloring;reversible logic","","1","27","","","","","","IEEE","IEEE Conferences"
"A hybrid genetic algorithm for constrained hardware-software partitioning","P. -. Mudry; G. Zufferey; G. Tempesti","NA; NA; NA","2006 IEEE Design and Diagnostics of Electronic Circuits and systems","","2006","","","1","6","In this article, we propose a novel partitioning method for hardware-software codesign based on a genetic algorithm that has been enhanced for this specific task. Given a high-level program and an area constraint, our software considers different granularities levels to discover the most interesting blocks to be implemented in ad hoc functional units that can then be used as new instructions in a move processor. Various optimizations are conducted to obtain a clean, very fast (in the order of a few seconds) and efficient partitioning on programs ranging from a few to several hundreds of lines of code.","","1-4244-0185","10.1109/DDECS.2006.1649561","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1649561","","Genetic algorithms;Hardware;Computer architecture;Digital systems;Computer industry;Application software;Time factors;Simulated annealing;Clustering algorithms;Fuzzy logic","genetic algorithms;hardware-software codesign;logic partitioning","hybrid genetic algorithm;hardware-software partitioning method;hardware-software codesign;high-level program;area constraint;ad hoc functional units;move processor","","2","19","","","","","","IEEE","IEEE Conferences"
"HOPA: A Near Optimization Algorithm for Pathfinding","J. Wang; Q. Wang; B. Chen; H. Chen","NA; NA; NA; NA","2006 International Conference on Wireless Communications, Networking and Mobile Computing","","2006","","","1","4","Based on the classic Dijkstra algorithm, this paper introduces the decision mechanism of AI into pathfinding, proposes a heuristic optimistic pathfinding algorithm, HOPA, puts forward the conditions of getting the optimum solution from this algorithm and further gives the proof. These two algorithms are tested contrastively, and the results show that the algorithm proposed in this paper may have a good prospect in some fields","2161-9646;2161-9654","1-4244-0517","10.1109/WiCOM.2006.392","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4149569","","Geography;Heuristic algorithms;Databases;Virtual reality;Navigation;Information systems;Artificial intelligence;Testing;Computer networks;Application software","artificial intelligence;computational complexity;graph theory;heuristic programming","HOPA;AI decision mechanism;heuristic optimistic pathfinding algorithm","","","7","","","","","","IEEE","IEEE Conferences"
"WCET Analysis of Modern Processors Using Multi-Criteria Optimisation","U. Khan; I. Bate","NA; NA","2009 1st International Symposium on Search Based Software Engineering","","2009","","","103","112","The Worst-Case Execution Time (WCET) is an important execution metric for real-time systems, and an accurate estimate for this increases the reliability of subsequent schedulability analysis. Performance enhancing features on modern processors, such as pipelines and caches, however, make it difficult to accurately predict the WCET. One technique for finding the WCET is to use test data generated using search algorithms. Existing work on search-based approaches has been successfully used in both industry and academia based on a single criterion function, the WCET, but only for simple processors. This paper investigates how effective this strategy is for more complex processors and to what extent other criteria help guide the search, e.g. the number of cache misses. Not unexpectedly the work shows no single choice of criteria work best across all problems. Based on the findings recommendations are proposed on which criteria are useful in particular situations.","","978-0-7695-3675","10.1109/SSBSE.2009.20","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5033188","","Real time systems;Vehicle dynamics;Aerodynamics;Aerospace industry;Software engineering;Job shop scheduling;Processor scheduling;Information analysis;Software testing;Computer science","processor scheduling;program processors;search problems","WCET analysis;modern processors;multicriteria optimisation;worst-case execution time;execution metric;schedulability analysis;search algorithms;single criterion function","","6","21","","","","","","IEEE","IEEE Conferences"
"Dynamically Optimized Production Planning Using Cross-Layer SOA","D. Savio; S. Karnouskos; D. Wuwer; T. Bangemann","NA; NA; NA; NA","2008 32nd Annual IEEE International Computer Software and Applications Conference","","2008","","","1361","1365","Responding to the dynamic requirements of the changing market and optimising costs are two challenges faced by corporate manufacturing plants globally. In order to be agile, modern manufacturing plants employ optimized supply chain mechanisms to reduce the response time of the market needs. However bringing changes to the shop floor after a production is planned is costly. In this paper we demonstrate how Service Oriented Architecture (SOA) driven approaches offer the flexibility to adapt manufacturing plants based on a dynamic production plan in close cooperation with a backend system. The methods discussed were deployed on a prototype test rig and integrated with SAP ERP. The results introduce dynamic behaviour of the production plan by adapting to the changing nature of the shop floor, and at the same time, providing the real time status of the machines to the enterprise services which optimize further the production plan dynamically.","0730-3157;0730-3157","978-0-7695-3262","10.1109/COMPSAC.2008.219","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4591782","","Web services;Production;Software;Databases;Real time systems;Manufacturing;Image color analysis","enterprise resource planning;production engineering computing;production planning;software architecture;supply chain management","dynamically optimized production planning;cross-layer service oriented architecture;dynamic requirements;manufacturing plants;supply chain mechanisms;dynamic production plan;SAP ERP;enterprise services","","5","10","","","","","","IEEE","IEEE Conferences"
"On improving performance and energy profiles of sparse scientific applications","K. Malkowski; Ingyu Lee; P. Raghavan; M. J. Irwin","Dept. of Comput. Sci. & Eng., Pennsylvania State Univ., University Park, PA, USA; Dept. of Comput. Sci. & Eng., Pennsylvania State Univ., University Park, PA, USA; Dept. of Comput. Sci. & Eng., Pennsylvania State Univ., University Park, PA, USA; Dept. of Comput. Sci. & Eng., Pennsylvania State Univ., University Park, PA, USA","Proceedings 20th IEEE International Parallel & Distributed Processing Symposium","","2006","","","8 pp.","","In many scientific applications, the majority of the execution time is spent within a few basic sparse kernels such as sparse matrix vector multiplication (SMV). Such sparse kernels can utilize only a fraction of the available processing speed because of their relatively large number of data accesses per floating point operation, and limited data locality and data re-use. Algorithmic changes and tuning of codes through blocking and loop unrolling schemes can improve performance but such tuned versions are typically not available in benchmark suites such as the SPEC CFP 2000. In this paper, we consider sparse SMV kernels with different levels of tuning that are representative of this application space. We emulate certain memory subsystem optimizations using SimpleScalar and Wattch to evaluate improvements in performance and energy metrics. We also characterize how such an evaluation can be affected by the interplay between code tuning and memory subsystem optimizations. Our results indicate that the optimizations reduce execution time by over 40%, and the energy by over 85%, when used with power control modes of CPUs and caches. Furthermore, the relative impact of the same set of memory subsystem optimizations can vary significantly depending on the level of code tuning. Consequently, it may be appropriate to augment traditional benchmarks by tuned kernels typical of high performance sparse scientific codes to enable comprehensive evaluations of future systems.","1530-2075","1-4244-0054","10.1109/IPDPS.2006.1639589","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1639589","benchmarks;memory optimizations;voltage scaling;performance evaluation;application code tuning;power optimizations;sparse matrix kernels","Kernel;Sparse matrices;Application software;Computer architecture;Computational modeling;Power control;Scientific computing;Microprocessors;Computer science;Power engineering and energy","sparse matrices;vectors;optimisation;mathematics computing;power control;power engineering computing;benchmark testing","sparse scientific applications;sparse kernels;sparse matrix vector multiplication;data accesses;data locality;data reuse;benchmark suites;SPEC CFP 2000;memory subsystem optimizations;SimpleScalar;Wattch;energy metrics;performance evaluation;application code tuning;power control modes;CPU","","2","30","","","","","","IEEE","IEEE Conferences"
"Evaluation of a Decentralized Architecture for Large Scale Collaborative Intrusion Detection","C. V. Zhou; S. Karunasekera; C. Leckie","NICTA Victoria Research Laboratory, Department of Computer Science and Software Engineering, The University of Melbourne, Australia. Email: cvzhou@csse.unimelb.edu.au; NICTA Victoria Research Laboratory, Department of Computer Science and Software Engineering, The University of Melbourne, Australia. Email: shanika@csse.unimelb.edu.au; NICTA Victoria Research Laboratory, Department of Computer Science and Software Engineering, The University of Melbourne, Australia. Email: caleckie@csse.unimelb.edu.au","2007 10th IFIP/IEEE International Symposium on Integrated Network Management","","2007","","","80","89","An important problem in network intrusion detection is how to detect large scale coordinated attacks such as scans, worms and denial-of-service attacks. These coordinated attacks can be difficult to detect at an early stage, since the evidence of the attack may be widely distributed across different subnetworks in the Internet. A critical issue for research is how to detect these large scale attacks by correlating information from multiple intrusion detection systems in an efficient manner. Several collaborative detection systems have been proposed in the literature. However, these proposals have lacked large scale testing in real networks, and the practicalities of how to optimize the trade-off between detection accuracy and reaction time of these systems has not been demonstrated. To address these challenges, we propose LarSID, a scalable decentralized large scale intrusion detection framework. LarSID provides a service for defending against attacks by sharing potential evidence of intrusions between participant intrusion detection systems via a distributed hash table (DHT) architecture. In particular, we investigate how to optimize the trade-off between detection accuracy and reaction time of LarSID based on an analysis of a large, real-world intrusion detection dataset (DShield Dataset), which has been collected from over 1600 firewall administrators across the world. LarSID has been deployed and tested on the PlanetLab testbed, and is built on top of OpenDHT - a public DHT service. Our experimental results show significant reductions in detection latency compared to a centralized detection architecture. Currently, LarSID has been deployed on 128 PlanetLab nodes as a large scale intrusion detection service.","1573-0077","1-4244-0798-21-4244-0799","10.1109/INM.2007.374772","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4258524","","Large-scale systems;Collaboration;Intrusion detection;Computer crime;Collaborative work;Internet;Testing;Computer architecture;Collaborative software;Laboratories","cryptography;groupware;Internet;software architecture","decentralized architecture;collaborative intrusion detection;network intrusion detection;Internet;distributed hash table;OpenDHT","","16","38","","","","","","IEEE","IEEE Conferences"
"Optimizing the length of checking sequences","R. M. Hierons; H. Ural","Sch. of Inf. Syst., Comput. & Math., Brunel Univ., Uxbridge, UK; NA","IEEE Transactions on Computers","","2006","55","5","618","629","A checking sequence, generated from a finite state machine, is a test sequence that is guaranteed to lead to a failure if the system under test is faulty and has no more states than the specification. The problem of generating a checking sequence for a finite state machine M is simplified if M has a distinguishing sequence: an input sequence D~ with the property that the output sequence produced by M in response to D is different for the different states of M. Previous work has shown that, where a distinguishing sequence is known, an efficient checking sequence can be produced from the elements of a set A of sequences that verify the distinguishing sequence used and the elements of a set /spl gamma/ of subsequences that test the individual transitions by following each transition t by the distinguishing sequence that verifies the final state of t. In this previous work, A is a predefined set and /spl gamma/ is defined in terms of A. The checking sequence is produced by connecting the elements of /spl gamma/ and A to form a single sequence, using a predefined acyclic set E/sub c/ of transitions. An optimization algorithm is used in order to produce the shortest such checking sequence that can be generated on the basis of the given A and E/sub c/. However, this previous work did not state how the sets A and E/sub c/ should be chosen. This paper investigates the problem of finding appropriate A and E/sub c/ to be used in checking sequence generation. We show how a set A may be chosen so that it minimizes the sum of the lengths of the sequences to be combined. Further, we show that the optimization step, in the checking sequence generation algorithm, may be adapted so that it generates the optimal E/sub c/. Experiments are used to evaluate the proposed method.","0018-9340;1557-9956;2326-3814","","10.1109/TC.2006.80","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1613841","Finite state machine;checking sequence;test minimization;distinguishing sequence.","Finite state machines;Software requirements and specifications;Minimization methods","finite state machines;formal specification;formal verification;minimisation","finite state machine;test sequence;optimization algorithm;checking sequence generation algorithm;test minimisation","","44","30","","","","","","IEEE","IEEE Journals & Magazines"
"Runtime-optimised intra-4×4 mode-decision for H.264/AVC video encoding","H. Jordan; F. H. Seitner; M. Bleyer; M. Gelautz","Vienna University of Technology, Institute of Automation and Control, Gusshausstrasse 26, A-1040, Austria; Vienna University of Technology, Institute for Software Technology and Interactive Systems, Favoritenstrasse 9-11, A-1040, Austria; Vienna University of Technology, Institute for Software Technology and Interactive Systems, Favoritenstrasse 9-11, A-1040, Austria; Vienna University of Technology, Institute for Software Technology and Interactive Systems, Favoritenstrasse 9-11, A-1040, Austria","2009 Proceedings of 6th International Symposium on Image and Signal Processing and Analysis","","2009","","","227","232","We describe a method that considerably improves the computational behaviour of H.264 intra-only encoders. Such intra-only encoders come to use in video-cutting and low-latency video coding where temporal prediction via using inter-frames is no feasible option. We identify the spatial prediction step as the computational bottleneck in intra-only encoders. In this step, the encoder tests various modes that represent predictions of the current macroblock's or sub-macroblock's texture from spatial neighbouring pixels in order to find the mode of lowest residuum. Unfortunately, testing the complete set of allowed modes is computational expensive. However, as is demonstrated by an analysis provided in this paper, it is reasonable to assume that a large percentage of blocks preserve their prediction modes over time. Based on this assumption we develop two algorithms that improve the computation time in the prediction step. These algorithms differ by their criteria used to decide whether a block's coding mode can be propagated from a temporal preceding frame. Computational speed is enhanced, since we test the full set of modes only for blocks that fail these criteria. Experimental results show that our methods considerably improve the execution time of an intra-only encoder and only show small impact on data-rate and image quality.","1845-5921","978-953-184-135-1978-953-184-134","10.1109/ISPA.2009.5297753","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5297753","","Automatic voltage control;Encoding;Testing;Image quality;Runtime;Automation;Automatic control;Control systems;Interactive systems;Video coding","video coding","runtime-optimised intra-4times4 mode-decision;H.264/AVC video encoding;H.264 intra-only encoder;video cutting;video coding;temporal prediction;block coding;image quality","","","9","","","","","","IEEE","IEEE Conferences"
"A reality check on sip-based streaming applications on the next generation mobile test network","C. Balakrishna; K. Al-Begain","Faculty of Advanced Technology, University of Glamorgan, Mid-Glamorgan, United Kingdom; Faculty of Advanced Technology, University of Glamorgan, Mid-Glamorgan, United Kingdom","2008 3rd International Conference on Communication Systems Software and Middleware and Workshops (COMSWARE '08)","","2008","","","302","308","The telecom and the Internet world is converging towards all-IP network architecture and the operators are keen to provide innovative multimedia services coupled with advanced features such as seamless mobility across heterogeneous access networks to the users. Session Initiation Protocol (SIP) provides the required application-level abstraction to enable advanced mobility and session management for multimedia services across heterogeneous access networks. However, SIP-based handoff procedures suffer from delay and packet losses that hamper the user experience for delay-sensitive multimedia applications. In this paper we have attempted to perform a reality-check on the performance of such SIP-based multimedia applications across the existing heterogeneous access network technologies by measuring delay, mean opinion score (MOS) and throughputs experienced, thus evaluating the user-experience. The paper presents the performance results obtained on a test-setup connected to a live operator network. Hence the results obtained are expected to be of value to the service providers to optimize their server settings to deliver optimum quality of service, to network providers to tune their access networks and to application developers to decide when to perform the handoff from one access network to the other to maintain the quality of experience for the user. The paper also proposes recommendation for minimizing the delays and optimizing the performance of SIP-based streaming applications.","","978-1-4244-1796-4978-1-4244-1797","10.1109/COMSWA.2008.4554430","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4554430","SIP;MOS;Seamless Mobility;Handoff;Test Network","Next generation networking;Testing;Delay;Streaming media;Telecommunications;IP networks;Web and internet services;Access protocols;Performance evaluation;Throughput","Internet;IP networks;media streaming;signalling protocols","SIP-based streaming application;next generation mobile test network;Internet;IP network architecture;multimedia service;session initiation protocol;SIP-based handoff procedure;delay-sensitive multimedia application;mean opinion score","","","14","","","","","","IEEE","IEEE Conferences"
"Joint Optimization of Hardware and Network Costs for Distributed Computer Systems","D. Ardagna; C. Francalanci; M. Trubian","NA; NA; NA","IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans","","2008","38","2","470","484","Multiple combinations of hardware and network components can be selected to design an information technology (IT) infrastructure that satisfies requirements. The professional criterion to deal with these degrees of freedom is cost minimization. However, a scientific approach has been rarely applied to cost minimization, particularly for the joint optimization of hardware and network systems. This paper provides an overall methodology for combining hardware and network designs in a single cost minimization problem for multisite computer systems. Costs are minimized by applying a heuristic optimization approach to a sound decomposition of the problem. We consider most of the design alternatives that are enabled by current hardware and network technologies, including server sizing, localization of mutitier applications, and reuse of legacy systems. The methodology is empirically verified with a database of costs that has also been built as part of this paper. Verifications consider several test cases with different computing and communication requirements. Cost reductions are evaluated by comparing the cost of methodological results with those of architectural solutions that are obtained by applying professional design guidelines. The quality of heuristic optimization results is evaluated through comparison with lower bounds.","1083-4427;1558-2426","","10.1109/TSMCA.2007.914749","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4447651","Client server;costs;distributed computing;networks;Client server;costs;distributed computing;networks","Hardware;Cost function;Computer networks;Distributed computing;Information technology;Minimization methods;Network servers;Application software;Databases;Testing","cost reduction;distributed processing;optimisation","distributed computer systems;hardware-network joint optimization;information technology infrastructure;cost minimization;multisite computer system;heuristic optimization approach","","7","41","","","","","","IEEE","IEEE Journals & Magazines"
"A Fast Cooperative Algorithm for Stereo Matching","C. Yuan; Y. Xu; Q. Yi; C. Qi","NA; NA; NA; NA","2009 International Conference on Computational Intelligence and Software Engineering","","2009","","","1","6","This paper presents a fast stereo algorithm for obtaining disparity maps efficiently. We use a 3D model for storing and computing the depth map. The initial matching by intensity similarity is very fast by using the computational optimization. At the improving matching reliability step, two assumptions that were originally proposed by Marr and Poggio are adopted: uniqueness and continuity. It means there is only one unique depth value for each pixel in the disparity maps and the depths of most pixels are continuous. We presents a novel restrict function using inhibition area without iteration and enhanced by the initial matching result for keeping the sharpness. We present the results comparing with the well-known algorithm based on SMP which confirm that our algorithm is better than SMP on accuracy.","","978-1-4244-4507","10.1109/CISE.2009.5364441","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5364441","","Stereo vision;Iterative algorithms;Software algorithms;Software engineering;Computer science;Paper technology;Microcomputers;Computer errors;Testing;Impedance matching","computer vision;image matching;stereo image processing","cooperative algorithm;stereo matching;disparity maps;3D model;depth map storage;depth map computing;image matching;intensity similarity;computational optimization;matching reliability;restrict function","","","17","","","","","","IEEE","IEEE Conferences"
"Development of a highly optimized Preemptive Real Time Operating System (pRTOS)","M. A. M. Hasan; S. Ahmad","Department of Computer Science & Engineering, Rajshahi University of Engineering & Technology, Bangladesh; Department of Computer Science & Engineering, University of Rajshahi, Rajshahi-6205, Bangladesh","2008 11th International Conference on Computer and Information Technology","","2008","","","52","55","A real-time operating system (RTOS) is software which ensures that time critical events are processed as efficiently as possible. In this paper, an attempt has been taken to develop a real time operating system, named preemptive real time operating system (pRTOS), in which all of the important issues regarding to a real time application have been considered. In this pRTOS, strictly preemptive scheduling algorithm has been used. This scheduling policy makes sure that important tasks are handled first and the less important later. The Bitmap technique has been used to find out the highest priority task from the unsorted ready list. The complexity of this technique for selecting the highest priority task is O(1), which is much faster than the linear search technique having complexity of O(n). This pRTOS can support 64 priority levels ranges from 0 to 63. In addition with this, it is a highly configurable RTOS. Moreover, it can be adopted in a board range of hardware platform, say, Intel x86, MIPS, Hitachi SH, Power PC and Strong ARM processors. This RTOS has been tested on Intel x86 and from the obtained result, it has been found that our developed pRTOS can be used for various application, say, for automated industrial systems, control-systems, high-tech electronics/electrical products and home applications.","","978-1-4244-2135-0978-1-4244-2136","10.1109/ICCITECHN.2008.4803052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4803052","RTOS;Scheduling;Context Switching;Priority Resolution;Task Synchronization;Preemptive Real Time Operating System (pRTOS)","Real time systems;Operating systems;Electronic equipment testing;Electrical products industry;Software systems;Application software;Scheduling algorithm;Job shop scheduling;Hardware;Automatic testing","computational complexity;operating systems (computers);scheduling","preemptive real time operating system;real-time operating system;preemptive scheduling algorithm;linear search technique","","","9","","","","","","IEEE","IEEE Conferences"
"Analysis and Composition of Web Components Based on p-Calculus","T. He; H. Miao; L. Li","NA; NA; NA","2009 International Joint Conference on Computational Sciences and Optimization","","2009","1","","677","679","The quality delivered by existing Web applications is often poor. In this paper, the formal verification techniques for web compositions are presents, and the formal models of web composition are proposed. The definition of well-typedness and congruence of web composition basing on pi-calculus are given. At the same time, this paper analysis both static and dynamic composition, replacement of web components, and proposes methods to recover from the incongruence during the web composition. The significance of the work is twofold: on one hand, it provides a formal basis for better understanding of the composition of web components; on the other hand, it lays the ground work for both model checking and specification-based verifying on the web applications.","","978-0-7695-3605","10.1109/CSO.2009.387","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5193785","","Formal verification;Optical wavelength conversion;Contracts;Connectors;Information analysis;Software quality;Software testing;Information technology;Application software;Time factors","formal specification;formal verification;Internet","Web components composition;Web components analysis;pi-calculus;formal verification techniques;formal models;model checking;specification-based verification","","","7","","","","","","IEEE","IEEE Conferences"
"Optimizing the HW/SW boundary of an ECC SoC design using control hierarchy and distributed storage","Xu Guo; P. Schaumont","Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, 24061, USA; Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, 24061, USA","2009 Design, Automation & Test in Europe Conference & Exhibition","","2009","","","454","459","Hardware/Software codesign of Elliptic Curve Cryptography has been extensively studied in recent years. However, most of these designs have focused on the computational aspect of the ECC hardware, and not on the system integration into a SoC architecture. We study the impact of the communication link between CPU and coprocessor hardware for a typical ECC design, and demonstrate that the SoC may become performance-limited due to coprocessor data- and instruction-transfers. A dual strategy is proposed to remove the bottleneck: introduction of local control as well as local storage in the coprocessor. We quantify the impact of this strategy on a prototype implementation for Field Programmable Gate Arrays (FPGA) and measured an average speed-up in the resulting design of 9.4 times over the baseline ECC system, while the resulting system area increases by a factor of 1.6. The optimal area-time product improvement of our ECC coprocessor is 4.3 times compared to that of the baseline ECC coprocessor. Using design space exploration of a large number of system configurations using the latest FPGA technology and tools, we show that the optimal choice of ECC coprocessor parameters is strongly dependent on the efficiency of system-level communication.","1530-1591;1558-1101","978-1-4244-3781-8978-3-9810-8015","10.1109/DATE.2009.5090708","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5090708","","Design optimization;Distributed control;Coprocessors;Elliptic curve cryptography;Hardware;Field programmable gate arrays;Computer architecture;Communication system control;Prototypes;Area measurement","field programmable gate arrays;hardware-software codesign;public key cryptography;system-on-chip","HW/SW boundary;ECC SoC design;control hierarchy;distributed storage;hardware/software codesign;elliptic curve cryptography;field programmable gate arrays;system-level communication","","","20","","","","","","IEEE","IEEE Conferences"
"An efficient and optimized FPGA Feedback M-PSK Symbol Timing Recovery Architecture based on the Gardner Timing Error Detector","E. Sciagura; P. Zicari; S. Perri; P. Corsonello","DEIS - University of Calabria; DEIS - University of Calabria; DEIS - University of Calabria; DEIS - University of Calabria","10th Euromicro Conference on Digital System Design Architectures, Methods and Tools (DSD 2007)","","2007","","","102","108","This paper presents an efficient and optimized FPGA implementation of a complete digital Symbol Timing Recovery (STR) architecture based on a digital PLL loop structure. Matlab modelling and then a complete hardware communication system test, reveal that the implemented STR circuit offers the best performances compared with the other implemented works present in literature. When implemented on a Xilinx Virtex-2P XC2VP7 FF672 FPGA chip the proposed STR circuit occupies just 138 slices, uses 2 embedded multipliers and reaches a clock frequency of 106 MHz; a symbol rate of 10 Msymbol/sec can be reached when 10 samples per symbol are employed. The obtained results are promising for its use in software defined radio system applications.","","0-7695-2978-X978-0-7695-2978","10.1109/DSD.2007.4341456","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4341456","","Timing;Field programmable gate arrays;Feedback;Circuit testing;Detectors;Phase locked loops;Mathematical model;Hardware;System testing;Performance evaluation","electronic engineering computing;error detection;field programmable gate arrays;phase locked loops;phase shift keying;software radio;synchronisation","optimized FPGA implementation;M-PSK symbol timing recovery architecture;Gardner timing error detector;digital PLL loop structure;Matlab modelling;hardware communication system;Xilinx Virtex-2P XC2VP7 FF672 FPGA chip;software defined radio system applications;field programmable gate arrays;frequency 106 MHz","","2","10","","","","","","IEEE","IEEE Conferences"
"A proposed architecture and ontology for a software system for managing patient’s health record","C. Cenan","Dept. Computer Science, Technical University of Cluj-Napoca, Romania","2008 IEEE International Conference on Automation, Quality and Testing, Robotics","","2008","3","","123","127","The paper presents the architecture of a software system managing the patient's health record and outlines an ontology which is then used as a framework for implementation. The use of this system will help us to realize the full potential of modern, electronic, health care and to optimize this functionality, ontologies are needed. Standards for coded medical terminologies and for a common representation of clinical data will allow patient information to be transmitted clearly and unambiguously between different computers and different software applications in a secure form which is easily searched, interpreted, and manipulated, and thus most useful. We propose architecture to support distributed tasks and data in a health care network. An experimental implementation of the proposed methodology is in work in a small health care network in order to show the usefulness of the proposals as well as their effectiveness in electronic management of patient's health record.","","978-1-4244-2576-1978-1-4244-2577","10.1109/AQTR.2008.4588895","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4588895","","Medical services;Ontologies;Medical diagnostic imaging;Databases;Standards;Software systems;Terminology","distributed processing;health care;medical information systems;ontologies (artificial intelligence)","software system ontology;software system management;clinical data representation;patient information;distributed tasks architecture;health care network;patient health record electronic management","","","18","","","","","","IEEE","IEEE Conferences"
"Why should they believe us? Determinism, non-determinism and evidence","D. Budgen","David Budgen","19th Conference on Software Engineering Education & Training (CSEET'06)","","2006","","","4","4","Summary form only given. In software engineering, as in computing science, the topics that we teach to our students can be considered as falling into two broad categories: the deterministic, and the non-deterministic. Deterministic topics are those where a specific scenario or operation leads to outcomes that can be assessed in terms of true/false values, and so this classification encompasses large elements of computer architecture, databases, metrics and testing. However, much of the software engineering body of knowledge is really concerned with much more non-deterministic processes such as requirements elicitation, design, construction, maintenance etc. These are activities in which humans play a central role, making value judgements that result in outcomes that are more appropriately assessed by using some form of better/worse ranking than through a true/false categorisation. How much we recognise the existence of this distinction in our teaching is a moot point. Many of our students, educated in the classical science paradigm, will be familiar with the type of reasoning that leads to the outcomes for the deterministic elements. In my presentation, I examine some of the reasons why this experience may not be adequate when they encounter the non-deterministic elements of our subject, and hence why we may need to inculcate some degree of understanding of the evidence-based paradigm in order to support both our teaching and also their learning. I will discuss the nature of this paradigm, present some experiences of how it may be adapted for use in Software Engineering, and review some of the questions that it raises.","1093-0175;2377-570X","0-7695-2557","10.1109/CSEET.2006.41","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1617322","","","computer science education;software engineering;teaching","software engineering education;computing science;teaching;evidence-based paradigm;learning","","","","","","","","","IEEE","IEEE Conferences"
"A Survey of Lightweight-Cryptography Implementations","T. Eisenbarth; S. Kumar; C. Paar; A. Poschmann; L. Uhsadel","Ruhr University Bochum; Philips Research Europe; Ruhr University Bochum; Ruhr University Bochum; Catholic University of Leuven","IEEE Design & Test of Computers","","2007","24","6","522","533","The tight cost and implementation constraints of high-volume products, including secure RFID tags and smart cards, require specialized cryptographic implementations. The authors review recent developments in this area for symmetric and asymmetric ciphers, targeting embedded hardware and software. In this article, we present a selection of recently published lightweight-cryptography implementations and compare them to state-of-the-art results in their field.","0740-7475;1558-1918","","10.1109/MDT.2007.178","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4397176","RFID;DESL;Present;lightweight cryptography;embedded security;pervasive computing","Hardware;Elliptic curve cryptography;Energy consumption;Pervasive computing;Moore's Law;Security;Design optimization;Cost function;Clocks;Software testing","cryptography;embedded systems;hardware-software codesign;microprocessor chips;radiofrequency identification;smart cards","lightweight-cryptography;secure RFID tag;smart card;symmetric cipher;asymmetric cipher;embedded hardware;embedded software","","149","15","","","","","","IEEE","IEEE Journals & Magazines"
"PathExpander: Architectural Support for Increasing the Path Coverage of Dynamic Bug Detection","S. Lu; P. Zhou; W. Liu; Y. Zhou; J. Torrellas","University of Illinois at Urbana Champaign, USA; University of Illinois at Urbana Champaign, USA; University of Illinois at Urbana Champaign, USA; University of Illinois at Urbana Champaign, USA; University of Illinois at Urbana Champaign, USA","2006 39th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO'06)","","2006","","","38","52","Dynamic software bug detection tools are commonly used because they leverage run-time information. However, they suffer from a fundamental limitation, the path coverage problem: they detect bugs only in taken paths but not in non-taken paths. In other words, they require bugs to be exposed in the monitored execution. This paper makes one of the first attempts to address this fundamental problem with a simple hardware extension. First, we propose PathExpander, a novel design that dynamically increases the code path coverage of dynamic bug detection tools with no programmer involvement. As a program executes, PathExpander selectively executes non-taken paths in a sandbox without side effects. This enables dynamic bug detection tools to find bugs that are present in these non-taken paths and would otherwise not be detected. Second, we propose a simple hardware extension to control the huge overhead in its pure software implementation to a moderate level. To further minimize overhead, PathExpander provides an optimization option to execute non-taken paths on idle cores in chip multi-processor architectures that support speculative execution. To evaluate PathExpander, we use three dynamic bug detection methods: dynamic software-only checker (CCured), dynamic hardware-assisted checker (iWatcher) and assertions; and conduct side-by-side comparison with PathExpander's counterpart software implementation. Our experiments with seven buggy programs using general inputs that do not expose the tested bugs show that PathExpander is able to help these tools detect 21 (out of 38) tested bugs that are otherwise missed. This is because PathExpander increases the code coverage of each test case from 40% to 65% on average, based on the branch coverage metric. When applications are tested with multiple inputs, the cumulative coverage also significantly improves by 19%. We also show that PathExpander introduces modest false positives (4 on average) and overhead (less than 9.9%). The 3-4 orders of magnitude lower overhead compared with pure-software implementation further justifies the hardware design in PathExpander","1072-4451;2379-3155","0-7695-2732","10.1109/MICRO.2006.40","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4041834","","Computer bugs;Monitoring;Hardware;Benchmark testing;Runtime;Software testing;Computer science;Software tools;Programming profession;Computer architecture","computer architecture;program debugging","PathExpander evaluation;dynamic software bug detection;path coverage problem;multiprocessor architecture;speculative execution;dynamic software-only checker;dynamic hardware-assisted checker;software implementation;buggy program","","10","41","","","","","","IEEE","IEEE Conferences"
"Multi-level ant system - a new approach through the new pheromone update for ant colony optimization","Dinh Quang Huy; Do Duc Dong; Hoang Xuan Huan","NA; NA; NA","2006 International Conference onResearch, Innovation and Vision for the Future","","2006","","","55","58","","","1-4244-0316","10.1109/RIVF.2006.1696418","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1696418","","Ant colony optimization;Traveling salesman problems;NP-hard problem;Learning;Application software;Testing;Chemicals;Education;Information technology;Libraries","","","","3","10","","","","","","IEEE","IEEE Conferences"
"Engineering RFID systems through Electromagnetic Modeling","S. Gakhar; J. Feldkamp; M. Perkins; R. Sun; C. J. Reddy","Kimberly-Clark Corp.; Kimberly-Clark Corp.; Kimberly-Clark Corp.; EM Software and Systems; EM Software and Systems","2008 IEEE International Conference on RFID","","2008","","","344","349","RFID technology provides a more granular visibility for industrial assets and inventory thereby offering a strategic advantage to the business. This paper introduces a technique for deploying RFID systems and engineering solutions based on fundamental electromagnetic understanding of the deployed scenarios. A virtual database comprising of RFID tags, antennae and experimental setup was first created and verified. Experiments were then conducted to identify trends both in lab-based and virtual environments. These trends were then validated against each other to show feasibility of using electromagnetic models to simulate and engineer RFID systems.","2374-0221","978-1-4244-1711-7978-1-4244-1712","10.1109/RFID.2008.4519346","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4519346","Radio frequency identification (RFID);Electromagnetic Modeling FEKO;System optimization;antenna or tag placement","Systems engineering and theory;Radiofrequency identification;Electromagnetic modeling;RFID tags;System testing;Power system modeling;Plastic products;Indium tin oxide;Sun;Software systems","radiofrequency identification","RFID system engineering;electromagnetic modeling;industrial assets;inventory;business;virtual database;RFID tags;antennae;virtual environments","","4","7","","","","","","IEEE","IEEE Conferences"
"A Multi-subpopulation Accelerating Genetic Algorithm Based on Attractors (MAGA): Performance in Function Optimization","Z. Lin; Y. Li","Wuhan University, China; Wuhan University, China","Third International Conference on Natural Computation (ICNC 2007)","","2007","5","","560","564","A multi-subpopulation accelerating genetic algorithm based on attractors(MAGA) is proposed to cope with the drawback of genetic algorithms. MAGA views the excellent individuals as attractors and generates local small populations in the neighbor of them to maintain the diversity of the population. In the course of searching, MAGA constantly shrinks the searching neighbor and uses the accelerating operators to speed up the evolution of MAGA. The convergence analysis shows MAGA can converge to global optimization under some circumstances. Finally, MAGA's efficiency is validated through optimization of two benchmark functions.","2157-9555;2157-9563","0-7695-2875-9978-0-7695-2875","10.1109/ICNC.2007.73","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4344902","","Acceleration;Genetic algorithms;Convergence;Software engineering;Magnetooptic recording;Temperature;Entropy;Simulated annealing;Testing;Distributed computing","functional analysis;genetic algorithms;mathematical operators;search problems","multisubpopulation accelerating genetic algorithm;accelerating operators;function optimization;convergence analysis;MAGA searching","","","10","","","","","","IEEE","IEEE Conferences"
"Performance detection of an embedded system using Boosting Algorithm","Wenting Li; Yan Lin","School of Automation and Electrical Engineering, Beijing University of Aeronautics & Astronautics, China; School of Automation and Electrical Engineering, Beijing University of Aeronautics & Astronautics, China","2009 4th IEEE Conference on Industrial Electronics and Applications","","2009","","","1286","1290","The boosting algorithm which we introduce is a representational ensemble classification methodology. It is well known that the boosting algorithm can improve the accuracy of any given learning algorithm and train the strong classifiers efficiently. A specific embedded hardware is designed for target identification task. This paper also evaluates the performance and implementation issues for the boosting classification on the embedded hardware. Considering the limited source and the characteristics of the embedded system, this paper proposed an optimal memory allocation method for system optimization which is combining the general software optimization methods. And the method we proposed can also be used for the other embedded system which is support the cache configuration. Some testing samples show the effectiveness of the proposed technique.","2156-2318;2158-2297","978-1-4244-2799-4978-1-4244-2800","10.1109/ICIEA.2009.5138409","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5138409","Boosting;optimization;DSP;cache","Embedded system;Boosting;Signal processing algorithms;Hardware;Optimization methods;Embedded software;Application software;Machine learning algorithms;Digital signal processing;Face detection","cache storage;embedded systems;learning (artificial intelligence);pattern classification;storage allocation","performance detection;embedded system;boosting algorithm;representational ensemble classification methodology;learning algorithm;target identification;optimal memory allocation method;system optimization;software optimization;cache configuration","","","10","","","","","","IEEE","IEEE Conferences"
"Autoreclosure in Extra High Voltage Lines Using Taguchi's Method and Optimized Neural Networks","D. Z. F.; K. S. R. Rao","NA; NA","2009 International Conference on Computer Engineering and Technology","","2009","2","","151","155","This paper presents a method to discriminate a temporary fault from a permanent one in an extra high voltage (EHV) transmission line so that improper reclosing of the line onto a fault is avoided. The fault identification prior to reclosing is based on optimized artificial neural network associated with standard Error Back-Propagation, Levenberg Marquardt Algorithm and Resilient Back-Propagation training algorithms together with Taguchipsilas Method. The algorithms are developed using MATLAB software. A range of faults are simulated on EHV modeled transmission line using SimPowerSytems, and the spectra of the fault data are analyzed using fast Fourier transform to extract features of each type of fault. For both training and testing purposes, the neural network is fed with the normalized energies of the DC component, the fundamental and the first four harmonics of the faulted voltages. The developed algorithm is effectively trained, verified and validated with a set of training, dedicated testing and validation data respectively.","","978-0-7695-3521-0978-1-4244-3334","10.1109/ICCET.2009.171","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4769577","Autoreclosure;EHV transmission line faults;artificial neural networks;Levenberg Marquardt algorithm;back-propagation algorithm;RPROP;Taguchi's method","Voltage;Optimization methods;Neural networks;Artificial neural networks;Transmission lines;Testing;Fault diagnosis;Software algorithms;MATLAB;Analytical models","backpropagation;fast Fourier transforms;feature extraction;mathematics computing;neural nets;power engineering computing;power transmission faults;power transmission lines","temporary fault;permanent fault;extra high voltage transmission line;autoreclosure;fault identification;optimized artificial neural network;error backpropagation;resilient backpropagation training algorithms;Levenberg Marquardt algorithm;Taguchi method;MATLAB software;SimPowerSytems;fast Fourier transform;feature extraction;training;harmonics","","","16","","","","","","IEEE","IEEE Conferences"
"Direction matters in high-dimensional optimisation","C. MacNish; Xin Yao","School of Computer Science & Software Engineering, University of Western Australia, Nedlands 6009, Australia; School of Computer Science, University of Birmingham, B15 2TT, U.K.","2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational Intelligence)","","2008","","","2372","2379","Directional biases are evident in many benchmarking problems for real-valued global optimisation, as well as many of the evolutionary and allied algorithms that have been proposed for solving them. It has been shown that directional biases make some kinds of problems easier to solve for similarly biased algorithms, which can give a misleading view of algorithm performance. In this paper we study the effects of directional bias for high- dimensional optimisation problems. We show that the impact of directional bias is magnified as dimension increases, and can in some cases lead to differences in performance of many orders of magnitude. We present a new version of the classical evolutionary programming algorithm, which we call unbiased evolutionary programming (UEP), and show that it has markedly improved performance for high-dimensional optimisation.","1089-778X;1941-0026","978-1-4244-1822-0978-1-4244-1823","10.1109/CEC.2008.4631115","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4631115","","Algorithm design and analysis;Optimization;Programming;Benchmark testing;Evolutionary computation;Next generation networking;Equations","evolutionary computation","high-dimensional optimisation;real-valued global optimisation;benchmarking problems;directional biases;classical evolutionary programming algorithm;unbiased evolutionary programming","","7","11","","","","","","IEEE","IEEE Conferences"
"Static Security Optimization for Real-Time Systems","M. Lin; L. Xu; L. T. Yang; X. Qin; N. Zheng; Z. Wu; M. Qiu","NA; NA; NA; NA; NA; NA; NA","IEEE Transactions on Industrial Informatics","","2009","5","1","22","37","An increasing number of real-time applications like railway signaling control systems and medical electronics systems require high quality of security to assure confidentiality and integrity of information. Therefore, it is desirable and essential to fulfill security requirements in security-critical real-time systems. This paper addresses the issue of optimizing quality of security in real-time systems. To meet the needs of a wide variety of security requirements imposed by real-time systems, a group-based security service model is used in which the security services are partitioned into several groups depending on security types. While services within the same security group provide the identical type of security service, the services in the group can achieve different quality of security. Security services from a number of groups can be combined to deliver better quality of security. In this study, we seamlessly integrate the group-based security model with a traditional real-time scheduling algorithm, namely earliest deadline first (EDF). Moreover, we design and develop a security-aware EDF schedulability test. Given a set of real-time tasks with chosen security services, our scheduling scheme aims at optimizing the combined security value of the selected services while guaranteeing the schedulability of the real-time tasks. We study two approaches to solve the security-aware optimization problem. Experimental results show that the combined security values are substantially higher than those achieved by alternatives for real-time tasks without violating real-time constraints.","1551-3203;1941-0050","","10.1109/TII.2009.2014055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4799217","Embedded real-time systems;optimization;security","Real time systems;Radar tracking;Information security;Control systems;Computer science;Rail transportation;Application software;Medical control systems;Scheduling algorithm;Testing","data integrity;embedded systems;scheduling;security of data","static security optimization;real-time systems;group-based security service model;real-time scheduling algorithm;earliest deadline first;security-aware optimization problem","","47","36","","","","","","IEEE","IEEE Journals & Magazines"
"Recorders, Reasoners and Artificial Intelligence - Integrated Diagnostics on Military Transport Aircraft","B. Chidambaram; P. Pigg; P. L. Horn; M. A. Talbot; D. D. Gilbertson; K. C. Cerise","5301 Bolsa Ave., MC H013-B319, Huntington Beach, CA 92647, 714-896-7017, balaguruna.chidambaram@boeing.com; NA; NA; NA; NA; NA","2006 IEEE Aerospace Conference","","2006","","","1","9","A research group at Boeing has developed a prototype for an integrated diagnostic system to optimize maintenance on military transport aircraft by decreasing maintenance costs and increasing aircraft availability. The integrated diagnostic system comprises an on-board recorder, and a ground-based reasoner that analyzes the recorded data, to optimize maintenance. The functions of the ground-based reasoner (GBR) include identification of root-cause, filtering of false alarms, and prioritization of maintenance actions. The technologies used include expert systems/fast state recognition methods, data mining technologies and Bayesian analyses. The ground-based reasoner provides an open plug-n-play software framework for incorporating these technologies into a software tool, for field maintenance. The tool has a simple, intuitive graphic user interface that is designed to help the end-user, the maintenance technician, with everyday maintenance tasks. The integrated diagnostic system prototype is currently undergoing testing on pre-delivery test flights for the C-17 military transport, at a Boeing facility, and initial results in applying the system to the aerial delivery subsystem and the hydraulic subsystem are discussed","1095-323X","0-7803-9545","10.1109/AERO.2006.1656074","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1656074","","Military aircraft;Artificial intelligence;Prototypes;Software tools;System testing;Cost function;Data analysis;Filtering;Diagnostic expert systems;Data mining","aerial equipment;artificial intelligence;Bayes methods;data mining;diagnostic reasoning;hydraulic systems;military aircraft;military computing;recorders","artificial intelligence;military transport aircraft;integrated diagnostic system;maintenance costs;aircraft availability;on-board recorder;ground-based reasoner;root-cause identification;false alarm filtering;maintenance action prioritization;expert systems recognition methods;fast state recognition methods;data mining technologies;Bayesian analyses;open plug-n-play software framework;software tool;field maintenance;graphic user interface;maintenance technician;maintenance tasks;pre-delivery test flights;C-17 military transport;aerial delivery subsystem;hydraulic subsystem","","1","4","","","","","","IEEE","IEEE Conferences"
"An Optimized RFID-Based Academic Library","A. Fennani; H. Hamam","NA; NA","2008 Second International Conference on Sensor Technologies and Applications (sensorcomm 2008)","","2008","","","44","48","We present a solution for integrating RFID technology in academic libraries by offering cost effectiveness and good quality of service. On the one hand, we challenge hardware issues which include optimization of the layout of the RFID system by minimizing the total number of used devices, avoiding collision between them and capturing all tags. On other hand, the main challenge in designing powerful RFID middleware is the use of a modular and generic programming method that enables reusing some software parts in various applications, offering flexibility, to a large extent, in introducing new equipments like readers and sensors and allowing for the use of devices from different suppliers. We succeeded in developing a user-friendly application that enables full automatic management of the library as well as a test bed for real time optimizing the layout of our RFID system.","","978-0-7695-3330","10.1109/SENSORCOMM.2008.63","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4622638","RFID smart Library optimization","Radiofrequency identification;Libraries;Sensors;Software;Books;Antennas;Mobile communication","academic libraries;human computer interaction;library automation;middleware;radiofrequency identification","RFID technology;academic library;quality of service;collision avoidance;RFID middleware;generic programming method","","3","16","","","","","","IEEE","IEEE Conferences"
"Using Parallel Processing Tools to Predict Rotorcraft Performance, Stability, and Control","D. Carico; C. He","Naval Air Warfare Center Aircraft Division, Rotary Wing Ship Suitability, (301) 342-1382, dean.carico@navy.mil; Advanced Rotorcraft Technology (ART), Inc., Director of Research, (650)968-1464, he@flightlab.com","2007 IEEE Aerospace Conference","","2007","","","1","11","This paper discusses the development of the High Performance Computing (HPC) Collaborative Simulation and Test (CST) portfolio CST-03 program, one of the projects in the Common HPC Software Support Initiative (CHSSI) portfolio. The objective of this development was to provide computationally scalable tools to predict rotorcraft performance, stability, and control. The ability to efficiently predict and optimize vehicle performance, stability, and control from high fidelity computer models would greatly enhance the design and testing process and improve the quality of systems acquisition. Through this CHSSI development, the US Navy Test Pilot School performance, stability, and control test procedures were fully implemented in a high performance parallel computing environment. These Navy flight test support options were parallelized, implemented, and validated in the FLIGHTLAB comprehensive, multidisciplinary modeling environment. These tools were designed to interface with other CST compatible models and a standalone version of the tools (FLIGHTLAB-ASPECT) was delivered for use independent of the FLIGHTLAB development system. Tests on the MAUI Linux cluster indicated that there was over 25 times speedup using 32 CPUs. The tests also met the accuracy criteria as defined for the Beta trial.","1095-323X","1-4244-0524-61-4244-0525","10.1109/AERO.2007.352736","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4161583","","Parallel processing;Stability;High performance computing;Collaborative software;Portfolios;Military computing;Computational modeling;Software testing;Software performance;Design optimization","aerospace computing;aerospace simulation;helicopters","parallel processing tools;rotorcraft performance;high performance computing;collaborative simulation;CST-03 program;common HPC software support initiative;computationally scalable tools;US Navy Test Pilot School;FLIGHTLAB;FLIGHTLAB-ASPECT;MAUI Linux cluster","","1","10","","","","","","IEEE","IEEE Conferences"
"A Methodology for Rapid Optimization of HandelC Specifications","J. C. Libby; K. B. Kent","NA; NA","2009 IEEE/IFIP International Symposium on Rapid System Prototyping","","2009","","","81","87","Utilizing high level hardware description languages for the creation of customized circuits facilitates the rapid development and deployment of new hardware. While hardware design languages increase the speed at which hardware can be developed, creating hardware designs that are both efficient in resource usage and processing speed can be time consuming and require much experience. This problem is compounded more by the long design cycle times that are introduced by the long compilation and synthesis times that are required to translate a high level hardware description language to a circuit. This problem is addressed by performing some of the optimizations automatically, pre-synthesis, reducing the total number of synthesis cycles that are required, saving much development time.","2150-5500;2150-5519","978-0-7695-3690","10.1109/RSP.2009.31","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5158503","HandelC;Hardware Design Flow;Field Programmable Gate Array;Rapid Prototyping","Optimization methods;Hardware design languages;Prototypes;Computer science;Circuit synthesis;Parallel processing;Design optimization;Testing;Communication system control;Concurrent computing","formal specification;hardware description languages;software prototyping","rapid optimization;HandelC specifications;high level hardware description languages;design cycle times;synthesis cycles","","","16","","","","","","IEEE","IEEE Conferences"
"Research on pitch-controlled wind turbine system based on bladed","X. Yang; B. Duan; Z. Jing; G. Chen","College of Information Engineering, Xiangtan University, Xiangtan, 411105, Hunan Province, China; College of Information Engineering, Xiangtan University, Xiangtan, 411105, Hunan Province, China; College of Information Engineering, Xiangtan University, Xiangtan, 411105, Hunan Province, China; College of Information Engineering, Xiangtan University, Xiangtan, 411105, Hunan Province, China","2009 International Conference on Sustainable Power Generation and Supply","","2009","","","1","6","With the increasing share of the wind power in the power system, the impact of its integration is becoming more widespread. Based on the wind farm output forecasting and local electric network active power optimization results, the wind turbines electric control parameters are calculated as inputs to the pitch controller. A pitch control power regulation method based on application integration is proposed. The purpose of this controller is to make sure the wind farm output power maximal. The pitch-controlled actuator was designed and combined with BLADED software to set up the simulation test-bed. Making use of external controller of GH bladed wind turbine design software, the pitch controller is leaded in GH bladed in order to verify the performance of the controller. The simulation result has confirmed that the control strategy is feasible.","2156-9681;2156-969X","978-1-4244-4934","10.1109/SUPERGEN.2009.5348105","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5348105","BLADED;Pitch control;Power regulation;Variable speed wind turbine;wind farm","Wind turbines;Wind farms;Wind energy;Power systems;Wind forecasting;Application software;Power generation;Actuators;Software testing;Software design","actuators;power engineering computing;power generation control;wind power plants;wind turbines","wind power system;wind farm output forecasting;electric network active power optimization;wind turbines electric control parameter;pitch control power regulation method;pitch-controlled actuator;BLADED software","","1","7","","","","","","IEEE","IEEE Conferences"
"Compile-time decided instruction cache locking using worst-case execution paths","H. Falk; S. Plazar; H. Theiling","Computer Science 12, University of Dortmund, D-44221 Dortmund; Computer Science 12, University of Dortmund, D-44221 Dortmund; AbsInt Angewandte Informatik, Science Park 1, D-66123 Saarbru-cken","2007 5th IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS)","","2007","","","143","148","Caches are notorious for their unpredictability. It is difficult or even impossible to predict if a memory access results in a definite cache hit or miss. This unpredictability is highly undesired for real-time systems. The Worst-Case Execution Time (WCET) of a software running on an embedded processor is one of the most important metrics during real-time system design. The WCET depends to a large extent on the total amount of time spent for memory accesses. In the presence of caches, WCET analysis must always assume a memory access to be a cache miss if it can not be guaranteed that it is a hit. Hence, WCETs for cached systems are imprecise due to the overestimation caused by the caches. Modern caches can be controlled by software. The software can load parts of its code or of its data into the cache and lock the cache afterwards. Cache locking prevents the cache's contents from being flushed by deactivating the replacement. A locked cache is highly predictable and leads to very precise WCET estimates, because the uncertainty caused by the replacement strategy is eliminated completely. This paper presents techniques exploring the lockdown of instruction caches at compile-time to minimize WCETs. In contrast to the current state of the art in the area of cache locking, our techniques explicitly take the worst-case execution path into account during each step of the optimization procedure. This way, we can make sure that always those parts of the code are locked in the I-cache that lead to the highest WCET reduction. The results demonstrate that WCET reductions from 54% up to 73% can be achieved with an acceptable amount of CPU seconds required for the optimization and WCET analyses themselves.","","978-1-5959-3824-4978-1-5959-3824-4978-1-5959-3824","10.1145/1289816.1289853","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5753829","","Benchmark testing;Optimization;Algorithm design and analysis;Context;Software;Transform coding;Real time systems","cache storage;embedded systems;instruction sets","memory access;compile time decided instruction;cache locking;worst-case execution path;embedded processor;real-time system;WCET;optimization;software controlled instruction","","17","10","","","","","","IEEE","IEEE Conferences"
"Functional test selection based on unsupervised support vector analysis","O. Guzey; L. Wang; J. Levitt; H. Foster","University of CA - Santa Barbara, USA; University of CA - Santa Barbara, USA; Mentor Graphics Corporation, USA; Mentor Graphics Corporation, USA","2008 45th ACM/IEEE Design Automation Conference","","2008","","","262","267","Extensive software-based simulation continues to be the mainstream methodology for functional verification of designs. To optimize the use of limited simulation resources, coverage metrics are essential to guide the development of effective test suites. Traditional coverage metrics are defined based on either a functional model or a structural model of the design. If our goal is to select a subset of tests from a set of tests, using these coverage metrics require simulation of the entire set before the effectiveness of tests can be compared. In this paper, we propose a novel methodology that estimates the input space covered by a set of tests. We use unsupervised support vector analysis to learn such a space, resulting in a subset of tests that represent the original set of tests. A direct application of this methodology is to select tests before simulation in order to reduce simulation cycles. Consequently, simulation effectiveness can be improved. Experimental results based on application of the proposed methodology to the OpenSparc Tl processor are reported to demonstrate the practicality of our approach.","0738-100X","978-1-60558-115","10.1145/1391469.1391536","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4555820","Functional verification;Support Vector;Learning","Algorithm design and analysis;Permission;Graphics;Functional analysis;Hardware;Logic design;Logic testing;Writing;Random number generation;Time to market","circuit simulation;integrated circuit testing;logic CAD;support vector machines;unsupervised learning","functional test selection;unsupervised support vector analysis;software-based simulation;functional design verification;OpenSparc Tl processor;logic design","","13","12","","","","","","IEEE","IEEE Conferences"
"Thermal optimization of 3D microcontacts using DOE and CFD analysis","N. Kafadarova; A. Andonova; S. Andreev; R. Arnaudov; S. Tzanova","Technical College/Technical University, Sofia ¿ Plovdiv branch, 25 Tsanko Diustabanov St, 4000, Bulgaria; Department of Microelectronics/Technical University Sofia, 8 Kliment Ohridski St., 1000, Bulgaria; Department of Microelectronics/Technical University Sofia, 8 Kliment Ohridski St., 1000, Bulgaria; Department of Microelectronics/Technical University Sofia, 8 Kliment Ohridski St., 1000, Bulgaria; Department of Microelectronics/Technical University Sofia, 8 Kliment Ohridski St., 1000, Bulgaria","2008 2nd Electronics System-Integration Technology Conference","","2008","","","535","540","The present article describes an approach for optimization of thermal performance of 3D micro-components from pin-ring type implemented in IC package to PCB assembly and micro mechanical actuators by common use of DOE and CFD simulations. The goal of the considered approach is to define a real concurrent process for design of reliable microelectronic systems for specific applications. A model, containing all of the requisite design factors such as sizes, material and form was created using the commercially available CFD software, FLOTHERM. The results of simulation of test structures are verified by thermovision measurements by infrared camera P640 of FLIR. Advantages and disadvantages of different studied constructions of micro-contacts are analyzed in respect of better parameters of the process of heat transfer.","","978-1-4244-2813-7978-1-4244-2814","10.1109/ESTC.2008.4684406","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4684406","Computational Fluid Dynamics;Design of Experiments;Micro-contacts;Infrared Thermography;Design Optimization","US Department of Energy;Computational fluid dynamics;Integrated circuit packaging;Assembly;Actuators;Process design;Microelectronics;Application software;Testing;Cameras","computational fluid dynamics;electronic engineering computing;heat transfer;integrated circuit packaging;microactuators;printed circuits","thermal optimization;3D microcontacts;3D micro-components;pin-ring type;CFD analysis;IC package;PCB assembly;micro mechanical actuators;microelectronic systems;CFD software;FLOTHERM;thermovision measurements;infrared camera;heat transfer","","","6","","","","","","IEEE","IEEE Conferences"
"Research on Mechanism Optimization of ZRP Cache Information Processing in Mobile Ad Hoc Network","H. Li; F. Qiu; Y. Liu","NA; NA; NA","2007 International Conference on Wireless Communications, Networking and Mobile Computing","","2007","","","1593","1596","In the research area of mobile ad hoc network, ZRP which integrates advantages of proactive routing protocols and on-demand routing protocols has been the crucial research now. In this paper, the principle and the cache mechanism of ZRP are analyzed. In allusion to the problems of late-deletion and early-deletion in ZRP, the optimized A-ZRP which associates the routing information priority with the latest access time is proposed. It improves information store format of cache in the routing table. Finally, the simulation experiment shows that A-ZRP shows advantages to ZRP in terms of the radius of routing zone, routing overhead, delivery fraction and packet delay based on the same transmission load and node movement. In A-ZRP, the processing of path discovery is reduced, the routing overhead is decrease and the performance of the delivery fraction and the packet delay are improved. The routing protocol performance of mobile ad hoc ZRP is improved.","2161-9646;2161-9654","1-4244-1311-71-4244-1312-5978-1-4244-1311-9978-1-4244-1312","10.1109/WICOM.2007.401","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4340176","","Information processing;Mobile ad hoc networks;Routing protocols;Delay;Educational technology;Educational institutions;Software testing;Wireless networks;Spread spectrum communication;Transmitters","ad hoc networks;cache storage;mobile radio;routing protocols","mobile ad hoc network;MANET;cache mechanism optimization;ZRP cache information processing;proactive routing protocol;on-demand routing protocol;routing table","","5","7","","","","","","IEEE","IEEE Conferences"
"Observability-based coverage-directed path search using PBO for automatic test vector generation","J. C. Costa; J. C. Monteiro","TU Lisbon, IST / INESC-ID, 1000-029 Lisboa, Portugal; TU Lisbon, IST / INESC-ID, 1000-029 Lisboa, Portugal","2009 17th IFIP International Conference on Very Large Scale Integration (VLSI-SoC)","","2009","","","153","158","In this paper, we address the problem of finding a minimal set of execution paths that achieve a user-specified level of observability coverage. Under this metric, a program statement is only considered covered if its execution has influence on some output. We use Pseudo-Boolean Optimization (PBO) to model the problem of finding the paths that are most likely to increase code coverage. Generated paths are then validated to check for feasibility. This methodology was implemented into a fully functional tool that is capable of handling real programs specified in the C language.","2324-8432;2324-8440","978-1-4577-0236-5978-1-4577-0237-2978-1-4577-0235","10.1109/VLSISOC.2009.6041346","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6041346","","Observability;Hardware;Optimization;Embedded software;Generators;Measurement","C language;program testing","coverage-directed path search;automatic test vector generation;observability coverage;program statement;pseudo-Boolean optimization;code coverage;fully functional tool;C language","","1","14","","","","","","IEEE","IEEE Conferences"
"The best bidding price based on Ant Colony Algorithm in electric power market","S. Li; L. Gao; G. Xu","Department of Electrical and Electronics Engineering, North China Electric Power University, in Beijing, China; Department of Electrical and Electronics Engineering, North China Electric Power University, in Beijing, China; Institute of Software, Chinese Academy of Science","2009 International Conference on Sustainable Power Generation and Supply","","2009","","","1","6","In electric power market, the research of best bidding price is an important study, in which the calculation and description of node price is the key issue. Based on the highly effective ant colony algorithm, this paper proposes a methodology of best bidding for solving the optimal price model of the electric power market. We use the random global search capability of ant colony algorithm to deal with the constraints of power system operation and solve the optimization problem between power producers bidding and consumers demand, to estimate the wholesale or retail price in the electricity market. Finally, we use the IEEE 30 - bus test system as a pilot system to test the proposed methodology, and the test results and analysis show that the computational model has a certain degree of practical significance in our electric power market.","2156-9681;2156-969X","978-1-4244-4934","10.1109/SUPERGEN.2009.5348327","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5348327","Ant Colony Algorithm;Bidding price;Nodal price;Electric power market","Ant colony optimization;Power system modeling;System testing;Power systems;Consumer electronics;Power engineering and energy;Electricity supply industry;Constraint optimization;Cost function;Guidelines","optimisation;power markets;search problems","best bidding price;ant colony algorithm;electric power market;node price;optimal price model;random global search capability;power system operation constraint;IEEE 30-bus test system;pilot system","","1","20","","","","","","IEEE","IEEE Conferences"
"Compartmental Modeling in Positron Emission Tomography: A model selection approach","J. E. M. Moreira da Silva; S. S. Furuie","Escola Politécnica da Universidade de Sío Paulo - EPUSP, Brazil; Escola Politécnica da Universidade de Sío Paulo - EPUSP, Brazil","2009 Pan American Health Care Exchanges","","2009","","","29","34","Compartmental Modeling is used in Positron Emission Tomography (PET) and it is an important tool for analysis and kinetic studies of living systems. Its application allows doctors and radiologists to provide diagnosis and treatment for several diseases (e.g. heart ischemia) by image processing, representing a non invasive way to quantify biochemical and physiological processes. Because of a large number of compartment models available, the task to choose the most suitable in a statistical sense may be difficult sometimes. The current work presents an assessment method for compartmental models for cardiology studies using Information Criterion approach for simulated experiments, being helpful to start a model development. The methodology consists of statistical assessment of one, two and three compartments models using features obtained from experimental data. It enables to analyze and make a decision about the most suitable number of compartments to be applied in a particular clinical exam or study. Synthetic curves were created to test estimation task and model choice was made using Akaike's Information Criterion. Fitting curve procedure employs Levenberg- Marquardt and Nelder-Mead optimizations techniques with sensitivities equations approach. Signal-noise ratio of tracer concentrations curves were estimated from experimental data (5 patients from Heart Institute of Medicine School of University of Sao Paulo, Brazil) and considered to be Gaussian in all simulated cases. Conclusion: Identification process was tested successfully for simulated data. For one and two compartments structures, 60 measures were enough to distinguish what model was employed to synthesize the respective data thanks to Akaike's Information Criterion. However, for three compartments simulated data, 200 points were necessary. This result shows that one needs more measures for complex models identification. Sampling carefully is a must when using three compartment models for 60 minutes scans, for instance. The next step consists of identification of real exams using commercial software as gold standard.","2327-8161;2327-817X","978-1-4244-3668-2978-1-4244-3669","10.1109/PAHCE.2009.5158359","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5158359","Positron Emission Tomography;PET;compartmental modeling;nonlinear estimation;sensitivities equations;system identification","Positron emission tomography;Heart;Testing;Medical simulation;Kinetic theory;Cardiac disease;Cardiovascular diseases;Ischemic pain;Image processing;Cardiology","cardiology;optimisation;positron emission tomography","positron emission tomography;compartmental modeling;living systems;cardiology;Akaike information criterion approach;Levenberg-Marquardt optimization;Nelder-Mead optimization;signal-noise ratio;commercial software;gold standard","","","7","","","","","","IEEE","IEEE Conferences"
"A Performance Benchmark of a Multimedia Service Delivery Framework","R. Herpertz; J. M. Espinosa Carlín","NA; NA","2009 Mexican International Conference on Computer Science","","2009","","","137","141","The IP Multimedia Subsystem is a control overlay network that enables the delivery of multimedia services to end subscribers. One of the core steps towards the optimization of an IMS deployment consists of verifying that the involved resources deliver the desired performance when the system is operating under stress conditions. With this goal in mind, this paper presents an IMS performance evaluation based on the IMS/NGN Performance Benchmark specification from ETSI, using well-known open source software reference implementations and state-of-the-art analyzing tools for collecting the related measurements.","","978-1-4244-5258","10.1109/ENC.2009.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5452543","","Benchmark testing;System testing;Telecommunication standards;Multimedia systems;Next generation networking;Performance analysis;Stress;Open source software;Software measurement;Load modeling","IP networks;multimedia communication;optimisation;performance evaluation","performance benchmark;multimedia service delivery framework;IP multimedia subsystem;control overlay network;IMS deployment optimization;IMS performance evaluation;ETSI;open source software reference;state-of-the-art analyzing tools","","4","6","","","","","","IEEE","IEEE Conferences"
"Customizing deep brain stimulation to the patient using computational models","C. C. McIntyre; A. M. Frankenmolle; J. Wu; A. M. Noecker; J. L. Alberts","Cleveland Clinic Foundation, Cleveland, OH 44195 USA; Cleveland Clinic Foundation, Cleveland, OH 44195 USA; Cleveland Clinic Foundation, Cleveland, OH 44195 USA; Cleveland Clinic Foundation, Cleveland, OH 44195 USA; Cleveland Clinic Foundation, Cleveland, OH 44195 USA","2009 Annual International Conference of the IEEE Engineering in Medicine and Biology Society","","2009","","","4228","4229","Bilateral subthalamic (STN) deep brain stimulation (DBS) is effective in improving the cardinal motor signs of advanced Parkinson's disease (PD); however declines in cognitive function have been associated with this procedure. The aim of this study was to assess cognitive-motor performance of 10 PD patients implanted with STN DBS systems during either clinically determined stimulation settings or settings derived from a computational model. Cicerone DBS software was used to define the model parameters such that current spread to non-motor areas of the STN was minimized. Clinically determined and model defined parameters were equally effective in improving motor scores on the traditional clinical rating scale (UPDRS-III). Under modest dual-task conditions, cognitive-motor performance was worse with clinically determined compared to model derived parameters. In addition, the model parameters provided a 66% reduction in power consumption. These results indicate that the cognitive-motor declines associated with bilateral STN can be mitigated, without compromising motor benefits, utilizing stimulation parameters that minimize current spread into non-motor regions of the STN.","1094-687X;1558-4615","978-1-4244-3296","10.1109/IEMBS.2009.5334592","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5334592","","Brain stimulation;Brain modeling;Computational modeling;Satellite broadcasting;Parkinson's disease;Testing;USA Councils;Power system modeling;Energy consumption;Optimization methods","brain;diseases;medical diagnostic computing;neurophysiology;power consumption","computational models;bilateral subthalamic deep brain stimulation;cognitive-motor performance;Parkinson disease patient;cicerone DBS software;nonmotor areas;traditional clinical rating scale;power consumption","Algorithms;Cognition;Computer Simulation;Computers;Deep Brain Stimulation;Electrodes;Electrophysiology;Equipment Design;Humans;Motor Neurons;Motor Skills;Parkinson Disease;Signal Processing, Computer-Assisted;Software","7","4","","","","","","IEEE","IEEE Conferences"
"XSS Application Worms: New Internet Infestation and Optimized Protective Measures","J. Shanmugam; M. Ponnavaikko","BITS; SRM University, Chennai","Eighth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing (SNPD 2007)","","2007","3","","1164","1169","There has been considerable increase in application layer attacks. Research surveys show that the cross site scripting (XSS) attack is most common among all the application layer attacks. Ajax Web technology, by design makes number of calls to the Web server to process a user request. This increases the bandwidth usage and response time due increase in the number of calls to the Web server. If security mechanisms are implemented to protect the application, then the server performance will suffer due to the additional processing required thereby resulting in increased response time. If security mechanisms are implemented to protect the application, then the server performance will suffer due to the increased response time because of the increase in number of requests. This problem demands an efficient approach to protect the Web application from XSS attacks and to block the malicious attempts from reaching the Web application. This paper presents a thread based solution for efficient process utilization of the Web server and to prevent XSS threats. The proposed solution has been tested using Java/JSP on JBOSS server on around 2000 vulnerable XSS input collected from various research sites, white hat and black hat sites. The model is also tested with the combination of non vulnerable input and vulnerable input to assess the performance. The approach is found to be effective compared to the earlier research works.","","0-7695-2909-7978-0-7695-2909","10.1109/SNPD.2007.514","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4288025","Application-Level Web Security;Component-based Design;security vulnerabilities.","Internet;Protection;Web server;Web pages;Java;Delay;Payloads;Computer hacking;Bandwidth;Testing","file servers;Internet;invasive software;Java","worms;Internet infestation;optimized protective measures;application layer attacks;cross site scripting attack;Ajax Web technology;Web server;security mechanisms;Web application;process utilization;Java/JSP;JBOSS server","","3","6","","","","","","IEEE","IEEE Conferences"
"Automatic Parametrization of Edge Detection Algorithms","L. Csink; S. Sergyan","Budapest Tech, Institute of Software Technology, Bécsi út 96/B, Budapest, H-1034, Hungary. csink.laszlo@nik.bmf.hu; Budapest Tech, Institute of Software Technology, Bécsi út 96/B, Budapest, H-1034, Hungary. sergyan.szabolcs@nik.bmf.hu","2007 5th International Symposium on Intelligent Systems and Informatics","","2007","","","119","121","Selecting the right parameters is a vital issue in image processing algorithms. Normally this is done after experimenting with a limited number of pictures, and then one just hopes that the parameters will work adequately with all the other images. Experimenting with the parameters, however, consumes precious human time. In this paper we present a technique that aims at finding the right parametrization of edge detection algorithms by comparing several approaches and deciding on the optimal one automatically.","1949-047X;1949-0488","978-1-4244-1442-0978-1-4244-1443","10.1109/SISY.2007.4342636","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4342636","","Image edge detection;Software algorithms;Image processing;Testing;Image databases;Intelligent systems;Informatics;Humans;Optimization methods;Information retrieval","edge detection;image processing","automatic parametrization;edge detection algorithms;image processing algorithms;parametrization","","3","7","","","","","","IEEE","IEEE Conferences"
"A high-quality pseudorandom numbers generator based on twi-layer couple cellular automata","X. Xia; Y. Li; J. Zhu","School of Computer, Wuhan University, China; CO 430079; State Key Laboratory of Software Engineering, Wuhan University, China; CO 430072; State Key Laboratory of Software Engineering, Wuhan University, China; CO 430072","2009 IEEE Congress on Evolutionary Computation","","2009","","","2265","2272","This paper proposes a new class of cellular automata, twi-layer couple cellular automata (TLCCA), with specific application to pseudorandom number generation. TLCCA consists of two layer each of which is a one dimensional CA. Two different rules are selected in the lower-layer CA on account of hybrid CA had more complex behavior. The upper-layer CA is divided into two parts. These two parts have a novel neighbourhood, which called couple-structure neighbourhood. By this neighbourhood, two parts in upper layer interplay with each other. ENT test suites are adopted to test the randomness of PRNG. In order to find a stable PRNG, entropy, chi-square and serial correlation coefficient and their variability need to be considered. So a multi-objectives optimization algorithm is proposed. The results of experiment indicate that the TLCCA PRNG can obtain credible random number using no less than 48 cells. The merits of TLCCA PRNG are simpler structure, higher efficiency and better robusticity.","1089-778X;1941-0026","978-1-4244-2958-5978-1-4244-2959","10.1109/CEC.2009.4983222","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4983222","","Hardware;Random number generation;Sampling methods;Throughput;Content addressable storage;Character generation;Costs;Automatic testing;Field programmable gate arrays;Genetic algorithms","cellular automata;correlation methods;entropy;optimisation;random number generation;statistical analysis","pseudorandom number generation;twi-layer couple cellular automata;TLCCA PRNG;couple-structure neighbourhood;ENT test suite;entropy method;chi-square method;serial correlation coefficient;multiobjective optimization algorithm","","","20","","","","","","IEEE","IEEE Conferences"
"FPGA Implementation of a Digital FM Modem","I. Hatai; I. Chakrabarti","NA; NA","2009 International Conference on Information and Multimedia Technology","","2009","","","475","479","In this paper an FPGA implementation of a high performance programmable digital FM modem has been done for targeting towards the Software Defined Radio (SDR) application. The proposed design consists of the reprogrammable, area optimized and low-power features. The modulator and demodulator contain a compressed direct digital synthesizer (DDS) for generating the carrier frequency with spurious free dynamic range of more than 70 dB. The demodulator has been implemented based on the digital phase locked loop (DPLL) technique. The same DDS has been used for demodulating the modulated signal. The proposed FM modem has been implemented and tested using Virtex2Pro University board as a target device. Implementation of the FM modem can run maximum 103 MHz, by taking less than 8k gate equivalent in the XC2VP-30 FPGA device.","","978-1-4244-5383-2978-0-7695-3922","10.1109/ICIMT.2009.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381161","Frequency Modulation (FM);Software Defined Radio (SDR);Field Programmable Gate Array (FPGA);Digital Phase Locked Loop (DPLL);Direct Digital Synthesizer (DDS)","Field programmable gate arrays;Modems;Frequency modulation;Demodulation;Software radio;Application software;Design optimization;Digital modulation;Frequency synthesizers;Dynamic range","field programmable gate arrays;frequency modulation;modems;phase locked loops;software radio","digital FM modem;software defined radio;SDR;direct digital synthesizer;DDS;carrier frequency;spurious free dynamic range;digital phase locked loop;DPLL technique;Virtex2Pro University board;target device;XC2VP-30 FPGA device","","1","28","","","","","","IEEE","IEEE Conferences"
"Selecting Best Practices for Effort Estimation","T. Menzies; Z. Chen; J. Hihn; K. Lum","Department of Computer Science, West Virginia University, Morgantown, WV 26506-610; 941 W. 37th Place, SAL 337, Los Angeles, CA 90089; Jet Propulsion Laboratory, 4800 Oak Grove Drive, Pasadena, CA 91109-8099; Jet Propulsion Laboratory, 4800 Oak Grove Drive, Pasadena, CA 91109-8099","IEEE Transactions on Software Engineering","","2006","32","11","883","895","Effort estimation often requires generalizing from a small number of historical projects. Generalization from such limited experience is an inherently underconstrained problem. Hence, the learned effort models can exhibit large deviations that prevent standard statistical methods (e.g., t-tests) from distinguishing the performance of alternative effort-estimation methods. The COSEEKMO effort-modeling workbench applies a set of heuristic rejection rules to comparatively assess results from alternative models. Using these rules, and despite the presence of large deviations, COSEEKMO can rank alternative methods for generating effort models. Based on our experiments with COSEEKMO, we advise a new view on supposed ""best practices"" in model-based effort estimation: 1) Each such practice should be viewed as a candidate technique which may or may not be useful in a particular domain, and 2) tools like COSEEKMO should be used to help analysts explore and select the best method for a particular domain","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.114","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015511","Model-based effort estimation;COCOMO;deviation;data mining.","Best practices;Testing;Statistical analysis;Data mining;Predictive models;Humans;Guidelines;Software safety;Estimation error;Linear regression","data mining;project management;software cost estimation;statistical analysis","effort estimation method;standard statistical method;COSEEKMO toolkit;heuristic rejection rule","","99","37","","","","","","IEEE","IEEE Journals & Magazines"
"Relevancy Ranking of Web Services","E. Al-Masri; Q. H. Mahmoud","Department of Computing and Information Science (CIS), University of Guelph, ON, N1G 2W1 Canada; Department of Computing and Information Science (CIS), University of Guelph, ON, N1G 2W1 Canada","2007 IEEE International Conference on Systems, Man and Cybernetics","","2007","","","783","788","In order for Web services to truly become a standard approach for just-in-time application integration, we need to enhance the discovery mechanism by assisting clients to select relevant Web services of interest. In recent years, there have been several standards that regulate how services can be published, discovered, or used (i.e. UDDI, WSDL, SOAP). Many of these standards have the potential of enhancing the discovery process, however, there are major technical challenges associated with these standards. One of these challenges is the client's ability to control and manage the discovery process for finding Web services of interest. Clients should be able to find relevant services much more efficiently. To address this issue, we propose a Web service ranking (WSR) algorithm for measuring the relevancy of Web services to particular clients' requirements. This paper presents experimental validation, results and analysis of the presented ideas.","1062-922X","978-1-4244-0990-7978-1-4244-0991","10.1109/ICSMC.2007.4413652","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4413652","","Web services;Meteorological radar;Quality of service;Application software;Costs;Information analysis;Testing;Organizational aspects;Software quality;Ontologies","just-in-time;Web services","just-in-time application integration;Web service ranking algorithm","","1","19","","","","","","IEEE","IEEE Conferences"
"Genetic Programming meets Model-Driven Development","T. Weise; M. Zapf; M. Ullah; U. Khan; K. Geihs","Univ. of Kassel Wilhelmshoher, Kassel; Univ. of Kassel Wilhelmshoher, Kassel; Univ. of Kassel Wilhelmshoher, Kassel; Univ. of Kassel Wilhelmshoher, Kassel; Univ. of Kassel Wilhelmshoher, Kassel","7th International Conference on Hybrid Intelligent Systems (HIS 2007)","","2007","","","332","335","Genetic programming is known to provide good solutions for many problems like the evolution of network protocols and distributed algorithms. Then, it is most likely a hardwired module of a design framework where it assists the engineer in optimizing specific aspects in system development. In this paper we show how the utility of genetic programming can be increased remarkably by isolating it as a component and integrating it into the model-driven software development process. Our genetic programming framework produces XMI-encoded UML models that can easily be loaded into widely available modeling tools, which in turn offer code generation as well as additional analysis and test capabilities. We use the evolution of a distributed election algorithm as an example to illustrate how genetic programming can be combined with model-driven development.","","0-7695-2946-1978-0-7695-2946","10.1109/HIS.2007.11","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4344073","","Genetic programming;Unified modeling language;Application software;Protocols;Software systems;Hybrid intelligent systems;Distributed algorithms;Design engineering;Design optimization;Testing","formal specification;genetic algorithms;Unified Modeling Language","genetic programming;model-driven software development;XMI-encoded UML model;code generation;distributed election algorithm","","","11","","","","","","IEEE","IEEE Conferences"
"A New Method for Constructing Decision Tree Based on Rough Set Theory","L. Huang; M. Huang; B. Guo; Z. Zhuang","NA; NA; NA; NA","2007 IEEE International Conference on Granular Computing (GRC 2007)","","2007","","","241","241","One of the keys to constructing decision tree model is to choose standard for testing attribute, for the criteria of selecting test attributes influences the classification accuracy of the tree. There exists diversity choosing standards for testing attribute based on entropy, Bayesian, and so on. In this paper, the degree of dependency of decision attribute on condition attribute, based on rough set theory, is used as a heuristic for selecting the attribute that will best separate the samples into individual classes. The results of example and experiments show that compared with the entropy-based approach, our approach is a better way to select nodes for constructing decision tree.","","0-7695-3032-X978-0-7695-3032","10.1109/GrC.2007.13","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4403102","","Decision trees;Set theory;Information systems;Classification tree analysis;Software testing;Feature extraction;Educational institutions;Software standards;Entropy;Bayesian methods","decision trees;optimisation;pattern classification;rough set theory","rough set theory;decision tree model;testing attribute;classification accuracy;heuristic","","5","10","","","","","","IEEE","IEEE Conferences"
"Dual frequency planar microstrip antenna","S. Chakrabarti","SAMEER Kolkata Centre, Plot-L2, Block-GP, Sector-V, Salt Lake Electronics Complex, 700091, India","2009 Applied Electromagnetics Conference (AEMC)","","2009","","","1","4","In this paper a simple technique to develop a two layer dual frequency antenna with identical polarization at two far apart frequencies has been presented. Dual rectangular resonator proximity coupled planar microstrip antennas has been designed, fabricated and tested. The design optimisation was carried out using commercial MoM based simulation software. The patch width, intermediate spacing between the coplanar patches are optimized by successive simulations using the commercial software. Measured gain at both the frequencies is over 6 dBi. Measured results are also presented.","","978-1-4244-4818-0978-1-4244-4819","10.1109/AEMC.2009.5430723","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5430723","","Microstrip antennas;Frequency;Patch antennas;Polarization;Bandwidth;Design optimization;Antenna measurements;Costs;Feeds;Information technology","method of moments;microstrip antennas;multifrequency antennas;optimisation;planar antennas","dual frequency planar microstrip antenna;polarization;design optimisation;MoM based simulation software;microstrip patch antenna","","","9","","","","","","IEEE","IEEE Conferences"
"Optimizing In-Queue Flight Regimes","D. D. Dimitrakiev; N. D. Nikolova; K. I. Tenekedjiev","Technical University of Varna, 9010 Varna, 1 Studentska Str., Bulgaria. telephone: +359 52 383 670, e-mail: ddimitrakiev@yahoo.com; Technical University of Varna, 9010 Varna, 1 Studentska Str., Bulgaria. telephone: +359 52 383 670, e-mail: natalia@dilogos.com; Technical University of Varna, 9010 Varna, 1 Studentska Str., Bulgaria. telephone: +359 52 383 670, e-mail: kiril@dilogos.com","2006 3rd International IEEE Conference Intelligent Systems","","2006","","","142","149","A simulation platform in MATLAB was developed to investigate dynamics of level in-queue flights under different control influence. Insight is given to a supplement of this system, which optimizes the work regime in terms of flight level, speed and order. The objectives of the optimization is to achieve economy of the flight through minimization of fuel loss, while providing control possibilities over the speed and flight level of the queue. The optimal flight parameters will not change during the queue flight, till new events trigger new optimization procedure. The effect of each flight regime over the queue is described as a multidimensional observation vector. A hierarchical additive value function is constructed under strict certainty and mutual preferential independence of the coordinates, which reflects the preferences of a rational decision maker over the observation vectors. An algorithm to find the optimal flight parameters is proposed, and the possibilities of the software that executes it are described. A numerical experiment is conducted, which illustrates the necessity of situation optimization","1541-1672;1941-1294","1-4244-0195-X1-4244-01996","10.1109/IS.2006.348408","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4155415","flight economy;in-queue control;in queue flight;situational optimization","Aerospace simulation;Safety;Mathematical model;Telephony;Engines;Testing;MATLAB;Air traffic control;Aircraft;Intelligent systems","aerospace control;aerospace simulation;mathematics computing;optimisation","in-queue flight regime;Matlab;in-queue flight dynamics;optimization;multidimensional observation vector;optimal flight parameter","","","23","","","","","","IEEE","IEEE Conferences"
"A G2 framework for supervisory control of Ecosimpro experiments","J. L. Lozano; J. M. Sanchez; F. Mesa","Escuela Politecnica Superior, Cadiz Univ., Spain; Escuela Politecnica Superior, Cadiz Univ., Spain; NA","MELECON 2006 - 2006 IEEE Mediterranean Electrotechnical Conference","","2006","","","409","412","This document presents a framework for the supervision and control of processes using the expert system generator G2 and Ecosimpro, which is a powerful software for systems modelling and simulation. The special emphasis of this work is on the of generation experiments and on the supervision and control of modelling problems. This may be used as a previous study and testing of a real plant for the nominal operational conditions before to implement in it the G2 supervisory control toolkits. This paper proposes a real time optimization (RTO) architecture that links the intelligent features of G2 to the high-fidelity real-time Ecosimpro experiments via TCP/IP. The paper also describes its practical application to a system of thermal solar energy","2158-8473;2158-8481","1-4244-0087","10.1109/MELCON.2006.1653125","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1653125","","Supervisory control;Power system modeling;Real time systems;Process control;Expert systems;Power generation;Software systems;Testing;TCPIP;Solar energy","expert systems;power engineering computing;power generation control;SCADA systems;software engineering;solar power stations;thermal power stations;transport protocols","G2 framework;supervisory control;Ecosimpro experiments;expert system generator;real time optimization;TCP/IP;thermal solar energy","","","10","","","","","","IEEE","IEEE Conferences"
"A hardware/software co-solution to achieving high throughput required by motion estimation part in H.264/AVC HDTV real-time application","Z. Chen; T. Ikenaga; S. Goto","The Graduate School of Information, Production andSystems, Waseda University, Kitakyushu, Fukuoka, Japan; The Graduate School of Information, Production andSystems, Waseda University, Kitakyushu, Fukuoka, Japan; The Graduate School of Information, Production andSystems, Waseda University, Kitakyushu, Fukuoka, Japan","2008 IEEE International Symposium on VLSI Design, Automation and Test (VLSI-DAT)","","2008","","","128","131","The high throughput required by Motion Estimation (ME) part in H.264/AVC High Definition TV (HDTV) real-time application is very difficult to achieve. Currently one kind of the solutions to this problem is multipling the processing element (PE) array to construct redundant PE array structure. Although redundant structure can take advantages of parallel processing to achieve high throughput, meanwhile it bring in linear increasing of hardware cost. In this paper, a hardware/software co-solusion is proposed to achieve the required throughput of ME part in H.264/AVC HDTV realtime application. In software side, one adaptive search range (ASR) algorithm which is previously proposed by us [12] is firstly introduced and then experimentally proved can improve the throughput 11.48 times averagely in HDTV1080p video sequence. In hardware side, a previously proposed architecture called SAD-tree [17] is firstly introduced. Then based on this architecture, optimization that increase the frequence is proposed. The hardware implementing result of the optimized architecture shows the proposed optimization can triple the frequence. Finally, it is illustrated that the hardware/software co-solution can help to achieve the required throughput.","","978-1-4244-1616-5978-1-4244-1617","10.1109/VDAT.2008.4542429","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4542429","required throughput;ME;ASR;SAD-tree","Hardware;Throughput;Motion estimation;Automatic voltage control;HDTV;Application software;Computer architecture;Frequency;TV;Parallel processing","high definition television;motion estimation;video coding","hardware cosolution;software cosolution;motion estimation;H.264;AVC;parallel processing;adaptive search range algorithm;HDTV1080p video sequence;SAD tree","","4","17","","","","","","IEEE","IEEE Conferences"
"Energy Consumption Measurement Technique for Automatic Instruction Set Characterization of Embedded Processors","M. Wendt; M. Grumer; C. Steger; R. WeiB; U. Neffe; A. Muhlberger","Institute for Technical Informatics, Graz University of Technology, Inffeldgasse, 16 - 8010 Graz, Austria. Phone: +43 316 8736412, Fax: +43 316 8736903, E-mail: wendt@iti.tugraz.at; Institute for Technical Informatics, Graz University of Technology, Inffeldgasse, 16 - 8010 Graz, Austria; Institute for Technical Informatics, Graz University of Technology, Inffeldgasse, 16 - 8010 Graz, Austria; Institute for Technical Informatics, Graz University of Technology, Inffeldgasse, 16 - 8010 Graz, Austria; NXP Semiconductors, Business Line Identification, Micronweg, 1 - 8101 Gratkorn, Austria. E-mail: ulrich.neffe@nxp.com; NXP Semiconductors, Business Line Identification, Micronweg, 1 - 8101 Gratkorn, Austria","2007 IEEE Instrumentation & Measurement Technology Conference IMTC 2007","","2007","","","1","4","The steadily increasing performance of mobile devices implies also a rise in power consumption. To counteract this trend it is mandatory to accomplish software power optimizations based on accurate power consumption models characterized for the processor. This paper presents an environment for automated instruction set characterization based on physical power measurements. For an accurate current measurement a high performance sampling technique has been established, which can be either clock or energy driven. The performance of those techniques is analyzed and the advantages over the conventional solution of a series resistor are discussed.","1091-5281","1-4244-1080-01-4244-0588","10.1109/IMTC.2007.379342","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4258184","Software energy estimation;processor characterization;current measurement;clock driven sampling;energy driven sampling","Energy consumption;Measurement techniques;Current measurement;Sampling methods;Power measurement;Integrated circuit measurements;Clocks;Voltage;Hardware;Costs","embedded systems;instruction sets;integrated circuit testing;low-power electronics;mobile handsets;power aware computing;resistors","automatic instruction set characterization;embedded processors;energy consumption measurement technique;mobile devices;software power optimizations;power consumption models;current measurement;sampling technique;series resistor","","6","9","","","","","","IEEE","IEEE Conferences"
"RFC 2544 performance evaluation and internal measurements for a Linux based open router","R. Bolla; R. Bruschi","Dept. of Commun., Comput. & Syst. Sci., Genoa Univ., Italy; Dept. of Commun., Comput. & Syst. Sci., Genoa Univ., Italy","2006 Workshop on High Performance Switching and Routing","","2006","","","6 pp.","","Recent technological advances give a good chance to do something really effective in the field of open Internet equipments, also called open routers (ORs). Some initiatives have been activated since the last few years to investigate the OR and related issues. But despite these activities, large interesting areas still require a deeper investigation. This work tries to give a contribution by reporting the results of an in-depth activity of optimization and testing realized on a PC open router architecture based on Linux software and COTS hardware. The main target approached in this paper has been the forwarding performance evaluation of different OR Linux-based SW architectures. This analysis has been performed with both external (throughput and latencies) and internal (profiling) measurements. In particular, for what concerns the external measurements, a set of RFC2544 compliant tests has been proposed and analyzed","2325-5552;2325-5560","0-7803-9569","10.1109/HPSR.2006.1709673","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1709673","","Linux;Computer architecture;Internet;Software testing;Hardware;Performance analysis;Performance evaluation;Throughput;Delay;Particle measurements","Internet;Linux;optimisation;performance evaluation;software architecture;telecommunication network routing","RFC2544 performance evaluation;Internet;optimization;PC open router architecture;OR Linux-based software architecture;COTS hardware","","11","22","","","","","","IEEE","IEEE Conferences"
"Finite element analysis of an air spring concerning initial pressure and parameters of cord fabric layer","Shi Wenku; Jiang Wan; Huang Ying; Yao Weimin; Ya Hao; Liu Zubin","State Key Laboratory of Automobile Dynamic Simulation, Jilin University, Changchun, China; State Key Laboratory of Automobile Dynamic Simulation, Jilin University, Changchun, China; State Key Laboratory of Automobile Dynamic Simulation, Jilin University, Changchun, China; State Key Laboratory of Automobile Dynamic Simulation, Jilin University, Changchun, China; FAW Group Corporation R&D Center, Changchun, China; FAW Group Corporation R&D Center, Changchun, China","2009 Asia-Pacific Conference on Computational Intelligence and Industrial Applications (PACIIA)","","2009","1","","496","499","A finite element analysis (FEA) model of an air spring was carried out with the software ABAQUS to study the influence of cord fabric layer parameters on the elastic characteristic of air spring. The FEA model was builded and meshed meanwhile by writing program file. The cord fabric was simulated by rebar element. The compressed gas in the cavity of air spring was represented by the hydrostatic fluid element. Results of simulation coincide well with the experimental data gathered by the MTS831 elastic component testing system. The static elastic characteristic of air spring was simulated under different parameters to ascertain the initial pressure and cord fabric Parameters'impact, utilizing sensitivity analysis method. The study reveals that: the cord angle and cord interval influence the elastic characteristics of air spring relatively more and can both be adjusted easily; cord cross section area and layer number have certain impact on air spring elastic characteristics however their adjusts might influence the robustness and distorting pattern of whole structure; considering the robustness of air bag, the initial pressure should not vary too much. The conclusion has certain referencing significance for the design and optimizing of air spring of the same kind.","","978-1-4244-4606","10.1109/PACIIA.2009.5406380","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5406380","air spring;cord fabric;FEA;ABAQUS;sensitivity analysis method","Finite element methods;Springs;Fabrics;Vehicle dynamics;Analytical models;Sensitivity analysis;Robustness;Design optimization;Vehicle safety;Mathematical model","automotive components;elasticity;fabrics;finite element analysis;hydrostatics;mathematics computing;rebar;springs (mechanical);testing;vehicle dynamics","air spring;cord fabric layer;finite element analysis model;ABAQUS software;rebar element;compressed gas;hydrostatic fluid element;MTS831 elastic component testing system;static elastic characteristic;air bag","","","14","","","","","","IEEE","IEEE Conferences"
"Beam Stop for Electron Accelerator Beam Characterisation","G. Roach; V. Sharp; J. Tickner; J. Uher","NA; NA; NA; NA","IEEE Transactions on Nuclear Science","","2009","56","4","2330","2335","Electron linear accelerator applications involving the generation of hard X-rays frequently require accurate knowledge of the electron beam parameters. We developed a beam stop device which houses a tungsten Bremsstrahlung target and enables the electron beam current, energy and position to be monitored. The beam stop consisted of four plates. The first was a removable aluminium (Al) transmission plate. Then followed the tungsten target. Behind the target there were four Al quadrant plates for beam position measurement. The last plate was a thick Al back-stop block. Currents from the four quadrants and the back-stop were measured and the beam lateral position, energy and current were calculated. The beam stop device was optimised using Monte-Carlo simulation, manufactured (including custom-made electronics and software) in our laboratory and tested at the ARPANSA (Australian Radiation Protection and Nuclear Safety Agency) linear accelerator in Melbourne. The electron beam energy was determined with a precision of 60 keV at beam energies between 11 and 21 MeV and the lateral beam position was controlled with a precision of 200 mum. The relative changes of the beam current were monitored as well.","0018-9499;1558-1578","","10.1109/TNS.2009.2025179","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5204728","Accelerator beam line instrumentation;accelerator control systems","Particle beams;Electron beams;Electron accelerators;Linear accelerators;Tungsten;Monitoring;Position measurement;Software safety;Electronic equipment testing;X-rays","bremsstrahlung;electron accelerators;electron beams;linear accelerators;Monte Carlo methods;optimisation;particle beam diagnostics","electron accelerator beam characterisation;electron linear accelerator;hard X-rays;electron beam position;tungsten Bremsstrahlung target;electron beam current;electron beam energy;beam monitoring;aluminium transmission plate;beam position measurement;optimisation;Monte-Carlo simulation;ARPANSA;electron volt energy 11 MeV to 21 MeV","","","11","","","","","","IEEE","IEEE Journals & Magazines"
"Visualization and optimization of agent coordination strategies in Mobile Agent Network","M. Gulic; M. Ivanisevic; M. Kusek; K. Jurasovic; G. Jezic","University of Zagreb, Faculty of Electrical Engineering and Computing, Unska 3, 10000, Croatia; University of Zagreb, Faculty of Electrical Engineering and Computing, Unska 3, 10000, Croatia; University of Zagreb, Faculty of Electrical Engineering and Computing, Unska 3, 10000, Croatia; University of Zagreb, Faculty of Electrical Engineering and Computing, Unska 3, 10000, Croatia; University of Zagreb, Faculty of Electrical Engineering and Computing, Unska 3, 10000, Croatia","MELECON 2008 - The 14th IEEE Mediterranean Electrotechnical Conference","","2008","","","234","239","This paper deals with the visualization of simulation results and the optimization of operation distribution used by agents to perform operations in the mobile agent network model. This model consists of agents capable of performing remote software management operations and of various network elements and nodes agents use during operation execution. For the purpose of testing different operation distributions which define how these operations are performed a mobile agent network simulator was developed. This simulator allows users to define operations agents must perform and the physical network they use in the process. The data from the simulator is gathered during simulation, visualized using a visualization framework and analyzed at a later time by a human user. The paper will also show an approach based on genetic algorithm that optimizes the distribution of operations to reduce network load and the total execution time.","2158-8473;2158-8481","978-1-4244-1632-5978-1-4244-1633","10.1109/MELCON.2008.4618440","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4618440","Coordination;Genetic algorithms;Large-scale network;Simulation;Visualization","Load modeling;Biological cells;Genetic algorithms;Software;Network topology;Java;Optimization","genetic algorithms;mobile computing;mobile radio;multi-agent systems;software management","agent coordination strategies;mobile agent network;remote software management operations;network simulator;data simulator;visualization framework;human user;genetic algorithm","","","12","","","","","","IEEE","IEEE Conferences"
"Entity-based collaboration tools for intelligence analysis","E. A. Bier; S. K. Card; J. W. Bodnar","Palo Alto Research Center, Inc., 3333 Coyote Hill Road, California, 94304, USA; Palo Alto Research Center, Inc., 3333 Coyote Hill Road, California, 94304, USA; Science Applications International Corporation, 1710 SAIC Drive, McLean, VA 22102, USA","2008 IEEE Symposium on Visual Analytics Science and Technology","","2008","","","99","106","Software tools that make it easier for analysts to collaborate as a natural part of their work will lead to better analysis that is informed by more perspectives. We are interested to know if software tools can be designed that support collaboration even as they allow analysts to find documents and organize information (including evidence, schemas, and hypotheses). We have modified the Entity Workspace system, described previously, to test such designs. We have evaluated the resulting design in both a laboratory study and a study where it is situated with an analysis team. In both cases, effects on collaboration appear to be positive. Key aspects of the design include an evidence notebook optimized for organizing entities (rather than text characters), information structures that can be collapsed and expanded, visualization of evidence that emphasizes events and documents (rather than emphasizing the entity graph), and a notification system that finds entities of mutual interest to multiple analysts.","","978-1-4244-2935","10.1109/VAST.2008.4677362","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4677362","sensemaking;information foraging;collective intelligence;exploratory search;information workspace;entity-based;collaboration;intelligence analysis;visualization;semantic notebook;argumentation marshalling;visual analytics;H.5.2 [User Interfaces]: Graphical user interfaces (GUI);H.3.3 [Information Search and Retrieval]: Information filtering;H.4 [Information Systems Applications]: H.4.m Miscellaneous;H.5.3 [Group and Organization Interfaces]: Collaborative computing, Computer-supported cooperative work, Web-based interaction","Collaborative tools;Collaborative work;Software tools;Collaborative software;Information analysis;Text analysis;System testing;Laboratories;Design optimization;Organizing","entity-relationship modelling;groupware;software tools","entity-based collaboration tools;intelligence analysis;software tools;entity workspace system;information structures","","19","17","","","","","","IEEE","IEEE Conferences"
"Using Formal Methods and Agent-Oriented Software Engineering for Modeling NASA Swarm-Based Systems","C. A. Rouff; M. G. Hinchey; J. Pena; A. Ruiz-Cortes","SAIC, Advanced Technologies and Solutions Business Unit, McLean, VA 22102. rouffc@saic.com; Loyola College in Maryland, Computer Science Department, Baltimore, MD, USA. mike.hinchey@usa.net; University of Seville, Seville, Spain. joaquinp@us.es; University of Seville, Seville, Spain. aruiz@us.es","2007 IEEE Swarm Intelligence Symposium","","2007","","","348","355","NASA is conducting research on advanced technologies for future exploration using intelligent swarms of robotic vehicles. One of these missions is the Autonomous Nano Technology Swarm (ANTS) mission that will explore the asteroid belt using 1,000 cooperative autonomous spacecraft. From an engineering point of view, the complexity and emergent behavior of this kind of system is one of the main challenges that has to be overcome, since it makes the behavior of the swarm unpredictable. In NASA, many approaches are being explored towards this goal, mainly, a tailored software engineering approach, called agent-oriented software engineering, and formal methods. In this paper, we report on the main advances we have made towards modeling, implementing, and testing NASA swarms-based concept missions","","1-4244-0708","10.1109/SIS.2007.367958","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4223195","","Software engineering;NASA;Space technology;Intelligent robots;Intelligent vehicles;Particle swarm optimization;Orbital robotics;Remotely operated vehicles;Belts;Space vehicles","aerospace computing;aerospace control;control engineering computing;cooperative systems;formal verification;mobile robots;multi-robot systems;software agents;space vehicles","formal methods;agent-oriented software engineering;NASA swarm-based systems;intelligent swarms;robotic vehicles;Autonomous Nano Technology Swarm mission;cooperative autonomous spacecraft","","4","33","","","","","","IEEE","IEEE Conferences"
"Efficiency optimization considerations for standard induction motor fed by PWM inverter","E. M. Tsampouris; A. G. Kladas","Laboratory of Electrical Machines and Power Electronics, Faculty of Electrical and Computer Engineering, National Technical University of Athens, Greece; Laboratory of Electrical Machines and Power Electronics, Faculty of Electrical and Computer Engineering, National Technical University of Athens, Greece","2009 8th International Symposium on Advanced Electromechanical Motion Systems & Electric Drives Joint Symposium","","2009","","","1","6","The paper investigates efficiency in standard induction motors fed by PWM inverters, taking into consideration criteria in the control algorithm enabling to minimize motor losses. For this purpose an overview of the relevant techniques was carried out, adequate simulation software tools were adopted and an experimental setup involving a 2 HP induction motor supplied by an inverter driven by DSP was developed. It was shown that the main efficiency improvement can be obtained by conveniently tuning the magnetizing flux to the desired operating conditions while the switching frequency choice does not considerably affect the overall motor losses, in the case considered.","","978-1-4244-5150-0978-1-4244-5152","10.1109/ELECTROMOTION.2009.5259130","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5259130","","Induction motors;Pulse width modulation inverters;Equivalent circuits;Circuit testing;Switching frequency;Frequency conversion","induction motors;PWM invertors","efficiency optimization;standard induction motor;PWM inverter;control algorithm;motor loss;simulation software tools;magnetizing flux;switching frequency choice;power 2 hp","","4","9","","","","","","IEEE","IEEE Conferences"
"Developments on software tools for data acquisition and numerical simulation for gamma-ray scanning","R. Alami; A. Ouardi; H. Laghyam; S. Lhaiba; B. Bensitel; D. Benchekroun","Centre de Physique des Particules de Marseille, CNRS-IN2P3, Universit&#x00E9; de la M&#x00E9;diterran&#x00E9;e Aix-Marseille II, 13288 Marseille CEDEX 09, France; Centre de Physique des Particules de Marseille, CNRS-IN2P3, Universit&#x00E9; de la M&#x00E9;diterran&#x00E9;e Aix-Marseille II, 13288 Marseille CEDEX 09, France; Centre de Physique des Particules de Marseille, CNRS-IN2P3, Universit&#x00E9; de la M&#x00E9;diterran&#x00E9;e Aix-Marseille II, 13288 Marseille CEDEX 09, France; Centre de Physique des Particules de Marseille, CNRS-IN2P3, Universit&#x00E9; de la M&#x00E9;diterran&#x00E9;e Aix-Marseille II, 13288 Marseille CEDEX 09, France; Centre de Physique des Particules de Marseille, CNRS-IN2P3, Universit&#x00E9; de la M&#x00E9;diterran&#x00E9;e Aix-Marseille II, 13288 Marseille CEDEX 09, France; Laboratoire de Physique des Particules - Facult&#x00E9; des Sciences Ain Chock Casablanca, Morocco","2009 1st International Conference on Advancements in Nuclear Instrumentation, Measurement Methods and their Applications","","2009","","","1","6","The gamma-ray scanning technique is widely used for evaluating the operating characteristics of distillation columns considered as the most critical components in petrochemical plants. Gamma-ray scanning provides essential data to optimize the performance of the columns and to identify maintenance requirements. New and more efficient data acquisition and processing software has been developed for Gamma-ray scanning. This new software permits to conduct gamma scanning explorations with all necessary information concerning the columns directly incorporated in the software. New features help a lot to interpret on-line the obtained results. A first version of the new data acquisition and processing software has been successfully tested in real conditions of utilization. For enhancing the interpretation of the experimental results of the technique, Geant 3 and Geant 4 Monte carlo simulation tools, developed at the CERN, have been used, to help identifying any possible malfunction inside a column and providing specific information, as a level liquid on a tray. In this study, the Geant simulations are performed for a distillation column. A set of parameters, principally the source energy (<sup>60</sup>Co) and the source-detector separation are fixed. Also many simulations were carried out to prove that there is no effect of the distillate chemical composition on the obtained results. To assure that the code runs correctly, a number of simulations were done to reproduce a density profile in the ideal case (no malfunctions in column). The distillation column process malfunctions that have been examined were principally liquid entrainment and weeping between trays. These phenomena were modelled by varying the thickness of the liquid layer on and under the tray.","","978-1-4244-5207-1978-1-4244-5208","10.1109/ANIMMA.2009.5503678","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5503678","Distillation columns;Gamma scanning;Geant Monte Carlo simulation","Software tools;Data acquisition;Numerical simulation;Distillation equipment;Petrochemicals;Production;Software testing;Chemicals;Inspection;Reservoirs","data acquisition;distillation equipment;gamma-ray spectroscopy;monopoly;Monte Carlo methods;physics computing","software tool;data acquisition;gamma ray scanning;distillation column;petrochemical plant;Geant 3 Monte carlo simulation;Geant 4 Monte carlo simulation;CERN;source-detector separation;density profile;liquid entrainment;weeping","","","10","","","","","","IEEE","IEEE Conferences"
"Towards Four-Layer Framework of Combinatorial Problems","M. S. Levin","NA","2008 32nd Annual IEEE International Computer Software and Applications Conference","","2008","","","873","878","Four-layer framework of combinatorial problems/models is suggested for applied problems structuring and solving: (1) basic combinatorial models and multicriteria DM-models (e.g., ranking, clustering, knapsack problem, multiple choice problem, multicriteria ranking, combinatorial synthesis, assignment/allocation); (2) composite models/procedures (e.g., multicriteria combinatorial problems); (3) basic (standard) solving frameworks, e.g.: (i) hierarchical morphological multicriteria design (HMMD) (ranking, combinatorial synthesis), (ii) multistage design (two-level HMMD), (iii) special multistage composite framework (clustering, assignment/location, multiple choice problem); and (4) domain-oriented solving frameworks, e.g.: (a) design of modular software, (b) design of test inputs for multi-function system testing, (c) combinatorial planning of medical treatment, (d) improvement of communication network. The multi-layer scheme covers 'decision cycle', i.e., problem statement, models, algorithms/procedures, solving schemes, decisions, decision analysis and improvement. The framework of problems corresponds to open complex systems.","0730-3157;0730-3157","978-0-7695-3262","10.1109/COMPSAC.2008.180","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4591685","Open complex systems;system architecture;combinatorial optimization;decisin making;problem structuring;problem solving;artificial intelligence","Hidden Markov models;Software;Resource management;Computational modeling;Integrated circuits;Forecasting;Integrated circuit modeling","combinatorial mathematics;problem solving","combinatorial problems;multicriteria DM-models;composite models;hierarchical morphological multicriteria design;domain-oriented solving;modular software design;multifunction system testing;combinatorial planning;medical treatment;communication network","","4","33","","","","","","IEEE","IEEE Conferences"
"Priority-Processing for Optimized Real-Time Performance with Limited Processing Resources","C. Hentschel; S. Schiemenz","Brandenburg University of Technology, Cottbus, Germany; Brandenburg University of Technology, Cottbus, Germany","2008 Digest of Technical Papers - International Conference on Consumer Electronics","","2008","","","1","2","For future multi-media-systems (MMS) a clear trend can be observed which leads from dedicated hardware to signal processing in software. A problem is to guarantee real-time performance on programmable platforms with varying computational resources. Scalable video algorithms (SVA) which adapt automatically their output quality to the available resources represent thereby a novel concept. We propose the principle of ""Priority-Processing"" as an efficient way for self-adapting video algorithms to the available platform resources. An SVA for sharpness enhancement has been implemented and tested. Even at varying platform resources real-time processing with very good output quality has been achieved.","2158-3994;2158-4001","978-1-4244-1458-1978-1-4244-1459","10.1109/ICCE.2008.4588059","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4588059","","Signal processing algorithms;Control systems;Hardware;Application software;Robust stability;Digital signal processing;Regulators;Video signal processing;Software performance;Testing","multimedia systems;video signal processing","multimedia-systems;scalable video algorithms;priority-processing;self-adapting video algorithms","","3","3","","","","","","IEEE","IEEE Conferences"
"Architectural support for low overhead detection of memory violations","S. Ghose; L. Gilgeous; P. Dudnik; A. Aggarwal; C. Waxman","State University of New York, Binghamton, 13902, USA; State University of New York, Binghamton, 13902, USA; State University of New York, Binghamton, 13902, USA; State University of New York, Binghamton, 13902, USA; State University of New York, Binghamton, 13902, USA","2009 Design, Automation & Test in Europe Conference & Exhibition","","2009","","","652","657","Violations in memory references cause tremendous loss of productivity, catastrophic mission failures, loss of privacy and security, and much more. Software mechanisms to detect memory violations have high false positive and negative rates or huge performance overhead. This paper proposes architectural support to detect memory reference violations in inherently unsafe languages such as C and C++. In this approach, the ISA is extended to include ldquosafetyrdquo instructions that provide compile-time information on pointers and objects. The microarchitecture is extended to efficiently execute the safety instructions. We explore optimizations, such as delayed violation detection and stack-based handling of local pointers, to reduce the performance overhead. Our experiments show that the synergy between hardware and software results in this approach having less than 5% average performance overhead, while an exclusively software mechanism incurs 480% impact for the same benchmarks.","1530-1591;1558-1101","978-1-4244-3781-8978-3-9810-8015","10.1109/DATE.2009.5090747","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5090747","","Software performance;Safety;Computer bugs;Instruction sets;Microarchitecture;Hardware;Application software;Pollution;High performance computing;Usability","C++ language;program debugging;software architecture","architectural support;low overhead detection;memory violations;memory references;catastrophic mission failures;software mechanisms;C++;violation detection;stack-based handling","","11","37","","","","","","IEEE","IEEE Conferences"
"Efficient and Scalable Compiler-Directed Energy Optimization for Realtime Applications","P. Huang; S. Ghiasi","Electrical and Computer Engineering, University of California, Davis, pohuang@ece.ucdavis.edu; Electrical and Computer Engineering, University of California, Davis, soheil@ece.ucdavis.edu","2007 Design, Automation & Test in Europe Conference & Exhibition","","2007","","","1","6","We present a compilation technique that targets realtime applications running on embedded processors with combined dynamic voltage scaling (DVS) and adaptive body biasing (ABB) capabilities. Considering the delay and energy penalty of switching between operating modes of the processor, our compiler judiciously inserts mode switch instructions in selected locations of the code and generates executable binary that is guaranteed to meet the deadline constraint. More importantly, our algorithm runs very fast and comes reasonably close to the theoretical limit of energy optimization using DVS+ABB. At 65 nm technology, we improve the energy dissipation of the generated code by an average of11.4% under deadline constraints. While our technique's improvement in energy dissipation over conventional DVS is marginal (3%) at 130nm, the average improvement continues to grow to 4.7%, 8.8% and 15.4% for 90nm, 65nm and 45nm technology nodes, respectively. Compared to a recent ILP-based competitor, we improve the runtime by more than three orders of magnitude, while producing improved results","1530-1591;1558-1101","978-3-9810801-2","10.1109/DATE.2007.364386","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4211896","","Optimizing compilers;Switches;Frequency;Dynamic voltage scaling;Energy consumption;Voltage control;Delay;Runtime;CMOS technology;Application software","power aware computing;program compilers;real-time systems","compiler;energy optimization;realtime applications;embedded processors;dynamic voltage scaling;adaptive body biasing;energy dissipation;45 nm;65 nm;90 nm","","2","12","","","","","","IEEE","IEEE Conferences"
"Development of a model based efficiency optimization for IPMSM drive","M. N. Uddin; F. Abera","Dept. of Electrical Engineering, Lakehead University, Thunder Bay, ON, Canada, P7B 5E1; Dept. of Electrical Engineering, Lakehead University, Thunder Bay, ON, Canada, P7B 5E1","2009 Canadian Conference on Electrical and Computer Engineering","","2009","","","837","840","A model based efficiency optimization algorithm for speed control of interior permanent magnet synchronous motor (IPMSM) drive is proposed in this paper. In order to optimize the efficiency the loss minimization algorithm is developed based on motor model and operating conditions. The d-axis armature current is utilized to minimize the losses of the IPMSM in a closed loop vector control environment. The complete simulation model for the closed loop vector control of IPMSM drive incorporating the proposed loss minimization algorithm is developed using Matlab/Simulink software. The effectiveness of the proposed algorithm is tested in simulation at different dynamic operating conditions. It is found for the results that the efficiency of the proposed IPMSM drive is improved significantly as compared to the conventional i<sub>d</sub>=0 control technique.","0840-7789","978-1-4244-3509-8978-1-4244-3508","10.1109/CCECE.2009.5090246","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5090246","Efficiency optimization;permanent-magnet synchronous motor;speed control;vector control","Permanent magnet motors;Synchronous motors;AC motors;Reluctance motors;Minimization methods;Mathematical model;Machine vector control;Rotors;Torque;Iron","closed loop systems;machine vector control;minimisation;permanent magnet motors;synchronous motor drives;torque control;velocity control","interior permanent magnet synchronous motor drive;model-based efficiency optimization algorithm;loss minimization algorithm;d-axis armature current;closed loop vector control;speed command;torque command","","3","9","","","","","","IEEE","IEEE Conferences"
"A novel framework for fast embedded face detection system","Jian Xiao; Gugang Gao; Chen Hu; Haidong Fengz","National ASIC System Engineering Research Center, Southeast University, Nanjing 210096, China; National ASIC System Engineering Research Center, Southeast University, Nanjing 210096, China; National ASIC System Engineering Research Center, Southeast University, Nanjing 210096, China; Nanjing University of Posts and Telecommunications, 210003, China","2007 7th International Conference on ASIC","","2007","","","32","35","In this paper, we propose a novel design for one fast embedded face detection system, which can be applied in many real-time applications, such as teleconferencing, user interfaces, and security access control. Our framework includes 3 parts: one fast face detection method based on optimized AdaBoost algorithm with high speed and high detection rate, one SOC hardware framework to speed up detection operations and one software distribution strategy to optimize the memory sub-system. In embedded system domain, face detection is a great challenge because of its limited hardware source and low frequency. Our system prototype is built on the base of one SOC named 'Garfield' to test our design. Experimental results illuminate the validity of our face detection system.","2162-7541;2162-755X","978-1-4244-1131-3978-1-4244-1132","10.1109/ICASIC.2007.4415560","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4415560","","Face detection;Optimization methods;Hardware;Real time systems;Application software;Teleconferencing;User interfaces;Security;Access control;Software algorithms","face recognition;integrated circuit design;system-on-chip","fast embedded face detection system;optimized AdaBoost algorithm;SOC hardware framework;software distribution strategy;memory subsystem;embedded system domain;hardware source","","","11","","","","","","IEEE","IEEE Conferences"
"A lower-dimensional-search evolutionary algorithm and its application in constrained optimization problems","Sanyou Zeng; Hui Shi; Hui Li; Guang Chen; Lixin Ding; Lishan Kang","Dept. of Computer Science, Technology Research Center for Space Science&Technology and The State Key Laboratory of Geological Processes and Mineral Resources, China University of Geosciences, Wuhan, 430074, China; Dept. of Computer Science, Technology Research Center for Space Science&Technology and The State Key Laboratory of Geological Processes and Mineral Resources, China University of Geosciences, Wuhan, 430074, China; School of Managnent, Huazliong University of Science & Technology Hubei Wuhan 430074, China; Dept. of Computer Science, Technology Research Center for Space Science&Technology and The State Key Laboratory of Geological Processes and Mineral Resources, China University of Geosciences, Wuhan, 430074, China; StateKey Laboratory of Software Engineering, Wuhan University, 430072, Hubei, China; Dept. of Computer Science, Technology Research Center for Space Science&Technology and The State Key Laboratory of Geological Processes and Mineral Resources, China University of Geosciences, Wuhan, 430074, China","2007 IEEE Congress on Evolutionary Computation","","2007","","","1255","1260","This paper proposes a new evolutionary algorithm, called lower-dimensional-search evolutionary algorithm (LDSEA). The crossover operator of the new algorithm searches a lower-dimensional neighbor of the parent points where the neighbor center is the barycenter of the parents therefore the new algorithm converges fast, especially for high-dimensional constrained optimization problems. The niche-impaction operator and the mutation operator preserve the diversity of the population to make the LDSEA algorithm not to be trapped in local optima as much as possible. What's more is that the LDSEA algorithm is simple and easy to be implemented. We have used the 24 constrained benchmark problems [18] to test the LDSEA algorithm. The experimental results show it works better than or competitive to a known effective algorithm [7] for higher-dimensional constrained optimization problems.","1089-778X;1941-0026","978-1-4244-1339-3978-1-4244-1340","10.1109/CEC.2007.4424614","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4424614","Evolutionary algorithm;Constraint optimization problems;Solution dominance","Evolutionary computation;Constraint optimization","evolutionary computation;search problems","lower-dimensional-search evolutionary algorithm;constrained optimization problems;lower-dimensional neighbor;niche-impaction operator;mutation operator","","2","19","","","","","","IEEE","IEEE Conferences"
"Simulation of a Self-Optimising Digital Ecosystem","T. Kurz; T. J. Heistracher","ITS Department, Salzburg University of Applied Sciences, Puch/Salzburg, Austria, e-mail: thomas.kurz@fh-salzburg.ac.at; ITS Department, Salzburg University of Applied Sciences, Puch/Salzburg, Austria, e-mail: thomas.heistracher@fh-salzburg.ac.at","2007 Inaugural IEEE-IES Digital EcoSystems and Technologies Conference","","2007","","","165","170","Small and medium enterprises (SMEs) most often lack resources for custom-made software solutions that support best their core businesses. In this paper a simulation framework for self-optimising SME networks, namely Evolutionary Environment Simulator (EvESimulator), is introduced. The Evolutionary Environment, which the framework bases on, is an infrastructural component for distributed and decentralised service creation and service improvement based on mechanisms that are operating similarly in the living environment. This paper concentrates on the simulation of these mechanisms for SME networks. Built upon the widely-used simulation framework Repast, the cooperation behaviour of companies is investigated that use self-optimizing software services and distribute related information amongst them. The simulator is capable of importing real-world data from real businesses thereby enabling conceptual studies and hypothesis testing. It is applied in the context of three utilisation scenarios that investigate critical mass for sustainability, clustering in general, and usage-based clustering. The first results of the EvESimulator reveal a dynamic creation of a growing network of businesses that is clearly outperforming centralized topologies in the long run.","2150-4938;2150-4946","1-4244-0467-31-4244-0470","10.1109/DEST.2007.371964","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4233698","Digital Business Ecosystem;Evolutionary Environment;Agent;Simulation","Ecosystems;Network topology;Testing;Vocabulary;Companies;Social network services;Analytical models;Natural languages;Prototypes;Peer to peer computing","small-to-medium enterprises;software architecture","self-optimising digital ecosystem;small and medium enterprises;custom-made software solution;evolutionary environment simulator;infrastructural component;decentralised service creation;usage-based clustering;service oriented architecture","","3","8","","","","","","IEEE","IEEE Conferences"
"Completely verifying memory consistency of test program executions","C. Manovit; S. Hangal","Stanford Univ., CA, USA; NA","The Twelfth International Symposium on High-Performance Computer Architecture, 2006.","","2006","","","166","175","An important means of validating the design of commercial-grade shared memory multiprocessors is to run a large number of pseudo-random test programs on them. However, when intentional data races are placed in a test program, there may be many correct results according to the memory consistency model supported by the system. For popular memory models like SC and TSO, the problem of verifying correctness of an execution is known to be NP-complete. As a result, analysis techniques implemented in the past have been incomplete: violations of the memory model are flagged if provable, otherwise the result is inconclusive and it is assumed optimistically that the machine's results are correct. In this paper, we describe for the first time a practical, new algorithm which can solve this problem with certainty, thus ensuring that incorrect behavior of a large, complex multiprocessor cannot escape. We present results of our analysis algorithm on test programs run on a newly designed multiprocessor system built by Sun Microsystems. We show that our algorithm performs very well, typically analyzing a program with 512 K memory operations distributed across 60 processors within a few minutes. Our algorithm runs in less than 2.6 times the time taken by an incomplete baseline algorithm which may miss errors. Our approach greatly increases the confidence in the correctness of the results generated by the multiprocessor, and allows us to potentially uncover more bugs in the design than was previously possible.","1530-0897;2378-203X","0-7803-9368","10.1109/HPCA.2006.1598123","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1598123","","System testing;Algorithm design and analysis;Vehicle crash testing;Computer bugs;Sun;Multiprocessing systems;Lamps;Software testing;Performance analysis;Spine","shared memory systems;microprogramming;performance evaluation;parallel architectures","memory consistency verification;commercial-grade shared memory multiprocessor;pseudo-random test program execution;NP-complete;data race;Sun Microsystems","","13","18","","","","","","IEEE","IEEE Conferences"
"Program Optimization of Stencil Based Application on the GPU-Accelerated System","G. Wang; X. Yang; Y. Zhang; T. Tang; X. Fang","NA; NA; NA; NA; NA","2009 IEEE International Symposium on Parallel and Distributed Processing with Applications","","2009","","","219","225","Graphic Processing Unit (GPU), with many light-weight data-parallel cores, can provide substantial parallel computational power to accelerate general purpose applications. But the powerful computing capacity could not be fully utilized for memory-intensive applications, which are limited by off-chip memory bandwidth and latency. Stencil computation has abundant parallelism and low computational intensity which make it a useful architectural evaluation benchmark. In this paper, we propose some memory optimizations for a stencil based application mgrid from SPEC 2 K benchmarks. Through exploiting data locality in 3-level memory hierarchies and tuning the thread granularity, we reduce the pressure on the off-chip memory bandwidth. To hide the long off-chip memory access latency, we further prefetch data during computation through double-buffer. In order to fully exploit the CPU-GPU heterogeneous system, we redistribute the computation between these two computing resource. Through all these optimizations, we gain 24.2 x speedup compared to the simple mapping version, and get as high as 34.3 x speedup when compared with a CPU implementation.","2158-9178;2158-9208","978-0-7695-3747","10.1109/ISPA.2009.70","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5207932","GPGPU;CUDA;mgrid;stencil;heterogeneous system","Concurrent computing;Application software;Acceleration;Yarn;Delay;Distributed processing;Distributed computing;Bandwidth;Parallel processing;Prefetching","benchmark testing;coprocessors;data handling;distributed processing;memory architecture;network operating systems;software engineering;storage management","memory optimization;stencil based application;GPU-accelerated System;graphic processing unit;SPEC 2K benchmarks;data prefetching;heterogeneous system","","2","18","","","","","","IEEE","IEEE Conferences"
"CAD of throttle valves parameters of telescopic shock absorber based on multi-points speed characteristic","Changcheng Zhou; Shicai Ji; Zhiyun Zheng","School of Transportation and Vehicle Engineering, Shandong University of Technology, Zibo 255049, China; School of Transportation and Vehicle Engineering, Shandong University of Technology, Zibo 255049, China; College of Information & Engineering, Zhengzhou University, 450052, China","2009 International Conference on Industrial Mechatronics and Automation","","2009","","","456","459","For the design of throttle valves parameters based on multi-point speed characteristic, there is not any accurate and reliable CAD software for telescopic shock absorber. The speed characteristic of multi-points was analyzed, and the mathematic model of it was founded by nonlinear least square fitting. The optimization design method of curve fitting of valves parameters for multi-points speed characteristic is established. Based on this, the CAD soft ware of valves parameters of telescopic shock absorber was developed, and its characteristics were analyzed. A practical design example of valve parameters of telescope-shock absorber with the CAD soft ware was given, the performance test was conducted for the telescopic shock absorber designed by CAD. The experiment results show that the model of optimization design for the valves parameters based on multi-points speed characteristic is accurate, and the CAD soft ware is reliable.","","978-1-4244-3817","10.1109/ICIMA.2009.5156661","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5156661","CAD software;throttle valves parameters;optimal design;multi-points speed-characteristic;least square fitting","Valves;Shock absorbers;Design automation;Optical design;Design optimization;Mathematics;Mathematical model;Least squares methods;Design methodology;Curve fitting","CAD;curve fitting;mechanical engineering computing;shock absorbers;telescopes;valves","CAD software;throttle valves parameter;telescopic shock absorber;multipoint speed characteristic;mathematical model;curve fitting;optimization design method;nonlinear least square fitting","","","10","","","","","","IEEE","IEEE Conferences"
"Mesh your Senses: Multimedia Applications over WiFi-based Wireless Mesh Networks","R. Riggio; K. Gomez; T. Rasheed; M. Gerola; D. Miorandi","NA; NA; NA; NA; NA","2009 6th IEEE Annual Communications Society Conference on Sensor, Mesh and Ad Hoc Communications and Networks Workshops","","2009","","","1","3","This demo aims at (i) validating the design choices we have made in conceiving and deploying the WING testbed, and (ii) showing the capability of out software toolkit to properly support heterogeneous multimedia applications. Additionally, the mesh networking toolkit's fault management features is demonstrated. We hope that our wireless mesh networking toolkit is considered by both researchers and practitioners as platform of choice to test innovative solutions and to provide end-users with wireless connectivity. WING is an experimental multi-radio WMN testbed designed and built exploiting commodity hardware and open-source software components. WING implements a flexible and scalable WMN architecture capable of supporting next-generation Internet services with a particular focus on multimedia applications. The WING project aims at providing an open-platform on top of which innovative solution can be implemented and tested in a realistic environment. Currently, the testbed consist of 10 nodes deployed at CREATE-NET premises and implementing a two-tiers architecture. Other well-known IEEE 802.11-based WMNs include Roofnet, Hyacinth, Microsoft's MCL, and Meraki. We establish the uniqueness of our mesh solution in that it is capable of achieving both service differentiation and performance isolation in IEEE 802.11-based WMNs. While not providing strict QoS performance bounds, the proposed scheme aims at enhancing the perceived quality of experience by combining opportunistic scheduling and packet aggregation and by implementing a DiffServ-like architecture in order to provide traffic prioritization.","2155-5486;2155-5494","978-1-4244-3938","10.1109/SAHCNW.2009.5172945","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5172945","","Wireless mesh networks;Telecommunication traffic;Computer architecture;Ethernet networks;Software testing;Open source hardware;Open source software;Web and internet services;Application software;Design engineering","fault tolerance;Internet;multimedia communication;scheduling;telecommunication computing;telecommunication network management;telecommunication traffic;wireless LAN","heterogeneous multimedia application;WiFi-based wireless mesh network;software toolkit;fault management;hardware-open-source software component;next-generation Internet service;opportunistic scheduling;packet aggregation;WING platform","","5","12","","","","","","IEEE","IEEE Conferences"
"Evolutionary multiobjective industrial design: the case of a racing car tire-suspension system","A. Benedetti; M. Farina; M. Gobbi","Dept. of Mech. Eng., Politecnico di Milano, Milan, Italy; NA; NA","IEEE Transactions on Evolutionary Computation","","2006","10","3","230","244","When dealing with multiobjective optimization (MO) of the tire-suspension system of a racing car, a large number of design variables and a large number of objectives have to be taken into account. Two different models have been used, both validated on data coming from an instrumented car, a differential equation-based physical model, and a neural network purely numerical model. Up to 23 objective functions have been defined, at least 14 of which are in strict conflict of each other. The equivalent scalar function based and the objective-as-constraint formulations are intentionally avoided due to their well-known limitations. A fuzzy definition of optima, being a generalization of Pareto optimality, is applied to the problem. The result of such an approach is that subsets of Pareto optimal solutions (on such a problem, a big portion of the entire search space) can be properly selected as a consequence of input from the designer. The obtained optimal solutions are compared with the reference vehicle and with the optima previously obtained with design of experiment techniques and different MO optimization strategies. The proposed strategy improves both the reference (actual) car and previously obtained optima (scalar preference function) in the majority of objectives with technically significant improvements. Moreover, the strategy offers an univoque criterion for the choice among tradeoff solutions in the 14-dimensional objective space. The problem is used as a test of a proposed optimal design strategy for industrial problems, integrating differential equation and neural networks modeling, design of experiments, MO, and fuzzy optimal-based decision making. Such a linked approach gives also a unified view of where to concentrate the computational effort.","1089-778X;1089-778X;1941-0026","","10.1109/TEVC.2005.860763","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1637685","Evolutionary algorithms;multiobjective optimization (MO);tire-suspension system design","Computer aided software engineering;Design optimization;Neural networks;Differential equations;Numerical models;Testing;Decision making;Prototypes;Power system modeling;Instruments","design of experiments;automobile industry;suspensions (mechanical components);design engineering;differential equations;neural nets;fuzzy set theory;Pareto optimisation;decision making","evolutionary multiobjective industrial design;racing car tire-suspension system multiobjective optimization;instrumented car;differential equation-based physical model;neural network purely numerical model;Pareto optimal solutions;design of experiment techniques;scalar preference function;univoque criterion;fuzzy optimal-based decision making","","13","51","","","","","","IEEE","IEEE Journals & Magazines"
"Improving multi-objective clustering through support vector machine: Application to gene expression data","A. Mukhopadhyay; U. Maulik; S. Bandyopadhyay","Dept of Computer Science & Engg, University of Kalyani, 741235, W.B., India; Dept of Computer Science & Engg, Jadavpur University, Kolkata-700032, W.B., India; Machine Intelligence Unit, Indian Statistical Institute, Kolkata-700108, W.B., India","TENCON 2008 - 2008 IEEE Region 10 Conference","","2008","","","1","6","Microarray technology facilitates the monitoring of the expression profile of a large number of genes across different experimental conditions simultaneously. This article proposes a novel approach that combines a recently proposed multiobjective fuzzy clustering scheme with support vector machine (SVM), to yield improved solutions. The multiobjective technique is first used to produce a set of non-dominated solutions. The non-dominated set is then used to find some high-confidence points using a fuzzy voting technique. The SVM classifier is trained by this high-confidence points. Finally the remaining points are classified using the trained classifier. Results demonstrating the effectiveness of the proposed technique are provided for three real life gene expression data sets. Moreover statistical significance test has been conducted to establish the significant superiority of the proposed technique.","2159-3442;2159-3450","978-1-4244-2408-5978-1-4244-2409","10.1109/TENCON.2008.4766630","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4766630","Fuzzy clustering;Support Vector Machine;multiobjective optimization;Pareto-optimality;cluster validity measures;microarray gene expression data","Support vector machines;Gene expression;Support vector machine classification;Computer science;Testing;Clustering algorithms;Application software;Machine intelligence;Computerized monitoring;Condition monitoring","bioinformatics;fuzzy set theory;genetics;learning (artificial intelligence);optimisation;pattern classification;pattern clustering;statistical testing;support vector machines","multiobjective fuzzy clustering;support vector machine;gene expression data;microarray technology;nondominated set;fuzzy voting technique;SVM classifier;machine learning;statistical significance test","","","31","","","","","","IEEE","IEEE Conferences"
"Optimistic Parallel Discrete Event Simulation Based on Multi-core Platform and its Performance Analysis","N. Su; H. Hou; F. Yang; Q. Li; W. Wang","NA; NA; NA; NA; NA","2009 International Conference on Complex, Intelligent and Software Intensive Systems","","2009","","","675","680","The development of computer processor has stepped into the era of multi-core, providing a good chance to spread the parallel discrete event simulation. The parallel programming model and synchronization problem during the parallelization of discrete event simulation on multi-core platform were discussed. A parallel discrete event simulator based on multi-core platform was designed and implemented using the optimistic synchronization algorithm. On an HP multi-core server with up to 8 cores, both the overheads of the parallel simulator and the effects of event granularity, process number, lookahead on the simulation performance were tested using the Phold model. The experiment results show that the optimistic parallel discrete event simulation based on multi-core platform could achieve good speedup for simulation applications with coarse-grained events.","","978-1-4244-3569-2978-0-7695-3575","10.1109/CISIS.2009.59","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5066860","","Discrete event simulation;Performance analysis;Object oriented modeling;Computational modeling;Yarn;Multicore processing;Concurrent computing;Design optimization;Processor scheduling;Computer architecture","discrete event simulation;multiprocessing systems;parallel programming","optimistic parallel discrete event simulation;computer processor;parallel programming model;optimistic synchronization algorithm;HP multicore server;Phold model","","2","15","","","","","","IEEE","IEEE Conferences"
"Simulation-based health and contingency management","M. J. Roemer; Liang Tang; G. Kacprzynski; G. Vachtsevanos; Jianhua Ge","Impact Technol., Rochester, NY, USA; Impact Technol., Rochester, NY, USA; Impact Technol., Rochester, NY, USA; NA; NA","2006 IEEE Aerospace Conference","","2006","","","11 pp.","","This paper presents a concept and associated software test bench for simulation-based health and contingency management that can be applied to integrated vehicle health management architectures. The simulation-based design test bench was developed in the Matlab/Simulinktrade environment and can be applied to various platforms for developing, testing and verifying automated prognostic and health management technologies and fault accommodation strategies. Specifically, this paper introduces the required software components and integrated health management and adaptive control architecture for performing these tasks. The Simulinktrade modules provide for ""plug 'n play"" integration of vehicle models, prognostic and health management (PHM) algorithms, and adaptive control logic into a simulation-based design environment. By integrating dynamic, model-based health management, intelligent control, and reasoner strategies, the simulation test bench allows designers to examine optimal fault accommodation techniques that increases availability, improve safety, and optimize maintenance resource planning for complex vehicle systems. The described software will enable the end users to initially configure the system and monitor the effectiveness of the vehicle simulation under various fault scenarios. It also enables users to rapidly design and simulate the health and contingency management components for integrated system health assessments","1095-323X","0-7803-9545","10.1109/AERO.2006.1656084","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1656084","","Contingency management;Prognostics and health management;Computer architecture;Automatic testing;Adaptive control;Mathematical model;Vehicle safety;Software testing;Software performance;Plugs","adaptive control;control engineering computing;mathematics computing;military computing;military vehicles;preventive maintenance","contingency management;software test bench;integrated vehicle health management architectures;Matlab;Simulink;integrated health management;adaptive control architecture;intelligent control","","3","12","","","","","","IEEE","IEEE Conferences"
"Research of the Image Segmentation Based on Ant Colony Algorithm","ZheYan; H. Gu","NA; NA","2009 10th ACIS International Conference on Software Engineering, Artificial Intelligences, Networking and Parallel/Distributed Computing","","2009","","","106","109","Ant colony algorithm, based on behavior of real ants, is a natural approach to establish from their nest to food source. An ant moves randomly and detects a previously laid pheromone on a path in order to find the shortest way between their nest and the food source. Ant system algorithm is an important methodology to apply on non-linear optimal problems recently. Each ant chooses the next position to visit in accordance with the visibility of the position and the pheromone intensity. In this paper, an ant colony algorithm for image segmentation is proposed. We discuss the ideas behind ant algorithms and describe how they have been implemented with regards to the image segmentation problem. The proposed approach has been tested on different images. The experimental results demonstrate the effectiveness and practicability of the proposed algorithm.","","978-0-7695-3642","10.1109/SNPD.2009.58","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5286684","image segmentation;ant colony;iterations","Image segmentation;Cities and towns;Insects;Artificial intelligence;Intelligent networks;Software algorithms;Image analysis;Software engineering;Distributed computing;Geophysics","image segmentation;optimisation","ant colony algorithm;image segmentation","","","8","","","","","","IEEE","IEEE Conferences"
"Artificial Neural Network Based Software Sensor for Yeast Biomass Concentration during Industrial Production","B. Li; L. Li","Light Industry and Chem. Eng. Institute, South China Univ. of Tech., Guangzhou, China, 510640. lcbingli@scut.edu.cn; Light Industry and Chem. Eng. Institute, South China Univ. of Tech., Guangzhou, China, 510640. felinli@scut.edu.cn","2006 International Conference on Computational Intelligence and Security","","2006","2","","955","958","The artificial neural network is a potential 'sensor' in the complex bioprocess. The recurrent neural network (RNN) was employed as the software sensor to measure the biomass concentration during the baker's yeast industrial production, owing to its good ability in dealing with non-linear and time-varying process. Based on the data sets provided by the plant, input variables were selected as air flow rate (G), ethanol concentration (Eth), volume of the contents in the reactor (Vol), temperature (T), pH and their time-delay values as well as the predicted values of yeast biomass concentration at delayed time. The topology of the RNN was optimized to be 11-16-1. The RNN showed good generalization ability for the testing samples. The robustness of the RNN was evaluated by adding deliberately inflicted noises to the G and Eth. The RNN showed higher robustness to the noise from Eth than that from G","","1-4244-0604-81-4244-0605","10.1109/ICCIAS.2006.295402","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4076098","","Artificial neural networks;Biosensors;Fungi;Biomass;Computer industry;Production;Recurrent neural networks;Noise robustness;Software measurement;Input variables","biosensors;chemical variables measurement;fermentation;production engineering computing;recurrent neural nets","artificial neural network;software sensor;yeast biomass concentration;bioprocess;recurrent neural network;yeast industrial production;nonlinear process;time-varying process;air flow rate;ethanol concentration;robustness","","1","10","","","","","","IEEE","IEEE Conferences"
"On-pendant robotic assembly parameter optimization","G. Zhang; A. Bell; Hui Zhang; Jianmin He; Jianjun Wang; C. Martinez","Corporate Research Center, ABB Inc., Windsor, CT 06095, USA; Flexible Automation, ABB Inc., Auburn Hills, MI 48326, USA; Corporate Research Center, ABB Inc., Windsor, CT 06095, USA; Corporate Research Center, ABB Inc., Windsor, CT 06095, USA; Corporate Research Center, ABB Inc., Windsor, CT 06095, USA; Corporate Research Center, ABB Inc., Windsor, CT 06095, USA","2008 7th World Congress on Intelligent Control and Automation","","2008","","","547","552","This paper presents on-pendant assembly parameter optimization with an industrial robot with force control. ABB industrial robot and its latest controller IRC5 is used as the host platform of the development; design of experiment (DOE) method is used as an optimization tool; and automatic transmission torque converter assembly is used as a test case. All tasks such as data manipulation, data analysis, result presentation and user interface, except robot motion execution, were performed on a graphical teach pendent. Teach pedant GUI building software ScreenMaker is used in the development. This on-pendant parameter optimization tool greatly reduced the complexity of assembly parameter optimization process and the workload of robot userpsilas.","","978-1-4244-2113-8978-1-4244-2114","10.1109/WCICA.2008.4592982","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4592982","Industrial robot;force control;robotic assembly;parameter optimization","Assembly;Robots;Force control;Force;Optimization;Service robots;Robotic assembly","computational complexity;control engineering computing;design of experiments;force control;graphical user interfaces;robotic assembly","on-pendant robotic assembly parameter optimization;force control;ABB industrial robot;IRC5 controller;design of experiment method;automatic transmission torque converter assembly;teach pedant GUI building software;ScreenMaker","","5","7","","","","","","IEEE","IEEE Conferences"
"Proactive monitoring of security policy accomplishment in computer networks","I. Kotenko; V. Bogdanov","St. Petersburg Institute for Informatics and Automation, 39, 14th Liniya, St. Petersburg, Russia; St. Petersburg Institute for Informatics and Automation, 39, 14th Liniya, St. Petersburg, Russia","2009 IEEE International Workshop on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications","","2009","","","364","369","One of topical tasks of policy-based security management is checking that the security policy stated in organization corresponds to its implementation in the computer network. The paper considers an approach to proactive monitoring of security policy performance and security mechanisms functioning. This approach is based on different strategies of automatic imitation of possible users' actions in the computer network, including exhaustive search, express-analysis and generating the optimized test sequences. It is applicable to different security policies. The paper describes stages, generalized algorithms and main peculiarities of the suggested approach and formal methods used to fulfill the test sequence optimization. We consider the generalized architecture of the proactive monitoring system ¿proactive security scanner¿ (PSC) developed and its implementation.","","978-1-4244-4901-9978-1-4244-4882","10.1109/IDAACS.2009.5342961","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5342961","Homeland Security;Security and Reliability;Security policy monitoring;Test sequence optimization","Computerized monitoring;Computer security;Computer networks;National security;Computer network management;Testing;Computer network reliability;Application software;Data security;Conferences","computer networks;security of data","proactive monitoring;security policy accomplishment;computer networks;policy-based security management;security mechanisms functioning;test sequence optimization;proactive security scanner","","1","25","","","","","","IEEE","IEEE Conferences"
"Practical Issues in Sequential Equivalence Checking through Alignability: Handling Don't Cares and Generating Debug Traces","I. Moon; P. Bjesse; C. Pixley","Advanced Technology Group, Synopsys, Inc., mooni@synopsys.com; Advanced Technology Group, Synopsys, Inc., bjesse@synopsys.com; Advanced Technology Group, Synopsys, Inc., pixley@synopsys.com","2006 IEEE International High Level Design Validation and Test Workshop","","2006","","","170","175","Reset states are often not known for a given design until late in the design cycle. There is therefore a need for sequential equivalence checking algorithms that work in the absence of this information. One popular choice for such a notion is alignability, which has the allure of not necessarily needing a symbolic traversal of every state in the system. However, to use alignability in practice, there are several obstacles that needs to be overcome. First of all, the standard alignability theory does not take into account that the golden design often may specify a range of acceptable implementations by means of designer annotated don't care states and don't care logic. Second, when two designs are unalignable, the fact that there are no specified initial state makes it hard to generate a meaningful counter-example to aid the designer in debugging the designs. We address these issues by extending the standard alignability theory so that it handles don't care information in the absence of reset states, and by devising a heuristic for finding a meaningful initial state for the debug traces. Our experimental results show that our theory extensions are necessary to be able to deal with a large portion of the industrial designs that we have applied our alignability checking tool to. We also show how our debug trace heuristic has allowed us to find a real optimization tool bug","1552-6674","1-4244-0679-X1-4244-0680","10.1109/HLDVT.2006.319985","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4110084","","Design optimization;Logic design;Hardware;Sequential analysis;Conferences;Moon;Debugging;Boolean functions;Impedance;Inspection","circuit optimisation;electronic design automation;logic CAD;program debugging;software tools","sequential equivalence checking;don't cares handling;debug traces generation;alignability theory;design debugging;industrial designs;alignability checking tool","","2","14","","","","","","IEEE","IEEE Conferences"
"Swarm's flight: Accelerating the particles using C-CUDA","L. d. P. Veronese; R. A. Krohling","Departmento de Informática, PPGI, Universidade Federal do Espírito Santo, CEP 29060-270, Vitória, Brazil; Departmento de Informática, PPGI, Universidade Federal do Espírito Santo, CEP 29060-270, Vitória, Brazil","2009 IEEE Congress on Evolutionary Computation","","2009","","","3264","3270","With the development of Graphics Processing Units (GPU) and the Compute Unified Device Architecture (CUDA) platform, several areas of knowledge are being benefited with the reduction of the computing time. Our goal is to show how optimization algorithms inspired by Swarm Intelligence can take profit from this technology. In this paper, we provide an implementation of the Particle Swarm Optimization (PSO) algorithm in C-CUDA. The algorithm was tested on a suite of well-known benchmark optimization problems and the computing time has been compared with the same algorithm implemented in C and Matlab. Results demonstrate that the computing time can significantly be reduced using C-CUDA. As far as we know, this is the first implementation of PSO in C-CUDA.","1089-778X;1941-0026","978-1-4244-2958-5978-1-4244-2959","10.1109/CEC.2009.4983358","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4983358","Particle Swarm Optimization;Graphics Processing Unit (GPU);Compute Unified Device Architecture(CUDA) C-CUDA platform;Computational Performance Evaluation","Acceleration;Particle swarm optimization;Graphics;Computer architecture;Yarn;Probability distribution;Distributed computing;Random number generation;Central Processing Unit;Application software","computer architecture;coprocessors;mathematics computing;particle swarm optimisation","swarm flight;C-CUDA;graphics processing unit;compute unified device architecture;CUDA platform;optimization algorithm;swarm intelligence;particle swarm optimization","","19","15","","","","","","IEEE","IEEE Conferences"
"An Improved PSO Algorithm and its Application to Grid Scheduling Problem","Y. Bu; W. Zhou; J. Yu","NA; NA; NA","2008 International Symposium on Computer Science and Computational Technology","","2008","1","","352","355","With the advent of the grid, task scheduling in heterogeneous environments becomes more and more important. The model of grid scheduling is analyzed in this paper. The optimal objective is to minimize the total completing time. This paper presents an improved particle swarm optimization (PSO) algorithm with discrete coding rule for grid scheduling problem. The improved PSO algorithm can keep all the advantages of the standard PSO, such as implementation simplicity, low computational burden, and few control parameters, etc. A set of experiments show that the algorithm is stable and presents low variability. The preliminary results obtained in this research are auspicious. We also tested the improved PSO algorithm against the MaxMin heuristic and found that improved PSO outperforms MaxMin by the total makespan and other performance.","","978-0-7695-3498-5978-1-4244-3746","10.1109/ISCSCT.2008.93","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4731443","PSO algorithm;grid;task scheduling;makespan","Scheduling algorithm;Processor scheduling;Grid computing;Distributed computing;High performance computing;Throughput;Particle swarm optimization;Computer science;Application software;Biological systems","grid computing;particle swarm optimisation;scheduling;task analysis","PSO algorithm;grid scheduling problem;task scheduling;particle swarm optimization algorithm;discrete coding rule;MaxMin heuristic;grid computing","","6","8","","","","","","IEEE","IEEE Conferences"
"Wireless Multicast Support for the NS-2 Emulation Environment","S. Penz","NA","2007 15th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems","","2007","","","267","273","The development of applications and protocols for future wireless communication systems requires powerful tools for testing and optimization in mobile ad-hoc networks. This especially holds true for applications that are based on relatively unreliable wireless multicast communication. In this paper we propose an extension to the ns-2 network simulator that allows for testing and optimization of real-world applications in a flexibly configurable emulation environment. This testbed supports wireless unicast and multicast routing mechanisms in parallel and thus gives real-world implementations a comprehensive and transparent access to a network that behaves like a mobile ad-hoc environment.","1526-7539;2375-0227","978-1-4244-1854-1978-1-4244-1853","10.1109/MASCOTS.2007.69","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4674426","","Emulation;Application software;Software testing;Ad hoc networks;System testing;Routing;Multicast protocols;Wireless communication;Bandwidth;Computer science","ad hoc networks;mobile radio;multicast protocols;optimisation;routing protocols","wireless multicast communication;protocols;mobile ad-hoc networks;ns-2 network simulator;optimization;real-world applications;routing mechanisms","","2","18","","","","","","IEEE","IEEE Conferences"
"New Modeling and Analysis of Three-Stage Electro-Hydraulic Servo Valve","X. Liu; Q. Huang; D. Cong; J. Han; L. Xiao","NA; NA; NA; NA; NA","2008 International Workshop on Modelling, Simulation and Optimization","","2008","","","146","150","To develop higher performance three-stage electro-hydraulic servo valve, a new modeling software, AMEsim, is used to develop its non-linear model according to the construction and principles presented in this paper. The results of simulation and experimental test about an existing valve are consistent, which proves the model is right. Based on the analysis of the model, the performances of three-stage electro-hydraulic servo valve are mainly affected by pilot valve performance, spool and sleeve performance, position transducer performance and servo controller performance. The methods of modeling and analysis possess universal significance to other similarity system.","","978-0-7695-3484","10.1109/WMSO.2008.83","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4756976","","Servomechanisms;Valves;Transducers;Testing;Performance analysis;Sliding mode control;Power system modeling;Mathematical model;Libraries;Analytical models","control system CAD;digital simulation;electrohydraulic control equipment;servomechanisms;valves","three-stage electrohydraulic servo valve;modeling software;AMEsim;nonlinear model;pilot valve performance;spool performance;sleeve performance;position transducer performance;servo controller performance","","4","7","","","","","","IEEE","IEEE Conferences"
"An Improved Quasi-physics and Quasi-human Algorithm for Solving the Job Shop Scheduling Problem","S. Zhang; J. Wu; A. Yin","NA; NA; NA","2009 International Joint Conference on Computational Sciences and Optimization","","2009","1","","132","135","In this paper, an improved quasi-physics and quasi-human algorithm, called IQ&Q, is proposed to solve the job shop scheduling problem. The algorithm adopts a physical model to describe the job shop scheduling problem. The strategy of quasi-physics and quasi-human and random strategy is introduced to search the solution space and to determine the global minimum solution. This algorithm has been tested on many common problem benchmarks with various sizes. Computational experiments show that this algorithm is better BQ&Q and HA.","","978-0-7695-3605","10.1109/CSO.2009.410","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5193659","","Scheduling algorithm;Job shop scheduling;Pistons;Finance;Containers;Resumes;Educational technology;Benchmark testing;Software algorithms;Educational institutions","job shop scheduling","improved quasiphysics and quasihuman algorithm;job shop scheduling problem;IQ&Q;global minimum solution;BQ&Q;quasi-physics and quasi-human algorithm","","1","4","","","","","","IEEE","IEEE Conferences"
"A Novel Method for XML Scheme Matching","Z. Jiuzhen; Z. Shidong; Y. Zhongmin","NA; NA; NA","2009 International Forum on Information Technology and Applications","","2009","2","","332","335","XML Schema is becoming a critical technology for e-business applications and the emergence of web services. With its widespread adoption and its web accessibility, XML Schema matching is becoming imperative. This paper presents an approach to elements matching between two XML Schemas using similarity measure and constraint optimization. In our method, we first transform the schema matching problem into a tree matching problem by transforming schemas to be matched into labeled (including node label and edge label) trees. The similarity measure considers element categories and properties. In order to get an optimal matching, we compute the structural similarity value based on the neighbors of each element (ancestor, children, and sibling). We test our method experimentally on three groups of XML Schemas. The experiments show that the proposed method has a high degree of accuracy.","","978-0-7695-3600","10.1109/IFITA.2009.452","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5231190","schema matching;XML Schema;similarity measure","XML;Application software;Testing;Information technology;Optimal matching;Computer science;Web services;Constraint optimization;Optimization methods;Information retrieval","pattern matching;tree data structures;XML","XML scheme matching;XML schema;e-business application;Web services;elements matching;similarity measure;constraint optimization;tree matching problem;node label;edge label;optimal matching;structural similarity value","","","8","","","","","","IEEE","IEEE Conferences"
"Application of an Improved Particle Swarm Optimization for Fault Diagnosis","C. Wang; S. Xia","NA; NA","2009 Second International Workshop on Knowledge Discovery and Data Mining","","2009","","","527","530","In this paper, the feasibility of using probabilistic causal-effect model is studied and we apply it in particle swarm optimization algorithm (PSO) to classify the faults of mine hoist. In order to enhance the PSO performance, we propose the probability function to nonlinearly map the data into a feature space in probabilistic causal-effect model, and with it, fault diagnosis is simplified into optimization problem from the original complex feature set. The proposed approach is applied to fault diagnosis, and our implementation has the advantages of being general, robust, and scalable. The raw datasets obtained from mine hoist system are preprocessed and used to generate networks fault diagnosis for the system. We studied the performance of the improved PSO algorithm and generated a Probabilistic Causal-effect network that can detect faults in the test data successfully. It can get &gt;90% minimal diagnosis with cardinal number of fault symptom sets greater than 25.","","978-0-7695-3543","10.1109/WKDD.2009.15","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4771990","probabilistic causal-effect model;fault diagnosis","Particle swarm optimization;Fault diagnosis;Diagnostic expert systems;Artificial neural networks;Data mining;Application software;Computer science;Computer architecture;Support vector machines;Information processing","data mining;fault diagnosis;particle swarm optimisation;probability","particle swarm optimization;fault diagnosis;probabilistic causal-effect model;feature space;mine hoist system","","4","12","","","","","","IEEE","IEEE Conferences"
"Evolutionary-Reduced Ordered Binary Decision Diagram","H. Moeinzadeh; M. Mohammadi; H. Pazhoumand-dar; A. Mehrbakhsh; N. Kheibar; N. Mozayani","NA; NA; NA; NA; NA; NA","2009 Third Asia International Conference on Modelling & Simulation","","2009","","","142","145","Reduced ordered binary decision diagram (ROBDD) is a memory-efficient data structure which is used in many applications such as synthesis, digital system, verification, testing and VLSI-CAD. The size of an ROBDD for a function can be increased exponentially by the number of independent variables of the function that is called ldquomemory explosion problemrdquo. The choice of the variable ordering largely influences the size of the OBDD especially for large input variables. Finding the optimal variable ordering is an NP-complete problem, hence, in this paper, two evolutionary methods (GA and PSO) are used to find optimal order of input variable in binary decision diagram. Some benchmarks form LGSynth91 are used to evaluate our suggestion methods. Obtained results show that evolutionary methods have the ability to find optimal order of input variable and reduce the size of ROBDD considerably.","2376-1164;2376-1172","978-1-4244-4154-9978-0-7695-3648","10.1109/AMS.2009.130","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5071973","Genetic Algorithm;Particle Swarm Optimization;Reduce Order Binary Decision Diagram","Data structures;Boolean functions;Data engineering;Binary decision diagrams;Input variables;Explosions;Asia;Computational modeling;Computer simulation;Application software","binary decision diagrams;Boolean functions;computational complexity;data structures;genetic algorithms;particle swarm optimisation","evolutionary-reduced ordered binary decision diagram;memory-efficient data structure;memory explosion problem;NP-complete problem;Boolean function;optimal variable ordering;genetic algorithm;particle swarm optimisation","","","18","","","","","","IEEE","IEEE Conferences"
"Research on soft measurement algorithm of torque measurement in transmission system","X. Lu; X. Yan; H. Jin","College of Mechanical Engineering, Yanshan University Qinhuangdao, 066004, P.R. China; College of Mechanical Engineering, Yanshan University Qinhuangdao, 066004, P.R. China; College of Mechanical Engineering, Yanshan University Qinhuangdao, 066004, P.R. China","2009 9th International Conference on Electronic Measurement & Instruments","","2009","","","3-289","3-293","A dynamic torque testing method based on computer system is put forward in this article, which adopts non-contact measurement and associates with virtual instrument technology to do high speed sampling of two-way sine signals to acquire two sets of discrete data. Cross-correlation principle is applied to treat these data to solve phase spectrum of cross correlation function. Finally torsion angle can be gotten to realize measurement of torque by acquisition of phase difference of measurand signals. This measuring method emphasize software algorithm, adopt information processing technique and simplify complex hardware circuit. Measurement result is more accurate by application of software algorithm to eliminate influence of temperature drift, acoustic noise and interfering signal. The key problem of using correlation method to measure phase lies with complete alternation sampling. When signal frequency changes, value of sample frequency should be rectified timely. For this reason, a late-model generalized predictive algorithm is inducted, which is generalized predictive adapting optimization algorithm with time-varying forgetting factor recursive least squares - recursive extended least squares (RLS - RELS) performance criteria weighted. By on-line recursion to estimate parameters, the algorithm speeds up the computing speed of system, avoid matrix calculation, increase efficiency and accuracy and enhance robustness. Frequency tracing based on this algorithm can realize whole period sampling of effective signal.","","978-1-4244-3863-1978-1-4244-3864","10.1109/ICEMI.2009.5274305","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5274305","Generalized prediction;Non-contact torque measurement;Frequency tracing;Virtual instrument","Torque measurement;Acoustic measurements;Phase measurement;Frequency;Sampling methods;Software measurement;Software algorithms;Prediction algorithms;Least squares methods;System testing","correlation methods;least squares approximations;measurement systems;optimisation;recursive estimation;signal sampling;torque measurement;virtual instrumentation","soft measurement algorithm;torque measurement;transmission system;dynamic torque testing method;noncontact measurement;virtual instrument technology;two-way sine signal sampling;high speed sampling;cross-correlation principle;information processing technique;hardware circuit;temperature drift;acoustic noise;phase measurement;generalized predictive adapting optimization algorithm;recursive extended least squares algorithm;frequency tracing","","","9","","","","","","IEEE","IEEE Conferences"
"Indexing Noncrashing Failures: A Dynamic Program Slicing-Based Approach","C. Liu; X. Zhang; J. Han; Y. Zhang; B. K. Bhargava","Department of Computer Science, University of Illinois-UC, Urbana, IL 61801 USA, chaoliu@cs.uiuc.edu; Department of Computer Science, Purdue University, West Lafayette, IN 47907 USA, xyzhang@cs.purdue.edu; Dept of Computer Science, University of Illinois-UC, Urbana, IL 61801 USA, hanj@cs.uiuc.edu; Dept of Computer Science, Purdue University, West Lafayette, IN 47907 USA, zhangyu@cs.purdue.edu; Dept of Computer Science, Purdue University, West Lafayette, IN 47907 USA, bb@cs.purdue.edu","2007 IEEE International Conference on Software Maintenance","","2007","","","455","464","Recent software systems usually feature an automated failure reporting component, with which a huge number of failures are collected from software end-users. With a proper support of failure indexing, which identifies failures due to the same fault, the collected failure data can help developers prioritize failure diagnosis, among other utilities of the failure data. Since crashing failures can be effectively indexed by program crashing venues, current practice has seen great success in prioritizing crashing failures. A recent study of bug characteristics indicates that as excellent memory checking tools are widely adopted, semantic bugs and the resulting noncrashing failures have become dominant. Unfortunately, the problem of how to index non-crashing failures has not been seriously studied before. In previous study, two techniques have been proposed to index noncrashing failures, and they are T-Proximity and R-Proximity. However, as T-Proximity indexes failures by the profile of the entire execution, it is generally not effective because most information in the profile is fault-irrelevant. On the other hand, although R-Proximity is more effective than T-Proximity, it relies on a sufficient number of correct executions that may not be available in practice. In this paper, we propose a dynamic slicing-based approach, which does not require any correct executions, and is comparably effective as R-Proximity. A detailed case study with gzip is reported, which clearly strates the advantages of the proposed approach.","1063-6773","978-1-4244-1255-6978-1-4244-1256","10.1109/ICSM.2007.4362658","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4362658","","Indexing;Fault diagnosis;Computer science;Computer crashes;Software testing;Chaos;Computer bugs;Software systems;Java;Failure analysis","program debugging;program slicing","dynamic program slicing-based approach;software system;automated failure reporting component;failure diagnosis;bug characteristic;memory checking tool;program noncrashing failure indexing;R-Proximity index failure;T-Proximity index failure","","2","23","","","","","","IEEE","IEEE Conferences"
"Coupling simulation with HeuristicLab to solve facility layout problems","A. Beham; M. Kofler; S. Wagner; M. Affenzeller","Heuristic and Evolutionary Algorithms Laboratory, Upper Austria University of Applied Sciences, School of Informatics/Communications/Media, Hagenberg, 4232, Austria; Heuristic and Evolutionary Algorithms Laboratory, Upper Austria University of Applied Sciences, School of Informatics/Communications/Media, Hagenberg, 4232, Austria; Heuristic and Evolutionary Algorithms Laboratory, Upper Austria University of Applied Sciences, School of Informatics/Communications/Media, Hagenberg, 4232, Austria; Heuristic and Evolutionary Algorithms Laboratory, Upper Austria University of Applied Sciences, School of Informatics/Communications/Media, Hagenberg, 4232, Austria","Proceedings of the 2009 Winter Simulation Conference (WSC)","","2009","","","2205","2217","In this paper we describe the optimization of a facility layout scenario, which involves coupling simulation with the optimization environment HeuristicLab. For this purpose we show a problem formulation that acts as an interface between these two domains of problem modeling and optimization, and discuss optimization methodologies and their results for a number of artificial test problems as well as more complex real-world problems. HeuristicLab was designed with both practitioners and algorithm developers in mind. Practitioners benefit from a graphical user interface that facilitates so-called interactive algorithm engineering, where algorithms can be adjusted without actually writing code. Algorithm developers are aided in the development process by the plug-in based, easily extensible architecture and integrated parallelization functionality.","0891-7736;1558-4305","978-1-4244-5771-7978-1-4244-5770-0978-1-4244-5772","10.1109/WSC.2009.5429238","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5429238","","Medical simulation;Evolutionary computation;Algorithm design and analysis;Optimization methods;Testing;Graphical user interfaces;Writing;Software algorithms;Genetic algorithms;Simulated annealing","digital simulation;facilities layout;graphical user interfaces;interactive systems;optimisation","coupling simulation;facility layout problems;optimization environment;HeuristicLab;optimization methodology;graphical user interface;interactive algorithm engineering;integrated parallelization functionality","","3","32","","","","","","IEEE","IEEE Conferences"
"Application of a 3D computer simulation tool as a decision making tool for optimizing transformer protection","R. Brady; G. Perigaud; S. Muller; M. Petrovan-Boiarciuc; B. Landis","Transformer Protector Corporation, TPC, 1880 Treble Dr., Humble, TX 77338 USA; Transformer Protector Corporation, TPC, 1880 Treble Dr., Humble, TX 77338 USA; Transformer Protector Corporation, TPC, 1880 Treble Dr., Humble, TX 77338 USA; Transformer Protector Corporation, TPC, 1880 Treble Dr., Humble, TX 77338 USA; Transformer Protector Corporation, TPC, 1880 Treble Dr., Humble, TX 77338 USA","2009 IEEE Power & Energy Society General Meeting","","2009","","","1","8","Oil-filled transformer explosions are caused by low impedance faults that result in arcing in transformer tanks once the oil loses its dielectric properties These arcs generate dynamic pressure waves which interact with the structure walls as they propagate throughout the transformer, raising static pressure and ultimately resulting in tank ruptures and possible fire. A simulation tool has been developed not only to study transformer explosions caused by arcing events, but also to develop and optimize an explosion prevention technique. The explosion prevention technique involves the rapid evacuation of oil within milliseconds of a dynamic pressure peak. The effects of position, size, and protection orientation were studied over a series of tests on a set of transformer geometries. From these tests, recommendations for optimizing the efficiency of the protection device are made.","1932-5517","978-1-4244-4241","10.1109/PES.2009.5275234","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5275234","Pressure effects;protection;transformer;optimization;simulation","Application software;Computer simulation;Decision making;Protection;Oil insulation;Explosions;Petroleum;Testing;Impedance;Dielectrics","arcs (electric);decision making;explosions;power engineering computing;power transformer insulation;power transformer protection;pressure;transformer oil","3D computer simulation tool;decision making tool;transformer protection optimization;oil filled transformer;low impedance faults;transformer tank arcing;static pressure;tank ruptures","","1","7","","","","","","IEEE","IEEE Conferences"
"Fast block based connected components labeling","C. Grana; D. Borghesani; R. Cucchiara","Dipartimento di Ingegneria dell'Informazione, Università degli Studi di Modena e Reggio Emilia, Italy; Dipartimento di Ingegneria dell'Informazione, Università degli Studi di Modena e Reggio Emilia, Italy; Dipartimento di Ingegneria dell'Informazione, Università degli Studi di Modena e Reggio Emilia, Italy","2009 16th IEEE International Conference on Image Processing (ICIP)","","2009","","","4061","4064","In this paper we present a new optimization technique for the neighborhood computation in connected component labeling focused on images stored in raster scan order. This new technique is based on a 2 × 2 square block analysis of the image, and it exploits the fact that, when using 8-connection, the pixels of a 2 × 2 square are all connected to each other. This implies that they will share the same label at the end of the computation. To prove the effectiveness of our proposal, we show a comprehensive comparison of the most used and advanced connected components labeling techniques presented so far. The tests are conducted on high resolution images obtained from digitized historical manuscripts and a set of transformations is applied in order to show the algorithms behavior at different image resolutions and with a varying number of labels.","1522-4880;1522-4880;2381-8549","978-1-4244-5654-3978-1-4244-5653-6978-1-4244-5655","10.1109/ICIP.2009.5413731","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5413731","connected component labeling;comparison;union-find","Labeling;Image resolution;Computer vision;Proposals;Testing;Data structures;US Department of Transportation;Image analysis;Pixel;Application software","image resolution;optimisation","fast block based connected components labeling;optimization technique;neighborhood computation;raster scan order;square block analysis;connected components labeling techniques;high resolution images;digitized historical manuscripts;image resolutions","","12","11","","","","","","IEEE","IEEE Conferences"
"The implementation and optimization of AMR speech codec on DSP","Jie Yang; Sheng sheng Yu; Mian Zhao","Department of Computer Science, Huazhong University of Sci&Tech, Wuhan, 430074, China; Department of Computer Science, Huazhong University of Sci&Tech, Wuhan, 430074, China; Department of Computer Science, Huazhong University of Sci&Tech, Wuhan, 430074, China","2007 International Symposium on Intelligent Signal Processing and Communication Systems","","2007","","","365","367","The adaptive multi-rate (AMR) is the mandatory speech codec for GSM system, ranging from 12.2 kbps down to 4.75 kbps. To satisfy the requirement of need for both audio and video simultaneously, a kind of hardware and software of AMR speech encoder based on high-performance chips TMS320C64X DSP is proposed. Considering the limited resource of DSP chips, the optimization scheme is referred to later. The result shows that the CPU-load is reduced from 38 percent to 4 percent. And the time of processing each frame is also reduced greatly after optimizing. It is verified with all the test vectors provided by 3 GPP, and the stable operation on the real-time testing board was also confirmed. So, more resource is reserved for video coding on DSP chips. This is helpful for implementing both speech coder and video coder on one DSP chip.","","978-1-4244-1446-8978-1-4244-1447","10.1109/ISPACS.2007.4445899","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4445899","AMR;speech codec;DSP","Speech codecs;Digital signal processing;Digital signal processing chips;Speech coding;Testing;Linear predictive coding;Clocks;Speech processing;Signal processing;VLIW","cellular radio;digital signal processing chips;speech codecs","speech codec optimization;adaptive multirate speech codec;GSM system;AMR speech encoder;DSP chips;TMS320C64X DSP;CPU-load","","","6","","","","","","IEEE","IEEE Conferences"
"Enhancements in the Orphan Process for Wireless Personal Area Networks: Real Implementation Scenarios","A. Garcia-Sanchez; F. Garcia-Sanchez; J. Garcia-Haro","NA; NA; NA","2009 Fourth International Conference on Software Engineering Advances","","2009","","","371","377","Wireless personal area networks (WPAN) are wireless sensor networks (WSN) designed for allowing a low-cost, low-power and short-range wireless communication. The IEEE 802.15.4 is the WPAN standard which satisfies these features and facilitates diverse self-configuring operation processes for adapting to different environments and applications. One of these processes is responsible for resolving the loss of coverage of a device, namely an orphan device. It has associated a long procedure when the device has to return to the wireless network, incurring in a high delay and energy consumption. This behavior implies that the Orphan device does not perform its assigned tasks during this delay increasing simultaneously its power consumption. We have designed a new algorithm, called optimized orphan algorithm (OOA), which offers a significant better delay and energy consumption performance. The OOA algorithm is implemented on real devices and evaluated under different scenarios. In this paper, the results obtained are presented and discussed as well.","","978-1-4244-4779-4978-0-7695-3777","10.1109/ICSEA.2009.60","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5298208","Wireless Sensor Networks;Orphan Process;Performance Evaluation","Wireless personal area networks;Software engineering;Programming;Education;Collaborative software;Computational modeling;Information technology;Software testing;Computer simulation;Computer aided software engineering","personal area networks;wireless sensor networks","wireless personal area networks;wireless sensor networks;wireless communication;IEEE 802.15.4;optimized orphan algorithm","","2","10","","","","","","IEEE","IEEE Conferences"
"Energy and Execution Time Analysis of a Software-based Trusted Platform Module","N. Aaraj; A. Raghunathan; S. Ravi; N. K. Jha","Department of Electrical Engineering, Princeton University, Princeton, NJ 08544, naaraj@princeton.edu; NEC Laboratories America, Princeton, NJ 08540, anand@nec-labs.com; Texas Instruments R&D Center, Bangalore, India, srivaths.ravi@ti.com; Department of Electrical Engineering, Princeton University, Princeton, NJ 08544, jha@princeton.edu","2007 Design, Automation & Test in Europe Conference & Exhibition","","2007","","","1","6","Trusted platforms have been proposed as a promising approach to enhance the security of general-purpose computing systems. However, for many resource-constrained embedded systems, the size and cost overheads of a separate trusted platform module (TPM) chip are not acceptable. One alternative is to use a software-based TPM (SW-TPM), which implements TPM functions using software that executes in a protected execution domain on the embedded processor itself. However, since many embedded systems have limited processing capabilities and are battery-powered, it is also important to ensure that the computational and energy requirements for SW-TPMs are acceptable. In this work, an evaluation of the energy and execution time overheads for a SW-TPM implementation on a Sharp Zaurus PDA was performed. The execution time and energy required by each TPM command was characterized through actual measurements on the target platform. In addition, the overheads of using SW-TPM in the context of various end applications, including trusted boot of the Linux operating system (OS), secure file storage, secure VoIP client, and secure Web browser was also evaluated. Furthermore, it was observed that for most TPM commands, the overheads are primarily due to the use of 2048-bit RSA operations that are performed within SW-TPM. In order to alleviate SW-TPM overheads, the use of elliptic curve cryptography (ECC) as a replacement for the RSA algorithm specified in the trusted computing group (TCG) standards was evaluated. Experiments indicate that this optimization can significantly reduce SW-TPM overheads (an average of 6.51times execution time reduction and 6.75times energy consumption reduction for individual TPM commands, and an average of 10.25times execution time reduction and 10.75times energy consumption reduction for applications). This work demonstrates that ECC-based SW-TPMs are a viable approach to realizing the benefits of trusted computing in resource-constrained embedded systems","1530-1591;1558-1101","978-3-9810801-2","10.1109/DATE.2007.364446","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4211956","","Embedded system;Embedded computing;Elliptic curve cryptography;Energy consumption;Security;Costs;Embedded software;Protection;Personal digital assistants;Performance evaluation","cryptography;embedded systems;energy consumption;Internet telephony;Linux;notebook computers","energy analysis;execution time analysis;software-based trusted platform module;general-purpose computing system security;Sharp Zaurus PDA;Linux operating system;secure file storage;secure VoIP;secure Web browser;elliptic curve cryptography;trusted computing group;resource-constrained embedded systems","","14","25","","","","","","IEEE","IEEE Conferences"
"Improved Bolstering Error Estimation for Gene Ranking","K. N. T. Huynh; J. H. Phan; T. M. Vo; M. D. Wang","Wallace H. Coulter Department of Biomedical Engineering, Georgia Institute of Technology and Emory University, 313 Ferst Drive, Atlanta, GA 30332.; Wallace H. Coulter Department of Biomedical Engineering, Georgia Institute of Technology and Emory University, 313 Ferst Drive, Atlanta, GA 30332.; Wallace H. Coulter Department of Biomedical Engineering, Georgia Institute of Technology and Emory University, 313 Ferst Drive, Atlanta, GA 30332.; Wallace H. Coulter Department of Biomedical Engineering, Georgia Institute of Technology and Emory University, 313 Ferst Drive, Atlanta, GA 30332. 404-385-2954, maywang@bme.gatech.edu","2007 29th Annual International Conference of the IEEE Engineering in Medicine and Biology Society","","2007","","","4633","4636","Many methods have been proposed to identify differentially expressed genes in diseased tissues. The performance of the method is closely related to the evaluation metric. We examine several error estimation algorithms (i.e., cross validation, bootstrap, resubstitution, and resubstitution with bolstering) for three classifiers (i.e., support vector machine, Fisher's discriminant, and signed distance function). To control the classifier's data-overfitting problem, usually caused by small sample size for many real datasets, we generate synthetic datasets based on real data. This way, we can monitor sample size impact when evaluating the metrics. We find that resubstitution with bolstering has the best result, especially with respect to computational efficiency. However, classical bolstering tends to bias in high dimensions. Thus, we further investigate ways to reduce bolstering estimation bias without increasing computational intensity. Results of our investigation indicate that the estimator tends to become unbiased as the sample size increases. We also find that modified bolstering is the best among all metrics in terms of estimation accuracy and computational efficiency.","1094-687X;1558-4615","978-1-4244-0787-3978-1-4244-0788","10.1109/IEMBS.2007.4353372","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4353372","","Error analysis;Testing;Gene expression;Support vector machines;Support vector machine classification;Smoothing methods;Computational efficiency;Diseases;Robustness;Training data","biological tissues;error compensation;genetics;support vector machines","bolstering error estimation;gene ranking;diseased tissues;error estimation algorithms;support vector machine;Fisher's discriminant;signed distance function;data overfitting problem;estimation accuracy;computational efficiency","Animals;Computer Simulation;Gene Expression Profiling;Gene Expression Regulation;Humans;Oligonucleotide Array Sequence Analysis;Selection Bias;Sensitivity and Specificity;Software","1","10","","","","","","IEEE","IEEE Conferences"
"Array Bounds Check Elimination for Java Based on Sparse Representation","K. Yang; Z. Huang; M. Yang","NA; NA; NA","2009 Seventh ACIS International Conference on Software Engineering Research, Management and Applications","","2009","","","189","196","As a type-safe program language, Java requires bounds checks of array accesses. Whenever an array element is accessed, a cmp (compare) instruction is executed to check whether the index value is within the valid bounds. Array bounds checks may prevent many useful optimizations because of precise exception. We present a new ABCE (array bounds check elimination) algorithm to eliminate redundant checks based on sparse representation for a Java static compiler. In contrast to other approaches performing in JVMs, we adhere to the design principle of the static compiler to optimize scientific Java applications. The algorithm is a light-weight algorithm working on an intermediate representation in static single assignment form. It fully removes bounds checks if it can be proven that they never fail. Whenever possible, it moves bounds checks out of loops to reduce the total number of executed checks. If such a check fails, the executing program branches into the unmodified loop to preserve the exception semantics of Java. For the scientific SciMark 2.0 benchmark suite, this algorithm removes on average 76% of bounds check instructions. The evaluation shows a speedup near to the theoretical maximum for LU test case.","","978-0-7695-3903","10.1109/SERA.2009.11","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381765","Java;array bounds check elimination;optimization;performance","Java;Partial response channels;Runtime;Phased arrays;Software engineering;Conference management;Engineering management;Application software;Parallel processing;Design optimization","data flow analysis;Java;program compilers","sparse representation;type-safe program language;array bounds check elimination algorithm;redundant checks;Java static compiler;scientific Java applications;light-weight algorithm;static single assignment form;SciMark benchmark suite","","","13","","","","","","IEEE","IEEE Conferences"
"Magellan: A Search and Machine Learning-based Framework for Fast Multi-core Design Space Exploration and Optimization","S. Kang; R. Kumar","Coordinated Science Laboratory, 1308 West Main St, Urbana, IL 61801; Coordinated Science Laboratory, 1308 West Main St, Urbana, IL 61801","2008 Design, Automation and Test in Europe","","2008","","","1432","1437","In this paper, we treat multi-core processor design space exploration as an application-driven machine learning problem. We develop two machine learning-based techniques for efficiently exploring the processor design space. We observe that these techniques result in multi-core processors whose performance is comparable (within 1%) to a processor design that requires an exhaustive exploration of the design space. These techniques often take orders of magnitude (a factor of 3800 at the minimum) less time for coming up with these processors. The benefits are up to 13% over intelligent search techniques that have been adapted to do multi-core design space exploration. We leverage the knowledge gained in this research to develop Magellan - a framework for accelerating multi-core design space exploration and optimization. Magellan can be used to find the highest throughput processors of a given type for a given area, power, or time budget. It can be used to aid even experienced processor designers that prefer to rely on intuition by allowing fast refinements to an input design.","1530-1591;1558-1101","978-3-9810801-3-1978-3-9810801-4","10.1109/DATE.2008.4484875","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4484875","","Space exploration;Design optimization;Process design;Algorithm design and analysis;Multicore processing;Acceleration;Machine learning algorithms;Machine learning;Throughput;Libraries","hardware-software codesign;learning (artificial intelligence);multiprocessing systems","machine learning-based framework;fast multi-core design space exploration;application-driven machine learning problem;intelligent search techniques;Magellan;throughput processors","","1","7","","","","","","IEEE","IEEE Conferences"
"A novel differential evolution scheme combined with particle swarm intelligence","Xing Xu; Yuanxiang Li; Shenlin Fang; Yu Wu; Feng Wang","State Key Laboratory of Software Engineering, Wuhan University, 430072, CHINA; State Key Laboratory of Software Engineering, Wuhan University, 430072, CHINA; NA; NA; NA","2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational Intelligence)","","2008","","","1057","1062","Differential evolution (DE) and particle swarm optimization (PSO) are the evolutionary computation paradigms, and both have shown superior performance on complex nonlinear function optimization problems. This paper detects the underlying relationship between them and then qualitatively proves that the two heuristic approaches from different theoretical background are consistent in form. Within the general perspective, the PSO can be regarded as a kind of DE. Inspired by this, a novel variant of DE mixed with particle swarm intelligence (DE-SI) is presented. Comparison experiments involving ten test functions well studied in the evolutionary optimization literature are used to highlight some performance differences between the DE-SI, two versions of DE and two PSO variants. The results from our study show that DE-SI keeps the most rapid convergence rate of all techniques and obtains the global optima for most benchmark problems.","1089-778X;1941-0026","978-1-4244-1822-0978-1-4244-1823","10.1109/CEC.2008.4630927","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4630927","","Evolution (biology);Particle swarm optimization;Chromium;Equations;Mathematical model;Algorithm design and analysis;Classification algorithms","evolutionary computation;particle swarm optimisation","differential evolution;particle swarm intelligence;evolutionary computation;particle swarm optimization","","3","25","","","","","","IEEE","IEEE Conferences"
"Taguchi’s Method for optimized neural network based autoreclosure in extra high voltage lines","F. Desta Zahlay; K. S. Rama Rao","Universiti Teknologi PETRONAS, Department of Electrical and Electronics Engineering, Tronoh, Malaysia; Universiti Teknologi PETRONAS, Department of Electrical and Electronics Engineering, Tronoh, Malaysia","2008 IEEE 2nd International Power and Energy Conference","","2008","","","901","906","This paper presents a method to discriminate the temporary faults from the permanent ones in an extra high voltage transmission line so that improper reclosing of the line into a fault is avoided. The fault identification prior to reclosing is based on optimized artificial neural network associated with Levenberg Marquardt algorithm to train the ANN and Taguchipsilas Method to find optimal parameters of the algorithm and number of hidden neurons. The algorithms are developed using MATLABtrade software. A range of faults are simulated using SimPowerSytemstrade and the spectra of the fault data are analyzed using Fast Fourier Transform which facilitates extraction of distinct features of each fault type. For both training and testing purposes, the neural network is fed with the normalized energies of the DC component, the fundamental and the first four harmonics of the faulted voltages. The developed algorithm is verified with dedicated testing data. The results show that it is possible to effectively distinguish the type of fault and practically avoid reclosing into faults.","","978-1-4244-2404-7978-1-4244-2405","10.1109/PECON.2008.4762603","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4762603","Autoreclosure;transmission line faults;EHV transmission line;artificial neural networks;Levenberg Marquardt algorithm;Taguchi’s method","Optimization methods;Neural networks;Voltage;Artificial neural networks;Testing;Transmission lines;Fault diagnosis;Neurons;Software algorithms;Computer languages","fast Fourier transforms;high-voltage engineering;neural nets;power system analysis computing;power transmission faults;power transmission lines;Taguchi methods","Taguchi method;artificial neural network;extra high voltage lines;transmission line autoreclosure;temporary faults;permanent faults;fault identification;Levenberg Marquardt algorithm;hidden neurons;MATLABtrade software;SimPowerSytemstrade;fast Fourier transform","","1","22","","","","","","IEEE","IEEE Conferences"
"Synthesis of Task and Message Activation Models in Real-Time Distributed Automotive Systems","W. Zheng; M. di Natale; C. Pinello; P. Giusto; A. S. Vincentelli","University of California at Berkeley, zhengwei@eecs.berkeley.edu; University of California at Berkeley; General Motors Research, dinatale@eecs.berkeley.edu; General Motors Research; Cadence Berkeley Labs, pinello@cadence.com; General Motors Research, paolo.giusto@gm.com; University of California at Berkeley, alberto@eecs.berkeley.edu","2007 Design, Automation & Test in Europe Conference & Exhibition","","2007","","","1","6","Modern automotive architectures support the execution of distributed safety- and time-critical functions on a complex networked system with several buses and tens of ECUs. Schedulability theory allows the analysis of the worst case end-to-end latencies and the evaluation of the possible architecture configurations options with respect to timing constraints. The paper presents an optimization framework, based on an ILP formulation of the problem, to select the communication and synchronization model that leverages the trade-offs between the purely periodic and the precedence constrained data-driven activation models to meet the latency and jitter requirements of the application. The authors demonstrate its effectiveness by optimizing a complex automotive architecture","1530-1591;1558-1101","978-3-9810801-2","10.1109/DATE.2007.364573","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4211778","","Real time systems;Automotive engineering;Delay;Timing;Constraint optimization;Application software;Time factors;Jitter;Aerospace electronics;Processor scheduling","automotive electronics;optimisation;real-time systems;task analysis;timing jitter","task synthesis;message activation models;real-time systems;distributed automotive systems;automotive architectures;complex networked system;schedulability theory;timing constraints;optimization framework;ILP formulation;jitter requirements","","14","10","","","","","","IEEE","IEEE Conferences"
"High Level Power Optimization by Type Inference on the Generation of Application Specific Circuits on FPGAs","J. M. Claver; G. Leon","Department of Computer Science, University of Valencia, Burjassot, Spain. Email: jclaver@uv.es; Department of Computer Science and Eng., University Jaume I, Castellón, Spain. Email: leon@uji.es","2007 International Conference on Field Programmable Logic and Applications","","2007","","","629","632","We describe the optimization of power consumption obtained by a high level environment developed for the automatic generation of application specific circuits on FPGA. The methodology used is based on the transformation of the whole algorithm in a graph of LUTs that implements all the required operations without the use of library components. The quality of the obtained circuitry is guaranteed by the use of ""type inference"". Our environment automatically optimizes the word-length and size of operators, and at the same time, reduces the internal data paths and the switching activity. Thus, in the extreme cases tested, the resulting generated circuits offer an important improvement in area usage of up to 95%, and power consumption is reduced by up to 98%.","1946-147X;1946-1488","978-1-4244-1059-0978-1-4244-1060","10.1109/FPL.2007.4380733","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4380733","","Power generation;Field programmable gate arrays;Energy consumption;Circuit synthesis;CMOS technology;CMOS logic circuits;Application software;Computer science;Inference algorithms;Circuit testing","application specific integrated circuits;field programmable gate arrays","FPGA;power consumption;automatic generation;application specific circuits","","","8","","","","","","IEEE","IEEE Conferences"
"Architectural Optimization of Decomposition Algorithms for Wireless Communication Systems","A. Irturk; B. Benson; N. Laptev; R. Kastner","NA; NA; NA; NA","2009 IEEE Wireless Communications and Networking Conference","","2009","","","1","6","Matrix decomposition is required in various algorithms used in wireless communication applications. FPGAs strike a balance between ASICs and DSPs, as they have the programmability of software with performance capacity approaching that of a custom hardware implementation. However, FPGA architectures require designers to make a countless number of system, architectural and logic design decisions. By performing design space exploration, a designer can find the optimal device for a specific application, however very few tools exist which can accomplish this task. This paper presents automatic generation and optimization of decomposition methods using a core generator tool, GUSTO, that we developed to enable easy design space exploration with different parameterization options such as resource allocation, bit widths of the data, number of functional units and organization of controllers and interconnects. We present a detailed study of area and throughput tradeoffs of matrix decomposition architectures using different parameterizations.","1525-3511;1558-2612","978-1-4244-2948-6978-1-4244-2947","10.1109/WCNC.2009.4917602","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4917602","","Wireless communication;Matrix decomposition;Application software;Field programmable gate arrays;Space exploration;Digital signal processing;Software performance;Hardware;Computer architecture;Logic design","application specific integrated circuits;automatic test pattern generation;digital signal processing chips;field programmable gate arrays;logic design;matrix decomposition;resource allocation;wireless sensor networks","architectural optimization;decomposition algorithms;wireless communication systems;matrix decomposition;ASIC;DSP;performance capacity;FPGA architectures;logic design;design space exploration;optimal device;automatic generation;core generator tool;GUSTO;resource allocation","","1","17","","","","","","IEEE","IEEE Conferences"
"LEAD: a methodology for learning efficient approaches to medical diagnosis","S. J. Fakih; T. K. Das","Dept. of Ind. & Manage. Syst. Eng., Univ. of South Florida, Tampa, FL, USA; Dept. of Ind. & Manage. Syst. Eng., Univ. of South Florida, Tampa, FL, USA","IEEE Transactions on Information Technology in Biomedicine","","2006","10","2","220","228","Determining the most efficient use of diagnostic tests is one of the complex issues facing medical practitioners. With the soaring cost of healthcare, particularly in the US, there is a critical need for cutting costs of diagnostic tests, while achieving a higher level of diagnostic accuracy. This paper develops a learning based methodology that, based on patient information, recommends test(s) that optimize a suitable measure of diagnostic performance. A comprehensive performance measure is developed that accounts for the costs of testing, morbidity, and mortality associated with the tests, and time taken to reach diagnosis. The performance measure also accounts for the diagnostic ability of the tests. The methodology combines tools from the fields of data mining (rough set theory, in particular), utility theory, Markov decision processes (MDP), and reinforcement learning (RL). The rough set theory is used in extracting diagnostic information in the form of rules from the medical databases. Utility theory is used in bringing various nonhomogenous performance measures into one cost based measure. An MDP model together with an RL algorithm facilitates obtaining efficient testing strategies. The methodology is implemented on a sample problem of diagnosing solitary pulmonary nodule (SPN). The results obtained are compared with those from four alternative testing strategies. Our methodology holds significant promise to improve the process of medical diagnosis","1089-7771;1558-0032","","10.1109/TITB.2005.855538","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1613947","Markov decision processes (MDP);medical diagnosis;reinforcement learning;rough sets","Medical diagnosis;Testing;Costs;Medical diagnostic imaging;Data mining;Set theory;Utility theory;Medical tests;Medical services;Optimization methods","data mining;learning (artificial intelligence);Markov processes;medical diagnostic computing;rough set theory;utility theory","learning efficient approaches to medical diagnosis;diagnostic performance;comprehensive performance measure;testing costs;data mining;rough set theory;utility theory;Markov decision processes;reinforcement learning;diagnostic information extraction;medical databases;nonhomogenous performance measures;solitary pulmonary nodule","Algorithms;Artificial Intelligence;Decision Making, Computer-Assisted;Decision Support Techniques;Diagnosis, Computer-Assisted;Medical Records Systems, Computerized;Software;User-Computer Interface","13","26","","","","","","IEEE","IEEE Journals & Magazines"
"SUM IP Core Generator for Solving Task for RKHS Series Summation","V. Hahanov; S. Chumachenko; D. Melnik; A. Taran","NA; Kharkov National University of Radio Electronics, Ukraine, 61166, Kharkov, Lenin Avenue, 14, e-mail: ri@kture.kharkov.ua; NA; NA","2007 9th International Conference - The Experience of Designing and Applications of CAD Systems in Microelectronics","","2007","","","258","259","Program system SUM IP core generator - means for verification of models - formulas of series summation in reproducing Kernel Hilbert space (RKHS) which allows to carry out input of the description of the model-formula with the help of the GUI-interface is offered; to model models - formulas with the help of software products Mathematica, Sinplify, Modelsim, Riviera, Active HDL; to generate initial files IP-core in languages of the description of equipment VHDL, Verilog, system C; to generate scripts - files for modelling, synthesis, implementation, time modelling; to synthesize tests, parameters, conditions for verification on basis Testbench; to carry out post- synthesis modelling for revealing mistakes in codes.","","966-533-587","10.1109/CADSM.2007.4297539","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4297539","","Mathematical model;Hardware design languages;System testing;Software testing;Communication standards;Kernel;Hilbert space;Design optimization;Automatic testing;Manufacturing","automatic programming;graphical user interfaces;hardware description languages;Hilbert spaces;mathematics computing","SUM IP core generator;RKHS series summation;reproducing kernel Hilbert space;GUI-interface;program system;tasks solution","","","16","","","","","","IEEE","IEEE Conferences"
"Overlapping Probabilities of Top Ranking Gene Lists, Hypergeometric Distribution, and Stringency of Gene Selection Criterion","W. Fury; F. Batliwalla; P. K. Gregersen; W. Li","Senior Bioinformatics Scientist, Regeneron Pharmaceutical, Inc. Tarrytown, NY 10591, USA. wen.fury@regeneron.com; Research Scientists, Robert S Boas Center for Genomics and Human Genetics, Feinstein Institute for Medical Research, North Shore LIJ Health System, Manhasset, NY 11030, USA fb@nshs.edu; Research Scientists, Robert S Boas Center for Genomics and Human Genetics, Feinstein Institute for Medical Research, North Shore LIJ Health System, Manhasset, NY 11030, USA peterg@nshs.edu; Research Scientists, Robert S Boas Center for Genomics and Human Genetics, Feinstein Institute for Medical Research, North Shore LIJ Health System, Manhasset, NY 11030, USA wli@nslij-genetics.org","2006 International Conference of the IEEE Engineering in Medicine and Biology Society","","2006","","","5531","5534","When the same set of genes appear in two top ranking gene lists in two different studies, it is often of interest to estimate the probability for this being a chance event. This overlapping probability is well known to follow the hypergeometric distribution. Usually, the lengths of top-ranking gene lists are assumed to be fixed, by using a pre-set criterion on, e.g., p-value for the t-test. We investigate how overlapping probability changes with the gene selection criterion, or simply, with the length of the top-ranking gene lists. It is concluded that overlapping probability is indeed a function of the gene list length, and its statistical significance should be quoted in the context of gene selection criterion","1557-170X","1-4244-0032-51-4244-0033","10.1109/IEMBS.2006.260828","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4463058","","Probability;Diseases;Testing;Bioinformatics;Proteins;Ontologies;Cities and towns;USA Councils;Medical treatment;Pharmaceuticals","genetics;molecular biophysics;statistical distributions","overlapping probability;hypergeometric distribution;gene selection criterion;gene list length;statistical analysis","Algorithms;Cluster Analysis;Data Interpretation, Statistical;Databases, Protein;Gene Expression Profiling;Gene Expression Regulation;Humans;Models, Genetic;Models, Statistical;Models, Theoretical;Oligonucleotide Array Sequence Analysis;Pattern Recognition, Automated;Probability;Software","29","14","","","","","","IEEE","IEEE Conferences"
"Application of optimizing the parameters of SVM using genetic simulated annealing algorithm","L. Cao; S. Zhou; R. Li; F. Wu; T. Liu","The Key Laboratory of Control Engineering, Chongqing Communication Institute, China, 400035; The Key Laboratory of Control Engineering, Chongqing Communication Institute, China, 400035; The Key Laboratory of Control Engineering, Chongqing Communication Institute, China, 400035; The Key Laboratory of Control Engineering, Chongqing Communication Institute, China, 400035; The Department of Automation , Chongqing University of Post and Telecommunication, China, 400065","2008 7th World Congress on Intelligent Control and Automation","","2008","","","5381","5385","The genetic simulated annealing algorithm can get global solution with low computational load. By means of this algorithms optimization method, the support vector machines (SVM) radial basis probabilistic kernel parameters of the performance was found out. A special software was developed on this method, it can be used in different field and improved the application of SVM in industry area. Then, a model of the battery capacity was established, and its correctness was tested by contrast with cross validation.","","978-1-4244-2113-8978-1-4244-2114","10.1109/WCICA.2008.4593806","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4593806","SVM;genetic simulated annealing algorithm;battery capacity","Support vector machines;Simulated annealing;Software algorithms;Genetics;Automation;Learning systems;Computational modeling","genetic algorithms;probability;simulated annealing;support vector machines","parameter optimisation;SVM;genetic simulated annealing algorithm;support vector machine;radial basis probabilistic kernel;battery capacity","","1","6","","","","","","IEEE","IEEE Conferences"
"In-Vehicle Navigation System Design Based On HCI Optimization","Y. Wang; Z. Yang; Q. Guan; L. Zhang; J. Wang","PH.D Candidate, Transportation college of Jilin University, Nanling Campus of Jilin University, 130022, phone: +86-0431-5095767; E-mail: sophiawy2003@163.com; Professor, Transportation college of Jilin University, Nanling Campus of Jilin University, 130022, phone: +86-0431-5095891; E-mail: yangzs@jlu.edu.cn; PH.D Candidate, Transportation college of Jilin University, Nanling Campus of Jilin University, 130022, phone: +86-0431-5095767; E-mail: wwhg2003its@jlu.edu.cn; PH.D Candidate, Transportation college of Jilin University, Nanling Campus of Jilin University, 130022, phone: +86-0431-5095767; E-mail: zhanglin9820@yahoo.com; PH.D Candidate, Transportation college of Jilin University, Nanling Campus of Jilin University, 130022, phone: +86-0431-5095767; E-mail: wangjian115@163.com","2006 IEEE International Conference on Vehicular Electronics and Safety","","2006","","","299","304","In-Vehicle Navigation System is becoming more and more popular. Its demand is increasing dramatically every year. This paper proposes a multi-modal scheme of hardware and software design considering ergonomic issues. Then, a field test was carried out to validate its usability and friendly interface. Some interfaces problems are discovered. Finally, some solutions are proposed to solve these problems. After improving and enhancement, the system meets the users' demands.","","1-4244-0758-31-4244-0759","10.1109/ICVES.2006.371603","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4234039","In-Vehicle Navigation System;Human Computer Interaction (HCI);Speech Recognition;3D Display","Navigation;Human computer interaction;Design optimization;Transportation;Educational institutions;Usability;Ergonomics;Hardware;Software design;Speech synthesis","computerised navigation;ergonomics;human computer interaction;human factors;optimisation;road vehicles;user interfaces","in-vehicle navigation system;HCI optimization;multimodal scheme;ergonomic issues;usability validation;friendly interface","","","12","","","","","","IEEE","IEEE Conferences"
"A generic platform for estimation of multi-threaded program performance on heterogeneous multiprocessors","A. Sahu; M. Balakrishnan; P. R. Panda","Department of Computer Science and Engineering, Indian Institute of Technology Delhi, Hauz Khas, New Delhi, India, 110016; Department of Computer Science and Engineering, Indian Institute of Technology Delhi, Hauz Khas, New Delhi, India, 110016; Department of Computer Science and Engineering, Indian Institute of Technology Delhi, Hauz Khas, New Delhi, India, 110016","2009 Design, Automation & Test in Europe Conference & Exhibition","","2009","","","1018","1023","This paper deals with a methodology for software estimation to enable design space exploration of heterogeneous multiprocessor systems. Starting from fork-join representation of application specification along with high level description of multiprocessor target architecture and mapping of application components onto architecture resource elements, it estimates the performance of application on target multiprocessor architecture. The methodology proposed includes the effect of basic compiler optimizations, integrates light weight memory simulation and instruction mapping for complex instruction to improve the accuracy of software estimation. To estimate performance degradation due to contention for shared resources like memory and bus, synthetic access traces coupled with interval analysis technique is employed. The methodology has been validated on a real heterogeneous platform. Results show that using estimation it is possible to predict performance with average errors of around 11%.","1530-1591;1558-1101","978-1-4244-3781-8978-3-9810-8015","10.1109/DATE.2009.5090813","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5090813","","Delay estimation;Application software;Computer architecture;Space technology;Space exploration;Software performance;Optimizing compilers;Computer science;Design engineering;Paper technology","multiprocessing systems;VLSI","heterogeneous multiprocessors;software estimation;VLSI technology","","4","20","","","","","","IEEE","IEEE Conferences"
"Optimizing Waste Fuel Boiler Control with Multivariable Predictive Control","D. Smith","5470 Shilshole Ave. NW, Suite 420, Seattle, WA 98107","Conference Record of 2006 Annual Pulp and Paper Industry Technical Conference","","2006","","","1","4","Multivariable predictive control, MPC, has been used in the continuous process industry for more than a decade. This strategy relies on a model created with test data from the process. The modeling produces a matrix of relationships between the ""manipulated variables"" and the ""control variables"" and ""constraint variables"". The MPC supervisory control software supplies the DCS, PLC or other regulatory controller with remote setpoints for the manipulated variables that will drive the control and constraint variables to their desired values. The control equations are solved simultaneously on a frequent interval to provide very tight control of the control variables and constraint limits. The technique can be applied to many pulp and paper applications including the waste fuel boilers that are an important part of the energy balance at today's mills. Significant improvements in efficiency have been achieved by optimizing the fuel and air flow to the boilers in order to minimize excess O2 and heat losses","0190-2172","1-4244-0281","10.1109/PAPCON.2006.1673775","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1673775","","Boilers;Predictive control;Fuel processing industries;Industrial control;Industrial relations;Testing;Supervisory control;Distributed control;Programmable control;Equations","boilers;control engineering computing;multivariable control systems;paper industry;paper mills;predictive control;waste recovery","waste fuel boiler control;multivariable predictive control;continuous process industry;supervisory control software;regulatory controller;energy balance;heat losses","","2","","","","","","","IEEE","IEEE Conferences"
"Performance Evaluation and Optimization of Parallel Grid Computing Applications","D. Becker; W. Frings; F. Wolf","NA; NA; NA","16th Euromicro Conference on Parallel, Distributed and Network-Based Processing (PDP 2008)","","2008","","","193","199","The combination of independent and potentially heterogeneous parallel machines creates a powerful metacomputer. Such a metacomputer can be used to run a single parallel application if a single machine does not provide enough CPUs. However, achieving satisfactory application performance on such a metacomputer is difficult since instances of grid-related as well as non grid-related performance properties may introduce various wait states during communication and synchronization. In our earlier work, we have introduced an extension to the SCALASCA tool set for recording event traces of metacomputing applications and searching them automatically for patterns of inefficient behavior related to wide-area communication. Here, we show how this extension in combination with statistical analyses and time-line visualization provided by VAMPIR can be applied to evaluate and optimize the performance of a multi-physics production code running on a heterogeneous and geographically dispersed metacomputer.","1066-6192;2377-5750","978-0-7695-3089","10.1109/PDP.2008.27","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4457124","Performance tools;grid computing;metacomputing","Grid computing;Application software;Parallel machines;Metacomputing;Delay;Optimization;Performance analysis;Testing;Computer science;Statistical analysis","grid computing;parallel machines;parallel programming;program visualisation;software performance evaluation;statistical analysis","performance evaluation;parallel grid computing;heterogeneous parallel machines;metacomputer;SCALASCA tool set;metacomputing;wide-area communication;statistical analyses;time-line visualization;VAMPIR;multiphysics production code","","1","10","","","","","","IEEE","IEEE Conferences"
"Scalable Techniques for Transparent Privatization in Software Transactional Memory","V. J. Marathe; M. F. Spear; M. L. Scott","NA; NA; NA","2008 37th International Conference on Parallel Processing","","2008","","","67","74","We address the recently recognized <i>privatization</i> <i>problem</i> in software transactional memory (STM) runtimes, and introduce the notion of <i>partially</i> <i>visible</i> <i>reads</i> (PVRs) to heuristically reduce the overhead of transparent privatization. Specifically, PVRs avoid the need for a ""privatization fence"" in the absence of conflict with concurrent readers. We present several techniques to trade off the cost of enforcing partial visibility with the precision of conflict detection. We also consider certain special-case variants of our approach, e.g., for predominantly read-only workloads. We compare our implementations to prior techniques on a multicore <i>Niagara1</i> system using a variety of artificial workloads. Our results suggest that while no one technique performs best in all cases, a dynamic hybrid of PVRs and strict in-order commits is stable and reasonably fast across a wide range of load parameters. At the same time, the remaining overheads are high enough to suggest the need for programming model or architectural support.","0190-3918;2332-5690","978-0-7695-3374","10.1109/ICPP.2008.69","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4625834","privatization;STM;performance","Privatization;Optimization;Throughput;Data structures;Clocks;Benchmark testing;Runtime","multiprocessing programs;software engineering","transparent privatization;software transactional memory;partially visible reads;conflict detection;special-case variants;multicore Niagara1 system","","10","17","","","","","","IEEE","IEEE Conferences"
"Parameter Optimization of an Active Tags Based Pedestrian Tracking System on a Distributed Emulator","J. Nakata; R. Beuran; T. Kawakami; T. Okada; K. Chinen; Y. Tan; Y. Shinoda","NA; NA; NA; NA; NA; NA; NA","2009 Computation World: Future Computing, Service Computation, Cognitive, Adaptive, Content, Patterns","","2009","","","563","568","In this paper, we introduce a distributed emulator for a pedestrian tracking system using active tags that is currently being developed by the authors. The emulator works on StarBED, which is a network testbed consisting of hundreds of PCs connected to each other by Ethernet. The three major components of the emulator (the processor emulator of the active tag micro-controller, RUNE, and QOMET) are all implemented on StarBED. We present the structure of the emulator, how it functions and the results from the emulation of the pedestrian tracking system. We confirmed that the emulator is not only useful for the evaluation phase of the development, but also applicable for the parameter optimization of the pedestrian tracking system by making emulations with different parameters.","","978-1-4244-5166-1978-0-7695-3862","10.1109/ComputationWorld.2009.39","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5359653","ubiquitous networks;active tag;radio emulation;distributed testbed;supporting software","Emulation;Testing;Distributed computing;Wireless networks;Interference;Microprogramming;Computational modeling;Wireless LAN;Operating systems;Communications technology","identification technology;local area networks;microcontrollers;optimisation;tracking","active tags based pedestrian tracking system;parameter optimization;distributed emulator;network testbed;StarBED;Ethernet;active tag microcontroller","","","15","","","","","","IEEE","IEEE Conferences"
"Towards an empirical method of efficiency testing of system parts: A methodological study","W. Brinkman; R. Haakma; D. G. Bouwhuis","NA; NA; NA","Interacting with Computers","","2007","19","3","342","356","Current usability evaluation methods are essentially holistic in nature. However, engineers that apply a component-based software engineering approach might also be interested in understanding the usability of individual parts of an interactive system. This paper examines the efficiency dimension of usability by describing a method, which engineers can use to test, empirically and objectively, the physical interaction effort to operate components in a single device. The method looks at low-level events, such as button clicks, and attributes the physical effort associated with these interaction events to individual components in the system. This forms the basis for engineers to prioritise their improvement effort. The paper discusses face validity, content validity, criterion validity, and construct validity of the method. The discussion is set within the context of four usability tests, in which 40 users participated to evaluate the efficiency of four different versions of a mobile phone. The results of the study show that the method can provide a valid estimation of the physical interaction event effort users made when interacting with a specific part of a device.","0953-5438;1873-7951","","10.1016/j.intcom.2007.01.002","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8147077","Efficiency;Usability testing;HCI methodology;Usability evaluation method;Log file analysis;Empirical method","","","","","1","","","","","","","OUP","OUP Journals & Magazines"
"An ACO-based Approach to Improve C-means Clustering Algorithm","W. Huang; J. Gou; H. Wu","Zhejiang University, Hangzhou; Huaqiao University, Quanzhou; HangZhou Dianzi University, Huzhou","2006 International Conference on Computational Inteligence for Modelling Control and Automation and International Conference on Intelligent Agents Web Technologies and International Commerce (CIMCA'06)","","2006","","","12","12","This paper presents an improved C-means clustering algorithm based on ACO. The proposed method use pheromone to evaluate individual colony's iterative result. In contrast with the existing C-means clustering algorithm, method in the paper need not appoint the number and pre-centers of clusters beforehand and it updates pheromone according to the transfer process of data points among different temporary clusters so as to avoid the local optima and reduce the iterative times to find actual cluster centers. We test its convergence performance with CRM data sets from China Unicom Corp. The experimental results show feasibility of design rationale.","","0-7695-2731","10.1109/CIMCA.2006.38","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4052660","","Clustering algorithms;Iterative algorithms;Ant colony optimization;Partitioning algorithms;Software algorithms;Educational institutions;Iterative methods;Computational modeling;Computational intelligence;Cities and towns","data analysis;iterative methods;optimisation;pattern clustering","improved c-means clustering algorithm;ant colony optimisation;iterative method;data analysis","","","19","","","","","","IEEE","IEEE Conferences"
"Research on automatic flaw detection of pipeline girth weld by ultrasonic phased array system","X. Zhan; D. Zhou; Shili Chen; S. Jin","College of Aeronautical Automation, Civil Aviation University of China, Tianjin, China; College of Aeronautical Automation, Civil Aviation University of China, Tianjin, China; State Key Laboratory of Precision Measuring Technology and Instruments, Tianjin University, China; State Key Laboratory of Precision Measuring Technology and Instruments, Tianjin University, China","2009 International Conference on Mechatronics and Automation","","2009","","","4310","4315","An automatic ultrasonic phased array inspection system used for nondestructive testing (NDT) of pipeline girth weld is developed. The linear phased array transducer is optimized by numerical analysis based on a mathematical model. Then hardware and software designs of the system are presented, realizing dynamically focusing in the detection area. A series of technological difficulties are solved. Finally, an automatic flaw detection testing platform on pipeline girth weld is set up. Experimental results illustrate the transducer optimum design improves system performance and the system has good capabilities of ultrasonic transmitting and echo synthesis, greatly improving testing flexibility and testing speed whereas reducing the system's volume and weight. It can be extended to inspect an object with rough surface or irregular structure in industrial NDT field.","2152-7431;2152-744X","978-1-4244-2692-8978-1-4244-2693","10.1109/ICMA.2009.5246569","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5246569","automatic flaw detection;ultrasonic phased array;nondestructive testing;electronic scanning","Phased arrays;Phase detection;Pipelines;Welding;Ultrasonic transducer arrays;Ultrasonic transducers;System testing;Inspection;Nondestructive testing;Numerical analysis","computerised instrumentation;inspection;numerical analysis;pipelines;production engineering computing;transducers;ultrasonic materials testing;welds","automatic flaw detection;pipelines;girth welds;ultrasonic phased array system;nondestructive testing;linear phased array transducer;numerical analysis","","1","11","","","","","","IEEE","IEEE Conferences"
"Dynamic Scratch-Pad Memory Management for Irregular Array Access Patterns","G. Chen; O. Ozturk; M. Kandemir; M. Karakoy","Computer Science and Engineering Department, Pennsylvania State university, University Park, PA 16802, USA, gchen@cse.psu.edu; NA; NA; NA","Proceedings of the Design Automation & Test in Europe Conference","","2006","1","","1","6","There exist many embedded applications such as those executing on set-top boxes, wireless base stations, HDTV, and mobile handsets that are structured as nested loops and benefit significantly from software managed memory. Prior work on scratchpad memories (SPMs) focused primarily on applications with regular data access patterns. Unfortunately, some embedded applications do not fit in this category and consequently conventional SPM management schemes will fail to produce the best results for them. In this work, we propose a novel compilation strategy for data SPMs for embedded applications that exhibit irregular data access patterns. Our scheme divides the task of optimization between compiler and runtime. The compiler processes each loop nest and inserts code to collect information at runtime. Then, the code is modified in such a fashion that, depending on the collected information, it dynamically chooses to use or not to use the data SPM for a given set of accesses to irregular arrays. Our results indicate that this approach is very successful with the applications that have irregular patterns and improves their execution cycles by about 54% over a state-of-the-art SPM management technique and 23% over the conventional cache memories. Also, the additional code size overhead incurred by our approach is less than 5% for all the applications tested","1530-1591;1558-1101","3-9810801-1","10.1109/DATE.2006.243810","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1657023","","Memory management;Scanning probe microscopy;Application software;Runtime;Base stations;HDTV;Mobile handsets;Embedded software;Optimizing compilers;Cache memory","cache storage;embedded systems;memory architecture;storage management","dynamic memory management;scratch-pad memory management;irregular array access patterns;embedded applications;wireless base stations;HDTV;mobile handsets;software managed memory;data access patterns;compiler process;information collection;cache memories;code size","","20","17","","","","","","IEEE","IEEE Conferences"
"Found on Artificial Neural Network and Genetic Algorithm Design the ASSEL Roll Profile","C. Xi; X. Xiaona; W. Yufeng; Z. Xidi","NA; NA; NA; NA","2008 International Conference on Computer Science and Software Engineering","","2008","1","","40","44","This article introduces a method of optimizing design of ASSEL roller profile. It obtains the feed angle and toe angle for all rolling gauges with genetic artificial neural network (GANN), and to optimize weight coefficient and network structure by training of weight value of genetic neural network. The model was tested on the imported ASSEL mill. The results indicate that the inaccuracy of feed angle and toe angle obtained based on the model were both less than plusmn2%. With obtained feed angle and toe angle, ASSEL roller characteristic parameters of each gauge can be calculated via roller profile design formulas provided by MEER of Germany. The ASSEL roll profile characteristic parameter values are determined by searching all common characteristics and approximate attribute in the possible gauges with parallel optimization method, one of the generic algorithms (GA).","","978-0-7695-3336","10.1109/CSSE.2008.1435","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721686","ASSEL mill;Roller profile design;ANN;GA;Parallel optimization","Artificial neural networks;Genetic algorithms;Algorithm design and analysis;Milling machines;Feeds;Design optimization;Design methodology;Computer science;Geometry;Software engineering","design engineering;gauges;genetic algorithms;learning (artificial intelligence);neural nets;parallel algorithms;rollers (machinery);rolling mills","genetic artificial neural network algorithm design;ASSEL rolling mill profile;rolling gauge feed angle;rolling gauge toe angle;parallel optimization algorithm;neural network training","","1","10","","","","","","IEEE","IEEE Conferences"
"A Multiobjective Design of a Patient and Anaesthetist-Friendly Neuromuscular Blockade Controller","P. Fazendeiro; J. V. de Oliveira; W. Pedrycz","NA; NA; NA","IEEE Transactions on Biomedical Engineering","","2007","54","9","1667","1678","During surgeries (especially in long ones), patients are subject to a substantial amount of drug dosage necessary to achieve the required neuromuscular blockade level. This paper aims at the development of a fuzzy controller that satisfies two important goals: 1) an optimization of the amount of drug (atracurium) required to induce an adequate level of relaxation and 2) a concomitant ability to explain the undertaken control decision at the level of natural language. For instance, statements of the form ldquoSince the difference between the target and the current blockade level is near zero, a small quantity of drug infusion is currently being applied,rdquo where ldquonear zerordquo and ldquosmallrdquo are linguistic terms that are represented as fuzzy sets. In this sense, we can regard this controller as a construct that is human friendly and highly interpretable (transparent). To address the two objectives outlined above, we consider the use of a multiobjective evolutionary optimization. How the quality of the control action and the controller interpretability are formalized and captured in this optimization framework is presented. The effectiveness of the approach is demonstrated through a comprehensive suite of experiments involving 100 simulated patients (used for training) and 500 patients (forming the test set), validating the approach for application in the operating theater.","0018-9294;1558-2531","","10.1109/TBME.2007.895109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4291659","Accuracy;fuzzy controller;interpretability;multiobjective evolutionary optimization;neuromuscular blockade;semantic integrity","Neuromuscular;Drugs;Surges;Fuzzy control;Informatics;Natural languages;Fuzzy sets;Humans;Testing;Surgery","drugs;fuzzy control;medical control systems;surgery","multiobjective design;anaesthetist;neuromuscular blockade controller;drug dosage;fuzzy controller;atracurium;drug infusion","Anesthesia;Computer Simulation;Decision Support Systems, Clinical;Drug Therapy, Computer-Assisted;Humans;Models, Biological;Neuromuscular Blockade;Neuromuscular Blocking Agents;Software;Software Design","30","29","","","","","","IEEE","IEEE Journals & Magazines"
"Cross-Entropy Based Testing","H. Chockler; E. Farchi; B. Godlin; S. Novikov","NA; NA; NA; NA","Formal Methods in Computer Aided Design (FMCAD'07)","","2007","","","101","108","In simulation-based verification, we check the correctness of a given program by executing it on some input vectors. Even for medium-size programs, exhaustive testing is impossible. Thus, many errors are left undetected. The problem of increasing the exhaustiveness of testing and decreasing the number of undetected errors is the main problem of software testing. In this paper, we present a novel approach to software testing, which allows us to dramatically raise the probability of catching rare errors in large programs. Our approach is based on the cross-entropy method. We define a performance function, which is higher in the neighborhood of an error or a pattern we are looking for. Then, the program is executed many times, choosing input vectors from some random distribution. The starting distribution is usually uniform, and it is changed at each iteration based on the vectors with highest value of the performance function in the previous iteration. The crossentropy method was shown to be very efficient in estimating the probabilities of rare events and in searching for solutions for hard optimization problems. Our experiments show that the cross-entropy method is also very efficient in locating rare bugs and patterns in large programs.We show the experimental results of our cross-entropy based testing tool and compare them to the performance of ConTest and of Java scheduler.","","0-7695-3023-0978-0-7695-3023","10.1109/FAMCAD.2007.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4401988","","Software testing;Computer bugs;Optimization methods;Iterative methods;Design automation;Laboratories;Computational modeling;Computer simulation;Java;Analytical models","","","","2","26","","","","","","IEEE","IEEE Conferences"
"Constant Row Maximizing Problem for Covering Arrays","P. Quiz-Ramos; J. Torres-Jimenez; N. Rangel-Valdez","NA; NA; NA","2009 Eighth Mexican International Conference on Artificial Intelligence","","2009","","","159","164","A Covering Array denoted by CA(N; t,k,¿) is a matrix of size N × k, where each tuple of t columns has at least one time each of the v<sup>t</sup> combinations of symbols. The C As are combinatorial objects used for software testing and design of experiments in: biology, agriculture, medicine, etc. CAs can be constructed using heuristic algorithms, greedy search and algebraic procedures. The Hartman Style Rising Procedures (HSRP) are algebraic procedures to construct large C As. These procedures create large CAs using small CAs. If the small C As have many constant rows, the HSRP provides better large CAs. In this paper we present the constant Row Maximizing Problem (CMRP) for CAs. We propose 4 distinct models to maximize the number of constant rows in a CA. The models were tested with binary and senary CAs and we improved some upper bounds.","","978-0-7695-3933","10.1109/MICAI.2009.28","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5372701","","Content addressable storage;Software testing;Costs;System testing;Heuristic algorithms;Upper bound;Design for experiments;Polynomials;Artificial intelligence;Laboratories","combinatorial mathematics;computational complexity;optimisation;polynomials","constant row maximizing problem;covering arrays;software testing;heuristic algorithms;greedy search;algebraic procedures;Hartman style rising procedures;combinatorial objects;design of experiments","","1","16","","","","","","IEEE","IEEE Conferences"
"New fast search algorithm for base layer of H.264 scalable video coding extension","L. Lima; D. Alfonso; L. Pezzoni; R. Leonardi","University of Brescia, Brescia, Italy; Advanced System Technology (AST) - STMicroelectronics, Agrate Brianza, Italy; Advanced System Technology (AST) - STMicroelectronics, Agrate Brianza, Italy; University of Brescia, Brescia, Italy","2007 Data Compression Conference (DCC'07)","","2007","","","393","393","In this contribution, a fast search motion estimation algorithm for H.264/AVC SVC (scalable video coding) base layer with hierarchical B-frame structure for temporal decomposition is presented and compared with fast search motion estimation algorithm in JSVM software, that is the reference software for H.264/AVC SVC. The proposed technique is a block-matching based motion estimation algorithm working in two steps, called Coarse search and Fine search. The Coarse search is performed for each frame in display order, and for each 16x16 macroblock chooses the best motion vector at half pel accuracy. Fine search is performed for each frame in encoding order and finds the best prediction for each block type, reference frame and direction, choosing the best motion vector at quarter pel accuracy using R-D optimization. Both Coarse and Fine Search test 3 spatial and 3 temporal predictors, and add to the best one a set of updates.","1068-0314","0-7695-2791","10.1109/DCC.2007.58","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4148794","","Video coding;Testing;Motion estimation;Software algorithms;Automatic voltage control;Static VAr compensators;Displays;Encoding;Performance loss;Helium","audio coding;motion estimation;search problems;video coding","fast search algorithm;H.264 scalable video coding;motion estimation algorithm;audio video coding;hierarchical B-frame structure;temporal decomposition;JSVM software;block-matching;coarse search;fine search;R-D optimization","","2","2","","","","","","IEEE","IEEE Conferences"
"Applications of HMM in Protocol Anomaly Detection","J. Zhao; H. Huang; S. Tian; X. Zhao","NA; NA; NA; NA","2009 International Joint Conference on Computational Sciences and Optimization","","2009","2","","347","349","A protocol anomaly detection model based on hidden Markov model (HMM) is given in this work which can verify normal and abnormal traffic. Then we demonstrate the model's correctness and effectiveness by using MIT Lincoln Laboratory 1999 DARPA Intrusion Detection Evaluation Data Set.","","978-0-7695-3605","10.1109/CSO.2009.51","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5193967","HMM;IDS","Hidden Markov models;Protocols;Intrusion detection;Telecommunication traffic;Traffic control;Laboratories;Testing;Application software;Information technology;Data security","computer networks;hidden Markov models;protocols;telecommunication security;telecommunication traffic","hidden Markov model;protocol anomaly detection;network traffic;intrusion detection;network security","","1","11","","","","","","IEEE","IEEE Conferences"
"Exam HIP","S. Shirali-Shahreza; M. Shirali-Shahreza; A. Movaghar","Computer Engineering Department, Sharif University of Technology, Azadi Street, Tehran, IRAN. Email: shirali@ce.sharif.edu; Computer Science Department, Sharif University of Technology, Azadi Street, Tehran, IRAN. Email: shirali@cs.sharif.edu; Computer Engineering Department, Sharif University of Technology, Azadi Street, Tehran, IRAN. Email: movaghar@sharif.edu","2007 International Workshop on Anti-Counterfeiting, Security and Identification (ASID)","","2007","","","415","418","Nowadays a great deal of information is circulated through the Internet. Some of it exclusively belongs to a special group of people and requires protection and safeguarding against unauthorized access. To this end some systems called HIP (human interactive proofs) have been introduced to tell humans and computers apart or various groups of people apart. In this paper, a new method for telling users apart by help of showing picture of a multiple-choice question has been presented. This multiple-choice question is related to some protected information in the field of user's profession. Since picture of question is viewed the computer cannot answer it. On the other hand, only experts and professional people or people related to the concerned subject can answer the question and have access to information because of professional nature of the question (for example a medical question). This project has been implemented by PHP scripting language.","2163-5048;2163-5056","1-4244-1035-51-4244-1035","10.1109/IWASID.2007.373668","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4244860","CAPTCHA (Completely Automatic Public Turing test to tell Computer and Human Apart);Digital Identification;HIP (Human Interactive Proof);OCR (Optical Character Recognition);Pattern Recognition","Hip;Humans;Internet;Protection;Optical character recognition software;Automatic testing;Biomedical imaging;Optimized production technology;Computer science;Optical computing","authorisation","human interactive proofs;multiple choice question;information protection;information access","","4","9","","","","","","IEEE","IEEE Conferences"
"Optimizing Polynomial Expressions by Algebraic Factorization and Common Subexpression Elimination","A. Hosangadi; F. Fallah; R. Kastner","NA; NA; NA","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","","2006","25","10","2012","2022","Polynomial expressions are frequently encountered in many application domains, particularly in signal processing and computer graphics. Conventional compiler techniques for redundancy elimination such as common subexpression elimination (CSE) are not suited for manipulating polynomial expressions, and designers often resort to hand optimizing these expressions. This paper leverages the algebraic techniques originally developed for multilevel logic synthesis to optimize polynomial expressions by factoring and eliminating common subexpressions. The proposed algorithm was tested on a set of benchmark polynomial expressions where savings of 26.7% in latency and 26.4% in energy consumption were observed for computing these expressions on the StrongARM SA1100 processor core. When these expressions were synthesized in custom hardware, average energy savings of 63.4% for minimum hardware constraints and 24.6% for medium hardware constraints over CSE were observed","0278-0070;1937-4151","","10.1109/TCAD.2006.875712","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1677687","Circuit complexity;common subexpression elimination (CSE);high-level synthesis;polynomials","Polynomials;Hardware;Application software;Signal processing;Computer graphics;Design optimization;Optimizing compilers;Logic functions;Signal synthesis;Signal processing algorithms","high level synthesis;logic design;multivalued logic;polynomials","polynomial expression optimization;algebraic factorization;common subexpression elimination;algebraic techniques;multilevel logic synthesis;StrongARM SA1100 processor core;hardware constraints","","23","30","","","","","","IEEE","IEEE Journals & Magazines"
"Optimized data transfer for time-dependent, GPU-based glyphs","S. Grottel; G. Reina; T. Ertl","Institute for Visualization and Interactive Systems, Universität Stuttgart, Germany; Institute for Visualization and Interactive Systems, Universität Stuttgart, Germany; Institute for Visualization and Interactive Systems, Universität Stuttgart, Germany","2009 IEEE Pacific Visualization Symposium","","2009","","","65","72","Particle-based simulations are a popular tool for researchers in various sciences. In combination with the availability of ever larger COTS clusters and the consequently increasing number of simulated particles the resulting datasets pose a challenge for real-time visualization. Additionally the semantic density of the particles exceeds the possibilities of basic glyphs, like splats or spheres and results in dataset sizes larger by at least an order of magnitude. Interactive visualization on common workstations requires a careful optimization of the data management, especially of the transfer between CPU and GPU. We propose a flexible benchmarking tool along with a series of tests to allow the evaluation of the performance of different CPU/GPU combinations in relation to a particular implementation. We evaluate different uploading strategies and rendering methods for point-based compound glyphs suitable for representing the aforementioned datasets. CPU and GPU-based approaches are compared with respect to their rendering and storage efficiency to point out the optimal solution when dealing with time-dependent datasets. The results of our research are of general interest since they can be transferred to other applications where CPU-GPU bandwidth and a high number of graphical primitives per dataset pose a problem. The employed tool set for streamlining the measurement process is made publicly available.","2165-8765;2165-8773","978-1-4244-4404","10.1109/PACIFICVIS.2009.4906839","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4906839","I.3.6 [Computer Graphics]: Methodology and Techniques I.3.6 [Computer Graphics]: Graphics data structures and data types I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism","Data visualization;Computer graphics;Rough surfaces;Surface roughness;Application software;Data structures;Hardware;Interactive systems;Workstations;Benchmark testing","data visualisation;electronic data interchange;optimisation;rendering (computer graphics)","optimized data transfer;GPU-based glyphs;particle-based simulations;COTS;real-time visualization;interactive visualization;rendering","","12","21","","","","","","IEEE","IEEE Conferences"
"A C compiler based methodology for implementing audio DSP applications on a class of embedded systems","M. Djukic; N. Cetic; J. Kovacevic; M. Popovic","Faculty of Technical Sciences, University of Novi Sad, Trg Dositeja Obradovica 6, Serbia, Yugoslavia; Faculty of Technical Sciences, University of Novi Sad, Trg Dositeja Obradovica 6, Serbia, Yugoslavia; Faculty of Technical Sciences, University of Novi Sad, Trg Dositeja Obradovica 6, Serbia, Yugoslavia; Faculty of Technical Sciences, University of Novi Sad, Trg Dositeja Obradovica 6, Serbia, Yugoslavia","2008 IEEE International Symposium on Consumer Electronics","","2008","","","1","4","This paper describes a methodology for a common task of audio application implementation from the referent C code to executable image targeting an audio fixed-point mid-scale DSPs. This methodology tries to efficiently cover the gap between the referent code and the assembler code by usage of the C compiler, which supports fixed-point types defined in C language extensions for the embedded processors. By relaying on C++ classes this methodology deliveries a possibility to debug a DSP compiler ready C code in a C++ environment (e.g. Visual C++). The methodology was successfully applied to several audio applications, such as Dolby Volume, SRS TSHD, SRS VIQ, and Audyssey Dynamic EQ, and their implementation to Cirrus Logic Coyote DSP family. Experience with those applications shows that this methodology greatly shortens time to market for DSP firmware product.","0747-668X;2159-1423","978-1-4244-2422","10.1109/ISCE.2008.4559481","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4559481","Embedded software;audio applications;DSP compiler","Digital signal processing;Embedded system;Application software;Optimizing compilers;Assembly systems;Logic;Testing;Program processors;Programming profession;Relays","audio signal processing;digital signal processing chips;embedded systems;object-oriented programming;program assemblers;program compilers;program debugging","C compiler based methodology;audio DSP applications;embedded systems;assembler code;C language extensions;embedded processors;C++ classes;DSP compiler debugging;Dolby Volume;SRS TSHD;SRS VIQ;Audyssey Dynamic EQ;Cirrus Logic Coyote DSP family;DSP firmware product;fixed-point mid-scale DSP;referent code","","1","7","","","","","","IEEE","IEEE Conferences"
"Approach of intersection signal control based on PSO","Zhong Qin; ZhiPing Fan; Xiaodong Zang; Huawei Gong","School of Civil Engineering, Guangzhou University, Guangzhou, 510006, China; College of Computer Science & Educational Software, Guangzhou University, Guangdong, 510006, China; School of Civil Engineering, Guangzhou University, Guangzhou, 510006, China; School of Civil Engineering, Guangzhou University, Guangzhou, 510006, China","2009 Chinese Control and Decision Conference","","2009","","","4421","4425","Intersection as the city's road network node, is the meeting point for distribution of traffic flow. In fact, traffic signal control is partition of traffic flows in time and space, so that the traffic flows no conflict obtain right-of-way in order. This paper present the real-time signal timing optimization in intersection, in which, the Kalman filter is used to forecast short-term traffic flow, the particle swarm optimization optimize the timing parameters. Simulation tests confirmed that the algorithm is superior to the traditional signal timing algorithm, which reduce the average delay time effectively.","1948-9439;1948-9447","978-1-4244-2722-2978-1-4244-2723","10.1109/CCDC.2009.5192341","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5192341","PSO;Kalman Filtering;Signal Intersection","Communication system traffic control;Timing;Traffic control;Roads;Delay effects;Economic forecasting;Telecommunication traffic;Safety;Genetic algorithms;Signal processing","filtering theory;forecasting theory;Kalman filters;particle swarm optimisation;road traffic;signal processing","intersection signal control;PSO;particle swarm optimisation;city road network node;traffic flow distribution;traffic signal control;real-time signal timing optimization;Kalman filter;short-term traffic flow forecast;delay time","","1","12","","","","","","IEEE","IEEE Conferences"
"Assessment of ANN-based auto-reclosing scheme developed on single machine-infinite bus model with IEEE 14-bus system model data","D. Z. Fitiwi; K. S. R. Rao","Department of Electrical and Electronics Engineering, Universiti Teknologi PETRONAS, 31750 Tronoh, Malaysia; Department of Electrical and Electronics Engineering, Universiti Teknologi PETRONAS, 31750 Tronoh, Malaysia","TENCON 2009 - 2009 IEEE Region 10 Conference","","2009","","","1","6","This paper focuses on methods to discriminate a temporary fault from a permanent one, and accurately determine fault extinction time in an extra high voltage (EHV) transmission line in a bid to develop a self-adaptive automatic reclosing scheme. Consequently, improper reclosing of the line onto a fault is avoided. The fault identification prior to reclosing is based on optimized artificial neural network associated with three different training algorithms. In addition, Taguchi's methodology is employed in optimizing parameters that significantly influence during and post-training performance of the neural network. A comparison of overall performance of the three algorithms, developed and coded in MATLAB<sup>TM</sup> software environment, is also presented. To validate the work, the developed technique in a single machine infinite bus (SMIB) model has been tested by data obtained from benchmark IEEE 14-bus system model simulations. The results show the efficacy of the developed adaptive automatic reclosing method.","2159-3442;2159-3450","978-1-4244-4546-2978-1-4244-4547","10.1109/TENCON.2009.5395874","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5395874","Taguchi's method;Levenberg Marquardt;Resilient Back-propagation;Autoreclosure;Neural Network","Mathematical model;Artificial neural networks;Voltage;Transmission lines;Fault diagnosis;Optimization methods;Software algorithms;MATLAB;Software performance;System testing","learning (artificial intelligence);neural nets;power engineering computing;power transmission faults;power transmission lines;Taguchi methods","ANN-based auto-reclosing scheme assessment;single machine-infinite bus model;IEEE 14-bus system model data;extra high voltage transmission line;self-adaptive automatic reclosing scheme;fault identification;optimized artificial neural network;Taguchi methodology;MATLAB software environment","","3","27","","","","","","IEEE","IEEE Conferences"
"Adaptive Data Placement in an Embedded Multiprocessor Thread Library","P. Stanley-Marbell; K. Lahiri; A. Raghunathan","Dept of ECE, Carnegie Mellon University Pittsburgh, PA pstanley@ece.cmu.edu; NA; NA","Proceedings of the Design Automation & Test in Europe Conference","","2006","1","","1","2","Embedded multiprocessors pose new challenges in the design and implementation of embedded software. This has led to the need for programming interfaces that expose the capabilities of the underlying hardware. In addition, for systems that implement applications consisting of multiple concurrent threads of computation, the optimized management of inter-thread communication is crucial for realizing high-performance. This paper presents the design of an application-adaptive thread library that conforms to the IEEE POSIX 1003.1c threading standard (Pthreads). The library adapts the placement of both explicitly marked application data objects, as well as implicitly created data objects, in a physically distributed on-chip memory architecture, based on the application's data access characteristics","1530-1591;1558-1101","3-9810801-1","10.1109/DATE.2006.244065","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1656977","","Yarn;Random access memory;Data structures;Central Processing Unit;Memory architecture;Cyclic redundancy check;Wireless LAN;Cryptography;Software libraries;National electric code","application program interfaces;distributed memory systems;embedded systems;memory architecture","adaptive data placement;embedded multiprocessors;embedded software;programming interfaces;interthread communication;application-adaptive thread library;IEEE POSIX 1003.1c threading standard;distributed on-chip memory architecture","","1","4","","","","","","IEEE","IEEE Conferences"
"Testing Anti-Frozen Characters of Air-Water Plates Heat Exchangers by FLUENT","H. Wu; G. Sun; Z. Lin","NA; NA; NA","2009 Asia-Pacific Power and Energy Engineering Conference","","2009","","","1","4","Physical structure are compared between air-water plate heat exchangers (PHE) and tube-fin type heat exchangers in this paper, which show that the air-water PHE has the stable character in handling the cold air below 0degC because of its compact and knock-down structure. Then the courses of water being iced in the motionless state and in the flowing state were simulated by the CFD software Fluent with the model of Solidification/Melting. The results show that the corrugated passages of the air-water PHE are anti-frozen structures for flowing water, which is appropriate to the air-water plate heater's optimization and manufacture.","2157-4839;2157-4847","978-1-4244-2487-0978-1-4244-2486","10.1109/APPEEC.2009.4918861","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4918861","","Testing;Solid modeling;Water heating;Heat engines;Computational fluid dynamics;Temperature;Ice;Heat transfer;Sun;Educational institutions","computational fluid dynamics;heat exchangers;heat transfer;laminar flow;melting;pipe flow;solidification","anti-frozen characters;air-water plates heat exchangers;FLUENT;tube-fin type heat exchangers;CFD software;solidification;melting","","","11","","","","","","IEEE","IEEE Conferences"
"Adaptive Graph-Cut Algorithm to Video Moving Objects Segmentation","C. Guo; P. Wang","NA; NA","2009 2nd International Congress on Image and Signal Processing","","2009","","","1","5","An adaptive Graph-cut algorithm to video moving objects segmentation was proposed. By the Kalman prediction of the number of objectives pixels and objectives-background pixel-pairs, and adaptive updating of the nodes flux, the Graph-cut algorithm was successfully applied to video moving objects segmentation. It was achieve to continuous global optimization segmentation of video moving objects. Experimental results show that the quantitative detection parameters of this algorithm are performing well in complex background conditions.","","978-1-4244-4129-7978-1-4244-4131","10.1109/CISP.2009.5303436","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5303436","","Object segmentation;Image segmentation;Kalman filters;Markov random fields;Virtual colonoscopy;Educational institutions;Testing;Cameras;Computer vision;Application software","graph theory;image segmentation;optimisation;video signal processing","video moving object segmentation;graph-cut algorithm;Kalman prediction;optimization","","1","8","","","","","","IEEE","IEEE Conferences"
"Efficient Design Space Exploration of High Performance Embedded Out-of-Order Processors","S. Eyerman; L. Eeckhout; K. De Bosschere","ELIS, Ghent University, Sint-Pietersnieuwstraat 41, B-9000 Gent, Belgium, seyerman@elis.UGent.be; NA; NA","Proceedings of the Design Automation & Test in Europe Conference","","2006","1","","1","6","Previous work on efficient customized processor design primarily focused on in-order architectures. However, with the recent introduction of out-of-order processors for high-end high-performance embedded applications, researchers and designers need to address how to automate the design process of customized out-of-order processors. Because of the parallel execution of independent instructions in out-of-order processors, in-order processor design methodologies which subdivide the search space in independent components are unlikely to be effective in terms of accuracy for designing out-of-order processors. In this paper we propose and evaluate various automated singleand multi-objective optimizations for exploring out-of-order processor designs. We conclude that the newly proposed genetic local search algorithm outperforms all other search algorithms in terms of accuracy. In addition, we propose two-phase simulation in which the first phase explores the design space through statistical simulation; a region of interest is then simulated through detailed simulation in the second phase. We show that simulation time speedups can be obtained of a factor 2.2times to 7.3times using two-phase simulation","1530-1591;1558-1101","3-9810801-1","10.1109/DATE.2006.243735","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1656905","","Out of order;Space exploration;Process design;Application software;Design methodology;Design optimization;Genetics;Computational modeling;Multimedia systems;Image segmentation","electronic design automation;embedded systems;integrated circuit design;logic design;microprocessor chips;optimisation;search problems","design space exploration;embedded out-of-order processors;design automation;parallel execution;in-order processor design;genetic local search algorithm;multiobjective optimizations","","26","18","","","","","","IEEE","IEEE Conferences"
"Wireless healthcare monitoring system","F. Touati; N. Hamza; L. Khriji","Department of Electrical and Computer, Engineering Sultan Qaboos Univeristy, Al-Khod, Muscat, Oman; Department of Electrical and Computer Engineering Sultan Qaboos, Univeristy, Al-Khod, Muscat, Oman; Department of Electrical and Computer Engineering Sultan Qaboos, Univeristy, Al-Khod, Muscat, Oman","2009 4th International Design and Test Workshop (IDT)","","2009","","","1","6","Rapid advances in information technology and telecommunications, and more specifically wireless and mobile communications are leading to the emergence of a new type of information infrastructure that has the potential of supporting an array of advanced services for healthcare. This paper developed a wireless biomedical sensor network prototype system based on ISM band for healthcare. It offers flexibility and mobility to save cost and energy spent on wiring. The framework hardware and software structure, related programming, the accuracy of the system and an optimization of power consumption are also discussed in this paper. Comparing the system which uses ISM band with traditional wired network system for healthcare, it has advantage of low cost, low power and wider coverage.","2162-0601;2162-061X","978-1-4244-5750-2978-1-4244-5748-9978-1-4244-5750","10.1109/IDT.2009.5404164","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5404164","Biomedical sensor;ISM band;Multi-purpose system;energy saving;Autonomous healthcare","Medical services;Wireless sensor networks;Costs;Biomedical monitoring;Information technology;Wireless communication;Mobile communication;Biosensors;Software prototyping;Prototypes","application program interfaces;health care;optimisation;power aware computing;telemedicine;wireless sensor networks","wireless healthcare monitoring system;wireless biomedical sensor network;ISM band;hardware structure;software structure;programming;system accuracy;optimization;power consumption","","1","12","","","","","","IEEE","IEEE Conferences"
"An Introduction to High-Level Synthesis","P. Coussy; D. D. Gajski; M. Meredith; A. Takach","Universit&#x0E9; de Bretagne-Sud, Lab-STICC; University of California; Forte Design Systems; Mentor Graphics","IEEE Design & Test of Computers","","2009","26","4","8","17","High-level synthesis raises the design abstraction level and allows rapid generation of optimized RTL hardware for performance, area, and power requirements. This article gives an overview of state-of-the-art HLS techniques and tools.","0740-7475;1558-1918","","10.1109/MDT.2009.69","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5209958","high-level synthesis;RTL abstraction;custom processors;hardware synthesis and verification;architectures;design and test","High level synthesis;Assembly;Application software;Circuit simulation;Design methodology;Space exploration;Computer architecture;Design optimization;Hardware design languages;Circuit synthesis","high level synthesis","high-level synthesis;optimized RTL hardware;abstraction level design;HLS techniques","","112","25","","","","","","IEEE","IEEE Journals & Magazines"
"Automatically building service evaluation metadata in a grid environment","Zhihui Du; Yinong Chen; Lei Wu; Suihui Zhu","Dept. of Comput. Sci. & Technol., Tsinghua Univ., Beijing, China; NA; NA; NA","The Fourth IEEE Workshop on Software Technologies for Future Embedded and Ubiquitous Systems, and the Second International Workshop on Collaborative Computing, Integration, and Assurance (SEUS-WCCIA'06)","","2006","","","6 pp.","","This paper presents a mechanism that automatically build evaluation metadata for grid services by using the information from the execution of different applications. A new MetaService AMDBS, automatic metadata building service, is designed to support this mechanism. The typical feature of this mechanism is that it can get not only the absolute evaluation information, but also the relative global ranking result through the collaboration of different AMDBSs. Web service group testing is used as an example of a grid application to accelerate the process of evaluation metadata building. Simulation experiment results show that the HP (heavy-powerful) algorithm which is designed based on the dynamic evaluation metadata can improve the application performance significantly, compared with the RR (round-robin) algorithm which does not use the evaluation metadata. The effect of using evaluation metadata in different task and resource conditions is also demonstrated by experiment results","","0-7695-2560","10.1109/SEUS-WCCIA.2006.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1611739","Service Evaluation Metadata;Grid Computing;Group Testing;MetaService","Building services;Web services;International collaboration;Testing;Round robin;Service oriented architecture;Resource management;Monitoring;Application software;Conferences","grid computing;Internet;meta data","service evaluation metadata;grid service environment;MetaService;automatic metadata building service;Web service group testing;grid computing application;dynamic evaluation metadata","","1","19","","","","","","IEEE","IEEE Conferences"
"Primal-dual interior-point methods solver based on kernel functions for Linear Optimization","M. E. Ghami; I. Ivanov; T. Steihaug","University of Bergen, Department of Informatics, Post Box 7803, N-5020, Norway; University of Bergen, Department of Informatics, Post Box 7803, N-5020, Norway; TUDelft Department of Information, Systems and Algorithms, P.O. Box 5031, 2600 GA, The Netherlands","2009 International Multiconference on Computer Science and Information Technology","","2009","","","743","749","In this paper we present the theory and practical aspects of implementing the path following interior point methods for linear optimization, based on kernel functions. We will investigate the influence of the choice of the kernel function on the computational behavior of the generic primal-dual algorithm for Linear Optimization. We find that the finite kernel function gives the best results for more than 50 % of the tested problems compared to the standard log-barrier method.","2157-5525","978-1-4244-5314","10.1109/IMCSIT.2009.5352756","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5352756","Kernel function;Interior-point Algorithm;Large-update;Iteration number","Kernel;Optimization methods;Testing;Informatics;Computer science;Information technology;Information systems;Helium;MATLAB;Software libraries","linear programming;mathematics computing","primal dual interior point methods solver;kernel function;linear optimization;path following interior point method","","1","15","","","","","","IEEE","IEEE Conferences"
"Optimization of Regular Expression Pattern Matching Circuits on FPGA","Cheng-Hung Lin; Chih-Tsun Huang; Chang-Ping Jiang; Shih-Chieh Chang","Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; NA; NA; NA","Proceedings of the Design Automation & Test in Europe Conference","","2006","2","","1","6","Regular expressions are widely used in network intrusion detection system (NIDS) to represent patterns of network attacks. Since traditional software-only NIDS cannot catch up to the speed advance of networks, many previous works propose hardware architectures on FPGA to accelerate attack detection. The challenge of hardware implementation is to accommodate the regular expressions to FPGAs of the large number of attacks. Although the minimization of logic equations has been studied intensively in the CAD area, the minimization of multiple regular expressions has been largely neglected. This paper presents an architecture allowing our algorithm to extract and share common sub-regular expressions. Experimental results show that our sharing scheme significantly reduces the area of regular expression circuits","1530-1591;1558-1101","3-9810801-1","10.1109/DATE.2006.244157","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1657107","","Pattern matching;Circuits;Field programmable gate arrays;Intrusion detection;Hardware;Minimization;Acceleration;Logic design;Equations;Impedance matching","digital arithmetic;field programmable gate arrays;logic design;security of data","pattern matching circuits;FPGA;network intrusion detection system;network attacks;attack detection;regular expression circuits","","5","7","","","","","","IEEE","IEEE Conferences"
"Conceptual framework of a dynamic resource allocation test bed and its practical realization with ProModel","I. R. Wior; Z. J. Zhao; M. Luo; J. B. Zhang; S. S. Ge; H. C. Lau","Hamburg University of Technology, Germany; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Singapore Institute of Manufacturing Technology, Singapore; Singapore Institute of Manufacturing Technology, Singapore; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; School of Information Systems, Singapore Management University, Singapore","2009 IEEE Control Applications, (CCA) & Intelligent Control, (ISIC)","","2009","","","1613","1618","For the purpose of generating a standardized environment to test scheduling solvers on scheduling problems and to rank the results in comparison to other solvers, a simulation-based framework for a dynamic resource allocation test bed is proposed and realized. Our focus is on multiple-machine problem formulations, predictable dynamics, event based control and especially their combination. The framework separates the system into the test bed, the solver and a problem generator. Finally a test bed within this framework is realized with the commercial simulation software ProModel.","1085-1992","978-1-4244-4601-8978-1-4244-4602","10.1109/CCA.2009.5281132","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5281132","Resource allocation;Test bed;Simulationbased;Immediate dynamics;Predictable dynamics","Resource management;Job shop scheduling;Dynamic scheduling;System testing;Predictive models;Dynamic programming;Computer aided manufacturing;Virtual manufacturing;Processor scheduling;Scheduling algorithm","discrete event simulation;resource allocation;scheduling","dynamic resource allocation test bed;ProModel;scheduling problem;simulation-based framework;multiple-machine problem formulation;predictable dynamics;event based control","","","6","","","","","","IEEE","IEEE Conferences"
"An automated design flow for vibration-based energy harvester systems","L. Wang; T. J. Kazmierski; B. M. Al-Hashimi; S. P. Beeby; Dibin Zhu","School of Electronics and Computer Science, University of Southampton, UK; School of Electronics and Computer Science, University of Southampton, UK; School of Electronics and Computer Science, University of Southampton, UK; School of Electronics and Computer Science, University of Southampton, UK; School of Electronics and Computer Science, University of Southampton, UK","2009 Design, Automation & Test in Europe Conference & Exhibition","","2009","","","1391","1396","This paper proposes, for the first time, an automated energy harvester design flow which is based on a single HDL software platform that can be used to model, simulate, configure and optimise energy harvester systems. A demonstrator prototype incorporating an electromagnetic mechanical-vibration-based micro-generator and a limited number of library models has been developed and a design case study has been carried out. Experimental measurements have validated the simulation results which show that the outcome from the design flow can improve the energy harvesting efficiency by 75%.","1530-1591;1558-1101","978-1-4244-3781-8978-3-9810-8015","10.1109/DATE.2009.5090881","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5090881","","Vibrations;Hardware design languages;Design optimization;Software prototyping;Prototypes;Software libraries;Electromagnetic modeling;Energy measurement;Fluid flow measurement;Electromagnetic measurements","electronic design automation;energy harvesting;hardware description languages;micromechanical devices;prototypes;vibrations","automated design flow;vibration-based energy harvester system;single HDL software platform;electromagnetic mechanical-vibration-based microgenerator;library models;energy harvesting efficiency;demonstrator prototype;hardware description language","","4","18","","","","","","IEEE","IEEE Conferences"
"Optimization of Layer-based Scheduling Algorithms for Mixed Parallel Applications with Precedence Constraints Using Move-blocks","R. Kunis; G. Rünger","NA; NA","2009 17th Euromicro International Conference on Parallel, Distributed and Network-based Processing","","2009","","","70","77","The efficient scheduling of large mixed parallel applications is challenging. Most existing algorithms utilize scheduling heuristics and approximation algorithms to determine a good schedule as basis for an efficient execution in large scale scientific computing. This paper concentrates on the scheduling of mixed parallel applications represented by task graphs with parallel tasks and precedence constraints between them. Layer-based scheduling algorithms for homogeneous target platforms are improved by adding a move-blocks phase that further reduces the resulting parallel runtime.The layer-based scheduling approach is described and the move-blocks algorithm is introduced in detail. The move-blocks extension provides better scheduling results for small as well as for large problems but has only a small increase in runtime.This is shown by a comparison of the modified and the original algorithms over a wide range of test cases.","1066-6192;2377-5750","978-0-7695-3544","10.1109/PDP.2009.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4912917","parallel computing;scheduling;mixed parallel applications","Constraint optimization;Scheduling algorithm;Processor scheduling;Optimal scheduling;Approximation algorithms;Heuristic algorithms;Runtime;Application software;Large-scale systems;Scientific computing","approximation theory;graph theory;natural sciences computing;optimisation;parallel algorithms;processor scheduling","layer-based scheduling heuristics algorithm;large mixed parallel application;precedence constraint;move-block algorithm;approximation algorithm;large scale scientific computing;parallel task graph;homogeneous target platform;optimization","","1","10","","","","","","IEEE","IEEE Conferences"
"An improved Particle Swarm Optimization with adaptive jumps","Hui Wang; Yong Liu; Zhijian Wu; Hui Sun; Sanyou Zeng; Lishan Kang","Department of Computer Sciences and Technology, Nanchang Institute of Technology, Nanchang 330099 China. He also is a master student of School of Computer, China University of Geosciences, Wuhan 430074 China; University of Aizu, Tsuruga, Ikki-machi, Aizu-Wakamatsu, Fukushima 965-8580 Japan; State Key Laboratory of Software Engineering, Wuhan University, 430072 China; Department of Computer Science and Technology, Nanchang Institute of Technology, 330099 China; School of Computer, China University of Geosciences, Wuhan 430074 China; School of Computer, China University of Geosciences, Wuhan 430074 China","2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational Intelligence)","","2008","","","392","397","Particle swarm optimization (PSO) has shown its fast search speed in many complicated optimization and search problems. However, PSO could often easily fall into local optima. This paper presents an improved PSO with adaptive jump. The proposed method combines a novel jump strategy and an adaptive Cauchy mutation operator to help escape from local optima. The new algorithm was tested on a suite of well-known benchmark functions with many local optima. Experimental results were compared with some similar PSO algorithms based on Gaussian distribution and Cauchy distribution, and showed better performance on those test functions.","1089-778X;1941-0026","978-1-4244-1822-0978-1-4244-1823","10.1109/CEC.2008.4630827","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4630827","","Evolutionary computation","particle swarm optimisation","particle swarm optimization;adaptive jump strategy;adaptive Cauchy mutation operator","","5","14","","","","","","IEEE","IEEE Conferences"
"Hiding Cache Miss Penalty Using Priority-based Execution for Embedded Processors","S. Park; A. Shrivastava; Y. Paek","SO&R Research Group, Seoul National University, Korea. shpark@optimizer.snu.ac.kr; CML Research Group, Arizona State University, USA. Aviral.Shrivastava@asu.edu; SO&R Research Group, Seoul National University, Korea. ypaek@snu.ac.kr","2008 Design, Automation and Test in Europe","","2008","","","1190","1195","The contribution of memory latency to execution time continues to increase, and latency hiding mechanisms become ever more important for efficient processor design. While high-end processors can use elaborate techniques like multiple issue, out-of-order execution, speculative execution, value prediction etc. to tolerate high memory latencies, they are often not viable solutions for embedded processors, due to significant area, power and chip complexity overheads. This paper proposes a hardware-software cooperative approach, called priority-based execution to hide cache miss penalty for embedded processors. The compiler classifies the instructions into low-priority and high-priority instructions. The processor executes the high-priority instructions, but delays the execution of low priority instructions. They are executed on a cache miss to hide the cache miss penalty. We empirically evaluate our proposal on the Intel XScale compiler and microarchitecture. Experimental results on benchmarks from Multimedia, MediaBench, MiBench, and SPEC2000 demonstrate an average 17% performance improvements, hiding 75% cache miss penalty.","1530-1591;1558-1101","978-3-9810801-3-1978-3-9810801-4","10.1109/DATE.2008.4484840","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4484840","","Delay;Out of order;Proposals;Microarchitecture;Pipelines;Hardware;Registers;Process design;Costs;Performance analysis","embedded systems;hardware-software codesign;program compilers;storage management","priority-based execution;memory latency;latency hiding mechanism;embedded processor design;hardware-software cooperative approach;cache miss penalty hiding;instruction classification;high priority instruction;low priority instruction;Intel XScale compiler","","1","32","","","","","","IEEE","IEEE Conferences"
"The New Non-Equidistant Optimum GM(1,1) with Modified the nth Components Taken as Initial Value and Its Application Research to Metal Cutting","Y. Luo; L. Li","NA; NA","2009 International Conference on Computational Intelligence and Software Engineering","","2009","","","1","4","Based on grey system theory, a new kind of nonequidistant optimum grey model GM(1,1) with optimizing modified the nth components taken as initial value of response function of grey differential equation and the background value to test data process was proposed, the method of precision inspection was introduced. The example was given. The method can be used for model establishing on equal interval, as well as on non-interval. Moreover, GM(1,1) model's fitting precision and prediction is advanced and the scope of application is enlarged. The model is simple and practical, and has a generalizing value in metal cutting data process.","","978-1-4244-4507","10.1109/CISE.2009.5365734","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5365734","","Mathematical model;Data processing;Predictive models;System testing;Probability;Statistical analysis;Educational institutions;Mechanical engineering;Art;Optimization methods","cutting;differential equations;grey systems;metals;wear","metal cutting;grey system theory;nonequidistant optimum grey model;grey differential equation;wear mechanism","","","11","","","","","","IEEE","IEEE Conferences"
"An Investigation into Concurrency Control Mechanisms in Data Service Layers","K. Goundar; S. Singh; X. F. Ye","NA; NA; NA","14th Asia-Pacific Software Engineering Conference (APSEC'07)","","2007","","","318","325","Service-oriented applications are conceptualised with the notion of efficiently acquiring and processing distributed data. Presently, accessing distributed data can account for up to 70 percent or more of the time spent developing such applications. Hence, one of the first things to be service-enabled in service-oriented architecture is to efficiently access and process data. To avoid hard-coding applications, we recommend the use of a data service layer (DSL) to act as a single point of access to reusable, real-time heterogeneous data. Currently, commercial integration products use two approaches, business information warehouse or virtual data federation. In this paper, we present two novel, proof-of-concept prototype DSLs that combine the two approaches using concurrency control i. e. one with Optimistic concurrency control and another with pessimistic concurrency control. Both approaches are capable of efficiently coordinating client transactions that engage multiple data sources. We also discuss the performance tests carried-out and analyse the results.","1530-1362;1530-1362","0-7695-3057","10.1109/ASPEC.2007.40","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4425870","","Concurrency control;DSL;Service oriented architecture;Business;Information systems;Distributed computing;Application software;Testing;Performance analysis;Semiconductor optical amplifiers","concurrency control;software architecture","optimistic concurrency control;data service layer;distributed data processing;distributed data acquisition;service-oriented architecture;virtual data federation;business information warehouse;pessimistic concurrency control;client transaction","","1","20","","","","","","IEEE","IEEE Conferences"
"Research on Dynamics Modeling and Genetic Algorithm Optimization of Automobile Steer-By-Wire System","Y. Lei-yan; Y. Ping-li; L. Feng","NA; NA; NA","2009 Second International Conference on Information and Computing Science","","2009","3","","78","81","Automobile Steering by Wire (SBW) system is an advanced steering system, which cuts off the mechanical connection between steering wheel and steered wheels. It controls the steering motor to steer front wheels actively so as to improve handling and stabilities and active safety, and controls steering wheel motor to supply the driver with ideal road feel. Driver-vehicle-road closed loop models of SBW are built based on Matlab/Simulink software platform, including steering wheel assembly and driver model. Model parameters are optimized using genetic algorithm. Real car test proves that the model fits experiments well.","2160-7443;2160-7451","978-0-7695-3634","10.1109/ICIC.2009.225","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5168808","#NAME?","Vehicle dynamics;Genetic algorithms;Automobiles;Mathematical model;Wheels;Road safety;Wire;Steering systems;Stability;Assembly","automobiles;genetic algorithms;mechanical engineering computing;steering systems","dynamics modeling;genetic algorithm;optimization;automobile steer-by-wire system;advanced steering system;mechanical connection;steered wheel;steering wheel motor;driver-vehicle-road closed loop model;Matlab/Simulink software platform;steering wheel assembly;driver model","","","7","","","","","","IEEE","IEEE Conferences"
"From Cradle to Sprint: Creating a Full-Lifecycle Request Pipeline at Nationwide Insurance","K. G. Fisher; A. Bankston","NA; NA","2009 Agile Conference","","2009","","","223","228","After a successful transition from a prescriptive waterfall process to Scrum and XP, the Corporate Internet Solutions group at Nationwide Insurance found velocity and efficiency stumbling due to the competing and vague priorities of corporate silos. This presentation discusses how the team evolved the traditional Scrum process to better manage 17 dependent projects, and reluctant internal business partners, through a combination of activities including clear Pre-Discovery activities, scenario planning, RITE usability testing, and kanban-style visual management systems.","","978-0-7695-3768","10.1109/AGILE.2009.72","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5261080","Scrum;Product Management;User Experience;User Centered Design;Business Alignment;Product Owner","Pipelines;Insurance;Companies;Project management;Usability;Web and internet services;Collaboration;Process planning;System testing;Loans and mortgages","kanban;software development management;software houses","full lifecycle request pipeline;waterfall process;Nationwide Insurance;corporate silos;Scrum process;business partners;scenario planning;usability testing;kanban-style visual management system;Corporate Internet Solution","","2","2","","","","","","IEEE","IEEE Conferences"
"Efficient SAT-based techniques for Design of Experiments by using static variable ordering","M. N. Velev; P. Gao","Aries Design Automation, LLC, USA; Aries Design Automation, LLC, USA","2009 10th International Symposium on Quality Electronic Design","","2009","","","371","376","Design of experiments (DOE) is an important problem for ensuring the quality of EDA with applications to the evaluation of techniques and tools in all sub-fields of EDA, e.g., yield and variability optimization, error correcting codes, and software testing. DOE can be formulated as a quasigroup completion problem (QCP). We propose and compare 23 heuristics for efficient solving of QCPs by translation to Boolean satisfiability (SAT) and exploiting static Boolean variable ordering to solve the resulting SAT formulas. This comparison is based on both satisfiable and unsatisfiable instances with varying numbers of empty cells. The translation to SAT is done with the minimal (2-D) and extended (3-D) encodings by Kautz et al. The contributions of the paper include: 1) proposal and comparison of the 23 heuristics; 2) study of the benefits from the 3-D vs. the 2-D encoding, and from local symmetry-breaking constraints, given the static variable ordering heuristics; and 3) identification of the most efficient single heuristic, and portfolios of heuristics that can be run in parallel on multiple cores in a modern CPU. Compared to the default dynamic variable ordering heuristic in the SAT solver, when using static variable-ordering heuristics we achieve an average speedup of 2.8times with the single best heuristic, 7.2times with the best portfolio of two parallel heuristics, 13.6times with the best portfolio of four parallel heuristics, and speedups on individual benchmarks of up to 3 orders of magnitude.","1948-3287;1948-3295","978-1-4244-2952-3978-1-4244-2953","10.1109/ISQED.2009.4810323","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4810323","Boolean Satisfiability (SAT);Design of Experiments (DOE);Quasigroup Completion Problems (QCPs);static variable ordering;statistical design;error correcting codes","US Department of Energy;Portfolios;Electronic design automation and methodology;Application software;Error correction codes;Encoding;Design automation;Design optimization;Software testing;Proposals","Boolean algebra;computability;design of experiments;error correction codes","SAT-based techniques;static variable ordering;design of experiments;error correcting codes;software testing;quasigroup completion problem;Boolean satisfiability;static Boolean variable ordering;local symmetry-breaking constraints;static variable-ordering heuristics;parallel heuristics","","1","41","","","","","","IEEE","IEEE Conferences"
"Adaptive High-End Microprocessor for Power-Performance Efficiency","P. Trancoso","University of Cyprus, Cyprus","9th EUROMICRO Conference on Digital System Design (DSD'06)","","2006","","","221","228","Microprocessor development costs are considerably high. To minimize these costs, manufacturers produce a single design that better satisfies, in average, a wide range of applications. Nevertheless, as applications have different characteristics and users have different demands, this single design is suboptimal in many situations. As a consequence there is a need to design microprocessors that are ""flexible"" and adapt to the different scenarios. This paper presents a simple multiple parameter selection algorithm to achieve high power-performance efficiency gains for each application. The proposed algorithm is tested using detailed simulation and a set of different applications from multimedia, scientific and database workloads. The experimental results show that when the selection algorithm is applied to a baseline configuration optimized for the corresponding workload, the efficiency gains are up to 30%. When the baseline used is optimized for a different workload then the gains may go up by an order of magnitude. The effectiveness of the simple selection algorithm with the proposed optimizations is impressive as the efficiency gains obtained represent 96% of the gains obtained with an ideal parameter selection algorithm","","0-7695-2609","10.1109/DSD.2006.98","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1690043","","Microprocessors;Multimedia databases;Hardware;Costs;Application software;Computer science;Manufacturing;Testing;Process design;Proposals","logic design;logic testing;microprocessor chips;optimisation;power aware computing","adaptive high-end microprocessor;power-performance efficiency;baseline configuration;parameter selection algorithm;optimization","","","22","","","","","","IEEE","IEEE Conferences"
"Establishing work design criteria through a highest expected utility neural network model: An automotive trim case study","R. Quintana; M. T. Leung","University of Texas at San Antonio, Department of Management Science and Statistics, One UTSA Circle, 78249-0632, U.S.A.; University of Texas at San Antonio, Department of Management Science and Statistics, One UTSA Circle, 78249-0632, U.S.A.","2008 IEEE International Conference on Industrial Engineering and Engineering Management","","2008","","","1164","1168","The purpose of this research is two-folded. First, it attempts to maximize productivity and yield as well as to minimize the spatial fixed cost through the development of a work design decision making tool for optimizing work cell configurations and ergonomics. Second, the case study applies influence diagram and neural network to analyze and evaluate work/process design criteria. Practically, the industrial problem is to compare the new stand-up sewing cells against the traditional sit-down sewing layout while taking into consideration of ergonomic effect (repetitive motion injury (RMI) likelihood), floor space (SF), yield (%), and cost ($). The work design decision making tool used in the current study is based on influence diagram run on the HUGIN neural network software. Statistical tests are used to benchmark and validate the experimental results and actual data. Findings suggest that neural network is an effective alternative in solving work design problem.","2157-3611;2157-362X","978-1-4244-2629-4978-1-4244-2630","10.1109/IEEM.2008.4738053","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4738053","Work design;ergonomics;neural network","Utility theory;Neural networks;Automotive engineering;Decision making;Ergonomics;Productivity;Cost function;Design optimization;Process design;Aerospace industry","automotive components;cost reduction;ergonomics;neural nets;occupational safety;production engineering computing;productivity;statistical testing","expected utility neural network model;automotive trim;productivity maximisation;spatial fixed cost minimisation;work design decision making tool;work cell configuration optimization;ergonomics optimization;influence diagram;work design criteria;process design criteria;industrial problem;stand-up sewing cells;sit-down sewing layout;repetitive motion injury likelihood;floor space;HUGIN neural network software;statistical tests","","","12","","","","","","IEEE","IEEE Conferences"
"Optimization in a Health Care System: a Liver Transplantation Example","G. da Gama Torres; R. P. Martins Ferreira; H. P. L. Luna; A. S. Lima","DCC-ICEX-UFMG, Brazil; NA; NA; NA","19th IEEE Symposium on Computer-Based Medical Systems (CBMS'06)","","2006","","","794","799","This text presents a study in modeling a waiting list of liver transplantation, and the use of the resources that affects the system efficiency. The approach uses the graph theory to modeling the system as a workflow. The objective is to know how the system is, and how it will work after changing some parameters. The focus is the planning and the system management. The work encloses aspects of knowledge representation and optimization. An experiment is described and some conclusions are extracted","1063-7125","0-7695-2517","10.1109/CBMS.2006.124","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1647668","","Medical services;Decision theory;Mathematical model;Liver diseases;Transportation;Testing;Computational modeling;Workflow management software;Concrete;Routing","graph theory;health care;knowledge representation;liver;medical information systems;optimisation;surgery","optimization;health care system;liver transplantation;system efficiency;graph theory;system management;knowledge representation","","","10","","","","","","IEEE","IEEE Conferences"
"Systematic Data Structure Exploration of Multimedia and Network Applications realized Embedded Systems","L. Papadopoulos; C. Baloukas; N. Zompakis; D. Soudris","VLSI Design and Testing Center, Department of Electrical and Computer Engineering, Democritus Univ. Thrace, 67100 Xanthi, Greece. lpapadop@ee.duth.gr; VLSI Design and Testing Center, Department of Electrical and Computer Engineering, Democritus Univ. Thrace, 67100 Xanthi, Greece. cmpalouk@ee.duth.gr; VLSI Design and Testing Center, Department of Electrical and Computer Engineering, Democritus Univ. Thrace, 67100 Xanthi, Greece. nzompaki@ee.duth.gr; VLSI Design and Testing Center, Department of Electrical and Computer Engineering, Democritus Univ. Thrace, 67100 Xanthi, Greece. dsoudris@ee.duth.gr","2007 International Conference on Embedded Computer Systems: Architectures, Modeling and Simulation","","2007","","","58","65","In the last years, there is a trend towards network and multimedia applications to be implemented in portable devices. These applications usually contain complex dynamic data structures. The appropriate selection of the dynamic data type (DDT) combination of an application affects the performance and the energy consumption of the whole system. Thus, DDT exploration methodology is used to perform tradeoffs between design factors, such as performance and energy consumption. In this paper we provide a new approach to the DDT exploration procedure, based on a new library of DDTs which remedies the limitations of an existing and allows the DDT optimization of a wide range of application domains. Using the new library, we performed DDT exploration in network and multimedia benchmarks and achieved performance and energy consumption improvements up to 85% and 43% respectively.","","1-4244-1058","10.1109/ICSAMOS.2007.4285734","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4285734","","Data structures;Multimedia systems;Embedded system;Energy consumption;Application software;Information retrieval;Libraries;Memory;Runtime;Very large scale integration","data structures;embedded systems;multimedia computing;telecommunication networks","DDT library;dynamic data type combination;embedded system;network benchmark;multimedia benchmark;systematic data structure exploration","","1","15","","","","","","IEEE","IEEE Conferences"
"A Self-adaptive Ant Colony Algorithm for Phylogenetic Tree Construction","J. Guo; L. Chen; L. Qin; C. Wang","Yangzhou University, Yangzhou, 225009, China; Yangzhou University, Yangzhou, 225009, China; Yangzhou University, Yangzhou, 225009, China; Yangzhou University, Yangzhou, 225009, China","2006 International Conference on Hybrid Information Technology","","2006","1","","78","78","To solve the phylogenetic tree construction problem, a new method using adaptive ant colony algorithm based on the equilibrium of the ant distribution is presented. Before the problem is solved by the developed ant colony optimization, the input species were represented using a fully-connected graph built according to the evolutionary distance between each pair of species. The process of constructing a phylogenetic tree uses a pheromone graph. The information weight of the pheromone graph is adaptively updated according to the pheromone left by ants in their seeking process. The algorithm dynamically adjusts the influence of each ant to the trail information updating and the selected probability of the path according to the equilibrium of the ant distribution. The phylogenetic tree constructing method proposed here is tested using some test cases to compare its results with that of the neighbor-joining (NJ) programs in the PHYLIP software package and the TSP-Approach. Experimental results show that our algorithm is easier to implement and more efficient","","0-7695-2674","10.1109/ICHIT.2006.253468","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021071","","Phylogeny;Software testing;Topology;Chaos;Computer science;Ant colony optimization;Heuristic algorithms;Software packages;Proteins;Equations","biology computing;genetics;optimisation;self-adjusting systems;trees (mathematics)","self-adaptive ant colony algorithm;phylogenetic tree construction;pheromone graph;neighbor-joining programs;ant distribution","","1","14","","","","","","IEEE","IEEE Conferences"
"Concurrent simulation and optimization models for mining planning","M. M. Fioroni; L. A. G. Franzese; T. J. Bianchi; L. Ezawa; L. R. Pinto; G. de Miranda","Paragon Tecnologia, 1435, Clodomiro Amazonas St, 5th floor, São Paulo, 04537-012, Brazil; Paragon Tecnologia, 1435, Clodomiro Amazonas St, 5th floor, São Paulo, 04537-012, Brazil; VALE, Aguas Claras Mine, Ligaçao Av, 3580, Nova Lima, MG, 34000-000, Brazil; VALE, Aguas Claras Mine, Ligaçao Av, 3580, Nova Lima, MG, 34000-000, Brazil; Federal Universty of Minas Gerais, Presidente Antônio Carlos Av, 6627, B. Horizonte, 30161-010, Brazil; Federal Universty of Minas Gerais, Presidente Antônio Carlos Av, 6627, B. Horizonte, 30161-010, Brazil","2008 Winter Simulation Conference","","2008","","","759","767","One of the most important challenges for mining engineers is to correctly analyze and generate short-term planning schedules, or simply month mining plan. The objective is to demonstrate how simulation and optimization models were combined, with simultaneous execution, in order to achieve a feasible, reliable and accurate solution for this problem. A tool based on Arena simulation software and Lingo was developed, tested and approved within VALE (former CVRD Brazil), with excellent results, presented in this paper.","0891-7736;1558-4305","978-1-4244-2707-9978-1-4244-2708","10.1109/WSC.2008.4736138","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4736138","","Read only memory;Ores;Analytical models;Design optimization;Discrete event simulation;Costs;Iron;Floors;Reliability engineering;Software tools","mining;planning;simulation","short-term planning schedules;month mining plan;simulation models;optimization models;Arena simulation software;Lingo;VALE","","6","11","","","","","","IEEE","IEEE Conferences"
"Using an RBF Neural Network to Locate Program Bugs","W. E. Wong; Y. Shi; Y. Qi; R. Golden","NA; NA; NA; NA","2008 19th International Symposium on Software Reliability Engineering (ISSRE)","","2008","","","27","36","We propose an RBF (radial basis function) neural network-based fault localization method to help programmers locate bugs in a more effective way. An RBF neural network with a three-layer feed-forward structure is employed to learn the relationship between the statement coverage of a test case and its corresponding execution result. The trained network is then given as input a set of virtual test cases, each covering only a single statement. The output of the network for each test case is considered to be the suspiciousness of the corresponding statement; a statement with a higher suspiciousness has a higher likelihood of containing a bug. The set of statements ranked in descending order by their suspiciousness are then examined by programmers one by one until a bug is located. Three case studies on different programs (space, grep and make) were conducted with each faulty version having exactly one bug. An additional program gcc was also used to demonstrate the concept of extending the proposed method to programs with multiple bugs. Our experimental data suggest that an RBF neural network-based fault localization method is more effective in locating a program bug (by examining less code before the first faulty statement containing the bug is identified) than another popular method, Tarantula, which also uses the coverage and execution results to compute the suspiciousness of each statement.","1071-9458;2332-6549","978-0-7695-3405","10.1109/ISSRE.2008.15","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700307","fault localization;program debugging;RBF (radial basis function) neural network;suspiciousness of code;successful test;failed test;EXAM score","Neural networks;Computer bugs;Testing;Biological neural networks;Programming profession;Neurons;Feedforward neural networks;Feedforward systems;Computer networks;Artificial neural networks","program debugging;radial basis function networks","program bugs;RBF neural network;radial basis function;fault localization method;three-layer feed-forward structure;Tarantula","","11","44","","","","","","IEEE","IEEE Conferences"
"Functional Electrical Stimulation: A MatLab Based Tool for Designing Stimulation Patterns","S. Dosen; D. B. Popovic","Student Member, IEEE, SMI, Department for Health Science and Technology, Aalborg University, DK; Faculty of Technical Sciences, University of Novi Sad, Serbia. phone: +45 96358828; fax: +45-98154008; e-mail: sdosen@hst.aau.dk; Member, IEEE, SMI, Department for Health Science and Technology, Aalborg University, DK; Faculty of Electrical Engineering, University of Belgrade, Serbia. phone: +45 96358726; fax: 45-98154008; e-mail: dbp@smi.auc.dk","2006 International Conference of the IEEE Engineering in Medicine and Biology Society","","2006","","","5404","5407","We developed user-friendly software that generates stimulation profiles by using user-customized model-based control of walking. The model is a multi-segment structure with pin and ball joints. A pair of an agonist and an antagonistic muscles acts at each joint. Each muscle is modeled by a three-compartment multiplicative model. The control is based on optimization that uses a cost function that minimizes the tracking error of the joint angles and levels of muscles activations. The inputs to the simulation are trajectories and user characteristic model parameters. The outputs of the simulation are levels of muscle activations vs. time. The software allows for interactive testing of various walking trajectories and model parameters since the simulation is integrated into a database of individuals and reference trajectories. The simulation was realized in the MatLab environment with multiple windows graphical user interface. Here we present an example: stimulation patterns for the shank-foot system that is applicable for walking control in hemiplegic individuals","1557-170X","1-4244-0032-51-4244-0033","10.1109/IEMBS.2006.259687","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4463026","","Neuromuscular stimulation;Mathematical model;Muscles;Legged locomotion;Cost function;Error correction;Software testing;Databases;Graphical user interfaces;Control systems","digital simulation;gait analysis;graphical user interfaces;interactive systems;mathematics computing;medical computing;medical control systems;muscle;optimal control;physiological models;position control;tracking","functional electrical stimulation;MatLab;user-friendly software;user-customized model-based control;walking trajectories;multisegment structure;pin joints;ball joints;antagonistic muscles;three-compartment multiplicative model;optimization;cost function;tracking error;muscle activations;multiple windows graphical user interface;shank-foot system;hemiplegic individuals","Algorithms;Biomechanical Phenomena;Computer Simulation;Computers;Electric Stimulation;Hemiplegia;Humans;Models, Statistical;Models, Theoretical;Muscles;Programming Languages;Prostheses and Implants;Software;User-Computer Interface;Walking","3","8","","","","","","IEEE","IEEE Conferences"
"Space-efficient FPGA-accelerated collision detection for virtual prototyping","A. Raabe; S. Hochgurtel; J. Anlauf; G. Zachmann","Tech. Comput. Sci., Bonn Univ., Germany; Tech. Comput. Sci., Bonn Univ., Germany; Tech. Comput. Sci., Bonn Univ., Germany; NA","Proceedings of the Design Automation & Test in Europe Conference","","2006","2","","6 pp.","","We present a space-efficient, FPGA-optimized architecture to detect collisions among virtual objects. The design consists of two main modules, one for traversing a hierarchical acceleration data structure, and one for intersecting triangles. This paper focuses on the former. The design is based on a novel algorithm for testing discretely oriented polytopes for overlap in 3D space. In addition, we derive a new overlap test algorithm that can be implemented using fixed-point arithmetic without producing false negatives and with bounded error. SystemC simulation results on different levels of abstraction show that real-time collision detection of complex objects at rates required by force-feedback and physically-based simulations can be obtained. In addition, synthesis results show that the design can still be fitted into a six-million gates FPGA. Furthermore, we compare our FPGA-based design with a fully parallelized ASIC-targeted architecture and a software implementation","1530-1591;1558-1101","3-9810801-1","10.1109/DATE.2006.243875","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1657140","","Virtual prototyping;Object detection;Testing;Acceleration;Data structures;Algorithm design and analysis;Fixed-point arithmetic;Real time systems;Field programmable gate arrays;Computer architecture","collision avoidance;data structures;field programmable gate arrays;fixed point arithmetic;virtual prototyping","FPGA-accelerated collision detection;virtual prototyping;FPGA-optimized architecture;virtual objects;hierarchical acceleration data structure;3D space;overlap test algorithm;fixed-point arithmetic;SystemC simulation;real-time collision detection;force-feedback simulations;physically-based simulations;ASIC-targeted architecture;software implementation","","5","14","","","","","","IEEE","IEEE Conferences"
"Virtual prototyping increases productivity - A case study","P. Avss; S. Prasant; R. Jain","Infineon Technologies Pvt Limited, Bangalore, India; Infineon Technologies Pvt Limited, Bangalore, India; Infineon Technologies Pvt Limited, Bangalore, India","2009 International Symposium on VLSI Design, Automation and Test","","2009","","","96","101","With the advancement in technology, more and more functionality is being integrated into SoCs. A typical SoC contains one or more micro-controllers, several peripherals and embedded memories. In the software arena, there is a whole lot of embedded software that goes into products, built using these complex SoCs. In this era of consumer driven economy, all the product design groups are under a tremendous pressure to meet the aggressive time-to-market schedules and still deliver the right solution the first time. This creates a need for having a robust product flow, which enables different teams to work simultaneously and coherently. Following are some of the key activities in any product development flow. 1) System Engineering 2) Map customer requirements to design features. 3) Optimize design to meet the requirements in the best possible way. 4) Hardware design 5) Design, develop and integrate different Hardware (HW) or design modules/blocks 6) Develop reference models for validating different modules/blocks/sub-systems 7) Software development 8) Design, develop and integrate different Software (SW) modules 9) Develop reference models for validating these modules/sub-systems 10) System Validation 11) Build a system 12) Port the software onto the system 13) Validate the system with true system scenarios. 14) Customer Delivery. Traditionally, many of these development activities have always been mostly sequential in nature. This type of sequential flow cannot help in meeting the time-to-market requirements of today's consumer products. Some of the most popular alternatives to this kind of sequential development flow include a) FPGA prototyping of the system b) Develop prototype using Instruction Set Simulators (ISS) c) Virtual System Prototyping (VSP). The first option lacks the required flexibility and need to have the complete micro-architecture defined before designing the prototype. Traditional ISS solutions are used for simulating processors with few or no peripherals connected. ISS solutions are therefore used for verifying small portions of the embedded code and not directly suitable for true system simulations. VSP definitely addresses these issues. The concept of VSP is based on creating a software model of the entire hardware system including external components (e.g. base station model for checking the base band systems). This model can be used to explore and analyze different architectures. Once an optimal architecture has been chosen, the same model can be used as an executable specification. HW design teams can use the VSP as a golden reference model against which they can verify the functionality of different modules and/or subsystems in the design. SW teams can use VSP to start their development work, as soon as the architecture is defined and the corresponding VSP is available. As a part of this work, attempt has been made to highlight advantages and challenges of virtual prototyping with a case study.","","978-1-4244-2781-9978-1-4244-2782","10.1109/VDAT.2009.5158104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5158104","","Virtual prototyping;Productivity;Hardware;Embedded software;Time to market;Computer architecture;Product design;Robustness;Product development;Design engineering","time to market;virtual prototyping","virtual system prototyping;productivity;SoC;microcontrollers;embedded memories;embedded software;time-to-market schedules;product development flow;system engineering;reference models;software development;time-to-market requirements;consumer products;sequential development flow;instruction set simulators;simulating processors;embedded code;software model","","2","19","","","","","","IEEE","IEEE Conferences"
"Optimizing Sorting with Machine Learning Algorithms","X. Li; M. J. Garzaran; D. Padua","Department of Electrical and Computer Engineering, University of Delaware, xli@ece.udel.edu; Department of Computer Science, University of Illinois at Urbana-Champaign, garzaran@cs.uiuc.edu; Department of Computer Science, University of Illinois at Urbana-Champaign, padua@cs.uiuc.edu","2007 IEEE International Parallel and Distributed Processing Symposium","","2007","","","1","6","The growing complexity of modern processors has made the development of highly efficient code increasingly difficult. A promising automatic code generation strategy is implemented by library generators. This approach has mainly been applied to scientific codes which can be optimized by identifying code characteristics that depend only on the target machine. In this paper, we study the generation of sorting routines whose performance also depends on the characteristics of the input data. We present two approaches to generate efficient sorting routines. First, we consider the problem of selecting the best ""pure"" sorting algorithm as a function of the characteristics of the input data. We used machine learning algorithms to compute a function for each target machine that, at runtime, is used to select the best algorithm. Our second approach generalizes the first approach and can build new sorting algorithms from a few primitive operations. We use genetic algorithms and a classifier system to build hierarchically-organized hybrid sorting algorithms. Our results show that the algorithms generated using this second approach are quite effective and perform significantly better than the many conventional sorting implementations we tested. In particular, the routines generated using the second approach performs better than the most popular libraries available today: IBM ESSL, INTEL MKL and the C+ + STL.","1530-2075","1-4244-0909-81-4244-0910","10.1109/IPDPS.2007.370499","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4228227","","Sorting;Machine learning algorithms;Signal processing algorithms;Libraries;Spirals;Character generation;Power generation;Tiles;Genetic algorithms;Optimizing compilers","genetic algorithms;learning (artificial intelligence);optimising compilers;software libraries","sorting routines opimization;machine learning algorithms;automatic code generation strategy;library generators;genetic algorithms;classifier system;IBM ESSL library;INTEL MKL library;C++ STL library","","2","11","","","","","","IEEE","IEEE Conferences"
"A two-step approach for solving the flexible job shop scheduling problem","A. Yin; X. Zhao","UFIDA software school of Jiangxi University of Finance and Economics, Nanchang, 330013, China; UFIDA software school of Jiangxi University of Finance and Economics, Nanchang, 330013, China","2009 IEEE International Conference on Granular Computing","","2009","","","716","720","The flexible job shop scheduling problem is one of the most difficult production scheduling problems in industry. This paper presents a two-step approach to solve this problem. In the first step, all the operations are disputed to the machines such that each operation is processed by a certain machine which means that the problem becomes the job shop scheduling problem. In the second step, a quasi-physic and quasi-human algorithm is proposed to solve the job shop scheduling problem. Our approach is tested on several common benchmarks, and the computational experiments show that this two-step approach is quite effective and efficient. Further more, The quasi-physic and quasi-human algorithm is a new idea for solving the job scheduling problem, and it could be a very good basic procedure for designing effective and efficient heuristic algorithm by combining with other heuristic strategy for the problem.","","978-1-4244-4830","10.1109/GRC.2009.5255027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5255027","Flexible Job Shop Scheduling;NP hard;Quasi-Physics;Quasi-Human","Job shop scheduling;Scheduling algorithm;Heuristic algorithms;Educational institutions;Finance;Postal services;Job production systems;Computer industry;Industrial economics;Machinery production industries","combinatorial mathematics;job shop scheduling;optimisation","two-step approach;flexible job shop scheduling problem;quasi-physic algorithm;quasi-human algorithm;hard combinatorial optimization problem","","","8","","","","","","IEEE","IEEE Conferences"
"A scalable auto-tuning framework for compiler optimization","A. Tiwari; C. Chen; J. Chame; M. Hall; J. K. Hollingsworth","University of Maryland, Department of Computer Science, College Park, 20740 USA; University of Utah, School of Computing, Salt Lake City, 84112 USA; University of Southern California, Information Sciences Institute, Marina del Ray, 90292 USA; University of Utah, School of Computing, Salt Lake City, 84112 USA; University of Maryland, Department of Computer Science, College Park, 20740 USA","2009 IEEE International Symposium on Parallel & Distributed Processing","","2009","","","1","12","We describe a scalable and general-purpose framework for auto-tuning compiler-generated code. We combine Active Harmony's parallel search backend with the CHiLL compiler transformation framework to generate in parallel a set of alternative implementations of computation kernels and automatically select the one with the best-performing implementation. The resulting system achieves performance of compiler-generated code comparable to the fully automated version of the ATLAS library for the tested kernels. Performance for various kernels is 1.4 to 3.6 times faster than the native Intel compiler without search. Our search algorithm simultaneously evaluates different combinations of compiler optimizations and converges to solutions in only a few tens of search-steps.","1530-2075","978-1-4244-3751-1978-1-4244-3750","10.1109/IPDPS.2009.5161054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5161054","","Optimizing compilers;Programming profession;Program processors;Application software;Kernel;Tuning;Parallel architectures;Costs;Computer architecture;Software libraries","parallel processing;program compilers","scalable autotuning framework;compiler optimization;autotuning compiler-generated code;CHiLL compiler;parallel search backend;computation kernels;Intel compiler;search algorithm","","60","24","","","","","","IEEE","IEEE Conferences"
"The Application Research of Underwater Acoustic Source Numbers Estimation by Blind Separation Algorithm","H. He; Y. Cang","NA; NA","2009 International Conference on Computational Intelligence and Software Engineering","","2009","","","1","4","The estimation of underwater acoustic source number is the basement of the blind separation algorithm of underwater acoustic signal. By using variance maximization of principal component analysis algorithm, multi-principal components can be extracted. Then three kinds of information theory index: AIC, MDL and Bayesian can be used to define the threshold of the principal component numbers that can be estimated the numbers of the source signal correctly. A series of simulation tests have been done which discussed the estimation accuracy of three kind rules under different signal noise rate. Moreover, the effect of the hydrophone numbers on the source number estimation accuracy also has been discussed. The results show that the algorithm has good noise robust and estimation accuracy in underwater acoustic signal source numbers estimation.","","978-1-4244-4507","10.1109/CISE.2009.5364873","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5364873","","Underwater acoustics;Source separation;Sensor arrays;Signal processing algorithms;Principal component analysis;Bayesian methods;Sonar equipment;Covariance matrix;Underwater communication;Educational institutions","acoustic radiators;blind source separation;hydrophones;information theory;optimisation;principal component analysis;underwater sound","underwater acoustic source numbers estimation;blind separation algorithm;underwater acoustic signal;variance maximization;principal component analysis algorithm;multiprincipal components;information theory index;principal component numbers;simulation tests;hydrophone numbers;source number estimation","","","9","","","","","","IEEE","IEEE Conferences"
"Netlist-level IP protection by watermarking for LUT-based FPGAs","M. Schmid; D. Ziener; J. Teich","Hardware/Software Co-Design, Department of Computer Science, University of Erlangen-Nuremberg, Germany; Hardware/Software Co-Design, Department of Computer Science, University of Erlangen-Nuremberg, Germany; Hardware/Software Co-Design, Department of Computer Science, University of Erlangen-Nuremberg, Germany","2008 International Conference on Field-Programmable Technology","","2008","","","209","216","This paper presents a novel approach to watermark FPGA designs on the netlist level. We restrict the dynamically addressable part of the logic table, thus freeing space for insertion of signature bits into lookup tables (LUTs). In this way, we tightly integrate the watermark with the design so that simply removing mark carrying components would damage the intellectual property core. Converting functional LUTs to LUT-based RAMs or shift registers prevents deletion due to optimization. With this technique, we take watermark carrying components out of the scope of optimization algorithms to achieve complete transparency towards development environments. We can extract the marks from the bitfile of an FPGA. The method was tested on a Xilinx Virtex-II Pro FPGA and showed low overhead in terms of timing and resources at a reasonable number of water-marked cells.","","978-1-4244-2795-6978-1-4244-2796-3978-1-4244-3783","10.1109/FPT.2008.4762385","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4762385","","Protection;Watermarking;Field programmable gate arrays;Table lookup;Hardware;Intellectual property;Security;Computer science;Logic;Shift registers","digital signatures;field programmable gate arrays;industrial property;logic design;optimisation;table lookup;watermarking","LUT-based FPGA;IP protection;watermark FPGA design;netlist level;lookup table;shift register;optimization algorithm;intellectual property;field programmable gate arrays;signature bit","","14","18","","","","","","IEEE","IEEE Conferences"
"Evolution Strategies for Constants Optimization in Genetic Programming","C. L. Alonso; J. L. Montaña; C. E. Borges","NA; NA; NA","2009 21st IEEE International Conference on Tools with Artificial Intelligence","","2009","","","703","707","Evolutionary computation methods have been used to solve several optimization and learning problems. This paper describes an application of evolutionary computation methods to constants optimization in genetic programming. A general evolution strategy technique is proposed for approximating the optimal constants in a computer program representing the solution of a symbolic regression problem. The new algorithm has been compared with a recent linear genetic programming approach based on straight-line programs. The experimental results show that the proposed algorithm improves such technique.","1082-3409;2375-0197","978-1-4244-5619-2978-0-7695-3920","10.1109/ICTAI.2009.35","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5366517","Evolution Strategy;Symbolic Regression;Straight-line Program","Genetic programming;Bismuth;Vectors;Optimization methods;Evolutionary computation;Genetic mutations;Artificial intelligence;Application software;Testing;Algorithm design and analysis","genetic algorithms;regression analysis","constants optimization;evolutionary computation methods;learning problems;computer program;symbolic regression problem;linear genetic programming approach","","1","9","","","","","","IEEE","IEEE Conferences"
"Compiler-Directed Variable Latency Aware SPM Management to CopeWith Timing Problems","O. Ozturk; G. Chen; M. Kandemir; M. Karakoy","Pennsylvania State University; Pennsylvania State University; Pennsylvania State University; Imperial College, UK","International Symposium on Code Generation and Optimization (CGO'07)","","2007","","","232","243","This paper proposes and experimentally evaluates a compiler-driven approach that operates an on-chip scratch-pad memory (SPM) assuming different latencies for the different SPM lines. Our goal is to reduce execution cycles without creating any reliability problems due to variations in access latencies. The proposed scheme achieves its goal by evaluating the reuse of different data items and adopting a reuse and latency aware data-to-SPM placement. It also employs data migration within SPM when it helps to cut down the number of execution cycles further. We also discuss an alternate scheme that can reduce latency of select SPM locations by controlling a circuit level mechanism in software to further improve performance. We implemented our approach within an optimizing compiler and tested its effectiveness through extensive simulations. Our experiments with twelve embedded application codes show that the proposed approach performs much better than the worst-case based design paradigm (16.2% improvement on the average) and comes close (within 5.7%) to an hypothetical best-case design (i.e., one with no process variation) where every SMP locations uniformly have low latency","","0-7695-2764","10.1109/CGO.2007.6","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4145118","","Delay;Scanning probe microscopy;Timing;Circuits;Software performance;CMOS technology;Technology management;Engineering management;Computer science;Pressing","optimising compilers","compiler-directed variable latency aware;SPM management;timing problems;compiler-driven approach;on-chip scratch-pad memory;data reuse;data migration;optimizing compiler","","3","43","","","","","","IEEE","IEEE Conferences"
"Millimeter wave micromachined antenna development","S. K. Koul","Centre for Applied Research in Electronics, Indian Institute of Technology Delhi, India","2008 International Conference on Recent Advances in Microwave Theory and Applications","","2008","","","70","70","In this paper, design and modeling of different types of micromachined patch antennas are described. After initial design, these antenna geometries are optimized using commercial software tools and then fabricated on high resistivity silicon wafers. To start with, a thin dielectric film of silicon nitride or silicon dioxide is deposited on silicon substrates by radio frequency (RF) sputtering. After patterning the antenna conductor on top of the dielectric film, the dielectric membrane is released using bulk-micromachining technique. These antennas are then assembled on a special test jig with K-connectors and tested for their performance characteristics. The measured characteristics of a millimeter wave micromachined circular patch antenna realized on 270 mum thick silicon substrate.","","978-1-4244-2690-4978-1-4244-2691","10.1109/AMTA.2008.4763271","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4763271","","Silicon;Dielectric films;Dielectric substrates;Millimeter wave technology;Patch antennas;Radio frequency;Testing;Millimeter wave measurements;Semiconductor device modeling;Geometry","antenna radiation patterns;dielectric thin films;micromachining;microstrip antennas;millimetre wave antennas;silicon compounds;sputter deposition","millimeter wave micromachined antenna design;optimized antenna geometries;commercial software tools;high resistivity silicon wafers;thin dielectric film;silicon nitride deposition;silicon dioxide deposition;radio frequency sputtering;RF sputtering;antenna conductor;dielectric membrane;bulk-micromachining technique;test jig;antenna assembling;K-connectors;antenna measured characteristics;thick silicon substrate;antennas radiation efficiency;size 270 mum;Si;SiN;SiO<sub>2</sub>","","","","","","","","","IEEE","IEEE Conferences"
"Multi-object optimal design of rapid prototyping based on uniform experiment","L. Xiaoyan","College of Mechanical Engineering, University of Shanghai for Science and Technology, Shanghai 200093, China","Tsinghua Science and Technology","","2009","14","S1","206","211","Experiment of rapid prototyping (RP) is a multi-level and multi-factor problem with long process cycle. As powder-based RP method, three dimensional printing (3DP), and selective laser sintering (SLS) are the most prominent RP methods for their flexibility in material selecting. The research of 3DP process shows that much experiments work can be greatly decreased by the method of uniform design with restricted mixtures for RP products' performance to its forming material. Then, according to backward regression methods, the binomial and inverse terms nonlinear regression equations were set up by statistic analysis and computation. Together with evaluation function method, multi-object optimization model for performance of products to the powder mixtures was developed, and the optimization problem was solved by software MATLAB. After that, the optimum results were solved and tested by experiment with the same condition as before. Differences between the calculation results and testing results, which including product's density, compression strength, surface evaluation, and deformation in three dimensions, are less than 100/0, and have the same error direction. Uniform design method for regression and optimization was proved to be an excellent method to obtain optimum results for multi-level and multi-factor experiments of RP and other process which have a long processing cycle.","1007-0214","","10.1016/S1007-0214(09)70093-8","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6075793","rapid prototyping;three dimensional printing (3DP);uniform design;optimization","Powders;Materials;Optimization;Mathematical model;Surface treatment;Testing;Compounds","","","","","","","","","","","TUP","TUP Journals & Magazines"
"FUMS Technologies for Advanced Structural PHM","H. Azzam; A. Smith; F. Beaven; I. Hebden","Smiths Aerospace - Southampton, School Lane, Chandlers Ford, Eastleigh, Hampshire SO53 4YG, UK. +44(0)2380242008, hesham.azzam@smiths-aerospace.com; Smiths Aerospace - Southampton, School Lane, Chandlers Ford, Eastleigh, Hampshire SO53 4YG, UK. +44(0)2380242000, andrew.j.smith@smiths-aerospace.com; Smiths Aerospace - Southampton, School Lane, Chandlers Ford, Eastleigh, Hampshire SO53 4YG, UK. +44(0)2380242087, frank.beaven@smiths-aerospace.com; BAE SYSTEMS, Warton Aerodrome, Preston, Lancashire, PR4 1AX, UK. +44(0)1772852335, iain.hebden@baesystems.com","2007 IEEE Aerospace Conference","","2007","","","1","12","Over the past seven years, Smiths and BAE SYSTEMS have launched collaborative work to evolve a certifiable practical Structural Prognostic Health Management (SPHM) system. The collaborative work has built on BAE SYSTEMS' vast advanced technology experience and on Smiths' unique experience that has produced intelligent Fleet and Usage Management Software (FUMS<sup>TM</sup> ) including fusion, prognostic and decision support algorithms combining model-based and Artificial Intelligence (AI) techniques. This paper describes the recent advances and optimisation of the Smiths algorithms for damage detection and Operational Load Monitoring (OLM). A combination of FUMS<sup>TM</sup> signal processing and AI techniques have been applied to acoustic emission sensor data to locate and classify damage of different types in composite and metallic structures. The FUMS<sup>TM</sup> damage detection software has been embedded in real-time hardware to support ground tests. Techniques have been implemented to enable adequate calibration of OLM algorithms using data from flight tests. The techniques should address concerns raised about the accuracy of algorithms trained to synthesise strains throughout the entire flight envelope from data recorded close to the edge of the flight envelope. Working with the UK MOD, Smiths has continued the evaluation of FUMS<sup>TM</sup> software that allows aircraft design authorities and military operators to build their force life management applications without the need for software rewriting.","1095-323X","1-4244-0524-61-4244-0525","10.1109/AERO.2007.352908","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4161665","","Prognostics and health management;Signal processing algorithms;Artificial intelligence;Collaborative work;Acoustic signal detection;Military aircraft;Technology management;Collaborative software;Software algorithms;Monitoring","aerospace computing;artificial intelligence;decision support systems;groupware","structural prognostic health management;decision support algorithms;artificial intelligence techniques;Smiths algorithms;operational load monitoring;signal processing;damage detection software;real-time hardware;force life management applications;software rewriting;intelligent fleet and usage management software;aircraft testing;aerospace computing","","1","10","","","","","","IEEE","IEEE Conferences"
"A new sequencing method in Web-based education","L. de-Marcos; J. J. Martinez; J. A. Gutierrez; R. Barchino; J. M. Gutierrez","Computer Science Department, University of Alcalá, Madrid, Spain; Computer Science Department, University of Alcalá, Madrid, Spain; Computer Science Department, University of Alcalá, Madrid, Spain; Computer Science Department, University of Alcalá, Madrid, Spain; Computer Science Department, University of Alcalá, Madrid, Spain","2009 IEEE Congress on Evolutionary Computation","","2009","","","3219","3225","The process of creating e-learning contents using reusable learning objects (LOs) can be broken down in two sub-processes: LOs finding and LO sequencing. Sequencing is usually performed by instructors, who create courses targeting generic profiles rather than personalized materials. This paper proposes an evolutionary approach to automate this latter problem while, simultaneously, encourages reusability and interoperability by promoting standards employment. A model that enables automated curriculum sequencing is proposed. By means of interoperable competency records and LO metadata, the sequencing problem is turned into a constraint satisfaction problem. Particle swarm optimization (PSO) and genetic algorithm (GA) agents are designed, built and tested in real and simulated scenarios. Results show both approaches succeed in all test cases, and that they handle reasonably computational complexity inherent to this problem, but PSO approach outperforms GA.","1089-778X;1941-0026","978-1-4244-2958-5978-1-4244-2959","10.1109/CEC.2009.4983352","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4983352","","Electronic learning;Artificial intelligence;Costs;Genetic algorithms;Particle swarm optimization;Testing;Courseware;Adaptive systems;Assembly;Code standards","computer aided instruction;constraint theory;genetic algorithms;Internet;open systems;particle swarm optimisation;software reusability","Web-based education;e-learning;reusable learning objects;interoperability;constraint satisfaction problem;particle swarm optimization;genetic algorithm;sequencing method","","4","22","","","","","","IEEE","IEEE Conferences"
"Fault-tolerant average execution time optimization for general-purpose multi-processor system-on-chips","M. Vayrynen; V. Singh; E. Larsson","Department of Computer Science, Linköping University, Sweden; Supercomputer Education and Research Centre, Indian Institute of Science, India; Department of Computer Science, Linköping University, Sweden","2009 Design, Automation & Test in Europe Conference & Exhibition","","2009","","","484","489","Fault-tolerance is due to the semiconductor technology development important, not only for safety-critical systems but also for general-purpose (non-safety critical) systems. However, instead of guaranteeing that deadlines always are met, it is for general-purpose systems important to minimize the average execution time (AET) while ensuring fault-tolerance. For a given job and a soft (transient) error probability, we define mathematical formulas for AET that includes bus communication overhead for both voting (active replication) and rollback-recovery with checkpointing (RRC). And, for a given multi-processor system-on-chip (MPSoC), we define integer linear programming (ILP) models that minimize AET including bus communication overhead when: (1) selecting the number of checkpoints when using RRC, (2) finding the number of processors and job-to-processor assignment when using voting, and (3) defining fault-tolerance scheme (voting or RRC) per job and defining its usage for each job. Experiments demonstrate significant savings in AET.","1530-1591;1558-1101","978-1-4244-3781-8978-3-9810-8015","10.1109/DATE.2009.5090713","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5090713","","Fault tolerant systems;System-on-a-chip;Error probability;Voting;Circuit faults;Fault tolerance;Checkpointing;Integer linear programming;Production;Fault detection","checkpointing;fault tolerance;integer programming;linear programming;microprocessor chips;probability;safety-critical software;system buses;system-on-chip","fault-tolerant average execution time optimization;general-purpose multiprocessor system-on-chips;semiconductor technology development;safety-critical systems;general-purpose systems;fault-tolerance;soft error probability;bus communication overhead;voting active replication;rollback-recovery with checkpointing;integer linear programming models;job-to-processor assignment","","6","12","","","","","","IEEE","IEEE Conferences"
"Electronic Logbook for Space System Integration & Test Operations","A. T. Kavelaars; E. Bloom; R. Claus; K. Fouts; S. Tuvi","GMV Space Systems; NA; NA; NA; Stanford Linear Accelerator","IEEE Transactions on Aerospace and Electronic Systems","","2009","45","1","167","178","In the highly technological aerospace world, paper is still widely used to document space system integration and test (I&T) operations. E-logbook (electronic logbook) is a new technology designed to replace current documentation processes of space system I&T operations, such as connector mate and demate, flight hardware and flight software component installation, material mixes or electronic ground support equipment (EGSE) validation. It also includes new documentation concepts, such as the shift log, which improves project awareness and optimizes the shift hand-over process, and the configuration log, which instantly reports on the global I&T state of the space system and greatly enhances information gathering prior to major test events or project reviews. The design of E-logbook focuses not only on a reliable and efficient relational database, but also on an ergonomic human-computer interactive (HCI) system of graphical user interfaces (GUI) that can help reduce human error and improve I&T discipline and management oversight. E-Logbook has been used for the I&T of the large area telescope (LAT) of the gamma-ray large area space telescope (GLAST) scientific satellite at the Stanford Linear Accelerator Center (SLAC). After 19 months of operation, more than 41,000 records have been created for the different documentation components or I&T logs, with no data having been corrupted or critically lost. 94% of the operators and 100% of the management exposed to E-logbook prefer it to paper logbooks and recommend its use in the aerospace industry.","0018-9251;1557-9603;2371-9877","","10.1109/TAES.2009.4805271","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4805271","","Electronic equipment testing;System testing;Space technology;Aerospace testing;Documentation;Aerospace materials;Aerospace electronics;Graphical user interfaces;Telescopes;Paper technology","aerospace computing;document handling;graphical user interfaces;human computer interaction;relational databases","electronic logbook;space system integration;integration and test operations;electronic ground support equipment;component installation;shift log;configuration log;human-computer interactive system;graphical user interfaces;GUI;large area telescope;gamma-ray large area space telescope;Stanford Linear Accelerator Center","","2","14","","","","","","IEEE","IEEE Journals & Magazines"
"Scheduling Timed Modules for Correct Resource Sharing","C. Seceleanu; P. Pettersson; H. Hansson","NA; NA; NA","2008 1st International Conference on Software Testing, Verification, and Validation","","2008","","","102","111","Real-time embedded systems typically include concurrent tasks of different priorities with time-dependent operations accessing common resources. In this context, unsynchronized parallel executions may lead to hazard situations caused by e.g., race conditions. To be able to detect such faulty system behaviors before implementation, we introduce a unified model of resource constrained, scheduled real-time system descriptions, in Alur's and Henzinger's rigorous framework of timed reactive modules. We take a component-based design perspective and construct the realtime system model, by refinement, as a composition of realtime periodic preemptible tasks with encoded functionality, and a fixed-priority scheduler, all modeled as timed modules. For the model, we express the notions of race condition and redundant locking, formally, as invariance properties that can be verified by model-checking.","2159-4848","978-0-7695-3127","10.1109/ICST.2008.70","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4539537","real-time systems;component-based design;timed modules;resource management","Resource management;Real time systems;Scheduling;Embedded system;Protocols;Delay;Concurrent computing;Timing;Encoding;Software testing","embedded systems;parallel processing;peer-to-peer computing;program verification","resource sharing;real-time embedded systems;time-dependent operations;unsynchronized parallel executions;resource constrained unified model;component-based design;fixed-priority scheduler;invariance properties;model-checking verification","","","20","","","","","","IEEE","IEEE Conferences"
"On Chip Instrument application to SoC analysis","N. Stollon","HDL Dynamics, USA","2008 IEEE International High Level Design Validation and Test Workshop","","2008","","","74","74","Complex SoC, including multicore technology is increasing the need for embedded instruments in ASSP, ASIC, and FPGA. This is recognized need in industry with many competing design for debug (DfD) approaches and solutions being proposed and implemented. On Chip Instruments, a hardware based method of inserting data gathering and analysis IP into a SoC, is one of the cornerstones of DfD.","1552-6674","978-1-4244-2922","10.1109/HLDVT.2008.4695879","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4695879","","Instruments;Hardware;Performance analysis;System-on-a-chip;Design for disassembly;Information analysis;Design optimization;Software performance;Control systems;Application specific integrated circuits","field programmable gate arrays;system-on-chip","on chip instrument;SoC;multicore technology;ASIC;FPGA;design for debug","","","","","","","","","IEEE","IEEE Conferences"
"A hardware implementation of layer 2 MPLS","R. Peterkin; D. Ionescu","Sch. of Inf. Technol. & Eng., Ottawa Univ., Ont., Canada; Sch. of Inf. Technol. & Eng., Ottawa Univ., Ont., Canada","Third IEEE International Workshop on Electronic Design, Test and Applications (DELTA'06)","","2006","","","2 pp.","404","This paper presents a hardware architecture for layer 2 Multi Protocol Label Switching (MPLS). MPLS is a protocol framework used primarily to prioritize internet traffic and improve bandwidth utilization. Furthermore it increases the performance of internet applications and overall efficiency. However, most existing MPLS solutions are entirely software based which decreases performance. MPLS performance can be enhanced by executing core tasks in hardware while allowing other tasks to be executed in software to guard against performance degradation. This paper proposes a hardware design of MPLS on an FPGA for increased performance and efficiency.","","0-7695-2500","10.1109/DELTA.2006.3","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1581247","","Hardware;Multiprotocol label switching;Protocols;Internet;Software performance;Computer architecture;Bandwidth;Application software;Degradation;Field programmable gate arrays","multiprotocol label switching;Internet;field programmable gate arrays","hardware implementation;layer 2 MPLS;multi protocol label switching;protocol framework;Internet traffic;bandwidth utilization;hardware design;FPGA","","","","","","","","","IEEE","IEEE Conferences"
"Broadband Characterization of GaN Transistors for Software Defined Radio Power Amplifier Applications","H. Patterson; F. Scarpitto; B. Bielick","General Dynamics C4 Systems, Scottsdale, AZ; General Dynamics C4 Systems, Scottsdale, AZ; General Dynamics C4 Systems, Scottsdale, AZ","MILCOM 2007 - IEEE Military Communications Conference","","2007","","","1","7","A technique for applying a series of narrowband load pull measurements of a Gallium Nitride (GaN) transistor to the design of a broadband power amplifier (PA) is presented with potential application to software defined radio (SDR). Load pull uses computer controlled tuners to apply a series of impedances at discrete frequencies to the device under test (DUT) and then calculates performance contours such as gain, efficiency, output power, etc. The basics of load pull testing and its application to device characterization for PA design are reviewed. The operation of a load pull system is presented, along with test fixture design to accommodate broad band measurements. A GaN device is measured and contours of output power and efficiency are determined for each frequency. These results are interconnected to obtain an impedance locus vs. frequency from which an output matching network is designed. The technique is applied to the design of a 500 MHz to 2500 MHz single-stage GaN PA with a saturated output power of +41 dBm. It achieves a minimum efficiency of 35% at nominal drain voltage and achieves a minimum efficiency of 46% when subjected to drain voltage optimization over frequency.","2155-7578;2155-7586","978-1-4244-1512-0978-1-4244-1513","10.1109/MILCOM.2007.4455135","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4455135","","Gallium nitride;Software radio;Broadband amplifiers;Power amplifiers;Application software;Frequency;Power generation;Power measurement;Impedance;Testing","","","","2","7","","","","","","IEEE","IEEE Conferences"
"Improving testability and soft-error resilience through retiming","S. Krishnaswamy; I. L. Markov; J. P. Hayes","IBM T.J. Watson Research Center, Rt. 134, Yorktown Heights, NY 10598, USA; University of Michigan, EECS Department, Ann Arbor, 41809, USA; University of Michigan, EECS Department, Ann Arbor, 41809, USA","2009 46th ACM/IEEE Design Automation Conference","","2009","","","508","513","State elements are increasingly vulnerable to soft errors due to their decreasing size, and the fact that latched errors cannot be completely eliminated by electrical or timing masking. Most prior methods of reducing the soft error rate (SER) involve combinational redesign, which tends to add area and decrease testability, the latter a concern due to the prevalence of manufacturing defects. Our work explores the fundamental relations between the SER of sequential circuits and their testability in scan mode, and appears to be the first to improve both through retiming. Our retiming methodology relocates registers so that [1] registers become less observable with respect to primary outputs, thereby decreasing overall SER, and [2] combinational nodes become more observable with respect to registers (but not with respect to primary outputs), thereby increasing scan testability. We present experimental results which show an average decrease of 42% in the SER of latches, and an average improvement of 31% random pattern testability.","0738-100X","978-1-6055-8497","10.1145/1629911.1630043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5227049","Testability;Soft Errors;Retiming","Resilience;Circuit testing;Registers;Logic testing;Sequential analysis;Timing;Sequential circuits;Logic design;Manufacturing;Algorithm design and analysis","circuit reliability;circuit testing;combinational circuits;hardware-software codesign;network synthesis;optimising compilers;sequential circuits","state elements;soft errors;retiming;testability improvement;sequential circuits;registers","","5","21","","","","","","IEEE","IEEE Conferences"
"Data Acquisition of Cylinder Combustion Pressure Oriented to Diesel Engine Control","J. Wang; Y. Zhang; Q. Xiong; X. Ding","NA; NA; NA; NA","2009 IEEE Circuits and Systems International Conference on Testing and Diagnosis","","2009","","","1","4","Increasing demand of electronic control and operation condition monitor in diesel engine require to decrease acquisition system for on-line diagnosis and optimize system configure, it is presented the acquisition method including data process and data communication, on this basis, acquisition system oriented to diesel engine control is developed, hardware and software of this system is designed: Fiber Pressure Sensor (FPS) circuit and Digital Signal Processor (DSP), Universal Serial Bus(USB) transmitting circuit and other procedure programming. Bench test of this system is made in four-cylinder diesel engine, its acquisition performance and stability are analyzed, test result is shown that acquisition system can meet the basic demand of diesel engine.","2324-8475;2324-8491","978-1-4244-2587","10.1109/CAS-ICTD.2009.4960795","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4960795","","Data acquisition;Engine cylinders;Combustion;Diesel engines;Pressure control;Communication system control;Control systems;Circuit testing;System testing;Condition monitoring","combustion;condition monitoring;data acquisition;diesel engines;digital signal processing chips;engine cylinders;peripheral interfaces;pressure sensors","data acquisition;cylinder combustion pressure;diesel engine control;electronic control;operation condition monitoring;fiber pressure sensor circuit;digital signal processor;universal serial bus transmitting circuit;four-cylinder diesel engine","","","5","","","","","","IEEE","IEEE Conferences"
"Transparent Risk-free Synchronization in the High-Level-Architecture Interoperability Standard","F. Quaglia; A. Santoro","Dipartimento di Informatica e Sistemistica, Università di Roma "La Sapienza"; Dipartimento di Informatica e Sistemistica, Università di Roma "La Sapienza"","2007 12th IEEE Symposium on Computers and Communications","","2007","","","995","1000","The high-level-architecture (HLA) is an IEEE standard for the interoperability and integration of (autonomous) simulation packages and applications (termed federates in the HLA context). It is based on a middleware-level component referred to as run-time-infrastructure (RTI) offering a set of interoperability services to the overlying simulation software. Time-management is the suite of services allowing synchronized execution among the federates, which, according to the HLA specification, covers pure conservative and pure optimistic synchronization schemes. In this paper we provide the design and implementation of a software layer, we refer to as risk-free-speculator (RFS), which supports an optimistic oriented intermediate approach to synchronization embedding the aggressiveness property of optimistic systems, but discarding risk. This is done in a totally transparent manner to the overlying applications, and does not even require any modification of the underlying RTI. The effectiveness of RFS has been tested against simulated demonstration exercises using the Joint Semi-Automated Forces (JSAF) simulation program.","1530-1346","978-1-4244-1520-5978-1-4244-1521","10.1109/ISCC.2007.4381588","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4381588","","Context modeling;Application software;Computational modeling;Packaging;Embedded software;Design optimization;Middleware;System testing;Modems;Manufacturing processes","digital simulation;middleware;open systems;synchronisation","transparent risk-free synchronization;high-level-architecture;IEEE interoperability standard;middleware-level component;run-time-infrastructure;time-management;risk-free-speculator;RFS software layer;Joint Semi-Automated Forces;JSAF simulation program","","","19","","","","","","IEEE","IEEE Conferences"
"A Fast Adaptive Memetic Algorithm for Online and Offline Control Design of PMSM Drives","A. Caponio; G. L. Cascella; F. Neri; N. Salvatore; M. Sumner","NA; NA; NA; NA; NA","IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)","","2007","37","1","28","41","A fast adaptive memetic algorithm (FAMA) is proposed which is used to design the optimal control system for a permanent-magnet synchronous motor. The FAMA is a memetic algorithm with a dynamic parameter setting and two local searchers adaptively launched, either one by one or simultaneously, according to the necessities of the evolution. The FAMA has been tested for both offline and online optimization. The former is based on a simulation of the whole system-control system and plant-using a model obtained through identification tests. The online optimization is model free because each fitness evaluation consists of an experimental test on the real motor drive. The proposed algorithm has been compared with other optimization approaches, and a matching analysis has been carried out offline and online. Excellent results are obtained in terms of optimality, convergence, and algorithmic efficiency. Moreover, the FAMA has given very robust results in the presence of noise in the experimental system","1083-4419;1941-0492","","10.1109/TSMCB.2006.883271","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4067068","Electric drives;evolutionary algorithms;local search;memetic algorithms;synchronous motors","Programmable control;Adaptive control;Control design;Algorithm design and analysis;Optimal control;Synchronous motors;Heuristic algorithms;System testing;Motor drives;Convergence","control system synthesis;machine control;machine testing;optimal control;permanent magnet motors;synchronous motor drives","fast adaptive memetic algorithm;online control design;offline control design;optimal control system;permanent-magnet synchronous motor drive;optimization;convergence","Algorithms;Artificial Intelligence;Biomimetics;Computer Simulation;Computer-Aided Design;Equipment Failure Analysis;Models, Theoretical;Online Systems;Robotics;Software;Systems Theory;Transducers;User-Computer Interface","145","49","","","","","","IEEE","IEEE Journals & Magazines"
"An analytic model and optimization technique based methods for fault diagnosis in power systems","Zhiwei Liao; Fushuan Wen; Wenxin Guo; Xiangzhen He; Wei Jiang; Taifu Dong; Junhui Liang; Binghua Xu","Department of Electrical Engineering, South China University of Technology, Guangzhou 510641, China; Department of Electrical Engineering, South China University of Technology, Guangzhou 510641, China; Department of Electrical Engineering, South China University of Technology, Guangzhou 510641, China; Department of Electrical Engineering, South China University of Technology, Guangzhou 510641, China; Department of Electrical Engineering, South China University of Technology, Guangzhou 510641, China; Department of Electrical Engineering, South China University of Technology, Guangzhou 510641, China; Guangdong Power Dispatching Center, Guangzhou 510620, China; Shanghai Extra High Voltage Power Transmission Company, 200063, China","2008 Third International Conference on Electric Utility Deregulation and Restructuring and Power Technologies","","2008","","","1388","1393","An analytic model for fault diagnosis of power system using optimization technique is expressed as unconstrained 0-1 integer programming problem, and consequently faulty equipment identification can be solved by refined mathematical operation. Considering the configuration of automatic devices in modern power systems, such as protective relays and reclosing relays, an improved analytic model and optimization technique-based method for fault diagnosis of power system is proposed in this paper. The evaluation criteria of the presented model is improved considering the relationship of multiple main protective relays, backup protective relays, malfunctioning protective relays and reclosing relays. Improvements of analytic model for fault diagnosis of electric power system based on optimization techniques are presented firstly. A brief description about the modulars and functions of the online fault diagnosis software which is developed by the authors for Jiangsu Provincial Power Company is given. The adopted EMS data acquisition method and simulated online test results for the power system of Jiangsu Power Company are described.","","978-7-900714-13","10.1109/DRPT.2008.4523623","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4523623","power system;fault diagnosis;analytic model;optimization technique","Power system modeling;Optimization methods;Fault diagnosis;Power system analysis computing;Power system faults;Power system relaying;Power system protection;Protective relaying;Power system simulation;Mathematical model","fault diagnosis;integer programming;power system analysis computing;power system faults;power system protection;power system relaying;relay protection","optimization technique;fault diagnosis;power systems;unconstrained integer programming problem;faulty equipment identification;protective relays;reclosing relays;Jiangsu Provincial Power Company;EMS data acquisition method","","2","8","","","","","","IEEE","IEEE Conferences"
"Calysto","D. Babic; A. Hu","University of British Columbia, Vancouver, BC, Canada; University of British Columbia, Vancouver, BC, Canada","2008 ACM/IEEE 30th International Conference on Software Engineering","","2008","","","211","220","Automatically detecting bugs in programs has been a long-held goal in software engineering. Many techniques exist, trading-off varying levels of automation, thoroughness of coverage of program behavior, precision of analysis, and scalability to large code bases. This paper presents the Calysto static checker, which achieves an unprecedented combination of precision and scalability in a completely automatic extended static checker. Calysto is interprocedurally path-sensitive, fully context-sensitive, and bit-accurate in modeling data operations - comparable coverage and precision to very expensive formal analyses - yet scales comparably to the leading, less precise, static-analysis-based tool for similar properties. Using Calysto, we have discovered dozens of bugs, completely automatically, in hundreds of thousands of lines of production, open-source applications, with a very low rate of false error reports. This paper presents the design decisions, algorithms, and optimizations behind Calysto's performance.","0270-5257;1558-1225","978-1-4244-4486-1978-1-60558-079","10.1145/1368088.1368118","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4814132","formal verification;static analysis;static checking","Scalability;Computer bugs;Automation;Software testing;Formal verification;Java;Computer science;Permission;Software engineering;Context modeling","program debugging;program diagnostics;program verification","scalable extended static checking;precise extended static checking;bugs detection;software engineering;program behavior;analysis precision;Calysto static checker;automatic extended static checker;data operations;formal analysis;static analysis;open source application","","26","38","","","","","","IEEE","IEEE Conferences"
"An Optimized Message Passing Framework for Parallel Implementation of Signal Processing Applications","S. Saha; J. Schlessman; S. Puthenpurayil; S. S. Bhattacharyya; W. Wolf","Department of Electrical and Computer Engineering and Institute for Advanced Computer Studies, University of Maryland, College Park, USA, 20742. ssaha@umd.edu; Department of Electrical Engineering, Princeton University, Princeton, USA, 08554. jschless@princeton.edu; Department of Electrical and Computer Engineering and Institute for Advanced Computer Studies, University of Maryland, College Park, USA, 20742. purayil@umd.edu; Department of Electrical and Computer Engineering and Institute for Advanced Computer Studies, University of Maryland, College Park, USA, 20742. ssb@umd.edu; Electrical and Computer Engineering Department, Georgia Institute of Technology, Atlanta, USA, 30332. wayne.wolf@ece.gatech.edu","2008 Design, Automation and Test in Europe","","2008","","","1220","1225","Novel reconfigurable computing platforms enable efficient realizations of complex signal processing applications by allowing exploitation of parallelization resulting in high throughput in a cost-efficient way. However, the design of such systems poses various challenges due to the complexities posed by the applications themselves as well as the heterogeneous nature of the targeted platforms. One of the most significant challenges is communication between the various computing elements for parallel implementation. In this paper, we present a communication interface, called the signal passing interface (SPI), that attempts to overcome this challenge by integrating relevant properties of two different yet important paradigms in this context - dataflow and the message passing interface (MPI). SPI is targeted towards signal processing applications and, due to its careful specialization, more performance-efficient for their embedded implementation. It is also more easier and intuitive to use. Earlier, a preliminary version of SPI was presented [12] which was restricted to static dataflow behavior. Here, we present a more complete version of SPI with new features to address both static and dynamic dataflow behavior, and to provide new optimization techniques. We develop a hardware description language (HDL) realization of the SPI library, and demonstrate its functionality on the Xilinx Virtex-4 FPGA. Details of the HDL-based SPI library along with experiments with two signal processing applications on the FPGA are also presented.","1530-1591;1558-1101","978-3-9810801-3-1978-3-9810801-4","10.1109/DATE.2008.4484845","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4484845","","Message passing;Signal processing;Application software;Field programmable gate arrays;Concurrent computing;Hardware design languages;Libraries;Protocols;Time factors;Educational institutions","application program interfaces;digital signal processing chips;embedded systems;field programmable gate arrays;hardware description languages;logic CAD;message passing;parallel processing","message passing interface;parallel implementation;reconfigurable computing;communication interface;signal passing interface;static dataflow behavior;dynamic dataflow behavior;optimization techniques;hardware description language;HDL;SPI library;Xilinx Virtex-4 FPGA;MPI;embedded implementation","","1","15","","","","","","IEEE","IEEE Conferences"
"Optimizing the Supplier Selection and Service Portfolio of a SOA Service Integrator","J. C. Lang; T. Widjaja; P. Buxmann; W. Domschke; T. Hess","NA; NA; NA; NA; NA","Proceedings of the 41st Annual Hawaii International Conference on System Sciences (HICSS 2008)","","2008","","","89","89","The service-oriented architecture (SOA) paradigm promises to enable software vendors to compose software systems using services purchased from various suppliers. In this paper, we analyze the impact of the SOA paradigm on the structure of the value chain in the software industry. We consider a new actor, the SOA service integrator (Sel), who integrates services from various suppliers in his portfolio in addition to own services in order to be able to fulfill heterogeneous functionality requirements of customers. His decision problem of selecting suppliers and deciding which services to integrate is modeled as a linear 0-1 programming model with a profit maximization objective. The model incorporates service make-or-buy decisions, and considers various cost types, namely the costs of establishing supplier relationships, integrating and using services. Based on this model and generated test problems, we analyze the optimal supplier selection and service portfolio under various cost scenarios.","1530-1605","978-0-7695-3075","10.1109/HICSS.2008.338","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4438792","","Portfolios;Service oriented architecture;Costs;Enterprise resource planning;Companies;Management information systems;Computer industry;Semiconductor optical amplifiers;Operations research;Software systems","software architecture","supplier selection;service portfolio;service-oriented architecture paradigm;software industry value chain;service integrator;heterogeneous functionality requirements;profit maximization objective;optimal supplier selection;cost scenarios","","1","46","","","","","","IEEE","IEEE Conferences"
"Artificial Immune Systems and Differential Evolution Based Approaches Applied to Multi-Stage Transmission Expansion Planning","L. S. Rezende; A. M. Leite da Silva; L. M. Honorio","NA; NA; NA","2009 15th International Conference on Intelligent System Applications to Power Systems","","2009","","","1","6","Transmission expansion planning (TEP) is a complex optimization task to ensure that the power system will meet the demand in an adequate quality level to customers along the planning horizon, while minimizing investment, operational, and interruption costs. Optimization approaches based on metaheuristics have demonstrated a good potential to find high quality solutions. Their success is related to the ability to avoid local optima by exploring the basic structure of each problem. Numerous advantages can be linked to these tools: a simple software complexity, an ability to mix integer and non-integer variables, and a faster time-response. This paper presents a performance comparison between two optimization tools based on artificial immune systems and differential evolution to solve the multi-stage TEP problem. The proposed methodology includes the search for the least cost solution, bearing in mind investments and operational costs related to ohmic transmission losses. The multi-stage nature of the TEP is also taken into consideration. Case studies on a small test system and on a real sub-transmission network are presented and discussed.","","978-1-4244-5097-8978-1-4244-5098","10.1109/ISAP.2009.5352957","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5352957","","Artificial immune systems;Power system planning;Meeting planning;Investments;Cost function;Power generation;Propagation losses;Ant colony optimization;Uncertainty;Software tools","artificial immune systems;evolutionary computation;optimisation;power transmission planning","artificial immune systems;differential evolution approach;multistage transmission expansion planning;power system;optimization approach;metaheuristics;software complexity;multistage TEP problem;ohmic transmission losses;subtransmission network","","2","18","","","","","","IEEE","IEEE Conferences"
"Reliable multiprocessor system-on-chip synthesis","C. Zhu; Z. Gu; R. P. Dick; L. Shang","ECE Department, Queen's University, Kingston, ON K7L 3N6, Canada; EECS Department, Northwestern University, Evanston, IL 60208, U.S.A.; EECS Department, Northwestern University, Evanston, IL 60208, U.S.A.; ECE Department, Queen's University, Kingston, ON K7L 3N6, Canada","2007 5th IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS)","","2007","","","239","244","This article presents a multiprocessor system-on-chip synthesis (MPSoC) algorithm that optimizes system mean time to failure. Given a set of directed acyclic periodic graphs of communicating tasks, the proposed algorithm determines a processor core allocation, level of system-level and processor-level structural redundancy, assignment of tasks to processors, floorplan, and schedule in order to minimize system failure rate and area while meeting functionality and timing constraints. Changes to the thermal profile resulting from changes in allocation, assignment, scheduling, and floorplan are modeled and optimized during synthesis, as is the impact of thermal profile on temperature-dependent failure mechanisms. The proposed techniques have the potential to substantially increase MPSoC system mean time to failure compared to area-optimized solutions. If power densities are high and the dominant lifetime failure mechanisms are strongly dependent on temperature, our results indicate that thermal and structural redundancy optimization during synthesis have the potential to greatly increase MPSoC lifetime with low area cost.","","978-1-5959-3824-4978-1-5959-3824-4978-1-5959-3824","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5753848","","Optimization;Algorithm design and analysis;Redundancy;Thermal analysis;Benchmark testing;Mathematical model","circuit layout;directed graphs;multiprocessing systems;processor scheduling;redundancy;system recovery;system-on-chip","multiprocessor system-on-chip synthesis;directed acyclic periodic graph;processor core allocation;system level structural redundancy optimisation;processor level structural redundancy optimisation;thermal profile;temperature dependent failure mechanism;MPSoC system;power density","","","23","","","","","","IEEE","IEEE Conferences"
"POWER: Planning and Deployment Platform for Wireless Sensor Networks","J. Li; Y. Bai; H. Ji; J. Ma; Y. Tian; D. Qian","Beihang University, China; Beihang University, China; Beihang University, China; Handan Polytechnic College, China; Beihang University, China; Beihang University, China","2006 Fifth International Conference on Grid and Cooperative Computing Workshops","","2006","","","432","436","Impeded by the bottleneck of the information collection in the information chain, researchers are more and more interested in the development of wireless sensor networks; the applications of wireless sensor networks are growing too. But less attention are paid to wireless sensor networks planning. To resolve deployment challenges, reduce the deployment risk, we propose to build the planning and deployment platform for wireless sensor networks. POWER is a software environment for planning and deploying wireless sensor networks applications into actual environment. POWER has three main parts, network deployment, simulation, and performance evaluation and optimization. Simulation is the foundation of the POWER. Lifetime is the most important performance evaluation metrics. The target of the POWER is to supply an integrated and optimal deployment solution for an actual application","","0-7695-2695","10.1109/GCCW.2006.73","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4031587","","Wireless sensor networks;Routing protocols;Media Access Protocol;Monitoring;Application software;Computer networks;Educational institutions;Impedance;Power supplies;Testing","performance evaluation;telecommunication computing;telecommunication network planning;wireless sensor networks","wireless sensor networks planning;POWER software environment;network deployment;network simulation;network optimization;performance evaluation metrics","","3","15","","","","","","IEEE","IEEE Conferences"
"Seesaw method for combining parameter estimates","J. C. Spall","Appl. Phys. Lab., Johns Hopkins Univ., Laurel, MD, USA","2006 American Control Conference","","2006","","","6 pp.","","This paper introduces a ""seesaw"" scheme for parameter estimation where the overall parameter vector is divided into two parts. The estimate of the overall vector is formed by oscillating between the estimates for each of the two parts. A fundamental advantage of such a scheme is the preservation of potentially large investments in software while allowing for an extension of capability to include new vector for estimation. A specific case of interest involves cross-sectional data where there is interest in estimating the mean vector and covariance matrix of the initial state vector as well as certain parameters associated with the dynamics of the underlying differential equations (e.g., power spectral density parameters). This paper shows that under reasonable conditions the seesaw scheme will converge to the joint estimate for the full vector of unknown parameters","0743-1619;2378-5861","1-4244-0209-31-4244-0210","10.1109/ACC.2006.1657540","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1657540","","Parameter estimation;Convergence;Optimization methods;State estimation;Covariance matrix;System identification;Sun;Software testing;System testing;Software systems","covariance matrices;differential equations;parameter estimation","seesaw method;parameter estimation;parameter vector;mean vector;covariance matrix;initial state vector;differential equations;power spectral density parameters;system identification;optimization;state-space model","","2","19","","","","","","IEEE","IEEE Conferences"
"Toward Developing Genetic Algorithms to Aid in Critical Infrastructure Modeling","M. R. Permann","Idaho National Laboratory, Idaho Falls, Idaho, may.permann@inl.gov","2007 IEEE Conference on Technologies for Homeland Security","","2007","","","192","197","Today's society relies upon an array of complex national and international infrastructure networks such as transportation, telecommunication, financial and energy. Understanding these interdependencies is necessary in order to protect our critical infrastructure. The Critical Infrastructure Modeling System, CIMS<sup>copy</sup>, examines the interrelationships between infrastructure networks. CIMS<sup>copy</sup> development is sponsored by the National Security Division at the Idaho National Laboratory (INL) in its ongoing mission for providing critical infrastructure protection and preparedness. A genetic algorithm (GA) is an optimization technique based on Darwin's theory of evolution. A GA can be coupled with CIMS<sup>copy</sup> to search for optimum ways to protect infrastructure assets. This includes identifying optimum assets to enforce or protect, testing the addition of or change to infrastructure before implementation, or finding the optimum response to an emergency for response planning. This paper describes the addition of a GA to infrastructure modeling for infrastructure planning. It first introduces the CIMS<sup>copy</sup> infrastructure modeling software used as the modeling engine to support the GA. Next, the GA techniques and parameters are defined. Then a test scenario illustrates the integration with CIMS<sup>copy</sup> and the preliminary results.","","1-4244-1052-51-4244-1053","10.1109/THS.2007.370044","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4227807","","Genetic algorithms;Protection;Laboratories;National security;Resource management;Engines;Software testing;System testing;Road transportation;Medical services","genetic algorithms;safety-critical software;security","genetic algorithms;infrastructure networks;Critical Infrastructure Modeling System;critical infrastructure protection;critical infrastructure preparedness;optimization;Darwin theory of evolution;response planning","","2","6","","","","","","IEEE","IEEE Conferences"
"Performance Comparision of Autocorrelation and CPRDIC Algorithm Implemented on FPGA for OFDM Based WLAN","P. S. Reddy; G. R. Reddy","NA; NA","2009 International Conference on Communication Software and Networks","","2009","","","575","579","This paper deals with design and implementation of autocorrelator and CORDIC algorithm for OFDM based WLAN on FPGA. The autocorrelator is used for frame detection and carrier frequency offset estimation. The CORDIC is used to estimate the frequency offset and to calculate the division in the channel estimation algorithm. A fast pipelined CORDIC architecture and autocorrelator is designed and implemented on FPGA. HDL and test bench is developed to simulate and verify the functionality of both the modules. The design is implemented on Spartan-2 100K, Spartan-3 100K and Virtex-2 2 Million gate capacity devices to check the performances of hardware implementation. It is found that as the technology changes from 130nm FPGA to 90nm FPGA the design speed increases from 100 MHz to 229 MHz for autocorrelator and 24 MHz to 55 MHz, at the same time the design consumes more power this is due to the fact that Virtex devices have more gate capacity and hence more power consumption. The sequential logic also reduces drastically. The design consumes almost equal number of macros on the entire implementation platform. The design is optimized for area and power requirements.","","978-0-7695-3522","10.1109/ICCSN.2009.172","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5076918","Keywords:Autocorrelation;CORDIC;FPGA;OFDM;WLAN","Autocorrelation;Field programmable gate arrays;OFDM;Wireless LAN;Frequency estimation;Algorithm design and analysis;Frequency conversion;Channel estimation;Hardware design languages;Testing","channel estimation;digital arithmetic;field programmable gate arrays;frequency estimation;OFDM modulation;wireless LAN","autocorrelation algorithm;CORDIC algorithm;FPGA;OFDM based WLAN;frame detection;carrier frequency offset estimation;channel estimation algorithm;HDL;test bench;hardware implementation;Virtex devices;gate capacity;sequential logic;orthogonal frequency division multiplexing systems;wireless local area networks;field programmable gate arrays","","","11","","","","","","IEEE","IEEE Conferences"
"An effective term weighting method using random walk model for information retrieval","M. R. Islam; B. D. Sarker; M. R. Islam","Computer Science and Engineering Discipline, Khulna University, Bangladesh; Computer Science and Engineering Discipline, Khulna University, Bangladesh; Computer Science and Engineering Discipline, Khulna University, Bangladesh","2008 International Conference on Computer and Communication Engineering","","2008","","","1357","1362","Document representation is one of the most fundamental issues in information retrieval application. In this aspect, to rank a document term weighting system has crucial importance. The graph-based ranking algorithms represent documents as a graph. The weight of a vertex in the graph is calculated based on the global information of that vertex. The similarity of the subject of a document to a query can be calculated by various ways and a particular calculation provides ranking to the document. This paper introduces an effective random-walk term weighting method to rank a document by considering position of terms (vertex) within the document and information gain of terms corresponds to the whole set of documents. The experiments on standard test collections show that our approach improves the recall and precision.","","978-1-4244-1691-2978-1-4244-1692","10.1109/ICCCE.2008.4580827","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4580827","","Information retrieval;Citation analysis;Voting;Equations;Computer science;Application software;Testing;Information analysis;Web sites;Social network services","document handling;information retrieval","effective term weighting method;random walk model;information retrieval;document representation;graph-based ranking algorithms","","","8","","","","","","IEEE","IEEE Conferences"
"The Physionet/Computers in Cardiology challenge 2008: T-wave alternans","G. B. Moody","Massachusetts Institute of Technology, Cambridge, USA","2008 Computers in Cardiology","","2008","","","505","508","The 9th annual PhysioNet/Computers in Cardiology challenge invited participants to measure T-wave alternans (TWA) in a set of 100 two-minute electrocardiograms that included subjects with a variety of risk factors for sudden cardiac death (including ventricular tachyarrhythmias, transient myocardial ischemia, and acute myocardial infarctions), healthy controls, and synthetic ECGs with calibrated amounts of artificial TWA. The participantspsila TWA estimates were used to develop a ranking of the 100 test cases in order of TWA content, and the Kendall rank correlation coefficient between this reference ranking and each individual participantpsilas ranking of the 100 cases was calculated as a score (between -1 and 1; actual scores were between 0.11 and 0.92). The challenge yielded insights into the strengths and weaknesses of classic and novel TWA analyses, open-source implementations of a variety of methods, and a set of freely available ECGs with reference rankings of TWA content.","0276-6574;2325-8853","978-1-4244-3706","10.1109/CIC.2008.4749089","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4749089","","Cardiology;Electrocardiography;Myocardium;Databases;Testing;Particle measurements;Ischemic pain;Signal processing algorithms;Open source software;Maximum likelihood detection","cardiovascular system;diseases;electrocardiography;medical computing","Physionet;Computers in Cardiology Challenge 2008;T-wave alternans;electrocardiograms;risk factors;sudden cardiac death;ventricular tachyarrhythmias;transient myocardial ischemia;acute myocardial infarctions;TWA content;Kendall rank correlation coefficient;open-source implementation","","18","13","","","","","","IEEE","IEEE Conferences"
"A New Chromatic Adaptation Model for Different Illuminations","W. Xiaxia; X. Dehong","NA; NA","2008 International Conference on Computer Science and Software Engineering","","2008","6","","38","41","Illumination of a view environment illuminated by different illuminants can vary dramatically, which includes both the physical intensity (or luminance ) of the illumination and the chromaticity of the illumination. Several visual experiments show that Changes in illumination can cause the appearance of a color viewed under the corresponding illuminant to change. But dynamic mechanism of human visual system (HVS) has an ability of adaptation, which optimizes the visual response to particular view condition of illumination so that color appearance of the color object under different illuminants could be similar or same depending on the degree of adaptation of human view system. In some literatures, many chromatic adaptation models were proposed as the dynamic mechanism of the human view system. However, according to the studies of biopsychology and anatomy, changes of the range and the gain in cone sensitivity with wavelength intricately are both caused by changes of luminance of the illuminant. That is, luminance of the illuminant not only can influence the quantity of cones response but also can modify the spectral distribution of cones response of human eyes. So the paper proposes a new chromatic adaptation model which bases on eye's adaptation of luminance and chromaticity with changes of illumination. The results of experiments are carried out to assess the performance of the proposed model by comparing that of CIECAT97[3] and CIECAT2000 [4] chromatic models.","","978-0-7695-3336","10.1109/CSSE.2008.584","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4723191","chromatic adaptation;chromaticity;luminance;HVS","Adaptation model;Lighting;Humans;Biopsy;Anatomy;Printing;Software packages;Packaging;Mathematics;Testing","colour graphics;visual perception","chromatic adaptation model;view environment illumination;physical intensity;luminance;visual experiments;dynamic mechanism;human visual system;visual response;color appearance;color object;human view system;biopsychology;anatomy;spectral distribution;cones response;human eyes","","","14","","","","","","IEEE","IEEE Conferences"
"Parametric Study and Optimal Design in Wire Bonding Process for Mini Stack-Die Package","H. Hsu; S. Yu; Y. Hsu; W. Chang; M. Lin; R. Lin; P. Chin; H. Ho; M. Lu; C. Lee; C. Chen; C. Liao; Y. Huang; S. Fu; L. Chen","Department of Mechanical and Automation Engineering, I-Shou University, Taiwan, China. Contact email: hchsu@isu.edu.tw; Department of Mechanical and Automation Engineering, I-Shou University, Taiwan, China; Department of Mechanical and Automation Engineering, I-Shou University, Taiwan, China; Department of Mechanical and Automation Engineering, I-Shou University, Taiwan, China; Department of Mechanical and Automation Engineering, I-Shou University, Taiwan, China; Department of Mechanical and Automation Engineering, I-Shou University, Taiwan, China; Department of Mechanical and Automation Engineering, I-Shou University, Taiwan, China; Division of IC Packaging Backend, OSE Corp, I-Shou University, Taiwan, China; Division of IC Packaging Backend, OSE Corp, I-Shou University, Taiwan, China; Division of IC Packaging Backend, OSE Corp, I-Shou University, Taiwan, China; Department of Electronic Engineering, I-Shou University, Taiwan, China; Department of Electronic Engineering, I-Shou University, Taiwan, China; Department of Electronic Engineering, I-Shou University, Taiwan, China; Department of Electronic Engineering, I-Shou University, Taiwan, China; Department of Electronic Engineering, I-Shou University, Taiwan, China","2006 7th International Conference on Electronic Packaging Technology","","2006","","","1","6","The purpose of this research is to study the parametric factors of wire bonding and optimal design rules for 4-layer mini stack-die package. This paper demonstrates the characteristic of low loop height, fine bond pad pitch and long overhead staggered chip in mini SD package. The loop height is limited to 3.5mil, the diameter of gold wire is 0.8 mil and the bond pad pitch is 45mum. The collaborated relationships among bonding force, bonding time, temperature and thermal-ultrasonic contact pressure have been evaluated to improve the manufacturing process. A three-dimensional solid model of SD card based on finite element ANSYS software is developed to predict the bond pull and ball shear tests and the stress distributions in the overall model. The predicted thermal-induced displacements were found to be very good agreement with the Moire interferometer experimental in-plane u and v deformations. The developed finite element model is then applied to evaluate the reliability of the JEDEC standard JESD22-A104 thermal cycle test. The thermal-induced warpage of SD package is predicted during the process of reflow. The electromagnetic of gold wire in the first bond as well as the second bond is determined. A series of parametric studies have been conducted to validate the developed finite element model in this paper, and the optimizations of manufacturing process are summarized","","1-4244-0619-61-4244-0620","10.1109/ICEPT.2006.359731","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4198852","low loop height;fine bond pad pitch;Moiré interferometer;thermal cycle test","Parametric study;Wire;Bonding processes;Packaging;Finite element methods;Gold;Bonding forces;Manufacturing processes;Solid modeling;Predictive models","finite element analysis;gold;lead bonding;moire fringes;optimisation;packaging;reflow soldering;reliability","optimal design;wire bonding;mini stack-die package;low loop height;fine bond pad pitch;gold wire;thermal-ultrasonic contact pressure;3D solid model;finite element ANSYS software;Moire interferometer;reliability;JEDEC standard;JESD22-A104 thermal cycle test;reflow;optimizations","","2","5","","","","","","IEEE","IEEE Conferences"
"Efficient architecture for collision detection between heterogeneous data structures application for vision-guided robots","J. Himmelstein; G. Ginioux; E. Ferre; A. Nakhaei; F. Lamiraux; J. Laumond","Kineo CAM, Toulouse, France; Kineo CAM, Toulouse, France; Kineo CAM, Toulouse, France; LAAS-CRNS, Toulouse, France; LAAS-CRNS, Toulouse, France; LAAS-CRNS, Toulouse, France","2008 10th International Conference on Control, Automation, Robotics and Vision","","2008","","","522","529","Many collision detection methods exist, each specialized for certain data types under certain constraints. In order to enable rapid development of efficient collision detection procedures, we propose an extensible software architecture that allows for cross-queries between data types, while permitting the time and memory optimizations needed for high-performance. By decomposing collision detection into well-defined algorithmic and data components, we can use the same tree-descent algorithm to execute proximity queries, regardless the data type. We validate our implementation on a path planning problem in which a vision guided humanoid represented by an OBB tree explores a dynamic environment composed of voxel maps.","","978-1-4244-2286-9978-1-4244-2287","10.1109/ICARCV.2008.4795573","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4795573","collision detection;software design;robot navigation","Data structures;Robot vision systems;Testing;Robot sensing systems;Mobile robots;Path planning;Humanoid robots;Robotics and automation;Application software;Object detection","collision avoidance;control engineering computing;robot vision;software architecture;tree data structures","collision detection;heterogeneous data structures;vision-guided robots;software architecture;cross-queries;tree-descent algorithm","","1","17","","","","","","IEEE","IEEE Conferences"
"Intel’s Post Silicon functional validation approach","B. Tommy; F. Igor; M. Robert","Intel Corporation, Intel Development Center, M.T.M. Scientific Industries Center, Haifa 31015, Israel; Intel Corporation, Intel Development Center, M.T.M. Scientific Industries Center, Haifa 31015, Israel; Intel Corporation, 2088 Center Drive, MS 301, DuPont, WA 98327, USA","2007 IEEE International High Level Design Validation and Test Workshop","","2007","","","53","56","CPU Post-Silicon functional validation is the last ""guardian"" logic-wise before delivering the product to the market. In each CPU generation, the challenges are larger due to increasingly complex architectures, budget constraints and shorter schedules. Success can be achieved just with the novel approaches across different validation teams, and with a complex of state-of-the-art validation software, hardware, execution and silicon debug environments. Budget constraints lead to high automation and efficient validation process. Though Intel Corporation has different divisions, mutual help and hard work and optimization ensures high quality product within the schedule.","1552-6674","978-1-4244-1480","10.1109/HLDVT.2007.4392786","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4392786","","Silicon;Manufacturing;Time to market;Computer bugs;Controllability;Debugging;Job shop scheduling;Prototypes;Energy management;Temperature dependence","formal verification;logic design;logic testing;microprocessor chips","Intel Corporation;post silicon functional validation;product quality;CPU generation;corner-case bugs;complex architecture;silicon debug environment","","","4","","","","","","IEEE","IEEE Conferences"
"An Improved Clonal Selection Algorithm and Its Application in Function Optimization Problems","Y. Luo; Z. Jiang","NA; NA","2008 Second International Symposium on Intelligent Information Technology Application","","2008","2","","118","121","Combining the advantage of the interior study mechanism of biological immune system and evolutionary algorithm, this paper proposed an improved clonal selection algorithm (ICSA) which can solve the problem of easily being trapped in local minima and slow convergence of clonal selection algorithm. The improved algorithm included orthogonal crossover, simplex crossover, clone and selection. The idea of evolutionary computation was integrated into clone selection and a new mutation operator was proposed. The new algorithm can guarantee the diversity of the population and improve the global search ability. Theoretical analysis has proved that ICSA converges to the global optimum. Different functions were utilized to test this method and the simulation results have shown that the proposed ICSA algorithm has good performance.","","978-0-7695-3497","10.1109/IITA.2008.328","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4739739","","Cloning;Diversity reception;Application software;Immune system;Evolutionary computation;Convergence;Information technology;Biology computing;Information science;Genetic mutations","functions;genetic algorithms;minimisation;search problems","clonal selection convergence algorithm;function optimization problem;biological immune system;evolutionary algorithm;local minima;orthogonal crossover;simplex crossover;global search;genetic algorithm","","","10","","","","","","IEEE","IEEE Conferences"
"Optimization of an Industrial Sensor and Data Acquisition Laboratory Through Time Sharing and Remote Access","L. Costas-Perez; D. Lago; J. Farina; J. J. Rodriguez-Andina","NA; NA; NA; NA","IEEE Transactions on Industrial Electronics","","2008","55","6","2397","2404","This paper presents an educational laboratory that has been implemented for the practical education in sensors, data acquisition, and basic control skills. The use of the laboratory has been optimized by the availability of a remote access infrastructure that allows the definition and booking of time slots related to the laboratory sites. Given the many kinds of existing sensors, conditioning circuits, and actuators, setting up an educational framework is a complex (and expensive) task, even if only the main design alternatives are taken into account. An additional and fundamental issue to be considered for the optimization of any educational resource or teaching/learning methodology is the possibility to adapt it to the capabilities of different profiles, i.e., students enrolled in different courses. The proposed solution has been designed to be used by both nonexperienced students, who act as plain users testing predefined experiments, and advanced ones, who can demonstrate the design skills they have learnt, by developing their own applications and conditioning circuits. Accordingly, the remote access infrastructure allows different kinds of users to be defined, whose capabilities, restrictions, and software requirements depend on their level of knowledge.","0278-0046;1557-9948","","10.1109/TIE.2008.921687","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4490308","Data acquisition systems (DASs);DataSockets (DS);Java applets;LabVIEW;remote laboratories;virtual instrumentation (VI);Data Acquisition Systems (DAS);DataSockets (DS);Java Applets;LabVIEW;Remote Laboratories;Virtual Instrumentation","Data acquisition;Laboratories;Time sharing computer systems;Actuators;Sensor phenomena and characterization;Circuit testing;Control systems;Optimization methods;Education;Application software","data acquisition;laboratories;virtual instrumentation","industrial sensor;data acquisition laboratory;time sharing;remote access;educational laboratory;teaching-learning methodology","","24","25","","","","","","IEEE","IEEE Journals & Magazines"
"Reconfigurability-Aware Structural Mapping for LUT-Based FPGAs","K. Bruneel; D. Stroobandt","NA; NA","2008 International Conference on Reconfigurable Computing and FPGAs","","2008","","","223","228","In many applications, subsequent tasks differ only in a specific set of parameters. Because of their reconfigurability, FPGAs (field programmable gate arrays) can be configured with an optimized configuration every time these parameter values change. This results in configurations that are smaller and faster than their generic counterparts. Unfortunately, the overhead involved in generating these configurations at run-time with conventional tools is very large. However, if the incoming tasks only differ in a set of parameter values, the use of Tunable LUT (TLUT) circuits can drastically reduce this overhead. A TLUT circuit is a LUT circuit in which the truth tables of the LUTs are expressed as a function of a set of parameters. At run-time the truth tables for a specific set of parameter values can rapidly be calculated by evaluating these functions. Up to now TLUT circuits had to be designed manually resulting in a huge design cost. This paper introduces TMAP2, a software tool based on conventional structural mapping that automatically generates a TLUT circuit starting from an arbitrary Boolean circuit.We have tested TMAP2 on a set of 12 micro-benchmarks and we show a substantial reduction in both the circuits area and maximum depth compared to conventional implementations.","2325-6532","978-1-4244-3748-1978-0-7695-3474","10.1109/ReConFig.2008.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4731798","technology mapping;CAD;FPGA;dynamic reconfiguration","Field programmable gate arrays;Table lookup;Runtime;Circuit optimization;Multiplexing;Tunable circuits and devices;Costs;Circuit testing;Information systems;Software tools","field programmable gate arrays;software tools","LUT-based FPGAs;reconfigurability-aware structural mapping;field programmable gate arrays;tunable LUT circuits;LUT circuit;software tool;Boolean circuit","","2","8","","","","","","IEEE","IEEE Conferences"
"Enhanced Methodology and Tools for Exploring Domain-Specific Coarse-Grained FPGAs","H. Parvez; Z. Marrakchi; H. Mehrez","NA; NA; NA","2008 International Conference on Reconfigurable Computing and FPGAs","","2008","","","121","126","This paper presents a new environment for the exploration of domain-specific coarse-grained FPGAs. An architecture description mechanism is used to define a coarse-grained architecture. A software flow is used to map a netlist on the defined architecture. The software flow not only maps the instances of a target netlist on their respective blocks in the architecture, but also refines the position of the blocks on the architecture. This environment can also be used to define and optimize a domain-specific architecture for a set of netlists to be mapped on it at mutually exclusive times. A set of DSP test-benches are used to show the effectiveness of various techniques used in this work.","2325-6532","978-1-4244-3748-1978-0-7695-3474","10.1109/ReConFig.2008.53","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4731781","Coarse-grained FPGA;Exploration environment;Floor-planning","Field programmable gate arrays;Routing;Pins;Testing;Computer architecture;Digital signal processing;Packaging;Logic;Software libraries;Table lookup","digital signal processing chips;field programmable gate arrays;software architecture","domain-specific coarse-grained FPGA;architecture description mechanism;coarse-grained architecture;software flow;DSP test-benches","","","13","","","","","","IEEE","IEEE Conferences"
"A hardware-software framework for high-reliability people fall detection","M. Grassi; A. Lombardi; G. Rescio; P. Malcovati; M. Malfatti; L. Gonzo; A. Leone; G. Diraco; C. Distante; P. Siciliano; V. Libal; J. Huang; G. Potamianos","Department of Electrical Engineering, University of Pavia, 27100, Italy; Department of Electrical Engineering, University of Pavia, 27100, Italy; Department of Electrical Engineering, University of Pavia, 27100, Italy; Department of Electrical Engineering, University of Pavia, 27100, Italy; Integrated Optical Sensor Group, FBK-irst, Povo, TN 38050, Italy; Integrated Optical Sensor Group, FBK-irst, Povo, TN 38050, Italy; Institute for Microelectronics and Microsystems, CNR-IMM, Lecce 73100, Italy; Institute for Microelectronics and Microsystems, CNR-IMM, Lecce 73100, Italy; Institute for Microelectronics and Microsystems, CNR-IMM, Lecce 73100, Italy; Institute for Microelectronics and Microsystems, CNR-IMM, Lecce 73100, Italy; IBM T.J. Watson Research Center, Yorktown Heights, NY 10598, USA; IBM T.J. Watson Research Center, Yorktown Heights, NY 10598, USA; IBM T.J. Watson Research Center, Yorktown Heights, NY 10598, USA","SENSORS, 2008 IEEE","","2008","","","1328","1331","This paper presents a hardware and software framework for reliable fall detection in the home environment, with particular focus on the protection and assistance to the elderly. The integrated prototype includes three different sensors: a 3D time-of-flight range camera, a wearable MEMS accelerometer and a microphone. These devices are connected with custom interface circuits to a central PC that collects and processes the information with a multi-threading approach. For each of the three sensors, an optimized algorithm for fall-detection has been developed and benchmarked on a collected mulitimodal database. This work is expected to lead to a multi-sensory approach employing appropriate fusion techniques aiming to improve system efficiency and reliability.","1930-0395","978-1-4244-2580-8978-1-4244-2581","10.1109/ICSENS.2008.4716690","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4716690","","Wearable sensors;Hardware;Integrated circuit reliability;Protection;Senior citizens;Software prototyping;Prototypes;Cameras;Micromechanical devices;Accelerometers","accelerometers;benchmark testing;bioMEMS;health care;microphones;patient monitoring;prototypes;wearable computers","hardware-software framework;people fall detection;elderly;prototype;3D time-of-flight range camera;wearable MEMS accelerometer;microphone;benchmark","","16","12","","","","","","IEEE","IEEE Conferences"
"Tutorial proposal efficient solving of optimization problems using advanced boolean satisfiability and Integer Linear Programming techniques","F. A. Aloul","Department of Computer Engineering American University in Sharjah, U.A.E.","2009 International Conference on Microelectronics - ICM","","2009","","","6","11","Recent years have seen a tremendous growth in the number of research and development groups at universities, research labs, and companies that have started using Boolean Satisfiability (SAT) algorithms for solving different decision and optimization problems in Computer Science and Engineering. This has lead to the development of highly-efficient SAT solvers that have been successfully applied to solve a wide-range of problems in Electronic Design Automation (EDA), Artificial Intelligence (AI), Networking, Fault Tolerance, Security, and Scheduling. Examples of such problems include automatic test pattern generation for stuck-at faults (ATPG), formal verification of hardware and software, circuit delay computation, FPGA routing, power leakage minimization, power estimation, circuit placement, graph coloring, wireless communications, wavelength assignment, university classroom scheduling, and failure diagnosis in wireless sensor networks. SAT solvers have recently been extended to handle Pseudo-Boolean (PB) constraints which are linear inequalities with integer coefficients. This feature allowed SAT solvers to handle optimization problems, as opposed to only decision problems, and to be applied to a variety of new applications. Recent work has also showed that free open source SAT-based PB solvers can compete with the best generic Integer Linear Programming (ILP) commercial solvers such as CPLEX. This tutorial is aimed at introducing the latest advances in S AT technology. Specifically, we describe the simple new input format of SAT solvers and the common SAT algorithms used to solve decision/optimization problems. In addition, we highlight the use of SAT algorithms in solving a variety of EDA decision and optimization problems and compare its performance to generic ILP solvers. This should guide researchers in solving their existing optimization problems using the new SAT technology. Finally, we provide a prospective on future work on SAT.","2159-1660;2159-1679","978-1-4244-5816-5978-1-4244-5814-1978-1-4244-5815","10.1109/ICM.2009.5418586","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5418586","","Tutorial;Proposals;Integer linear programming;Electronic design automation and methodology;Educational institutions;Artificial intelligence;Processor scheduling;Automatic test pattern generation;Circuits;Delay estimation","","","","","53","","","","","","IEEE","IEEE Conferences"
"Considering Data-Mining Techniques in User Preference Learning","P. Vojtáš; A. Eckhardt","NA; NA","2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology","","2008","3","","33","36","In this paper we deal with the problem of learning user preferences from userpsilas scoring of a small sample of objects with labels from a very small linearly ordered set. The main task of this process is to use these preferences for a top-k query, which delivers the user with an ordered list of k highest ranked objects. We deal with a problem of many ties in the highest score. Two algorithms for learning objective and utility functions are presented. We experiment and compare them to some classical data-mining methods. We use several measures (RMSE and rank correlations ...) to evaluate efficiency of these methods.","","978-0-7695-3496","10.1109/WIIAT.2008.53","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4740721","preference learning;data mining;user preferences","Intelligent agent;Software engineering;Computer science;Testing;Learning systems;Data mining;Abstracts;Human computer interaction","data mining;learning (artificial intelligence);mean square error methods;query processing","data-mining techniques;user preference learning;top-k query;highest ranked objects;learning objective;utility functions;data-mining methods;RMSE;rank correlations","","1","7","","","","","","IEEE","IEEE Conferences"
"Optimal Remote Center-of-Motion Location for Robotics-Assisted Minimally-Invasive Surgery","R. C. O. Locke; R. V. Patel","Canadian Surgical Technologies & Advanced Robotics (CSTAR), 339 Windermere Road, London, Ontario, N6A 5A5, Canada; Department of Electrical and Computer Engineering, The University of Western Ontario, London, Ontario, N6A 5B9, Canada. rlocke@uwo.ca; Canadian Surgical Technologies & Advanced Robotics (CSTAR), 339 Windermere Road, London, Ontario, N6A 5A5, Canada; Department of Electrical and Computer Engineering, The University of Western Ontario, London, Ontario, N6A 5B9, Canada. rajni@eng.uwo.ca","Proceedings 2007 IEEE International Conference on Robotics and Automation","","2007","","","1900","1905","A novel technique is described for isotropy-based kinematic optimization of specific robot characteristics. The new technique has advantages over existing techniques when designing robotic systems for specific, unconventional tasks, and for constrained motion. In this paper, the technique is used to assist in the selection of a remote center-of-motion (RCM) location for a research testbed that is being developed at CSTAR to study robotics-assisted minimally-invasive surgery. The optimization technique allows isotropy to be considered with respect to the surgical tool tip while operating under the RCM constraint. Global isotropy over a minimally-invasive surgical workspace is evaluated for a set of candidate RCM locations, and an optimal RCM location with respect to isotropy is selected. The isotropy results are compared with experimental data for a number of candidate RCM locations. The experimental results confirm the usefulness of the optimization technique.","1050-4729","1-4244-0602-11-4244-0601","10.1109/ROBOT.2007.363599","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4209363","Remote center-of-motion;kinematic optimization;minimally-invasive surgery;medical robotics","Robots;Surgery;Kinematics;Robotics and automation;Manipulators;Testing;Medical robotics;Control systems;Couplings;Software tools","end effectors;manipulator kinematics;medical robotics;optimal control;optimisation;surgery","optimal remote center-of-motion location;robotics-assisted minimally-invasive surgery;isotropy-based kinematic optimization;robot characteristics;constrained motion;surgical tool tip;medical robotics","","30","13","","","","","","IEEE","IEEE Conferences"
"A hardware compilation flow for instance-specific VLIW cores","M. Koester; W. Luk; G. Brown","Department of Computing, Imperial College London, UK; Department of Computing, Imperial College London, UK; Department of Computer Science, Indiana University, Bloomington, USA","2008 International Conference on Field Programmable Logic and Applications","","2008","","","619","622","Hardware compilers for high-level programming languages are important tools to reduce the design productivity gap in hardware development. In this paper a hardware compilation approach is described, which is able to generate a hardware description based on a specification in a high-level programming language such as ANSI C. No modification of the program specification is required, allowing it to be suitable for a hardware and a software implementation at the same time. The parallelism is extracted by using VLIW optimization techniques. The generated hardware implementation is an instance-specific VLIW core, which is defined by its high-level program specification. To demonstrate the principle of the design flow, a prototype is presented which uses the VEX compiler as the front-end and the Handel-C tool chain as the back-end. The resulting instance-specific VLIW cores of several test functions are compared to equivalent software implementations.","1946-147X;1946-1488","978-1-4244-1960-9978-1-4244-1961","10.1109/FPL.2008.4630023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4630023","","Hardware;Magnetic cores;Parallel processing;Software;Computer architecture;Registers;Memory management","high level languages;instruction sets;optimisation;program compilers","hardware compilation flow;instance-specific VLIW cores;hardware compilers;high-level programming languages;hardware description;ANSI C;program specification;VLIW optimization;VEX compiler;Handel-C tool chain","","7","8","","","","","","IEEE","IEEE Conferences"
"ProActive Access Control for Business Process-Driven Environments","M. Kohler; A. Schaad","NA; NA","2008 Annual Computer Security Applications Conference (ACSAC)","","2008","","","153","162","Users expect that systems react instantly. This is specifically the case for user-centric workflows in business process-driven environments. In today's enterprise systems most actions executed by a user have to be checked against the system's access control policy and require a call to the access control component. Hence, improving the performance of access control decisions will improve the overall performance experienced by the end user significantly. In this paper we propose a caching strategy which pre-computes caching entries by exploiting the fact that the executions of business processes are based on the execution of actions in a predefined order. We propose an accompanying architecture and present the results of our conducted benchmark.","1063-9527","978-0-7695-3447","10.1109/ACSAC.2008.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721553","Access Control;Business Process;Caching;Workflow","Access control;Delay;Computer security;Application software;History;Environmental management;Design optimization;Performance gain;Benchmark testing","authorisation;cache storage;software architecture;workflow management software","proactive access control;business process-driven environments;user-centric workflows;enterprise systems;caching strategy;accompanying architecture","","4","13","","","","","","IEEE","IEEE Conferences"
"Improvement in Reliability and Energy Yield Prediction of Thin-Film CdS/CdTe PV Modules","M. Ross; G. Rich; L. Petacci; J. Klammer","First Solar LLC, Perrysburg, Ohio USA 43551; First Solar LLC, Perrysburg, Ohio USA 43551; First Solar LLC, Phoenix, Arizona USA 85040; First Solar GmbH, D-55116 Mainz, Germany","2006 IEEE 4th World Conference on Photovoltaic Energy Conference","","2006","2","","2148","2151","In this work, we illustrate improvement in thin-film PV module durability via process optimization. Data are presented from large installations, allowing accurate estimation of failure rates and distributions of failure modes. Improvement in product quality is also described; we show that recent thin-film products can meet industry expectations for consistent power output. In addition, results of product characterization performed at First Solar are presented. Dependence of module output on irradiance and temperature is illustrated, and we show that common assumptions about such dependence (based on experience with conventional PV technology) may not hold for thin-film modules. Predicted behavior of module output (as computed with PV system modeling software) and real-world data are compared. It is shown that adjustment of the parametric description of the module can be used to successfully reduce discrepancy between predicted and actual module behaviors","0160-8371","1-4244-0016-31-4244-0017","10.1109/WCPEC.2006.279930","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4060096","","Transistors;Production;Manufacturing processes;Failure analysis;Thin film devices;Cadmium compounds;Condition monitoring;Inspection;Automatic testing;System testing","cadmium compounds;durability;II-VI semiconductors;optimisation;quality control;reliability;semiconductor thin films;solar cells;thin film devices","reliability;thin-film PV modules;durability;process optimization;quality control;thin-film products;CdS-CdSe","","4","3","","","","","","IEEE","IEEE Conferences"
"A simulation setup to optimize particle flow velocimetry","H. Gao; F. Kremer; H. F. Choi; J. Voigt; P. Claus; J. D'hooge","Lab on Cardiovascular Imaging and Dynamics, Department of Cardiovascular Diseases, Catholic University of Leuven, Belgium; Lab on Cardiovascular Imaging and Dynamics, Department of Cardiovascular Diseases, Catholic University of Leuven, Belgium; Lab on Cardiovascular Imaging and Dynamics, Department of Cardiovascular Diseases, Catholic University of Leuven, Belgium; Lab on Cardiovascular Imaging and Dynamics, Department of Cardiovascular Diseases, Catholic University of Leuven, Belgium; Lab on Cardiovascular Imaging and Dynamics, Department of Cardiovascular Diseases, Catholic University of Leuven, Belgium; Lab on Cardiovascular Imaging and Dynamics, Department of Cardiovascular Diseases, Catholic University of Leuven, Belgium","2009 IEEE International Ultrasonics Symposium","","2009","","","1379","1382","Particle Flow Velocimetry (PFV) has been introduced as a new ultrasound methodology to measure two-dimensional intraventricular flow patterns. It can potentially provide important new information on cardiac hemodynamics and function but how to optimize the whole process (such as frame rate, line density, contrast concentration) is still not clear. The aim of this study was therefore to build a simulation environment allowing to optimize this methodology by combining computational fluid dynamics (CFD) and ultrasound simulations. A 2D model of the left ventricular (LV) geometry was generated and meshed in order to be used as input to commercially available CFD software. An analytic description of a typical ventricular inflow velocity profile (showing an early and atrial filling phase) was used as a boundary condition at the inlet of the LV model and the dynamic flow field was simulated. Point scatterers were subsequently put at random positions within the model and their positions were updated over time based on the simulated flow field. From this dynamic scatterer field, ultrasound data could subsequently be obtained using a convolution-based model previously introduced by our lab. In order to test the simulation setup, RF signals from both a PW Doppler acquisition in the inlet portion of the model and a 2D color Doppler image sequence were simulated. For the PW Doppler spectrogram, the normalized RMSE of the estimated velocities relative to the CFD reference was 3.09%. Moreover, good qualitative agreement was found between the CFD and the color Doppler measurement. In conclusion, a simulation setup was constructed and shown to work correctly. It will be useful for optimizing PFV and for developing flow tracking methods in echocardiography further.","1948-5727;1051-0117;1948-5719","978-1-4244-4389-5978-1-4244-4390","10.1109/ULTSYM.2009.5441655","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5441655","","Computational fluid dynamics;Ultrasonic imaging;Computational modeling;Optimization methods;Particle scattering;Particle measurements;Ultrasonic variables measurement;Hemodynamics;Solid modeling;Geometry","channel flow;computational fluid dynamics;echocardiography;flow visualisation;haemodynamics","particle flow velocimetry;ultrasound methodology;intraventricular flow patterns;cardiac hemodynamics;computational fluid dynamics;ultrasound simulations;left ventricular geometry;point scatterers;dynamic scatterer field;convolution-based model;RF signals;PW Doppler acquisition;Doppler image sequence;echocardiography","","1","12","","","","","","IEEE","IEEE Conferences"
"Summarization of JBIG2 Compressed Indian Language Textual Images","U. Garain; A. K. Datta; U. Bhattacharya; S. K. Parui","Indian Statistical Institute, 203, B. T. Road, Kolkata 700108, India; NA; NA; NA","18th International Conference on Pattern Recognition (ICPR'06)","","2006","3","","344","347","This paper presents a method for automatic summarization of JBIG2 coded textual images without optical character recognition (OCR). Compressed images are partially (less than 10% of the uncompressed image size) decompressed and text lines and words are marked. A few features are computed at each sentence level. Based on the feature values sentences are then marked as a summary sentence or not. The system finally generates a set of sentences as summary. In addition, sentences are ranked within the summary. Experiment considers Indian language text images. Test results show a sentence selection efficiency of about 56% when judged against summarization generated by human. A nonparametric (distribution-free) rank statistic shows a correlation coefficient of 0.28 as a measure of the (minimum) strength of the associations between sentence ranking by machine and human","1051-4651","0-7695-2521","10.1109/ICPR.2006.1090","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1699536","","Image coding;Optical character recognition software;Prototypes;Character recognition;Humans;Image retrieval;Libraries;Testing;Statistical distributions;Information retrieval","data compression;document image processing;image coding;natural languages","JBIG2 compressed textual image;Indian language textual image summarization;nonparametric distribution-free rank statistic","","2","13","","","","","","IEEE","IEEE Conferences"
"Parallel Randomized State-Space Search","M. B. Dwyer; S. Elbaum; S. Person; R. Purandare","University of Nebraska-Lincoln, USA; University of Nebraska-Lincoln, USA; University of Nebraska-Lincoln, USA; University of Nebraska-Lincoln, USA","29th International Conference on Software Engineering (ICSE'07)","","2007","","","3","12","Model checkers search the space of possible program behaviors to detect errors and to demonstrate their absence. Despite major advances in reduction and optimization techniques, state-space search can still become cost-prohibitive as program size and complexity increase. In this paper, we present a technique for dramatically improving the cost- effectiveness of state-space search techniques for error detection using parallelism. Our approach can be composed with all of the reduction and optimization techniques we are aware of to amplify their benefits. It was developed based on insights gained from performing a large empirical study of the cost-effectiveness of randomization techniques in state-space analysis. We explain those insights and our technique, and then show through a focused empirical study that our technique speeds up analysis by factors ranging from 2 to over 1000 as compared to traditional modes of state-space search, and does so with relatively small numbers of parallel processors.","0270-5257;1558-1225","0-7695-2828","10.1109/ICSE.2007.62","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4222563","","Costs;Java;Concurrent computing;Error correction;Computer science;Computer errors;Parallel processing;Performance analysis;Logic;Data structures","optimisation;program testing","parallel randomized state-space search;model checkers;error detection;optimization techniques;parallel processors","","20","29","","","","","","IEEE","IEEE Conferences"
"Allocation of extra components to k<inf>i</inf>-out-of-m<inf>i</inf>subsystems using the NPI method","Z. Khuhro; F. Naureen; A. Salhi","Department of Mathematical Sciences, University of Essex, Colchester, CO4 3SQ, UK; Department of Mathematical Sciences, University of Essex, Colchester, CO4 3SQ, UK; Department of Mathematical Sciences, University of Essex, Colchester, CO4 3SQ, UK","2009 2nd International Conference on Computer, Control and Communication","","2009","","","1","6","The allocation of components to systems remains a challenge due to the components success and failure rate which is unpredictable to design engineers. Optimal algorithms often assume a restricted class for the allocation and yet still require a high-degree polynomial time complexity. Heuristic methods may be time-efficient but they do not guarantee optimality of the allocation. This paper introduces a new and efficient model of a system consisting of k<sub>i</sub>-out-of-m<sub>i</sub>subsystems for allocation of extra components. This model is more general than the traditional k-out-of-n one. This system which consists of subsystem i (i = 1, 2, ..., x) is working if at least k<sub>i</sub>(out of m<sub>i</sub>) components are working. All subsystems are independent and the components within subsystem i (i = 1, 2, ..., x) are exchangeable. Components exchangeable with those of each subsystem have been tested. For subsystem i, n<sub>i</sub>components have been tested for faults and none were discovered in s<sub>i</sub>of these n<sub>i</sub>components. We assume zero-failure testing, that is, we are assuming that none of the components tested is faulty so s<sub>i</sub>= n<sub>i</sub>, i = 1, 2, ..., x. We are using lower and upper probability that a system consisting of x independent k<sub>i</sub>-out-of-m<sub>i</sub>subsystems works. This allocation problem dealt with in this paper can be categorised as either to which subsystem the expected number of extra components should be allocated subject to achieving maximum reliability (Lower probability) of the system consisting subsystems, so s<sub>i</sub>= n<sub>i</sub>, i = 1, 2, ..., x. The resulting component allocation problems are too complicated to be solved by traditional approaches; therefore, the nonparametric predictive inference (NPI) method is used to solve them. These results show that NPI is a powerful tool for solving these kinds of problems which are helpful for design engineers to make optimal decisions. The paper also includes suggestions for further research.","","978-1-4244-3313-1978-1-4244-3314","10.1109/IC4.2009.4909211","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4909211","independent subsystems;exchangeable components;lower and upper probability;NPI;reliability;zero-failure testing","Testing;Design engineering;Software performance;Mathematics;Computer science;Polynomials;Reliability engineering;Power engineering and energy;Power system reliability;Pareto optimization","computational complexity;consecutive system reliability;design engineering;probability","ki-out-of-misubsystems;NPI method;polynomial time complexity;zero-failure testing;reliability;nonparametric predictive inference method","","1","13","","","","","","IEEE","IEEE Conferences"
"Parallel on Analog Circuit Synthesis","Y. Wu; L. Chen; T. Ye","NA; NA; NA","2009 International Conference on Computational Intelligence and Software Engineering","","2009","","","1","5","Analog circuit synthesis is a very important step of analog design automation flow. Due to the great effects on circuit performance degradation from the parasitic, it is a great time-consuming task, especially when the feature size is becoming smaller and smaller, operation frequency is becoming higher and higher, and the circuit scale is becoming larger and larger. In this paper, we present a parallel analog circuit synthesis flow with combination of functionality analysis based circuit partitioning, performance spec decomposition, test bench generation, and performance evaluation equation generation; it makes the high complex large scale circuit synthesis problem become multi low complex circuit synthesis problems of small scales, which enables parallel computing and speeds up the optimization process with full leverage of state-of-art computing resources through efficient task partitioning, management, and scheduling.","","978-1-4244-4507","10.1109/CISE.2009.5365259","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5365259","","Analog circuits;Circuit synthesis;Circuit testing;Design automation;Circuit optimization;Degradation;Frequency;Circuit analysis;Performance analysis;Equations","electronic engineering computing;message passing;network synthesis;performance evaluation","analog circuit synthesis;analog design automation flow;circuit performance degradation;parallel analog circuit synthesis flow;functionality analysis;circuit partitioning;performance spec decomposition;test bench generation;performance evaluation equation generation;optimization process","","","5","","","","","","IEEE","IEEE Conferences"
"Discovery of Core-Nodes in Event-Based Social Networks","S. Yuan; Q. Bai; M. Zhang; K. T. Win","NA; NA; NA; NA","2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery","","2009","2","","430","434","Most previous actor-node ranking algorithms for event-based social networks only consider how many events an actor participates in. However in event-based social networks, we should also consider the influence of events when we rank actor-nodes. In this paper we formally define event-based social networks and related concepts, then we propose rules to construct an event-based social network. Algorithms are presented to discover the activity and importance of each actor-node. We test the algorithms by analysing the DBLP data set. In the experiment actors in DBLP data set are ranked based on their activity, importance, and combination of activity and importance, respectively.","","978-0-7695-3735","10.1109/FSKD.2009.335","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5359485","Knowledge Discovery;Event-Based Social Network;Data Mining;Ranking","Social network services;Computer science;Data analysis;Fuzzy systems;Software engineering;Australia;Bipartite graph;Testing;Algorithm design and analysis;Data mining","data analysis;data mining;social networking (online)","core-nodes discovery;event-based social networks;actor-node ranking algorithms;DBLP data set;knowledge discovery;data mining;data analysis","","1","4","","","","","","IEEE","IEEE Conferences"
"A mission planning approach for UAV applications","W. A. Kamal; R. Samar","National Engineering & Scientific Commission, Islamabad, Pakistan; National Engineering & Scientific Commission, Islamabad, Pakistan","2008 47th IEEE Conference on Decision and Control","","2008","","","3101","3106","In this paper, a 2-D mission planning approach is developed for UAV applications. The main contribution of the paper is the development of an extension to the Bellman Ford algorithm that enables incorporation of constraints directly into the algorithm during run time. The dynamical constraints of the vehicle, such as its angle of turn, can therefore be catered for. Furthermore, a procedure for computing a number of sub-optimal paths is developed so that a range of options is available to the user for selection. These sub-optimal paths are generated in an order of priority (optimality). An objective function is developed which models different conflicting objectives in a unified framework; different objectives can be assigned different weights. The objectives may include minimizing the length of the path, keeping the path as straight as possible, flying over areas of interest, etc. The algorithm is integrated into a software package and tested for complex mission objectives, and results are discussed.","0191-2216","978-1-4244-3123-6978-1-4244-3124","10.1109/CDC.2008.4739187","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4739187","","Unmanned aerial vehicles;Path planning;Iterative algorithms;Software algorithms;Software packages;Software testing;Photography;Robots;Constraint optimization;Trajectory","aircraft;path planning;remotely operated vehicles","2D mission planning;UAV applications;unmanned aerial vehicle;Bellman Ford algorithm;software package","","2","16","","","","","","IEEE","IEEE Conferences"
"Implementation of H.264 on TMS320DM642","L. Hui; X. Ru; L. Zhi","NA; NA; NA","2008 International Conference on Embedded Software and Systems","","2008","","","605","609","H.264/AVC is the newest video standard to resolve the efficient transmission of video data. Compared to previous generations, it delivers better quality at lower bit rates. However, its computational complexity increases almost exponentially, which is still a problem and has motivated much research into H.264 real-time codec both in industry and academia. H.264 baseline real-time video decoder implementation based on TMS320DM642 and JM86 test code is described in this paper. Firstly, the structure of JM86 is rearranged into three parallel tasks using BIOS multithreading technology. Then, the data structure is redesigned to make full use of the DSP memory and to improve the addressing capability of array and pointer. Finally, lots of time-consuming functions of JM86 are rewritten with assembly language. The system highly speeded up the decoding while maintaining the quality as well. For video sequence of D1 format, the decoding speed reached 25 frames per-second and for CIF format, it reached 60 frames per-second.","","978-0-7695-3287","10.1109/ICESS.2008.56","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4595616","H.264/AVC;TM320DM642;video-decoding","Decoding;Streaming media;Digital signal processing;Transforms;Assembly;Optimization;Bit rate","computational complexity;data structures;decoding;digital signal processing chips;image sequences;multi-threading;video codecs;video coding","H.264/AVC video standard;TMS320DM642;computational complexity;H.264 real-time codec;H.264 real-time video decoder;JM86 test code;BIOS multithreading technology;parallel task;data structure;DSP memory;assembly language;video sequence","","","4","","","","","","IEEE","IEEE Conferences"
"A Leaf Sequencing Software for Intensity-Modulated Radiation Therapy","Shuang Luan; Chao Wang; D. Z. Chen; X. S. Hu","University of New Mexico, USA; NA; NA; NA","19th IEEE Symposium on Computer-Based Medical Systems (CBMS'06)","","2006","","","3","8","This paper presents a leaf sequencing software called SLS (static leaf sequencing) for intensity-modulated radiation therapy (IMRT). SLS seeks to produce improved clinical IMRT treatment plans by (1) shortening their treatment times and (2) minimizing their machine delivery errors. Our SLS software is implemented using the C programming language on Linux workstations and is designed as a separate module to complement the current commercial treatment planning systems. The input to SLS is discrete radiation intensity maps computed by current planning systems, and its output is (modified) optimized control sequences for the radiotherapy machines. Our SLS approach is very different from the commonly used planning methods in medical literature in that it is based on graph algorithms and computational geometry techniques. Comparisons of SLS with the CORVUS commercial planning system indicated that for the same set of discrete radiation intensity maps, treatment times can be shortened by over 30% by our SLS plans while maintaining the same treatment quality. We have used SLS in clinical applications at two cancer treatment centers. This paper discusses the various aspects of the implementation, installation, commissioning, and testing of our SLS software system","1063-7125","0-7695-2517","10.1109/CBMS.2006.14","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1647537","","Biomedical applications of radiation;Laser sintering;Computer languages;Linux;Workstations;Medical control systems;Radio control;Control systems;Computational geometry;Application software","C language;Linux;medical computing;radiation therapy","leaf sequencing software;intensity-modulated radiation therapy;static leaf sequencing;C programming language;Linux workstations;optimized control sequences;radiotherapy machines;graph algorithms;computational geometry techniques;CORVUS commercial planning system","","","19","","","","","","IEEE","IEEE Conferences"
"moPGA: Towards a New Generation of Multi-objective Genetic Algorithms","H. Soh; M. Kirley","student in the Department of Computer Science and Software Engineering, University of Melbourne, Australia; Institute of High Performance Computing (IHPC), Agency for Science, Technology and Research (A* STAR), Singapore, (email: sohsh@ihpc.a-star.edu.sg).; NA","2006 IEEE International Conference on Evolutionary Computation","","2006","","","1702","1709","This paper describes a multi-objective parameter-less genetic algorithm (moPGA), which combines several recent developments including efficient non-dominated sorting, linkage learning, isin-Dominance, building-block mutation and convergence detection. Additionally, a novel method of clustering in the objective space using an isin-Pareto Set is introduced. Comparisons with well-known multi-objective GAs on scalable benchmark problems indicate that the algorithm scales well with problem size in terms of number of function evaluations and quality of solutions found. moPGA was built for easy usage and hence, in addition to the problem function and encoding, there are only two required user defined parameters; (1) the maximum running time or generations and (2) the precision of the desired solutions (isin).","1089-778X;1941-0026","0-7803-9487","10.1109/CEC.2006.1688513","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1688513","","Genetic algorithms;Clustering algorithms;Genetic mutations;Bayesian methods;Constraint optimization;Sorting;Evolutionary computation;Computer science;Software engineering;Testing","convergence;genetic algorithms;Pareto optimisation;set theory;sorting;statistical analysis","moPGA algorithm;multiobjective parameter-less genetic algorithm;nondominated sorting;linkage learning;isin-Dominance;building-block mutation;convergence detection;clustering method;isin-Pareto Set","","10","28","","","","","","IEEE","IEEE Conferences"
"Bit-Width Constrained Memory Hierarchy Optimization for Real-Time Video Systems","B. Thornberg; M. Palkovic; Q. Hu; L. Olsson; P. G. Kjeldsberg; M. O'Nils; F. Catthoor","NA; NA; NA; NA; NA; NA; NA","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","","2007","26","4","781","800","The great variety of pixel dynamics of real-time video-processing systems (RTVPS), ranging from color, grayscale, or binary pixels, means that a careful design and specification of bit widths is required. It is obvious that the bit-width specification will affect the total memory storage requirement. However, what is not so obvious is that the bit-width specification will also affect the design of the memory hierarchy, an impact similar for both hardware and software implementations. We have developed an integer-nonlinear-program formulation for the optimization of the memory hierarchy of RTVPS. An active surveillance video camera is introduced as a test case. We demonstrate how the optimization model can reduce the on-chip memory storage by 61% compared to a nonoptimal memory hierarchy","0278-0070;1937-4151","","10.1109/TCAD.2006.884569","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4135378","Bit width;memory hierarchy;polyhedral;v ideo","Constraint optimization;Real time systems;Buffer storage;Digital signal processing;Clocks;Design optimization;Processor scheduling;Gray-scale;Hardware;Surveillance","integer programming;nonlinear programming;semiconductor storage;video equipment;video signal processing;video surveillance","bit-width constrained memory hierarchy optimization;real-time video-processing systems;pixel dynamics;video-processing systems;on-chip memory storage;bit-width specification;integer-nonlinear-program formulation;active surveillance video camera","","4","42","","","","","","IEEE","IEEE Journals & Magazines"
"HOLMES: Effective statistical debugging via efficient path profiling","T. M. Chilimbi; B. Liblit; K. Mehra; A. V. Nori; K. Vaswani","Microsoft Research Redmond, USA; University of Wisconsin-Madison, USA; Microsoft Research India, India; Microsoft Research India, India; Microsoft Research India, India","2009 IEEE 31st International Conference on Software Engineering","","2009","","","34","44","Statistical debugging aims to automate the process of isolating bugs by profiling several runs of the program and using statistical analysis to pinpoint the likely causes of failure. In this paper, we investigate the impact of using richer program profiles such as path profiles on the effectiveness of bug isolation. We describe a statistical debugging tool called HOLMES that isolates bugs by finding paths that correlate with failure. We also present an adaptive version of HOLMES that uses iterative, bug-directed profiling to lower execution time and space overheads. We evaluate HOLMES using programs from the SIR benchmark suite and some large, real-world applications. Our results indicate that path profiles can help isolate bugs more precisely by providing more information about the context in which bugs occur. Moreover, bug-directed profiling can efficiently isolate bugs with low overheads, providing a scalable and accurate alternative to sparse random sampling.","0270-5257;1558-1225","978-1-4244-3453","10.1109/ICSE.2009.5070506","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5070506","","Debugging;Computer bugs;Programming profession;Computer crashes;Statistical analysis;Software testing;Benchmark testing;Sampling methods;Optimization;Instruments","program debugging;statistical analysis","HOLMES;effective statistical debugging;efficient path profiling;statistical analysis;program profiles;path profiles;bug isolation;iterative bug-directed profiling","","74","25","","","","","","IEEE","IEEE Conferences"
"A PCE-based Connectivity Provisioning Management Framework","E. Grampin; A. Castro; M. German; F. Rodriguez; G. Tejera; M. Sanguinetti","Instituto de Computación, Universidad de la República, Montevideo, Uruguay, E-mail: grampin@fing.edu.uy; Instituto de Computación, Universidad de la República, Montevideo, Uruguay, E-mail: acastro@fing.edu.uy; Instituto de Computación, Universidad de la República, Montevideo, Uruguay, E-mail: mgerman@fing.edu.uy; Instituto de Computación, Universidad de la República, Montevideo, Uruguay, E-mail: rodrigue@fing.edu.uy; Instituto de Computación, Universidad de la República, Montevideo, Uruguay, E-mail: gtejera@fing.edu.uy; Instituto de Computación, Universidad de la República, Montevideo, Uruguay, E-mail: msangui@fing.edu.uy","2007 Latin American Network Operations and Management Symposium","","2007","","","76","83","Many carriers and service providers (SPs) use MPLS to achieve traffic engineering (TE) objectives, such as network resources optimization, support for end-to-end QoS guarantees services, aggregated traffic measurement and fast recovery against link/node/shared risk link group (SRLG) failures. The three main traffic engineering components of MPLS are the routing component, responsible for the discovery of the network topology, the path computation component, responsible for the assignment of resources to traffic demands, and the signaling component, responsible for the establishment of the label switched paths (LSPs) along the computed path. Path computation under QoS and administrative constraints is usually performed sub-optimally in network nodes, which are mainly dedicated to traffic forwarding. Off-load of this task to specialized network entities is a key aspect of the path computation element (PCE) architecture. This paper describes an ongoing implementation of such architecture seeking for cooperation of control and management plane components for overall network operation and management optimization. Early functional testing of the software components, performed in cooperation with a service provider, is presented.","","978-1-4244-1181-8978-1-4244-1182","10.1109/LANOMS.2007.4362462","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4362462","Connectivity Provisioning;Path Computation Element;Distributed systems","Telecommunication traffic;Computer networks;Multiprotocol label switching;Computer architecture;Computer network management;Tellurium;Routing;Network topology;Communication system traffic control;Software testing","multiprotocol label switching;optimisation;quality of service;telecommunication network management;telecommunication network routing;telecommunication network topology;telecommunication traffic","PCE-based connectivity provisioning management framework;service providers;MPLS;traffic engineering;network resources optimization;end-to-end QoS guarantees services;aggregated traffic measurement;shared risk link group failures;routing component;network topology;label switched paths;administrative constraints;traffic forwarding;path computation element;management plane components;network operation;management optimization","","1","23","","","","","","IEEE","IEEE Conferences"
"Development of A USB-Based programmable SDLC measurement unit","C. Pei; L. Baohui; H. Chao; Y. Lan","School of Astronautics, Beihang University 37 Xueyuan Road, Beijing, China; China Yilai Aerospace Electronics Co. LTD 51 Yongding Road, Haidian District; School of Astronautics, Beihang University 37 Xueyuan Road, Beijing, China; School of Astronautics, Beihang University 37 Xueyuan Road, Beijing, China","2009 9th International Conference on Electronic Measurement & Instruments","","2009","","","3-96","3-99","USB-based portable measurement instrument has becoming more and more popular in field application. Recently, isolated RS-485 based synchronous data link control (SDLC) connection was used in aerospace payloads. Custom versions of SDLC protocol was designed for optimizations of specific applications, so do the SDLC protocol measurement units. A USB-based SDLC protocol measurement unit's design is presented in this paper. The measurement system consists of a high speed USB connected measurement unit and a robust laptop computer. All the power the measurement unit need is supplied by the USB power cord. The SDLC protocol is implemented on FPGA, a high speed USB MCU is introduced to manage the high speed data link to the computer. We design a special structure to fit the requirement of flexible SDLC commands handling. An over-sampling serial data front end is introduced to support flexible data rate communication, test data shows that the measurement unit supports data rate within 2Mbps~10Mbps.","","978-1-4244-3863-1978-1-4244-3864","10.1109/ICEMI.2009.5274136","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5274136","protocol test;SDLC;USB;High Speed Communication","Measurement units;Protocols;Universal Serial Bus;Instruments;Aerospace control;Payloads;Design optimization;Application software;Velocity measurement;Robustness","data communication;field programmable gate arrays;microcontrollers;peripheral interfaces;protocols","USB-based programmable SDLC measurement unit;isolated RS-485;synchronous data link control;aerospace payloads;SDLC protocol;laptop computer;USB power cord;FPGA;USB MCU;flexible SDLC commands handling","","","5","","","","","","IEEE","IEEE Conferences"
"Declarative optimization-based drama management in interactive fiction","M. J. Nelson; M. Mateas; D. L. Roberts; C. L. Isbell","Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA, USA; Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA, USA; Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA, USA; Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA, USA","IEEE Computer Graphics and Applications","","2006","26","3","32","41","Our work relates to automatically guiding experiences in large, open-world interactive dramas and story-based experiences where a player interacts with and influences a story. A drama manager (DM) is a system that watches a story as it progresses, reconfiguring the world to fulfill the author's goals. A DM might notice a player doing something that fits poorly with the current story and attempt to dissuade him or her. This is accomplished using soft actions such as having a nonplayer character start a conversation with a player to lure him or her to something else, or by more direct actions such as locking doors. We present work applying search-based drama management (SBDM) to the interactive fiction piece Anchorhead, to further investigate the algorithmic and authorship issues involved. Declarative optimization-based drama management (DODM) guides the player by projecting possible future stories and reconfiguring the story world based on those projections. This approach models stories as a set of possible plot points, and an author-specified evaluation function rates the quality of a particular plot-point sequence","0272-1716;1558-1756","","10.1109/MCG.2006.55","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1626184","interactive drama;computer gaming;drama management;game-tree search;reinforcement learning","Delta modulation;Cities and towns;Project management;Technology management;Testing","humanities;interactive systems;learning (artificial intelligence)","declarative optimization-based drama management;interactive fiction;story-based experience;reinforcement learning","Communication;Computer Graphics;Computer-Assisted Instruction;Drama;Narration;Software;User-Computer Interface","19","5","","","","","","IEEE","IEEE Journals & Magazines"
"Software Defined Radio-based Multi-Mode DVB-RCS Terminals","F. De Rango; A. Santamaria; F. Veltri; M. Tropea; S. Marano","D.E.I.S. Department, University of Calabria, Italy, 87036. e-mail: derango@deis.unical.it; D.E.I.S. Department, University of Calabria, Italy, 87036. e-mail: santamaria@si.deis.unical.it; D.E.I.S. Department, University of Calabria, Italy, 87036. e-mail: fveltri@deis.unical.it; D.E.I.S. Department, University of Calabria, Italy, 87036. e-mail: mtropea@deis.unical.it; D.E.I.S. Department, University of Calabria, Italy, 87036. e-mail: marano@deis.unical.it","MILCOM 2007 - IEEE Military Communications Conference","","2007","","","1","7","The recent increase of multimedia applications lead to the design of new terminals capable of working with different technologies. Software defined radio (SDR), a collection of hardware and software technologies that allows reconfigurable system architectures, should be the solution to the described problems. The new types of terminals that use the SRD technique are called multi-mode terminals. In our study, we intend that a multi-mode terminal is a terminal capable of communicating both with a Bent Pipe (BP) satellite and with an On Board Processor (OBP) satellite. The goal of our work is to plan a satellite system based on the DVB-RCS standard in which the RCSTs are able to adapt itself to the channel state, configuring the transmission chain via software, respecting a QoS constraint on the PER. These considerations bring us to introduce the Supervisor (SPV) element: on the basis of inputs coming from upper layer and from the information on the channel noise power level provided by an estimator, the SPV can adapt it to channel condition producing optimized directives and parametric values to the opportune reconfigurable adaptive blocks. For this purpose, a simplified analysis on satellite channel under Additive White Gaussian Noise (AWGN) has been led out in order to obtain Markov chain model useful to test the SDR platform. In order to model the error characteristic of satellite channel (both uplink and downlink), we employed the ""Gilbert-Elliot model"" based on two states Discrete Time Markov Chain (DTMC).","2155-7578;2155-7586","978-1-4244-1512-0978-1-4244-1513","10.1109/MILCOM.2007.4455031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4455031","","Digital video broadcasting;Satellite broadcasting;AWGN;Artificial satellites;Additive white noise;Application software;Software radio;Hardware;Computer architecture;Software standards","","","","","14","","","","","","IEEE","IEEE Conferences"
"A Distributed Whiteboard Infrastructure for Information Fusion Simulations","M. Brohede; S. F. Andler","NA; NA","2008 32nd Annual IEEE Software Engineering Workshop","","2008","","","134","140","We argue that architecural ideas from DeeDS, a distributed real-time databases, can be used to create a whiteboard infrastructure usable in information fusion applications as well as in fault tolerant simulations. We discuss the need for fault-tolerant real-time simulation in information fusion, and how this can be supported by the whiteboard infrastructure. There are many reasons to perform real-time simulations (including environment simulation) - the use of real hardware or systems can be prohibitively expensive or dangerous to people and equipment, or may simply not be available at all. The combination of real and simulated objects, or nodes, is of particular interest when it is prohibitively expensive, dangerous, or impossible to test sufficient numbers of collaborating nodes in a realistic application. The need for fault tolerance of such real-time simulations, with a mixture of real and simulated nodes, occurs because it is difficult/impossible to restart physical nodes or wait for restart of simulated nodes, if one or more simulation instances fail. The key problem when mixing real and simulated nodes is the occurrence of ""external actions"" that cannot easily be undone and redone when a failure has occurred. We describe a natural approach to support multiple degrees of fault tolerance in realtime simulations, based on optimistic synchronization on top of a whiteboard architecture. The whiteboard architecture is a natural and useful infrastructure metaphor for information fusion, an information-based exchange that allows real and simulated nodes to be freely mixed.","1550-6215","978-0-7695-3617","10.1109/SEW.2008.29","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5328420","infrastructure;whiteboard;information fusion;simulation","Fault tolerance;Real time systems;Distributed databases;Testing;Hardware;Collaboration;Data engineering;Conferences;Application software;Humans","distributed databases;fault tolerant computing;real-time systems;sensor fusion","distributed whiteboard infrastructure;information fusion simulations;distributed real-time databases;fault tolerant simulations;real-time simulations;optimistic synchronization;information-based exchange","","","14","","","","","","IEEE","IEEE Conferences"
"Optimal distribution substation locating in large distribution systems","S. Najafi; A. Vahidnia; H. Hatami; S. Abachizadeh","Electrical Engineering Department, Azarbayjan University of TarbiatMoallem, Tabriz, Iran; Electrical Engineering Department, Shahrood University of Technology, Shahrood, Iran; Electrical Engineering Department, University of Tabriz, Tabriz, Iran; MONA Consultant Engineers Co., Tabriz, Iran","2009 Transmission & Distribution Conference & Exposition: Asia and Pacific","","2009","","","1","6","The problem of distribution system planning is a large-scale, very complex and difficult to define problem. This paper presents the application of an efficient genetic algorithm (GA) for the optimal design of large distribution system, solving the optimal sizing and locating of substation using their corresponding fixed and variable cost as well as any operating and optimization constraints. As a new concept loss characteristic matrix has introduced in this optimal substation placement problem. In order to reduce the computation time and being confident from the feasibility of huge search space the basic operator of GA has been designed such a way that avoids and suppresses from infeasible solution. The value of loads density in every load black determined by mid-term and long-term load forecasting module. The developed GA-based software has been tested in a large distribution system with real size.","2160-8636;2160-8644","978-1-4244-5230","10.1109/TD-ASIA.2009.5357019","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5357019","GA;DSP;Loss Characteristic Matrix","Substations;Large-scale systems;Genetic algorithms;Algorithm design and analysis;Cost function;Constraint optimization;Design optimization;Load forecasting;Software testing;System testing","genetic algorithms;load forecasting;matrix algebra;power distribution planning;power engineering computing;substations","optimal distribution substation;genetic algorithm;distribution system planning;concept loss characteristic matrix;long-term load forecasting module;midterm load forecasting module","","1","13","","","","","","IEEE","IEEE Conferences"
"Using Machine Learning to Support Debugging with Tarantula","L. C. Briand; Y. Labiche; X. Liu","NA; NA; NA","The 18th IEEE International Symposium on Software Reliability (ISSRE '07)","","2007","","","137","146","Using a specific machine learning technique, this paper proposes a way to identify suspicious statements during debugging. The technique is based on principles similar to Tarantula but addresses its main flaw: its difficulty to deal with the presence of multiple faults as it assumes that failing test cases execute the same fault(s). The improvement we present in this paper results from the use of C4.5 decision trees to identify various failure conditions based on information regarding the test cases' inputs and outputs. Failing test cases executing under similar conditions are then assumed to fail due to the same fault(s). Statements are then considered suspicious if they are covered by a large proportion of failing test cases that execute under similar conditions. We report on a case study that demonstrates improvement over the original Tarantula technique in terms of statement ranking. Another contribution of this paper is to show that failure conditions as modeled by a C4.5 decision tree accurately predict failures and can therefore be used as well to help debugging.","1071-9458;2332-6549","0-7695-3024-9978-0-7695-3024","10.1109/ISSRE.2007.31","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4402205","","Machine learning;Debugging;Testing;Failure analysis;Decision trees;Machine learning algorithms;Reliability engineering;Software reliability;Software quality;Laboratories","","","","16","26","","","","","","IEEE","IEEE Conferences"
"Customer Segmentation Algorithm of Wireless Content Service Based on Ant K-Means","Z. Zhongding; M. Xuemei; L. Guangcan","NA; NA; NA","2009 International Forum on Computer Science-Technology and Applications","","2009","1","","267","269","China wireless content service is at the beginning years of establishment, and will keep growing at top speed in the future. Pheromone of wireless content services can scatter by wireless uses, and gets effect of ant colony clustering. Basically, the process of customer segmentation is the process of ant looking for food. This paper presents ACO market segmenting algorithm of wireless content services based on customer consumption characteristics. This algorithm uses demographic, geographic, attitudinal and behavioral data available from across the enterprise to develop highly accurate segments. The algorithm has been implemented and tested on several real datasets and preliminary computational experience is very encouraging. In other word it has been proved that this algorithm will definitely converge to optimal solution in almost runs.","","978-1-4244-5423-5978-1-4244-5422-8978-0-7695-3930","10.1109/IFCSTA.2009.70","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5385082","Wireless Content Service;Ant Colony Optimization (ACO);Data Clustering;Customer Segmentation","Clustering algorithms;Cities and towns;Demography;Content management;Engineering management;Partitioning algorithms;Computer applications;Application software;Educational institutions;Scattering","marketing;optimisation;pattern clustering;telecommunication services","customer segmentation algorithm;China wireless content service;ant colony clustering;ACO market segmenting algorithm;customer consumption characteristics;demographic data;geographic data;attitudinal data;behavioral data;ant K-means","","","10","","","","","","IEEE","IEEE Conferences"
"Dynamic admission control and path allocation for SLAs in DiffServ networks","H. A. Harhira; S. Pierre","Departement of Software and Computer Engineering, Ecole Polytechnique de Montreal, Canada; Departement of Software and Computer Engineering, Ecole Polytechnique de Montreal, Canada","2009 Canadian Conference on Electrical and Computer Engineering","","2009","","","132","136","Today's converged networks are mainly characterized by their support of real-time and high priority traffic requiring a certain level of quality of service (QoS). In this context, traffic classification and prioritization are key features in providing preferential treatments of the traffic in the core of the network. In this paper, we address the joint problem of path allocation and admission control (JPAC) of new Service Level Agreements (SLA) in a DiffServ domain. In order to maximize the resources utilization and the number of admitted SLAs in the network, we consider a statistical bandwidth constraints allowing for a certain overbooking over the network's links. SLAs' admissibility decisions are based on solving to optimality an integer linear programming (ILP) model. When tested by simulations, numerical results confirm that the proposed model can be solved to optimality for real-sized instances within acceptable computation times and substantially reduces the SLAs blocking probability, compared to a the Greedy mechanism proposed in the literature.","0840-7789","978-1-4244-3509-8978-1-4244-3508","10.1109/CCECE.2009.5090106","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5090106","DiffServ;dynamic admission control;Quality of Service (QoS);Dynamic routing","Admission control;Quality of service;Diffserv networks;Telecommunication traffic;Communication system traffic control;Bandwidth;Resource management;Probes;Software quality;Computer networks","DiffServ networks;greedy algorithms;integer programming;IP networks;linear programming;quality of service;resource allocation;telecommunication traffic","dynamic admission control;SLA;DiffServ networks;quality of service;QoS;traffic classification;traffic prioritization;joint problem of path allocation and admission control;Service Level Agreements;resources utilization;integer linear programming;blocking probability;Greedy mechanism","","","10","","","","","","IEEE","IEEE Conferences"
"Architectures for efficient face authentication in embedded systems","N. Aaraj; S. Ravi; S. Raghunathan; N. K. Jha","Dept. of Electr. Eng., Princeton Univ., NJ, USA; NA; NA; NA","Proceedings of the Design Automation & Test in Europe Conference","","2006","2","","6 pp.","","Biometrics represents a promising approach for reliable and secure user authentication. However, they have not yet been widely adopted in embedded systems, particularly in resource-constrained devices such as cell phones and personal digital assistants (PDAs). In this paper, we investigate the challenges involved in using face-based biometrics for authenticating a user to an embedded system. To enable high authentication accuracy, we consider robust face verifiers based on principal component analysis/linear discriminant analysis (PCA-LDA) algorithms and Bayesian classifiers, and their combined use (multi-modal biometrics). Since embedded systems are severely constrained in their processing capabilities, algorithms that provide sufficient accuracy tend to be computationally expensive, leading to unacceptable authentication times. On the other hand, achieving acceptable performance often comes at the cost of degradation in the quality of results. Our work aims at developing embedded processing architectures that improve face verification speed with minimal hardware requirements and without any compromise in verification accuracy. We analyze the computational characteristics of face verifiers when running on an embedded processor, and systematically identify opportunities for accelerating their execution. We then present a range of targeted hardware and software enhancements that include the use of fixed-point arithmetic, various code optimizations, application-specific custom instructions and co-processors, and parallel processing capabilities in multi-processor systems-on-chip (SoCs). We evaluated the proposed architectures in the context of open-source face verification algorithms running on a commercial embedded processor (Xtensa from Tensilica). Our work shows that fast, in-system verification is possible even in the context of many resource-constrained embedded systems. We also demonstrate that high authentication accuracy can be achieved with minimum hardware overheads, while requiring no modifications to the core face verification algorithms","1530-1591;1558-1101","3-9810801-1","10.1109/DATE.2006.244155","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1657105","","Authentication;Embedded system;Biometrics;Hardware;Personal digital assistants;Embedded computing;Computer architecture;Open source software;Cellular phones;Robustness","biometrics (access control);embedded systems;face recognition;fixed point arithmetic;hardware-software codesign;logic design;message authentication;principal component analysis;system-on-chip","face authentication;embedded systems;user authentication;resource-constrained devices;face-based biometrics;robust face verifiers;principal component analysis;linear discriminant analysis;Bayesian classifiers;multimodal biometrics;face verification;embedded processor;fixed-point arithmetic;code optimizations;application-specific custom instructions;application-specific co-processors;parallel processing;multiprocessor systems-on-chip","","5","26","","","","","","IEEE","IEEE Conferences"
"Field data evaluation and continuous health assessment of critical avionics subsystem degradation","P. L. Dussault; C. S. Byington; P. W. Kalgren; A. J. Boodhansingh","Diagnostic/Prognostic Lab., AMRDEC, USA; NA; NA; NA","2006 IEEE Aerospace Conference","","2006","","","8 pp.","","Effectively utilizing existing on-board and at-wing data that encompasses environmental, built-in-test (BIT), vibration, and maintenance test results, can realize improved maintenance, survivability and critical failure rates via the identification of degrading components prior to critical failure. The authors have developed software to show the feasibility of onboard and at-wing prognostics using field data from an army aircraft platform as a multi-faceted case study for predicting and diagnosing the health of a mission-critical targeting subsystem. The two use cases presented, complimentary embedded and at-wing paradigms, illustrate the mining of data from multiple sources, trending and ranking of anomalous indicators, development of a relational model for on-board BIT data, automated advanced reasoning for reduced ambiguity, and valuable avionics system prognostics. The application of these techniques to the systems engineering process, allow for the ground truth feedback of valuable engineering test results for validation of the prognostic health management (PHM) process","1095-323X","0-7803-9545","10.1109/AERO.2006.1656092","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1656092","","Aerospace electronics;Degradation;Military aircraft;Mission critical systems;Data mining;Systems engineering and theory;Feedback;System testing;Engineering management;Prognostics and health management","aerospace engineering;aerospace expert systems;aircraft maintenance;aircraft testing;built-in self test;life testing;military avionics;remaining life assessment","field data evaluation;continuous health assessment;avionics subsystem degradation;degrading component identification;army aircraft platform;complimentary embedded paradigm;at-wing paradigm;data mining;prognostic health management process;PHM","","2","7","","","","","","IEEE","IEEE Conferences"
"A runtime verification monitoring approach for embedded industrial controllers","C. Watterson; D. Heffernan","Centre for Telecommunications Value-Chain Research (CTVR), CSRC, E&CE Dept., University of Limerick, Ireland; Centre for Telecommunications Value-Chain Research (CTVR), CSRC, E&CE Dept., University of Limerick, Ireland","2008 IEEE International Symposium on Industrial Electronics","","2008","","","2016","2021","Complexity in industrial control systems has grown exponentially during the past decade. The reliability of such systems is dependant on trustable embedded controllers. The design of such embedded controllers is moving towards reliability-centric hardware/software co-design frameworks. This paper proposes a novel approach to the development of such embedded controllers, by proposing a special embedded monitoring scheme. An experimental evaluation framework is described that supports runtime verification of a software application executing in an embedded system, where the processor is a Java Optimised Processor (JOP) soft processor, instantiated in the fabric of an FPGA (field programmable gate array). The experimental system employs the Java-MaC (Java Monitoring and Checking) runtime verification method, arranged to indirectly monitor the execution behaviour of the application software in its native environment. A case study example is described, which demonstrates the verification of a condition for a software model of a railroad crossing system. The example shows that such a runtime verification scheme can be used effectively as a software testing approach for such a specialised embedded controller. The issues of how to minimise the overhead impact of the monitoring scheme and how to provide an interface for the monitor are considered.","2163-5137;2163-5145","978-1-4244-1665-3978-1-4244-1666","10.1109/ISIE.2008.4677023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4677023","","Runtime;Monitoring;Industrial control;Java;Embedded software;Application software;Field programmable gate arrays;Electrical equipment industry;Control systems;Hardware","field programmable gate arrays;industrial control","runtime verification monitoring;embedded industrial controllers;Java optimised processor;field programmable gate array;software testing","","3","26","","","","","","IEEE","IEEE Conferences"
"Adaptive Power Management in Energy Harvesting Systems","C. Moser; L. Thiele; D. Brunelli; L. Benini","Swiss Federal Institute of Technology Zurich, moser@tik.ee.ethz.ch; Swiss Federal Institute of Technology Zurich, thiele@tik.ee.ethz.ch; University of Bologna, dbrunelli@deis.unibo.it; University of Bologna, lbenini@deis.unibo.it","2007 Design, Automation & Test in Europe Conference & Exhibition","","2007","","","1","6","Recently, there has been a substantial interest in the design of systems that receive their energy from regenerative sources such as solar cells. In contrast to approaches that attempt to minimize the power consumption we are concerned with adapting parameters of the application such that a maximal utility is obtained while respecting the limited and time-varying amount of available energy. Instead of solving the optimization problem on-line which may be prohibitively complex in terms of running time and energy consumption, we propose a parameterized specification and the computation of a corresponding optimal on-line controller. The efficiency of the new approach is demonstrated by experimental results and measurements on a sensor node","1530-1591;1558-1101","978-3-9810801-2","10.1109/DATE.2007.364689","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4211894","","Energy management;Power system management;Energy consumption;Wireless sensor networks;Sensor phenomena and characterization;Solar energy;Technology management;Software performance;Application software;Energy storage","circuit optimisation;integrated circuit design;low-power electronics","adaptive power management;energy harvesting systems;regenerative sources;solar cells;power consumption;energy consumption;on-line controller;sensor node","","60","14","","","","","","IEEE","IEEE Conferences"
"Rewirings based on the eigenvectors of the Laplacian matrix for enhancing synchronizability of dynamical networks","M. Jalili; A. A. Rad","Computer Engineering Department, Sharif University of Technology, Tehran, Iran; School of Computer and Communication Sciences, Ecole Polytechnique Federal de Lausanne (EPFL), Switzerland","2009 7th IEEE International Conference on Industrial Informatics","","2009","","","588","593","Synchronizability of dynamical networks can be defined as the ease by which the network synchronizes its activity. We propose a method for enhancing the synchronizability of dynamical networks by efficient rewirings. The method is based on the eigenvectors corresponding to the second smallest and the largest eigenvalue of the Laplacian matrix and a modified version of the simulated annealing approach is used to perform the optimization task. Starting from a simple network, i.e. an undirected and unweighted network, and at each step, an edge is selected for disconnection and two non-adjacent nodes for creating at edge in between. The effectiveness of the algorithm is tested on artificially constructed networks such as random, Watts-Strogatz, and scale-free ones. We also investigate the coincidence of two measures of synchronizability, i.e. the eigen-ratio of the Laplacian matrix and the cost of synchronization, in the optimized networks.","1935-4576;2378-363X","978-1-4244-3759-7978-1-4244-3760","10.1109/INDIN.2009.5195869","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5195869","synchronization;syncrhonizability;complex dynamical networks;master-stability-function method;simulated anealing optimization;rewiring","Laplace equations;Computer networks;Eigenvalues and eigenfunctions;Optimization methods;Joining processes;Simulated annealing;Testing;Cost function;Complex networks;Application software","complex networks;eigenvalues and eigenfunctions;Laplace equations;matrix algebra","efficient rewirings;eigenvectors;Laplacian matrix;synchronizability enhancement;dynamical networks;optimization task;Watts-Strogatz networks;random networks;scale-free networks","","","33","","","","","","IEEE","IEEE Conferences"
"Optimized Multipinhole Design for Mouse Imaging","K. Vunckx; J. Nuyts; B. Vanbilloen; M. De Saint-Hubert; D. Vanderghinste; D. Rattat; F. M. Mottaghy; M. Defrise","NA; NA; NA; NA; NA; NA; NA; NA","IEEE Transactions on Nuclear Science","","2009","56","5","2696","2705","The aim of this study was to enhance high-sensitivity imaging of a limited field of view in mice using multipinhole collimators on a dual head clinical gamma camera. A fast analytical method was used to predict the contrast-to-noise ratio (CNR) in many points of a homogeneous cylinder for a large number of pinhole collimator designs with modest overlap. The design providing the best overall CNR, a configuration with 7 pinholes, was selected. Next, the pinhole pattern was made slightly irregular to reduce multiplexing artifacts. Two identical, but mirrored 7-pinhole plates were manufactured. In addition, the calibration procedure was refined to cope with small deviations of the camera from circular motion. First, the new plates were tested by reconstructing a simulated homogeneous cylinder measurement. Second, a Jaszczak phantom filled with 37 MBq <sup>99m</sup>Tc was imaged on a dual head gamma camera, equipped with the new pinhole collimators. The image quality before and after refined calibration was compared for both heads, reconstructed separately and together. Next, 20 short scans of the same phantom were performed with single and multipinhole collimation to investigate the noise improvement of the new design. Finally, two normal mice were scanned using the new multipinhole designs to illustrate the reachable image quality of abdomen and thyroid imaging. The simulation study indicated that the irregular patterns suppress most multiplexing artifacts. Using body support information strongly reduces the remaining multiplexing artifacts. Refined calibration improved the spatial resolution. Depending on the location in the phantom, the CNR increased with a factor of 1 to 2.5 using the new instead of a single pinhole design. The first proof of principle scans and reconstructions were successful, allowing the release of the new plates and software for preclinical studies in mice.","0018-9499;1558-1578","","10.1109/TNS.2009.2030194","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5280507","Design methodology;image enhancement;multiplexing;small animal imagers;SPECT instrumentation","Design optimization;Mice;Collimators;Magnetic heads;Cameras;Calibration;Image reconstruction;Imaging phantoms;Optical imaging;Image quality","biomedical imaging;calibration;collimators;optimisation;phantoms","optimized multipinhole design;high-sensitivity imaging;multipinhole collimator design;dual head clinical gamma camera;analytical method;contrast-to-noise ratio;CNR;homogeneous cylinder measurement;multiplexing artifacts;Jaszczak phantom;abdomen imaging;thyroid imaging;body support information;refined calibration method;spatial resolution;single pinhole design;multipinhole plate;limited field of view mouse imaging","","17","33","","","","","","IEEE","IEEE Journals & Magazines"
"The road not taken: Estimating path execution frequency statically","R. P. L. Buse; W. Weimer","University of Virginia, Charlottesville, USA; University of Virginia, Charlottesville, USA","2009 IEEE 31st International Conference on Software Engineering","","2009","","","144","154","A variety of compilers, static analyses, and testing frameworks rely heavily on path frequency information. Uses for such information range from optimizing transformations to bug finding. Path frequencies are typically obtained through profiling, but that approach is severely restricted: it requires running programs in an indicative environment, and on indicative test inputs. We present a descriptive statistical model of path frequency based on features that can be readily obtained from a program's source code. Our model is over 90% accurate with respect to several benchmarks, and is sufficient for selecting the 5% of paths that account for over half of a program's total runtime. We demonstrate our technique's robustness by measuring its performance as a static branch predictor, finding it to be more accurate than previous approaches on average. Finally, our qualitative analysis of the model provides insight into which source-level features indicate ldquohot pathsrdquo.","0270-5257;1558-1225","978-1-4244-3453","10.1109/ICSE.2009.5070516","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5070516","","Frequency estimation;Roads;Runtime;Information analysis;Benchmark testing;Robustness;Data analysis;Performance analysis;Particle measurements;Scalability","program compilers;program debugging;program diagnostics;program testing;statistical analysis","path execution frequency estimation;compilers;static analyses;testing frameworks;bug finding;descriptive statistical model;performance measurement;static branch predictor;hot paths;program source code","","21","33","","","","","","IEEE","IEEE Conferences"
"Notice of Retraction<br>The Forecast of Tornado Variety Based on Function S-rough Sets","Z. Jiang; Q. Taorong; N. Bin; L. Wen; W. Weili","NA; NA; NA; NA; NA","2009 International Forum on Information Technology and Applications","","2009","3","","40","42","The tornado speed is estimated according to its path length and width, but sometimes from the information detected by the Dopper radar, we found that the path length and the width of the tornado did not have intersection in wind speed ranks. Although the forecast of the wind speed rank is accurate, the wind speed may change owing to other unknown factors. Because it, these information is rough from its character which could not to express in accurate math's methods. We choose the max of the wind speed from any possibility in wind speed estimation, and forecast the wind speed possible change in the variational condition based on function singular rough sets. It was proved to be feasible and effective after tested.;Notice of Retraction<br><br>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<br><br>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<br><br>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<br><br>","","978-0-7695-3600","10.1109/IFITA.2009.235","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5232054","Function Singular Rough Sets;Tornado;Path Length;Path Width","Tornadoes;Rough sets;Wind speed;Wind forecasting;Information technology;Application software;Technology forecasting;Radar detection;Radar applications;Testing","Doppler radar;rough set theory;storms;weather forecasting","tornado forecast;function s-rough sets;Dopper radar;wind speed rank","","","8","","","","","","IEEE","IEEE Conferences"
"Spillover-Partitioning Call Admission Control for Revenue Optimization with QoS Guarantees for Multiple Service Classes in Mobile Wireless Networks","O. Yilmaz; I. Chen; G. Kulczycki; W. Frakes","NA; NA; NA; NA","22nd International Conference on Advanced Information Networking and Applications - Workshops (aina workshops 2008)","","2008","","","1299","1304","We propose and analyze spillover-partitioning call admission control (CAC) for servicing multiple service classes in mobile wireless networks for revenue optimization with quality of service (QoS) guarantees. We evaluate the performance of spillover-partitioning CAC in terms of execution speed and optimal revenue obtainable by comparing it with existing CAC algorithms, including partitioning, threshold, and partitioning-threshold hybrid admission control algorithms. We also investigate fast spillover-partitioning CAC which effectively trades off solution optimality for solution efficiency. We demonstrate through test cases that spillover-partitioning CAC outperforms existing CAC algorithms for revenue optimization with QoS guarantees in both solution optimality and solution efficiency for serving multiple QoS service classes in wireless networks.","","978-0-7695-3096","10.1109/WAINA.2008.50","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4483098","Call admission control;quality of service;performance analysis;revenue optimization;mobile networks","Call admission control;Wireless networks;Partitioning algorithms;Quality of service;Admission control;Bandwidth;Mobile computing;Land mobile radio cellular systems;Base stations;Application software","mobile radio;quality of service;radio networks;telecommunication congestion control","spillover-partitioning call admission control;revenue optimization;QoS guarantees;multiple service classes;mobile wireless networks;quality of service guarantees","","1","12","","","","","","IEEE","IEEE Conferences"
"Feature Selection Based on Genetic Algorithms for Speaker Recognition","M. Zamalloa; G. Bordel; L. J. Rodriguez; M. Penagarikano","Grupo de Trabajo en Tecnologías del Software, Departamento de Electricidad y Electrónica, Facultad de Ciencia y Tecnología, UPV/EHU. mzamalloa00l@ikasle.ehu.es; Grupo de Trabajo en Tecnologías del Software, Departamento de Electricidad y Electrónica, Facultad de Ciencia y Tecnología, UPV/EHU; Grupo de Trabajo en Tecnologías del Software, Departamento de Electricidad y Electrónica, Facultad de Ciencia y Tecnología, UPV/EHU; Grupo de Trabajo en Tecnologías del Software, Departamento de Electricidad y Electrónica, Facultad de Ciencia y Tecnología, UPV/EHU","2006 IEEE Odyssey - The Speaker and Language Recognition Workshop","","2006","","","1","8","The Mel-frequency cepstral coefficients (MFCC) and their derivatives are commonly used as acoustic features for speaker recognition. The issue arises of whether some of those features are redundant or dependent on other features. Probably, not all of them are equally relevant for speaker recognition. Reduced feature sets allow more robust estimates of the model parameters. Also, less computational resources are required, which is crucial for real-time speaker recognition applications using low-resource devices. In this paper, we use feature weighting as an intermediate step towards feature selection. Genetic algorithms are used to find the optimal set of weights for a 38-dimensional feature set, consisting of 12 MFCC, their first and second derivatives, energy and its first derivative. To evaluate each set of weights, speaker recognition errors are counted over a validation dataset. Speaker models are based on empirical distributions of acoustic labels, obtained through vector quantization. On average, weighting acoustic features yields between 15% and 25% error reduction in speaker recognition tests. Finally, features are sorted according to their weights, and the K features with greatest average ranks are retained and evaluated. We conclude that combining feature weighting and feature selection allows to reduce costs without degrading performance","","1-424400471-11-4244-0472","10.1109/ODYSSEY.2006.248087","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4013504","","Genetic algorithms;Speaker recognition;Mel frequency cepstral coefficient;Cepstral analysis;Robustness;Loudspeakers;Vector quantization;Acoustic testing;Diversity reception;Costs","acoustic signal processing;cepstral analysis;feature extraction;genetic algorithms;speaker recognition;vector quantisation","Mel-frequency cepstral coefficient;MFCC;acoustic feature;speaker recognition;feature selection;genetic algorithm;validation dataset;empirical distribution;vector quantization","","4","16","","","","","","IEEE","IEEE Conferences"
"Humanoid robot simulation with a joint trajectory optimized controller","J. L. Lima; J. C. Goncalves; P. G. Costa; A. P. Moreira","Department of Electrical and Computer Engineering, Faculty of Engineering of University of Porto, Portugal; Department of Electrical and Computer Engineering, Faculty of Engineering of University of Porto, Portugal; Department of Electrical and Computer Engineering, Faculty of Engineering of University of Porto, Portugal; Department of Electrical and Computer Engineering, Faculty of Engineering of University of Porto, Portugal","2008 IEEE International Conference on Emerging Technologies and Factory Automation","","2008","","","986","993","This paper describes a joint trajectory optimized controller for a humanoid robot simulator following the real robot characteristics. As simulation is a powerful tool for speeding up the control software development, the proposed accurate simulator allows to fulfil this goal. The simulator, based on the open dynamics engine and GLScene graphics library, provides instant visual feedback. The proposed simulator, with realistic dynamics, allows to design and test behaviours and control strategies without access to the real hardware in order to carry out research on robot control without damaging the real robot in the early stages of the development. The joints controller techniques, such as acceleration, speed and energy consumption minimization are discussed and experimental results are presented in order to validate the proposed simulator.","1946-0740;1946-0759","978-1-4244-1505-2978-1-4244-1506","10.1109/ETFA.2008.4638514","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4638514","","Robots;Joints;Servomotors;Acceleration;Equations;Humanoid robots;Mathematical model","acceleration control;control engineering computing;feedback;humanoid robots;position control;software libraries;velocity control","humanoid robot simulation;trajectory optimized controller;control software development;open dynamics engine;GLScene graphics library;visual feedback;robot control;acceleration control;speed control;energy consumption minimization","","1","13","","","","","","IEEE","IEEE Conferences"
"Automatic ADL-based Operand Isolation for Embedded Processors","A. Chattopadhyay; B. Geukes; D. Kammler; E. M. Witte; O. Schliebusch; H. Ishebabi; R. Leupers; G. Ascheid; H. Meyr","Integrated Signal Processing Systems RWTH Aachen University 52056 Aachen, Germany anupam@iss.rwth-aachen.de; NA; NA; NA; NA; NA; NA; NA; NA","Proceedings of the Design Automation & Test in Europe Conference","","2006","1","","1","6","Cutting-edge applications of future embedded systems demand highest processor performance with low power consumption to get acceptable battery-life times. Therefore, low power optimization techniques are strongly applied during the development of modern application specific instruction set processors (ASIPs). Electronic system level design tools based on architecture description languages (ADL) offer a significant reduction in design time and effort by automatically generating the software tool-suite as well as the register transfer level (RTL) description of the processor. In this paper, the automation of power optimization in ADL-based RTL generation is addressed. Operand isolation is a well-known power optimization technique applicable at all stages of processor development. With increasing design complexity several efforts have been undertaken to automate operand isolation. In pipelined datapaths, where isolating signals are often implicitly available, the traditional RTL-based approach introduces unnecessary overhead. We propose an approach which extracts high-level structural information from the ADL representation and systematically uses the available control signals. Our experiments with state-of-the-art embedded processors show a significant power reduction (improvement in power efficiency)","1530-1591;1558-1101","3-9810801-1","10.1109/DATE.2006.243993","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1656958","","Embedded system;Energy consumption;Application software;Application specific processors;Architecture description languages;Software tools;Registers;Automation;Power generation;Data mining","embedded systems;hardware description languages;high level synthesis;instruction sets;low-power electronics;system-on-chip","automatic ADL;operand isolation;embedded processors;embedded systems;low power optimization techniques;application specific instruction set processors;electronic system level design tools;architecture description languages;register transfer level description;pipelined datapaths","","3","17","","","","","","IEEE","IEEE Conferences"
"Influence of procedure cloning on WCET prediction","P. Lokuciejewski; H. Falk; M. Schwarzer; P. Marwedel; H. Theiling","Computer Science 12, University of Dortmund, D-44221 Dortmund, Germany; Computer Science 12, University of Dortmund, D-44221 Dortmund, Germany; Computer Science 12, University of Dortmund, D-44221 Dortmund, Germany; Computer Science 12, University of Dortmund, D-44221 Dortmund, Germany; AbsInt Angewandte Informatik, Science Park 1, D-66123 Saarbr&#x00FC;cken, Germany","2007 5th IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS)","","2007","","","137","142","For the worst-case execution time (WCET) analysis, especially loops are an inherent source of unpredictability and loss of precision. This is caused by the difficulty to obtain safe and tight information on the number of iterations executed by a loop in the worst case. In particular, data-dependent loops whose iteration counts depend on function parameters are extremely difficult to analyze precisely. Procedure cloning helps by making such data-dependent loops explicit within the source code, thus making them accessible for high-precision WCET analyses. This paper presents the effect of procedure cloning applied at the source-code level on worst-case execution time. The optimization generates specialized versions of functions being called with constant values as arguments. In standard literature, it is used to enable further optimizations like constant propagation within functions and to reduce calling overhead. We show that procedure cloning for WCET minimization leads to significant improvements. Reductions of the WCET from 12% up to 95% were measured for real-life benchmarks. These results demonstrate that procedure cloning improves analyzability and predictability of real-time applications dramatically. In contrast, average-case performance as the criterion procedure cloning was developed for is reduced by only 3% at most. Our results also show that these WCET reductions only implied small overhead during WCET analysis.","","978-1-5959-3824-4978-1-5959-3824-4978-1-5959-3824","10.1145/1289816.1289852","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5753828","","Cloning;Optimization;Benchmark testing;Context;Timing;Thumb;Real time systems","embedded systems;optimisation;program compilers","procedure cloning;worst case execution time analysis;data dependent loop;source code level;constant propagation;average case performance;WCET analysis","","","10","","","","","","IEEE","IEEE Conferences"
"Framework for performance analysis of RTOS-enabled embedded systems on FPGA","A. F. Gardezi; M. N. Ahsan; S. Masud","Department of Computer Science and Engineering, Lahore University of Management Sciences, Sector-U, D.H.A, 54792, Pakistan; Department of Computer Science and Engineering, Lahore University of Management Sciences, Sector-U, D.H.A, 54792, Pakistan; Department of Computer Science and Engineering, Lahore University of Management Sciences, Sector-U, D.H.A, 54792, Pakistan","2009 International Symposium on Performance Evaluation of Computer & Telecommunication Systems","","2009","41","","35","40","Over the past decade, the consumer market has been flooded with variegated embedded devices that are progressively becoming cheaper, faster and more power-efficient. New applications are constantly appended into these devices. This work is aimed at emulating such devices using some benchmarks and observing the performance gains that can be achieved by modifications to the design of different hardware as well as software components. We present a modular environment where new tasks can be flexibly incorporated into the proposed framework. Leon-3 soft core microprocessor core and RTEMS real time operating system (RTOS) have been employed in the experiments. The system was implemented on a Xilinx FPGA and also tested using the TSIM2 simulator. Using this framework, we have demonstrated an example which optimizes time slices allocated by RTOS to different applications in an embedded device. In particular, we have shown that at a certain optimal value, the performance in terms of execution time attains a steady state. The framework can be extended to include a range of reconfigurable platforms, embedded processors, RTOS and corresponding applications.","","978-1-4244-4165-5978-1-56555-328","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5224146","Reconfigurable Processor Cores;Instruction Set Simulation;RTOS;Embedded Systems;Real-time Performance","Performance analysis;Embedded system;Field programmable gate arrays;Application software;Performance gain;Hardware;Software performance;Microprocessors;Real time systems;Operating systems","embedded systems;field programmable gate arrays;microprocessor chips;operating systems (computers);optimisation","RTOS-enabled embedded system;FPGA;field programmable gate array;software component;hardware component;Leon-3 soft core microprocessor core;RTEMS real time operating system;TSIM2 simulator;optimized time slice allocation;reconfigurable platform;embedded processor;performance analysis","","","22","","","","","","IEEE","IEEE Conferences"
"Optimization of the magnetic circuit of an axial inductor machine based on the calculation and analysis of magnetic field","J. Dirba; N. Levin; S. Orlova; V. Pugachov; L. Ribickis","RIGA Technical University, Kalku Street, Riga, Latvia; Institute of Physical Energetics, Aizkraukles Street, Riga, Latvia; RIGA Technical University, Kalku Street, Riga, Latvia; Institute of Physical Energetics, Aizkraukles Street, Riga, Latvia; RIGA Technical University, Kalku Street, Riga, Latvia","2009 13th European Conference on Power Electronics and Applications","","2009","","","1","8","The work is devoted to investigation into the possibilities to optimize the axial inductor machine. Modelling problems are solved for the external source of magnetic field in the machine's cross-section. Optimal values are found for geometrical parameters of the tooth zone, which allow the maximum EMF in the armature winding coils to be reached. To validate the theoretical assumptions and the adopted methods for determination of the optimal parameters of an inductor generator's tooth zone the experimental tests have been carried out.","","978-1-4244-4432-8978-90-75815-13","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5278726","AC machine;electrical machine;modelling;software","Magnetic circuits;Inductors;Magnetic analysis;Magnetic fields;Teeth;Stator cores;Magnetic flux;Machine windings;Rotors;Stator windings","asynchronous machines;electric machines;machine windings;magnetic circuits","magnetic circuit;axial inductor machine;magnetic field;armature winding coils;AC machine;electrical machine","","","7","","","","","","IEEE","IEEE Conferences"
"Workloads of the Future","J. M. Rabaey; D. Burke; K. Lutz; J. Wawrzynek","University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley","IEEE Design & Test of Computers","","2008","25","4","358","365","Along with changing technologies and design techniques, target applications span a wide range: from large-scale computing to personal services and perceptual interfaces. The authors of this article characterize these workloads of the future and argue for a new set of benchmarks to guide the exploration and optimization of future systems.","0740-7475;1558-1918","","10.1109/MDT.2008.118","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4584462","ubiquitous computing;distributed computing;benchmarks;metrics;system-level design","Computer interfaces;Application software;Benchmark testing;Hardware;Marketing and sales;Mobile communication;Pervasive computing;Portable computers;Network servers;Web server","large-scale systems;optimisation;reviews","design techniques;target applications;large-scale computing;perceptual interfaces","","8","7","","","","","","IEEE","IEEE Journals & Magazines"
"Timing yield-aware color reassignment and detailed placement perturbation for double patterning lithography","M. Gupta; K. Jeong; A. B. Kahng","CSE Departments, UC San Diego, La Jolla, CA; CSE Departments, UC San Diego, La Jolla, CA; CSE Departments, UC San Diego, La Jolla, CA","2009 IEEE/ACM International Conference on Computer-Aided Design - Digest of Technical Papers","","2009","","","607","614","Double patterning lithography (DPL) is a likely resolution enhancement technique for IC production in 32 nm and below technology nodes. However, DPL gives rise to two independent, un-correlated distributions of linewidth on a chip, resulting in a 'bimodal' linewidth distribution and an increase in performance variation. suggested that new physical design mechanisms could reduce harmful covariance terms that contribute to this performance variation. In this paper, we propose new bimodal-aware timing analysis and optimization methods to improve timing yield of standard-cell based designs that are manufactured using DPL. Our first contribution is a DPL-aware approach to timing modeling, based on detailed analysis of cell layouts. Our second contribution is an ILP-based maximization of 'alternate' mask coloring of instances in timing-critical paths, to minimize harmful covariance and performance variation. Third, we propose a dynamic programming-based detailed placement algorithm that solves mask coloring conflicts and can be used to ensure ¿double patterning correctness¿ after placement or even after detailed routing, while minimizing the displacement of timing-critical cells with manageable ECO impact. With a 45 nm library and open-source design testcases, our timing-aware recoloring and placement optimizations together achieve up to 232 ps (resp. 36.22 ns) reduction in worst (resp. total) negative slack, and 78% (resp. 65%) reduction in worst (resp. total) negative slack variation.","1092-3152;1558-2434","978-1-60558-800","10.1145/1687399.1687512","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5361231","","Timing;Lithography;Page description languages;Production;Optimization methods;Pulp manufacturing;Routing;Libraries;Open source software;Testing","circuit optimisation;integer programming;integrated circuit layout;integrated logic circuits;linear programming;masks;nanolithography;NOR circuits;timing","double patterning lithography;bimodal-aware timing analysis;timing yield-aware color reassignment;detailed placement perturbation;cell layouts;ILP-based maximization;alternate mask coloring;timing-critical paths;harmful covariance;dynamic programming-based detailed placement algorithm;double patterning correctness;detailed routing;timing-aware recoloring;placement optimization;negative slack;size 45 nm","","8","25","","","","","","IEEE","IEEE Conferences"
"Analysis and Optimization of the Acquisition Times in the Magnetic Resonance Imaging","G. Andria; F. Attivissimo; G. Cavone; A. M. L. Lanzolla","Dep. of Environmental Engineering and Sustainable Development, Polytechnic of Bari, Viale del Turismo, 8, 74100 Taranto, Italy. andria@misure.poliba.it; Department of Electrics and Electronics (DEE) - Polytechnic of Bari, Via E. Orabona 4, 70125 Bari. attivissimo@misure.poliba.it; Department of Electrics and Electronics (DEE) - Polytechnic of Bari, Via E. Orabona 4, 70125 Bari. cavone@misure.poliba.it; Dep. of Environmental Engineering and Sustainable Development, Polytechnic of Bari, Viale del Turismo, 8, 74100 Taranto, Italy. lanzolla@misure.poliba.it","2008 IEEE Instrumentation and Measurement Technology Conference","","2008","","","1179","1183","The main purpose of this work is to analyze the magnetic resonance imaging (MRI) methodologies used in noninvasive clinical investigation, with the aim of reducing the time of acquisition by assuring a good quality of the images. This objective is of fundamental importance to both increase the number of clinical tests produced with this equipments and to reduce the radiation doses in the patients. For this purpose the parameters influencing the time acquisition and the signal-to-noise ratio (SNR) were investigated and a software platform to optimize the imaging time acquisition was developed. This work was carried out in collaboration with Martina Franca (Italy) Hospital. Tanks to the staff of this sanitary structure it has been possible to get the necessary material for the correct development of the simulation algorithm and the sample images on which the algorithm has been applied and verified.","1091-5281","978-1-4244-1540-3978-1-4244-1541","10.1109/IMTC.2008.4547218","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4547218","magnetic resonance imaging;relaxation time;contrast image;simulator","Image analysis;Magnetic analysis;Magnetic resonance imaging;Magnetization;Magnetic field measurement;Hydrogen;Magnetostatics;Radio frequency;Biological system modeling;Humans","biomedical MRI;brain;computational complexity;dosimetry;image denoising;medical image processing;optimisation;pattern clustering","data acquisition time optimization;magnetic resonance imaging;MRI methodologies;noninvasive clinical investigation;image quality;radiation doses reduction;signal-to-noise ratio;SNR;software platform;Martina Franca Hospital;brain images;discrete clustering;sanitary structures","","1","10","","","","","","IEEE","IEEE Conferences"
"Preliminary evaluation of an energy and reserve co-optimization market design for the ISO New England","R. Entriken; M. S. DePillis","Electric Power Research Institute, Palo Alto, CA 94394 USA; ISO New England, Holyoke ME, 01040 USA","2009 IEEE Power & Energy Society General Meeting","","2009","","","1","8","This paper documents the two parallel activities, the development of a market simulation model of forward and real-time reserve trading, and the results and analysis of numerous experiments on those markets. The experiments serve to incrementally construct our understanding of these markets and to validate the performance of the simulation software. The overall character of the results is typical of economic systems with many interacting commodities utilizing scarce transport. In this case, the forward and real-time reserve products are traded along with energy, transmission, and interfaces. Interfaces act to hold transmission system capacity in reserve, by restricting the total flow over multiple lines to be less than their combined transfer capability. The complications of these interactions can be understood from the constructive process followed throughout the report. We consider the single simultaneous settlement of energy and reserve over uncongested and congested networks and also two settlements of reserve. Competitive and strategic bidding strategies play a critical role in benchmarking and in understanding the potential for the exercise of market power. Experiments with a simplified version of the ISO-NE power system match the past market performance well and help identify potential issues in data collection and market performance.","1932-5517","978-1-4244-4241","10.1109/PES.2009.5275788","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5275788","Electricity Markets;Agent-Based Simulation;Economics;Market Power;Competition;Investment;Resource Adequacy;Reserve","ISO;Analytical models;Power generation economics;Power system economics;Power system simulation;Electricity supply industry;Testing;Costs;Software performance;Investments","power markets;power system simulation;software performance evaluation","cooptimization market design;market simulation model;economic systems;strategic bidding strategies;competitive bidding strategies","","1","12","","","","","","IEEE","IEEE Conferences"
"Multicore programming techniques for high-performance ATE","M. Ravindran; J. Meisel","National Instruments, 11500 N. Mopac Expwy, Austin, TX 78759, USA; National Instruments, 11500 N. Mopac Expwy, Austin, TX 78759, USA","2008 IEEE AUTOTESTCON","","2008","","","442","446","As CPU clock rates approach their theoretical physical limits, technology is moving toward new processors with multiple processing cores. Industry experts recognize that taking advantage of multicore processors for programming applications is challenging. This paper examines the various multicore programming techniques that programmers can use to overcome these challenges for both existing and future programs and discusses the following two key focus areas of multicore technology adoption - reducing test time &amp; optimizing complex ATE analysis.","1088-7725;1558-4550","978-1-4244-2225-8978-1-4244-2226","10.1109/AUTEST.2008.4662655","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4662655","Multicore;task parallelism;data parallelism;pipelining;ATE;improving performance","Program processors;Programming;Magnetic cores;Parallel processing;Pipeline processing;Periodic structures;Software","automatic test equipment;microprocessor chips;microprogramming","automatic test equipment;multicore programming;high-performance ATE;CPU clock rates;multiple processing cores;multicore processors;ATE analysis","","1","5","","","","","","IEEE","IEEE Conferences"
"A New Approach for Decision Tree Based on Principal Component Analysis","J. Hu; J. Deng; M. Sui","NA; NA; NA","2009 International Conference on Computational Intelligence and Software Engineering","","2009","","","1","4","Classification algorithm has always been a hot issue in data mining. Decision tree algorithm is the most active part in this area, but it is a NP problem to construct the optimization decision tree. With the development of the information collection technology, the requirements of the mass data mining have become increasingly higher. When dealing with large, continuous, even with the noise and abnormal data, the traditional decision tree algorithm seems very incompetent, encountering the efficiency of the bottleneck and classification error. In this paper, there exist the shortcomings for the decision tree algorithm to deal with multi-attribute data sources. The multivariate statistical methods is proposed to make the principal component analysis on multi-attribute data, reducing dimensionality, devoicing processing and transforming the traditional decision tree algorithm to form a new algorithm model. Comparing with the traditional decision tree algorithm, the experimental results show that this method can not only simplify the decision tree model, but also can improve prediction accuracy of the decision tree.","","978-1-4244-4507","10.1109/CISE.2009.5366006","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5366006","","Decision trees;Principal component analysis;Classification tree analysis;Data mining;Tree data structures;Testing;Data engineering;Computer errors;Statistical analysis;Predictive models","data mining;decision trees;pattern classification;principal component analysis","decision tree;principal component analysis;classification algorithm;data mining;NP problem;optimization;information collection technology;multiattribute data sources;multivariate statistical method;dimensionality reduction;devoicing processing;prediction accuracy","","3","9","","","","","","IEEE","IEEE Conferences"
"Optimal sensor layout for bridge health monitoring based on dual-structure coding genetic algorithm","M. Huang; J. Li; H. Zhu","Sch. of Civil Eng. & Mech., Huazhong Univ. of Sci. & Technol., Wuhan, China; Sch. of Civil Eng. & Mech., Huazhong Univ. of Sci. & Technol., Wuhan, China; Sch. of Civil Eng. & Mech., Huazhong Univ. of Sci. & Technol., Wuhan, China","2009 International Conference on Computational Intelligence and Software Engineering","","2009","","","1","4","A novel sensor layout method is proposed for bridge health monitoring. The aim of the method is to select sensor locations from a set of possible candidate positions for achieving the best identification of modal frequencies and mode shapes. Sensor layout problem is a combinatorial optimization problem and genetic algorithm is suitable to solve the problem. Commonly, one dimension coding is often adopted in the application of genetic algorithm, in the paper, the dual-structure coding, partially matched crossover, inversion mutation and adaptive probabilities of crossover and mutation are adopted to solve the sensor layout problem. In the end, through a simulation study of a continuous beam, it can be seen that the method can overcome the disadvantages of slow convergence rate and easily getting stuck at a local optimum when traditional genetic algorithm is applied, it accelerates convergence greatly and assures to find global optimum and can be applied in health monitoring of bridge structure widely.","","978-1-4244-4507","10.1109/CISE.2009.5366481","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5366481","","Bridges;Monitoring;Genetic algorithms;Sensor systems;Testing;Costs;Mathematics;Mathematical model;Genetic mutations;Civil engineering","beams (structures);bridges (structures);combinatorial mathematics;condition monitoring;genetic algorithms;probability;structural engineering","optimal sensor layout;bridge health monitoring;dual-structure coding;genetic algorithm;sensor location;modal frequency identification;mode shape;combinatorial optimization;partially matched crossover;inversion mutation;adaptive probability;continuous beam;bridge structure","","4","10","","","","","","IEEE","IEEE Conferences"
"SVis: A Computational Steering Visualization Environment for Surface Structure Determination","J. Hernando; J. Martinez; V. Martin; M. F. Lopez; J. A. Martin-Gago","NA; NA; NA; NA; NA","2009 Second International Conference in Visualisation","","2009","","","36","39","The arrangement of atoms at the surface of a solid accounts for many of its properties: hardness, chemical activity, corrosion, etc. are dictated by the precise surface structure. Hence, finding it, has a broad range of technical and industrial applications. The ability to solve this problem opens the possibility of designing by computer materials with properties tailored to specific applications. Since the search space grows exponentially with the number of atoms, its solution cannot be achieved for arbitrarily large structures. Presently, a trial and error procedure is used: an expert proposes an structure as a candidate solution and tries a local optimization procedure on it. The solution relaxes to the local minimum in the attractor basin corresponding to the initial point, that might be the one corresponding to the global minimum or not. This procedure is very time consuming and, for reasonably sized surfaces, can take many iterations and much effort from the expert. Here we report on a visualization environment designed to steer this process in an attempt to solve bigger structures and reduce the time needed. The idea is to use an immersive environment to interact with the computation. It has immediate feedback to assess the quality of the proposed structure in order to let the expert explore the space of candidate solutions. The visualization environment is also able to communicate with the de facto local solver used for this problem. The user is then able to send trial structures to the local minimizer and track its progress as they approach the minimum. This allows for simultaneous testing of candidate structures. The system has also proved very useful as an educational tool for the field.","","978-0-7695-3734","10.1109/VIZ.2009.21","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5230705","Computational Steering;Visualization systems and Software;Graphical User Interfaces","Visualization;Surface structures;Application software;Feedback;Space exploration;Testing;Atomic layer deposition;Chemical industry;Corrosion;Computer errors","expert systems;graphical user interfaces;physics computing;surface structure;virtual reality","computational steering;visualization environment;surface structure determination;atom property;surface visualization","","","5","","","","","","IEEE","IEEE Conferences"
"Aqua-Net: An underwater sensor network architecture: Design, implementation, and initial testing","Z. Peng; Z. Zhou; J. Cui; Z. J. Shi","Computer Science and Engineering Department, University of Connecticut, Storrs, CT 06269; Computer Science and Engineering Department, University of Connecticut, Storrs, CT 06269; Computer Science and Engineering Department, University of Connecticut, Storrs, CT 06269; Computer Science and Engineering Department, University of Connecticut, Storrs, CT 06269","OCEANS 2009","","2009","","","1","8","Underwater sensor networks (UWSNs) have emerged as a powerful technique for a wide range of aquatic applications. New challenges require novel research at every layer of the protocol stack. To efficiently compare various design, algorithms, and protocols, there is a pressing need for a generic architecture for underwater sensor networks. In this paper, we present such an architecture, called Aqua-Set, for various underwater solutions and applications. Aqua-Net follows a layered structure and meanwhile could support cross-layer optimization. It is flexible and easy to use. We have implemented several protocols in Aqua-Net and tested in a lab environment. Specifically, in this paper we present the design, implementation, and testing of a MAC protocol, called UW-Aloha, as a case study. The initial results show that our analysis, and lab testing match very well. We consider Aqua-Net a groundwork and valuable tool for future advances of the underwater sensor network research.","0197-7385","978-1-4244-4960-6978-0-933957-38","10.23919/OCEANS.2009.5422199","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5422199","","Testing;Media Access Protocol;Acoustic sensors;Computer architecture;Application software;Underwater acoustics;Sensor phenomena and characterization;Access protocols;Electromagnetic scattering;Computer science","oceanographic techniques;underwater acoustic telemetry;wireless sensor networks","Aqua-Net;underwater sensor network architecture;layered structure;cross layer optimization;MAC protocol;UW-Aloha","","14","31","","","","","","IEEE","IEEE Conferences"
"Modeling and Refining Heterogeneous Systems With SystemC-AMS: Application to WSN","M. Vasilevski; F. Pecheux; N. Beilleau; H. Aboushady; K. Einwich","University Paris VI, Pierre & Marie Curie LIP6-SoC Laboratory, 75252 Paris, France, michel.vasilevski@lip6.fr; University Paris VI, Pierre & Marie Curie LIP6-SoC Laboratory, 75252 Paris, France, francois.pecheux@lip6.fr; University Paris VI, Pierre & Marie Curie LIP6-SoC Laboratory, 75252 Paris, France, nicolas.beilleau@lip6.fr; University Paris VI, Pierre & Marie Curie LIP6-SoC Laboratory, 75252 Paris, France, hassan.aboushady@lip6.fr; Fraunhofer IIS/EAS, Dresden, Germany. karsten.einwich@eas.iis.fraunhofer.de","2008 Design, Automation and Test in Europe","","2008","","","134","139","The paper presents a system-level approach for the modeling and simulation of a paradigmatic wireless sensor network composed of two nodes using SystemC-AMS, an open-source C++ extension to the OSCI SystemC standard dedicated to the description of heterogeneous systems containing digital, analog, RF hardware IPs as well as embedded software. The paper is composed of three parts. The first part details the modeled WSN (physical sensor, sigma-delta ADC, ATMEGA128 8- bit microcontroller running the embedded application, QPSK-based 2.4 GHz RF transceiver), presents the corresponding implementation in SystemC-AMS, and gives an insight on how multi-frequency simulation is handled in SystemC-AMS. The second part shows how to introduce several RF designer specifications (noise figure, IIP3, ...) into models and how to express them in SystemC-AMS. The third part proves that the combination of C++ and RF baseband equivalent dramatically reduces simulation time while keeping excellent accuracy and code readability. The paper concludes on the possibilities offered by this approach in terms of validation and optimization of heteregeneous systems through open-source simulation.","1530-1591;1558-1101","978-3-9810801-3-1978-3-9810801-4","10.1109/DATE.2008.4484675","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4484675","","Wireless sensor networks;Radio frequency;Open source software;Software standards;Hardware;Embedded software;Sensor systems and applications;Delta-sigma modulation;Microcontrollers;Application software","C++ language;microcontrollers;quadrature phase shift keying;sigma-delta modulation;wireless sensor networks","heterogeneous systems;SystemC-AMS;paradigmatic wireless sensor network;C++;OSCI SystemC standard;physical sensor;sigma-delta ADC;ATMEGA128 microcontroller;QPSK;RF transceiver;RF designer;frequency 2.4 GHz;word length 8 bit","","7","21","","","","","","IEEE","IEEE Conferences"
"Heuristic Algorithms for Efficient Wireless Multimedia Network Design","V. Pasias; D. A. Karras; R. C. Papademetriou","University of Portsmouth, UK; Automation and Hellenic Open University, Greece; University of Portsmouth, UK","32nd EUROMICRO Conference on Software Engineering and Advanced Applications (EUROMICRO'06)","","2006","","","372","378","This paper presents two novel heuristic algorithms for the design of wireless access multimedia networks. Emphasis was given to the design of CDMA based wireless multimedia networks and fixed wireless multimedia networks. The objectives of these methods are first to place a number of access points/base stations in a number of candidate sites and then to assign a number of fixed wireless terminals to the selected access points/base stations. Both methods are based on graph theory and they are essentially greedy algorithms. Except from capacity constraints, wireless reception characteristics are also considered. The algorithms are capable of designing medium and large-scale networks at polynomial time. Both algorithms were compared with an analogous optimisation problem through a series of tests. The results indicate that as regards design costs the performance of the heuristics is very close to this of the equivalent optimisation problem. The solution times for the heuristics are smaller especially when the number of the candidate access points/base stations becomes large","1089-6503;2376-9505","0-7695-2594","10.1109/EUROMICRO.2006.40","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1690161","Multimedia networks;Wireless Access;Networks;Integer Linear Programming (ILP);Heuristic Algorithms;Graph Theory.","Heuristic algorithms;Algorithm design and analysis;Femtocell networks;Multiaccess communication;Graph theory;Greedy algorithms;Large-scale systems;Polynomials;Testing;Cost function","code division multiple access;graph theory;multimedia communication;optimisation;radio access networks","heuristic algorithm;CDMA based wireless multimedia network design;fixed wireless multimedia network design;graph theory;capacity constraint;wireless reception characteristic;optimisation problem;integer linear programming","","2","10","","","","","","IEEE","IEEE Conferences"
"Efficiency of the gossip algorithm for wireless sensor networks","E. Zanaj; M. Baldi; F. Chiaraluce","Dipartimento di Elettronica, Intelligenza Artificiale e Telecomunicazioni Università Politecnica delle Marche Ancona, Italy; Dipartimento di Elettronica, Intelligenza Artificiale e Telecomunicazioni Università Politecnica delle Marche Ancona, Italy; Dipartimento di Elettronica, Intelligenza Artificiale e Telecomunicazioni Università Politecnica delle Marche Ancona, Italy","2007 15th International Conference on Software, Telecommunications and Computer Networks","","2007","","","1","5","Gossip is a well-known technique for distributed computing in an arbitrarily connected network of nodes. The gossip algorithm, which is very simple to implement, takes into account strong limitations in computational, communication and energy resources that usually characterize nodes in sensor networks. In gossip, the computational burden is distributed among the nodes, and computation and communication are managed in a very quick and efficient way: in essence, each node acquires its own measure and, starting from it, exchanges information with its neighbors, according with a connection probability distribution. In this paper, we study the performance of gossip algorithms, aiming to test the agreement between analytical results on the averaging time and the actual convergence time for specific scenarios, estimated through numerical simulations. Furthermore, we apply an optimization technique, recently appeared in the literature, that theoretically allow to accelerate convergence. Numerical simulations permit to assess the effects of optimization in real cases, thus evaluating the actual improvement in performance it achieves.","","978-953-6114-93-1978-953-6114-95","10.1109/SOFTCOM.2007.4446065","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4446065","","Wireless sensor networks;Distributed computing;Numerical simulation;Computer networks;Energy resources;Sensor phenomena and characterization;Probability distribution;Testing;Performance analysis;Algorithm design and analysis","distributed algorithms;numerical analysis;optimisation;probability;wireless sensor networks","gossip algorithm;wireless sensor networks;distributed computing;probability distribution;numerical simulations;optimization technique","","8","8","","","","","","IEEE","IEEE Conferences"
"A Memetic Algorithm for VLSI Floorplanning","M. Tang; X. Yao","NA; NA","IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)","","2007","37","1","62","69","Floorplanning is an important problem in very large scale integrated-circuit (VLSI) design automation as it determines the performance, size, yield, and reliability of VLSI chips. From the computational point of view, VLSI floorplanning is an NP-hard problem. In this paper, a memetic algorithm (MA) for a nonslicing and hard-module VLSI floorplanning problem is presented. This MA is a hybrid genetic algorithm that uses an effective genetic search method to explore the search space and an efficient local search method to exploit information in the search region. The exploration and exploitation are balanced by a novel bias search strategy. The MA has been implemented and tested on popular benchmark problems. Experimental results show that the MA can quickly produce optimal or nearly optimal solutions for all the tested benchmark problems","1083-4419;1941-0492","","10.1109/TSMCB.2006.883268","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4067090","Floorplanning;genetic algorithm (GA);local search;memetic algorithm (MA);very large scale integrated circuit (VLSI)","Very large scale integration;Search methods;Genetic algorithms;Space technology;Design automation;Space exploration;Circuit testing;Benchmark testing;Integrated circuit interconnections;NP-hard problem","circuit layout CAD;circuit optimisation;computational complexity;genetic algorithms;integrated circuit layout;search problems;VLSI","memetic algorithm;VLSI floorplanning problem;very large scale integrated-circuit chip;design automation;NP-hard problem;hybrid genetic algorithm;local search method;optimal solution","Algorithms;Artificial Intelligence;Biomimetics;Computer Simulation;Computer-Aided Design;Equipment Design;Equipment Failure Analysis;Models, Theoretical;Semiconductors;Software;Systems Theory","88","18","","","","","","IEEE","IEEE Journals & Magazines"
"An evolutionary algorithm for optimization of XML publish/subscribe middleware in electronics production","I. M. Delamer; J. L. Martinez Lastra; O. Perez","NA; NA; NA","Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA 2006.","","2006","","","681","688","Computer aided manufacturing using XML (CAMX) production systems are built on Message-Oriented Middleware Frameworks, offering standards-based communication among machines and control software applications. CAMX Frameworks implement Publish/Subscribe of XML messages through an entity called the Message Broker (MSB). In order to create scalable frameworks, distributed MSB systems need to be deployed. In the distributed case, the topology optimization problem arises, as clients need to be assigned to one of several available MSB nodes. The problem is a variation of the graph partitioning problem, known to be NP-complete. The problem is augmented with weighted directed edges, multiple levels of cluster capacity, complex weight functions, and multiple optimization criteria. A solution was developed based on an evolutionary multi-objective algorithm. The developed algorithm was tested for optimizing two flexible electronics assembly systems used as case scenarios. Robust convergence time was bounded to a few seconds, demonstrating the suitability of the proposed algorithm for rapid topology reconfiguration in response to changes in the system","1050-4729","0-7803-9505","10.1109/ROBOT.2006.1641789","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1641789","","Evolutionary computation;XML;Clustering algorithms;Message-oriented middleware;Partitioning algorithms;Electronic equipment testing;Computer aided manufacturing;Production systems;Communication system control;Control systems","assembling;computational complexity;computer aided manufacturing;control engineering computing;electronics industry;evolutionary computation;flexible manufacturing systems;graph theory;middleware;XML","evolutionary algorithm;XML Publish/Subscribe Middleware;computer aided manufacturing;CAMX production systems;Message-Oriented Middleware Frameworks;standards-based communication;control software;Message Broker;distributed systems;topology optimization problem;graph partitioning problem;NP-complete problem;evolutionary multi-objective algorithm;flexible electronics assembly systems","","1","22","","","","","","IEEE","IEEE Conferences"
"DATE 2007 ""Best Industrial Designs"" Session: From Algorithm to First 3.5G Call in Record Time - A Novel System Design Approach Based on Virtual Prototyping and its Consequences for Interdisciplinary System Design Teams","M. Brandenburg; A. Schollhorn; S. Heinen; J. Eckmuller; T. Eckart","Infineon Technologies AG, Munich, Germany; Infineon Technologies AG, Munich, Germany; Infineon Technologies AG, Munich, Germany; Infineon Technologies AG, Munich, Germany; Infineon Technologies AG, Munich, Germany","2007 Design, Automation & Test in Europe Conference & Exhibition","","2007","","","1","3","Increasing system complexity not only in wireless communications forces design teams to avoid errors during the process of system refinement thereby keeping ambiguities during system implementation at a minimum. On the other hand the chosen system design approach has to ensure that a system design project rapidly advances through all stages of refinement from an algorithmic model to a real ""system on chip"" (SoC) while maintaining backwards equivalence of the produced HW and FW/SW code with the original algorithmic model. This system design challenge also demands a new interdisciplinary team approach encompassing all design skills ranging from concept to HW and FW/SW engineering as well as system verification to increase the overlap in the system concept, implementation and verification phase. But how do these interdisciplinary teams cooperate efficiently, as they are used to metaphorically ""speak different design languages""? Resulting in an industry record development time for a 3.5G UMTS modem the employment of a novel system design approach is shown which serves as common system design language, avoiding the babylonian language disaster of isolated engineering worlds. The motivation for an increasing overlap of system concept, implementation and verification phases is obvious: it can save time (to market) in the magnitude of several months or even more and thus drastically shorten design cycles by parallel development of HW and FW/SW. The proposed approach also helps to avoid costly redesign cycles due to conceptual errors and optimizes the quality of the developed system HW and FW/SW thereby also substantially reducing system development R&amp;D costs","1530-1591;1558-1101","978-3-9810801-2","10.1109/DATE.2007.364394","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4211904","","Algorithm design and analysis;Virtual prototyping;Refining;Design engineering;Wireless communication;System-on-a-chip;Maintenance engineering;Natural languages;3G mobile communication;Modems","microprocessor chips;software prototyping","industrial designs;virtual prototyping;interdisciplinary system;increasing system complexity;wireless communications;system on chip","","2","1","","","","","","IEEE","IEEE Conferences"
"A novel approach for the identification of totally symmetric Boolean functions in the application of efficient system design","G. Paul; A. Tiwari; A. Pal; C. R. Mandal","Department of Computer Science and Engg., Indian Institute of Technology - Kharagpur, WB - 721302, INDIA; Department of Computer Science and Engg., Indian Institute of Technology - Kharagpur, WB - 721302, INDIA; Department of Computer Science and Engg., Indian Institute of Technology - Kharagpur, WB - 721302, INDIA; Department of Computer Science and Engg., Indian Institute of Technology - Kharagpur, WB - 721302, INDIA","2008 3rd International Design and Test Workshop","","2008","","","243","248","Identification of totally symmetric function (TSF) is one of the important methods to achieve an efficient system design with minimized cost. Finding TSF is sometimes very useful in the application like VLSI testing, logic and physical deign, cryptology etc. apart from its wide use in optimization problems. McCluskey had already established the method of decomposition to compute TSF. Later Biswas came up with an enhanced approach using the method of ordered partition, but the method suffers from excessive and complex computation for functions with large number of variables. We, in this paper, claim that some of Biswas's assertions for finding TSF and its related theorems are incorrect and also logically prove our claim. We also propose a new efficient and simpler solution for finding TSF in much fewer steps than Biswas's method. Result and examples show that the proposed technique works significantly faster than the Biswas's method with the gain of 43% in computational speed.","2162-0601;2162-061X","978-1-4244-3479-4978-1-4244-3478","10.1109/IDT.2008.4802506","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4802506","Totally symmetric function;Elementary symmetric function;Unity ratio;Decomposition;Ordered partition;Indicator partition","Boolean functions;Logic testing;Application software;Cryptography;Design optimization;Computer science;Cost function;Very large scale integration;Delay;Binary decision diagrams","Boolean functions","efficient system design;cost minimisation;ordered partition;Biswas's assertions;totally symmetric Boolean functions identification","","","12","","","","","","IEEE","IEEE Conferences"
"Stochastic Modeling and Optimization for Robust Power Management in a Partially Observable System","Q. Qiu; Y. Tan; Q. Wu","Department of Electrical and Computer Engineering, Binghamton University, State University of New York, Binghamton, New York 13902, USA, qqiu@binghamton.edu; Department of Electrical and Computer Engineering, Binghamton University, State University of New York, Binghamton, New York 13902, USA, ytan3@binghamton.edu; Department of Electrical and Computer Engineering, Binghamton University, State University of New York, Binghamton, New York 13902, USA, qwu@binghamton.edu","2007 Design, Automation & Test in Europe Conference & Exhibition","","2007","","","1","6","As the hardware and software complexity grows, it is unlikely for the power management hardware/software to have a full observation of the entire system status. In this paper, we propose a new modeling and optimization technique based on partially observable Markov decision process (POMDP) for robust power management, which can achieve near-optimal power savings, even when only partial system information is available. Three scenarios of partial observations that may occur in an embedded system are discussed and their modeling techniques are presented. The experimental results show that, compared with power management policy derived from traditional Markov decision process model that assumes the system is fully observable, the new power management technique gives significantly better performance and energy tradeoff","1530-1591;1558-1101","978-3-9810801-2","10.1109/DATE.2007.364385","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4211895","","Stochastic systems;Power system modeling;Robustness;Energy management;Power system management;Hardware;Stochastic processes;Engineering management;Energy consumption;Embedded system","embedded systems;integrated circuit modelling;low-power electronics;Markov processes","stochastic modeling;stochastic optimization;robust power management;partially observable system;partially observable Markov decision process;partial system information;embedded system","","10","14","","","","","","IEEE","IEEE Conferences"
"Influence of development parameters on electron resist contrast","M. A. Knyazev; A. A. Svintsov; S. I. Zaitsev","Institute of Microelectronics Technology, Russian Academy of Sciences, 142432 Chernogolovka, Moscow region, Russia; Institute of Microelectronics Technology, Russian Academy of Sciences, 142432 Chernogolovka, Moscow region, Russia; Institute of Microelectronics Technology, Russian Academy of Sciences, 142432 Chernogolovka, Moscow region, Russia, e-mail: zaitsev@ipmt-hpm.ac.ru","2007 Digest of papers Microprocesses and Nanotechnology","","2007","","","92","93","A 3D micro/nano structuring is finding more and more applications in different fields of since at last time. One of the 3D micro/nano structuring method is e-beam lithography. Measurement of electron resist characteristics especially contrast is very important for 3D structure fabrication. For this purposes the new express method for contrast determination was developed. This work is devoted to application of the method to investigation of contrast dependence on development conditions. It is based on an expression for resist development velocity: nu = nu<sub>0</sub> (D/D<sub>0</sub>)<sup>gamma</sup> where v is development rate of the electron resist exposed with dose D, nu<sub>0</sub> is the development rate of the electron resist exposed with dose D<sub>0</sub> (resist sensitivity), and gamma is a contrast. The main feature of the method is the specially designed 3D test structure ""dose wedge"" in detail described. Three different profiles can, generally, be revealed after the development of the exposed ""dose wedge"". As shown, when gamma is higher than the real resist contrast gamma<sub>exp</sub> the profile is concave, when gamma &lt; gamma<sub>exp</sub> the profile is convex, and when gamma and gamma<sub>exp</sub> are equal the profile is plane. Therefore, when gamma = gamma<sub>exp</sub> the change in the residual resist thickness is linear. The real test structures consist of several dose wedges. The article shows the results of the test structure development of a PMMA 950 K. resist exposed by a JEOL 840 A under NanoMaker hardware/software system control at 25 kV. The exposure data were designed using the software part of the NanoMaker. When the residual resist thickness grows linearly, the distance between neighboring intensity minima or maxima is constant. In the case gamma &gt; gamma<sub>exp</sub>, the distance decreases with a thickness increment and, vice versa, if gamma &lt; gamma<sub>exp</sub> the distance between minima or maxima increases with a thickness increment. For the test structure, the value of the resist contrast is 7. The considered method is similar to approach developed for measurements of proximity function parameters. The main advantage of the method in comparison with others is it speed (design, exposure, development and observation take typically 30 minutes). So this method gives possibility to investigate resist contrast as function of different lithography parameters for acceptable time. The results of the investigations can help to find ways for resist contrast manipulating. As example of the method application the contrast of the PMMA 950 K. for two development parameters were investigated. First, the method was used for the investigation of the PMMA 950 K. temperature contrast dependence. The test structures were exposed at 25 kV in the PMMA 950 K film and then they were developed in IPA-H<sub>2</sub>O 8:1 for different temperatures. A significant influence of the developer temperature was observed, and as shown in fig 3a, contrast decreases in two times when temperature changes from 21.5degC to 37.5degC. Second, the contrast for four volume ratio of the developer components IPA and H<sub>2</sub>O was found and graphed in fig. 3 b. So developer temperature and the developer staff have material effect on resist contrast value and these factors could be now quantitatively used for development control and optimization.","","978-4-9902472-4","10.1109/IMNC.2007.4456120","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4456120","","Electrons;Resists;Temperature;Lithography;Fabrication;Software testing;System testing;Hardware;Software systems;Control systems","electron resists;nanostructured materials;nanotechnology;optical microscopy;polymer films","electron resist;3D micro-nano structuring method;e-beam lithography;3D structure fabrication;PMMA resist;PMMA film;optical microscopy;temperature 21.5 C to 37.5 C","","","4","","","","","","IEEE","IEEE Conferences"
"Optimizing the Generation of Object-Oriented Real-Time Embedded Applications Based on the Real-Time Specification for Java","M. A. Wehrmeister; C. E. Pereira; L. B. Becker","Computer Science Institute, UFRGS, Brazil, mawehrmeister@inf.ufrgs.br; NA; NA","Proceedings of the Design Automation & Test in Europe Conference","","2006","1","","1","6","The object-oriented paradigm has become popular over the last years due to its characteristics that help managing the complexity in computer systems design. This feature also attracted the embedded systems community, as today's embedded systems need to cope with several complex functionalities as well as timing, power, and area restrictions. Such scenario has promoted the use of the Java language and its real-time extension (RTSJ) for embedded real-time systems design. Nevertheless, the RTSJ was not primarily designed to be used within the embedded domain. This paper presents an approach to optimize the use of the RTSJ for the development of embedded real-time systems. Firstly, it describes how to design real-time embedded applications using an API based on RTSJ. Secondly, it shows how the generated code is optimized to cope with the tight resources available, without interfering in the mandatory timing predictability of the generated system. Finally it discusses an approach to synthesize the applications on top of affordable FPGAs. The approach used to synthesize the embedded real-time system ensures a bounded timing behavior of the object-oriented aspects of the application, like the polymorphism mechanism and read/write access to object's data fields","1530-1591;1558-1101","3-9810801-1","10.1109/DATE.2006.244147","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1657000","","Java;Real time systems;Application software;Embedded system;Timing;Virtual machining;Computer science;Design engineering;Design automation;Automatic control","application program interfaces;embedded systems;field programmable gate arrays;Java;logic design;microprocessor chips","object-oriented real-time embedded applications;real-time specification;object-oriented paradigm;computer systems design;embedded systems community;Java language;real-time extension;embedded real-time systems design;field programmable gate arrays","","5","22","","","","","","IEEE","IEEE Conferences"
"Methods of Information Extraction and Analysis in Data Stream","F. Liu; Q. Tan; G. Zhang","School of Computer Science & Engineering, South China University of Technology, Guangzhou, 510640, P. R. China. fgliu@scut.edu.cn, software_james@163.com; School of Computer Science & Engineering, South China University of Technology, Guangzhou, 510640, P. R. China. tank_cs@163.comr, software_james@163.com; School of Computer Science & Engineering, South China University of Technology, Guangzhou, 510640, P. R. China. software_james@163.com","2007 2nd International Conference on Pervasive Computing and Applications","","2007","","","246","250","With the development of communication technology, information extraction and analysis in data stream becomes more and more universal in embedded pervasive computing systems. The increasing size of the data stream often makes the process of extraction and analysis becomes a bottleneck of the system, especially for embedded system with limited resource. This paper researches on various methods of information extraction and analysis in data stream, analyzes their advantages, disadvantages, applying occasions and ways to optimize the technology. Then it elaborates on optimized algorithms based on load balancing multithreading, which can also serve as a useful reference for the optimization of the key process or thread that may become the bottleneck in the embedded system. The optimization is applied into PTV (personalized TV), a digital set-top box, for the extraction and analysis on PSI and SI information in TS stream. The testing result proves that it can significantly reduce the executing time. At last, it puts forward the improving direction of the optimized algorithms using load-balancing multithreading based on priority.","","978-1-4244-0970-9978-1-4244-0971","10.1109/ICPCA.2007.4365448","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4365448","Information Extraction and Analysis;Multi-threading;Load-Balancing;Priority;PTV","Data mining;Data analysis;Information analysis;Embedded system;Multithreading;Communications technology;Pervasive computing;Optimization methods;Load management;Yarn","data analysis;digital television;embedded systems;information analysis;information retrieval;multi-threading;resource allocation;ubiquitous computing","information extraction;information analysis;data stream analysis;embedded pervasive computing systems;load balancing multithreading;personalized TV;digital set-top box","","","15","","","","","","IEEE","IEEE Conferences"
"Insulation design and analysis on high voltage vacuum interrupter","Xiu Shixin; Zhang Rui; Pang Xianhai","(State Key Lab of Electrical Insulation and Power Equipment, Xi'an Jiaotong University, 710049, China); (State Key Lab of Electrical Insulation and Power Equipment, Xi'an Jiaotong University, 710049, China); (State Key Lab of Electrical Insulation and Power Equipment, Xi'an Jiaotong University, 710049, China)","2008 23rd International Symposium on Discharges and Electrical Insulation in Vacuum","","2008","2","","410","413","In this paper, the structure of vacuum interrupter was designed and simulated, and the factors affecting the electric field distribution inside vacuum interrupter were analyzed. After simulating the electric field distribution by using 3D finite element software Ansoft, the main shield structure and electrode contour were designed and studied. By changing electrode contour, main shield structure, the gap length between electrode and shield and adding equalized shield, the insulation structure was optimized. Based on the design and analysis in this paper, the vacuum interrupter was manufactured. When the contact gap length was 60 mm, it passed withstand voltage test including 230 kV power frequency voltage test and 550 kV impulse voltage test.","1093-2941","978-973-755-382-9978-1-4244-2841","10.1109/DEIV.2008.4676815","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4676815","","Voltage;Interrupters;Switches;Dielectrics and electrical insulation;Electrodes;Impulse testing;Glass;Analytical models;Vacuum technology;Convergence","electrodes;finite element analysis;vacuum insulation;vacuum interrupters","insulation structure design;high voltage vacuum interrupter simulation;electric field distribution;3D finite element software Ansoft;shield structure;electrode contour;contact gap length;equalized shield;power frequency voltage test;impulse voltage test;voltage 230 kV;voltage 550 kV;distance 60 mm","","1","5","","","","","","IEEE","IEEE Conferences"
"A networking requirement application by multi-objective genetic algorithms with Sexual Selection","S. Sodsee; P. Meesad; Z. Li; W. Halang","Faculty of Mathematics and Computer Science, Fern Universität in Hagen, Germany; Faculty of Technical Education, King Mongkut's University of Technology North Bangkok, Thailand; Faculty of Mathematics and Computer Science, Fern Universität in Hagen, Germany; Faculty of Mathematics and Computer Science, Fern Universität in Hagen, Germany","2008 3rd International Conference on Intelligent System and Knowledge Engineering","","2008","1","","513","518","This paper is aimed to present a genetic algorithm focusing on the sexual selection used the Pareto based approach for solving multi-objective optimization problems. It uses a concept of sexual selection with different types of gender and mutation rates based on the sex to produce offspring. Its performance was evaluated by the well-known benchmark functions as well as also tested with a networking requirement application, which is a multi-objective optimization problem. The proposed algorithm could successfully find solutions that meet user¿s requirements.","","978-1-4244-2196-1978-1-4244-2197","10.1109/ISKE.2008.4730985","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4730985","","Genetic algorithms;Intelligent systems;Knowledge engineering;Evolutionary computation;Intelligent networks;Application software;Mathematics;Computer science;Computer science education;Systems engineering education","computer networks;formal specification;genetic algorithms;graphical user interfaces;Pareto optimisation","computer network requirement;multiobjective genetic algorithm;sexual selection;Pareto optimization;gender issue;mutation rate;GUI;formal specification","","1","9","","","","","","IEEE","IEEE Conferences"
"DDP: a tool for life-cycle risk management","S. L. Cornford; M. S. Feather; K. A. Hicks","Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA; Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA; Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA","IEEE Aerospace and Electronic Systems Magazine","","2006","21","6","13","22","At JPL we have developed and implemented a process for achieving life-cycle risk management. This process has been embodied in a software tool and is called defect detection and prevention (DDP). The DDP process can be succinctly stated as: determine where we want to be, what could get in the way and how we will get there. The ""determine where we want to be"" is captured as trees of requirements and the ""what could get in the way"" is captured as trees of potential failure modes. Scoring the impacts of these failure modes on the requirements results in a prioritized set of failure modes. The user then selects from a set of preventative measures, analyses, process controls and tests (PACTs) each of which has an effectiveness versus the various failure modes. It is the goal of the DDP process to optimally select the subset of the PACTs which minimizes the residual risk subject to the project resource constraints. The DDP process is intended to facilitate risk management over the entire project life cycle beginning with architectural and advanced technology decisions all the way through operation. As the project design, technology content, and implementation approach matures, the requirements and failure mode trees are elaborated upon to accommodate the additional information. Thus, the DDP process is a systematic, continuous, top-down approach to managing risk. Implementation of the DDP process requires a critical mass of expertise (usually the project team and a few specialists) and captures both their engineering judgement as well as available quantitative data. This additional data may result from models, layouts, prototype testing, other focused risk evaluations and institutional experiences. The DDP process also identifies areas where additional information would be advantageous, thus allowing a project to target critical areas of risk or risk uncertainty. This also allows the project to identify those areas which would benefit the most from application of other quantitative tools and methods (e.g. Monte Carlo simulations, FMECAs, fault trees). The software tool supports the DDP process by providing guidance for implementing the process steps, graphical visualizations of the various trees, their interrelationships and the current risk landscape. The tool is capable of supporting on-the-fly knowledge elicitation as well as integrating off-line deliberations. There are a variety of available outputs including graphs, trees and reports as well as clear identification of the driving requirements, ""tall-pole"" residual risks and the PACTs which have been selected and agreed upon. The DDP process has been applied at various levels of assembly including the system and subsystem levels, as well as down to the component level. Recently, significant benefits have been realized from application to advanced technologies, where the focus has been on increasing the infusion rates of these technologies by identification and mitigation of risks prior to delivery to a project","0885-8985;1557-959X","","10.1109/MAES.2006.1662004","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1662004","","Risk management;Software tools;Testing;Tree graphs;Failure analysis;Process control;Data engineering;Design engineering;Prototypes;Uncertainty","aerospace computing;failure analysis;project management;risk management;software tools","life-cycle risk management;software tool;defect detection and prevention;failure modes;preventative measures;process controls;project resource constraints;project design;technology content;risk uncertainty;on-the-fly knowledge elicitation","","4","8","","","","","","IEEE","IEEE Journals & Magazines"
"Implementing OpenGL ES on OpenGL","H. Lee; N. Baek","Huone Inc., Bokhyun-dong 573-13, Daegu 702-020, Korea; School of EECS, Kyungpook National University, Daegu 702-701, Korea","2009 IEEE 13th International Symposium on Consumer Electronics","","2009","","","999","1003","We present an OpenGL ES implementation which utilizes the existing OpenGL library, aiming to support various embedded systems in current consumer markets. Nowadays OpenGL ES becomes an improved version of OpenGL for embedded systems through introducing new features including the fixed-point numeric type. We optimized arithmetic operations on its specific data types such as fixed-point numbers, and achieved totally new optimized implementations of newly introduced features. Efficient ways of parameter conversions between our OpenGL ES implementation and underlying OpenGL are also accomplished even with strictly obeying the standard specifications. Our final implementation result of OpenGL ES 1.1 library completely provides more than 200 API functions in the standard specification and satisfies all the conformance tests. From the efficiency point of view, we compared execution speeds of real world applications to existing commercial implementations to finally show at most 33.147 times speed-ups, which is fastest among the same category implementations.","0747-668X;2159-1423","978-1-4244-2975-2978-1-4244-2976","10.1109/ISCE.2009.5156990","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5156990","OpenGL ES;embedded system;3D graphics library;software implementation","Embedded system;Graphics;Hardware;Application software;Software libraries;Embedded software;Consumer electronics;Fixed-point arithmetic;Testing;IEEE news","computer graphics","OpenGL ES;embedded systems;fixed-point numeric type;API functions","","5","20","","","","","","IEEE","IEEE Conferences"
"Robust non-preemptive hard real-time scheduling for clustered multicore platforms","M. Lombardi; M. Milano; L. Benini","DEIS - Università di Bologna, Italy; DEIS - Università di Bologna, Italy; DEIS - Università di Bologna, Italy","2009 Design, Automation & Test in Europe Conference & Exhibition","","2009","","","803","808","Scheduling task graphs under hard (end-to-end) timing constraints is an extensively studied NP-hard problem of critical importance for predictable software mapping on Multiprocessor System-on-chip (MPSoC) platforms. In this work we focus on an off-line (design-time) version of this problem, where the target task graph is known before execution time. We address the issue of scheduling robustness, i.e. providing hard guarantees that the schedule will meet the end-to-end deadline in presence of bounded variations of task execution times expressed as min-max intervals known at design time. We present a robust scheduling algorithm that proactively inserts sequencing constraints when they are needed to ensure that execution will have no inserted idle times and will meet the deadline for any possible combination of task execution times within the specified intervals. The algorithm is complete, i.e. it will return a feasible graph augmentation if one exists. Moreover, we provide an optimization version of the algorithm that can compute the shortest deadline that can be met in a robust way.","1530-1591;1558-1101","978-1-4244-3781-8978-3-9810-8015","10.1109/DATE.2009.5090773","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5090773","","Robustness;Multicore processing;Scheduling algorithm;Processor scheduling;Iterative algorithms;Timing;Resource management;Runtime;Real time systems;NP-hard problem","computational complexity;microprocessor chips;multiprocessing systems;optimisation;processor scheduling;real-time systems;system-on-chip","nonpreemptive hard real-time scheduling;clustered multicore platform;scheduling task graph;NP-hard problem;software mapping;multiprocessor system-on-chip platform;MPSoC platform;graph augmentation;optimization","","6","26","","","","","","IEEE","IEEE Conferences"
"Optimization of Return Loss Parameter of Feed Network of Turnstile Antenna in LEO Satellites Application","K. Keyghobad; J. M. Baabuei; T. Heydari","Islamic Azad University, Bonab Branch, Eastern Azerbaijan, Iran. kiyan43@yahoo.com; Islamic Azad University, Bonab Branch, Eastern Azerbaijan, Iran. jbaabuei@yahoo.com; Islamic Azad University, Bonab Branch, Eastern Azerbaijan, Iran. teimur_h@yahoo.com","The Second European Conference on Antennas and Propagation, EuCAP 2007","","2007","","","1","3","This paper , design of a turnstile antenna in UHF band (465 MHz) is carried out. optimum values for its main physical parameters such as monopole lengths, monopoles distance and angles are obtained by using HFSSV10 software. Antenna feed network is designed with microstrip lines to achieve in to the 90 degrees phase difference between adjacent monopoles and small size, simultaneously. Obtained scattering parameters from antenna design are used with microstrip feed network in Microwave Office software to achieve the desired VSWR characteristic of antenna at 465 MHz. In next step, the line and stub lengths are optimized in order to achieve a return loss better than 20 dB in operation frequency. Variable lumped capacitors are used in designed feed network for frequency fine tuning. The designed antenna and feed network are fabricated and tested. The diagrams represent a very good agreement between simulation and measurement.","0537-9989","978-0-86341-842","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4458689","LEO satellite;antenna;turnstile;VSWR","","electrical engineering computing;microstrip antennas;microwave antennas;monopole antennas;UHF antennas","return loss parameter;turnstile antenna;LEO satellites;monopole lengths;monopoles distance;antenna feed network;microstrip lines;antenna design;microstrip feed network;microwave office software","","","","","","","","","IET","IET Conferences"
"Effect of Battery Charge on Energy Consumption","H. Heinimäki; P. Niska; J. Ruutu","NA; NA; NA","2009 15th IEEE Real-Time and Embedded Technology and Applications Symposium","","2009","","","215","220","The power consumption of a mobile device has been measured with a continuously repeated multimedia use case starting with a full battery charge until the battery empties. We have discovered a clear dependency between the charge level of the battery and the power consumption of the device. The origin of this finding is associated with the DC-DC conversion made between the battery and the components inside the mobile device. The practical consequence is that e.g. software developers optimizing the energy consumption of their code must be aware of this phenomenon since minor changes in the code may be easily shadowed by the found effect. We provide some advice how to avoid or alleviate the error caused by the battery charge level.","1545-3421","978-0-7695-3636","10.1109/RTAS.2009.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4840582","software testing;software performance;energy management;DC-DC power conversion","Batteries;Energy consumption;Regulators;Voltage;Energy measurement;Power measurement;Energy management;DC-DC power converters;Software measurement;Hardware","cellular radio;computerised instrumentation;DC-DC power convertors;energy consumption;power engineering computing;secondary cells;voltage measurement","battery charge effect;energy consumption;mobile device;DC-DC conversion","","3","7","","","","","","IEEE","IEEE Conferences"
"Quick Design of Fuzzy Controllers With Good Interpretability in Mobile Robotics","M. Mucientes; J. Casillas","NA; NA","IEEE Transactions on Fuzzy Systems","","2007","15","4","636","651","This paper presents a methodology for the design of fuzzy controllers with good interpretability in mobile robotics. It is composed of a technique to automatically generate a training data set plus an efficient algorithm to learn fuzzy controllers. The proposed approach obtains a highly interpretable knowledge base in a very reduced time, and the designer only has to define the number of membership functions and the universe of discourse of each variable, together with a scoring function. In addition, the learned fuzzy controllers are general because the training set is composed of a number of automatically generated examples that cover the universe of discourse of each variable uniformly and with a predefined precision. The methodology has been applied to the design of a wall-following and moving object following behavior. Several tests in simulated environments using the Nomad 200 robot software and a comparison with another learning method show the performance and advantages of the proposed approach.","1063-6706;1941-0034","","10.1109/TFUZZ.2006.889889","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4286974","Ant colony optimization;behavior design;fuzzy control;learning;mobile robot navigation","Fuzzy control;Mobile robots;Robotics and automation;Automatic control;Fuzzy sets;Automatic generation control;Design methodology;Training data;Software testing;Software performance","control system CAD;fuzzy control;learning systems;mobile robots","fuzzy controller design;mobile robotics;scoring function;fuzzy controller learning;Nomad 200 robot software","","50","45","","","","","","IEEE","IEEE Journals & Magazines"
"Genetic algorithm implementation for distributed security systems optimization","P. Bykovyy; Y. Pigovsky; V. Kochan; A. Sachenko; G. Markowsky; S. Aksoy","Ternopil National Economic University, Research Institute of Intelligent Computer Systems, Ukraine; Ternopil National Economic University, Research Institute of Intelligent Computer Systems, Ukraine; Ternopil National Economic University, Research Institute of Intelligent Computer Systems, Ukraine; Department of Computer Science, Silesian University of Technology, 26 Roosevelta St, Zabrze, 41-800, Poland; Department of Computer Science, University of Maine, Orono, ME, USA; Electronics Engineering Department, Gebze Institute of Technology, Turkey","2008 IEEE International Conference on Computational Intelligence for Measurement Systems and Applications","","2008","","","120","124","This paper describes two algorithms for optimizing the design of distributed perimeter security systems. The first algorithm is a straightforward algorithm whose primary purpose is to help us formulate the problem formally. The second algorithm, is a genetic algorithm. Both algorithms were incorporated into a new module for our distributed security systems CAD software. Tests of the module show that it produced better solutions more quickly than any other algorithms that are known to us.","2159-1547;2159-1555","978-1-4244-2305-7978-1-4244-2306","10.1109/CIMSA.2008.4595845","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4595845","distributed security systems;genetic algorithms;security system optimization","Security;Artificial neural networks;Genetic algorithms;Sensors;Biological cells;Algorithm design and analysis;Design automation","CAD;distributed processing;genetic algorithms;security of data","genetic algorithm;distributed security systems optimization;CAD software","","1","16","","","","","","IEEE","IEEE Conferences"
"Population-Based Incremental Learning With Associative Memory for Dynamic Environments","S. Yang; X. Yao","NA; NA","IEEE Transactions on Evolutionary Computation","","2008","12","5","542","561","In recent years, interest in studying evolutionary algorithms (EAs) for dynamic optimization problems (DOPs) has grown due to its importance in real-world applications. Several approaches, such as the memory and multiple population schemes, have been developed for EAs to address dynamic problems. This paper investigates the application of the memory scheme for population-based incremental learning (PBIL) algorithms, a class of EAs, for DOPs. A PBIL-specific associative memory scheme, which stores best solutions as well as corresponding environmental information in the memory, is investigated to improve its adaptability in dynamic environments. In this paper, the interactions between the memory scheme and random immigrants, multipopulation, and restart schemes for PBILs in dynamic environments are investigated. In order to better test the performance of memory schemes for PBILs and other EAs in dynamic environments, this paper also proposes a dynamic environment generator that can systematically generate dynamic environments of different difficulty with respect to memory schemes. Using this generator, a series of dynamic environments are generated and experiments are carried out to compare the performance of investigated algorithms. The experimental results show that the proposed memory scheme is efficient for PBILs in dynamic environments and also indicate that different interactions exist between the memory scheme and random immigrants, multipopulation schemes for PBILs in different dynamic environments.","1089-778X;1089-778X;1941-0026","","10.1109/TEVC.2007.913070","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4454713","Associative memory scheme;dynamic optimization problems (DOPs);immune system-based genetic algorithm (ISGA);memory-enhanced genetic algorithm;multipopulation scheme;population-based incremental learning (PBIL);random immigrants","Associative memory;Genetic algorithms;Computer science;Design optimization;Evolutionary computation;System testing;Councils;Computer applications;Application software;Laboratories","content-addressable storage;evolutionary computation;learning (artificial intelligence)","population-based incremental learning;associative memory;dynamic environments;evolutionary algorithms;dynamic optimization problems;multiple population schemes;random immigrants","","141","48","","","","","","IEEE","IEEE Journals & Magazines"
"The Study on Knowledge Flow Efficiency Measure of Weighted RIN","Z. Zheng; B. Han","NA; NA","2008 International Workshop on Modelling, Simulation and Optimization","","2008","","","15","18","This paper puts forward the concept of knowledge exchange rate for describing precise essence of RIN. Using knowledge exchange rate explains weighted RIN graph and studies knowledge exchange distance among members under circumstance of strong ties and weak ties. This paper also puts forward knowledge flow efficiency estimation model after modifying network information exchange model of V.Latora &amp; M.Marchiori and has tested the model.","","978-0-7695-3484","10.1109/WMSO.2008.46","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4756947","","Fluid flow measurement;Weight measurement;Publishing;Books;Conference management;Technology management;Engineering management;Computer Society;Software engineering;Computer science","graph theory;innovation management;knowledge management","knowledge flow efficiency measure;knowledge exchange rate;weighted RIN graph;flow efficiency estimation model;network information exchange model;regional innovation network","","","10","","","","","","IEEE","IEEE Conferences"
"The Research on Building Enterprise Knowledge Management Performance Evaluating Indicator System","Tianyilin","NA","2008 International Workshop on Modelling, Simulation and Optimization","","2008","","","25","29","The enterprise's knowledge management performance evaluating indicator system is an important basis for the evaluation of enterprise knowledge management performance. Only if we establish a set of scientific and feasible enterprise knowledge management performance evaluating indicator system, the enterprise's knowledge management level could be reflected, and the gap and shortage could be found and corrected, so that the enterprise could adapt to the complex and changeable circumstances in the age of knowledge-based economy. The thesis proposes the principle for building enterprise knowledge management performance evaluating indicator system. Taking into account of the practical condition of the enterprise, the thesis select the enterprise knowledge management performance evaluating indicator set by using Delphi method. Then the evaluating indicator system hierarchy chart is established by AHP method. The thesis quotes Saaty 1<sub>~</sub>9 scaling procedure and average random coincidence indicator RI value, proceeds the quantifying and weighting calculation for the evaluating indicator, and establishes enterprise knowledge management performance evaluating indicator system model by coincidence test and other integrated methods for judgment matrix. Through the simulated experiment, the scientificalness and feasibility of the enterprise knowledge management performance evaluating indicator system model is proved.","","978-0-7695-3484","10.1109/WMSO.2008.21","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4756949","","Knowledge management;Publishing;Books;Conference management;Technology management;Engineering management;Computer Society;Software engineering;Computer science;Portals","knowledge management;matrix algebra;random processes","enterprise knowledge management performance evaluating indicator system;knowledge-based economy;Delphi method;analytic hierarchy process method;Saaty scaling procedure;average random coincidence indicator;judgment matrix","","","4","","","","","","IEEE","IEEE Conferences"
"Static Slicing for Pervasive Programs","H. Lu; W. k. Chan; T. h. Tse","The University of Hong Kong, Hong Kong; City University of Hong Kong, Hong Kong; The University of Hong Kong, Hong Kong","2006 Sixth International Conference on Quality Software (QSIC'06)","","2006","","","185","192","Pervasive programs should be context-aware, which means that program functions should react according to changing environmental conditions. Slicing, as an important class of code analysis techniques, can clarify the dependence between program artifacts and observable system states to facilitate debugging, testing, and other analyses. Existing program slicing techniques, however, do not take the contextual environment into account, resulting in incomplete slices for such kind of program. To tackle this problem, this paper proposes a novel static slicing approach. It develops a graphic representation that captures the context-triggered invocations and the pervasive concurrency features. We have also developed an algorithm to check the propagation dependence in processing inter-thread data dependence. Further optimizations are discussed","1550-6002;2332-662X","0-7695-2718","10.1109/QSIC.2006.60","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4032284","Pervasive concurrent program;static slicing.","Pervasive computing;Concurrent computing;Debugging;Context awareness;Context modeling;Flow graphs;System testing;Graphics;Temperature;Computer architecture","concurrency control;flow graphs;program debugging;program slicing;program testing;ubiquitous computing","static slicing;pervasive programs;context-aware programs;code analysis;program artifact dependence;program debugging;program testing;program analysis;program slicing;graphic representation;context-triggered invocations;pervasive concurrency;propagation dependence checking;interthread data dependence","","1","24","","","","","","IEEE","IEEE Conferences"
"Performance Analysis of Different Length Packets Processing by Network Processor Based Application","S. Zheng; Z. Rong-cai; X. Kang-min; J. Lie-hui","Computer Science Department, Information Engineering University, Zhengzhou Henan China, 450002. Tel: +86-0371-68849121, E-mail: shanzheng77@hotmail.com; NA; NA; NA","2006 International Symposium on Communications and Information Technologies","","2006","","","242","245","In this work, we present a new methodology to calculate the available compute budget which determines how much processing the network processor can perform on each packet with different length. To evaluate the method, we also did a wide range of tests with different length packets. Then we analysis the MicroEngine states with the test results, which is useful in building the applications on network processors and evaluating its performance. We also give several solutions to solving the applications' performance bounds such as latency bound and compute bound, which are caused by variable length packets","","0-7803-9740-10-7803-9741","10.1109/ISCIT.2006.340039","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4141550","","Performance analysis;Testing;Computer networks;Application software;Delay;Computer science;Electronic mail;IP networks;Communication system security;Data security","optimisation;packet radio networks;variable length codes","packets processing;network processor based application;MicroEngine states;performance evaluation;variable length packets","","","16","","","","","","IEEE","IEEE Conferences"
"Parallelizing irregular C codes assisted by interprocedural shape analysis","R. Asenjo; R. Castillo; F. Corbera; A. Navarro; A. Tineo; E. L. Zapata","Dpt. of Computer Architecture, University of Málaga, Complejo Tecnologico, Campus de Teatinos, E-29071, Spain; Dpt. of Computer Architecture, University of Málaga, Complejo Tecnologico, Campus de Teatinos, E-29071, Spain; Dpt. of Computer Architecture, University of Málaga, Complejo Tecnologico, Campus de Teatinos, E-29071, Spain; Dpt. of Computer Architecture, University of Málaga, Complejo Tecnologico, Campus de Teatinos, E-29071, Spain; Dpt. of Computer Architecture, University of Málaga, Complejo Tecnologico, Campus de Teatinos, E-29071, Spain; Dpt. of Computer Architecture, University of Málaga, Complejo Tecnologico, Campus de Teatinos, E-29071, Spain","2008 IEEE International Symposium on Parallel and Distributed Processing","","2008","","","1","12","In the new multicore architecture arena, the problem of improving the performance of a code is more in the software side than in the hardware one. However, optimizing irregular dynamic data structure based codes for such architectures is not easy, either by hand or compiler assisted. Regarding this last approach, shape analysis is a static technique that achieves abstraction of dynamic memory and can help to disambiguate, quite accurately, memory references in programs that create and traverse recursive data structures. This kind of analysis has promising applicability for accurate data dependence tests in loops or recursive functions that traverse dynamic data structures. However, support for interprocedural programs in shape analysis is still a challenge, especially in the presence of recursive functions. In this work we present a novel fully context-sensitive interprocedural shape analysis algorithm that supports recursion and can be used to uncover parallelism. Our approach is based on three key ideas: i) intraprocedural support based on ""coexistent links sets"" to precisely describe the memory configurations during the abstract interpretation of the C code; ii) interprocedural support based on ""recursive flow links"" to trace the state of pointers in previous calls; and Hi) annotations of the read/written heap locations during the program analysis. We present preliminary experiments that reveal that our technique compares favorably with related work, and obtains precise memory abstractions in a variety of recursive programs that create and manipulate dynamic data structures. We have also implemented a data dependence test over our interprocedural shape analysis. With this test we have obtained promising results, automatically detecting parallelism in three C codes, which have been successfully parallelized.","1530-2075","978-1-4244-1693-6978-1-4244-1694","10.1109/IPDPS.2008.4536285","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4536285","","Shape;Data structures;Computer architecture;Testing;Multicore processing;Software performance;Hardware;Optimizing compilers;Algorithm design and analysis;Read-write memory","data structures;parallel processing;software performance evaluation;systems analysis","interprocedural shape analysis;multicore architecture;irregular dynamic data structure;static technique;recursive functions;coexistent links sets;recursive flow links;program analysis","","6","18","","","","","","IEEE","IEEE Conferences"
"The use of selected diagnostic parameters as a feedback modifying the ECG interpretation","P. Augustyniak","AGH University of Science and Technology, Krakow, Poland","2006 Computers in Cardiology","","2006","","","825","828","The paper presents the principles and prototype implementation of adaptive wireless monitoring system using medical parameters as a feedback modifying the ECG interpretation process. The paper focuses on the decision-making procedures such as: optimization of patient description, resources availability prediction and assessment of diagnosis reliability. Human expert preferences as well as the data reliability factors were revised for the purpose of this implementation. The prototype limited-scale implementation was tested with use of 2751 one hour 12-leads ECG records. In that total, 857 signals were combined of a physiological and pathological parts representing one of 14 diseases most frequently observed in out-hospital patients. In 80,2% of the cases, the remotely modified software yielded results in the priority similar to the reference observed from the experts' survey.","0276-6574;2325-8853","978-1-4244-2532","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4511979","","Feedback;Electrocardiography;Prototypes;Biomedical monitoring;Adaptive systems;Patient monitoring;Condition monitoring;Medical diagnostic imaging;Decision making;Availability","electrocardiography;medical diagnostic computing;medical signal processing;optimisation;patient diagnosis;patient monitoring;patient treatment","ECG interpretation;adaptive wireless monitoring system;patient description optimization;resources availability prediction;diagnosis reliability;ECG signals;remotely modified software","","2","5","","","","","","IEEE","IEEE Conferences"
"KBS-MAQAO: A Knowledge Based System for MAQAO Tool","L. Djoudi; V. Khachidze; W. Jalby","NA; NA; NA","2009 11th IEEE International Conference on High Performance Computing and Communications","","2009","","","571","578","The quest for performance leads to an ever increasing processor complexity. Symmetrically compilers are following the same trend with deeper optimization chain involving a numerous set of techniques. As a result code performance is becoming more and more complex to guarantee, it is sensitive to butterfly effects and difficult to assess without extensive tuning and experiments. Currently, an array of tools is used to handle the performance tuning. Consequently, tuning is a time consuming task, burdensome with a poor productivity. Therefore, a modern approach is much needed, to address the complexity of the task in order to support the multidimensional aspect of performance and federate existing methods. We propose a new approach which is Knowledge Based System for MAQAO kernel (KBS-MAQAO). It represents case specific and general optimization techniques knowledge of an expert, stores the information about the application tested, the information about compiler used (successful/failed optimizations, compiler characteristics).","","978-1-4244-4600-1978-0-7695-3738","10.1109/HPCC.2009.98","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5167046","Artificial Intelligence;performance;optimization;tools","Knowledge based systems;Optimizing compilers;Multidimensional systems;Productivity;Microprocessors;Hardware;Information analysis;High performance computing;Kernel;Testing","knowledge based systems;program assemblers;program compilers;software performance evaluation","knowledge based system;processor complexity;butterfly effects;performance tuning;compiler characteristics","","1","28","","","","","","IEEE","IEEE Conferences"
"Specification and Design Considerations for Reliable Embedded Systems","A. Israr; S. A. Huss","Dept. of Computer Science, TU Darmstadt, Darmstadt, Germany. israr@iss.tu-darmstadt.de; Dept. of Computer Science, TU Darmstadt, Darmstadt, Germany. huss@iss.tu-darmstadt.de","2008 Design, Automation and Test in Europe","","2008","","","1111","1116","The objective of this paper is to introduce a novel representation as a means to consider both permanent and temporal errors in order to increase the overall reliability of an embedded system. The deployment of embedded systems in safety critical applications, e.g. in the automotive domain, demands that the fundamental set of design criteria consisting of functionality, timeliness, and production costs be extended to consider of reliability as an optimization criterion. Thus reliability engineering becomes part of the overall design flow for embedded systems. The proposed approach is based on the introduction of permanent/transient error decision diagrams and on dedicated algorithms for the generation of system implementation sets which feature maximum reliability at minimal costs in terms of redundant resources. The proposed approach is demonstrated for a control system taken from the automotive domain.","1530-1591;1558-1101","978-3-9810801-3-1978-3-9810801-4","10.1109/DATE.2008.4484824","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4484824","","Embedded system;Computer errors;Application software;Automotive engineering;Reliability engineering;Costs;Single event upset;Data structures;Computer science;Product safety","decision diagrams;embedded systems;optimisation;reliability","embedded systems;permanent errors;temporal errors;safety critical applications;automotive domain;optimization criterion;reliability engineering;permanent-transient error;decision diagrams;system implementation","","2","15","","","","","","IEEE","IEEE Conferences"
"Improving access to relevant data on faults, errors and failures in real systems","D. Penkler","Hewlett-Packard, France","2006 Sixth European Dependable Computing Conference","","2006","","","71","72","In order to be able to test the effectiveness and verify proposed techniques for enhanced availability based on field data from systems it is important to have reliability data of the components and the information necessary to characterize or model the system. This includes inter alia the type and number of components, their protection and dependency relations as well the automatic recovery mechanisms built into the system. An important benefit of making system models and logs available to the research community in a standard format is that it opens up the possibility for creating tools to assess and optimize deployed as well as hypothetical system configurations. Specialized tools for on-line and off-line analysis and classification of reliability data also become viable. Availability modeling tools could be benchmarked against actual data. Depending on the usefulness of such tools and the level of adoption of standard models and formats in the industry a market for reliability data analysis tools could emerge over time. These tools could be used during the design, deployment and operation phases of a system in order to predict or enhance the availability of the services it provides","","0-7695-2648","10.1109/EDCC.2006.14","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4020835","","Availability;Redundancy;ISO standards;Protection;Data security;Standardization;Code standards;System testing;Web server;Operating systems","data analysis;error handling;object-oriented programming;software reliability;system monitoring;system recovery","fault data access;error data access;failure data access;component reliability;system models;system logs;software reliability","","","","","","","","","IEEE","IEEE Conferences"
"Reuse of a HW/SW coverification environment during the refinement process of a functional C model down to an executable HW/SW specification","M. Winterholer; F. Schäfer","CTS, R&D, Cadence Design Systems, Feldkirchen, Germany; Munich, Germany","2009 Forum on Specification & Design Languages (FDL)","","2009","","","1","4","This paper presents a method reusing a coverification system testbench throughout the refinement process starting from a functional C model of a distributed system with the goal to optimize the HW/SW partitioning and distribution to multiple cores of a system. The partitioned system can then be used as an executable HW/SW specification in the ensuing design flow. The presented paradigm was validated using a distributed brake-by-wire design for the automotive industry.","1636-9874","978-2-9530504-1","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5404046","co-verification;partitioning;hw/sw co-design","Hardware;Coordinate measuring machines;Computer architecture;Communication system control;Testing;Performance evaluation;Software performance;Read only memory;System-level design;System performance","automobile industry;brakes;C language;formal specification;formal verification;hardware-software codesign","HW/SW coverification environment;refinement process;functional C model;executable HW/SW specification;coverification system testbench;distributed system;HW/SW partitioning;partitioned system;ensuing design flow;distributed brake-by-wire design;automotive industry","","","8","","","","","","IEEE","IEEE Conferences"
"Design and Implementation of Blackfin DSP-based Video Encoder in Network Surveillance System","Z. Li; D. Li; Q. Zhang; Q. Wu","NA; NA; NA; NA","2006 8th international Conference on Signal Processing","","2006","2","","","","Video encoder is an important part of video network surveillance system, and its design goals include combining real time coding, high performance, low cost and flexible control with small size. To meet all these requirements, DSPs with video ports provide an ideal solution. In this paper, we've designed and implemented a Blackfin DSP-based, high performance, low cost, low power consumption, real time network video encoder. The hardware system, software flow, algorithm optimization, Ethernet transmission and performance test are discussed in detail. This network video encoder has been successfully used in our MPEG-4, H.263 network video surveillance system","2164-5221;2164-523X","0-7803-9736-30-7803-9737","10.1109/ICOSP.2006.345640","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4129121","","Surveillance;Costs;Real time systems;Size control;Control systems;Digital signal processing;Energy consumption;Hardware;Software performance;Software systems","local area networks;video coding;video communication;video surveillance","Blackfin DSP-based video encoder;network surveillance system;software flow;hardware system;algorithm optimization;Ethernet transmission;MPEG-4","","","11","","","","","","IEEE","IEEE Conferences"
"Blurring the Layers of Abstractions: Time to Take a Step Back?","K. Flautner","ARM Ltd","13th IEEE International On-Line Testing Symposium (IOLTS 2007)","","2007","","","127","127","Silicon technology evolution over the last four decades has yielded an exponential increase in integration densities with steady improvements of performance and power consumption at each technology generation. This steady progress has created a sense of entitlement for the riches that future process generations would bring. Today, however, classical process scaling seems to be dead and living up to technology expectations requires continuous innovation at many levels, which comes at steadily progressing implementation and design costs. Solutions to problems need to cut across layers of abstractions and require coordination between software, architecture and circuit features. This talk will describe some of the recent work at ARM on designing architectures which dynamically optimize operating parameters for temporal conditions.","1942-9398;1942-9401","0-7695-2918","10.1109/IOLTS.2007.22","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4274833","","Research and development;Computer architecture;Silicon;Energy consumption;Power generation;Technological innovation;Costs;Circuits;Design optimization;Biographies","integrated circuit design;integrated circuit manufacture","abstraction layers blurring;silicon technology evolution;power consumption;ARM","","","","","","","","","IEEE","IEEE Conferences"
"Exploiting TLM and Object Introspection for System-Level Simulation","G. Beltrame; D. Sciuto; C. Silvano; D. Lyonnard; C. Pilkington","Politecnico di Milano Milano, Italy, beltrame@elet.polimi.it; NA; NA; NA; NA","Proceedings of the Design Automation & Test in Europe Conference","","2006","1","","1","6","The introduction of transaction level modeling (TLM) allows a system designer to model a complete application, composed of hardware and software parts, at several levels of abstraction. The simulation speed of TLM is orders of magnitude faster than traditional RTL simulation; nevertheless, it can become a limiting factor when considering a multi-processor system-on-chip (MP-SoC), as the analysis of these systems can be very complex. The main goal of this paper is to introduce a novel way of exploiting TLM features to increase simulation efficiency of complex systems by switching TLM models at runtime. Results show that simulation performance can be increased significantly without sacrificing the accuracy of critical application kernels","1530-1591;1558-1101","3-9810801-1","10.1109/DATE.2006.244004","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1656858","","Application software;Kernel;Analytical models;Runtime;Communication channels;Hardware;System-on-a-chip;Communication switching;Design optimization;Software engineering","circuit simulation;integrated circuit modelling;multiprocessing systems;system-on-chip","object introspection;system-level simulation;transaction level modeling;multi-processor system-on-chip","","12","11","","","","","","IEEE","IEEE Conferences"
"FPGA implementation for an optimized CORDIC module for OFDM system","O. A. Alim; N. Elboghdadly; M. A. Ashour; A. M. Elaskary","Faculty of Engineering, Department of Electrical Engineering, Beirut Arab University, Lebanon; Faculty of Engineering, Department of Electrical Engineering, Alexandria University, Egypt; National Center for Radiation Research and Technology (NCRRT), Atomic Energy Authority, Po. Box 29, Nasr City, Cairo, Egypt; National Center for Radiation Research and Technology (NCRRT), Atomic Energy Authority, Po. Box 29, Nasr City, Cairo, Egypt","2008 International Conference on Computer Engineering & Systems","","2008","","","21","26","OFDM Wireless systems are currently in the focus of research and development. High hardware cost of such systems is an initiative to redesign the critical functional blocks in order to satisfy timing and power constrains as well as to minimize overall circuit complexity and cost. One such critical functional block is the CoOrdinate Rotation DIgital Computer (CORDIC) that can be used in various applications in OFDM systems. In this paper we apply optimization algorithms to the CORDIC processor in terms of power, area and speed for efficient FPGA implementation. Three CORDIC modules have been selected and modified to satisfy optimization requirements. The circuits have been described in Very high speed integrated circuit Hardware Description Language (VHDL), synthesized, and implemented on Xilinx Spartan-3A FPGAs kit board using ISEtrade design tool suite. The prototyped results have been tested using suitable testbench waveforms. Mapping and timing reports have been summarized. Using Matlab code relation between the delay for the published and prototyped modules has been plotted.","","978-1-4244-2115-2978-1-4244-2116","10.1109/ICCES.2008.4772959","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4772959","","Field programmable gate arrays;OFDM;Cost function;Timing;Prototypes;Circuit testing;Research and development;Hardware;Complexity theory;Application software","digital arithmetic;field programmable gate arrays;OFDM modulation;radiocommunication;signal processing","FPGA implementation;optimized CORDIC module;OFDM system;wireless systems;critical functional blocks;CoOrdinate Rotation DIgital Computer;Very high speed integrated circuit Hardware Description Language;Xilinx Spartan-3A FPGAs kit board;ISEdesign tool suite;Matlab code relation","","2","14","","","","","","IEEE","IEEE Conferences"
"A Knowledge-Based Evolution Strategy for the Multi-Objective Minimum Spanning Tree Problem","M. D. Moradkhan; W. N. Browne","Department of Cybernetics of University of Reading,, RG6 6AY, U.K. (Tel: +44 (0) 118 948 1631, e-mail: mdmtig@yahoo.co.uk); NA","2006 IEEE International Conference on Evolutionary Computation","","2006","","","1391","1398","A fast knowledge-based evolution strategy, KES, for the multi-objective minimum spanning tree, is presented. The proposed algorithm is validated, for the bi-objective case, with an exhaustive search for small problems (4-10 nodes), and compared with a deterministic algorithm, EPDA and NSGA-II for larger problems (up to 100 nodes) using benchmark hard instances. Experimental results show that KES finds the true Pareto fronts for small instances of the problem and calculates good approximation Pareto sets for larger instances tested. It is shown that the fronts calculated by KES are superior to NSGA-II fronts and almost as good as those established by EPDA. KES is designed to be scalable to multi-objective problems and fast due to its small complexity.","1089-778X;1941-0026","0-7803-9487","10.1109/CEC.2006.1688471","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1688471","","Costs;Tree graphs;Evolutionary computation;Cybernetics;Polynomials;Benchmark testing;Design optimization;Application software;Computer networks;Pipelines","computational complexity;deterministic algorithms;evolutionary computation;minimisation;Pareto optimisation;set theory;tree searching;trees (mathematics)","knowledge-based evolution strategy design;multiobjective minimum spanning tree problem;search problem;deterministic algorithm;Pareto set theory;combinatorial optimization problem;graph theory;polynomial time algorithm","","1","33","","","","","","IEEE","IEEE Conferences"
"Change Priority Determination in IT Service Management Based on Risk Exposure","J. Sauve; R. Santos; R. Reboucas; A. Moura; C. Bartolini","Member; NA; NA; NA; Member","IEEE Transactions on Network and Service Management","","2008","5","3","178","187","In the Change Management process within IT Service Management, some activities need to evaluate the risk exposure associated with changes to be made to the infrastructure and services. The paper presents a method to evaluate risk exposure associated with a change. Further, we show how to use the risk exposure metric to automatically assign priorities to changes. The formal model developed for this purpose captures the business perspective by using financial metrics in the evaluation of risk. Thus the method is an example of Business-Driven IT Management. A case study, performed in conjunction with a large IT service provider, is reported and provides good results when compared to decisions made by human managers.","1932-4537;2373-7379","","10.1109/TNSM.2009.031105","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4805134","Change management, risk, change prioritization, IT service management, business-driven IT management","Risk management;Quality management;Scheduling;Information technology;Testing;Computational Intelligence Society;Software systems;Embedded software;Humans;Process control","business data processing;management of change;risk analysis","change priority determination;IT service management;risk exposure metric;financial metrics;business-driven IT management","","16","20","","","","","","IEEE","IEEE Journals & Magazines"
"Application of Dimensionality Reduction Analysis to Fingerprint Recognition","J. Luo; S. Lin; M. Lei; J. Ni","NA; NA; NA; NA","2008 International Symposium on Computational Intelligence and Design","","2008","2","","102","105","Dimensionality reduction is an important issue in Fingerprint recognition that often faces high-dimensional data. Two-dimensional principal component analysis (2DPCA) is one of the most popular methods for dimensionality reduction. A novel fingerprint recognition algorithm using 2DPCA has been proposed in this paper. Firstly, the prime features of original images can be attained by two-lever WT decomposition. Secondly, the features of dimensional reduction are solved by 2DPCA. Finally, fingerprint recognition can be realized by Ellipsoidal Basis Function Neural Network (EBFNN). The algorithm combines the optimization of the 2DPCA and the adaptability of EBFNN. The resulting algorithm is tested on three different fingerprint verification challenge datasets and demonstrates much higher performance in comparison to WT-2DPCA-RBF.","","978-0-7695-3311","10.1109/ISCID.2008.148","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4725467","Dimensionality reduction;Two-Dimensional Principal Component Analysis (2DPCA)","Fingerprint recognition;Neural networks;Feature extraction;Covariance matrix;Principal component analysis;Computational intelligence;Application software;Educational institutions;Design automation;Laboratories","data analysis;data reduction;feature extraction;fingerprint identification;neural nets;optimisation;principal component analysis;wavelet transforms","dimensionality reduction analysis;fingerprint recognition;high-dimensional data;two-dimensional principal component analysis optimization;ellipsoidal basis function neural network;wavelet transform","","1","8","","","","","","IEEE","IEEE Conferences"
"GIS and GPS Based Vehicle Guidance System","F. Pan; L. Zhang; F. Wang","NA; NA; NA","2008 International Conference on Intelligent Computation Technology and Automation (ICICTA)","","2008","2","","251","254","In order to enhance level of vehicle guidance system (VGS) in China, the framework, subsystem and critical technologies of VGS were studied, and geographic information system (GIS), Global Positioning Systems (GPS), etc. were applied into corresponding subsystems. Paying attention to the subsystem of route optimization, the mathematical model of road network was built. The storage structure of road network was ameliorated by adjacent node relation matrix and adjacent node weight matrix, which saves the storage space. The classical Dijkstra algorithm was improved with dynamic adjacent node relation matrix and adjacent node weight matrix, which increases the calculation efficiency, and the corresponding algorithm is given. Applying of GIS, GPS and so on, the studied VGS was realized. Test results show the studied VGS can exactly position vehicle, correctly find optimal route and guide travel in real road network.","","978-0-7695-3357","10.1109/ICICTA.2008.317","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4659761","ITS;Vehicle Guidance System;GIS;GPS;Optimal route algorithm","Geographic Information Systems;Global Positioning System;Navigation;Road vehicles;Telecommunication traffic;Vehicle driving;Wireless communication;Visual databases;Computer displays;Application software","automated highways;directed graphs;geographic information systems;Global Positioning System;matrix algebra;optimisation;road vehicles","GIS;GPS;vehicle guidance system;geographic information system;Global Positioning System;route optimization;mathematical model;road network;adjacent node relation matrix;adjacent node weight matrix;Dijkstra algorithm;directed graph","","3","6","","","","","","IEEE","IEEE Conferences"
"Utility-Function-Based Self-Adaptation in Elevator Group Control System","H. Wu; Q. Tan","NA; NA","2008 International Conference on Computer Science and Software Engineering","","2008","1","","1158","1161","Utility functions provide a natural and advantageous way for achieving self-adaptation in distributed systems. We implemented in a realistic prototype elevator group control system(EGCS), that demonstrates how utility functions can continually optimize the use of elevators in a dynamic environment. A global manager allocates elevators among the whole system based on throughput obtained from the monitors of the floors. We present empirical data that demonstrate the effectiveness of our utility function scheme in handling realistic, fluctuating workloads running on a test PC .","","978-0-7695-3336","10.1109/CSSE.2008.1299","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721958","Utility Function;Self-adaptation;Elevator Group Control System;EGCS","Elevators;Control systems;Prototypes;Resource management;Humans;Road transportation;Computer science;Software engineering;Distributed computing;Software prototyping","distributed control;lifts","utility-function-based self-adaptation;elevator group control system;distributed systems;global manager","","","12","","","","","","IEEE","IEEE Conferences"
"A budget constrained scheduling of workflow applications on utility Grids using genetic algorithms","J. Yu; R. Buyya","Grid Computing and Distributed Systems (GRIDS) Laboratory, Dept. of Computer Science and Software Engineering, The University of Melbourne, VIC 3010 Australia; Grid Computing and Distributed Systems (GRIDS) Laboratory, Dept. of Computer Science and Software Engineering, The University of Melbourne, VIC 3010 Australia","2006 Workshop on Workflows in Support of Large-Scale Science","","2006","","","1","10","Over the last few years, grid technologies have progressed towards a service-oriented paradigm that enables a new way of service provisioning based on utility computing models. Users consume these services based on their QoS (quality of service) requirements. In such ldquopay-per-userdquo grids, workflow execution cost must be considered during scheduling based on users' QoS constraints. In this paper, we propose a budget constraint based scheduling, which minimizes execution time while meeting a specified budget for delivering results. A new type of genetic algorithm is developed to solve the scheduling optimization problem and we test the scheduling algorithm in a simulated grid testbed.","2151-1373;2151-1381","978-1-4244-5215","10.1109/WORKS.2006.5282330","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5282330","","Genetic algorithms;Silicon;Photovoltaic effects;Optical interconnections;Microactuators;Electrostatics;Electrodes;Mirrors;Photovoltaic systems;Solar power generation","budgeting;genetic algorithms;grid computing;scheduling;workflow management software","budget constrained scheduling;workflow applications;utility grids;genetic algorithms;grid technologies;service-oriented paradigm;utility computing models;quality of service requirements;pay-per-use grids;workflow execution cost;scheduling optimization problem","","76","38","","","","","","IEEE","IEEE Conferences"
"Communication-aware allocation and scheduling framework for stream-oriented multi-processor systems-on-chip","M. Ruggiero; A. Guerri; D. Bertozzi; F. Poletti; M. Milano","DEIS, Bologna Univ., Italy; DEIS, Bologna Univ., Italy; NA; NA; NA","Proceedings of the Design Automation & Test in Europe Conference","","2006","1","","6 pp.","","This paper proposes a complete allocation and scheduling framework, where an MPSoC virtual platform is used to accurately derive input parameters, validate abstract models of system components and assess constraint satisfaction and objective function optimization. The optimizer implements an efficient and exact approach to allocation and scheduling based on problem decomposition. The allocation subproblem is solved through integer programming while the scheduling one through constraint programming. The two solvers can interact by means of no-good generation, thus building an iterative procedure which has been proven to converge to the optimal solution. Experimental results show significant speedups w.r.t. pure IP and CP exact solution strategies as well as high accuracy with respect to cycle accurate functional simulation. A case study further demonstrates the practical viability of our framework for real-life systems and applications","1530-1591;1558-1101","3-9810801-1","10.1109/DATE.2006.243950","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1656836","","Constraint optimization;Processor scheduling;Linear programming;Computational efficiency;Predictive models;Application software;System-on-a-chip;Computer architecture;Multiprocessing systems;Power system modeling","constraint handling;embedded systems;integer programming;microprocessor chips;multiprocessing systems;pipeline processing;processor scheduling;resource allocation;system-on-chip","communication-aware allocation;scheduling framework;stream-oriented multiprocessor systems-on-chip;system components;constraint satisfaction;objective function optimization;problem decomposition;integer programming;constraint programming","","28","25","","","","","","IEEE","IEEE Conferences"
"Advances in neutron science instrumentation at the Los Alamos Neutron Science Center (LANSCE)","J. A. O'Toole","Los Alamos National Laboratory, NM 87545 USA","2007 IEEE Nuclear Science Symposium Conference Record","","2007","1","","627","632","The Lujan Neutron Scattering Center within the Los Alamos Neutron Science Center (LANSCE) develops instrumentation in support of research employing neutrons to investigate issues of interest to its user community. The Spectrometer for Materials Research at Temperature and Stress (SMARTS) neutron diffractometer is optimized for the study of engineering materials. SMARTS automated sample positioning and data collection system concept: The system makes use of a pair of digital theodilites, a 4 axis sample positioning stage, the SolidWorks* CAD software with a custom add-in program, a calibration process, and a sample setup process. The system provides automatic positioning of the test point on the sample such that the neutron beam will interest that point for data collection. When one considers that for SMARTS the samples are contained within a tank that provides a desired test atmosphere, test temperature, and the sample is loaded to the desired strain or stress by the integral tensile machine, then one can see the advantages of this automated sample positioning and data collection system. Doubling the performance of the Lujan Center cold neutron source: The Lujan center obtains its source of neutrons through the use of a Target Moderator Reflector System (TMRS). The TMRS includes a pair of targets, six moderators, beryllium reflectors, a window insert, and an instrument insert. The cold neutron moderator has a water premoderator sized to reduce the heat load on the LH cryogenic system to the same level as that of the MK II design so that a costly increase in cooling capability is avoided. Next a liquid hydrogen filled moderator tank is employed to create the cold neutrons. This is followed by a beryllium reflector/filter that is contact cooled to near liquid hydrogen temperature. Using this arrangement, and maintaining the same particle beam parameters as for the MK II design, the cold neutron performance is increased by a factor of two.","1082-3654","978-1-4244-0922-8978-1-4244-0923","10.1109/NSSMIC.2007.4436411","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4436411","","Instruments;Temperature;Automatic testing;System testing;Particle beams;Tensile stress;Hydrogen;Neutron spin echo;Spectroscopy;Diffraction","CAD;data analysis;diffractometers;high energy physics instrumentation computing;moderators;neutron beams;neutron diffraction;neutron sources","neutron science instrumentation;Los Alamos neutron science center;LANSCE;Lujan Neutron Scattering Center;SMARTS;Spectrometer for Materials Research at Temperature and Stress;neutron diffractometer;engineering materials;data collection system;digital theodilites;automated 4 axis sample positioning;SolidWorks* CAD software;custom add-in program;calibration process;sample setup process;neutron beam;test atmosphere;test temperature;integral tensile machine;Lujan Center cold neutron source;target moderator reflector system;beryllium reflectors;window insert;instrument insert;cold neutron moderator;water premoderator;LH cryogenic system;MK II design;liquid hydrogen filled moderator tank;beryllium filter;near liquid hydrogen temperature;particle beam parameters","","","","","","","","","IEEE","IEEE Conferences"
"An Efficient Pareto-Coevolution Archive","L. Yang; H. Huang; X. Yang","Beijing Jiaotong University, China; Shandong University of Finance, China; Beijing Jiaotong University, China; Shandong University of Finance, China","Third International Conference on Natural Computation (ICNC 2007)","","2007","4","","484","488","Archive-based methods have been proposed for guaranteeing progress in coevolution. An important question in the practical applications is how to minimize the size of archives while remaining reliability. In the paper we present an efficient Pareto- Coevolution archive based on the problem structure which is identified in coevolution process. In the design, the size of test-archive is minimized while standing for currently global progress. Compared with IPCA on a Numbers game problem, experimental results show that our algorithm is more efficient.","2157-9555;2157-9563","0-7695-2875-9978-0-7695-2875","10.1109/ICNC.2007.193","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4344722","","Testing;Function approximation;Information technology;Reliability engineering;Finance;Application software;Process design;Performance evaluation","game theory;Pareto optimisation","Pareto-coevolution archive;problem structure;numbers game problem","","","11","","","","","","IEEE","IEEE Conferences"
"Custom formal verification technique for partial product reduction tree","M. Castellano; P. Baldrighi; C. Vacchi; M. Natuzzi; A. Furlan","Department of Electronics, University of Pavia, Italy; Department of Electronics, University of Pavia, Italy; Department of Electronics, University of Pavia, Italy; CPG Group, STMicroelectronics, Cornaredo, Italy; CPG Group, STMicroelectronics, Cornaredo, Italy","2008 International Conference on Microelectronics","","2008","","","377","380","Wide partial product reduction tree (PPRT) circuits are ideal for high speed low area inner products, and can be designed using merged arithmetic and a speed-optimized reduction algorithm. While the performance of small architectures can be checked successfully using a simulation, the problem with using a simulation to check a merged architecture model is that a merged structure results in a wide PPRT, which produces an extremely high number of partial products. Because the time required to test all of the partial product values is prohibitive, only a small percentage of the possible values can be tested, providing no real picture of the circuit's functionality. In this paper we present a method (implemented in the C programming language) of deriving the functionality of a PPRT circuit from its topology, and we present a custom PPRT fast simulation setup that can detect the most of the crisscross net errors.","2159-1660;2159-1679","978-1-4244-2369-9978-1-4244-2370","10.1109/ICM.2008.5393541","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5393541","Software verification and validation;Digital arithmetic;FIR digital filters;Theorem proving;Error analysis;Inners;Merging","Formal verification;Circuit simulation;Circuit testing;Finite impulse response filter;Arithmetic;Signal processing algorithms;Circuit topology;Merging;Microelectronics;Algorithm design and analysis","C language;circuit simulation;digital arithmetic;formal verification","formal verification;C programming language;custom PPRT fast simulation setup;arithmetic algorithm;speed-optimized reduction algorithm;partial product reduction tree circuit","","","10","","","","","","IEEE","IEEE Conferences"
"Regression verification","B. Godlin; O. Strichman","CS, Technion, Haifa, Israel; IE, Technion, Haifa, Israel","2009 46th ACM/IEEE Design Automation Conference","","2009","","","466","471","Proving the equivalence of successive, closely related versions of a program has the potential of being easier in practice than functional verification, although both problems are undecidable. There are two main reasons for this claim: it circumvents the problem of specifying what the program should do, and in many cases it is computationally easier. We study theoretical and practical aspects of this problem, which we call regression verification.","0738-100X","978-1-6055-8497","10.1145/1629911.1630034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5227039","Software verification;Equivalence checking","Automatic testing;Software testing;Computer science;Formal verification;Hardware;Computational modeling;Optimization;Delay;Programming","formal specification;formal verification","regression verification;program proving;functional verification;program specification","","4","14","","","","","","IEEE","IEEE Conferences"
"Real-time rendering of translucent objects with variable sizes","C. Yao; B. Wang","School of Software, Tsinghua University, Beijing, China 100084; School of Software, Tsinghua University, Beijing, China 100084","2009 11th IEEE International Conference on Computer-Aided Design and Computer Graphics","","2009","","","111","116","We present a novel technique for real-timely rendering translucent objects. Our technique is mainly based on translucent shadow maps with a new adaptive sampling strategy. In this sampling strategy, the hierarchy levels and positions for sampling are selected according to the size of target object. By our optimization, less storage in the texture is required than that in previous methods. Compared with previous methods, our technique provide a way to choose the proper sampling pattern and is applicable for a wider range of object sizes. With the implementation on GPU, the presented technique is able to render translucent objects in animated scenes where lights and material parameters vary real-timely without any lengthy preprocessing.","","978-1-4244-3699-6978-1-4244-3701","10.1109/CADCG.2009.5246923","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5246923","","Rendering (computer graphics);Mobile computing;Testing;Graphics;Cranes;Control systems;Computer architecture;Helium;Virtual reality;Computer science","computer vision;optimisation;rendering (computer graphics)","real-time rendering;translucent objects;translucent shadow maps;adaptive sampling strategy;animated scenes","","","15","","","","","","IEEE","IEEE Conferences"
"Surface Acid-Base Property of Nano-Zns Particles and Adsorption Behavior of Pb²+ and Cu²+","H. Yu; L. Yan; W. Zhang; J. Wang; C. Zhu","NA; NA; NA; NA; NA","2009 3rd International Conference on Bioinformatics and Biomedical Engineering","","2009","","","1","4","Zinc sulfide nanoparticles were prepared by hydrothermal method and characterized by TEM, FT-IR and BET surface area. The titration results show that the surface of ZnS nanoparticles possessed an obvious acid-base property. With increasing the amount of ZnS nanoparticles in the suspension, the pH buffer capacity of solution increased dramatically. There is a proportional relationship between the amount of ZnS nanoparticles added and the solution pH buffer capacity. The adsorption conditions of Pb<sup>2+</sup>and Cu<sup>2+</sup>by ZnS nanoparticles was also optimized. They were: titration time of 1.0 h, the amount of ZnS nanoparticles added of 0.1846 g and solution pH of 6.7 for Pb<sup>2+</sup>adsorption. For Cu<sup>2+</sup>, the conditions were 1.0 h, 0.1800 g and pH 6.5-7.0, respectively. The optimized pH values of Pb<sup>2+</sup>and Cu<sup>2+</sup>adsorption by ZnS nanoparticles were tested by the calculation of MEDUSA software.","2151-7614;2151-7622","978-1-4244-2902-8978-1-4244-2901","10.1109/ICBBE.2009.5163542","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5163542","","Nanoparticles;Zinc compounds;Lead;Solids;Temperature;Chemical analysis;Chemistry;Chemical engineering;Software testing;Copper","adsorption;copper;Fourier transform spectra;infrared spectra;lead;nanoparticles;pH;positive ions;suspensions;transmission electron microscopy;zinc compounds","surface acid-base property;Zinc sulfide nanoparticles;adsorption;hydrothermal method;TEM;FTIR spectra;BET surface area;titration;suspension;pH buffer capacity;MEDUSA software;ZnS;Cu2+;Pb2+","","","13","","","","","","IEEE","IEEE Conferences"
"Markov decision process toolbox","P. Shan; R. Li; S. Ning; Q. Yang","Dept. Electronic Information and Control Engineering, Guangxi University of Technology, Liu Zhou, China; Dept. Electronic Information and Control Engineering, Guangxi University of Technology, Liu Zhou, China; Dept. Electronic Information and Control Engineering, Guangxi University of Technology, Liu Zhou, China; Dept. Electronic Information and Control Engineering, Guangxi University of Technology, Liu Zhou, China","2009 IEEE International Workshop on Open-source Software for Scientific Computation (OSSC)","","2009","","","123","128","Markov decision process is optimal policy-making process which is based on the Markov process theory of random dynamical systems. It is also a theoretical tool to study optimization problems about multi-stage policy-making process in random environment. For its wide range of applications, developing the Markov decision process toolbox is of great significance for the scientific computing software SCILAB. Markov policy process consists of three main criterions: the expected total reward criterion, discount criterion and average criterion. Finally, taking the toys manufacturers as the example the effectiveness of the method is tested.","","978-1-4244-4452-6978-1-4244-4453","10.1109/OSSC.2009.5416859","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5416859","Markov decision process;SCILAB;expectation total reward criterion;discount criterion;average criterion","Control systems;Mathematical model;Markov processes;Application software;Scientific computing;Software tools;Toy manufacturing industry;Macroeconomics;Stochastic systems;Dynamic programming","decision theory;Markov processes;mathematics computing;natural sciences computing","Markov decision process toolbox;optimal policy-making process;random dynamical system;multistage policy-making process;scientific computing software;SCILAB;expected total reward criterion;discount criterion;average criterion;toys manufacturers","","1","12","","","","","","IEEE","IEEE Conferences"
"Ensuring Numerical Quality in Grid Computing","A. Frommer; M. Husken","NA; NA","12th GAMM - IMACS International Symposium on Scientific Computing, Computer Arithmetic and Validated Numerics (SCAN 2006)","","2006","","","20","20","Certain numerically intensive applications executed within a grid computing environment crucially depend on the properties of floating-point arithmetic implemented on the respective platform. Differences in these properties may have drastic effects. This paper identifies the central problems related to this situation. We propose an approach which gives the user valuable information on the various platforms available in a grid computing environment in order to assess the numerical quality of an algorithm run on each of these platforms. In this manner, the user will at least have very strong hints whether a program will perform reliably in a grid before actually executing it. Our approach extends the existing IeeeCC754 test suite by two ""grid-enabled"" modes: The first mode calculates a ""numerical checksum"" on a specific grid host and executes the job only if the checksum is identical to a locally generated one. The second mode provides the user with information on the reliability and IEEE 754-conformity of the underlying floating-point implementation of various platforms. Furthermore, it can help to find a set of compiler options to optimize the application's performance while retaining numerical stability.","","0-7695-2821-X978-0-7695-2821","10.1109/SCAN.2006.16","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4402410","","Grid computing;Floating-point arithmetic;Application software;Mesh generation;Software libraries;Testing;Optimizing compilers;Hardware;Standards development;Computer science","floating point arithmetic;grid computing;mathematics computing","grid computing;numerical quality;floating-point arithmetic;IeeeCC754 test suite;numerical checksum","","","16","","","","","","IEEE","IEEE Conferences"
"Flexibility-oriented Design Methodology for Reconfigurable ΔΣ Modulators","P. Sun; Y. Wei; A. Doboli","Department of Electrical and Computer Engineering, State University of New York at Stony Brook, Stony Brook, NY, 11794-2350, psun@ece.sunysb.edu; Department of Electrical and Computer Engineering, State University of New York at Stony Brook, Stony Brook, NY, 11794-2350, ywei@ece.sunysb.edu; Department of Electrical and Computer Engineering, State University of New York at Stony Brook, Stony Brook, NY, 11794-2350, adoboli@ece.sunysb.edu","2007 Design, Automation & Test in Europe Conference & Exhibition","","2007","","","1","6","This paper presents a systematic methodology for producing reconfigurable ΔΣ modulator topologies with optimized flexibility in meeting variable performance specifications. To increase their flexibility, topologies are optimized for performance attributes pertaining to ranges of values rather than being single values. Topologies are implemented on switched-capacitor reconfigurable mixed-signal architectures. As the number of configurable blocks is very small, it is extremely important that the topologies use as few blocks as possible. A case study illustrates the methodology for specifications from telecommunications area","1530-1591;1558-1101","978-3-9810801-2","10.1109/DATE.2007.364627","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4211832","","Design methodology;Topology;Hardware;Electronic design automation and methodology;Software performance;Application software;Embedded system;Bandwidth;Transfer functions;Nonlinear equations","delta-sigma modulation;electronic design automation;logic design;mixed analogue-digital integrated circuits;reconfigurable architectures;switched capacitor networks","flexibility-oriented design methodology;reconfigurable Σ-Δ modulators;optimized flexibility;performance attributes;switched capacitor architecture;reconfigurable mixed-signal architectures","","1","16","","","","","","IEEE","IEEE Conferences"
"A Digital Envelope Modulator for a WLAN OFDM Polar Transmitter in 90 nm CMOS","P. T. M. van Zeijl; M. Collados","NA; NA","IEEE Journal of Solid-State Circuits","","2007","42","10","2204","2211","A digital envelope modulator as part of a polar transmitter architecture for the 802.11a/g WLAN OFDM standards is investigated. The digital envelope modulator is quite similar to a state-of-the-art DAC design, but now it has been optimized to deal with envelope signals. A thermometer-coded envelope DAC has been implemented in a 90 nm digital CMOS process. Measurements of a test chip show the digital envelope modulator to reach an OFDM output power of 5 dBm for 54 Mb/s using 64 QAM at 2.45 GHz and fulfilling EVM specifications and in-band spectral mask requirements using just 12.7 mW from a 1.2 V supply. Combining the digital envelope modulator with an off-chip power amplifier gives an output power of 20.4 dBm, while fulfilling EVM specifications and in-band spectral mask requirements. The output power of the presented envelope DAC can be increased in a re-design by scaling device sizes. The envelope DAC is a key component in a software-defined-radio transmitter.","0018-9200;1558-173X","","10.1109/JSSC.2007.905239","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4317717","CMOS;digital transmitters;IEEE 802.11a;IEEE 802.11g;multi-standard;OFDM;WLAN;polar transmitter;RF transmitter;software-defined radio;wireless","Digital modulation;OFDM modulation;Wireless LAN;Transmitters;Power amplifiers;Power generation;Signal design;Design optimization;CMOS process;Power measurement","CMOS digital integrated circuits;digital-analogue conversion;OFDM modulation;radio transmitters;software radio;UHF power amplifiers;wireless LAN","digital envelope modulator;WLAN OFDM polar transmitter;IEEE 802.11a/g;DAC design;thermometer coded envelope;digital CMOS process;in-band spectral mask requirements;off-chip power amplifier;software defined radio transmitter;frequency 2.45 GHz;size 90 nm;power 12.7 mW;voltage 1.2 V","","45","17","","","","","","IEEE","IEEE Journals & Magazines"
"An Efficient HW/SW Implementation of the H.263 Video Coder in FPGA","A. B. Atitallah; P. Kadionik; F. Ghozzi; P. Nouel; N. Masmoudi; H. Levi","Laboratory of Electronics and Information Technology, National Engineers School of Sfax (E.N.I.S.), BP W, 3038 Sfax - TUNISIA; IXL laboratory -ENSEIRB - University Bordeauxl - CNRS UMR 5818, 351 Cours de la Libération, 33 405 Talence Cedex, FRANCE. benatita@enseirb.fr; IXL laboratory -ENSEIRB - University Bordeauxl - CNRS UMR 5818, 351 Cours de la Libération, 33 405 Talence Cedex, FRANCE. kadionik@enseirb.fr; Laboratory of Electronics and Information Technology, National Engineers School of Sfax (E.N.I.S.), BP W, 3038 Sfax - TUNISIA; IXL laboratory -ENSEIRB - University Bordeauxl - CNRS UMR 5818, 351 Cours de la Libération, 33 405 Talence Cedex, FRANCE. nouel@enseirb.fr; Laboratory of Electronics and Information Technology, National Engineers School of Sfax (E.N.I.S.), BP W, 3038 Sfax - TUNISIA. Nouri.Masmoudi@enis.rnu.tn; IXL laboratory -ENSEIRB - University Bordeauxl - CNRS UMR 5818, 351 Cours de la Libération, 33 405 Talence Cedex, FRANCE","2006 13th IEEE International Conference on Electronics, Circuits and Systems","","2006","","","814","817","In this paper, we present an efficient HW/SW codesign architecture for H.263 video coder and its FPGA implementation. Each module of the coder is investigated to find which approach between HW and SW is better to achieve real-time processing speed as well as flexibility. The hardware portion include the discrete cosine transform (DCT) and inverse DCT (IDCT). Remaining parts were realized in software with NIOS II softcore processor. This paper also introduces efficient design methods for HW and SW modules. In hardware, an efficient architecture for the 2-D DCT and 2-D IDCT is suggested to reduce the chip size. Software optimization technique is also explored by using the fast block-matching algorithm for motion estimation (ME). The whole design are described in VHDL language and implemented in stratix EP2S60 FPGA. Finally, the coder has been tested on the altera NIOS II development board.","","1-4244-0394-41-4244-0395","10.1109/ICECS.2006.379913","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4263491","","Field programmable gate arrays;Discrete cosine transforms;Hardware;Timing;Motion estimation;Video coding;Video compression;Laboratories;Code standards;Bit rate","data compression;discrete cosine transforms;field programmable gate arrays;hardware description languages;hardware-software codesign;motion estimation;optimisation;timing;video codecs","efficient HW-SW codesign architecture;H.263 video coder;FPGA implementation;real-time processing speed;real-time flexibility;2-D discrete cosine transform;2-D inverse DCT;NIOS II softcore processor;chip size reduction;software optimization technique;fast block-matching algorithm;motion estimation;VHDL language;stratix EP2S60 FPGA;altera NIOS II development board","","","18","","","","","","IEEE","IEEE Conferences"
"Reliable and Efficient Adaptive Streaming Mechanism for Multi-user SVC VoD System over GPRS/EDGE Network","L. Zhang; C. Yuan; Y. Zhong","NA; NA; NA","2008 International Conference on Computer Science and Software Engineering","","2008","3","","232","235","In this paper we propose and then implement a novel H.264/AVC Scalable Video Coding multi-user video-on-demand system for hand-held devices over GPRS/EDGE network. We design a reliable and efficient adaptive streaming mechanism for QoS support. The major contributions come from three folds: firstly, accumulation-based congestion control model is used for bandwidth estimation to ensure fairness among all the users. Secondly, for the purpose of handling time-varying bitrate, we propose a PLS algorithm which performs rate adaptation as well as optimizes the video quality in a R-D sense under the bandwidth constraint. Finally, to minimize adaptation delay, IDR frames are online inserted in the pre-encoded stream at the server by using transcoding. Furthermore, to improve system userspsila overall performance, transmission of packets is optimally scheduled through exploiting the multi-user diversity. More importantly, a practical system was implemented and further tested over existing GPRS/EDGE network deployed in China. The results demonstrate that the proposed mechanism achieves outstanding performance in terms of higher perceived video quality, smoother video playback, higher utilization of wireless link and fairer share of resources, compared to RTP Usage Model defined in 3 GPP Release7.","","978-0-7695-3336","10.1109/CSSE.2008.1167","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722330","H.264 SVC;Video-on-Demand;GPRS/EDGE;rate adaptation;congestion control;IDR frame;multiuser optimization","Static VAr compensators;Ground penetrating radar;Streaming media;Bandwidth;High definition video;Video sharing;Automatic voltage control;Video coding;Bit rate;Constraint optimization","3G mobile communication;packet radio networks;quality of service;video coding;video on demand;video streaming","adaptive streaming;multiuser SVC VOD system;GPRS network;EDGE network;H.264/AVC Scalable Video Coding;multiuser video-on-demand system;hand-held devices;QoS support;accumulation-based congestion control model;bandwidth estimation;time-varying bitrate handling;video quality optimisation;transcoding;video playback;3 GPP Release7","","2","12","","","","","","IEEE","IEEE Conferences"
"Evolutionary Algorithms Refining a Heuristic: A Hybrid Method for Shared-Path Protections in WDM Networks Under SRLG Constraints","Q. Zhang; J. Sun; G. Xiao; E. Tsang","NA; NA; NA; NA","IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)","","2007","37","1","51","61","An evolutionary algorithm (EA) can be used to tune the control parameters of a construction heuristic to an optimization problem and generate a nearly optimal solution. This approach is in the spirit of indirect encoding EAs. Its performance relies on both the heuristic and the EA. This paper proposes a three-phase parameterized construction heuristic for the shared-path protection problem in wavelength division multiplexing networks with shared-risk link group constraints and applies an EA for optimizing the control parameters of the proposed heuristics. The experimental results show that the proposed approach is effective on all the tested network instances. It was also demonstrated that an EA with guided mutation performs better than a conventional genetic algorithm for tuning the control parameters, which indicates that a combination of global statistical information extracted from the previous search and location information of the best solutions found so far could improve the performance of an algorithm","1083-4419;1941-0492","","10.1109/TSMCB.2006.883269","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4067092","Estimation of distribution algorithms (EDAs);evolutionary algorithm (EA);guided mutation;hyperheuristics;memetic algorithm (MA);network protection;shared-risk link group (SRLG)","Evolutionary computation;Protection;WDM networks;Optimal control;Encoding;Wavelength division multiplexing;Constraint optimization;Testing;Genetic mutations;Genetic algorithms","evolutionary computation;optical fibre networks;optical links;telecommunication network reliability;wavelength division multiplexing","evolutionary algorithm;hybrid method;shared-path protection;WDM network;control parameter;optimization problem;three-phase parameterized construction;shared-risk link group constraint;conventional genetic algorithm;global statistical information;wavelength division multiplexing","Algorithms;Artificial Intelligence;Biomimetics;Computer Communication Networks;Computer Simulation;Evolution;Models, Genetic;Signal Processing, Computer-Assisted;Software;Systems Theory","24","47","","","","","","IEEE","IEEE Journals & Magazines"
"Measurement of worst-case power delivery noise and construction of worst case current for graphics core simulation","Fern Nee Tan","Penang Design Center, Intel Microelectronics, Malaysia","2009 1st Asia Symposium on Quality Electronic Design","","2009","","","59","63","Worst case graphics core power delivery noise is a major indicator of graphics chip performance. The design of good graphics core power delivery network (PDN) is technically difficult because it is not easy to predict a worst case current stimulus during pre-silicon design stage. Many times, the worst case power delivery noise is observed when graphics benchmark software is run during post-silicon validation. At times like this, it is too late to rectify the power delivery noise issue unless many extra capacitor placeholders are placed during early design stage. To intelligently optimize the graphics core power delivery network design and determining the right amount of decoupling capacitors, this paper suggests an approach that setup a working platform to capture the worst case power delivery noise; and later re-construct the worst case power delivery current using Thevenin's Theorem. The measurement is based on actual gaming application instead of engineering a special stimulus that is generated thru millions of logic test-vectors. This approach is practical, direct and quick, and does not need huge computing resources; or technically skilled logic designers to design algorithms to build the stimulus.","","978-1-4244-4952","10.1109/ASQED.2009.5206296","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5206296","","Current measurement;Power measurement;Noise measurement;Graphics;Capacitors;Logic testing;Algorithm design and analysis;Semiconductor device measurement;Intelligent networks;Design optimization","circuit simulation;deconvolution;logic circuits;microprocessor chips;network synthesis","worst-case power delivery noise;worst case current;graphic core simulation;power delivery network;Thevenin theorem;logic designers;logic test-vectors;decoupling capacitors","","2","5","","","","","","IEEE","IEEE Conferences"
"Controller design for a Marnoch Thermal Energy Conversion generator","J. M. Eklund; Y. F. Kwok; J. Zheng; I. Marnoch","Faculty of Engineering and Applied Science, University of Ontario Institute of Technology, 2000 Simcoe Street North, Oshawa, L1H 7K4 Canada; Faculty of Engineering and Applied Science, University of Ontario Institute of Technology, 2000 Simcoe Street North, Oshawa, L1H 7K4 Canada; Faculty of Engineering and Applied Science, University of Ontario Institute of Technology, 2000 Simcoe Street North, Oshawa, L1H 7K4 Canada; Faculty of Engineering and Applied Science, University of Ontario Institute of Technology, 2000 Simcoe Street North, Oshawa, L1H 7K4 Canada","2009 IEEE Electrical Power & Energy Conference (EPEC)","","2009","","","1","6","This paper presents the design of a discrete event controller for Marnoch Thermal Energy Conversion (MTEC) system test bed for optimization its performance. The controller has been designed with a simulation of the MTE using MATLAB, Simulink, and Stateflow software. A UML model is developed to analyze inputs and outputs requirements for the controller which drives valves and flow direction between tanks using discrete events and logic analysis. This controller design is intended to allow for increasing complex versions of the MHE as additional tank pairs can be added for efficiency and power output increases. The controller has as inputs the tank pressures and piston position and output valve positions to control the flow of hot and cold water to heat and cool the air masses, and air flow values to control the air mass flows through the piston to convert the termal energy into mechanical energy.","","978-1-4244-4508-0978-1-4244-4509","10.1109/EPEC.2009.5420812","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5420812","Marnoch Thermal Engine;Waste heat generator;discrete event controller","Energy conversion;Pressure control;Weight control;Temperature control;Valves;Pistons;Water heating;Control systems;System testing;Design optimization","controllers;electric generators;thermoelectric conversion","Marnoch thermal energy conversion generator;discrete event controller;MATLAB;Simulink;Stateflow software;UML model;logic analysis","","2","5","","","","","","IEEE","IEEE Conferences"
"Prognostics usefulness criteria","J. K. Line; N. S. Clements","Lockheed Martin Aeronaut., Fort Worth, TX, USA; Lockheed Martin Aeronaut., Fort Worth, TX, USA","2006 IEEE Aerospace Conference","","2006","","","7 pp.","","Prognostics and health management (PHM) can provide remarkable insight for maintenance management of large systems, but must be implemented with a healthy respect for the end user and a practical view of the hardware and software capabilities. The F-35 Joint Strike Fighter (JSF) program is implementing a comprehensive PHM system to maximize the supportability of the air system. The prognostic algorithms must have a defined minimum capability to aid implementation and verification. However, the complexity of the air system precludes creating exact requirements for remaining useful life and confidence. Instead, ""usefulness criteria"" are created to link the user need with the minimum capability of the algorithm. The usefulness criteria are a list of goals related to aircraft supportability which can be used to define the minimum acceptable time to maintenance indication for the prognostic algorithm. The goals in the usefulness criteria were applied to each prognostic algorithm in the F-35 PHM system. When assigned, these usefulness criteria provide a means to measure the improved performance of the aircraft and fleet maintenance as well as prioritize the implementation of the prognostic algorithms. The development and implementation of the algorithms in relation to these usefulness criteria are still in process, but it is expected that most algorithms exceeds the criteria. Those which do not meet the criteria are re-evaluated with a trade study to determine if further efforts in hardware and software development are warranted. This process of usefulness criteria development and application can be rigorously applied to the development of any PHM system. This paper covers the development of the usefulness criteria for the F-35 program and the implementation results to date","1095-323X","0-7803-9545","10.1109/AERO.2006.1656123","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1656123","","Prognostics and health management;Algorithm design and analysis;Software algorithms;Postal services;Hardware;Aircraft;Vehicles;Monitoring;Propulsion;Software testing","aircraft maintenance;program verification;remaining life assessment;software engineering","prognostics usefulness criteria;prognostics and health management;maintenance management;F-35 Joint Strike Fighter;prognostic algorithms;aircraft supportability;maintenance indication;aircraft maintenance;fleet maintenance","","1","5","","","","","","IEEE","IEEE Conferences"
"Traffic and Network Engineering in Emerging Generation IP Networks: A Bandwidth on Demand Model","A. B. Bagula; A. E. Krzesinski","Department of Mathematical Sciences, Computer Science Section, University of Stellenbosch, 7600 Stellenbosch, South Africa; Department of Electronics, Computer and Software systems, Royal Institute of Technology, Isaffordsgatan 39, Stockholm, Sweden. Email: abbagula@kth.se, bagula@cs.sun.ac.za.; Department of Mathematical Sciences, Computer Science Section, University of Stellenbosch, 7600 Stellenbosch, South Africa. Email: aek1@cs.sun.ac.za.","2006 IEEE First International Workshop on Bandwidth on Demand","","2006","","","36","43","This paper assesses the performance of a network management scheme where network engineering (NE) is used to complement traffic engineering (TE) in a multi-layer setting where a data network is layered above an optical network. We present a TE strategy which is based on a multi-constraint optimization model consisting of finding bandwidth-guaranteed IP tunnels subject to contention avoidance minimization and bandwidth usage maximization constraints. The TE model is complemented by a NE model which uses a bandwidth trading mechanism to rapidly re-size and re-optimize the established tunnels (LSPs/lambdaSPs) under quality of service (QoS) mismatches between the traffic carried by the tunnels and the resources available for carrying the traffic. The resulting TE+NE strategy can be used to achieve bandwidth on demand (BoD) in emerging generation IP networks using a (G)MPLS- like integrated architecture in a cost effective way. We evaluate the performance of this hybrid strategy when routing, re-routing and re-sizing the tunnels carrying the traffic offered to a 23-node test network.","","1-4244-0793","10.1109/BOD.2006.320796","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4114884","","Telecommunication traffic;Traffic control;IP networks;Bandwidth;Tellurium;Data engineering;Quality of service;Engineering management;Optical fiber networks;Constraint optimization","IP networks;optimisation;quality of service;telecommunication traffic","generation IP networks;network engineering;traffic engineering;multi-layer setting;multi-constraint optimization model;quality of service;bandwidth on demand","","2","13","","","","","","IEEE","IEEE Conferences"
"A field test using agents for coordination of residential micro-chp","C. J. Warmer; M. P. F. Hommelberg; B. Roossien; J. K. Kok; J. W. Turkstra","Intelligent Grids program, Energy research Centre of the Netherlands, Westerduinweg 3, P.O. Box 1, 1755 ZG Petten, The Netherlands. e-mail: warmer@ecn.nl; Intelligent Grids program, Energy research Centre of the Netherlands, Westerduinweg 3, P.O. Box 1, 1755 ZG Petten, The Netherlands. e-mail: hommelberg@ecn.nl; Intelligent Grids program, Energy research Centre of the Netherlands, Westerduinweg 3, P.O. Box 1, 1755 ZG Petten, The Netherlands. e-mail: roossien@ecn.nl; Intelligent Grids program, Energy research Centre of the Netherlands, Westerduinweg 3, P.O. Box 1, 1755 ZG Petten, The Netherlands. e-mail: j.k.kok@ecn.nl; N.V. Nederlandse Gasunie, Gasunie Engineering & Technology, Groningen, The Netherlands.","2007 International Conference on Intelligent Systems Applications to Power Systems","","2007","","","1","4","In the Netherlands decentralised generation of heat and power by mu-CHP units in households is expected to penetrate the market at high speed in the coming years. Using ICT these mu-CHP units can be integrated into a smart power system or virtual power plant. Other local production and consumption of electricity such as PV, wind, heat pumps and electrical vehicles can be added to this cluster. The main goal of a smart power system is to optimize the value of decentralised power production and consumption in view of the total energy value chain. The PowerMatcher is a multi-agent based control concept (and software package) for coordination of demand and supply in electricity networks with a high share of distributed generation. The concept is demonstrated in several real life field tests. One of these field tests is a virtual power plant consisting of 10 mu-CHP units reducing the local peak demand of the common low-voltage grid segment the mu-CHP units are connected to. In this way the VPP supports the local distribution network operator (DNO) to defer reinforcements in the grid infrastructure (transformers and cables). To realize this VPP, an ICT-communication network containing a hardware and software infrastructure has been added to a test rollout of mu-CHP installations in The Netherlands. Main conclusion from the field test is that a peak reduction of 30 -50% can be achieved, depending on summer or winter season.","","978-986-01-2607","10.1109/ISAP.2007.4441634","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4441634","Distributed Generation;Virtual Power Plant;CHP;Smart Grids;Multi Agent Systems;Electronic Markets","Testing;Power generation;Power systems;Distributed power generation;Wind energy generation;Production;Energy consumption;Resistance heating;Heat pumps;Electric vehicles","multi-agent systems;power distribution control","residential micro-clip;smart power system;decentralised power production;multi-agent based control concept;software package;local distribution network operator;grid infrastructure","","13","10","","","","","","IEEE","IEEE Conferences"
"Approximating 3D shape using Bezier surface","M. S. Muhammad; M. T. Mahamood; T. Choi","Signal and Image Processing Lab, School of Information and Mechatronics, Gwangju Institute of Science and Technology, 261 Cheomdan-gwagiro, Buk Gu, 500712, Korea; Signal and Image Processing Lab, School of Information and Mechatronics, Gwangju Institute of Science and Technology, 261 Cheomdan-gwagiro, Buk Gu, 500712, Korea; Signal and Image Processing Lab, School of Information and Mechatronics, Gwangju Institute of Science and Technology, 261 Cheomdan-gwagiro, Buk Gu, 500712, Korea","2009 IEEE International Conference on Acoustics, Speech and Signal Processing","","2009","","","753","756","Estimating 3D shape of the object is an important research topic in the area of computer vision, with a wide range of applications. This paper introduces a new method for focused-based passive methods (like SFF) to approximate the 3D shape of the object using Bezier surface. The discrete nature of image sampling results in the loss of information between two consecutive images. Conventional approximation methods optimize or interpolate focus values. We have suggested interpolating depth values instead of focus value over a small patch on the object surface. The method approximates the surface more accurately and also reduces any noise caused by focus measure. The proposed method is tested and analyzed to demonstrate the effectiveness against traditional methods.","1520-6149;2379-190X","978-1-4244-2353-8978-1-4244-2354","10.1109/ICASSP.2009.4959693","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4959693","Shape from Focus;Bezier Surface;3D Shape Approximation;Passive Methods","Shape;Focusing;Computer vision;Application software;Image sampling;Approximation methods;Optimization methods;Noise reduction;Noise measurement;Testing","computer vision;image sampling;interpolation;shape recognition","3D shape approximation;Bezier surface;3D shape estimation;computer vision;focused-based passive method;image sampling;interpolation;object surface","","","7","","","","","","IEEE","IEEE Conferences"
"The Application of a Modified Algorithm on Quadratic Interpolation Models","Q. Zhou; Y. Li","College of Mathematics and Computer, Hebei University, Baoding, 071002, Hebei Province, China. E-MAIL: qinghua.zhou@gmail.com; College of Mathematics and Computer, Hebei University, Baoding, 071002, Hebei Province, China. E-MAIL: liyan@cmc.hbu.cn","2007 International Conference on Machine Learning and Cybernetics","","2007","5","","2494","2498","In this paper, we propose a modified method on quadratic interpolation models which are updated by Frobenius norm. The method uses the progressively obtained information during the iterations of the algorithm to form new search subspaces. Then the quadratic model is solved in the new subspaces. The main idea is to use the information revealed in the previous steps to construct more promising directions. The effectiveness is proved in that the numbers of function evaluations are reduced obviously for most testing problems. At the end of this paper, we tested the algorithm on some variable dimension problems.","2160-133X;2160-1348","978-1-4244-0972-3978-1-4244-0973","10.1109/ICMLC.2007.4370566","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4370566","Unconstrained optimization;Trust region method;Quadratic model;Lagrange function;Direct methods","Interpolation;Testing;Iterative methods;Machine learning;Cybernetics;Machine learning algorithms;Mathematical model;Application software;Educational institutions;Mathematics","interpolation;iterative methods;quadratic programming","quadratic interpolation models;trust region method;iteration method","","1","7","","","","","","IEEE","IEEE Conferences"
"Dependability analysis: performance evaluation of environment configurations","P. Vanhauwaert; R. Leveugle; P. Roche","NA; NA; NA","International Conference on Design and Test of Integrated Systems in Nanoscale Technology, 2006. DTIS 2006.","","2006","","","335","340","Prototyping-based fault injection environments are employed to perform dependability analysis and thus predict the behavior of circuits in presence of faults. A novel environment has been recently proposed to perform several types of dependability analyses in a common optimized framework. The approach takes advantage of hardware speed and of software flexibility to achieve optimized trade-offs between experiment duration and processing complexity. This paper discusses the possible repartition of tasks between hardware and embedded software with respect to the type of circuit to analyze and to the instrumentation achieved. The performances of the approach are evaluated for each configuration of the environment","","0-7803-9726","10.1109/DTIS.2006.1708652","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1708652","","Performance analysis;Circuit faults;Instruments;Hardware;Performance evaluation;Prototypes;Circuit analysis;Circuit simulation;Virtual prototyping;Field programmable gate arrays","fault simulation;field programmable gate arrays;integrated circuit testing","dependability analysis;performance evaluation;fault injection;software flexibility;task repartition","","","5","","","","","","IEEE","IEEE Conferences"
"An Improved Shock Graph Approach for Shape Recognition and Retrieval","H. Zaboli; M. Rahmati","Amirkabir University of Technology, Iran; Amirkabir University of Technology, Iran","First Asia International Conference on Modelling & Simulation (AMS'07)","","2007","","","438","443","Object recognition and shape matching are important issues in the field of image processing. Extraction and application of skeleton of a shape is widely used in these fields. In this paper shape matching and retrieval is performed using one of the skeleton-based methods called ""shock graphs"". By modifying and optimizing this method, results have been improved significantly, especially in the presence of occlusion and missing parts. In this extension, branch points are added as key points to the shock graph and its grammar and consequently a new grammar is developed. Our experimental results due to the modifications and extensions are presented with different examples and tests, especially in the presence of occlusion and missing parts","","0-7695-2845","10.1109/AMS.2007.13","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4148701","Shape matching;shape retrieval;skeleton;shock graph;branch point.","Electric shock;Shape;Skeleton;Image processing;Image recognition;Image retrieval;Object recognition;Application software;Optimization methods;Testing","feature extraction;grammars;graph theory;image matching;image retrieval;object recognition","shock graph;shape recognition;shape retrieval;object recognition;shape matching;image processing;skeleton extraction;skeleton-based methods","","3","13","","","","","","IEEE","IEEE Conferences"
"Improved Ant Colony Algorithm with Pheromone Mutation and its Applications in Flow-shop Problems","Bing Li; Xuemei Song","Tangshan college, Tangshan 063000. E-mail: flibing@sina.com.cn; NA","2006 6th World Congress on Intelligent Control and Automation","","2006","1","","3353","3356","Ant colony algorithm (ACA) has the disadvantages such as easily relapsing into local optimization and stagnation. Aimed at improving this problem existed in ACA, several new betterments are proposed and evaluated. In particular, pheromone mutation and stochastic search strategy were inducted. Then an improved ant colony algorithm with pheromone mutation was put forward. It was tested by a set of benchmark travelling salesman problems from the travelling salesman problem library. And the results of the examples show that it can not easily run into the local optimum and can converge at the global optimum. The improved algorithm was used to solve the flow-shop problems. It performs better than the other algorithms such as genetic algorithm","","1-4244-0332","10.1109/WCICA.2006.1712989","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1712989","Ant Colony Algorithm;Traveling Salesman Problem;Flow-shop Problem","Genetic mutations;Traveling salesman problems;Application software;Educational institutions;Automatic control;Ant colony optimization;Stochastic processes;Benchmark testing;Libraries;Genetic algorithms","flow shop scheduling;search problems;travelling salesman problems","ant colony algorithm;pheromone mutation;flow-shop problems;stochastic search strategy;traveling salesman problems;genetic algorithm","","1","8","","","","","","IEEE","IEEE Conferences"
"Geometrically optimized, Labr3: Ce scintillation sensor array for enhanced stand-off direction finding of gamma radiation sources","J. H. Winso; E. S. Ackermann; M. Fennell; R. Perez; J. Rolando; M. Pagey; R. Polichar; J. Martinez; J. Hovgaard; G. Kogut; H. R. Everett; D. Fellars; J. Baumbaugh; G. Mastny","Space Micro Incorporated, San Diego, CA 92121 USA; Space Micro Incorporated, San Diego, CA 92121 USA; Space Micro Incorporated, San Diego, CA 92121 USA; Space Micro Incorporated, San Diego, CA 92121 USA; Space Micro Incorporated, San Diego, CA 92121 USA; NA; Science Applications International Corporation, San Diego, CA 92127 USA; Science Applications International Corporation, San Diego, CA 92127 USA; Radiation Solutions Incorporated, Mississauga, Ont, Canada; Space and Naval Warfare System Center, San Diego, CA 92106 USA; Space and Naval Warfare System Center, San Diego, CA 92106 USA; Space and Naval Warfare System Center, San Diego, CA 92106 USA; Space and Naval Warfare System Center, San Diego, CA 92106 USA; Space and Naval Warfare System Center, San Diego, CA 92106 USA","2007 IEEE Nuclear Science Symposium Conference Record","","2007","2","","1009","1015","A radiation source identification and targeting (RadSITEtrade) innovation has been conceptually demonstrated that remotely detects and identifies one or more localized gamma sources simultaneously and also provides azimuth Directions of each source. The innovation exploits the superior energy resolution of cerium doped lanthanum bromide (LaBr3:Ce) Scintillators to select only photons emanating directly from the localized source(s). Laboratory bench testing provided physical confirmation that an array of unique, elongated geometry, LaBr3:Ce sensors performed as predicted by MCNP modeling to resolve source azimuth direction. The response of each sensor element is anisotropic providing a response dependent on the azimuth of the source relative to the sensor, while the integrated response of the entire array is isotropic, providing a source strength-distance indicator to act as a reference for an original software algorithm that processes each isotope response to determine azimuth. Laboratory tests have confirmed that an array of four (4) sensors would be sufficient to provide plusmn5deg azimuth determination over a 360deg field of view. The approach accommodates a large volume of scintillation material with minimal shielding &lt;20% compared to about 50% for Coded Aperture approaches to provide a high sensitivity and a wide (360deg) field of view. This patent pending innovation has been integrated with an iRobot ATRV robotic platform and autonomous approach to isotopic sources in a field environment has been demonstrated.","1082-3654","978-1-4244-0922-8978-1-4244-0923","10.1109/NSSMIC.2007.4437184","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4437184","","Sensor arrays;Gamma ray detectors;Gamma rays;Azimuth;Technological innovation;Energy resolution;Laboratories;Testing;Gamma ray detection;Radiation detectors","high energy physics instrumentation computing;radioactive sources;solid scintillation detectors","geometrically optimized sensor array;LaBr<sub>3</sub>:Ce scintillators;cerium doped lanthanum bromide scintillators;radiation source identification and targeting innovation;RadSITE;localized gamma sources;MCNP modeling;software algorithm;scintillation material;minimal shielding;coded aperture approaches;patent pending innovation;iRobot ATRV robotic platform;isotopic sources","","1","10","","","","","","IEEE","IEEE Conferences"
"A new coarse-grained FPGA architecture exploration environment","H. Parvez; Z. Marrakchi; U. Farooq; H. Mehrez","LIP6, Université Pierre et Marie Curie, 4, Place Jussieu, 75005 Paris, France; LIP6, Université Pierre et Marie Curie, 4, Place Jussieu, 75005 Paris, France; LIP6, Université Pierre et Marie Curie, 4, Place Jussieu, 75005 Paris, France; LIP6, Université Pierre et Marie Curie, 4, Place Jussieu, 75005 Paris, France","2008 International Conference on Field-Programmable Technology","","2008","","","285","288","This paper presents an exploration environment for the design of 2D island-style coarse grained FPGA architectures. An architecture description file defines various architectural parameters including the definition of new coarse grained blocks, the positioning of blocks in the architecture and the selection of routing network. Once the initial architecture is defined, a software flow places and routes a target netlist on the generated architecture. The placement cost of a netlist is optimized either by changing the position of netlist instances on its respective blocks or by changing the position of blocks on the architecture. A single FPGA architecture can also be obtained for mapping a set of netlists at mutually exclusive times. It has been found that the sum of the placement costs of all the netlists is found to be minimum if all the netlists are used to get a single architecture. A set of DSP test-benches is used to show the effectiveness of the various techniques used in this work.","","978-1-4244-2795-6978-1-4244-2796-3978-1-4244-3783","10.1109/FPT.2008.4762399","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4762399","","Field programmable gate arrays;Computer architecture;Routing;Pins;Cost function;Digital signal processing;Testing;Logic;Application software;Context modeling","field programmable gate arrays;reconfigurable architectures","coarse-grained FPGA architecture;field programmable gate arrays;architecture description file;DSP test-benches","","","12","","","","","","IEEE","IEEE Conferences"
"Low voltage ride-through analysis of 2 MW DFIG wind turbine - grid code compliance validations","S. Seman; J. Niiranen; R. Virtanen; J. Matsinen","ABB Oy, Drives, FI-00381 Helsinki, Finland; ABB Oy, Drives, FI-00381 Helsinki, Finland; ABB Oy, Drives, FI-00381 Helsinki, Finland; ABB Oy, Drives, FI-00381 Helsinki, Finland","2008 IEEE Power and Energy Society General Meeting - Conversion and Delivery of Electrical Energy in the 21st Century","","2008","","","1","6","The transient behavior of the 2 MW wind turbine under a symmetrical 3-phase voltage dip with zero remaining voltage and unsymmetrical phase-to-phase disturbance with 20 % residual voltage is experimentally studied. The results of the voltage dip tests are compared with simulation study. The comparison of the results shows close agreement; therefore validated simulation model is suitable for performing wind turbine voltage dip test simulations without a need to make the site tests. The developed model is also suitable for accurate transient simulation of the small wind park with reduced number of wind turbines. Additionally it can be used as a reference model for the optimization and verification of the simplified turbine model used in power system analysis software.","1932-5517","978-1-4244-1905-0978-1-4244-1906","10.1109/PES.2008.4596687","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4596687","Converter;DFIG;grid code;ride-through;unsymmetrical;voltage dip","Wind turbines;Voltage measurement;Voltage fluctuations;Integrated circuit modeling;Current measurement;Circuit faults;Transient analysis","asynchronous generators;power generation faults;power grids;wind turbines","DFIG;doubly fed induction generator;wind turbine;grid code compliance testing;voltage dip test;optimization;power system analysis software;phase-to-phase disturbance;power 2 MW","","6","12","","","","","","IEEE","IEEE Conferences"
"Design and Development of an Offset-Beam Slotted Waveguide Antenna","R. A. Bhatti; A. Ikram; J. K. Kayani","Center for Wireless Communication and Radar Technology, National Engineering and Scientific Commission, Islamabad.; Center for Wireless Communication and Radar Technology, National Engineering and Scientific Commission, Islamabad.; Center for Wireless Communication and Radar Technology, National Engineering and Scientific Commission, Islamabad.","2007 International Bhurban Conference on Applied Sciences & Technology","","2007","","","23","26","Planar slotted waveguide antennas usually have their main-beam pointed perpendicular to the antenna aperture. While this arrangement is most practical and convenient from design point of view, it suffers a serious setback in that it offers large RCS in the direction of antenna main-beam. This paper presents the design and testing of an offset-beam antenna for low RCS applications. A linear traveling wave antenna is designed at Ku-band. It consists of 18 radiating slots cut in the broadwall of the waveguide. A 25 dB Taylor aperture distribution is used to reduce sidelobes. The antenna is simulated and optimized using high frequency structure simulator (HFSS) software. Manufacturing of the antenna was done using high precision CNC milling machine. After assembly, the antenna was characterized by measuring its scattering parameters and radiation pattern. Measured results are fairly close to the theoretically predicted and simulated results. Gain of the antenna is 15.0 dB over a bandwidth of 500 MHz. Power absorbed into the load is 20% and return loss is below -30 dB. First side lobe level (SLL) is 23.5 dB down and azimuth beam width is 7.0deg with beam pointing at an offset angle of 20deg in azimuth plane.","2151-1403;2151-1411","978-969-8741-04","10.1109/IBCAST.2007.4379901","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4379901","","Slot antennas;Aperture antennas;Antenna measurements;Azimuth;Planar waveguides;Directive antennas;Testing;Application software;Frequency;Manufacturing","aperture antennas;planar antennas;slot antennas","offset beam slotted waveguide antenna;planar slotted waveguide antennas;antenna aperture;antenna main-beam direction;linear traveling wave antenna;aperture distribution;high frequency structure simulator software;CNC milling machine","","","4","","","","","","IEEE","IEEE Conferences"
"A new generation image processing algorithm might enable Real-Time visualization of 3D Echocardiographic data on a multi-view auto stereoscopic LCD","G. Saracino; N. Greenberg; S. Fukuda; T. Shiota; J. D. Thomas","Cleveland Clinic Foundation, OH, USA; Cleveland Clinic Foundation, OH, USA; Cleveland Clinic Foundation, OH, USA; Cleveland Clinic Foundation, OH, USA; Cleveland Clinic Foundation, OH, USA","2006 Computers in Cardiology","","2006","","","133","136","Visualization of Real-Time 3D Echocardiography (RT3DE) is currently limited by the bi-dimensional nature of conventional monitors. The objective of this study is to implement and evaluate an optimized image-processing algorithm to generate multi-view auto-stereoscopic images. Custom-made software rendered test images (n=5) on a 9-view auto-stereoscopic 3D LCD. We designed and compared two algorithms that used hardware acceleration of graphic adapters. The quality of the final 3D image with the two algorithms was comparable. This study showed that the new-generation algorithm is very efficient in processing complex rendering such as visualization of RT3DE data. This algorithm might be an enabling factor for realtime visualization of RT3DE on multi-view auto stereoscopic display, which might improve clinical imaging in a host of diseases and use of RT3DE in guidance of interventional procedures.","0276-6574;2325-8853","978-1-4244-2532","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4511806","","Image generation;Image processing;Data visualization;Rendering (computer graphics);Echocardiography;Software testing;Algorithm design and analysis;Hardware;Acceleration;Graphics","biology computing;data visualisation;echocardiography;liquid crystal displays;stereo image processing","image processing algorithm;real-time visualization;3D echocardiographic data;multiview auto stereoscopic LCD;custom-made software;hardware acceleration;multiview auto stereoscopic display;clinical imaging;diseases","","","10","","","","","","IEEE","IEEE Conferences"
"Research of the new low frequency, broadband free-flooded piezoelectric underwater acoustic transducer","W. Lu; Z. Zhang; Y. Lan","College of Underwater Acoustic Engineering, Harbin Engineering University, Heilongjiang 150001, China; College of Underwater Acoustic Engineering, Harbin Engineering University, Heilongjiang 150001, China; College of Underwater Acoustic Engineering, Harbin Engineering University, Heilongjiang 150001, China","2009 Symposium on Piezoelectricity, Acoustic Waves, and Device Applications (SPAWDA 2009)","","2009","","","10","10","The transducer which has characteristics of low frequency, broadband, deep submergence is an important direction in research of the underwater acoustic transducer. The resonant frequency of transducer was reduced and the characteristic of deep submergence was enhanced by using the free-flooded structure. A low frequency, broadband transducer with the multicavity free-flooded structure was studied with finite element method in the paper. Then, the finite element modal of transducer was set up with ANSYS software and the structure of the transducer was optimized. A low frequency, broadband transducer with the multicavity free-flooded structure was designed and the excitation of transducer was a free-flooded piezoelectric ring. Finally, the prototype for experiment was produced. Basing on testing, the bandwidth of the transducer was 3kHz-7kHz, which the ripple of the transmitting voltage response does not exceed ±3dB. The transducer achieved target of design.","","978-1-4244-4950","10.1109/SPAWDA.2009.5428992","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5428992","Low frequency;broadband;free-flooded;multicavity;finite element","Underwater acoustics;Acoustic transducers;Piezoelectric transducers;Finite element methods;Resonant frequency;Software prototyping;Prototypes;Testing;Bandwidth;Voltage","acoustic transducers;finite element analysis;piezoelectric transducers;underwater sound","piezoelectric underwater acoustic transducer;low frequency broadband transducer;resonant frequency;deep submergence;multicavity free-flooded structure;finite element method;ANSYS software;free-flooded piezoelectric ring;transmitting voltage response;frequency 3 kHz to 7 kHz","","","","","","","","","IEEE","IEEE Conferences"
"An Optimized Cell BE Special Function Library Generated by Coconut","C. K. Anand; W. Kahl","McMaster University, Hamilton; McMaster University, Hamilton","IEEE Transactions on Computers","","2009","58","8","1126","1138","Coconut, a tool for developing high-assurance, high-performance kernels for scientific computing, contains an extensible domain-specific language (DSL) embedded in Haskell. The DSL supports interactive prototyping and unit testing, simplifying the process of designing efficient implementations of common patterns. Unscheduled C and scheduled assembly language output are supported. Using the patterns, even nonexpert users can write efficient function implementations, leveraging special hardware features. A production-quality library of elementary functions for the cell BE SPU compute engines has been developed. Coconut-generated and -scheduled vector functions were more than four times faster than commercially distributed functions written in C with intrinsics (a nicer syntax for in-line assembly), wrapped in loops and scheduled by spuxlc. All Coconut functions were faster, but the difference was larger for hard-to-approximate functions for which register-level SIMD lookups made a bigger difference. Other helpful features in the language include facilities for translating interval and polynomial descriptions between GHCi, a Haskell interpreter used to prototype in the DSL, and Maple, used for exploration and minimax polynomial generation. This makes it easier to match mathematical properties of the functions with efficient calculational patterns in the SPU ISA. By using single, literate source files, the resulting functions are remarkably readable.","0018-9340;1557-9956;2326-3814","","10.1109/TC.2008.223","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4731241","Special function approximations;parallel and vector implementations;code generation;specialized application languages;SIMD processors;applicative (functional) programming.","Polynomials;DSL;Computer architecture;Approximation methods;Libraries;Microprocessors;Assembly","assembly language;C language;functional languages;software engineering","optimized cell BE special function library;Coconut;high-performance kernels;scientific computing;extensible domain-specific language;Haskell interpreter;interactive prototyping;unit testing;scheduled assembly language output;unscheduled C language output;spuxlc;register-level SIMD lookups;GHCi;DSL;Maple;minimax polynomial generation","","4","20","","","","","","IEEE","IEEE Journals & Magazines"
"Coasting point optimisation for mass rail transit lines using artificial neural networks and genetic algorithms","S. Ac¿kbas; M. T. Soylemez","NA; NA","IET Electric Power Applications","","2008","2","3","172","182","Energy consumption of a rail transit system depends on many parameters. One of the most effective methods of reducing energy consumption in a rail transit system is optimising the speed profile of the trains along the route. A new efficient method will be presented for the optimisation of the coasting points for trains in a global manner. The proposed approach includes realistic system modelling using multi-train, multi-line simulation software and application of artificial neural networks (ANN) and genetic algorithms (GA). The simulation software used can model regenerative braking and train performance at low voltages. Using ANN and GA together, optimal coasting points for long line sections covering five stations and two lines are achieved. Simulation software is used for creating training and test data for the ANN. These data are used for training of the ANN. Trained ANNs are then used for estimating energy consumption and travel time for new sets of coasting points. Finally, the outputs of the ANN are optimised to find optimal train coasting points. For this purpose, a fitness function with target travel time, energy consumption and weighting factors is proposed. An interesting observation is that the use of ANN increases the speed of optimisation. The proposed method is used for optimising coasting points for minimum energy consumption for a given travel time on the first 5 km section of Istanbul Aksaray-Airport metro line, where trains operate every 150 s. The section covers five passenger stations, which means four coasting points for each line. It has been demonstrated that an eight input ANNs can be trained with acceptable error margins for such a system.","1751-8660;1751-8679","","10.1049/iet-epa:20070381","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4519794","","","digital simulation;electrical engineering computing;genetic algorithms;neural nets;railway engineering","coasting point optimisation;mass rail transit lines;artificial neural networks;genetic algorithms;energy consumption;rail transit system;simulation software;regenerative braking;passenger stations","","22","","","","","","","IET","IET Journals & Magazines"
"Optimal parameter coordination of power system stabilizers in multi-machine power systems employing Harmony Search","Danyue Wu; Ting Huang; Guizhong Yang; Yan Yang; Wenxin Guo; Z. Lin; Fushuan Wen","Fujian Electric Power Test and Research Institute, Fuzhou 350007, China; Fujian Electric Power Test and Research Institute, Fuzhou 350007, China; Fujian Electric Power Dispatching Center, Fuzhou 350003, China; School of Electrical Engineering, South China University of Technology, Guangzhou 510640, China; School of Electrical Engineering, South China University of Technology, Guangzhou 510640, China; School of Electrical Engineering, Zhejiang University, Hangzhou 310027, China; School of Electrical Engineering, Zhejiang University, Hangzhou 310027, China","8th International Conference on Advances in Power System Control, Operation and Management (APSCOM 2009)","","2009","","","1","5","Power system stabilizers (PSSs) play an important role in suppressing low frequency oscillations. In multi-machine power systems, the objective of optimal PSS parameter tuning should not only be to damp electromechanical modes strongly related to the pertained generator, but also to take the damping status of other generators into account. Taking this in mind, a new assessment criterion from the perspective of generators is proposed based on the well-known eigenvalue analysis. An objective function reflecting the overall stability as well as the damp distribution among generators is developed. The Harmony Search (HS), an effective modern heuristic optimization algorithm developed in recent years, is employed for solving this constrained optimization problem. Finally, a software system is designed and implemented for applications to practical power systems. Two power systems, i.e. the New England 10-unit 39-bus system and the 64-unit 305-bus Fujian power system are served for demonstrating the feasibility and efficiency of the developed model and method.","","978-1-84919-214","10.1049/cp.2009.1857","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5528293","Power system stabilizer (PSS);multi-machine power system;damping criterion of generator;system stability criterion;Harmony Search","","","","","","","","","","","","IET","IET Conferences"
"Application of multi-objective bee colony optimization algorithm to Automated Red Teaming","M. Y. H. Low; M. Chandramohan; C. S. Choo","School of Computer Engineering, Nanyang Technological University, Nanyang Avenue, Singapore 639798; School of Computer Engineering, Nanyang Technological University, Nanyang Avenue, Singapore 639798; DSO National Laboratories 20 Science Park Drive, Singapore 118230","Proceedings of the 2009 Winter Simulation Conference (WSC)","","2009","","","1798","1808","Automated Red Teaming (ART) is an automated process for Manual Red Teaming which is a technique frequently used by the Military Operational Analysis community to uncover vulnerabilities in operational tactics. The ART makes use of multi-objective evolutionary algorithms such as SPEAII and NSGAII to effectively find a set of non-dominated solutions from a large search space. This paper investigates the use of a multi-objective bee colony optimization (MOBCO) algorithm with Automated Red Teaming. The performance of the MOBCO algorithm is first compared with a well known evolutionary algorithm NSGAII using a set of benchmark functions. The MOBCO algorithm is then integrated into the ART framework and tested using a maritime case study involving the defence of an anchorage. Our experimental results show that the MOBCO algorithm proposed is able to achieve comparable or better results compared to NSGAII in both the benchmark function and the ART maritime scenario.","0891-7736;1558-4305","978-1-4244-5771-7978-1-4244-5770-0978-1-4244-5772","10.1109/WSC.2009.5429184","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5429184","","Subspace constraints;Evolutionary computation;Web server;Military computing;Algorithm design and analysis;Benchmark testing;Internet;Computational modeling;Application software;Laboratories","evolutionary computation;military systems","multiobjective bee colony optimization algorithm;automated red teaming;manual red teaming;military operational analysis community;operational tactics;SPEAII;NSGAII;ART maritime scenario;MOBCO algorithm;multiobjective evolutionary algorithms","","8","24","","","","","","IEEE","IEEE Conferences"
"Application of elevator group control system based on genetic algorithm optimize BP fuzzy neural network","Yanqiu Wang; Jian Zhang; Yueling Zhao; Yu Wang","Liao Ning University Of Technology, Jin Zhou, 121001, China; Liao Ning University Of Technology, Jin Zhou, 121001, China; Liao Ning University Of Technology, Jin Zhou, 121001, China; Liao Ning University Of Technology, Jin Zhou, 121001, China","2008 7th World Congress on Intelligent Control and Automation","","2008","","","8702","8705","Its deficiency was revealed because of traffic pattern identification method of elevator group control system based on using BP neural network, and a new traffic patten identification model is proposed which is based on optimizing fuzzy neural network by genetic algorithm. The genetic algorithm is used to train fuzzy BP neural network, which can overcome the shortcoming of local minimum appeared while training the network, and the veracity of the whole traffic pattern identification model can be increased. At last, the sampled data are trained and tested Matlab software, and the simulation results indicate that the proposed identify model has very small error.","","978-1-4244-2113-8978-1-4244-2114","10.1109/WCICA.2008.4594612","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4594612","Elevator Group Control System;genetic algorithm;BP neural network;fuzzy neural network","Genetic algorithms;Elevators;Control systems;Gallium;Fuzzy control;Mathematical model;Fuzzy neural networks","backpropagation;fuzzy control;fuzzy neural nets;genetic algorithms;lifts;neurocontrollers","elevator group control system;genetic algorithm;BP fuzzy neural network;optimization;traffic pattern identification","","2","8","","","","","","IEEE","IEEE Conferences"
"Representation of line optimisation control in unified power-flow controller model for power-flow analysis","T. T. Nguyen; V. L. Nguyen","NA; NA","IET Generation, Transmission & Distribution","","2007","1","5","714","723","Drawing on constrained optimisation based on Newton's method, a systematic and general method for determining optimal reference inputs to unified power-flow controllers (UPFCs) in steady-state operation is developed. The method is directly applicable to UPFCs operation with a high-level line optimisation control. Through the selection of weighting coefficients used in the objective function which is formed from the weighted difference between the specified reference inputs and their optimal values, the method represents the priority assigned for any UPFC control function in constraint or limit resolution. Another key advance reported is that of combining the sparse Newton's method with the continuation technique for solving the nonlinear constrained optimisation. The composite algorithm extends substantially the region of convergence achieved with the conventional Newton's method. The method uses a general and flexible UPFC model based on nodal voltages developed. Any UPFC control functions together with operating limits can be included in the model. The steady-state formulation developed together with its software implementation is tested with a practical long-distance transmission interconnection where a UPFC is required.","1751-8687;1751-8695","","10.1049/iet-gtd:20060372","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4294998","","","load flow control;optimisation","line optimisation control;unified power-flow controller model;power-flow analysis;Newton's method;steady-state operation;UPFC;unified power-flow controllers;long-distance transmission interconnection","","1","","","","","","","IET","IET Journals & Magazines"
"Disparity Mapping for Navigation of Stereo Vision Autonomous Guided Vehicle","A. H. A. Hasan; R. A. Hamzah; M. H. Johar","NA; NA; NA","2009 International Conference of Soft Computing and Pattern Recognition","","2009","","","575","579","Stereo vision system is a practical method for depth gathering of objects and features in an environment. This paper presents the disparity mapping for navigation of stereo vision autonomous guided vehicle using block matching algorithm. The stereo vision baseline is based on horizontal configuration. The block matching technique is briefly described with the performance of its output. The disparity mapping is generated by the algorithm with reference to the left image coordinate. The algorithm is using sum of absolute differences (SAD) which runs in Matlab software.","","978-1-4244-5330-6978-0-7695-3879","10.1109/SoCPaR.2009.114","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5370036","Block matching;disparity mapping;epipolar line;obstacle detection;stereo vision","Navigation;Stereo vision;Remotely operated vehicles;Mobile robots;Design optimization;Printing;Integer linear programming;Constraint optimization;Testing;Containers","image matching;mobile robots;path planning;stereo image processing;vehicles","disparity mapping;stereo vision autonomous guided vehicle navigation;depth gathering;block matching;sum of absolute differences;Matlab software","","4","13","","","","","","IEEE","IEEE Conferences"
"Fast and Precise Points-to Analysis","J. Lundberg; T. Gutzmann; W. Löwe","NA; NA; NA","2008 Eighth IEEE International Working Conference on Source Code Analysis and Manipulation","","2008","","","133","142","Many software engineering applications require points-to analysis. Client applications range from optimizing compilers to program development and testing environments to reverse-engineering tools. In this paper, we present a new context-sensitive approach to points-to analysis where calling contexts are distinguished by the points-to sets analyzed for their target expressions. Compared to other well-known context-sensitive techniques, it is faster - twice as fast as the call string approach and by an order of magnitude faster than the object-sensitive technique - and requires less memory. At the same time, it provides higher precision than the call string technique and is similar in precision to the object-sensitive technique. These statements are confirmed by experiments.","","978-0-7695-3353","10.1109/SCAM.2008.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4637546","static points-to analysis;context-sensitive analysis","Sensitivity;Java;Object recognition;Memory management;Analytical models;Transfer functions;Merging","context-sensitive languages;program diagnostics;program testing;reverse engineering;software engineering","points-to analysis;software engineering application;program development;program testing;reverse-engineering tool;context-sensitive approach;object-sensitive technique;static program analysis","","1","38","","","","","","IEEE","IEEE Conferences"
"The Challenges of Synthesizing Hardware from C-Like Languages","S. A. Edwards","Columbia University","IEEE Design & Test of Computers","","2006","23","5","375","386","This article presents one side of an ongoing debate on the appropriateness of C-like languages as hardware description languages. The article examines various features of C and their mapping to hardware, and makes a cogent argument that vanilla C is not the right language for hardware description if synthesis is the goal. C-like languages are far more compelling for these tasks, and one in particular, SystemC, is now widely used, as are many ad hoc variants","0740-7475;1558-1918","","10.1109/MDT.2006.134","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1704728","Hardware synthesis;C-like languages;electronic system-level design","Circuit synthesis;Accidents;Switches;Hardware design languages;Optimizing compilers;Kernel;System software;Application software;Programming profession;System-level design","C language;hardware description languages;high level synthesis","C-like language;hardware synthesis;SystemC;hardware description language","","52","18","","","","","","IEEE","IEEE Journals & Magazines"
"A Flexible SoPC-based Fault Injection Environment","P. Vanhauwaert; R. Leveugle; P. Roche","NA; NA; NA","2006 IEEE Design and Diagnostics of Electronic Circuits and systems","","2006","","","190","195","Analyzing the behavior of ICs faced to soft errors is now mandatory, even for applications running at sea level, to prevent malfunctions in critical applications such as automotive. This paper introduces a novel prototyping-based fault injection environment that enables to perform several types of dependability analyses in a common optimized framework. The approach takes advantage of hardware speed and of software flexibility offered by embedded processors to achieve optimized trade-offs between experiment duration and processing complexity. The repartition of tasks between hardware and embedded software is defined with respect to the type of circuit to analyze","","1-4244-0185","10.1109/DDECS.2006.1649610","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1649610","","Circuit faults;Application software;Hardware;Embedded software;Sea level;Automotive engineering;Software prototyping;Prototypes;Performance analysis;Circuit analysis","embedded systems;fault tolerance;logic partitioning;microprocessor chips;programmable logic devices;system-on-chip","flexible SoPC-based fault injection environment;prototyping-based fault injection environment;dependability analyses;software flexibility;embedded processors;system-on-chip","","14","22","","","","","","IEEE","IEEE Conferences"
"An Application of Multicriteria Weighted Graph Similarity Method to Social Networks Analyzing","Z. Tarapata; R. Kasprzyk","NA; NA","2009 International Conference on Advances in Social Network Analysis and Mining","","2009","","","366","368","In the paper a concept of multicriteria weighted graphs similarity (MWGSP) method and its application to examine some properties of social networks is considered. The approach extends known approaches based on the graph similarity with two features: (1) the similarity is calculated as structural and non-structural (quantitative) in weighted graph, (2) choice of the most similar graph (subgraph) to graph (subgraph) representing examined objects is based on multicriteria decision. We test our similarity measures on email network for which the expected results are known, and on the terrorist net that prepared and executed September 11, 2001 attacks.","","978-0-7695-3689","10.1109/ASONAM.2009.33","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5231827","social networks;weighted graph similarity;multicriteria optimization;early warning system","Social network services;Terrorism;Chemical analysis;Paper technology;Cybernetics;Electronic mail;Testing;Computer errors;Histograms;Application software","graph theory;social aspects of automation;social networking (online)","multicriteria weighted graph similarity;social network analysis;multicriteria decision;similarity measures;email network","","3","7","","","","","","IEEE","IEEE Conferences"
"Fast complete memory consistency verification","Y. Chen; Yi Lv; W. Hu; T. Chen; Haihua Shen; Pengyu Wang; Hong Pan","Institute of Computing Technology, Chinese Academy of Sciences, Beijing 100190, China; Institute of Software, Chinese Academy of Sciences, Beijing 100190, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing 100190, China; University of Science and Technology of China, Hefei, Anhui 230027, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing 100190, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing 100190, China; Institute of Software, Chinese Academy of Sciences, Beijing 100190, China","2009 IEEE 15th International Symposium on High Performance Computer Architecture","","2009","","","381","392","The verification of an execution against memory consistency is known to be NP-hard. This paper proposes a novel fast memory consistency verification method by identifying a new natural partial order: time order. In multiprocessor systems with store atomicity, a time order restriction exists between two operations whose pending periods are disjoint: the former operation in time order must be observed by the latter operation. Based on the time order restriction, memory consistency verification is localized: for any operation, both inferring related orders and checking related cycles need to take into account only a bounded number of operations. Our method has been implemented in a memory consistency verification tool for CMP (chip multi processor), named LCHECK. The time complexity of the algorithm in LCHECK is O(C<sup>p</sup>p<sup>2</sup>n<sup>2</sup>) (where C is a constant, p is the number of processors and n is the number of operations) for soundly and completely checking, and O(p<sup>3</sup>n) for soundly but incompletely checking. LCHECK has been integrated into both pre and post silicon verification platforms of the Godson-3 microprocessor, and many bugs of memory consistency and cache coherence were found with the help of LCHECK.","1530-0897;2378-203X","978-1-4244-2932","10.1109/HPCA.2009.4798276","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4798276","memory consistency;cache coherence;verification;time order;pending period","Microarchitecture;Multiprocessing systems;Microprocessors;Observability;Costs;Silicon;Computer bugs;Algorithm design and analysis;Polynomials;Testing","computational complexity;memory architecture;microprocessor chips;optimisation;silicon","fast complete memory consistency verification;NP-hard;natural partial order;time order;multiprocessor systems;store atomicity;time order restriction;chip multi processor;LCHECK;time complexity;silicon verification platforms;Godson-3 microprocessor","","14","32","","","","","","IEEE","IEEE Conferences"
"An Agent-Based E-Learning Assessing and Instructing System","Z. Liu; Z. Wang; Z. Fang","Software College, Zhejiang University of Technology, Hangzhou, P.R. China, 310014. lzhi@zjut.edu.cn; Software College, Zhejiang University of Technology, Hangzhou, P.R. China, 310014. lzhi@zjut.edu.cn; Network Centre, Zhejiang University of Technology, Hangzhou, P.R. China, 310014. fzl@zjut.edu.cn","2006 10th International Conference on Computer Supported Cooperative Work in Design","","2006","","","1","6","The research work on e-learning has become very important field in the education. Many demands for instructing strategies in the adaptive learning have turn out. Base on these instructing strategies, the learners could reduce their blindness in the learning process. This paper proposes a method to build a Bayesian networks model in order to assess the learner's knowledge level and instruct the learner. This e-learning assessing and instructing system is designed and implemented based on multi-agent systems (ELAIS). In this system, the knowledge cognitive level, the learning priorities and weakness of a learner could be analyzed and assessed through the tracking information. Then the corresponding instruction will give to the learner in order to improve the learner's learning efficiency. The parameters assessment is achieved using EM algorithm and the assessment results in this system reveal the pretty accuracy to the real testing situation","","1-4244-0164-X1-4244-0165","10.1109/CSCWD.2006.253194","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4019230","E-Learning;Bayesian networks;multi-agent systems","Electronic learning;Bayesian methods;Multiagent systems;Collaborative work;Information analysis;Intelligent agent;Educational institutions;Blindness;System testing;Computer aided instruction","belief networks;computer aided instruction;multi-agent systems","Bayesian networks;e-learning assessing and instructing system;multi-agent systems;EM algorithm","","2","10","","","","","","IEEE","IEEE Conferences"
"Early Design Phase Power Performance Trade-Offs Using In-Situ Macro Models","C. Thangaraj; T. Chen","NA; NA","4th IEEE International Symposium on Electronic Design, Test and Applications (delta 2008)","","2008","","","539","544","Early phase space exploration with a focus on power performance tradeoffs has been shown to enable faster design convergence. Tightly coupled physical design considerations and system level models are needed to guarantee time-to- market. This paper describes a methodology for early design space exploration. A Pareto-front analysis using the proposed methodology was applied to an industrial design to perform design space exploration. The obtained low power design solution improved the designers best solution by additionally reducing power by 6% with 2% performance impact. The optimal trade-off solution improved designers best by additionally trading off 1.7% and 2.8% performance for power savings of 8.5% and 7.6% respectively. The absolute amount achieved here is not as important as knowing what the desired design options should be for each functional block in the design very early in the design phase.","","978-0-7695-3110","10.1109/DELTA.2008.42","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4459609","What-if analysis;Power-performance trade-off;pareto-front","Space exploration;Pareto analysis;Design optimization;Performance analysis;Electronic equipment testing;Application software;Energy consumption;Space technology;Coupling circuits;Power system modeling","Pareto analysis;space research","space exploration;phase power performance trade-offs;in-situ macro models;time-to-market;Pareto-front analysis","","1","18","","","","","","IEEE","IEEE Conferences"
"The NDIIPP preservation network: progress, problems, and promise","H. R. Tibbo; P. Leousis; L. E. Campbell","UNC Chapel Hill, Chapel Hill, NC; UNC Chapel Hill, Chapel Hill, NC; Library of Congress, Washington, DC","Proceedings of the 6th ACM/IEEE-CS Joint Conference on Digital Libraries (JCDL '06)","","2006","","","189","189","Summary form only given. The national digital information infrastructure and preservation program (NDIIPP), authorized by Congress in 2000 and led by the Library of Congress, is addressing a variety of critical issues that confront all who are working on long-term access to digital information: the nature of collecting in the digital age; the IP and data-privacy barriers that can stymie collection and access policies that serve the public interest; thorny technical and format problems that make digital data especially vulnerable to loss; curatorial practices that do not scale well in the digital realm; and above all, modeling and testing the organizational structures that are best positioned to ensure longevity of authentic digital data over time. NDIIPP makes multimillion-dollar investments in digital preservation research, testing preservation architectures, and building new models of collaborative collecting and preserving digital knowledge, reaching across a broad spectrum of communities to develop the key nodes of a national network of committed preserving institutions. This panel will outline several specific investments and describe how LC plans to continue building out a digital preservation network","","1-59593-354","10.1145/1141753.1141791","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4119119","Library of Congress;NDIIPP;data management;digital preservation","Software libraries;Investments;Testing;Buildings;Public policy;Intellectual property;Design optimization;Sorting;Human factors;Standardization","authorisation;data privacy;digital libraries;message authentication;public libraries","national digital information infrastructure preservation program;Library of Congress;organizational structure;multimillion-dollar investment;digital preservation network;IP-privacy;data-privacy;digital information access policy;digital data authentication","","","","","","","","","IEEE","IEEE Conferences"
"Wafer Reliability Evaluation and Monitoring for InGaAsP Devices","D. E. Verbitsky","Alefa-EEQRA, 6 Cactus Ct., Edison, NJ08820. Email: dverbitsky@verizon.net","2006 IEEE International Integrated Reliability Workshop Final Report","","2006","","","201","204","A simple procedure, evaluating wafer reliability by HALT before full regular processing, is justified and optimized. This procedure is aimed at early failure rate assessment, reliability adjustment, and yield enhancement per customer needs. A test plan concept, detailed design and realization are suggested. Test optimization with respect to customer requirements, field failures, and manufacturing losses are presented. Some regular flaws and recommendations are proposed. The procedure allows significant manufacturing and field savings along with increasing customer satisfaction. The procedure can be extended to manufacturing operation optimization and applied to related GaAs and Si based innovative technologies","1930-8841;2374-8036","1-4244-0296-41-4244-0297","10.1109/IRWS.2006.305246","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4098723","","Monitoring;Manufacturing;Failure analysis;Application software;Customer satisfaction;Life testing;Computer network reliability;Cable TV;Communication cables;Investments","gallium arsenide;III-V semiconductors;indium compounds;semiconductor device manufacture;semiconductor device reliability","wafer reliability evaluation;early failure rate assessment;reliability adjustment;yield enhancement;manufacturing operation optimization;InGaAsP","","6","13","","","","","","IEEE","IEEE Conferences"
"A Learning OCR System Using Short/Long-term Memory Approach and Hardware Implementation in FPGA","A. Ahmadi; M. A. Ritonga; M. A. Abedin; H. J. Mattausch; T. Koide","Research Center for Nanodevices and Systems, Hiroshima University, Higashi-Hiroshima, Japan, Email: ahmadi@sxsys.hiroshima-u.ac.jp; NA; NA; NA; NA","2006 IEEE International Conference on Evolutionary Computation","","2006","","","687","693","In this paper we propose a learning OCR system which is based on taking a short and long-term memory and a ranking mechanism which manages the transition of reference patterns between two memories. Also, an optimization algorithm is used to optimize the reference vectors magnitude as well as their distribution, continuously. The system was implemented in the FPGA platform and was tested with some real test data of Roman fonts and the classification results found acceptable. LSI architecture Comparing to other learning models like neural networks or A-means, the main advantage of the proposed algorithm is its simple design which makes it implementable in the hardware especially LSI architecture with no need to a large amount of resources.","1089-778X;1941-0026","0-7803-9487","10.1109/CEC.2006.1688378","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1688378","","Optical character recognition software;Field programmable gate arrays;Memory management;Neural networks;Neural network hardware;Associative memory;Control systems;System testing;Large scale integration;Mathematical model","document image processing;field programmable gate arrays;large scale integration;learning systems;neural nets;optical character recognition","learning OCR system;hardware implementation;FPGA;ranking mechanism;optimization algorithm;reference vectors magnitude;Roman fonts;neural networks;LSI architecture","","3","4","","","","","","IEEE","IEEE Conferences"
"Cache Miss Clustering for Banked Memory Systems","O. Ozturk; G. Chen; M. Kandemir; M. Karakoy","Computer Science and Engineering Department, Pennsylvania State University, University Park, PA 16802, USA. ozturk@cse.psu.edu; Computer Science and Engineering Department, Pennsylvania State University, University Park, PA 16802, USA. gchen@cse.psu.edu; Computer Science and Engineering Department, Pennsylvania State University, University Park, PA 16802, USA. kandemir@cse.psu.edu; Department of Computing, Imperial College, London SW7 2AZ, UK. M.Karakoy@imperial.ac.uk","2006 IEEE/ACM International Conference on Computer Aided Design","","2006","","","244","250","One of the previously-proposed techniques for reducing memory energy consumption is memory banking. The idea is to divide the memory space into multiple banks and place currently unused (idle) banks into a low-power operating mode. The prior studies - both hardware and software domain - in memory energy optimization via low-power modes do not take the data cache behavior explicitly into account. As a consequence, the energy savings achieved by these techniques can be unpredictable due to dynamic cache behavior at runtime. The main contribution of this paper is a compiler optimization, called the bank-aware cache miss clustering, that increases idle durations of memory banks, and as a result, enables better exploitation of available low-power capabilities supported by the memory system. This is because clustering cache misses helps to cluster cache hits as well, and this in turn increases bank idleness. We implemented our cache miss clustering approach within a compilation framework and tested it using seven array-intensive application codes. Our experiments show that cache miss clustering saves significant memory energy as a result of increased idle periods of memory banks","1092-3152;1558-2434","1-59593-389","10.1109/ICCAD.2006.320143","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4110181","","Hardware;Runtime;Optimizing compilers;Energy management;Permission;Computer science;Power engineering and energy;Energy consumption;Banking;Testing","cache storage;optimising compilers","banked memory system;memory energy consumption;compiler optimization;bank-aware cache miss clustering","","","19","","","","","","IEEE","IEEE Conferences"
"The TASM Language and the Hi-Five Framework: Specification, Validation, and Verification of Embedded Real-Time Systems","M. Ouimet; K. Lundqvist","NA; NA","14th Asia-Pacific Software Engineering Conference (APSEC'07)","","2007","","","567","567","Summary form only given, as follows. The complete presentation was not made available for publication as part of the conference proceedings. The Hi-Five framework is a holistic framework for the validation and verification of embedded real-time systems. The framework reuses the state of the art in formal verification and test case generation to provide an end-to-end solution to mitigate the typically high cost of validation and verification activities. The framework is based on a literate formal specification language, the Timed Abstract State Machine (TASM) language. The TASM language captures the three key aspects of embedded real-time system behavior, namely functional behavior, timing, and resource consumption. These aspects can be captured and analyzed using the TASM language and its associated toolset. Using the TASM language, the Hi-Five framework models systems at multiple levels of abstraction and provides traceability between related models. The framework provides an overarching approach to system engineering by leveraging the formal semantics of the TASM language to automate verification and test case generation. During the early phases of system engineering, incorporating nonfunctional properties in system models is an approximate activity at best. For example, before an implementation exists, it is challenging to specify behavior related to time and resource consumption. Nevertheless, gaining insight into the system designs, before the system is implemented, yields considerable benefits in terms of cost and time savings. For example, evaluating design properties, such as end-to-end latency and Quality of Service can help optimize designs or select between competing designs. The Hi-Five approach to resolving this apparent paradox is to use bi-directional traceability through levels of abstraction. The end result of the approach is an integrated development environment where the effect of changes can be efficiently managed and enforced through levels of abstraction, from requirements to implementation.","1530-1362;1530-1362","0-7695-3057","10.1109/ASPEC.2007.117","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4425910","","Real time systems;Time measurement;Software debugging;Computational modeling;Analytical models;Predictive models","","","","","","","","","","","IEEE","IEEE Conferences"
"Improving the Performance of Heuristic Searches with Judicious Initial Point Selection","S. A. Tahaee; A. H. Jahangir; H. Habibi-Masouleh","NA; NA; NA","2008 Fifth IEEE International Symposium on Embedded Computing","","2008","","","14","19","In this paper we claim that local optimization can produce proper start point for genetic search. We completely test this claim on partitioning problem and on the performance of genetic search in a real problem that is finding aggregation tree in the sensor networks. The presented method (named Tendency algorithm) increases the performance of heuristic searches, and can be used in parallel with other tuning methods. The paper justifies the logic behind tendency algorithm by measuring the ""entropy"" of solution (in regard to optimal solution), and by numerous empirical tests.","","978-0-7695-3348","10.1109/SEC.2008.65","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4690717","Hardware/Software Partitioning;Tabu Search;Simulated Annealing;Genetic Search;Sensor Networks;Aggregation Tree","Partitioning algorithms;Embedded computing;Logic testing;Hamming distance;Intelligent sensors;Genetic engineering;Entropy;Genetic algorithms;Costs;Acoustic sensors","entropy;optimisation;tree searching","heuristic searches;judicious initial point selection;local optimization;genetic search;partitioning problem;aggregation tree;sensor networks;Tendency algorithm;entropy","","5","14","","","","","","IEEE","IEEE Conferences"
"Automating Processor Customisation: Optimised Memory Access and Resource Sharing","R. Dimond; O. Mencer; W. Luk","Department of Computing, Imperial College, 180 Queens Gate, London. SW7 2RH, frgd00@doc.ic.ac.uk; NA; NA","Proceedings of the Design Automation & Test in Europe Conference","","2006","1","","1","6","We propose a novel methodology to generate application specific instruction processors (ASIPs) including custom instructions. Our implementation balances performance and area requirements by making custom instructions reusable across similar pieces of code. In addition to arithmetic and logic operations, table look-ups within custom instructions reduce costly accesses to global memory. We present synthesis and cycle-accurate simulation results for six embedded benchmarks running on customised processors. Reusable custom instructions achieve an average 319% speedup with only 5% additional area. The maximum speedup of 501% for the advanced encryption standard (AES) requires only 3.6% additional area","1530-1591;1558-1101","3-9810801-1","10.1109/DATE.2006.244087","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1656877","","Resource management;Hardware;Application specific processors;Computer aided instruction;Energy consumption;Application software;Educational institutions;Arithmetic;Logic;Cryptography","application specific integrated circuits;digital arithmetic;instruction sets;microprocessor chips;table lookup","application specific instruction processors;memory access;resource sharing;arithmetic operations;logic operations;table lookup;advanced encryption standard","","1","10","","","","","","IEEE","IEEE Conferences"
"Two Criteria for Model Selection in Multiclass Support Vector Machines","L. Wang; P. Xue; K. L. Chan","NA; NA; NA","IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)","","2008","38","6","1432","1448","Practical applications call for efficient model selection criteria for multiclass support vector machine (SVM) classification. To solve this problem, this paper develops two model selection criteria by <i>combining</i> or <i>redefining</i> the radius-margin bound used in binary SVMs. The combination is justified by linking the test error rate of a multiclass SVM with that of a set of binary SVMs. The redefinition, which is relatively heuristic, is inspired by the conceptual relationship between the radius-margin bound and the class separability measure. Hence, the two criteria are developed from the perspective of model selection rather than a generalization of the radius-margin bound for multiclass SVMs. As demonstrated by extensive experimental study, the minimization of these two criteria achieves good model selection on most data sets. Compared with the <i>k</i>-fold cross validation which is often regarded as a benchmark, these two criteria give rise to comparable performance with much less computational overhead, particularly when a large number of model parameters are to be optimized.","1083-4419;1941-0492","","10.1109/TSMCB.2008.927272","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4625980","Class separability measure;model selection;multiclass classification;multiclass support vector machines (SVMs);radius–margin bound;Class separability measure;model selection;multiclass classification;multiclass support vector machines (SVMs);radius–margin bound","Support vector machines;Support vector machine classification;Testing;Error analysis;Kernel;Diversity reception;Joining processes;Computational efficiency;Australia Council;Training data","minimisation;pattern classification;support vector machines","model selection criteria;multiclass support vector machine classification;radius-margin bound;binary SVM;k-fold cross validation;class separability measure","Algorithms;Artificial Intelligence;Computer Simulation;Models, Statistical;Pattern Recognition, Automated;Software;Software Validation","20","32","","","","","","IEEE","IEEE Journals & Magazines"
"A Feature-Oriented Requirements Tracing Method: A Study of Cost-benefit Analysis","S. Ahn; K. Chong","Soongsil University, Seoul, Korea; Soongsil University, Seoul, Korea","2006 International Conference on Hybrid Information Technology","","2006","2","","611","616","Establishing and maintaining traceability links places a big burden since complex systems have especially yield an enormous number of various artifacts although traceability links is useful for requirements change impact analysis, requirements conflict analysis, and requirements consistency checking. Hence, we propose a feature-oriented requirements tracing method including value consideration and intermediate catalysis. To achieve our goal in this paper, we present (1) a meta-model of feature-oriented requirements tracing, (2) a featureoriented requirement tracing process overview, and (3) cost-benefits analysis. The meta-model is a formalization of feature-oriented requirement tracing using UML notation. The feature-oriented requirement tracing process consists of requirements definition, feature modeling, feature prioritization, requirements linking, and traceability links evaluation. We also carry out cost-benefit analysis through a case study to demonstrate the feasibility of our approach.","","0-7695-2674","10.1109/ICHIT.2006.253670","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021275","","Cost benefit analysis;Software engineering;Joining processes;Unified modeling language;Software testing;Software reusability;Programming;Computer architecture;Software systems;Maintenance engineering","","","","2","14","","","","","","IEEE","IEEE Conferences"
"Surviving Insecure IT: Effective Patch Management","S. Liu; R. Kuhn; H. Rossman","US National Library of Medicine; US National Institute of Standards and Technology; Science Applications International Corporation","IT Professional","","2009","11","2","49","51","The amount of time to protect enterprise systems against potential vulnerability continues to shrink. Enterprises need an effective patch management mechanism to survive the insecure IT environment. Effective patch management is a systematic and repeatable patch distribution process which includes establishing timely and practical alerts, receiving notification of patches or discovering them, downloading patches and documentation, assessing and prioritizing vulnerabilities, performing testing, deploying patches, and auditing.","1520-9202;1941-045X","","10.1109/MITP.2009.38","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4804050","IT Professional;security;vulnerability;patch management;vulnerability alerts;vulnerability scan;vulnerability assessment","Application software;IEEE news;Information security;Mission critical systems;Databases;National security;Monitoring;Subscriptions;Documentation;Software development management","security of data;software maintenance","enterprise systems;patch management mechanism;insecure IT environment;patch distribution process","","3","","","","","","","IEEE","IEEE Journals & Magazines"
"Using an Evolutionary Algorithm for the Tuning of a Chess Evaluation Function Based on a Dynamic Boundary Strategy","H. Nasreddine; H. S. Poh; G. Kendall","Faculty of Engineering and Computer Science, The University of Nottingham, Malaysia Campus, Semenyih, Malaysia. kcznh@nottingham.edu.my; Faculty of Engineering and Computer Science, The University of Nottingham, Malaysia Campus, Semenyih, Malaysia. hendrasutanto@gmail.com; School of Computer Science, The University of Nottingham, Jubilee Campus, Nottingham, UK. gxk@cs.nott.ac.uk","2006 IEEE Conference on Cybernetics and Intelligent Systems","","2006","","","1","6","One of the effective ways of optimising the evaluation function of a chess game is by tuning each of its parameters. Evolutionary algorithms have become an appropriate choice as optimisers. In the past works related to this domain, the values of the parameters are within a fixed boundary which means that no matter how the recombination and mutation operators are applied, the value of a given parameter cannot go beyond its corresponding interval. In this paper, we propose a new strategy called ""dynamic boundary strategy"" where the boundaries of the interval of each parameter are dynamic. A real-coded evolutionary algorithm that incorporates this strategy and uses the polynomial mutation as its main exploitative tool is implemented. The effectiveness of the proposed strategy is tested by competing our program against a popular commercial chess software. Our chess program has shown an autonomous improvement in performance after learning for hundreds of generations","2326-8123;2326-8239","1-4244-0022-81-4244-0023","10.1109/ICCIS.2006.252366","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4017925","evaluation function;evolutionary algorithm;chess program","Evolutionary computation;Genetic mutations;Computer science;Polynomials;Software testing;Biological cells;Design for experiments;Engines;Computational intelligence;Computational Intelligence Society","computer games;evolutionary computation;games of skill;optimisation","chess evaluation function tuning;dynamic boundary strategy;evaluation function optimization;real-coded evolutionary algorithm;polynomial mutation","","5","11","","","","","","IEEE","IEEE Conferences"
"Model based failure detection using test signals from linearizations: A case study","S. L. Campbell; K. J. Drake; I. Andjelkovic; K. Sweetingham; D. Choe","Department of Mathematics, North Carolina State University, Raleigh, 27695-8205, USA; Advanced Machinery Systems, Code 9811 NSWCCD Philadelphia, PA 19153, USA; Department of Mathematics, North Carolina State University, Raleigh, 27695-8205, USA; Department of Mathematics, North Carolina State University, Raleigh, 27695-8205, USA; Department of Mathematics, North Carolina State University, Raleigh, 27695-8205, USA","2006 IEEE Conference on Computer Aided Control System Design, 2006 IEEE International Conference on Control Applications, 2006 IEEE International Symposium on Intelligent Control","","2006","","","2659","2664","An active approach for failure detection based on the use of minimal auxiliary signals has recently been introduced for linear systems with both additive and model uncertainty. This paper discusses the extension of this approach to nonlinear systems. A nonlinear case study is performed to carefully examine this approach. Evaluation of the performance of the new test signal on the nonlinear system is discussed and an evaluation algorithm presented. The extension and its evaluation requires integration of a variety of control, simulation, and optimization software. Both the application to failure detection and the software and numerical issues involved are discussed.","2165-3011;2165-302X","0-7803-9797","10.1109/CACSD-CCA-ISIC.2006.4777058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4777058","","Signal detection;System testing;Linear systems;Nonlinear systems;Mathematics;Uncertainty;Detectors;Control systems;Control system synthesis;Application software","","","","1","9","","","","","","IEEE","IEEE Conferences"
"Evolving Best-Response Strategies for Market-Driven Agents Using Aggregative Fitness GA","K. M. Sim; B. An","NA; NA","IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)","","2009","39","3","284","298","There are very few existing works that adopt genetic algorithms (GAs) for evolving the most successful strategies for different negotiation situations. Furthermore, these works did not explicitly model the influence of market dynamics. The contribution of this work is developing bargaining agents that can both: 1) react to different market situations by adjusting their amounts of concessions and 2) evolve their best-response strategies for different market situations and constraints using an aggregative fitness GA (AFGA). While many existing negotiation agents only optimize utilities, the AFGA in this work is used to evolve best-response strategies of negotiation agents that optimize their utilities, success rates, and negotiation speed in different market situations. Given different constraints and preferences of agents in optimizing utilities, success rates, and negotiation speed, different best-response strategies can be evolved using the AFGA. A testbed consisting of both: 1) market-driven agents (MDAs)-negotiation agents that make adjustable amounts of concessions taking into account market rivalry, outside options, and time preferences and 2) GA-MDAs-MDAs augmented with an AFGA, was implemented. Empirical results show that GA-MDAs achieved higher utilities, higher success rates, and faster negotiation speed than MDAs in a wide variety of market situations.","1094-6977;1558-2442","","10.1109/TSMCC.2009.2014880","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4810152","Automated negotiation;bargaining;genetic algorithms (GAs);negotiation agents","Genetic algorithms;Grid computing;Resource management;Constraint optimization;Testing;Software agents;Computer science","genetic algorithms;marketing;software agents","market-driven agents;aggregative fitness GA;genetic algorithm;market dynamics;bargaining agents;negotiation agents","","6","43","","","","","","IEEE","IEEE Journals & Magazines"
"A Novel Web Service Selection Based on Dynamic Quality Evaluation","Z. Liu; T. Liu; L. Cai; G. Yang","NA; NA; NA; NA","2009 International Conference on Computational Intelligence and Software Engineering","","2009","","","1","6","This paper proposes a novel method of Web service selection for the better quality based on environment of dynamic evaluating system. The evaluating system considered the end-user for the quality of service. This QoS evaluation framework which carries out a dynamic method of acquiring the quality data in the Web service composition based on the end-user feeling of web services. The procedure of processing the quality of service is extended quality of single service to multiple factor. Considering the end-user's perception, the distribution of dynamic evaluation can greatly improve the quality of Web service and its composition, which is not just adjusted according to end-user's requirements. A novel service selected algorithm with QoS is also presented in the paper, which chooses the different composition execute path to get optimized quality. Some experiments are illustrated to demonstrate effective.","","978-1-4244-4507","10.1109/CISE.2009.5364110","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5364110","","Web services;Quality of service;Simple object access protocol;Web and internet services;Computer science;Laboratories;Software quality;Software testing;Costs;Time factors","personal computing;quality of service;Web services","Web service selection based;dynamic quality evaluation;dynamic evaluating system;end user quality service;QoS evaluation framework;acquiring quality data;Web service composition;end user feeling web services;service extended quality;single service multiple factor;end users perception;end users requirements;composition execute path","","","24","","","","","","IEEE","IEEE Conferences"
"Information Security, Part of the IT-Integration Process","M. Mocanu; D. Mihaila","University "Politehnica" from Bucharest. mocanu@cs.pub.ro; University "Politehnica" from Bucharest. dan.mihaila@cs.pub.ro","2006 IEEE International Conference on Automation, Quality and Testing, Robotics","","2006","1","","375","380","IT-security and IT-integration are two important themes that preoccupy the companies. Both should give the company the possibility to gain more profit from the information processing. The present procedures in the integration process and in the design of ISMS have common parts and influence each other. These arguments for including the security issues, as an important part of the IT integration","","1-4244-0360-X1-4244-0361","10.1109/AQTR.2006.254564","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4022886","","Information security;Computer security;Data security;Protection;Companies;Computer viruses;Software systems;Biometrics;Information processing;Optimization methods","security of data","information security;IT-integration process;IT-security;information processing;ISMS","","","14","","","","","","IEEE","IEEE Conferences"
"A cognitive radio approach to realize coexistence optimized wireless automation systems","K. Ahmad; U. Meier; H. Kwasnicka; A. Pape; B. Griese","Institute Industrial IT, OWL University of Applied Sciences, 32657 Lemgo, Germany; Institute Industrial IT, OWL University of Applied Sciences, 32657 Lemgo, Germany; Wroc¿aw University of Technology, 27 Wybrze e Wyspia skiego, 50-370, Poland; Phoenix Contact Electronics, 31812 Bad, Pyrmont, Germany; Weidmueller Interface, 32758 Detmold, Germany","2009 IEEE Conference on Emerging Technologies & Factory Automation","","2009","","","1","8","The need for multiple radio systems in overlapping regions of a factory floor introduces a coexistence problem. The current research challenge is to design and realize radio systems that should be able to achieve a desired quality of service (QoS) in harsh, time-varying, coexisting industrial environments. As a solution cognitive radio (CR) built on top of a reconfigurable platform like software defined radio (SDR) can provide the required system properties. We provide a literature survey about existing coexistence problems and solutions and implement a CR testbed for experimental investigations. It can exploit three dimensions of the hyperspace, which are frequency, time, and transmission power. This CR testbed is investigated in the coexistence of typical radio systems. The experimental results show satisfactory robustness against any of these interferers.","1946-0740;1946-0759","978-1-4244-2727-7978-1-4244-2728","10.1109/ETFA.2009.5347040","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5347040","","Cognitive radio;Chromium;Quality of service;Manufacturing automation;Production facilities;Time varying systems;Software radio;Frequency;System testing;Robustness","cognitive radio;computerised instrumentation;factory automation;quality of service","cognitive radio;coexistence optimized wireless automation systems;multiple radio systems;quality of service;QoS;industrial environment","","6","30","","","","","","IEEE","IEEE Conferences"
"HPMR: Prefetching and pre-shuffling in shared MapReduce computation environment","S. Seo; I. Jang; K. Woo; I. Kim; J. Kim; S. Maeng","Computer Science Department, Korea Advanced Institute of Science and Technology (KAIST), South Korea; Computer Science Department, Korea Advanced Institute of Science and Technology (KAIST), South Korea; Search Engineerin, Yahoo! Inc, Seoul South Korea; Advanced Software Center, Samsung Advanced Institute, of Technology, South Korea; School of Info. and Comm., Sungkyunkwan University, South Korea; Computer Science Department, Korea Advanced Institute of Science and Technology (KAIST), South Korea","2009 IEEE International Conference on Cluster Computing and Workshops","","2009","","","1","8","MapReduce is a programming model that supports distributed and parallel processing for large-scale data-intensive applications such as machine learning, data mining, and scientific simulation. Hadoop is an open-source implementation of the MapReduce programming model. Hadoop is used by many companies including Yahoo!, Amazon, and Facebook to perform various data mining on large-scale data sets such as user search logs and visit logs. In these cases, it is very common to share the same computing resources by multiple users due to practical considerations about cost, system utilization, and manageability. However, Hadoop assumes that all cluster nodes are dedicated to a single user, failing to guarantee high performance in the shared MapReduce computation environment. In this paper, we propose two optimization schemes, prefetching and pre-shuffling, which improve the overall performance under the shared environment while retaining compatibility with the native Hadoop. The proposed schemes are implemented in the native Hadoop-0.18.3 as a plug-in component called HPMR (high performance MapReduce engine). Our evaluation on the Yahoo!Grid platform with three different workloads and seven types of test sets from Yahoo! shows that HPMR reduces the execution time by up to 73%.","1552-5244;2168-9253","978-1-4244-5011-4978-1-4244-5012","10.1109/CLUSTR.2009.5289171","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5289171","","Prefetching;Large-scale systems;Data mining;Parallel programming;Parallel processing;Machine learning;Computational modeling;Open source software;Facebook;Costs","data mining;distributed processing;optimisation;storage management","MapReduce programming model;distributed processing;parallel processing;Hadoop;data mining;optimization scheme;prefetching scheme;preshuffling scheme;high performance MapReduce engine","","55","19","","","","","","IEEE","IEEE Conferences"
"A dynamic scheduler for balancing HPC applications","C. Boneti; R. Gioiosa; F. J. Cazorla; M. Valero","Universitat Politecnica de Catalunya, Spain; Barcelona Supercomputing Center, Spain; Barcelona Supercomputing Center, Spain; Barcelona Supercomputing Center, Spain","SC '08: Proceedings of the 2008 ACM/IEEE Conference on Supercomputing","","2008","","","1","12","Load imbalance cause significant performance degradation in High Performance Computing applications. In our previous work we showed that load imbalance can be alleviated by modern MT processors that provide mechanisms for controlling the allocation of processors internal resources. In that work, we applied static, hand-tuned resource allocations to balance HPC applications, providing improvements for benchmarks and real applications. In this paper we propose a dynamic process scheduler for the Linux kernel that automatically and transparently balances HPC applications according to their behavior. We tested our new scheduler on an IBM POWER5 machine, which provides a software-controlled prioritization mechanism that allows us to bias the processor resource allocation. Our experiments show that the scheduler reduces the imbalance of HPC applications, achieving results similar to the ones obtained by hand-tuning the applications (up to 16%). Moreover, our solution reduces the application's execution time combining effect of load balance and high responsive scheduling.","2167-4329;2167-4337","978-1-4244-2834-2978-1-4244-2835","10.1109/SC.2008.5217785","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5217785","","Dynamic scheduling;Processor scheduling;Application software;Resource management;Degradation;High performance computing;Automatic control;Benchmark testing;Linux;Kernel","dynamic scheduling;Linux;operating system kernels;processor scheduling;resource allocation","dynamic scheduler;HPC applications;load imbalance;performance degradation;high performance computing applications;MT processors;hand-tuned resource allocations;dynamic process scheduler;Linux kernel;IBM POWER5 machine;software-controlled prioritization mechanism;processor resource allocation","","20","30","","","","","","IEEE","IEEE Conferences"
"A Low Cost Tile-based 3D Graphics Full Pipeline with Real-time Performance Monitoring Support for OpenGL ES in Consumer Electronics","R. Gu; T. Yeh; W. Hunag; T. Huang; C. Tsai; C. Lee; M. Chiang; S. Hsiao; Y. Chang; I. Huang","NA; NA; NA; NA; NA; NA; NA; NA; NA; NA","2007 IEEE International Symposium on Consumer Electronics","","2007","","","1","6","This paper presents a 3D graphics engine which is specifically designed to minimize the hardware cost while providing sufficient computing capability for consumer electronics with small to medium screen sizes (up to 800times600) such as digital television. The presented 3D engine consists of a fixed full 3D graphics pipeline for both geometry and rendering operation. This engine provides a standard AHB interface that makes it easily to be integrated into an AMBA-based SoC. The development of the 3D engine has gone through a rigorous design process: starting from system modeling (using System-C), RTL implementation, hardware/software co-simulation and FPGA verification to test chip fabrication. This 3D engine provides 3.3 M vertices/s and 278 Mpixels/s in maximum performance at 139 MHz using 0.18 silicon technology with 987 K gates that is sufficient for most applications for digital television. At the same time, a complete OpenGL-ES 1.1 API, windowing system, Linux operating system, device driver and a 3D performance monitoring tool have been developed for our 3D engine. This performance monitoring tool provides run-time performance information include frame rate, triangle rate, pixel rate, involved OpenGL function list, function counts, memory utilization and etc. Moreover, a built-in real-time AHB bus tracer is also provided to monitor the bus activities of the 3D engine and other components on the system bus. The bus tracer captures on-chip bus signals at ether cycle accurate or transaction levels and applies real-time compression to both levels of signals. With the performance monitoring tool and the bus tracer, the 3D application developer can easily analyze the communication of the components and fine tune the 3D application to optimize the entire SoC system performance and to satisfy performance/cost constrains on consumer electronics. Both of the hardware and software have been carefully verified and demonstrated on FPGA using ARM versatile SoC develop board.","0747-668X;2159-1423","978-1-4244-1109-2978-1-4244-1110","10.1109/ISCE.2007.4382225","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4382225","3D graphics pipeline;OpenGL ES;geometry engine;rendering engine;3D graphics performance monitoring","Costs;Graphics;Pipelines;Monitoring;Consumer electronics;Engines;Hardware;Application software;Digital TV;Field programmable gate arrays","application program interfaces;consumer electronics;coprocessors;field programmable gate arrays;Linux;public domain software;rendering (computer graphics);system-on-chip","3D graphics full pipeline;performance monitoring;consumer electronics;SoC;System-C;hardware-software cosimulation;FPGA verification;test chip fabrication;OpenGL- ES 1.1;API;windowing system;Linux operating system;ARM versatile board;rendering engine","","9","18","","","","","","IEEE","IEEE Conferences"
"A genetic programming approach to solve scheduling problems with parallel simulation","A. Beham; S. Winkler; S. Wagner; M. Affenzeller","Research Center Hagenberg, Upper Austrian University of Applied Sciences, Campus Hagenberg Softwarepark 11, A-4232, Austria; Research Center Hagenberg, Upper Austrian University of Applied Sciences, Campus Hagenberg Softwarepark 11, A-4232, Austria; Department of Software Engineering, Upper Austrian University of Applied Sciences, Campus Hagenberg Softwarepark 11, A-4232, Austria; Department of Software Engineering, Upper Austrian University of Applied Sciences, Campus Hagenberg Softwarepark 11, A-4232, Austria","2008 IEEE International Symposium on Parallel and Distributed Processing","","2008","","","1","5","Scheduling and dispatching are two ways of solving production planning problems. In this work, based on preceding works, it is explained how these two approaches can be combined by the means of an automated rule generation procedure and simulation. Genetic programming is applied as the creator and optimizer of the rules. A simulator is used for the fitness evaluation and distributed over a number of machines. Some example results suggest that the approach could be successfully applied in the real world as the results are more than human competitive.","1530-2075","978-1-4244-1693-6978-1-4244-1694","10.1109/IPDPS.2008.4536379","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4536379","","Genetic programming;Optimal scheduling;Production planning;Dispatching;Software engineering;Humans;Cost function;Assembly;Testing","dispatching;genetic algorithms;production planning;scheduling","genetic programming;scheduling problem;parallel simulation;dispatching;production planning;fitness evaluation","","9","9","","","","","","IEEE","IEEE Conferences"
"A genetic programming approach to solve scheduling problems with parallel simulation","A. Beham; S. Winkler; S. Wagner; M. Affenzeller","Research Center Hagenberg, Upper Austrian University of Applied Sciences, Campus Hagenberg, Softwarepark 11, A-4232, Austria; Research Center Hagenberg, Upper Austrian University of Applied Sciences, Campus Hagenberg, Softwarepark 11, A-4232, Austria; Department of Software Engineering, Upper Austrian University of Applied Sciences, Campus Hagenberg, Softwarepark 11, A-4232 Hagenberg, Austria; Department of Software Engineering, Upper Austrian University of Applied Sciences, Campus Hagenberg, Softwarepark 11, A-4232 Hagenberg, Austria","2008 IEEE International Symposium on Parallel and Distributed Processing","","2008","","","1","5","Scheduling and dispatching are two ways of solving production planning problems. In this work, based on preceding works, it is explained how these two approaches can be combined by the means of an automated rule generation procedure and simulation. Genetic programming is applied as the creator and optimizer of the rules. A simulator is used for the fitness evaluation and distributed over a number of machines. Some example results suggest that the approach could be successfully applied in the real world as the results are more than human competitive.","1530-2075","978-1-4244-1693-6978-1-4244-1694","10.1109/IPDPS.2008.4536362","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4536362","","Genetic programming;Optimal scheduling;Production planning;Dispatching;Software engineering;Humans;Cost function;Assembly;Testing","","","","1","9","","","","","","IEEE","IEEE Conferences"
"Fuzzy Theory and AHP Based Manufacturing Execution Systems (MES) Vendor Service Quality Evaluation Method Study","L. Chao; Li Qing","Department of Automation, Tsinghua University, Beijing, China. Telephone: 13811645065; e-mail: liang-c04@mails.tsinghua.edu.cn; Associate Professor, Department of Automation, Tsinghua University, Beijing, China. e-mail: liqing@tsinghua.edu.cn","2006 IEEE International Conference on Service Operations and Logistics, and Informatics","","2006","","","764","769","The service quality of manufacturing execution systems (MES) vendors gains more attentions than ever before, since more manufacturers plan to purchase MES for the improvement of plant operation and financial performance, and more software companies begin to supply the software. The paper supplies both an evaluation framework and a practical approach to evaluate the service quality of MES vendors. From the process viewpoint, the evaluation framework includes not only the after-sale supports, but also the MES software itself. The evaluation approach involves fuzzy theory and AHP, and ranks the alternatives based on the concept of positive/negative ideal alternative. Decision-maker's confidence level and preference on the fuzzy assessment results from respondents are incorporated in the approach. Finally, we validate the approach through a test","","1-4244-0318-91-4244-0317","10.1109/SOLI.2006.329086","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4125678","Evaluation;Service Quality;Manufacturing Execution Systems (MES);Fuzzy Theory;AHP","Fuzzy systems;Software quality;Software performance;Chaos;Information systems;Manufacturing automation;Electronic mail;Performance gain;Testing;Computer industry","decision making;fuzzy set theory;manufacturing data processing;manufacturing systems","fuzzy theory;AHP based manufacturing execution systems;vendor service quality evaluation method study;plant operation;financial performance;software companies","","1","23","","","","","","IEEE","IEEE Conferences"
"Optimized Memory Assignment for DSPs","G. Grewal; S. Coros; D. Banerji; A. Morton; M. Ventresca","Department of Computing and Information Science, University of Guelph, Guelph, ON, Canada, N1G 2W1; NA; NA; NA; NA","2006 IEEE International Conference on Evolutionary Computation","","2006","","","64","72","To increase memory bandwidth, many programmable Digital Signal Processors (DSPs) employ two on-chip data memories. This architectural feature supports higher memory bandwidth by allowing multiple data memory accesses to occur in parallel. Exploiting dual memory banks, however, is a challenging problem for compilers. This, in part, is due to the instruction-level parallelism, small numbers of registers, and highly specialized register capabilities of most DSPs. In this paper, we present a new methodology based on a genetic algorithm for assigning data to dual-bank memories. Our approach is global, and integrates several important issues in memory assignment within a single model. Special effort is made to identify those data objects that could potentially benefit from an assignment to a specific memory, or perhaps duplication in both memories. Our computational results show that the GA is able to achieve a 54% reduction in the number of memory cycles and a reduction in the range of 7% to 42% in the total number of cycles when tested with well-known DSP kernels and applications.","1089-778X;1941-0026","0-7803-9487","10.1109/CEC.2006.1688291","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1688291","","Digital signal processing;Registers;Bandwidth;Parallel processing;Genetic algorithms;Information science;Digital signal processors;Testing;Kernel;Application software","digital signal processing chips;genetic algorithms;storage management chips","optimized memory assignment;digital signal processors;memory bandwidth;multiple data memory accesses;kernels","","","11","","","","","","IEEE","IEEE Conferences"
"Evolving Cellular Automata - Based Flexible Job Shop Scheduling","T. Witkowski; A. Antczak; S. Elzway; P. Antczak","NA; NA; NA; NA","2009 Fifth International Conference on Natural Computation","","2009","2","","8","13","The paper presents evolving cellular automata-based scheduling production. The models have been implemented and tested, and the examples have been illustrated. The software of this model, allows us to analyze the process of construction schedule for many variants reflecting a variety of combinations other factors. The results indicate that the proposed algorithm is an efficient approach to solve FJSP.","2157-9555;2157-9563","978-0-7695-3736","10.1109/ICNC.2009.794","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5365811","","Job shop scheduling;Processor scheduling;Job production systems;Genetic programming;Paper technology;Testing;Simulated annealing;Particle swarm optimization;Genetic algorithms;Automatic programming","cellular automata;job shop scheduling","cellular automata;flexible job shop scheduling;process analysis;software model","","","26","","","","","","IEEE","IEEE Conferences"
"A DSP Based H.264 Dec oder for a Multi-Format IP Set-Top Box","F. Pescador; M. J. Garrido; C. Sanz; E. Juarez; D. Samper","Universidad Politécnica de Madrid. Spain. pescador@sec.upm.es; Universidad Politécnica de Madrid. Spain. matias@sec.upm.es; Universidad Politécnica de Madrid. Spain. cesar@sec.upm.es; Universidad Politécnica de Madrid. Spain. ejuarez@sec.upm.es; Universidad Politécnica de Madrid. Spain. dsamper@sec.upm.es","2008 Digest of Technical Papers - International Conference on Consumer Electronics","","2008","","","1","2","In this paper, the implementation of a digital signal processor (DSP) based H.264 decoder for a multi-format set-top box is described. Baseline and Main profiles are supported. Using several software optimization techniques, the decoder has been fitted into a low-cost DSP. The decoder alone has been tested in simulation, achieving real-time performance with a 600 MHz system clock. Finally, it has been integrated in a multi-format IP set-top box using a commercial development board based on the DSP @ 600 MHz. Tests in a real environment have been performed using this board with good results.","2158-3994;2158-4001","978-1-4244-1458-1978-1-4244-1459","10.1109/ICCE.2008.4588025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4588025","","Digital signal processing;Decoding;Performance evaluation;US Department of Transportation;Digital signal processors;Computed tomography;Finite impulse response filter;Motion compensation;System testing;Real time systems","digital signal processing chips;IP networks;video coding","DSP;H.264 decoder;multiformat IP set-top box;digital signal processor;software optimization techniques;system clock","","","14","","","","","","IEEE","IEEE Conferences"
"Investigation of the usability of 2D- and 3D-FEM for a hybrid stepper motor","A. Oswald; H. G. Herzog","Technische Universität München, Fachgebiet Energeiwandlungstechnik, Munich, Germany; Technische Universität München, Fachgebiet Energeiwandlungstechnik, Munich, Germany","2009 IEEE International Electric Machines and Drives Conference","","2009","","","535","542","The optimization of hybrid stepper motors and similar special machine geometries can not be performed effectively due to a large number of elements which follow from the fact, that the dimension of the air gap is tiny against the whole geometry and that it is necessary to model a relative big part of the machine. The presented paper shows how to model hybrid stepper motors with finite element method more effectively. Based on a certain machine, the limits and problems using full 3D models are identified. Further attempts with 2D models follow. It becomes clear that, depending on the features of special software, these 2D models can only be used to analyze certain aspects like detent torque or holding torque around small stationary working points. The biggest problem is to rotate the rotor part against a stator with impressed current. Therefore a possibility has been searched to model a full working machine without the disadvantages of a full 3D model. A solution was found with a reduced 3D model. This is the quintessence of the paper. We also investigated which modeling depth has to be performed. It will become clear which boundary conditions have to be taken into account to reduce the model. Moreover the theoretical background which is necessary to do the reduction is given. It can be seen, that the reduced model can be used to get reliable results for an optimization. To conclude a comparison between the different models and results shows how useful the reduced model will be, if optimization or investigation of such a machine shall be done.","","978-1-4244-4251-5978-1-4244-4252","10.1109/IEMDC.2009.5075258","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5075258","","Usability;Torque;Geometry;Stators;Resonance;Testing;Rotors;Solid modeling;Application software;Vibrations","finite element analysis;machine theory;optimisation;stepping motors","hybrid stepper motor;2D-FEM model;3D-FEM model;finite element optimisation method;detent torque;holding torque;special machine geometry","","6","7","","","","","","IEEE","IEEE Conferences"
"Research on Grid-Enabled Parallel Strategies of Automatic Wavelet-based Registration of Remote-Sensing Images and Its Application in ChinaGrid","H. Zhou; Y. Tang; X. Yang; H. Liu","National University of Defense Technology, China; National University of Defense Technology, China; National University of Defense Technology, China; National University of Defense Technology, China","Fourth International Conference on Image and Graphics (ICIG 2007)","","2007","","","725","730","ChinaGrid is an important project aiming to providing high performance services in a grid computing environment.One of the applications offered by ChinaGrid, image registration is described. It is an important processing step in many applications of remote sensing area, which is requires intensive computing power. Firstly, the serial and existing parallel strategies of wavelet-based automatic image registration are overviewed. And then, some grid- enabled optimization for two of them, Hybrid-Parallel (HP) and Group-Parallel (GP), is given. Thirdly, all of these parallel strategies are reevaluated for grid environment theoretically and a performance testing of these parallel algorithms is done on a simulated grid platform. Experimental results show that our optimization is effective but the basic simple parallel strategy Parameter-Parallel (PP) is the best choice for grid. The parallel algorithms proposed here have been integrated into related service system of ChinaGrid.","","978-0-7695-2929-50-7695-2929","10.1109/ICIG.2007.23","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4297176","","Remote sensing;Image registration;Concurrent computing;Grid computing;Parallel algorithms;Image processing;Application software;High performance computing;Testing;Supercomputers","grid computing;image registration;parallel algorithms;remote sensing;wavelet transforms","grid-enabled parallel strategies;automatic wavelet-based registration;remote-sensing images;ChinaGrid;grid computing environment;wavelet-based automatic image registration;grid-enabled optimization;hybrid-parallel optimization;group-parallel optimization;parameter-parallel optimization","","","17","","","","","","IEEE","IEEE Conferences"
"Robust Nonlinear Model Predictive Control With Partial State Information","E. Scholte; M. E. Campbell","NA; NA","IEEE Transactions on Control Systems Technology","","2008","16","4","636","651","A nonlinear controller methodology is developed based on nonlinear model predictive control and set-membership estimation, resulting in a controller which is robust to model uncertainties and bounded noise sources. The model predictive control makes direct use of the estimated bounds of the states and model parameters through integration as constraints in the optimization problem. This results in lower computation times and the ability to satisfy constraints with partial state information. Stability of the integrated estimation and control algorithm is guaranteed through the use of a contractive terminal cost. Real time implementation is developed through a unique sequential quadratic programming solver, which develops an initial feasible solution very quickly, and continues to optimize as time allows. The estimation and control algorithm is demonstrated in software simulation, hardware simulation, and in uninhabited aircraft flight tests. A formation flight example consisting of two aircraft is presented, showing how the controller can maintain a formation within given constraints and avoid collisions while in the presence of noise. Real time flight control results for an autonomous airplane demonstrate the capability of the algorithms to control a nonlinear aircraft in real time around a suddenly appearing threat, thus avoiding collisions.","1063-6536;1558-0865;2374-0159","","10.1109/TCST.2007.912120","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4480899","Model predictive control (MPC);nonlinear systems;set-membership estimation (SMF);uncertainty modeling","Robust control;Predictive models;Predictive control;Aerospace control;Aircraft;Noise robustness;Aerospace simulation;Uncertainty;State estimation;Constraint optimization","aircraft control;collision avoidance;nonlinear control systems;optimisation;predictive control;robust control","robust nonlinear model predictive control;partial state information;set-membership estimation;optimization problem;sequential quadratic programming solver;formation flight;uninhabited aircraft flight tests;real time flight control;autonomous airplane;nonlinear aircraft;collision avoidance","","30","37","","","","","","IEEE","IEEE Journals & Magazines"
"Research on structure dynamic optimization design of NC grinder","H. Ye; X. Chen; R. Hu","Zhejiang Texitile &Fashion College, Ningbo 315211, China; Ningbo University of Technology, 315016, China; Ningbo University of Technology, 315016, China","2006 International Technology and Innovation Conference (ITIC 2006)","","2006","","","1693","1698","Dynamic optimization design is the key issue to improve machining precision of the machine tool. The accurate finite element model (FEM) of the original numerical control (NC) grinder including dynamic model of combined surface was established based on the results via compared actually dynamic test with theoretical analysis, The method of sensitivity analysis was applied to optimize the layout and parameters of the strengthening ribs of parts. The technique of modal frequency separation was applied to keep frequency separation of the main parts one another, and the structure of the main parts was optimized. The result of dynamic optimization design shows that the first order natural frequency of the new grinder raises 17% compared with the original one, while the relative vibration displacement between head of the grinder and the work piece reduces 10% correspondingly. The grinding chatter marks are eliminated and the machining accuracy is greatly raised. Dynamic optimization design is the key issue to improve machining precision of the machine tool<sup>[1]</sup>. At present the machine tool manufacture enterprise tend to use the methods with experience, analogy and static design while developing new machine tools. Simple mechanics calculation is the main method to optimize structure parts about intensity, rigidity and vibration stability. The advanced dynamic design technologies and dynamic optimization software are hardly employed<sup>[2,3]</sup>. So it is difficult to obtain light weight design and high precision. Especially the high speed machine tools are more difficult to enhance machining precision because of the all kinds of effects about vibration stability and hot deforming of spindle system. The paper used computer modeling and analyzing, and studied the dynamic optimization method of machine tools retrofit design. Firstly the finite element model(FEM) has been established. Dynamic test results have been used to modify theoretical FEM to enhance the modeling precision. Secondly the method of sensitivity analysis was applied to optimize the layout and parameters of the strengthening bars of components. Thirdly The technique of mode frequency separation was applied to keep frequency separation of the main components one another, the increasing of amplitude of machine tool structure vibration has been avoided, and the structure of the main components was optimized. Finally the destination of dynamic optimization of the entire machine tool is reached.","0537-9989","0-86341-696","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4752277","Dynamical design;FEM;sensitivity analysis;NC grinder","","","","","","","","","","","","IET","IET Conferences"
"Middleware Challenges for Distributed Next Generation Experimental Infrastructures","G. Kormentzas","University of the Aegean, Department of Information and Communication Systems Engineering, GR-83200, Karlovassi, Greece. gkorm@aegean.gr","2007 IEEE 18th International Symposium on Personal, Indoor and Mobile Radio Communications","","2007","","","1","5","Next Generation Networks (NGNs) are driven by the convergence of existing fixed, mobile, and cable networks towards interconnected all-IP networks. A key attribute of NGNs is the provision of seamless multimedia information and communication services across access networks of different technologies that are going to coexist and interoperate. This coexistence and interoperation creates a number of open research and standardization issues making the necessity of testing an important key enabler towards the transmission of existing wireless networks to the next generation ones. A de facto requirement for reliable testing is the formation of big infrastructures that provide advance testing facilities incorporating a number of hardware and software components, clusters, and testbeds. The paper attempts to address this challenge by discussing how events- based middleware distributed systems could be adopted for the formulation of large scale interconnected testbeds consisted in physical and virtual infrastructures. These interconnected testbeds could constitute testing environments for advanced networking approaches/algorithms/mechanisms concerning next generation networking platforms coexistence and optimization. The test case of a distributed testbed being under development in the context of the UNITE projects is also discussed to illustrate better how a distributed middleware logic serves the ultimate goal of new advanced experimental infrastructures.","2166-9570;2166-9589","978-1-4244-1143-6978-1-4244-1144","10.1109/PIMRC.2007.4394870","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4394870","","Middleware;Next generation networking;Software testing;Logic testing;Multimedia communication;Mobile communication;Standardization;Wireless networks;Hardware;Large-scale systems","IP networks;middleware;mobile radio;multimedia communication","next generation network;middleware;fixed network;mobile network;cable network;interconnected all-IP network;multimedia information service;multimedia communication service;wireless network;optimization","","","12","","","","","","IEEE","IEEE Conferences"
"Efficient Transition-Mode Boolean Characteristic Function with Its Application to Maximum Instantaneous Current Analysis","C. Hsieh; J. Lin; S. Chang","National Tsing Hua University, Taiwan; National Tsing Hua University, Taiwan; National Tsing Hua University, Taiwan","8th International Symposium on Quality Electronic Design (ISQED'07)","","2007","","","602","606","Circuit timing and power analysis are important in various aspects of circuit optimization. In order to solve the problem of finding input vectors to activate worst-case timing or power, a concept called timed characteristic function (TCF) was proposed, which can characterize logical and temporal conditions simultaneously. Traditionally TCF assumes the (one-vector) floating mode of operation. In this paper we extend TCF for the (two-vector) transition mode and propose an efficient way to construct transition-mode TCF. Also, we apply the TCF concept to the analysis of the maximum instantaneous current (MIC) of a circuit, where transition-mode TCF is used to formulate gates' switching conditions. The experimental results show that on average, the TCF technique can derive about 30% more accurate MIC results than random simulation does for combinational circuits","1948-3287;1948-3295","0-7695-2795","10.1109/ISQED.2007.70","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4149101","","Microwave integrated circuits;Automatic test pattern generation;Delay;Combinational circuits;Timing;Energy consumption;Application software;Computer science;Circuit optimization;Switching circuits","integrated circuit design;logic design;low-power electronics;sequential circuits","transition-mode Boolean characteristic function;maximum instantaneous current analysis;timed characteristic function","","3","11","","","","","","IEEE","IEEE Conferences"
"Public Domain P2P File-Sharing Networks Measurements and Modeling","J. Lloret; J. R. Diaz; J. M. Jimenez; F. Boronat","Polytechnic University of Valencia, Spain; NA; NA; NA","International Conference on Internet Surveillance and Protection (ICISP&#146;06)","","2006","","","10","10","Since P2P file-sharing networks became extremely popular between Internet users, many researchers have tried to model those P2P networks. This article deals with the modeling of public domain P2P file-sharing networks in terms of some parameters such as their number of users and the number of files inside them along the time. To do so, we have been measuring five public P2P file-sharing networks (Gnutella, FastTrack, Opennap, Edonkey and MP2P) tracking their evolution for three years. The results will be discussed and compared with measurements taken from other authors. Results obtained could be used to design new P2P networks, to test their performance or to optimize P2P network's parameters","","0-7695-2649","10.1109/ICISP.2006.28","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1690394","","Application software;Telecommunication traffic;IP networks;Protocols;Testing;Time measurement;Design optimization;Computer applications;Computer networks;Internet","Internet;peer-to-peer computing;public domain software","public domain networks;P2P file-sharing networks;Internet","","1","32","","","","","","IEEE","IEEE Conferences"
"Design of the intelligent heat meter based on MSP430FW425","Bingjiang Gong; Yan Liu","College of Information and Electrical Engineering, Hebei University of Engineering, Handan, China; College of Information and Electrical Engineering, Hebei University of Engineering, Handan, China","2008 IEEE Conference on Cybernetics and Intelligent Systems","","2008","","","648","651","This paper describes an optimized hardware and software design for the heat meter. It is equipped with the international popular low-power consumption single-chip microcontroller MSP430FW425, temperature sensor Pt1000, impeller flow meter and zero energy-consuming magnetic sensor. They improve the precision and sensitivity of heat measurement. It also concludes the RFID module which has realized the function of advance payment.","2326-8123;2326-8239","978-1-4244-1673-8978-1-4244-1674","10.1109/ICCIS.2008.4670814","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4670814","Heat meter;temperature measurement;flow measurement;MSP430FW425","Resistance heating;Heat engines;Circuits;Temperature sensors;Hardware;Testing;Microcontrollers;Magnetic sensors;Pipelines;Educational institutions","computerised instrumentation;flowmeters;heat measurement;integrated circuit design;low-power electronics;measurement systems;microcontrollers;radiofrequency integrated circuits;temperature measurement;temperature sensors","intelligent heat meter design;MSP430FW425 single-chip microcontroller;low-power consumption;optimized hardware design;optimized software design;Pt1000 temperature sensor;impeller flow meter;magnetic sensor;heat measurement;RFID module;temperature measurement","","1","7","","","","","","IEEE","IEEE Conferences"
"Interdisciplinary Contents Management Using 5W1H Interface for Metadata","K. Shimazu; T. Arisawa; I. Saito","Keio University, Japan; Keio University, Japan; Keio University, Japan","2006 IEEE/WIC/ACM International Conference on Web Intelligence (WI 2006 Main Conference Proceedings)(WI'06)","","2006","","","909","912","In this paper, we propose a metadata exchange interface for interdisciplinary contents-sharing. The interface is obtained by introducing the research results of a specific sociological study. We have developed a contents management (CM) system on intra-university network to test the feasibility of our interface. The main feature of our system is its function of focusing searches in Web documents. Our interface module converts tag-labels into items of 5W1H. By conducting an experimental use of our system, we confirmed that the module on it is very important for contents sharing across various disciplines","","0-7695-2747","10.1109/WI.2006.104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4061494","","Content management;Chapters;Electrochemical machining;Sociology;System testing;Software libraries;Web page design;Design optimization;Network servers;Web server","content management;educational administrative data processing;meta data;search engines;user interfaces","metadata exchange interface;content-sharing;sociological study;content management system;Web document search;5W1H interface;intra-university network","","4","3","","","","","","IEEE","IEEE Conferences"
"A Fast Quasi-Dense Matching Method","J. Zhao; J. Chai; G. Men","NA; NA; NA","2009 International Asia Symposium on Intelligent Interaction and Affective Computing","","2009","","","100","103","The paper presents a fast quasi-dense matching algorithm using an adaptive window. The algorithm starts from a set of sparse seed matches, then propagates to the neighboring pixels, finally the most points in the images are matched. This algorithm adopts an additional level of incremental calculation to enhance the speed of computing the sum of squared difference (SSD). The confidence coefficient is introduced and the search window is varied in inverse proportion to it, namely realizing the adaptive search window. The algorithm has been tested with three stereo images and the results demonstrate its accuracy and efficiency.","","978-1-4244-5406-8978-1-4244-5405-1978-0-7695-3910","10.1109/ASIA.2009.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5376060","sum of squared differences.incremental computation;confidence coefficient.adaptive window;quasi-dense matching","Pixel;Stereo vision;Educational institutions;Testing;Asia;Computer vision;Application software;Optimization methods;Statistical analysis;Measurement standards","image matching;stereo image processing","fast quasi-dense matching method;sum of squared difference;adaptive search window;image pixel;confidence coefficient;stereo images;sparse seed matches","","2","12","","","","","","IEEE","IEEE Conferences"
"A fast matching algorithm with an adaptive window based on quasi-dense method","Guo-Zun Men; Jia-Li Chai; Jie Zhao","College of Economics, Hebei University, Baoding 071002, China; The College of Electronic and Information Engineering, Hebei University, Baoding 071002, China; The College of Electronic and Information Engineering, Hebei University, Baoding 071002, China","2009 International Conference on Machine Learning and Cybernetics","","2009","3","","1641","1646","This paper presents a fast quasi-dense matching algorithm using an adaptive window. The algorithm starts from a set of sparse seed matches, and then propagates to the neighboring pixels; finally the most points in the images are matched. During computing the normalized cross correlation (NCC), an additional level of incremental calculation is proposed to achieve further speed-up. The confidence coefficient is introduced and the search window is varied in inverse proportion to it. The algorithm has been tested with stereo images and the results demonstrate its accuracy and efficiency. The algorithm also can be applied to a wide range of image pairs including those with large disparity or without rectification even if part of the images is less textured. In particular, with big images and a large disparity range our algorithm turns out to be significantly faster.","2160-133X;2160-1348","978-1-4244-3702-3978-1-4244-3703","10.1109/ICMLC.2009.5212272","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5212272","Normalized Cross Correlation;Incremental Computation;Confidence Coefficient;Adaptive Window and Quasi-Dense Matching","Stereo vision;Machine learning;Cybernetics;Educational institutions;Pixel;Machine learning algorithms;Testing;Computer vision;Application software;Optimization methods","correlation methods;image matching;image texture;search problems;statistical analysis","image matching algorithm;adaptive window;quasi-dense method;NCC;normalized cross correlation;search window;image texture;statistical method","","2","15","","","","","","IEEE","IEEE Conferences"
"Fuzzy Compensation Support Vector Classification for Direction of Arrival Estimation","X. He; Z. Liu; B. Jiang","NA; NA; NA","2009 5th International Conference on Wireless Communications, Networking and Mobile Computing","","2009","","","1","4","This paper presents a new direction of arrival (DOA) estimation method based on a multi-class implementation of fuzzy compensation support vector machine (SVM). The proposed method can achieve higher accurate estimates for DOA while avoiding the all-direction peak value searching technique used in other traditional DOA estimation methods. Meanwhile, compared with other SVM-based DOA estimation, like LS-SVM algorithm, this approach reduces the training and testing time and performs better with larger data, so is easier to implement in real-time applications. Computer simulation results show the effectiveness of the proposed method.","2161-9646;2161-9654","978-1-4244-3691-0978-1-4244-3693-4978-1-4244-3692","10.1109/WICOM.2009.5301964","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5301964","","Direction of arrival estimation;Support vector machines;Support vector machine classification;Space technology;Neural networks;Testing;Performance evaluation;Constraint optimization;Helium;Application software","direction-of-arrival estimation;fuzzy set theory;pattern classification;support vector machines","fuzzy compensation support vector classification;direction of arrival estimation;DOA estimation method","","","9","","","","","","IEEE","IEEE Conferences"
"Low-Power Warp Processor for Power Efficient High-Performance Embedded Systems","R. Lysecky","Department of Electrical and Computer Engineering, University of Arizona, rlysecky@ece.arizona.edu","2007 Design, Automation & Test in Europe Conference & Exhibition","","2007","","","1","6","Researchers previously proposed warp processors, a novel architecture capable of transparently optimizing an executing application by dynamically re-implementing critical kernels within the software as custom hardware circuits in an on-chip FPGA. However, the original warp processor design was primarily performance-driven and did not focus on power consumption, which is becoming an increasingly important design constraint. Focusing on power consumption, we present an alternative low-power warp processor design and methodology that can dynamically and transparently reduce power consumption of an executing application with no degradation in system performance, achieving an average reduction in power consumption of 74%. We further demonstrate the flexibility of this approach to provide dynamic control between high-performance and low-power consumption","1530-1591;1558-1101","978-3-9810801-2","10.1109/DATE.2007.364581","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4211786","Warp processing;low-power;hardware/software partitioning;dynamically adaptable systems;embedded systems","Embedded system;Energy consumption;Process design;Computer architecture;Application software;Kernel;Hardware;Circuits;Field programmable gate arrays;Design methodology","embedded systems;field programmable gate arrays;microprocessor chips","embedded systems;hardware circuits;on-chip FPGA;power consumption;low-power warp processor design;dynamic control","","5","23","","","","","","IEEE","IEEE Conferences"
"Determining Optimal Filters for Binarization of Degraded Characters in Color Using Genetic Algorithms","H. Kohmura; T. Wakahara","Hosei University, Japan; NA","18th International Conference on Pattern Recognition (ICPR'06)","","2006","3","","661","664","This paper proposes a new binarization technique of characters in color using genetic algorithms (GA) to search for an optimal sequence of filters through a filter bank. The filter bank contains simple image processing filters as applied to one of the RGB color planes and logical/arithmetic operations between two color planes. First, we classify images of degraded characters extracted from the public ICDAR 2003 robust OCR dataset into several groups according to degradation categories. Then, in the learning stage, by selecting training samples from each degradation category we apply GA to the combinatorial optimization problem of determining a filter sequence that maximizes the average fitness value calculated between the filtered training samples and their respective target images ideally binarized by humans. Finally, in the testing stage, we apply the optimal filter sequence to binarization of remaining test samples. Experimental results show the promising ability of the proposed method against a variety of image degradation causes","1051-4651","0-7695-2521","10.1109/ICPR.2006.446","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1699612","","Degradation;Genetic algorithms;Filter bank;Color;Testing;Image processing;Arithmetic;Robustness;Optical character recognition software;Humans","combinatorial mathematics;filtering theory;genetic algorithms;image classification;image colour analysis","optimal filters;binarization;genetic algorithms;image classification;combinatorial optimization","","6","7","","","","","","IEEE","IEEE Conferences"
"Non-contact Explosion Dynamic Response Analyzing of Vessel Electric Station Structure","J. Jin; X. Wu","NA; NA","2009 International Conference on Engineering Computation","","2009","","","178","181","For evaluating the anti-impact ability of vessel electric station, the numerical simulation of air explosion to entire structure was done by fluid-structure coupling algorithm in LS-DYNA software. Non-contact explosive load was added omni directionally by sphere loading method. Based on simulating results, the weakness position was founded and the structure dynamic response curves were created. Furthermore the weakness of structure was improved. The numerical calculation results showed that: this method can simulate the shockwave transmitting procedure well and evaluate structure anti-impact ability efficiently. The calculating results can also provide input conditions for electric station additional equipments anti-impact calculating. The simulating study method has some reference value to anti-impact design of vessel equipments.","","978-0-7695-3655","10.1109/ICEC.2009.23","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5167120","structure optimize;anti-explosion evaluating;numerical simulation;sphere loading method;dynamic response","Explosions;Explosives;Marine vehicles;Testing;Equations;Numerical simulation;Weapons;Fluid dynamics;Mechanical engineering;Instruments","dynamic response;electric vehicles;explosions;impact (mechanical);numerical analysis;ships;shock waves;structural engineering computing","noncontact explosion dynamic response;vessel electric station structure;antiimpact ability;air explosion;fluid-structure coupling algorithm;LS- DYNA software;sphere loading method;shockwave","","","6","","","","","","IEEE","IEEE Conferences"
"Monitoring Serial Communications in Microcontroller Based Embedded Systems","M. Popa; A. S. Popa; V. Cretu; M. Micea","Computer and Software Engineering Department, Faculty of Automation and Computers, "Politehnica" University of Timisoara, ROMANIA; Department of Mechatronics, Faculty of Mechanical Engineering, "Politehnica" University of Timisoara, ROMANIA; Computer and Software Engineering Department, Faculty of Automation and Computers, "Politehnica" University of Timisoara, ROMANIA; Computer and Software Engineering Department, Faculty of Automation and Computers, "Politehnica" University of Timisoara, ROMANIA","2006 International Conference on Computer Engineering and Systems","","2006","","","56","61","More and more microcontrollers are embedded in a large area of products from industrial to domestic domains. A good example is the automobile, a modern one containing tens of microcontrollers. As their number increased the communication between them became necessary. The serial solution was preferred and a lot of serial buses and protocols were developed optimizing different parameters of the communication. Several examples are: RS232, LIN, SPI, CAN and so on. Monitoring serial communications is necessary in R&amp;D phase, e.g. for creating virtual transfer partners, and in testing and debugging phases. The paper describes a message based monitoring tool for the RS232 bus and monitoring tools for the LIN and SPI buses. Many microcontrollers contain the LIN and SPI buses and almost all of them include the RS232 bus. The created tools work in passive mode, monitoring the transfers and sending the data to a PC or in active mode (only for the LIN bus), interfering in the communication and sending headers, responses or injecting typical errors","","1-4244-0271-91-4244-0272","10.1109/ICCES.2006.320425","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4115485","","Monitoring;Microcontrollers;Embedded system;Protocols;Automobiles;Debugging;Intelligent sensors;System testing;Research and development;Automotive engineering","embedded systems;microcontrollers;peripheral interfaces","serial communication monitoring;microcontroller;embedded systems;serial buses;protocols;message based monitoring;RS232 bus","","3","12","","","","","","IEEE","IEEE Conferences"
"Using Smart Sampling to Discover Promising Regions and Increase the Efficiency of Differential Evolution","V. V. Melo; A. C. B. Delbem","NA; NA","2009 Ninth International Conference on Intelligent Systems Design and Applications","","2009","","","1394","1399","This paper presents a novel method to discover promising regions in a continuous search space. Using machine learning techniques, the algorithm named smart sampling was tested in hard known benchmark functions, and was able to find promising regions with solutions very close to the global optimum, significantly decreasing the number of evaluations needed by a metaheuristic to finally find this global optimum, when heuristically started inside a promising region. Results show favorable agreement with theories which state the importance of an adequate starting population. The results also present significant improvement in the efficiency of the tested metaheuristic, without adding any parameter, operator or strategy. Being a technique which can be used by any populational metaheuristic, the work presented here has profound implications for future studies of global optimization and may help solve considerably difficult optimization problems.","2164-7143;2164-7151","978-1-4244-4735-0978-0-7695-3872","10.1109/ISDA.2009.248","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5363936","Smart Sampling;Promising Regions;Differential Evolution;Global Numerical Optimization","Sampling methods;Clustering algorithms;Machine learning;Machine learning algorithms;Space exploration;Intelligent systems;Application software;Mathematics;Benchmark testing;Iterative algorithms","evolutionary computation;learning (artificial intelligence);sampling methods","smart sampling;differential evolution;promising regions;continuous search space;machine learning;populational metaheuristic;global optimization","","","18","","","","","","IEEE","IEEE Conferences"
"Ensuring numerical quality in grid computing","A. Frommer; M. Husken","NA; NA","2006 15th IEEE International Conference on High Performance Distributed Computing","","2006","","","335","336","We propose an approach which gives the user valuable information on the various platforms avail able in a grid in order to assess the numerical quality of an algorithm run on each of these platforms. In this manner, the user is provided with at least very strong hints whether a program performs reliably in a grid before actually executing it. Our approach extends IeeeCC754 by two ""grid-enabled"" modes: The first mode calculates a ""numerical checksum"" on a specific grid host and executes the job only if the check sum is identical to a locally generated one. The second mode provides the user with information on the reliability and IEEE 754-conformity of the underlying floating-point implementation of various platforms. In addition, it can help to find a set of compiler options to optimize the application's performance while retaining numerical stability","1082-8907","1-4244-0307","10.1109/HPDC.2006.1652171","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1652171","","Grid computing;Testing;Application software;Floating-point arithmetic;Hardware;Software libraries;Computer science;Scientific computing;Programming;Software standards","floating point arithmetic;grid computing;IEEE standards;numerical stability","numerical quality;grid computing;IeeeCC754;numerical checksum mode;numerical stability","","1","6","","","","","","IEEE","IEEE Conferences"
"Automated HDL Generation: Comparative Evaluation","Y. Yankova; K. Bertels; S. Vassiliadis; R. Meeuws; A. Virginia","Computer Engineering Laboratory, Delft University of Technologies, The Netherlands. Email: Y.D.Yankova@tudelft.nl; Computer Engineering Laboratory, Delft University of Technologies, The Netherlands. Email: K.L.M.Bertels@tudelft.nl; Computer Engineering Laboratory, Delft University of Technologies, The Netherlands. Email: S.Vassiliadis@tudelft.nl; Computer Engineering Laboratory, Delft University of Technologies, The Netherlands. Email: rmeeuws@ce.et.tudelft.nl; Computer Engineering Laboratory, Delft University of Technologies, The Netherlands. Email: avirginia@ce.et.tudelft.nl","2007 IEEE International Symposium on Circuits and Systems","","2007","","","2750","2753","Reconfigurable computing (RC) systems, coupling general purpose processor with reconfigurable components, offer a lot of advantages. Nevertheless, currently a designer needs both in-depth software and hardware design knowledge to develop applications for such platforms. The automated hardware generation addresses this problem. However, the success of such tools remains marginal. This paper discusses the reasons for the lack of success. It presents a quantitative and qualitative comparison of three hardware generators using the following criteria: quality of the hardware model, the supported HLL constructs, and the level of automation.","0271-4302;2158-1525","1-4244-0920-91-4244-0921","10.1109/ISCAS.2007.378622","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4253247","","Hardware design languages;Software design;Application software;Testing;Design optimization;Sparks;Design automation;Laboratories;Software tools;High level languages","electronic design automation;hardware description languages;high level languages;microprocessor chips","automated HDL generation;comparative evaluation;reconfigurable computing systems;general purpose processor;automated hardware generation","","11","12","","","","","","IEEE","IEEE Conferences"
"Fuzzy Control of Ionic Polymer-Metal Composites","H. Khadivi; B. S. Aghazadeh; C. Lucas","graduate student, Tehran University, Tehran, Iran. e-mail: h.haris@me.ut.ac.ir; graduate student, Tehran University, Tehran, Iran. e-mail: b.aghazadeh@me.ut.ac.ir; Department of Electrical and Computer Engineering, Tehran University, P. O. Box 14395-515, Tehran, Iran. e-mail: lucas@ipm.ir","2007 29th Annual International Conference of the IEEE Engineering in Medicine and Biology Society","","2007","","","4198","4201","Ionic polymer-metal composites (IPMCs) have advantages of softness and flexibility to be used in biomedical applications. In this paper a fuzzy logic controller (FLC) has been designed for achieving the goal of tracking. Also co- evolutionary-based genetic algorithms technique has been employed to optimize membership functions and fuzzy rules. The simulation results show that fuzzy controller has higher performance in comparison with other controllers.","1094-687X;1558-4615","978-1-4244-0787-3978-1-4244-0788","10.1109/IEMBS.2007.4353262","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4353262","","Fuzzy control;Polymers;Actuators;Fuzzy logic;Application software;Muscles;Nonlinear control systems;Sliding mode control;Control systems;Heart","actuators;biomedical materials;composite materials;control system synthesis;fuzzy control;genetic algorithms;polymers;prosthetics;tracking","ionic polymer-metal composites;biomedical applications;fuzzy logic controller design;evolutionary-based genetic algorithms technique;fuzzy rules;tracking performance","Algorithms;Animals;Composite Resins;Humans;Ions;Materials Testing;Metals;Models, Biological","2","13","","","","","","IEEE","IEEE Conferences"
"Evaluation of Factors Affecting the Efficacy of English Listening Comprehension Based on CALL","H. Zheng; L. Han; J. Guo","NA; NA; NA","2009 International Conference on Computational Intelligence and Software Engineering","","2009","","","1","4","This paper is intended to elaborate on the multimedia listening activity of the effect on the learners' listening comprehension. It first describes a course-support hypermedia language-learning environment based on computer-assisted language learning (CALL) system, and shows the design and implementation of the courseware in this environment. Further study was conducted to identify positive and negative factors influencing the effectiveness of CALL system on listening comprehension. The factors considered in this study were the teaching style, the time available for the students to study, help options and student's motivation. After a period of two-year research and test, the corresponding data analyses and results of this study confirmed the influence of the above factors on the effectiveness and usefulness of CALL system. In addition, it was concluded that no one factor alone, could optimize the effect of listening comprehension, and that a systems approach is indispensable in improving the efficacy of listening comprehension by CALL.","","978-1-4244-4507","10.1109/CISE.2009.5364806","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5364806","","Education;Educational institutions;Courseware;Natural languages;Data analysis;Data visualization;Electronic mail;System testing;Internet;Logic","courseware;hypermedia;multimedia computing;natural language processing","English listening comprehension;multimedia listening activity;course-support hypermedia language-learning environment;computer-assisted language learning system;courseware;teaching style;data analysis","","","6","","","","","","IEEE","IEEE Conferences"
"On Time-Aware Instrumentation of Programs","S. Fischmeister; P. Lam","NA; NA","2009 15th IEEE Real-Time and Embedded Technology and Applications Symposium","","2009","","","305","314","Software instrumentation is a key technique in many stages of the development process. It is of particular importance for debugging embedded systems. Instrumented programs produce data traces which enable the developer to locate the origins of misbehaviours in the system under test. However, producing data traces incurs runtime overhead in the form of additional computation resources for capturing and copying the data. The instrumentation may therefore interfere with the system's timing and perturb its behavior. In the worst case, this perturbation leads to new system behaviours that prevent the developer from locating the original misbehaviours. In this work, we propose an instrumentation technique for applications with temporal constraints, specifically targetting background/foreground systems. Our framework permits reasoning about space and time for software instrumentations. In particular, we propose a definition for trace reliability, which enables us to instrument real-time applications which aggressively push their time budgets. Using the framework, we present a method with low perturbation by optimizing the number of insertion points and trace buffer size for code size and time budgets. Finally, we apply the theory to a concrete case study and instrument the OpenEC firmware for the keyboard controller of the One Laptop Per Child project.","1545-3421","978-0-7695-3636","10.1109/RTAS.2009.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4840591","","Instruments;Application software;Debugging;Embedded system;System testing;Runtime;Timing;Interference constraints;Optimization methods;Concrete","embedded systems;program debugging","time-aware program instrumentation;software instrumentation;embedded system debugging;data traces;temporal constraints;insertion points;trace buffer size;code size;time budgets;OpenEC firmware;keyboard controller;One Laptop Per Child project","","5","23","","","","","","IEEE","IEEE Conferences"
"Model-based fault localization in large-scale computing systems","N. Maruyama; S. Matsuoka","Tokyo Institute of Technology, Japan; Tokyo Institute of Technology, Japan","2008 IEEE International Symposium on Parallel and Distributed Processing","","2008","","","1","12","We propose a new fault localization technique for software bugs in large-scale computing systems. Our technique always collects per-process function call traces of a target system, and derives a concise execution model that reflects its normal function calling behaviors using the traces. To find the cause of a failure, we compare the derived model with the traces collected when the system failed, and compute a suspect score that quantifies how likely a particular part of call traces explains the failure. The execution model consists of a call probability of each function in the system that we estimate using the normal traces. Functions with low probabilities in the model give high anomaly scores when called upon a failure. Frequently-called functions in the model also give high scores when not called. Finally, we report the function call sequences ranked with the suspect scores to the human analyst, narrowing further manual localization down to a small part of the overall system. We have applied our proposed method to fault localization of a known non-deterministic bug in a distributed parallel job manager. Experimental results on a three-site, 78-node distributed environment demonstrate that our method quickly locates an anomalous event that is highly correlated with the bug, indicating the effectiveness of our approach.","1530-2075","978-1-4244-1693-6978-1-4244-1694","10.1109/IPDPS.2008.4536310","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4536310","","Large-scale systems;Distributed computing;Fault detection;Humans;Scalability;Software architecture;Informatics;Computer bugs;Standards;Testing","fault location;parallel processing;program debugging;software fault tolerance","model-based fault localization;large-scale computing systems;software bugs;per-process function call traces;call probability;function call sequences;distributed parallel job manager","","6","20","","","","","","IEEE","IEEE Conferences"
"FEA-based Optimal Design of the Radial Magnetic Bearings in Magnetic Suspended Hard Disk Drive","G. Ding; Z. Zhou; Y. Hu; D. He","School of Electromechanical Engineering, Wuhan University of Technology, Wuhan, Hubei Province, 430070 China. dgp7910@163.com; School of Electromechanical Engineering, Wuhan University of Technology, Wuhan, Hubei Province, 430070 China. dgp7910@163.com; School of Electromechanical Engineering, Wuhan University of Technology, Wuhan, Hubei Province, 430070 China. dgp7910@163.com; School of Electromechanical Engineering, Wuhan University of Technology, Wuhan, Hubei Province, 430070 China. dgp7910@163.com","2006 2nd IEEE/ASME International Conference on Mechatronics and Embedded Systems and Applications","","2006","","","1","6","Magnetic suspended hard disk drive (MSHDD) is a novel concept of computer storage device in which conventional spindle bearings are substituted by magnetic bearings and other parts are kept unchanged to largely increase the rotating speed of the disks and actively control vibration of the disks. This paper presents the FEA-based optimization of a radial MB in a MSHDD test rig. The aim is to achieve maximum force in a limited space of the entire construction. The objective function is evaluated by static FEA-based 2D calculations. It includes the creation of the radial MB's FEA model, determination of the nonlinear solution of the magnetic scalar potential and the determination of force by Maxwell's stress tensor method. The optimization method and math model are presented with a detailed optimization flow. The optimization results and the geometry parameters' influence on the radial MB performance is concluded. Evaluated by magnetic field measurement data of the radial magnetic bearing, it is shown that FEA-based optimization achieved the maximum bearing force accurately and efficiently. The optimization has been performed with the FEA software-ANSYS EMAG","","0-7803-9721","10.1109/MESA.2006.296939","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4077766","Magnetic suspended hard disk drive (MSHDD);optimal design;magnetic bearings;finite element analysis (FEA)","Magnetic levitation;Hard disks;Optimization methods;Magnetic devices;Machine tool spindles;Vibration control;Testing;Tensile stress;Geometry;Magnetic field measurement","disc drives;electronic engineering computing;finite element analysis;hard discs;magnetic bearings;Maxwell equations;optimisation;vibration control","FEA-based optimal design;radial magnetic bearings;magnetic suspended hard disk drive;computer storage device;active vibration control;static FEA-based 2D calculations;Maxwell stress tensor method;ANSYS EMAG;finite element analysis;magnetic scalar potential","","3","6","","","","","","IEEE","IEEE Conferences"
"1265: An Approach for Spot Matching in 2-D Electrophoresis Gels","T. Srinark; C. Kambhamettu","Dept. of Computer Engineering, Faculty of Engineering, Kasetsart University, Bangkok 10900 Thailand. Thitiwan.S@ku.ac.th; Dept. of Comp. and Info. Sciences, University of Delaware, Newark, DE 19716 USA. chandra@cis.udel.edu","2006 International Conference on Image Processing","","2006","","","1165","1168","2-D electrophoresis is the study of expressions of proteins. The technique produces images that contain protein spots, One of the tasks in the analysis of these images is matching of protein spots in two corresponding images for differential expression study. In this paper, we propose an algorithm which integrates a hierarchical based and energy based methods. The hierarchical based method initially finds corresponding pairs of spots. We formulate a new matching energy, which consists of local spot structure similarity, image similarity, and a spatial constraint. The proposed energy is minimized to find corresponding pairs of spots by a greedy based optimization algorithm. We extensively tested our method with synthetic images and real 2-D gel images from different biological experiments.","1522-4880;2381-8549","1-4244-0480-01-4244-0481","10.1109/ICIP.2006.312764","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4106742","","Electrokinetics;Proteins;Software packages;Image analysis;Stochastic processes;Solid modeling;Interpolation;Robustness;Testing;Background noise","electrophoresis;gels;greedy algorithms;image matching;medical image processing;molecular biophysics;proteins","2-D electrophoresis gels;protein spots;image matching;hierarchical based method;energy based method;greedy based optimization algorithm","","","15","","","","","","IEEE","IEEE Conferences"
"Finding a Near-Maximum Independent Set of a Circle Graph by Using Genetic Algorithm with Conditional Genetic Operators","S. Wang; R. Wang; Z. Chen; K. Okazaki","NA; NA; NA; NA","2008 Fourth International Conference on Natural Computation","","2008","1","","597","600","The maximum independent set problem is of central importance combinatorial optimization problem. It has many practical applications in science and engineering. In this paper, we propose a genetic algorithm based approach to solve the problem. In the proposed approach, the genetic operators are performed basing on condition instead of probability. The proposed algorithm is tested on a large number of instances and the simulation results show that the proposed method is superior to its competitors.","2157-9555;2157-9563","978-0-7695-3304","10.1109/ICNC.2008.690","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4666915","Maximum independent set;Genetic algorithm;Crossover;Mutation;NP-complete","Genetic algorithms;Testing;RNA;Computer science;Application software;Random number generation;Codes;Geometry;Very large scale integration;NP-complete problem","genetic algorithms;graph theory;set theory","near-maximum independent set;circle graph;genetic algorithm;conditional genetic operators;central importance combinatorial optimization problem","","","14","","","","","","IEEE","IEEE Conferences"
"Optimising Web services performance with table driven XML","A. Ng","Dept. of Comput., Macquarie Univ., North Ryde, NSW, Australia","Australian Software Engineering Conference (ASWEC'06)","","2006","","","10 pp.","112","The performance of the SOAP protocol has often been regarded relatively poor and requiring undue amounts of processor time, storage and bandwidth due to its use of text-based, metadata-laden XML encoding. There are many proposals available to tackle this perceived problem, however none of these coherently consider the different aspects of the problem: (1) message size; (2) message structure; (3) accessing individual elements; and (4) interoperability with other Web services protocols. The technique proposed in this paper: Table driven XML (TDXML) takes such a coherent view and offers more compact messages, simpler message structure and easier access to individual elements when compared to conventional SOAP. TDXML also enables keyed access to individual elements to enable single pass message parsing for improved object serialisation and deserialisation. Experimental studies show that TDXML can be incorporated easily into a commercially available SOAP implementation with 30% improved latency performance. Furthermore, this paper reports that TDXML can also be implemented with other transport mechanisms and provide even better performance (over 100%) than SOAP in regard to network bandwidth and latency.","1530-0803;2377-5408","0-7695-2551","10.1109/ASWEC.2006.37","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1615043","","Web services;XML;Simple object access protocol;Encoding;Bandwidth;Delay;Java;Proposals;Access protocols;Testing","Internet;access protocols;XML;meta data;encoding;open systems;message passing","Web service performance;SOAP protocol;text-based XML encoding;metadata-laden XML encoding;message size problem;message structure problem;interoperability;table driven XML;TDXML;single pass message parsing;object serialization;object deserialisation;transport mechanism;network bandwidth","","6","39","","","","","","IEEE","IEEE Conferences"
"ROS-DMA: A DMA Double Buffering Method for Embedded Image Processing with Resource Optimized Slicing","C. Zinner; W. Kubinger","ARC Seibersdorf Research, Austria; NA","12th IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS'06)","","2006","","","361","372","Image processing on a Digital Signal Processor (DSP) often requires image data to be stored in external memory, because the amount of fast on-chip memory is usually very limited. Processing images in external memory causes significant performance drawbacks. This paper presents a double buffering method using Direct Memory Access (DMA), called Resource Optimized Slicing (ROS-DMA), which is intended to be used instead of a Level 2 (L2) data cache. The idea of ROS-DMA is to transfer image slices into small intermediate buffers of fast internal memory, where the processing can be completed utilizing the full processing power. Use of DMA enables the data transfers and the processing to be accomplished in parallel. The proposed method has the advantage of a modular implementation, making it easy to re-use components for various image processing operations. The sequence of transfers is organized in such a way that use of processor resources is optimized to achieve the shortest possible execution time. ROS-DMA can yield substantially better performance compared to using L2 cache. Furthermore, we expect that with ROS-DMA it will be easier to obtain reliable and tight Worst Case Execution Times (WCETs). Test runs achieved up to six times faster execution with ROS-DMA compared to using the L2 cache on a C6416 DSP from Texas Instruments.","1545-3421","0-7695-2516","10.1109/RTAS.2006.38","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1613350","","Image processing;Optimization methods;Digital signal processing;Delay;Computer vision;Software libraries;Hardware;Kernel;Buffer storage;Digital signal processors","","","","17","13","","","","","","IEEE","IEEE Conferences"
"Evaluating the impact of dynamic binary translation systems on hardware cache performance","A. Ruiz-Alvarez; K. Hazelwood","University of Virginia, USA; University of Virginia, USA","2008 IEEE International Symposium on Workload Characterization","","2008","","","131","140","Dynamic binary translation systems enable a wide range of applications such as program instrumentation, optimization, and security. DBTs use a software code cache to store previously translated instructions. The code layout in the code cache greatly differs from the code layout of the original program. This paper provides an exhaustive analysis of the performance of the instruction/trace cache and other structures of the micro-architecture while executing DBTs that focus on program instrumentation, such as DynamoRIO and Pin. We performed our evaluation along two axes. First, we directly accessed the hardware performance counters to determine actual cache miss counts. Second, we used simulation to analyze the spatial locality of the translated application. Our results show that when executing an application under the control of Pin or DynamoRIO, the icache miss counts actually increase over 2X. Surprisingly, the L2 cache and the L1 data cache show a much lower performance degradation or even break even with the native application. We also found that overall performance degradations are due to the instructions added by the DBT itself, and that these extra instructions outweigh any possible spatial locality benefits exhibited in the code cache. Our observations held regardless of the trace length, code cache size, or the presence of a hardware trace cache. These results provide a better understanding of the efficiency of current instrumentation tools and their effects on instruction/trace cache performance and other structures of the microarchitecture.","","978-1-4244-2777-2978-1-4244-2778","10.1109/IISWC.2008.4636098","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4636098","","Benchmark testing;Instruments;Hardware;Radiation detectors;Layout;Optimization;Linux","cache storage;program interpreters;software performance evaluation","dynamic binary translation systems;hardware cache performance;software code cache;program instrumentation;DynamoRIO;Pin","","2","29","","","","","","IEEE","IEEE Conferences"
"Simulation and Implementation of Discrete Cosine Transform for MPEG-4","S. Gharge; S. Krishnan","NA; NA","International Conference on Computational Intelligence and Multimedia Applications (ICCIMA 2007)","","2007","4","","137","141","MPEG video compression is used in digital television set-top boxes, DSS, HDTV, video conferencing, Internet &amp; such applications. The discrete cosine transforms (DCT) are used in MPEG &amp; JPEG compression standards. So the DCT component has stringent timing requirements. The high performance which is required can not be achieved by sequential implementation of algorithm. Consequently, high quality of performance can not be achieved. Hence different optimization techniques to improve the performance of the DCT are discussed here. For DCT, VHDL codes were written in behavioral model for pipelined memory optimization design which reduces the memory requirement and cost of implementation. This design was also implemented on FPGA Spartan3e which occupied approximately 69,468 total equivalent gate count. The FPGA testing was made by using software chip-scope.","","0-7695-3050","10.1109/ICCIMA.2007.323","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4426466","","Discrete cosine transforms;MPEG 4 Standard;Transform coding;Video compression;Field programmable gate arrays;Digital TV;Decision support systems;HDTV;Videoconference;Internet","data compression;discrete cosine transforms;memory architecture;video coding","discrete cosine transform;MPEG-4;MPEG video compression;JPEG compression standards;VHDL codes;pipelined memory optimization design;memory requirement;FPGA Spartan3e;FPGA testing;software chip-scope","","2","8","","","","","","IEEE","IEEE Conferences"
"Multiprocessor Synthesis for Periodic Hard Real-Time Tasks under a Given Energy Constraint","Heng-Ruey Hsu; Jian-Jia Chen; Tei-Wei Kuo","Department of Computer Science and Information Engineering Graduate Institute of Networking and Multimedia National Taiwan University, Taipei, Taiwan 106, ROC. Email: b89108@csie.ntu.edu.tw; NA; NA","Proceedings of the Design Automation & Test in Europe Conference","","2006","1","","1","6","The energy-aware design for electronic systems has been an important issue in hardware and/or software implementations, especially for embedded systems. This paper targets a synthesis problem for heterogeneous multiprocessor systems to schedule a set of periodic real-time tasks under a given energy consumption constraint. Each task is required to execute on a processor without migration, where tasks might have different execution times on different processor types. Our objective is to minimize the processor cost of the entire system under the given timing and energy consumption constraints. The problem is first shown being NP-hard and having no polynomial-time algorithm with a constant approximation ratio unless NP = P. We propose polynomial-time approximation algorithms with (m + 2)-approximation ratios for this challenging problem, where m is the number of the available processor types. Experimental results show that the proposed algorithms could always derive solutions with system costs close to those of optimal solutions","1530-1591;1558-1101","3-9810801-1","10.1109/DATE.2006.243932","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1657049","Energy-aware systems;Task scheduling;Real-time systems;Task partitioning;Multiprocessor synthesis","Energy consumption;Polynomials;Approximation algorithms;Hardware;Embedded software;Embedded system;Multiprocessing systems;Processor scheduling;Real time systems;Costs","computational complexity;energy conservation;multiprocessing systems;optimisation;polynomial approximation;processor scheduling;real-time systems","multiprocessor synthesis;periodic hard real-time tasks;heterogeneous multiprocessor systems;energy consumption constraint;timing constraints;NP-hard;constant approximation ratio;polynomial-time approximation algorithms","","10","18","","","","","","IEEE","IEEE Conferences"
"Optimal Radial Basis Function Neural Network power transformer differential protection","M. Tripathy; S. Ala","Department of Electrical Engineering, Motilal Nehru National Institute of Technology Allahabad, Allahabad-211004, India; NA","2009 IEEE Bucharest PowerTech","","2009","","","1","8","This paper presents a new algorithm for protection of power transformer by using optimal radial basis function neural network (ORBFNN). ORBFNN based technique is applied by amalgamating the conventional differential protection scheme of power transformer and internal faults are precisely discriminated from inrush condition. The proposed method neither depend on any threshold nor the presence of harmonic contain in differential current. The RBFNN is designed by using particle swarm optimization (PSO) technique. The proposed RBFNN model has faster learning and detecting capability than the conventional neural networks. A comparison in the performance of the proposed ORBFNN and more commonly reported feed forward back propagation neural network (FFBPNN), in literature, is made. The simulations of different faults, over-excitation, and switching conditions on three different power transformers are performed by using PSCAD/EMTDC software and presented algorithm is evaluated by using MATLAB. The test results show that the new algorithm is quick and accurate.","","978-1-4244-2234-0978-1-4244-2235","10.1109/PTC.2009.5282183","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5282183","RBFNN;Artificial neural network;Power transformer protection;FFBPNN;Differential relaying;PSO;Protective relaying","Radial basis function networks;Power transformers;Neural networks;Surge protection;Particle swarm optimization;Mathematical model;Feeds;Feedforward neural networks;Performance evaluation;PSCAD","fault diagnosis;particle swarm optimisation;power engineering computing;power transformer protection;radial basis function networks","optimal radial basis function neural network;ORBFNN-based technique;power transformer differential protection;internal fault discrimination;inrush fault condition;differential current harmonics;particle swarm optimization technique;switching condition;over-excitation condition;PSCAD-EMTDC software","","1","38","","","","","","IEEE","IEEE Conferences"
"An Improved RBF Network On-Line Learning Algorithm","Z. X. Ming; N. G. Liang","NA; NA","2009 Second International Symposium on Information Science and Engineering","","2009","","","547","552","RBF neural network off-line learning algorithm can only be trained by a set of samples at the same time, not by individual samples one by one, therefore its adaptiveness is poor. This paper presents an improved RBF network on-line learning algorithm based on resource allocation network. The network can be trained by individual samples respectively. At first a structure of RBF neural network with no hidden layer is created, then this network is trained using individual samples, according to current errors between output and input, the numbers and locations of the hidden layer nodes are dynamically added or deleted to optimize the structure of network. The strategy of increasing or decreasing nodes on-line is based on the recursive least squares algorithm. For the samples producing larger output response, the weights of neurons are retained otherwise deleted to improve RBF neural network convergence speed and real time. Nonlinear curve fitting tests including natural index, multiplication, trigonometric functions and gas content forecast are carried out by VC++ simulation software to verify the validity of this algorithm. Simulation tests show that the improved on-line learning algorithm for RBF network has higher forecasting accuracy, well generalization ability, fewer hidden nodes. It can be realized in embedded systems and has a good value in many application fields such as gas content forecast.","2160-1283;2160-1291","978-1-4244-6326-8978-1-4244-6325","10.1109/ISISE.2009.17","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5447311","radial basis function(RBF) neural network;on-line learning algorithm;resource allocation network;recursive least squares method","Radial basis function networks;Neural networks;Predictive models;Resource management;Least squares methods;Neurons;Convergence;Curve fitting;Software testing;Software algorithms","C++ language;computer aided instruction;curve fitting;embedded systems;forecasting theory;least squares approximations;radial basis function networks;simulation","online learning;RBF neural network;offline learning;recursive least squares algorithm;nonlinear curve fitting;VC++ simulation software;forecasting accuracy;embedded systems","","1","10","","","","","","IEEE","IEEE Conferences"
"Online muon reconstruction in the ATLAS level-2 trigger system","S. Armstrong; A. dos Anjos; J. T. M. Baines; C. P. Bee; M. Biglietti; J. A. Bogaerts; V. Boisvert; M. Bosman; B. Caron; P. Casado; G. Cataldi; D. Cavalli; M. Cervetto; G. Comune; P. C. Muino; A. De Santo; A. Di Mattia; M. D. Gomez; M. Dosil; N. Ellis; D. Emeliyanov; B. Epp; S. Falciano; A. Farilla; S. George; V. Ghete; S. Gonzalez; M. Grothe; S. Kabana; A. Khomich; G. Kilvington; N. Kostantinidis; A. Kootz; A. Lowe; C. Luci; L. Luminari; T. Maeno; F. Marzano; J. Masik; C. Meessen; A. G. Mello; G. Merino; R. Moore; P. Morettini; A. Negri; N. Nikitin; A. Nisati; C. Padilla; N. Panikashvili; F. Parodi; E. Pasqualucci; V. P. Reale; J. L. Pinfold; P. Pinto; Z. Qian; S. Resconi; S. Rosati; C. Sanchez; C. Santamarina; D. A. Scannicchio; C. Schiavi; E. Segura; J. M. de Seixas; S. Sivoklokov; R. Soluk; E. Stefanidis; S. Sushkov; M. Sutton; S. Tapprogge; E. Thomas; F. Touchard; B. V. Pinto; V. Vercesi; P. Werner; S. Wheeler; F. J. Wickens; W. Wiedenmann; M. Wielers; H. Zobernig","Brookhaven Nat. Lab., Upton, NY, USA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA","IEEE Transactions on Nuclear Science","","2006","53","3","1339","1346","To cope with the 40 MHz event production rate of LHC, the trigger of the ATLAS experiment selects events in three sequential steps of increasing complexity and accuracy whose final results are close to the offline reconstruction. The Level-1, implemented with custom hardware, identifies physics objects within Regions of Interests and operates with a first reduction of the event rate to 75 kHz. The higher trigger levels, Level-2 and Level-3, provide a software based event selection which further reduces the event rate to about 100 Hz. This paper presents the algorithm (/spl mu/Fast) employed at Level-2 to confirm the muon candidates flagged by the Level-1. /spl mu/Fast identifies hits of muon tracks inside the barrel region of the Muon Spectrometer and provides a precise measurement of the muon momentum at the production vertex. The algorithm must process the Level-1 muon output rate (/spl sim/20 kHz), thus particular care has been taken for its optimization. The result is a very fast track reconstruction algorithm with good physics performance which, in some cases, approaches that of the offline reconstruction: it finds muon tracks with an efficiency of about 95% and computes p/sub T/ of prompt muons with a resolution of 5.5% at 6 GeV and 4.0% at 20 GeV. The algorithm requires an overall execution time of /spl sim/1 ms on a 100 SpecInt95 machine and has been tested in the online environment of the Atlas detector test beam.","0018-9499;1558-1578","","10.1109/TNS.2006.872630","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1645041","ATLAS Level-2;online;trigger","Mesons;Production;Physics;Testing;Large Hadron Collider;Hardware;Software algorithms;Spectroscopy;Software measurement;Reconstruction algorithms","position sensitive particle detectors;nuclear electronics;muon detection;particle spectrometers;high energy physics instrumentation computing","online muon reconstruction;ATLAS Level-2 trigger system;event production rate;LHC;custom hardware;Level-2 trigger levels;Level-3 trigger levels;muonFast algorithm;muon tracks;muon spectrometer;muon momentum;fast track reconstruction algorithm;SpecInt95 machine;Atlas detector test beam","","2","13","","","","","","IEEE","IEEE Journals & Magazines"
"Advance reservations for distributed real-time workflows with probabilistic service guarantees","T. Cucinotta; K. Konstanteli; T. Varvarigou","Real-Time Systems Laboratory, Scuola Superiore Sant'Anna, Pisa, Italy; Advanced Distributed Computing Laboratory National Technical University of Athens, Greece; Advanced Distributed Computing Laboratory National Technical University of Athens, Greece","2009 IEEE International Conference on Service-Oriented Computing and Applications (SOCA)","","2009","","","1","8","This paper addresses the problem of optimum allocation of distributed real-time workflows with probabilistic service guarantees over a grid of physical resources made available by a provider. The discussion focuses on how such a problem may be mathematically formalised, both in terms of constraints and objective function to be optimized, which also accounts for possible business rules for regulating the deployment of the workflows. The presented formal problem constitutes a probabilistic admission control test that may be run by a provider in order to decide whether or not it is worth to admit new workflows into the system, and to decide what the optimum allocation of the workflow to the available resources is. Various options are presented which may be plugged into the formal problem description, depending on the specific needs of individual workflows.","2163-2871","978-1-4244-5300","10.1109/SOCA.2009.5410268","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5410268","advance reservations;real-time interactive workflows;probabilistic service guarantees","Real time systems;Resource management;Quality of service;Laboratories;Linux;Processor scheduling;Distributed computing;Constraint optimization;Admission control;System testing","grid computing;probability;quality of service;resource allocation;workflow management software","distributed real-time workflow allocation;probabilistic service guarantees;grid computing;objective function;probabilistic admission control test;resource allocation;formal problem description;quality of service","","4","19","","","","","","IEEE","IEEE Conferences"
"Online convergence detection for multiobjective aerodynamic applications","B. Naujoks; H. Trautmann","Faculty of Computer Science, TU Dortmund University (Germany); Faculty of Statistics, TU Dortmund University (Germany)","2009 IEEE Congress on Evolutionary Computation","","2009","","","332","339","Industry applications of multiobjective optimization problems mostly are characterized by the demand for high quality solutions on the one hand. On the other hand an optimization result is desired which at any rate meets the time constraints for the evolutionary multiobjective algorithms (EMOA). The handling of this trade-off is a frequently discussed issue in multiobjective evolutionary optimization.","1089-778X;1941-0026","978-1-4244-2958-5978-1-4244-2959","10.1109/CEC.2009.4982966","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4982966","","Convergence;Aerodynamics;Testing;Time factors;Statistical analysis;Aircraft;Aerospace industry;Automotive components;Application software;Computer science","aerodynamics;aerospace industry;convergence;evolutionary computation;optimisation","online convergence detection;multiobjective aerodynamic application;aircraft industry;multiobjective optimization problem;evolutionary multiobjective algorithm","","5","18","","","","","","IEEE","IEEE Conferences"
"An Intelligent Product Disassembly Planning Method","Wang Hui; Xiang Dong; Duan Guanghong","NA; NA; NA","Proceedings of the 2006 IEEE International Symposium on Electronics and the Environment, 2006.","","2006","","","357","357","Disassembly sequence planning problem is a NP-hard combinatorial optimization problem. Generally, with the increasing of components number, the computational complexity of searching for good disassembly solution(s) in a large search space of disassembly solutions will be increased more. Therefore, to avoid the exploded combination, heuristic methods are often used for the goal of finding optimum solution(s) at a high efficiency. In the proposed research, we present the disassembly feasibility information graph (DFIG) to describe product's disassembly operations (sequences) information. In fact, this graph is a model of simulated all possible disassembly operations, and meanwhile, stored relevant information of operations which includes feasibility of operations and disassembly processes if these operations are feasible. Based on this graph, product's disassembly sequences planning problem could be transformed into this problem: On the DFIG, to find out a path with an optimized sum value, which starts from the start point, and could tour all the components of product just one time, along with the weighed, directed edges","1095-2020;2378-7260","1-4244-0351","10.1109/ISEE.2006.1650092","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1650092","","Biological cells;Genetic algorithms;Computational intelligence;Instruments;Computational complexity;Path planning;Encoding;Testing;Filling;Software systems","assembly planning;computational complexity;design for disassembly;genetic algorithms;graph theory","disassembly sequence planning;disassembly feasibility information graph;genetic algorithm;computational complexity;NP-hard combinatorial optimization","","","","","","","","","IEEE","IEEE Conferences"
"Maintaining consistency between SystemC and RTL system designs","A. Bruce; A. Nightingale; N. Romdhane; M. M. K. Hashmi; S. Beavis; C. Lennard","ARM, Sheffield, UK; ARM, Sheffield, UK; ARM, Sheffield, UK; NA; NA; NA","2006 43rd ACM/IEEE Design Automation Conference","","2006","","","85","89","We describe how system design consistency can be maintained across multiple levels of design abstraction using a modular verification IP strategy. This strategy involves delivery of verification IP in an environment independent manner, utilizing a standard system verification architecture that leverages re-usable component verification drivers, transaction-based interfaces, and synchronization through a system-verification master. This enables a single test-bench to be applied for systems modeled both in SystemC, as well as at the RT level. The configuration of the verification test-bench is kept consistent with the design by using system-design meta-data described using the specifications of the SPIRIT Consortium","0738-100X","1-59593-381","10.1145/1146909.1146936","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1688766","Design;Standardization;Languages;Verification;Verification;VIP;Testbench;SystemC;TLM;Transactor;RTL;SPIRIT","System testing;Permission;System-level design;Standards;System-on-a-chip;Maintenance engineering;Design engineering;Systems engineering and theory;Design optimization;Process design","hardware description languages;hardware-software codesign;integrated circuit design;logic design","SystemC;register transfer level system designs;verification IP;standard system verification architecture;verification drivers;transaction-based interfaces;system-verification master;verification test-bench;system-design meta-data","","5","10","","","","","","IEEE","IEEE Conferences"
"TASTE: Testability Analysis Engine and Opened Libraries for Digital Data Path","J. Strnadel","NA","2008 11th EUROMICRO Conference on Digital System Design Architectures, Methods and Tools","","2008","","","865","872","Testability is one of the most important factors that are considered during design cycle along with reliability, speed, power consumption, cost and other factors important for a customer. Estimation of testability parameter strongly depends on how accurate information utilized for the estimation by a testability analysis method is. In the paper, two results of our previous, long-term research in the area of digital circuit testability analysis are summarized: principle of our testability analysis engine and libraries used to store the information outgoing from transparency models. The engine is general and accuracy of its results strongly depends on the information stored in the libraries. If simple transparency model is utilized, information about circuit testability could be far away from real state of the circuit. Otherwise, testability information can approximate so serious parameter such as fault-coverage factor.","","978-0-7695-3277","10.1109/DSD.2008.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4669327","digital;circuit;data path;testability analysis;transparency;netlist;library;fault coverage;estimation;fitness;exploration;optimization;design for testability;automation","Engines;Software libraries;Circuit testing;Information analysis;Energy consumption;Costs;Parameter estimation;Digital circuits;Circuit analysis;Circuit faults","circuit analysis computing;integrated circuit design;integrated circuit testing","testability analysis engine;digital data path;opened libraries;digital circuit testability analysis;fault-coverage factor","","1","25","","","","","","IEEE","IEEE Conferences"
"An effective forwarding architecture for SMP Linux routers","R. Bolla; R. Bruschi","Department of Communications, Computer and Systems Science (DIST), University of Genoa, Via Opera Pia 13, 16145 Genova, Italy; Department of Communications, Computer and Systems Science (DIST), University of Genoa, Via Opera Pia 13, 16145 Genova, Italy","2008 4th International Telecommunication Networking Workshop on QoS in Multiservice IP Networks","","2008","","","210","216","Recent technological advances provide an excellent opportunity to achieve truly effective results in the field of open Internet devices based on PC COTS Hardware, also known as Open Routers. In this environment, new interesting research topics, like parallel computing optimization, are related to the rapid migration of CPU architectures to the multi-core paradigm. Thus, with this idea, this contribution focuses on the packet forwarding performance in SMP Linux kernels, trying to identify and to characterize its architectural bottlenecks. Starting from this analysis, we propose an innovative architecture to optimize the forwarding performance when multi-processor or multi-core systems are used.","","978-1-4244-1844-2978-1-4244-1845","10.1109/ITNEWS.2008.4488155","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4488155","","Linux;Computer architecture;Kernel;Hardware;Parallel processing;Performance analysis;Open source software;Testing;Concurrent computing;Read-write memory","Internet;Linux;network operating systems;operating system kernels;packet switching;public domain software;telecommunication network routing","packet forwarding architecture;SMP Linux routers;SMP Linux kernels;multiprocessor systems;multicore systems;open Internet devices;PC COTS hardware;Open Router;open source software","","7","23","","","","","","IEEE","IEEE Conferences"
"Task-Oriented Integrated Use of Biological Web Data Sources","M. Kirac; A. Cakmak; G. Ozsoyoglu","Case Western Reserve University; NA; NA","18th International Conference on Scientific and Statistical Database Management (SSDBM'06)","","2006","","","81","90","Biological Web data sources have now become essential information sources for researchers. However, their use is tedious, labor-intensive, repetitive, and possibly involve the integration of data from multiple Web data sources. In this paper, as a first step towards the full integration of Web data sources, we propose a framework that allows an integrated use of biological sources in a task-oriented manner. We define and experimentally evaluate a toolkit-based framework for semi-automatically constructing an integrated (software) system that automates and optimizes the execution of a biology-related computational task at hand. To test and refine the principles of the framework, we build and evaluate ""pathway-infer"" as a benchmark integrated system","1551-6393","0-7695-2590","10.1109/SSDBM.2006.49","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1644300","","Biology computing;Software systems;Databases;Biological system modeling;Data mining;Data models;Software tools;System testing;Benchmark testing;Computer interfaces","biology computing;data models;information resources;Internet","biological Web data sources;information sources;biology-related computational task;toolkit-based framework;semiautomatic construction;pathway-infer;benchmark integrated system;data models","","","38","","","","","","IEEE","IEEE Conferences"
"Adaptive Evolvement of Query Plan Based on Low Cost in Dynamic Grid Database","N. Hu; Y. Luo; Y. Wang","NA; NA; NA","2008 Ninth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing","","2008","","","411","415","Grid database is a new research area combined with the database techniques and Grid techniques, while traditional database query and optimization techniques couldn't fully satisfy the need of Grid database because of the dynamic properties of Grid Nodes. The paper brings forward cost model in dynamic grid database environment , and also gives the dynamic query optimization algorithm used for the query plan to make adaptive evolvement along with the fluctuation of gird environment. Finally the paper tests the models and algorithms through experiment.","","978-0-7695-3263","10.1109/SNPD.2008.14","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4617406","dynamic;grid;query","Databases;Adaptation model;Query processing;Computational modeling;Bandwidth;Distributed databases;Fluctuations","distributed databases;grid computing;query processing","adaptive evolvement;query plan;dynamic grid database;database techniques;grid techniques;database query;dynamic query optimization algorithm","","2","9","","","","","","IEEE","IEEE Conferences"
"Data Block Partition and Database Based Large Scale Terrain 3D Visualization","Z. Fu; W. Zhang","NA; NA","2009 International Conference on Computational Intelligence and Software Engineering","","2009","","","1","4","This paper presents a database-based optimized algorithm for terrain data organization, storage, scheduling and multi-resolution rendering; besides, some technical means in data dynamical scheduling are illustrated, such as data encoding and indexing, view frustum culling, cache management and multi-threading. Due to the drawbacks of ROAM algorithm when dealing with large scale terrain data, ROAM is only applied to individual block to build multi-resolution LOD models. The evaluation function, cracks elimination, triangle strips and some other key questions are discussed as well. Experimental tests are conducted on the efficiency of terrain visualization in a LAN environment, results indicate a better performance on terrain rendering and fast roaming, this optimized algorithm can meet the requirements of large-scale terrain visualization.","","978-1-4244-4507","10.1109/CISE.2009.5366711","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5366711","","Visual databases;Large-scale systems;Data visualization;Dynamic scheduling;Partitioning algorithms;Scheduling algorithm;Cache storage;Encoding;Indexing;Strips","data visualisation;database indexing;rendering (computer graphics);solid modelling;terrain mapping;visual databases","data block partition;database based large scale terrain 3D visualization;database based optimized algorithm;terrain data organization;data storage;multiresolution rendering;data dynamical scheduling;cache management;multithreading;ROAM algorithm;multiresolution LOD model;evaluation function;cracks elimination;triangle strips;LAN environment;terrain rendering;data indexing","","","8","","","","","","IEEE","IEEE Conferences"
"Model-based energy management strategy development for hybrid electric vehicles","Guoqiang Ao; Jiaxi Qiang; Ziqiang Chen; Lin Yang","The Institute of Automotive Electronic Technology, Shanghai Jiao Tong University, 200030, China; The Institute of Automotive Electronic Technology, Shanghai Jiao Tong University, 200030, China; The Institute of Automotive Electronic Technology, Shanghai Jiao Tong University, 200030, China; The Institute of Automotive Electronic Technology, Shanghai Jiao Tong University, 200030, China","2008 IEEE International Symposium on Industrial Electronics","","2008","","","1020","1024","The use of simulation packages is essential for software engineers to evaluate a specific control strategy or algorithm. This paper presents an energy management strategy development process of hybrid electric vehicles (HEV) to demonstrate the model-based development methodology. The control algorithms are modeled, simulated and optimized in the environment of Matlab/Simulink/Stateflow. A code generator, TargetLink, is used to transit the algorithm model to the embedded production ANSI C code. The generated C code is tested and modified in the hardware-in-the-loop simulation system (HIL) to ease the burden of heavy testing. The HEV energy management control strategy was developed with the model-based V-model development methodology, which guarantees the control precision and help to shorten the development time.","2163-5137;2163-5145","978-1-4244-1665-3978-1-4244-1666","10.1109/ISIE.2008.4677230","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4677230","","Energy management;Hybrid electric vehicles;Mathematical model;System testing;Software packages;Packaging;Automotive engineering;Power engineering and energy;Software algorithms;Distributed power generation","digital simulation;energy management systems;hybrid electric vehicles;mathematics computing;power engineering computing","model-based energy management strategy development;for hybrid electric vehicles;simulation packages;Matlab;Simulink;Stateflow;TargetLink code generator;ANSI C code;hardware-in-the-loop simulation system","","2","11","","","","","","IEEE","IEEE Conferences"
"Real-time identification and visualization of human segment parameters","G. Venture; K. Ayusawa; Y. Nakamura","Department of Mechanical System Engineering, Tokyo University of Agriculture and Technology; Department of Mechano-Informatics, The University of Tokyo, Japan; Department of Mechano-Informatics, The University of Tokyo, Japan","2009 Annual International Conference of the IEEE Engineering in Medicine and Biology Society","","2009","","","3983","3986","Mass parametersof the body segments are mandatory to study motion dynamics. No systematic method to estimate them has been proposed so far. Rather, parameters are scaled from generic tables or estimated with methods inappropriate for in-patient care. Based on our previous works, we propose a real-time software that allows to estimate the whole-body segment parameters, and to visualize the progresses of the completion of the identification. The visualization is used as a feedback to optimize the excitation and thus the identification results. The method is experimentally tested.","1094-687X;1558-4615","978-1-4244-3296","10.1109/IEMBS.2009.5333620","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5333620","","Humans;Interpolation;Equations;Data visualization;USA Councils;Testing;Biological system modeling;Orthopedic surgery;Biomechanics;Nervous system","biomechanics;data visualisation;medical computing;optimisation;real-time systems","real-time identification;visualization;human segment parameters;mass parameters;motion dynamics;in-patient care;real-time software;optimization","Algorithms;Biomechanics;Biophysics;Computer Graphics;Computer Simulation;Computer Systems;Gait;Humans;Image Processing, Computer-Assisted;Models, Anatomic;Models, Statistical;Motion;Reproducibility of Results","18","17","","","","","","IEEE","IEEE Conferences"
"Joint Segmentation of Moving Object and Estimation of Background in Low-Light Video using Relaxation","P. M. Q. Aguiar; J. M. F. Moura","Institute for Systems and Robotics / IST, Lisboa, Portugal. E-mail: aguiar@isr.ist.utl.pt; ECE Dep., Carnegie Mellon University, Pittsburgh PA, USA. E-mail: moura@ece.cmu.edu","2007 IEEE International Conference on Image Processing","","2007","5","","V - 53","V - 56","When the scene background is known and the intensity of moving objects contrasts with the intensity of the background, the objects are easily captured by exploiting occlusion, e.g., background-subtraction. However, when processing general scenes, the background is not known and researchers have mostly attempted to segment moving objects by using motion cues rather than occlusion. Since motion can only be accurately computed at highly textured regions, current motion segmentation methods either fail to segment low textured objects, or require expensive regularization techniques. We present a computationally simple algorithm and test it with segmentation of moving objects in low texture / low contrast videos that are obtained in low-light scenes. The images in the sequence are modeled taking into account the rigidity of the moving object and the occlusion of the background. We formulate the problem as the minimization of a penalized likelihood cost. Relaxation of the weight of the penalty term leads to a simple solution to the nonlinear minimization. We describe experiments that illustrate the good performance of our method.","1522-4880;2381-8549","978-1-4244-1436-9978-1-4244-1437","10.1109/ICIP.2007.4379763","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4379763","Occlusion;background subtraction;motion segmentation;low contrast;relaxation;combinatorial optimization","Layout;Image segmentation;Computer vision;Shape;Maximum likelihood estimation;Motion segmentation;Costs;Video sequences;Signal to noise ratio;Application software","combinatorial mathematics;image motion analysis;image segmentation;image sequences;image texture;object detection;optimisation;video signal processing","moving object joint segmentation;low-light video background estimation;occlusion exploition;image texture;image sequence;penalized likelihood cost;combinatorial optimization;nonlinear minimization","","2","11","","","","","","IEEE","IEEE Conferences"
"Performance Improvement Using Parallel Simulation Protocol and Time Warp for DEVS Based Applications","Y. Sun; J. Nutaro","NA; NA","2008 12th IEEE/ACM International Symposium on Distributed Simulation and Real-Time Applications","","2008","","","277","284","DEVS is a formalism intended to model both discrete and continuous systems. The use of discrete events, rather than time steps, as the basis for simulation has been shown to reduce the computation time in many applications. Parallel DEVS is an extension to standard DEVS, which provides means to handle simultaneous scheduling. In this paper, we present an implementation of the parallel DEVS simulation protocol that uses a modified time warp optimistic algorithm for shared memory multiprocessor machine. This implementation is designed to execute the DEVS models in parallel and, at the same time to correctly simulate every model defined in terms of DEVS specification. Two test cases and the DEVSFIRE example are used to verify this algorithm. Preliminary experimental results are presented that show the implementation can speedup a DEVS simulation.","1550-6525","978-0-7695-3425","10.1109/DS-RT.2008.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700130","","Time warp simulation;Protocols;Computational modeling;Discrete event simulation;Computer simulation;Power system modeling;Application software;Computer science;Processor scheduling;Testing","formal specification;parallel processing;scheduling;shared memory systems;software performance evaluation;time warp simulation","performance improvement;parallel simulation protocol;parallel discrete event system simulation protocol;simultaneous scheduling;time warp optimistic algorithm;shared memory multiprocessor machine;dicrete event system specification;DEVSFIRE","","1","21","","","","","","IEEE","IEEE Conferences"
"Balancing wrapper chains of SoC core based on best interchange decreasing","Maoxiang Yi; Huaguo Liang; Zhengfeng Huang","Department of Electronic Science and Technology, Hefei University of Technology, HFUT, China; School of Computer and Information, Hefei University of Technology, HFUT, China; School of Computer and Information, Hefei University of Technology, HFUT, China","2008 International Symposium on System-on-Chip","","2008","","","1","4","An improved scheme for balancing wrapper chains partition of SoC core is proposed. Starting with the primary configuration created by LPT algorithm, we optimizes the current partition through the best interchange decreasing and iterative operation, in each step of which a pair of wrapper chains with maximum length difference is selected and the optimal two cells in the two wrapper chains are interchanged. Experiments are executed for the typical cores of the ITCpsila02 benchmarks. The results show that compared to the previous techniques, our scheme can create more balanced wrapper chains, decreasing the maximum scan shift length, hence the test application time of core.","","978-1-4244-2541-9978-1-4244-2542","10.1109/ISSOC.2008.4694880","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4694880","","Design for testability;Partitioning algorithms;Iterative algorithms;Automatic testing;Switches;Logic testing;Benchmark testing;Application software;NP-hard problem;System-on-a-chip","iterative methods;system-on-chip","wrapper chains partition balancing;SoC core;best interchange decreasing;LPT algorithm;iterative operation;maximum length difference;maximum scan shift length","","1","13","","","","","","IEEE","IEEE Conferences"
"Automated design of spacecraft power subsystems","R. J. Terrile; M. Kordon; D. Mandutianu; J. Salcedo; E. Wood; M. Hashemi","California Inst. of Technol., Pasadena, CA, USA; California Inst. of Technol., Pasadena, CA, USA; California Inst. of Technol., Pasadena, CA, USA; California Inst. of Technol., Pasadena, CA, USA; California Inst. of Technol., Pasadena, CA, USA; California Inst. of Technol., Pasadena, CA, USA","2006 IEEE Aerospace Conference","","2006","","","14 pp.","","This paper discusses the application of evolutionary computing to a dynamic space vehicle power subsystem resource and performance simulation in a parallel processing environment. Our objective is to demonstrate the feasibility, application and advantage of using evolutionary computation techniques for the early design search and optimization of space systems. With this approach, engineers specify several sets of conditional subsystem performance criteria to trade off subsystem goals of mass, cost, performance and risk. Once specified, the integrated evolutionary/simulation software will then automatically generate a design option for each criteria, selecting and sizing power elements based on the space system's anticipated performance in the simulated environment. Initial activity plans from two actual JPL missions, Mars exploration rovers (MER) and Deep Impact (DI) are used to test the software. Our results have shown human-competitive advantages by generating credible design concepts much faster than humans are able to and without the need for expert initial designs","1095-323X","0-7803-9545","10.1109/AERO.2006.1656181","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1656181","","Space vehicles;Computational modeling;Computer applications;Concurrent computing;Power engineering computing;Vehicle dynamics;Parallel processing;Application software;Evolutionary computation;Design optimization","aerospace computing;aerospace simulation;evolutionary computation;space vehicle power plants","automated design;spacecraft power subsystems;evolutionary computing;performance simulation;parallel processing;design search;optimization;simulation software;Mars exploration rovers;Deep Impact","","6","10","","","","","","IEEE","IEEE Conferences"
"Power watersheds: A new image segmentation framework extending graph cuts, random walker and optimal spanning forest","C. Couprie; L. Grady; L. Najman; H. Talbot","Université Paris-Est, Laboratoire, d'Informatique Gaspard-Monge, Equipe A3SI, ESIEE (93160 Noisy-le-Grand, France); Siemens Corporate Research, Department of Imaging and Visualization, (Princeton, N.J. 08540. USA); Université Paris-Est, Laboratoire, d'Informatique Gaspard-Monge, Equipe A3SI, ESIEE, France; Université Paris-Est, Laboratoire, d'Informatique Gaspard-Monge, Equipe A3SI, ESIEE, France","2009 IEEE 12th International Conference on Computer Vision","","2009","","","731","738","In this work, we extend a common framework for seeded image segmentation that includes the graph cuts, random walker, and shortest path optimization algorithms. Viewing an image as a weighted graph, these algorithms can be expressed by means of a common energy function with differing choices of a parameter q acting as an exponent on the differences between neighboring nodes. Introducing a new parameter p that fixes a power for the edge weights allows us to also include the optimal spanning forest algorithm for watersheds in this same framework. We then propose a new family of segmentation algorithms that fixes p to produce an optimal spanning forest but varies the power q beyond the usual watershed algorithm, which we term power watersheds. Placing the watershed algorithm in this energy minimization framework also opens new possibilities for using unary terms in traditional watershed segmentation and using watersheds to optimize more general models of use in application beyond image segmentation.","2380-7504;1550-5499;1550-5499","978-1-4244-4420-5978-1-4244-4419","10.1109/ICCV.2009.5459284","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5459284","","Image segmentation;Minimization methods;Tree graphs;Computer vision;Surface topography;User interfaces;Computer errors;Visualization;Benchmark testing;Application software","graph theory;image segmentation;optimisation","power watersheds;image segmentation framework;graph cuts;random walker;optimal spanning forest;shortest path optimization algorithms;energy minimization framework","","49","37","","","","","","IEEE","IEEE Conferences"
"Automation of 3D view acquisition for geometric tolerances verification","M. Germani; M. Mengoni; R. Raffaeli","Polytechnic University of Marche, Ancona, Italy; Polytechnic University of Marche, Ancona, Italy; Polytechnic University of Marche, Brecce Bianche, Ancona, Italy","2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops","","2009","","","1710","1717","Geometrical tolerances control of mechanical components requires methods and tools in order to improve the efficiency of process in terms of time. Dedicated software systems in order to plan and simulate the control and hardware tools in order to rapidly acquire the needed 3D information can support the process improvement. In this context it is important to use the 3D CAD (Computer Aided Design) model, as base to plan and pilot the whole process. The aim of present work is to describe an automatic geometrical tolerances measurement system usable during the design stage. It is based on three main tools: a CAD-based modular software tool, in order to plan, simulate, and pilot the whole verification process, a 3D optical digitizer, as shape acquisition system, and a multi-axis Degree of Freedom (DoF) robot arm in order to move the digitizer. This paper is focused on the developed algorithms to optimize the 3D views acquisition planning. Surface Normal Method and Visibility Map concepts have been reworked for range scanner positions determination and the optimal path is computed by a graph of alignable simulated scans. Experimental test cases are reported in order to show the system performance.","","978-1-4244-4442-7978-1-4244-4441","10.1109/ICCVW.2009.5457489","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5457489","","Computational modeling;Automatic control;Software systems;Hardware;Design automation;Context modeling;Software tools;Geometrical optics;Shape;Robotics and automation","CAD;data acquisition;mechanical engineering computing;solid modelling","3D view acquisition;geometric tolerances verification;mechanical components geometrical tolerances control;dedicated software system;3D information;3D CAD;computer aided design;automatic geometrical tolerances measurement;CAD based modular software tool;3D optical digitizer;shape acquisition system;multiaxis degree of freedom robot arm;3D views acquisition planning;surface normal method;visibility map","","2","22","","","","","","IEEE","IEEE Conferences"
"Solution of typization task","B. R. A'Ggel Al-Zabi; A. Kernytskyy; S. Tkatchenko","CAD/CAM Department, Lviv Polytechnic National University, 12, S. Bandery Str., 79013, UKRAINE; CAD/CAM Department, Lviv Polytechnic National University, 12, S. Bandery Str., 79013, UKRAINE; CAD/CAM Department, Lviv Polytechnic National University, 12, S. Bandery Str., 79013, UKRAINE","2009 5th International Conference on Perspective Technologies and Methods in MEMS Design","","2009","","","84","84","The new algorithm for solution of typization task is offered, which allows finding out in a circuit partly or fully equivalent subcircuits.","","978-1-4244-4516-5978-966-2191-06","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5069715","graph;scheme;equivalence;isomorphism;typization (functional decomposition)","Circuit testing;Tree graphs;Application software;Design automation;Microelectronics;Cost function;Design optimization;Computer aided manufacturing;CADCAM","equivalent circuits;graph theory","typization task;equivalent subcircuits;graph;scheme;isomorphism;functional decomposition","","","3","","","","","","IEEE","IEEE Conferences"
"Proactive e-Learning Management System","D. Zampunieris","University of Luxembourg, Luxembourg","Seventh IEEE International Conference on Advanced Learning Technologies (ICALT 2007)","","2007","","","645","646","This paper introduces a new kind of e-learning management system: proactive LMS. These e-learning platforms are designed to improve their users' online interactions by providing appropriate actions initiated by the LMS itself.","2161-3761;2161-377X","0-7695-2916","10.1109/ICALT.2007.210","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4281117","","Electronic learning;Least squares approximation;Testing;Design optimization;Technology management;Management training;Data acquisition;Communications technology;Environmental management;Software tools","computer aided instruction;interactive systems","proactive e-learning management system;online interactions;virtual educational environment;online training environment","","","6","","","","","","IEEE","IEEE Conferences"
"Hardware implementation of distributed speech recognition system front end","A. A. A. Sallab; H. Fahmy","NA; NA","2009 4th International Design and Test Workshop (IDT)","","2009","","","1","5","Modern speech recognition applications are heading towards embedded systems and hand-held devices. Distributed speech recognition (DSR) system architecture emerged to address this kind of applications. Most of the existing implementations of this system are presented in software fashion, with little consideration to the end product platform in which the system will be deployed. In this paper, an optimized hardware implementation of the front end part of the DSR specified in the basic ETSI Aurora standard ETSI ES 201 108 is presented in FPGA platform prototype, with consideration of migration to structured ASIC in case of mass-production. Main design issues and tips are highlighted. Results are presented in terms of hardware resources utilization, comparison of some basic system components to third party reference designs and compliance to the Aurora standard.","2162-0601;2162-061X","978-1-4244-5750-2978-1-4244-5748-9978-1-4244-5750","10.1109/IDT.2009.5404138","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5404138","","Hardware;Speech recognition;Application software;Telecommunication standards;Embedded system;Computer architecture;Field programmable gate arrays;Software prototyping;Prototypes;Application specific integrated circuits","embedded systems;field programmable gate arrays;speech recognition","distributed speech recognition system front end;embedded systems;handheld devices;ETSI Aurora standard ETSI ES 201 108;FPGA platform prototype;structured ASIC;mass production;third party reference designs","","1","12","","","","","","IEEE","IEEE Conferences"
"Event-driven elevator testing, control, monitoring, and maintenance","Y. Si; Y. Yang; W. Leong; S. Leong; C. Wong","Faculty of Science and Technology, University of Macau, China; Faculty of Science and Technology, University of Macau, China; Faculty of Science and Technology, University of Macau, China; Faculty of Science and Technology, University of Macau, China; Faculty of Science and Technology, University of Macau, China","2008 IEEE International Conference on Systems, Man and Cybernetics","","2008","","","3564","3569","Elevators are considered as important transportation systems for urban communities. Elevators are installed with onboard controllers (circuit boards) and these controllers can generate a large volume of signals and events. In this paper, we describe an event-driven system to test, control, and monitor a large number of on board elevator controllers. The integrated system consists of a virtual controller, control and monitoring terminals, a central server, a playback function with animation, a genetic algorithm based maintenance scheduling module, and a data warehouse for managing massive real-time elevator signals. Based on the event-driven architecture, the proposed system is capable of facilitating faster deployment of new types of elevators. The system also provides engineers with playback functions for troubleshooting any hardware or software errors. In order to reduce overhead cost, the proposed system is designed to optimize resource allocation in maintenance scheduling. By deploying data warehouse technology, the proposed system allows significant reduction of storage requirement for managing real-time signals.","1062-922X","978-1-4244-2384-2978-1-4244-2383","10.1109/ICSMC.2008.4811851","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4811851","","Elevators;Monitoring;Control systems;Circuit testing;Centralized control;Data warehouses;Real time systems;Transportation;Printed circuits;Signal generators","control engineering computing;data warehouses;genetic algorithms;lifts;maintenance engineering","event-driven elevator testing;elevator control;elevator monitoring;elevator maintenance;transportation systems;urban communities;onboard controllers;board elevator controllers;virtual controller;genetic algorithm;maintenance scheduling;data warehouse","","1","26","","","","","","IEEE","IEEE Conferences"
"Expert System for Quality Cost Planning, Monitoring and Control","S. Brad; M. Fulea; B. Mocan","Technical University of Cluj-Napoca, 15 Ctin Daicoviciu Street, Cluj-Napoca, 400020; Technical University of Cluj-Napoca, 15 Ctin Daicoviciu Street, Cluj-Napoca, 400020; Technical University of Cluj-Napoca, 15 Ctin Daicoviciu Street, Cluj-Napoca, 400020","2006 IEEE International Conference on Automation, Quality and Testing, Robotics","","2006","2","","53","58","On the market, there are some commercial available software tools for quality cost management. However, these tools do not incorporate specialized agents for handling complex tasks related to the current needs in quality cost planning, like interpretation of the results over a horizon of time and automatic generation of reliable guidelines to prioritize resources in order to improve the quality of the business processes. This paper is going to introduce the results of some researches performed by the authors in designing and developing an expert system for comprehensive monitoring, controlling and planning of quality costs within business processes. Results are already successful implemented in a large enterprise from chemical industry","","1-4244-0360-X1-4244-0361","10.1109/AQTR.2006.254599","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4022922","","Expert systems;Costs;Monitoring;Control systems;Automatic control;Process planning;Business;Software tools;Quality management;Guidelines","business data processing;costing;expert systems;quality control","expert system;quality cost planning;quality cost monitoring;quality cost control;business processes","","1","17","","","","","","IEEE","IEEE Conferences"
"DDSA: A Sampling and Validation Based Spectrum Access Algorithm in Wireless Networks","P. Yang; H. Wang; G. Chen","NA; NA; NA","2009 IEEE International Symposium on Parallel and Distributed Processing with Applications","","2009","","","374","381","Spectrum access scheme is a fundamental component in building efficient wireless networks. Conventional methods such as proactive channel assignment is costly due to large amount of protocol overhead. Also, those algorithms suffer from its inability in dealing with channel dynamics. The opportunistic methods however, spend more time on probing, and suffer from the myopic decisions as well. We present a decision based dynamic spectrum access algorithm (DDSA), which is built upon the Markov decision process (MDP), and could adaptively handle the DSA process for higher throughput. We employ quiet probing and dynamic controlling mechanisms in DDSA, so as to achieve a reduced protocol overhead and improved adaptivity. Different from previous methods, the DDSA is a model driven method, and we use the modeling technique on the IEEE 802.11 DCF for virtual channel state probing. The modeling technique could help us improve the accuracy on channel state, and reduce protocol overhead. Using a heuristic and adaptive algorithm named `hindsight optimization', we solve the hardness in computing the MDP. Moreover, under the feasibility testing and scaling processes, the validated decision can be confidentially applied for a congestion-free DSA.","2158-9178;2158-9208","978-0-7695-3747","10.1109/ISPA.2009.64","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5207909","","Sampling methods;Wireless networks;Access protocols;Wireless sensor networks;Throughput;Testing;Convergence;Costs;Distributed processing;Application software","channel allocation;Markov processes;optimisation;protocols;radio networks;wireless LAN","wireless networks;proactive channel assignment;protocol;decision based dynamic spectrum access algorithm;DDSA;Markov decision process;IEEE 802.11;virtual channel state probing;hindsight optimization","","","29","","","","","","IEEE","IEEE Conferences"
"Design and Characterization of Wafer Level SAW Filter Package Using LT-LT Wafer Structure","T. H. Kim; W. K. Jeung; S. J. Yang; S. M. Choi; M. J. Park; J. H. Park; S. Yi; J. S. Hwang; J. H. Lim; W. B. Kim","PKG Technology Team, Manufacture Engineering R&D Institute, Samsung Electro-Mechanics Co., LTD, 314, Meatan 3-Dong, Yeoungtong-Gu, Suwon, Gyunggi-Do, Korea; PKG Technology Team, Manufacture Engineering R&D Institute, Samsung Electro-Mechanics Co., LTD, 314, Meatan 3-Dong, Yeoungtong-Gu, Suwon, Gyunggi-Do, Korea; PKG Technology Team, Manufacture Engineering R&D Institute, Samsung Electro-Mechanics Co., LTD, 314, Meatan 3-Dong, Yeoungtong-Gu, Suwon, Gyunggi-Do, Korea; PKG Technology Team, Manufacture Engineering R&D Institute, Samsung Electro-Mechanics Co., LTD, 314, Meatan 3-Dong, Yeoungtong-Gu, Suwon, Gyunggi-Do, Korea; PKG Technology Team, Manufacture Engineering R&D Institute, Samsung Electro-Mechanics Co., LTD, 314, Meatan 3-Dong, Yeoungtong-Gu, Suwon, Gyunggi-Do, Korea; PKG Technology Team, Manufacture Engineering R&D Institute, Samsung Electro-Mechanics Co., LTD, 314, Meatan 3-Dong, Yeoungtong-Gu, Suwon, Gyunggi-Do, Korea; PKG Technology Team, Manufacture Engineering R&D Institute, Samsung Electro-Mechanics Co., LTD, 314, Meatan 3-Dong, Yeoungtong-Gu, Suwon, Gyunggi-Do, Korea; Packaging Project Team, Samsung Advanced Institute of Technology, San 14, Nongseo-Ri Giheung-Eup Yongin Gyunggi-Do, Korea; Packaging Project Team, Samsung Advanced Institute of Technology, San 14, Nongseo-Ri Giheung-Eup Yongin Gyunggi-Do, Korea; Packaging Project Team, Samsung Advanced Institute of Technology, San 14, Nongseo-Ri Giheung-Eup Yongin Gyunggi-Do, Korea","2006 International Conference on Electronic Materials and Packaging","","2006","","","1","5","Miniaturization is one of the driving design goals for large number of wireless applications, especially mobile phones. These market trends call for more thin and small size components with high reliability performance. In this paper, we describes the wafer level surface acoustic wave (SAW) filter package, 1.0 times 0.8 mm<sup>2</sup>, which is applicable for radio frequency (RF) stage in mobile phones. The SAW filter is reduced in size and thickness by using a 4"" wafer level package process technique. The technique uses interconnection via and LiTaO<sub>3</sub> (LT)-LiTaO<sub>3</sub> (LT) wafer bonding structure. The interconnection via is formed through LT wafer by using sand blasting or laser drilling method. The AuSn eutectic bonding enables the connection of the signal pad on the SAW chip, with gold metallized LT wafer package. This eutectic bonding ensures that the SAW chip is protected mechanically and connected electrically, with the package. In order to simulate and optimize the structure and characteristics of wafer level SAW filter package, we used HFSS and ADS software. Frequency responses of measurement and simulation are compared with wafer level SAW filter package. The results of reliability tests for wafer level SAW filter package was discussed.","","978-1-4244-0833-7978-1-4244-0834","10.1109/EMAP.2006.4430597","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4430597","","SAW filters;Wafer scale integration;Packaging;Surface acoustic waves;Wafer bonding;Mobile handsets;Radio frequency;Acoustic waves;Drilling;Gold","circuit simulation;electronics packaging;frequency response;integrated circuit interconnections;integrated circuit reliability;integrated circuit testing;laser beam machining;lithium compounds;mobile handsets;radiofrequency filters;surface acoustic wave filters;tantalum compounds;wafer bonding","mobile phones;high reliability performance;wafer level surface acoustic wave filter package;radio frequency stage;wafer bonding structure;sand blasting;laser drilling method;eutectic bonding;gold;structure simulation;HFSS software;ADS software;measurement frequency responses;simulation frequency responses;reliability testing;LiTaO<sub>3</sub>","","","5","","","","","","IEEE","IEEE Conferences"
"A Generalized Associative Petri Net for Reasoning","D. Shih; H. Chiang; B. Lin","NA; NA; NA","IEEE Transactions on Knowledge and Data Engineering","","2007","19","9","1241","1251","Although Bayesian networks (BNs) are increasingly being used to solve real-world risk problems, their use is still constrained by the difficulty of constructing the node probability tables (NPTs). A key challenge is to construct relevant NPTs using the minimal amount of expert elicitation, recognizing that it is rarely cost effective to elicit complete sets of probability values. We describe a simple approach to defining NPTs for a large class of commonly occurring nodes (called ranked nodes). The approach is based on the doubly truncated normal distribution with a central tendency that is invariably a type of weighted function of the parent nodes. In extensive real-world case studies, we have found that this approach is sufficient for generating the NPTs of a very large class of nodes. We describe one such case study for validation purposes. The approach has been fully automated in a commercial tool, called AgenaRisk, and is thus accessible to all types of domain experts. We believe that this work represents a useful contribution to the BN research and technology, since its application makes the difference between being able to build realistic BN models and not.","1041-4347;1558-2191;2326-3865","","10.1109/TKDE.2007.1068","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4288143","Ontology;Petri net;Reasoning;Association rule;Data mining","Petri nets;Production;Object oriented modeling;Ontologies;Fuzzy systems;Intelligent systems;Knowledge representation;Knowledge based systems;Association rules;Artificial intelligence","belief networks;normal distribution;probability;risk analysis;statistical testing","ranked node;Bayesian network model;real-world risk problem;node probability table testing;doubly truncated normal distribution;AgenaRisk software tool;probability elicitation","","17","40","","","","","","IEEE","IEEE Journals & Magazines"
"Profit based unit commitment problem under deregulated environment","I. J. Raglend; R. Kumar; S. P. Karthikeyan; K. Palanisamy; D. P. Kothari","Principal, Christian College of Engineering and Technology, Oddanchatiram, Dindugal District, TamilNadu, India; School of Electrical Sciences, Vellore Institute of Technology, Tamil Nadu India-632014; School of Electrical Sciences, Vellore Institute of Technology, Tamil Nadu, India-632014; School of Electrical Sciences, Vellore Institute of Technology, Tamil Nadu, India-632014; Vellore Institute of Technology, India - 632014","2009 Australasian Universities Power Engineering Conference","","2009","","","1","6","In this paper, an algorithm to solve the profit based unit commitment problem under deregulated environment has been proposed to determine the optimal generation schedule with maximum profit. Deregulation in power sector increases the efficiency of electricity production and distribution, offer lower prices, higher quality, a secure and a more reliable product. The proposed algorithm is developed from the view point of a generation company wishing to maximize a profit as a participant in the deregulated power and reserve markets. Unit commitment (UC) is an optimization problem of determining the schedule of generating units within a power system with a number of constraints. Unit commitment schedule depends on the market price in the deregulated market. More number of units are committed when the market price is higher. When more number of generating units are brought online more power is generated and participated in the deregulated market to get maximum profit. This paper presents a new approach of GENCOs profit based unit commitment in a day ahead competitive electricity markets. Generations, spinning reserve, non spinning reserve and system constraints are considered in the proposed formulation. The proposed approach has been tested on IEEE-30 bus system with 6 generating units as an individual GENCO. The results obtained are quite encouraging and useful in deregulated market. The algorithm and simulation are carried out using Matlab software.","","978-1-4244-5153-1978-0-86396-718","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5356610","Deregulation;Market Price;Profit based Unit Commitment;GENCO;Dynamic Programming;Economic Dispatch","Electricity supply industry deregulation;Power generation;Power system reliability;Spinning;Scheduling algorithm;Production;Constraint optimization;Electricity supply industry;System testing;Software algorithms","power generation scheduling;power markets","profit based unit commitment problem;deregulated environment;optimal generation schedule;GENCOs profit based unit commitment;electricity markets;IEEE-30 bus system;Matlab software","","","18","","","","","","IEEE","IEEE Conferences"
"Architectural design of a sensory node controller for optimized energy utilization in sensor networks","R. X. Gao; Zhaoyan Fan","Dept. of Mech. & Ind. Eng., Univ. of Massachusetts, Amherst, MA, USA; Dept. of Mech. & Ind. Eng., Univ. of Massachusetts, Amherst, MA, USA","IEEE Transactions on Instrumentation and Measurement","","2006","55","2","415","428","The increasing complexity of manufacturing machines and the continued demand for high productivity have led to growing applications of sensor networks to enable more reliable, timely, and comprehensive information gathering from the machines being monitored. An effective and efficient utilization of sensor networks requires new sensor designs that enable adaptive event-driven information gathering based on the condition of the machines, as well as a coordinated information distribution adjusted to the available communication bandwidth of the network. This paper investigates several fundamental aspects regarding the architectural design of a sensory node controller (SNOC). The SNOC is the key element in a large-scale sensor network that coordinates the operation of individual sensors and the communication among various sensing clusters to realize distributed intelligent sensing. A parametric SNOC design that dynamically adjusts the power supply and the data-acquisition procedure to reduce the overall energy consumption of the sensor network is presented. Considerations on both the hardware and software aspects of the design to achieve energy efficiency are described, and analytical formulations are derived. Simulation results for a sensor network consisting of 40 SNOCs, each coordinating eight physical sensors, have shown that the design is able to reduce the energy consumption by about 43%, as compared to traditional techniques. A prototype SNOC was designed and implemented, based on the platform of a commercially available microcontroller, and experimentally tested for its ability to dynamically adjust the power consumption. The study has provided a concrete input to the design optimization and experimental realization of an SNOC-based sensor network for machine-system monitoring.","0018-9456;1557-9662","","10.1109/TIM.2006.870321","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1608583","Machine-system monitoring;power efficiency;sensor networks;system-on-a-chip;vibration signal processing","Design optimization;Intelligent sensors;Energy consumption;Condition monitoring;Manufacturing;Productivity;Telecommunication network reliability;Bandwidth;Communication system control;Large-scale systems","intelligent sensors;wireless sensor networks;microcontrollers;system-on-chip;computerised monitoring;data acquisition","architectural design;sensory node controller;optimized energy utilization;sensor designs;adaptive event-driven information gathering;coordinated information distribution;network communication bandwidth;large-scale sensor network;sensing clusters;distributed intelligent sensing;power supply;data-acquisition procedure;energy consumption;microcontroller;machine-system monitoring;system-on-a-chip;vibration signal processing","","26","28","","","","","","IEEE","IEEE Journals & Magazines"
"Transmission systems power quality monitors allocation","D. C. S. Reis; P. R. C. Villela; C. A. Duque; P. F. Ribeiro","Energy Department of UFJF, Juiz de Fora MG, Brazil; Electrical Circuits Department of UFJF, Juiz de Fora MG, Brazil; Electrical Circuits Department of UFJF, Juiz de Fora MG, Brazil; CAPS/FSU, USA","2008 IEEE Power and Energy Society General Meeting - Conversion and Delivery of Electrical Energy in the 21st Century","","2008","","","1","7","This work develops and tests a branch and bound algorithm for solving optimum allocation of power quality monitors in a transmission power system. The optimization problem is solved by using 0-1 integer programming techniques and depends highly on network topology. The algorithm, which is implemented in Matlab software, minimizes the total cost of the monitoring system and found the optimum number and locations for monitors on the network studied, under a set of given network observability constraints. Case studies are presented for IEEE test networks and for CEMIG actual transmission power systems. Current and voltage values are estimated by using monitored variables to validate the obtained results.","1932-5517","978-1-4244-1905-0978-1-4244-1906","10.1109/PES.2008.4596309","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4596309","power quality;power quality monitoring;power quality monitors allocation;combinatorial optimization;branch-and-bound","Monitoring;Mathematical model;Transmission line matrix methods;Power systems;Resource management;Power quality;Transmission line measurements","integer programming;power supply quality;power system measurement;power transmission","transmission systems;power quality monitoring;optimum allocation;optimization;integer programming techniques;network topology;Matlab software;IEEE test networks;CEMIG actual transmission power systems","","16","21","","","","","","IEEE","IEEE Conferences"
"Component Based Performance Modelling of Wireless Routing Protocols","J. S. Baras; V. Tabatabaee; P. Purkayastha; K. Somasundaram","NA; NA; NA; NA","2009 IEEE International Conference on Communications","","2009","","","1","6","We propose a component based methodology for modelling and design of wireless routing protocols. Componentization is a standard methodology for analysis and synthesis of complex systems, or software. The feasibility of the component based design relies heavily on the compositionality property (i.e. system-level properties can be computed from properties of components). To provide a component based design methodology and to test compositionality for routing protocols, we have to develop a component based model of the wireless network. We present the main components of the routing protocol that should be modelled and focus on three main components: neighborhood discovery, selector of topology information to disseminate, and the path selection components. For each component, we identify the inputs, outputs, and a generic methodology for modelling. Throughout the paper, we use the Optimized Link State Routing (OLSR) protocol as a case study to demonstrate the effectiveness of our approach. Using the neighborhood discovery component, we present our design methodology and design a modified enhanced version of this component, and compare its performance to the original OLSR design.","1550-3607;1938-1883","978-1-4244-3435","10.1109/ICC.2009.5198840","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5198840","","Routing protocols;Wireless networks;Measurement;Cross layer design;Design methodology;Design optimization;Software systems;System testing;Network synthesis;Communications Society","radio networks;routing protocols;telecommunication network topology","component based performance modelling;wireless routing protocols;complex systems;topology information;generic methodology;optimized link state routing protocol","","4","7","","","","","","IEEE","IEEE Conferences"
"Petri-nets for formal verification of MAC protocols","R. J. Haines; G. R. Clemo; A. T. D. Munro","NA; NA; NA","IET Software","","2007","1","2","39","47","Full or partial reconfiguration of communications devices offers both optimised performance for niche scenario-specific deployments and support for de-regulated radio spectrum management. The correctness of the protocols or protocol-enhancements being deployed in such a dynamic and autonomous manner cannot easily be determined through traditional testing techniques. Formal description techniques are a key verification technique for protocols. The Petri-net formal description technique offers the best combination of intuitive representation, tool-support and analytical capabilities. Having described key features and analytical approaches of Reference-nets (an extended Petri-net formalism), a case study is presented applying this approach to a contemporary research area: IEEE 802.11 centralised control mechanisms to support delay-sensitive streams and bursty data traffic. This case study showcases the ability both to generate performance-oriented simulation results and to determine more formal correctness properties. The simulation results allow comparison with published results and show that a packet-expiration mechanism places greater demands on the contention-free resource allocation, while the mathematical analysis of the model reveals it to be free of deadlock and k-bounded with respect to resources. The work demonstrates the potential that the Petri-net formal method has for analysing process and protocol models to support reconfigurable devices","1751-8806;1751-8814","","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4197557","","","access protocols;formal verification;Petri nets","Petri nets;formal verification;MAC protocols;communications device reconfiguration;optimised performance;niche scenario-specific deployments;de-regulated radio spectrum management;formal description techniques;Reference-nets;extended Petri-net formalism;IEEE 802.11 centralised control mechanisms;delay sensitive streams;formal correctness properties;packet-expiration mechanism;contention-free resource allocation;mathematical analysis;reconfigurable devices;bursty data traffic","","1","","","","","","","IET","IET Journals & Magazines"
"Grey Model for Time-Dependent Vertical Ultimate Bearing Capacity of Concrete Pile in Soft Soil Area","W. Wang; T. Xia","NA; NA","2009 International Conference on Computational Intelligence and Software Engineering","","2009","","","1","4","Determining of vertical ultimate bearing capacity (VUBC) of concrete pile is very important to design and management of geotechnical engineering in soft soil area. However, it is not well solved because the VUBC increases with time after pile installation. In this paper, conventional model for time-VUBC relationships is introduced, and one new grey model is proposed to predict time-VUBC relationships based on optimized field investigation data. Then, correctness and applicability of the grey model are analyzed. The proposed grey model can instead of long-time field investigation with short-time investigation, which can save both cost and time for engineering construction. Finally, good agreements have been found between field investigations and grey model simulations. Results of this study can put good foundation for design and management of corresponding geotechnical engineering.","","978-1-4244-4507","10.1109/CISE.2009.5364072","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5364072","","Concrete;Soil;Testing;Mathematical model;Cities and towns;Engineering management;Design engineering;Predictive models;Reliability engineering;Civil engineering","concrete;foundations;geotechnical engineering;soil;structural engineering computing","Grey model;time-dependent vertical ultimate bearing capacity;soft soil area;concrete pile;geotechnical engineering;field investigation","","","10","","","","","","IEEE","IEEE Conferences"
"The PRAGMA Testbed - Building a Multi-Application International Grid","C. Zheng; D. Abramson; P. Arzberger; S. Ayyub; C. Enticott; S. Garic; M. J. Katz; Jae-Hyuck Kwak; Bu Sung Lee; P. M. Papadopoulos; S. Phatanapherom; S. Sriprayoonsakul; Y. Tanaka; Y. Tanimura; O. Tatebe; P. Uthayopas","San Diego Supercomputer Center, USA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA","Sixth IEEE International Symposium on Cluster Computing and the Grid (CCGRID'06)","","2006","2","","57","57","This practices and experience paper describes the coordination, design, implementation, availability, and performance of the Pacific Rim Applications and Grid Middleware Assembly (PRAGMA) grid testbed. Applications in high-energy physics, genome annotation, quantum computational chemistry, wildfire simulation, and protein sequence alignment have driven the middleware requirements, and the testbed provides a mechanism for international users to share software beyond the essential, de facto standard Globus core. In this paper, we describe how human factors, resource availability and performance issues have affected the middleware, applications and the testbed design. We also describe how middleware components in grid monitoring, grid accounting, grid remote procedure calls, grid-aware file systems, and grid-based optimization have dealt with some of the major characteristics of our testbed. We also briefly describe a number of mechanisms that we have employed to make software more easily available to testbed administrators","","0-7695-2585","10.1109/CCGRID.2006.1630948","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1630948","","Middleware;Application software;Availability;Quantum computing;Software testing;Buildings;Assembly;Physics;Genomics;Bioinformatics","grid computing;middleware","PRAGMA grid testbed;Pacific Rim Applications and Grid Middleware Assembly;multiapplication international grid;high-energy physics;genome annotation;quantum computational chemistry;wildfire simulation;protein sequence alignment;middleware requirements;Globus core;grid monitoring;grid accounting;grid remote procedure calls;grid-aware file systems;grid-based optimization","","11","17","","","","","","IEEE","IEEE Conferences"
"A Fast Learning Algorithm with Transductive Support Vector Machine","J. Xie; H. Dong; M. Li; G. Liu","NA; NA; NA; NA","2009 International Conference on Computational Intelligence and Software Engineering","","2009","","","1","4","Transductive inference based on support vector machine is a new research region in statistical learning theory. An improved algorithm is proposed in this paper, which overcome the disadvantages of studying process complexity and slow in the progressive transductive support vector machine learning algorithm. The algorithm optimized the samples which near the support vector only, and large number of samples were reduced, so the speed of algorithm is improved. Experiments show that the speed of this algorithm is improved with little influence on the performance.","","978-1-4244-4507","10.1109/CISE.2009.5365411","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5365411","","Machine learning;Support vector machines;Support vector machine classification;Machine learning algorithms;Inference algorithms;Laboratories;Nondestructive testing;Statistical learning;Pattern recognition;Learning systems","inference mechanisms;learning (artificial intelligence);statistical analysis;support vector machines","fast learning algorithm;transductive support vector machine;transductive inference;statistical learning theory;process complexity;machine learning","","","10","","","","","","IEEE","IEEE Conferences"
"A Linear Mean-Field Annealing Algorithm Solving Weapon Target Assignment Problem","C. Liu; H. Wang; Z. Qiu","NA; NA; NA","2009 International Conference on Computational Intelligence and Software Engineering","","2009","","","1","4","Traditional optimization algorithms have been applied to solve the Weapon Target Assignment (WTA) problem, but these algorithms usually solve small size instances. In this paper, we propose a linear mean-field algorithm with Hopfield network to solve large size assignments. This kind of algorithm that combines the advantage of neural network and simulated annealing can quickly converge to a global optimum solution. Compared with stochastic simulate annealing algorithm and mean-field annealing algorithm, some experimental results testify its feasibility.","","978-1-4244-4507","10.1109/CISE.2009.5365022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5365022","","Weapons;Simulated annealing;Stochastic processes;Educational institutions;Neural networks;Testing;Fires;Linear programming;Operations research;Hopfield neural networks","Hopfield neural nets;military computing;simulated annealing","weapon target assignment problem;neural network;simulated annealing;linear mean-field algorithm;Hopfield network","","","8","","","","","","IEEE","IEEE Conferences"
"Implementation of the Mathematical Model for Improving the Inter-instrument Agreement of Spectrocolorimeters","X. Wan; X. Huang","NA; NA","2008 International Conference on Computer Science and Software Engineering","","2008","6","","34","37","Implementation of the mathematical model for improving the inter-instrument agreement of spectrocolorimeters was analyzed, based on the problems of the model in the practical application, combined with characteristics of spectrocolorimeters in color measurement and spectral properties of surface color. Firstly the selection algorithm of standard specimens is presented based on the influence of standard specimens¿ spectral characteristic to the correct coefficients. Secondly the derivative solution of first-order and second-order derivative of spectral reflectance and the numerical solution of correction coefficients are optimized and evaluated. All of those form an improved implement method of the model. Lastly the implement method is more scientific and reasonable are proven by examples.","","978-0-7695-3336","10.1109/CSSE.2008.1418","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4723190","reflectance;Inter-instrument Agreement;Spectrocolorimeters","Mathematical model;Instruments;Wavelength measurement;Error correction;Reflectivity;Testing;Printing;Measurement standards;Color;Manufacturing","colorimeters;spectroscopy computing","mathematical model;interinstrument agreement;spectrocolorimeters;spectral reflectance;correction coefficients","","","10","","","","","","IEEE","IEEE Conferences"
"Autofocusing for Automated Microscopic Evaluation of Blood Smear and Pap Smear","X. Y. Liu; W. H. Wang; Y. Sun","Student Member, IEEE, Advanced Micro and Nanosystems Laboratory, University of Toronto. 5 King's College Road, Toronto, Ontario, Canada, M5S 3G8; Advanced Micro and Nanosystems Laboratory, University of Toronto. 5 King's College Road, Toronto, Ontario, Canada, M5S 3G8; Member, IEEE, Advanced Micro and Nanosystems Laboratory, University of Toronto. 5 King's College Road, Toronto, Ontario, Canada, M5S 3G8. phone: 1-416-946-0549; e-mail: sun@mie.utoronto.ca","2006 International Conference of the IEEE Engineering in Medicine and Biology Society","","2006","","","4718","4721","Autofocusing is a fundamental procedure towards automated microscopic evaluation of blood smear and pap smear samples for clinical diagnosis. This paper presents comparison results of 16 focus algorithms based on a total of 8000 bright-field images from 10 blood smear and pap smear samples. A ranking methodology adapted from our previously proposed ranking system is used for thoroughly evaluating the performance of the selected 16 focus algorithms. Experimental results demonstrate that the variance algorithm provides the best overall performance, which will be selected for our future implementation of an automated microscopic system for computer-assisted blood smear and pap smear evaluation. Together with our previously reported findings, we hypothesize that the variance algorithm or the Normalized variance algorithm is the optimal focus algorithm for all non- fluorescence microscopy applications","1557-170X","1-4244-0032-51-4244-0033","10.1109/IEMBS.2006.259263","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4462855","","Microscopy;Blood;Focusing;Hospitals;Cameras;Diseases;Cities and towns;USA Councils;Sun;Clinical diagnosis","biomedical measurement;blood;medical computing","autofocusing;automated microscopic evaluation;clinical diagnosis;variance algorithm;computer-assisted blood smear evaluation;computer-assisted pap smear evaluation;normalized variance algorithm;optimal focus algorithm;nonfluorescence microscopy","Algorithms;Artificial Intelligence;Automatic Data Processing;Automation;Equipment Design;Female;Humans;Image Enhancement;Image Interpretation, Computer-Assisted;Microscopy;Models, Statistical;Papanicolaou Test;Pattern Recognition, Automated;Software;Uterine Cervical Diseases;Vaginal Smears","3","15","","","","","","IEEE","IEEE Conferences"
"Real-world performance of current proactive multi-hop mesh protocols","M. Abolhasan; B. Hagelstein; J. C. -. Wang","Information & Communication Technology Research Institute, University of Wollongong, Wollongong, NSW 2522, Australia; Information & Communication Technology Research Institute, University of Wollongong, Wollongong, NSW 2522, Australia; Information & Communication Technology Research Institute, University of Wollongong, Wollongong, NSW 2522, Australia","2009 15th Asia-Pacific Conference on Communications","","2009","","","44","47","The proliferation of mesh or ad hoc network protocols has lead to a push for protocol standardisation. While there are a number of both open-source and proprietary mesh routing protocols being developed, there is only a small amount of literature available that shows relative strengths and weaknesses of different protocols. This paper investigates the performance of a number of available routing protocols using a real-world testbed. Three routing protocols - optimised link state routing (OLSR), better approach to mobile ad hoc network (B.A.T.M.A.N.) and BABEL - were chosen for this study. Our investigations focus on the multi-hopping performance and the ability of each routing protocol to recover from link failures. Our results show that B.A.T.M.A.N. and BABEL outperform OLSR both in terms of multi-hopping performance and in route re-discovery latency.","2163-0771","978-1-4244-4784-8978-1-4244-4785","10.1109/APCC.2009.5375690","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5375690","","Routing protocols;Mesh networks;Testing;Ad hoc networks;Open source software;Delay;Bandwidth;Convergence;Costs;Standards development","ad hoc networks;mobile radio;routing protocols","multihop mesh protocols;real world performance;mesh proliferation;ad hoc network protocols;protocol standardisation;mesh routing protocols;optimised link state routing;mobile ad hoc network","","29","10","","","","","","IEEE","IEEE Conferences"
"Application of Sensitivity Pruning Neural Networks in Surface Roughness Prediction","W. Wu; Z. Yuan-min; W. Hong-ling","NA; NA; NA","2009 Second International Conference on Intelligent Computation Technology and Automation","","2009","1","","48","51","The surface roughness is a key parameters in high speed machining and often hard to control. The prediction model for surface roughness was created based on artificial neural networks which have strong non-linear modeling ability. The sample data collection method was analyzed and BP neural networks was designed, but the traditional BP neural networks has many shortcomings like easily step into local minimum, with weak generalization ability and the middle layer neuron are hard to determine, so the sensitivity pruning algorithm applied. The simulation shows the method is effective and can provide a guidance to optimize cutting parameters and control surface quality.","","978-0-7695-3804","10.1109/ICICTA.2009.20","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5287710","","Neural networks;Rough surfaces;Surface roughness;Predictive models;Artificial neural networks;Iterative algorithms;Surface resistance;Software testing;Algorithm design and analysis;Mathematical model","backpropagation;control system synthesis;cutting;machining;minimisation;neurocontrollers;nonlinear control systems;quality control;sampling methods;surface roughness","sensitivity pruning artificial neural network algorithm;surface roughness prediction model;high-speed machining control;nonlinear modeling;BP neural network design;sample data collection method;local minimum;generalization ability;middle-layer neuron;cutting parameter optimization;surface quality control","","1","26","","","","","","IEEE","IEEE Conferences"
"AnalyseC: A Framework for Assessing Students' Programs at Structural and Semantic Level","W. Wu; G. Li; Y. Sun; J. Wang; T. Lai","Faculty of Computer, Guangdong University of Technology, Guangzhou, China. wuwm@scnu.edu.cn; Faculty of Computer, Guangdong University of Technology, Guangzhou, China. arieslgq@gmail.com; Faculty of Computer, Guangdong University of Technology, Guangzhou, China. sunyiyu_2004@l63.com; Faculty of Computer, Guangdong University of Technology, Guangzhou, China. wj_adr@163.com; Faculty of Computer, Guangdong University of Technology, Guangzhou, China. lazyboylai@21cn.com","2007 IEEE International Conference on Control and Automation","","2007","","","742","747","In this paper, new features of AnalyseC are proposed and implemented to assess student programming assignments in order to lighten teachers' assessment work and assist students in learning. Applied with software metrics, compiler optimization and visualization techniques, AnalyseC has been designed to automatically assess programming assignments written in C language at structural and semantic level. It is designed as a auxiliary tool for students in the data structure assignment system and has been implemented in Java. AnalyseC tested with specific data structure programming assignments, the test result is promising.","1948-3449;1948-3457","978-1-4244-0817-7978-1-4244-0818","10.1109/ICCA.2007.4376454","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376454","assessment;structural;semantic;visualization","Data structures;Software metrics;Automatic programming;Data visualization;Testing;Automatic control;Lighting control;Automation;Sun;Program processors","computer science education;educational administrative data processing;Java;programming","AnalyseC;student program assessment;student programming assignments;software metrics;compiler optimization;visualization techniques;C language;data structure assignment system;Java","","3","13","","","","","","IEEE","IEEE Conferences"
"An experimental investigation of VoIP and video streaming over fixed WiMAX","K. Pentikousis; J. Pinola; E. Piri; F. Fitzek","VTT Technical Research Centre of Finland, Kaitoväylä 1, FI-90571 Oulu, FINLAND; VTT Technical Research Centre of Finland, Kaitoväylä 1, FI-90571 Oulu, FINLAND; VTT Technical Research Centre of Finland, Kaitoväylä 1, FI-90571 Oulu, FINLAND; VTT Technical Research Centre of Finland, Kaitoväylä 1, FI-90571 Oulu, FINLAND","2008 6th International Symposium on Modeling and Optimization in Mobile, Ad Hoc, and Wireless Networks and Workshops","","2008","","","8","15","Despite the significant interest in WiMAX technology and deployment, there are very few publicly reported measurements from testbeds and field trials. As such, most WiMAX studies employ simulation and modeling. This paper contributes to our understanding of what is realistically possible using off-the-shelf fixed WiMAX equipment today. We employ multiple competing traffic sources over a point-to-multipoint WiMAX topology and measure the capacity of the WiMAX equipment to handle a multitude of VoIP flows between subscriber stations while delivering a variable number of video streams. We measure throughput, packet loss, and one-way delay for both line-of-sight (LOS) and non-line-of-sight (NLOS) conditions. For the one-way delay measurements we synchronize the clocks of all testbed hosts with a software-only, open source implementation of the IEEE 1588 Precision Time Protocol. We compare these one-way delay measurements with those obtained when GPS-based synchronization is used.","","978-963-9799-18","10.1109/WIOPT.2008.4586026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4586026","","Streaming media;WiMAX;Synchronization;Testing;Traffic control;Topology;Loss measurement;Throughput;Delay lines;Delay effects","Internet telephony;performance evaluation;synchronisation;telecommunication equipment;telecommunication network topology;telecommunication traffic;video streaming;WiMax","VoIP flows;video streaming;off-the-shelf fixed WiMAX equipment;multiple competing traffic sources;point-to-multipoint WiMAX topology;subscriber stations;packet loss;throughput measurement;one-way delay measurement;line-of-sight conditions;nonline-of-sight conditions;clock synchronization;software-only open source implementation;IEEE 1588 Precision Time Protocol;performance evaluation","","10","30","","","","","","IEEE","IEEE Conferences"
"Proposal of a data sparsification unit for a mixed-mode MAPS detector","A. Gabrielli; G. Batignani; S. Bettarini; F. Bosi; G. Calderini; R. Cenci; M. Dell'Orso; F. Forti; P. Giannetti; M. A. Giorgi; A. Lusiani; G. Marchiori; F. Morsani; N. Neri; E. Paoloni; G. Rizzo; J. Walsh; M. Massa; A. Cervelli; C. Andreoli; E. Pozzati; L. Ratti; V. Speziali; M. Manghisoni; V. Re; G. Traversi; L. Bosisio; G. Giacomini; L. Lanceri; I. Rachevskaia; L. Vitale; G. F. Dalla Betta; G. Soncini; G. Fontana; L. Pancheri; G. Verzellesi; D. Gamba; G. Giraudo; P. Mereu; M. Bruschi; B. Giacobbe; N. Semprini; R. Spighi; M. Villa; A. Zoccoli","Università di Bologna and INFN-Bologna, Italy; University degli Studi di Pisa, INFN-Pisa and Scuola Normale Superiore, Italy; University degli Studi di Pisa, INFN-Pisa and Scuola Normale Superiore, Italy; University degli Studi di Pisa, INFN-Pisa and Scuola Normale Superiore, Italy; University degli Studi di Pisa, INFN-Pisa and Scuola Normale Superiore, Italy; University degli Studi di Pisa, INFN-Pisa and Scuola Normale Superiore, Italy; University degli Studi di Pisa, INFN-Pisa and Scuola Normale Superiore, Italy; University degli Studi di Pisa, INFN-Pisa and Scuola Normale Superiore, Italy; University degli Studi di Pisa, INFN-Pisa and Scuola Normale Superiore, Italy; University degli Studi di Pisa, INFN-Pisa and Scuola Normale Superiore, Italy; University degli Studi di Pisa, INFN-Pisa and Scuola Normale Superiore, Italy; University degli Studi di Pisa, INFN-Pisa and Scuola Normale Superiore, Italy; University degli Studi di Pisa, INFN-Pisa and Scuola Normale Superiore, Italy; University degli Studi di Pisa, INFN-Pisa and Scuola Normale Superiore, Italy; University degli Studi di Pisa, INFN-Pisa and Scuola Normale Superiore, Italy; University degli Studi di Pisa, INFN-Pisa and Scuola Normale Superiore, Italy; University degli Studi di Pisa, INFN-Pisa and Scuola Normale Superiore, Italy; University degli Studi di Pisa, INFN-Pisa and Scuola Normale Superiore, Italy; University degli Studi di Pisa, INFN-Pisa and Scuola Normale Superiore, Italy; University degli Studi di Pavia and INFN, Italy; University degli Studi di Pavia and INFN, Italy; University degli Studi di Pavia and INFN, Italy; University degli Studi di Pavia and INFN, Italy; University degli Studi di Bergamo and INFN-Pavia, Italy; University degli Studi di Bergamo and INFN-Pavia, Italy; University degli Studi di Bergamo and INFN-Pavia, Italy; University degli Studi di Trieste and INFN, Italy; University degli Studi di Trieste and INFN, Italy; University degli Studi di Trieste and INFN, Italy; University degli Studi di Trieste and INFN, Italy; University degli Studi di Trieste and INFN, Italy; University di Trento and INFN-Padova, Italy; University di Trento and INFN-Padova, Italy; University di Trento and INFN-Padova, Italy; University di Trento and INFN-Padova, Italy; University di Modena e Reggio and INFN-Padova, Italy; University di Torino and INFN, Italy; University di Torino and INFN, Italy; University di Torino and INFN, Italy; Università di Bologna and INFN-Bologna, Italy; Università di Bologna and INFN-Bologna, Italy; Università di Bologna and INFN-Bologna, Italy; Università di Bologna and INFN-Bologna, Italy; Università di Bologna and INFN-Bologna, Italy; Università di Bologna and INFN-Bologna, Italy","2007 IEEE Nuclear Science Symposium Conference Record","","2007","2","","1471","1473","The Italian silicon-detectors-with-low-interaction-with material collaboration (SLIM5) has designed, fabricated and tested several prototypes of CMOS monolithic active pixel sensors (MAPS). This paper shows the design of a new mixed-mode chip prototype composed of a bidimensional matrix of pixels, and of an off-pixel digital readout sparsification circuit. The readout logic is based on commercial standard cells and implements an optimized non token readout technique. Also, a MAPS emulator software toool is presented. The project is aimed at overcoming the readout speed limit of future large-matrix pixel detectors for particle tracking, by matching the requirements of future high-energy physics experiments. The readout architecture extends the flexibility of the MAPS devices to be also used in first level triggers on tracks in vertex detectors.","1082-3654","978-1-4244-0922-8978-1-4244-0923","10.1109/NSSMIC.2007.4437277","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4437277","","Proposals;Detectors;Prototypes;Collaboration;Circuit testing;Materials testing;Software prototyping;CMOS logic circuits;Particle tracking;Physics","CMOS integrated circuits;digital readout;mixed analogue-digital integrated circuits;nuclear electronics;particle tracks;position sensitive particle detectors;trigger circuits","Silicon-detectors-with-Low-Interaction-with Material collaboration;SLIM5;Monolithic Active Pixel Sensors;MAPS detector;bidimensional matrix;off-pixel digital readout sparsification circuit;standard cells;emulator software tool;particle tracking;high-energy physics experiments;readout architecture;level trigger system;vertex detectors;CMOS;mixed-mode ASIC","","20","4","","","","","","IEEE","IEEE Conferences"
"An Efficient Experimental Methodology for Configuring Search-Based Design Algorithms","S. Poulding; P. Emberson; I. Bate; J. Clark","NA; NA; NA; NA","10th IEEE High Assurance Systems Engineering Symposium (HASE'07)","","2007","","","53","62","Many problems in high assurance systems design are only tractable using computationally expensive search algorithms. For these algorithms to be useful, designers must be provided with guidance as to how to configure the algorithms appropriately. This paper presents an experimental methodology for deriving such guidance that remains efficient when the algorithm requires substantial computing resources or takes a long time to find solutions. The methodology is shown to be effective on a highly-constrained task allocation algorithm that provides design solutions for high integrity systems. Using the methodology, an algorithm configuration is derived in a matter of days that significantly outperforms one resulting from months of 'trial-and-error' optimisation.","1530-2059","0-7695-3043-5978-0-7695-3043","10.1109/HASE.2007.27","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4404727","","Algorithm design and analysis;Systems engineering and theory;Cost function;Modeling;Simulated annealing;Design engineering;Computer science;Electronic mail;Design optimization;Testing","quality assurance;software quality","search-based design algorithms;high assurance systems design;computationally expensive search algorithms;substantial computing resources;highly-constrained task allocation algorithm;high integrity systems;trial-and-error optimisation","","4","24","","","","","","IEEE","IEEE Conferences"
"Embedded system of temperature testing based on DS18B20","H. Shen; J. Fu; Z. Chen","Institute of Manufacturing Engineering, Zhejiang University, Hangzhou, 310027, China; Institute of Manufacturing Engineering, Zhejiang University, Hangzhou, 310027, China; Institute of Manufacturing Engineering, Zhejiang University, Hangzhou, 310027, China","2006 International Technology and Innovation Conference (ITIC 2006)","","2006","","","2223","2226","The paper introduces a new system of temperature testing. It uses ARM S3C44BOX as its microcontroller, with the 1-Wire digital thermometer DS18B20 as a temperature sensor. This paper introduces the temperature testing principle of DS18B20, the design of hardware and software, and the optimized arithmetic of Search ROM command, which is most important in the software realization.","0537-9989","0-86341-696","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4752381","Temperature;S3C44BOX;DS18B20;route-remember arithmetic","","","","","5","","","","","","","IET","IET Conferences"
"Optimizing Cauchy Reed-Solomon Codes for Fault-Tolerant Network Storage Applications","J. S. Plank; Lihao Xu","University of Tennessee, USA; NA","Fifth IEEE International Symposium on Network Computing and Applications (NCA'06)","","2006","","","173","180","In the past few years, all manner of storage applications, ranging from disk array systems to distributed and wide-area systems, have started to grapple with the reality of tolerating multiple simultaneous failures of storage nodes. Unlike the single failure case, which is optimally handled with RAID level-5 parity, the multiple failure case is more difficult because optimal general purpose strategies are not yet known. Erasure coding is the field of research that deals with these strategies, and this field has blossomed in recent years. Despite this research, the decades-old Reed-Solomon erasure code remains the only space-optimal (MDS) code for all but the smallest storage systems. The best performing implementations of Reed-Solomon coding employ a variant called Cauchy Reed-Solomon coding, developed in the mid 1990's. In this paper, we present an improvement to Cauchy Reed-Solomon coding that is based on optimizing the Cauchy distribution matrix. We detail an algorithm for generating good matrices and then evaluate the performance of encoding using all implementations Reed-Solomon codes, plus the best MDS codes from the literature. The improvements over the original Cauchy Reed-Solomon codes are as much as 83% in realistic scenarios, and average roughly 10% over all cases that we tested","","0-7695-2640","10.1109/NCA.2006.43","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1659489","","Reed-Solomon codes;Fault tolerance;Peer to peer computing;Fault tolerant systems;Application software;Encoding;Computer science;Decoding;Testing;Data structures","fault tolerant computing;RAID;Reed-Solomon codes;storage management;wide area networks","Cauchy Reed-Solomon codes;fault-tolerant network storage applications;disk array system;distributed system;wide-area system;RAID level-5 parity;space-optimal code;Cauchy distribution matrix;MDS codes","","72","35","","","","","","IEEE","IEEE Conferences"
"High performance quadratic classifier and the application on pendigits recognition","Zheng Yi; J. Zhao; Jie Sun; S. S. Ge","Nat. Univ. of Singapore, Singapore; Nat. Univ. of Singapore, Singapore; NA; NA","2007 46th IEEE Conference on Decision and Control","","2007","","","3072","3077","A nonconvex quadratic classifier is proposed for pattern recognition. The classifier is obtained by solving a second-order cone optimization problem on the training data set. Numerical results are presented to compare this classifier with the Gaussian classifier and k-NN classifiers. Regarding to the application of hand written digits recognition, the computational result shows that the proposed quadratic classifier always achieves highest correctness in the testing stage although it takes the longest computational time in the training stage.","0191-2216","978-1-4244-1497-0978-1-4244-1498","10.1109/CDC.2007.4434191","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4434191","second-order cone optimization;pattern recognition;pen-digit recognition;quadratic programming","Pattern recognition;Testing;Application software;USA Councils;High performance computing;Sampling methods;Bayesian methods;Mathematical model;Gaussian distribution;Sun","concave programming;Gaussian processes;handwriting recognition;pattern recognition;quadratic programming","pendigits recognition;nonconvex quadratic classifier;pattern recognition;second-order cone optimization;Gaussian classifier;k-NN classifier;hand written digits recognition;quadratic programming","","1","9","","","","","","IEEE","IEEE Conferences"
"OpenMP Parallel Optimal Path Algorithm and its Performance Analysis","H. Cao; F. Wang; X. Fang; H. Tu; J. Shi","NA; NA; NA; NA; NA","2009 WRI World Congress on Software Engineering","","2009","1","","61","66","With the research and application of intelligent transportation system, there is a higher requirement for solving the optimal path problem in large scale transportation networks in real time. In order to get the effective optimal path algorithms for the actual transportation networks, three optimal path algorithms are chosen to be parallelized. The parallel optimal path algorithms are then implemented based on dual-core processor and the speed-up ratio and efficiency of parallel algorithms are tested and analyzed in actual Xi'an road networks with 4525 nodes and 6616 paths. The experiment results show that the parallel algorithms devised in this paper are efficient and the speed-up ratio of the three parallel algorithms are satisfied in solving optimal path problems in large scale networks.","","978-0-7695-3570","10.1109/WCSE.2009.107","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5318935","Dijkstra algorithm;A* algorithm;parallel algorithm;openMP;multi-core","Performance analysis;Yarn;Parallel algorithms;Concurrent computing;Dynamic scheduling;Processor scheduling;Distributed computing;Algorithm design and analysis;Application software;Intelligent transportation systems","application program interfaces;automated highways;graph theory;multi-threading;optimisation;parallel algorithms;transportation","OpenMP parallel optimal path algorithm;performance analysis;intelligent transportation system;large scale transportation network;dual-core processor;speed-up ratio;Xi'an road network;multithreading;Dijkstra algorithm","","2","9","","","","","","IEEE","IEEE Conferences"
"Experiences with multi-threading and dynamic class loading in a Java just-in-time compiler","V. Sundaresan; D. Maier; P. Ramarao; M. Stoodley","IBM Canada Ltd, Markham, Ont., Canada; IBM Canada Ltd, Markham, Ont., Canada; IBM Canada Ltd, Markham, Ont., Canada; IBM Canada Ltd, Markham, Ont., Canada","International Symposium on Code Generation and Optimization (CGO'06)","","2006","","","11 pp.","97","In this paper, we describe the techniques that have been implemented in the IBM TestaRossa (TR) just-in-time (JIT) compiler to safely perform aggressive code patching and collect accurate profiles in the context of a Java application employing multiple threads and dynamic class loading and unloading. Previous work in these areas either did not account for the synchronization cost of safety or dynamic class loading/unloading effects in a heavily multithreaded program or did not consider how different patching techniques may be required for different platforms where instruction cache coherence guarantees vary. We evaluate the space and time overhead to make our profiling framework correct, showing that privatizing the profiling variables to achieve correctness impacts execution time only minimally but it can grow the stack frames for profiled methods by less than 15% on average for the SPECjvm98 and SPECjbb2000 benchmarks. Since methods are profiled for only a brief time and the stack frames themselves are not large, we do not consider this growth to be prohibitive. The techniques reported in this paper are implemented in the 1.5.0 release of the IBM Developer Kit for Java targeting 12 different processor-operating system platforms.","","0-7695-2499","10.1109/CGO.2006.16","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1611532","","Java;Costs;Testing;Production;Yarn;Program processors;Performance evaluation;Safety;Computer languages;Software design","program compilers;Java;multi-threading;object-oriented programming","multithreading;dynamic class loading;Java just-in-time compiler;IBM TestaRossa just-in-time compiler;aggressive code patching;dynamic class unloading;IBM Developer Kit;processor-operating system platforms","","11","15","","","","","","IEEE","IEEE Conferences"
"Run-time Spatial Mapping of Streaming Applications to a Heterogeneous Multi-Processor System-on-Chip (MPSOC)","P. K. F. Holzenspies; J. L. Hurink; J. Kuper; G. J. M. Smit","University of Twente, Department of Electrical Engineering, Mathematics and Computer Science, P.O. Box 217, 7500 AE Enschede, The Netherlands. p.k.f.holzenspies@utwente.nl; University of Twente, Department of Electrical Engineering, Mathematics and Computer Science, P.O. Box 217, 7500 AE Enschede, The Netherlands; University of Twente, Department of Electrical Engineering, Mathematics and Computer Science, P.O. Box 217, 7500 AE Enschede, The Netherlands; University of Twente, Department of Electrical Engineering, Mathematics and Computer Science, P.O. Box 217, 7500 AE Enschede, The Netherlands","2008 Design, Automation and Test in Europe","","2008","","","212","217","In this paper, we present an algorithm for run-time allocation of hardware resources to software applications. We define the sub-problem of run-time spatial mapping and demonstrate our concept for streaming applications on heterogeneous MPSoCs. The underlying algorithm and the methods used therein are implemented and their use is demonstrated with an illustrative example.","1530-1591;1558-1101","978-3-9810801-3-1978-3-9810801-4","10.1109/DATE.2008.4484688","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4484688","","Runtime;System-on-a-chip;Resource management;Application software;Kernel;Digital signal processing;Hardware;Design optimization;Quality of service;Baseband","microprocessor chips;resource allocation;system-on-chip","run-time spatial mapping;streaming application;heterogeneous multiprocessor system-on-chip;MPSoC;hardware resource allocation","","6","11","","","","","","IEEE","IEEE Conferences"
"Compiler-driven FPGA-area allocation for reconfigurable computing","E. M. Panainte; K. Bertels; S. Vassiliadis","Dept. of Comput. Eng., Delft Univ. of Technol., Netherlands; Dept. of Comput. Eng., Delft Univ. of Technol., Netherlands; Dept. of Comput. Eng., Delft Univ. of Technol., Netherlands","Proceedings of the Design Automation & Test in Europe Conference","","2006","1","","6 pp.","","In this paper, we propose two FPGA-area allocation algorithms based on profiling results for reducing the impact on performance of dynamic reconfiguration overheads. The problem of FPGA-area allocation is presented as a 0-1 integer linear programming problem and efficient solvers are incorporated for finding the optimal solutions. Additionally, we discuss the FPGA-area allocation problem in two scenarios. In the first scenario, all hardware operations are allocated on the FPGA while in the second scenario, any hardware operation can be switched to software execution in order to provide an overall performance improvement. We evaluate our proposed algorithms using the MPEG2 and MJPEG encoder multimedia benchmarks and the hardware implementations for SAD, DCT, IDCT, quantization and VLC tasks. We show that a significant performance improvement (up to 61 %for MPEG2 and 94 % for MJPEG) is to be achieved when the proposed algorithms are used, while the reconfiguration overhead is reduced by at least 36 % for MJPEG","1530-1591;1558-1101","3-9810801-1","10.1109/DATE.2006.243738","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1656908","","Field programmable gate arrays;Hardware;Optimizing compilers;Integer linear programming;Runtime;Software performance;Discrete cosine transforms;Quantization;Delay;Radio spectrum management","field programmable gate arrays;integer programming;linear programming;logic design;program compilers;reconfigurable architectures","compiler-driven FPGA-area allocation;reconfigurable computing;dynamic reconfiguration overheads;integer linear programming;MPEG2 encoder multimedia benchmarks;MJPEG encoder multimedia benchmarks;reconfiguration overhead","","7","12","","","","","","IEEE","IEEE Conferences"
"Fundamentals of risk based inspection — a practical approach","G. A. Ghoneim; G. Sigurdsson","Det Norske Veritas, 400 Ravello Drive, Katy, TX 77449 USA; Det Norske Veritas NO-1322 H¿vik, Norway","OCEANS 2009","","2009","","","1","9","New API structural integrity management recommended practice and post-hurricane inspection bulletin, MMS proposed changes to CFR's on decommissioning, and increased interest in developing risk based inspection plans for offshore assets require better understanding of reliability principles. This paper will discuss the fundamentals of RBI from a practical viewpoint and gives details of recently published applicable DNV offshore standards. Offshore structures must be inspected to maintain an acceptable safety level throughout their lifetime. Inspections have traditionally been based upon experience, and judgment of likelihood and consequence of failure. Risk based inspection planning (RBI) for structures, as developed by DNV, represents a systematic, qualitative and quantitative approach which combines theoretical models, test results and in-service experiences. The method is specially developed for application to all types of offshore structures including jackets, jack-ups, TLPs, FPSOs, spars, semi-submersibles, GBSS, and subsea templates. The basis of RBI is to prioritize individual items and systems by considering the associated risks. Attention is given to high risk items, while low risk items receive a more appropriately lesser level of inspection. RBI focuses on cost optimization in all associated activities, to ensure a cost optimal inspection program. The RBI analysis is performed in two steps, comprising a risk screening and a subsequent inspection scheduling. Dedicated software models are utilized to establish the Risk Matrix and to calculate the ¿time to next inspection¿. The final deliverable of an RBI analysis is an inspection plan, in which inspection efforts are prioritized from an overall risk perspective.","0197-7385","978-1-4244-4960-6978-0-933957-38","10.23919/OCEANS.2009.5422330","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5422330","","Inspection;Cost function;Risk analysis;Asset management;Risk management;Standards publication;Maintenance;Safety;System testing;Performance analysis","condition monitoring;failure analysis;inspection;offshore installations;production planning;risk analysis;structural engineering","risk based inspection planning;API structural integrity management;post-hurricane inspection bulletin;minerals management services;MMS;offshore assets;DNV offshore standards;offshore structures;failure;jackets;jack-ups;semisubmersible platforms;subsea templates;cost optimization;inspection scheduling","","","14","","","","","","IEEE","IEEE Conferences"
"Circuit-aware Device Design Methodology for Nanometer Technologies: A Case Study for Low Power SRAM Design","Qikai Chen; S. Mukhopadhyay; A. Bansal; K. Roy","School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, USA qikaichen@purdue.edu; NA; NA; NA","Proceedings of the Design Automation & Test in Europe Conference","","2006","1","","1","6","In this paper, we propose a general circuit-aware device design methodology, which can improve the overall circuit design by taking advantages of the individual circuit characters during the device design phase. The proposed methodology analytically derives the optimal device in terms of the pre-specified circuit quality factor. We applied the proposed methodology to SRAM design and achieved significant reduction in standby leakage and access time (11% and 7%, respectively, for conventional 6T-SRAM). Also, we observed that the optimal devices selected depend considerably on the applied circuit techniques. We believe that the proposed circuit-aware device design methodology will be useful in the sub-90 nm technology, where different leakage components (sub threshold, gate, and junction tunneling) are comparable in magnitude. Also, in this work, we have presented a design automation framework for SRAM, which is conventional custom designed and optimized","1530-1591;1558-1101","3-9810801-1","10.1109/DATE.2006.243868","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1657034","","Design methodology;Nanoscale devices;Computer aided software engineering;Random access memory;Design optimization;Tunneling;Circuit synthesis;CMOS technology;Leakage current;Geometry","integrated circuit design;logic design;low-power electronics;nanoelectronics;SRAM chips;tunnelling","circuit-aware device design;nanometer technologies;low power SRAM design;quality factor;sub threshold tunneling;gate tunneling;junction tunneling;design automation framework;90 nm","","2","8","","","","","","IEEE","IEEE Conferences"
"Separating Active and On-Demand Behavior of Embedded Systems into Aspects","A. Tešanovic; T. Gustafsson; J. Hansson","Link&#x00F6;ping University, Department of Computer Science, Sweden; Link&#x00F6;ping University, Department of Computer Science, Sweden; Link&#x00F6;ping University, Department of Computer Science, Sweden","ITG FA 6.2 Workshop on Model-Based Testing, GI/ITG Workshop on Non-Functional Properties of Embedded Systems, 13th GI/ITG Conference Measuring, Modelling, and Evaluation of Computer and Communications","","2006","","","1","9","Requirements on embedded systems have increased over the years leading to an increased complexity of software and, consequently, higher development and maintenance costs. One major part of the cost is the way data is engineered and maintained in the system so that resources, such as CPU, are used efficiently. There exist various ways of optimizing CPU usage by controlling how often calculations on data are performed. These include (i) algorithms that ensure updating of data on-demand, i.e., when data becomes outdated, and/or (ii) appropriate event-condition-action rules implementing active behavior in the system and, thereby ensuring that calculations, specified by actions, are carried out when an event satisfying the condition occurs. Adding active behavior ensures that time-triggered embedded systems can also respond to events that occur aperiodically. One way of adding on-demand and active behavior to the system is by re-engineering the existing software. This is a time-consuming and costly solution that still does not provide efficient means for maintaining and evolving the system. Alternatively, if software is developed from scratch, the behavior can be integrated into the system structure during design and implementation phase, resulting in a fixed and monolithic architecture that cannot easily be changed as the system evolves. In this paper we propose a way of designing and implementing on-demand and active behavior in embedded software, where this behavior is specified and developed independently of an application, and then integrated with the software. We employ aspect-orientation as the means of realizing the independent design and development of the on-demand and active behavior. We also demonstrate that encapsulating on-demand and active behavior into aspects results in efficient development and evolution of data management in embedded software with respect to complexity and cost.","","978-3-8007-2956","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5755436","","USA Councils;Legged locomotion","","","","","","","","","","","VDE","VDE Conferences"
"A Study of Distribution Power Flow Analysis Using Physically Distributed Processors","M. Kleinberg; K. Miu; C. Nwankpa","NA; NA; NA","2006 IEEE PES Power Systems Conference and Exposition","","2006","","","518","521","With distributed intelligent devices becoming more prevalent throughout the distribution system, distributed control and monitoring schemes become more feasible. The coordination and communication of these devices is essential in order for each of these devices to optimize local control decisions. A simulation platform to accommodate the testing of such systems is then also required. The necessary considerations for distributed processing of distribution system analysis will be presented along with an implementation of a remotely distributed power flow analysis. The distributed power flow method is based on a partitioned model of the distribution system where each partition is simulated remotely and information is communicated between partitions. Network partition models will be presented along with a functioning software architecture to run such distributed processing studies","","1-4244-0178-X1-4244-0177","10.1109/PSCE.2006.296367","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4075806","","Load flow analysis;Communication system control;Distributed control;Load flow;Optimal control;Control systems;Distributed processing;Distributed computing;Power system protection;Intelligent networks","distributed control;distributed processing;load flow;power distribution control;power engineering computing;software architecture","distribution power flow analysis;distributed processing;distributed intelligent device;distributed control;monitoring;network partition model;software architecture","","3","8","","","","","","IEEE","IEEE Conferences"
"Comparison of the Properties of EEG Spindles in Sleep and Propofol Anesthesia","R. Ferenets; T. Lipping; P. Suominen; J. Turunen; P. Puumala; V. Jantti; S. Himanen; A. Huotari","Student Member, IEEE, Department of Information Technology, Pori, Tampere University of Technology, Pori, Finland; Biomedical Engineering Center, Tallinn University of Technology, Tallinn, Estonia, rain.ferenets@tut.fi; Senior Member, IEEE, Department of Information Technology, Pori, Tampere University of Technology, Pori, Finland; Department of Information Technology, Pori, Tampere University of Technology, Pori, Finland; Department of Information Technology, Pori, Tampere University of Technology, Pori, Finland; Department of Anaesthesiology, Tampere University Hospital, Tampere, Finland; Department of Clinical Neurophysiology, Tampere University Hospital, Tampere, Finland; Department of Clinical Neurophysiology, Tampere University Hospital, Tampere, Finland; Department of Anaesthesiology, Oulu University Hospital, Oulu, Finland","2006 International Conference of the IEEE Engineering in Medicine and Biology Society","","2006","","","6356","6359","Electroencephalogram spindle patterns corresponding to two different phenomena-natural sleep and propofol anesthesia-are compared. The spindles are extracted from 5 overnight sleep recordings and 10 recordings of deep propofol anesthesia. Mean frequency, angle of the trend in instant frequency as well as 3 nonlinear parameters-spectral entropy, approximate entropy, and Higuchi fractal dimension- are calculated to characterize the spindle waveforms. Using the Wilcoxon rank sum test with significance level of 0.01, all the mentioned features, except approximate entropy, differ significantly for the two types of EEG spindles","1557-170X","1-4244-0032-51-4244-0033","10.1109/IEMBS.2006.259909","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4463264","","Electroencephalography;Sleep;Anesthesia;Frequency;Entropy;Hospitals;Electrodes;Fractals;Anesthetic drugs;Information technology","drugs;electroencephalography;neurophysiology;sleep","EEG;sleep;propofol anesthesia;electroencephalogram spindle pattern;spectral entropy;approximate entropy;Higuchi fractal dimension;Wilcoxon rank sum test","Algorithms;Anesthesia;Anesthetics;Electroencephalography;Entropy;Humans;Models, Statistical;Nonlinear Dynamics;Polysomnography;Propofol;Signal Processing, Computer-Assisted;Sleep;Software;Time Factors","9","13","","","","","","IEEE","IEEE Conferences"
"Who's better? PESA or NSGA II?","L. Diosan; M. Oltean","NA; NA","Seventh International Conference on Intelligent Systems Design and Applications (ISDA 2007)","","2007","","","869","874","According to the no free lunch (NFL) theorems all black-box algorithms perform equally well when compared over the entire set of optimization problems. An important problem related to NFL is finding a test problem for which a given algorithm is better than another given algorithm. In this paper we propose an evolutionary approach for solving this problem: we will evolve multi-objective test functions for which a given algorithm A is better than another given algorithm B. The evolved functions are represented as binary strings. Several numerical experiments involving PESA and NSGA II are performed. The results show the effectiveness of the proposed approach. Several multi-objective problems for which PESA performs better than NSGA II and several multi-objective test problems for which NSGA II performs better than PESA have been evolved.","2164-7143;2164-7151","0-7695-2976-3978-0-7695-2976","10.1109/ISDA.2007.106","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4389717","","Testing;Performance evaluation;Evolutionary computation;Computer science;Intelligent systems;Application software;Mathematics;High performance computing;Genetic programming;Computer errors","evolutionary computation;formal logic","no free lunch theorem;black-box algorithm;optimization problem;multiobjective test functions;binary strings;evolutionary computation","","1","19","","","","","","IEEE","IEEE Conferences"
"A constraint based approach for building Operationally Responsive Space satellites","M. O. Kahraman; E. D. Swenson; J. T. Black","Aerospace Branch, Plan Principles Division, Turkish Air Force (TUAF), Ankara, TURKEY; Air Force Intitute of Technology (AFIT), Department of Aeronautics and Astronautics, Wright-Patterson Air Force Base, Ohio, U.S.A.; Air Force Intitute of Technology (AFIT), Department of Aeronautics and Astronautics, Wright-Patterson Air Force Base, Ohio, U.S.A.","2009 4th International Conference on Recent Advances in Space Technologies","","2009","","","506","511","The Operationally Responsive Space program requires flexible and responsive satellites to meet their user's needs. Traditional satellite design methods are typically iterative processes that optimize individual components, subsystems, and ultimately the entire satellite. This study focuses on developing a new approach for creating Responsive Satellites from Plug-and-Play components. The aim is to create an approach that quickly evaluates a wide variety of possible satellite configurations and identifies the best configurations that meet the user's needs and constraints. Satellite configurations are created by matching locations on the satellite structure with Plug-and-Play components. Various constraints are derived from the user's inputs at different levels of the configuration process. As the user provides more information related to Plug-and-Play satellite, additional constraints can be applied to reduce the number of Plug-and-Play satellite configurations resulting in manageable numbers of configurations from which the user can ultimately select the final configuration.","","978-1-4244-3626-2978-1-4244-3628-6978-1-4244-3627","10.1109/RAST.2009.5158250","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5158250","","Software tools;Space technology;Costs;Batteries;Military satellites;Testing;Software performance;Design methodology;Iterative methods;Design optimization","artificial satellites;design engineering","operationally responsive space program;constraint based approach;responsive satellites;satellite design methods;plug-and-play components","","","10","","","","","","IEEE","IEEE Conferences"
"New approach to training support vector machine","T. Faming; C. Mianyun; W. Zhongdong","Dept. of Control Science &#x0026; Engineering, Huazhong Univ. of Science and Technology, Wuhan 430074, P. R. China; Dept. of Control Science &#x0026; Engineering, Huazhong Univ. of Science and Technology, Wuhan 430074, P. R. China; Dept. of Control Science &#x0026; Engineering, Huazhong Univ. of Science and Technology, Wuhan 430074, P. R. China","Journal of Systems Engineering and Electronics","","2006","17","1","200","205","Support vector machine has become an increasingly popular tool for machine learning tasks involving classification, regression or novelty detection. Training a support vector machine requires the solution of a very large quadratic programming problem. Traditional optimization methods cannot be directly applied due to memory restrictions. Up to now, several approaches exist for circumventing the above shortcomings and work well. Another learning algorithm, particle swarm optimization, for training SVM is introduted. The method is tested on UCI datasets.","1004-4132","","10.1016/S1004-4132(06)60035-2","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6071318","support vector machine;quadratic programming problem;particle swarm optimization","Support vector machines;Training;Kernel;Optimization;Vectors;Classification algorithms;Acceleration","","","","","","","","","","","BIAI","BIAI Journals & Magazines"
"Implementation and Evaluation of Channel Assignment Tool for Multi-Radio Multi-Channel Wireless Mesh Networks","W. Xie; M. Xiao; F. Zhao; Y. Yao","NA; NA; NA; NA","2007 IEEE Wireless Communications and Networking Conference","","2007","","","2790","2794","With the rapid development of wireless mesh networks (WMNs) technology, more and more commercial products and even solutions are provided in trial applications. Especially, the 802.11-based WMNs are considered as one of the best networking technology to extend the coverage of wireless LAN. However, few practical optimization tools are available for the planning and design of commercial WMNs. In this paper, the channel assignment tool (CAT) designed for the optimized backhaul networking of multi-radio multi-channel (MRMC) WMNs is presented. In the proposed channel assignment algorithm, several crucial factors affecting real networking performance are taken into account. Besides, the CAT is implemented as a flexible software module, which can be easily combined with other optimization and planning tool, as well as works independently. Furthermore, through the outdoor test taken on the test-bed consisting of commercial wireless mesh routers, the optimal channel assignment scheme figured out by the CAT is verified to perform best in the real MRMC WMN.","1525-3511;1558-2612","1-4244-0658-71-4244-0659","10.1109/WCNC.2007.518","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4224763","","Wireless mesh networks;Testing;Electronic mail;Wireless LAN;Design optimization;Performance evaluation;Communications Society;USA Councils;Software tools;Mesh networks","channel allocation;telecommunication network routing;wireless LAN","channel assignment tool;wireless mesh networks;multi-radio multi-channel WMM;wireless LAN;IEEE 802.11;backhaul networking;wireless mesh routers","","","9","","","","","","IEEE","IEEE Conferences"
"Algorithms and Hardware Structures for Unobtrusive Real-Time Compression of Instruction and Data Address Traces","M. Milenkovic; A. Milenkovic; M. Burtscher","IBM, Austin, Texas; University of Alabama in Huntsville; Cornell University, Ithaca, New York","2007 Data Compression Conference (DCC'07)","","2007","","","283","292","Instruction and data address traces are widely used by computer designers for quantitative evaluations of new architectures and workload characterization, as well as by software developers for program optimization, performance tuning, and debugging. Such traces are typically very large and need to be compressed to reduce the storage, processing, and communication bandwidth requirements. However, preexisting general-purpose and trace-specific compression algorithms are designed for software implementation and are not suitable for runtime compression. Compressing program execution traces at runtime in hardware can deliver insights into the behavior of the system under test without any negative interference with normal program execution. Traditional debugging tools, on the other hand, have to stop the program frequently to examine the state of the processor. Moreover, software developers often do not have access to the entire history of computation that led to an erroneous state. In addition, stepping through a program is a tedious task and may interact with other system components in such a way that the original errors disappear, thus preventing any useful insight. The need for unobtrusive tracing is further underscored by the development of computer systems that feature multiple processing cores on a single chip. In this paper, we introduce a set of algorithms for compressing instruction and data address traces that can easily be implemented in an on-chip trace compression module and describe the corresponding hardware structures. The proposed algorithms are analytically and experimentally evaluated. Our results show that very small hardware structures suffice to achieve a compression ratio similar to that of a software implementation of gzip while being orders of magnitude faster. A hardware structure with slightly over 2 KB of state achieves a compression ratio of 125.9 for instruction address traces, whereas gzip achieves a compression ratio of 87.4. For data address traces, a hardware structure with 5 KB of state achieves a compression ratio of 6.1, compared to 6.8 achieved by gzip","1068-0314","0-7695-2791","10.1109/DCC.2007.10","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4148767","","Hardware;Algorithm design and analysis;Runtime;Computer aided instruction;Computer architecture;Software debugging;Software performance;Design optimization;Bandwidth;Compression algorithms","computer debugging;data compression;data handling;program debugging","hardware structures;unobtrusive real-time compression;data address traces;communication bandwidth;trace-specific compression algorithms;program execution traces;debugging tools;on-chip trace compression module;gzip;hardware structure","","6","23","","","","","","IEEE","IEEE Conferences"
"An Evaluation of Current Java Bytecode Decompilers","J. Hamilton; S. Danicic","NA; NA","2009 Ninth IEEE International Working Conference on Source Code Analysis and Manipulation","","2009","","","129","136","Decompilation of Java bytecode is the act of transforming Java bytecode to Java source code. Although easier than that of decompilation of machine code, problems still arise in Java bytecode decompilation. These include type inference of local variables and exception-handling. Since the last such evaluation (2003) several new commercial, free and open-source Java decompilers have appeared and some of the older ones have been updated. In this paper, we evaluate the currently available Java bytecode decompilers using an extension of the criteria that were used in the original study. Although there has been a slight improvement since this study, it was found that none passed all the tests, each of which were designed to target different problem areas. We give reasons for this lack of success and suggest methods by which future Java bytecode decompilers could be improved.","","978-0-7695-3793","10.1109/SCAM.2009.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5279917","java;bytecode;decompilation","Java;Testing;Application software;Open source software;High level languages;Virtual machining;Sun;Program processors;Optimizing compilers;Performance evaluation","exception handling;Java;program compilers;reasoning about programs","Java bytecode decompiler;Java bytecode decompilation;Java source code;machine code;type inference;local variables;exception handling","","5","34","","","","","","IEEE","IEEE Conferences"
"Design experience in TL modeling","N. Nemati; P. Prinetto; Z. Navabi","CAD Research Group, ECE Department, University of Tehran, 14399, Iran; Control and Computer Engineering Department, Politecnico di Torino, I-10129, Italy; CAD Research Group, ECE Department, University of Tehran, 14399, Iran","2009 4th International Conference on Design & Technology of Integrated Systems in Nanoscal Era","","2009","","","15","20","The complexities of designs are increasing progressively and the computational parts have become larger, so communications between various design modules have come to be more complicated and critical. This complexity makes the designs difficult to test and verify. TLM designates a transaction as the data transfer for communication and synchronization between two modules determined by the hardware/software system specification. In order to improve design methodology to have hardware correspondence, to make the design process faster, to have short time-to-market, and to make simulation speed faster, TLM is used. The TLM platform is a key for all the optimizations in the design. In this paper MP3 decoder, which is a part of the Mp3 standard, is designed in TLM to illustrate the TLM benefits through an actual design.","","978-1-4244-4320-8978-1-4244-4321","10.1109/DTIS.2009.4938015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4938015","Transaction level modeling;channels;MP3;Partitioning","Hardware;Digital audio players;Design methodology;Algorithm design and analysis;Process design;Decoding;Communication channels;Software algorithms;Digital systems;Software design","decoding;design","design experience;data transfer;hardware-software system specification;hardware correspondence;design process;TLM platform;MP3 decoder;Mp3 standard;transaction level modeling","","1","7","","","","","","IEEE","IEEE Conferences"
"Decision trees as information source for attribute selection","K. Fukuda; B. Martin","Environmental Science Programme, Department of Mathematics and Statistics, University of Canterbury, Private Bag 4800, Christchurch, New Zealand; Department of Computer Science and Software Engineering, University of Canterbury, Private Bag 4800, Christchurch, New Zealand","2009 IEEE Symposium on Computational Intelligence and Data Mining","","2009","","","101","108","Attribute selection (AS) is known to help improve the results of algorithmic learning processes by selecting fewer, but predictive, input attributes. This study introduces a new ranking filter AS method, the tree node selection (TNS) method. The idea of TNS is to determine significant but fewer attributes by searching through the pre-generated decision tree as information source in the manner of a pruning process. To test the performance of TNS, 33 benchmark datasets (UCI) with various numbers of instances, attributes and classes were investigated along with five known AS methods, and the results were tested with the C4.5 (unpruned) and naive Bayes classifiers. The performance, in terms of classification accuracy improvement, reduction in the number of attributes and the size of the generated decision tree are assessed by various statistical analyses for multiple comparisons. TNS was found to provide the most consistent performance for C4.5 and naive Bayes classifiers, and generated unpruned C4.5 trees with selected fewer attributes were generally found to achieve similar quality to pruned C4.5 trees without any attribute selection.","","978-1-4244-2765","10.1109/CIDM.2009.4938636","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4938636","","Decision trees;Classification tree analysis;Filters;Atmospheric measurements;Pollution measurement;Classification algorithms;Gain measurement;Air pollution;Benchmark testing;Statistical analysis","Bayes methods;data handling;decision trees","decision trees;information source;attribute selection;algorithmic learning processes;ranking filter AS method;tree node selection;pruning process;unpruned C4.5 trees;naive Bayes classifiers;pruned C4.5 trees","","2","18","","","","","","IEEE","IEEE Conferences"
"Conditional and Composite Temporal Constraints with Preferences","M. Mouhoub; A. Sukpan","University of Regina, Canada; NA","Thirteenth International Symposium on Temporal Representation and Reasoning (TIME'06)","","2006","","","95","102","Preferences in temporal problems are common but significant in many real world applications. In this paper, we extend our temporal reasoning framework, managing numeric and symbolic information, in order to handle preferences. Unlike the existing models managing single temporal preferences, ours supports four types of preferences, namely: numeric and symbolic temporal preferences, composite preferences and conditional preferences. This offers more expressive power in representing a wide variety of temporal constraint problems. The preferences are considered here as a set of soft constraints using a c-semiring structure with combination and projection operators. Solving temporal constraint problems with preferences consists of finding a solution satisfying all the temporal constraints while optimizing the preference values. This is handled by a variant of the branch and bound algorithm, we propose in this paper, and where constraint propagation is used to improve the time efficiency. Preliminary tests, we conducted on randomly generated temporal constraint problems with preferences, favor the forward checking principle as a constraint propagation strategy","1530-1311;2332-6468","0-7695-2617","10.1109/TIME.2006.10","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1635987","","Computer science;Application software;Constraint optimization;Testing;Approximation methods;Stochastic systems;Approximation algorithms","constraint handling;temporal reasoning;tree searching","conditional temporal constraint;composite temporal constraint;temporal reasoning framework;numeric information management;symbolic information management;numeric temporal preference;symbolic temporal preference;composite preference handling;soft constraint;c-semiring structure;preference value optimization;branch and bound algorithm;forward checking principle;constraint propagation","","1","28","","","","","","IEEE","IEEE Conferences"
"A Grid-based Ant Colony Algorithm for Automatic 3D Hose Routing","G. Thantulage; T. Kalganova; W. A. C. Fernando","Bio Inspired Intelligent System Group (BIIS), School of Engineering and Design, Brunel University, Uxbridge, Middlesex, UB8 3PH, UK (gishantha@ieee.org); NA; NA","2006 IEEE International Conference on Evolutionary Computation","","2006","","","48","55","Ant colony algorithms applied to difficult combinatorial optimization problems such as the traveling salesman problem (TSP) and the quadratic assignment problem. In this paper we propose a grid-based ant colony algorithm for automatic 3D hose routing. Algorithm uses the tessellated format of the obstacles and the generated hoses in order to detect collisions. The representation of obstacles and hoses in the tessellated format greatly helps the algorithm towards handling free-form objects and speed up the computations. The performance of the algorithm has been tested on a number of 3D models.","1089-778X;1941-0026","0-7803-9487","10.1109/CEC.2006.1688289","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1688289","","Hoses;Routing;Ant colony optimization;Military computing;Traveling salesman problems;Assembly;Circuit testing;Application software;Telecommunication traffic;Traffic control","assembling;evolutionary computation;travelling salesman problems","grid-based ant colony algorithm;automatic 3D hose routine;combinatorial optimization problems;traveling salesman problem;TSP;quadratic assignment problem;tessellated obstacles format;collision detection","","1","8","","","","","","IEEE","IEEE Conferences"
"Two sparsity-controlled schemes for 1-norm support vector classification","Shih-Yu Chiu; Leu-Shing Lan; Yu-Cheng Hwang","Department of Electronics Engineering, National Yunlin University of Science and Technology, Taiwan; Department of Electronics Engineering, National Yunlin University of Science and Technology, Taiwan; Department of Electronics Engineering, National Yunlin University of Science and Technology, Taiwan","2007 IEEE Northeast Workshop on Circuits and Systems","","2007","","","337","340","Support vector machines (SVMs) are currently a very active research area for machine learning, data mining, etc. Sparsity control is an issue deserving further attention for the improvement of existing support vector machines techniques. This work presents two new sparsity control methods for 1- norm support vector classification. The first scheme, called SVC-sc1, is formulated by adding a penalty term in the objective function, whereas the second scheme, called SVC-sc2, is obtained by adding an extra inequality to the original optimization problem. The common goal is to reduce the number of retained support vectors. Besides mathematical formulation, we present test results on the benchmark Ripley data set. The experimental results indicate that both schemes outperform the conventional SVC, whereas SVC-sc2 has a still better performance than SVC-sc1.","","978-1-4244-1163-4978-1-4244-1164","10.1109/NEWCAS.2007.4487961","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4487961","","Static VAr compensators;Support vector machines;Kernel;Support vector machine classification;Machine learning;Optical character recognition software;Image analysis;Time series analysis;Algorithm design and analysis;Training data","optimisation;support vector machines","sparsity-controlled schemes;support vector classification;support vector machines","","1","16","","","","","","IEEE","IEEE Conferences"
"Motion Estimation Optimization for H.264/AVC Using Source Image Edge Features","Z. Liu; J. Zhou; S. Goto; T. Ikenaga","NA; NA; NA; NA","IEEE Transactions on Circuits and Systems for Video Technology","","2009","19","8","1095","1107","The H.264/AVC coding standard processes variable block size motion-compensated prediction with multiple reference frames to achieve a pronounced improvement in compression efficiency. Accordingly, the computation of motion estimation increases in proportion to the product of the number of reference frame and the number of intermode. The mathematical analysis in this paper illustrates that the motion-compensated prediction errors are mainly determined by the detailed textures in the source image. The image block being rich in textures contains numerous high-frequency signals, which make variable block size and multiple reference frame techniques essential. On the basis of rate-distortion theory, in this paper, the spatial homogeneity of an image block is made as a relative concept with respect to the current quantization step. For the homogenous block, its futile reference frames and intermodes can be eliminated efficiently. It is further revealed that the sum of absolute differences value of an image block is mainly determined by the sum of its edge gradient amplitude and the current quantization step. Consequently, the image content-based early termination algorithm is proposed, and it outperforms the original method adopted by JVT reference software. Moreover, the dynamic search range algorithm based on the edge gradient amplitude of source image block is analyzed. One eminent advantage of the proposed edge-based algorithms is their efficiency to the macroblock-pipelining architecture, and another desirable feature is their orthogonality to fast block-matching algorithms. Experimental results show that when these algorithms are integrated with hybrid unsymmetrical-cross multi-hexagongrid search, an averaged 31.4-60.0% motion estimation time can be saved, whereas the averaging BDPSNR loss is 0.0497 dB for all tested sequences.","1051-8215;1558-2205","","10.1109/TCSVT.2009.2022796","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4914856","Edge gradient;fast mode decision;H264/AVC;motion estimation (ME);multiple reference frame (MRF);variable block size (VBS)","Motion estimation;Automatic voltage control;Quantization;Image coding;Mathematical analysis;Motion analysis;Rate-distortion;Software algorithms;Dynamic range;Heuristic algorithms","motion compensation;motion estimation;video coding","motion estimation optimization;H.264/AVC;source image edge features;variable block size motion compensated prediction;spatial homogeneity;edge based algorithms;fast block matching algorithms;macroblock pipelined architecture","","10","29","","","","","","IEEE","IEEE Journals & Magazines"
"Cycle-Based Decomposition of Markov Chains With Applications to Low-Power Synthesis and Sequence Compaction for Finite State Machines","A. Iranli; M. Pedram","NA; NA","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","","2006","25","12","2712","2725","This paper advances the state of the art by presenting a well-founded mathematical framework for modeling and manipulating Markov processes. The key idea is based on the fact that a Markov process can be decomposed into a collection of directed cycles with positive weights, which are proportional to the probability of the cycle traversals in a random walk. Two applications of this new formalism in the computer-aided design area are studied. In the first application, the authors present a new state assignment technique to reduce dynamic power consumption in finite state machines. The technique comprises of first decomposing the state machine into a set of cycles and then performing a state assignment by using Gray codes. The proposed encoding algorithm reduces power consumption by an average of 15%. The second application is sequence compaction for improving the efficiency of dynamic power simulators. The proposed method is based on the cycle decomposition of the Markov process representing the given input sequence and then selecting a subset of these cycles to construct the compacted sequence","0278-0070;1937-4151","","10.1109/TCAD.2006.882478","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4014531","Circuit synthesis;digital system testing;finite state machines;logic design","Compaction;Automata;Markov processes;Power system modeling;Very large scale integration;Application software;Energy consumption;Circuits;Resource management;Design optimization","finite state machines;Gray codes;high level synthesis;logic design;low-power electronics;Markov processes","cycle-based decomposition;Markov chains;low-power synthesis;sequence compaction;finite state machines;Markov process;cycle traversals;computer-aided design area;state assignment technique;dynamic power consumption;Gray codes;dynamic power simulators;circuit synthesis;digital system testing;logic design","","1","26","","","","","","IEEE","IEEE Journals & Magazines"
"IPv6 Application Performance Characterization Using a Virtual/Live Testbed","D. Green; R. Mayo; R. Reddy","(SRI International ¿ Fort Monmouth), 3 Revmont Drive, Shrewsbury, NJ 07702; (SRI International ¿ Fort Monmouth), 3 Revmont Drive, Shrewsbury, NJ 07702; (US Army CERDEC S&TCD ¿ Ft. Monmouth), AMSRD-CER-ST-SE, Ft. Monmouth, NJ 07703","MILCOM 2006 - 2006 IEEE Military Communications conference","","2006","","","1","4","This paper describes development of a testbed that allows for characterization of IPv6 networks and applications. Testing applications across large, next-generation communication networks is a high cost effort, which may be prohibitive to organizations or programs on limited budgets. In such cases, it would be desirable to study the effects of communications channels using a simulated network to provide transport for application packets (in this case, using OPNET). This testbed incorporates TCP and UDP server applications and live IT equipment (routers, switches, servers, etc.) at the edges of a virtual network simulation in the center in order to study and optimize performance and responsiveness to the applications under test","2155-7578;2155-7586","1-4244-0617-X1-4244-0618","10.1109/MILCOM.2006.302398","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4086641","IPv6;Modeling & Simulation;OPNET;Modeling and Simulation;Hardware-in-the-loop","Network servers;File servers;Next generation networking;Computer networks;Application software;Software testing;Communication networks;Costs;Communication channels;Switches","Internet;IP networks;telecommunication channels;telecommunication computing;transport protocols","IPv6 network;communication channel;application packet;OPNET;TCP;UDP;live IT equipment;router;virtual network simulation","","1","4","","","","","","IEEE","IEEE Conferences"
"Optimization of Core Point Detection in Fingerprints","N. Y. Khan; M. Y. Javed; N. Khattak; U. M. Y. Chang","NA; NA; NA; NA","9th Biennial Conference of the Australian Pattern Recognition Society on Digital Image Computing Techniques and Applications (DICTA 2007)","","2007","","","260","266","This paper compares and documents the work of an optimized fingerprint core point determination algorithm. This work focuses to present an efficient and precise way for the extraction of core point. Core Point is detected using least mean square algorithm. Orientation is figured out via a novel efficient technique. This technique is better in terms of time and accuracy as compared to other two techniques mentioned in this paper. Poin'care index is then used to find out the location of core point. Found point may be slightly in error so further refinement is performed. For further tuning, a small window of specific size (w x w) is cropped around that point. This area is now tuned via PC after making some enhancements to it. The results obtained are very good and are produced efficiently. Proposed algorithm provides the hybrid platform where refinement can be done via any technique.","","0-7695-3067","10.1109/DICTA.2007.4426805","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4426805","","Fingerprint recognition;Educational institutions;Digital images;Computer applications;Application software;Computer science;Focusing;Least mean square algorithms;Testing;Image texture","","","","6","20","","","","","","IEEE","IEEE Conferences"
"Improved algorithm for RDO in JPEG2000 encoder and its IC design","X. Xiang; L. Guolin; Z. Chun; Z. Li; W. Zhihua","Dept. of Electronic Engineering, Tsinghua Univ., Beijing 100084, P. R China; Dept. of Electronic Engineering, Tsinghua Univ., Beijing 100084, P. R China; Dept. of Electronic Engineering, Tsinghua Univ., Beijing 100084, P. R China; Dept. of Electronic Engineering, Tsinghua Univ., Beijing 100084, P. R China; Dept. of Electronic Engineering, Tsinghua Univ., Beijing 100084, P. R China","Journal of Systems Engineering and Electronics","","2006","17","2","430","436","An improved rate distortion optimization (RDO) algorithm in JPEG2000 is proposed. The proposed algorithm is suitable for integrated circuit (Ie) implementation and can reduce 30 computational cost. A hardware architecture which includes control unit, memory, divider, data converter is also given to implement the algorithm, The circuit based on the improved algorithm is tested on FPGAs and integrated in a JPG2000 chip codec core.","1004-4132","","10.1016/S1004-4132(06)60073-X","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6071362","rate distortion optimization;JPEG2000;integrated circuit (IC);codec core","Rate-distortion;Transform coding;Optimization;Encoding;Integrated circuits;Image coding;Algorithm design and analysis","","","","","","","","","","","BIAI","BIAI Journals & Magazines"
"I/O Scheduling Service for Multi-Application Clusters","A. Lebre; G. Huard; Y. Denneulin; P. Sowa","Laboratoire Informatique et Distribution - IMAG, Montbonnot Saint-Martin, France. Adrien.Lebre@imag.fr; Laboratoire Informatique et Distribution - IMAG, Montbonnot Saint-Martin, France. Guillaume.Huard@imag.fr; Laboratoire Informatique et Distribution - IMAG, Montbonnot Saint-Martin, France. Yves.Denneulin@imag.fr; Institute of Computer and Information Sciences, Czestochowa University of Technology, Poland. sowa@icis.pcz.pl","2006 IEEE International Conference on Cluster Computing","","2006","","","1","10","Distributed applications, especially the ones being I/O intensive, often access the storage subsystem in a nonsequential way (stride requests). Since such behaviours lower the overall system performance, many applications use parallel I/O libraries such as ROMIO to gather and reorder requests. In the meantime, as cluster usage grows, several applications are often executed concurrently, competing for access to storage subsystems and, thus, potentially canceling optimisations brought by parallel I/O libraries. The aIOLi project aims at optimising the I/O accesses within the cluster and providing a simple POSIX API. This article presents an extension of aIOLi to address the issue of disjoint accesses generated by different concurrent applications in a cluster. In such a context, good trade-off has to be assessed between performance, fairness and response time. To achieve this, an I/O scheduling algorithm together with a ""requests aggregator"" that considers both application access patterns and global system load, have been designed and merged into aIOLi This improvement led to the implementation of a new generic framework pluggable into any I/O file system layer. A test composed of two concurrent IOR benchmarks has shown improvements on read accesses by a factor ranging from 3.5 to 35 with POSIX calls and from 3.3 to 5 with ROMIO, both reference benchmarks have been executed on a traditional NFS server without any additional optimisations","1552-5244;2168-9253","1-4244-0328-61-4244-0327","10.1109/CLUSTR.2006.311854","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4100360","","Application software;File systems;Libraries;Throughput;Image storage;Delay;Benchmark testing;Hardware;Concurrent computing;Processor scheduling","processor scheduling;software libraries","I/O scheduling service;multiapplication clusters;distributed applications;storage subsystem;parallel I/O libraries;aIOLi project;POSIX API;requests aggregator;I/O file system layer","","11","27","","","","","","IEEE","IEEE Conferences"
"Automated characterisation system for liquid crystal displays","M. A. Geday; X. Quintana; N. Bennis; B. Cerrolaza; D. Perez-Medialdea; J. M. Oton","Depto. Tecnología Fotónica, ETSI Telecomunicación, Universidad Politécnica de Madrid, Ciudad Universitaria s/n, E-28040 Madrid, Spain. Phone: +34 91 549 5700 # 3414, Fax: +34 91 336 7319, Email: morten@tfo.upm.es; Depto. Tecnología Fotónica, ETSI Telecomunicación, Universidad Politécnica de Madrid, Ciudad Universitaria s/n, E-28040 Madrid, Spain; Depto. Tecnología Fotónica, ETSI Telecomunicación, Universidad Politécnica de Madrid, Ciudad Universitaria s/n, E-28040 Madrid, Spain; Depto. Tecnología Fotónica, ETSI Telecomunicación, Universidad Politécnica de Madrid, Ciudad Universitaria s/n, E-28040 Madrid, Spain; Depto. Tecnología Fotónica, ETSI Telecomunicación, Universidad Politécnica de Madrid, Ciudad Universitaria s/n, E-28040 Madrid, Spain; Depto. Tecnología Fotónica, ETSI Telecomunicación, Universidad Politécnica de Madrid, Ciudad Universitaria s/n, E-28040 Madrid, Spain","2007 Spanish Conference on Electron Devices","","2007","","","318","320","A complete program suite for the automated characterisation of liquid crystal displays (LCDs) has been developed in the Lab View 7.1 environment. It includes routines for basic electro-optical characterisation, i.e. generation of transmission-voltage curves applying triangular waveforms while measuring the switching voltages. And it provides an easily accessible interface for design of arbitrary waveforms, both for active matrix addressing (often necessary in nematic LCDs) and for passive matrix addressing (applicable to ferro-and antiferro-electric LCDs as well as some nematic LCDs). The software includes automated methods for generation of dynamic grey scales, for measurement of grey-grey transition times and for analysis of image sticking.","2163-4971","1-4244-0868","10.1109/SCED.2007.384057","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4271235","LCD characterisation;graphical user interface;automated data acquisition;arbitrary waveform generator","Liquid crystal displays;Signal generators;Design optimization;Optical pulses;Active matrix liquid crystal displays;Data acquisition;Character generation;Active matrix addressing;Oscilloscopes;Telecommunication standards","automatic test equipment;electro-optical devices;graphical user interfaces;liquid crystal displays;nematic liquid crystals;virtual instrumentation","automated characterisation system;liquid crystal displays;LCD;Lab View 7.1 environment;electro-optical characterisation;transmission-voltage curves;switching voltages;accessible interface;arbitrary waveforms;active matrix addressing;nematic LCD;passive matrix addressing;ferro-electric LCD;antiferro-electric LCD;dynamic grey scales;grey-grey transition times;image sticking analysis;graphical user interface","","","","","","","","","IEEE","IEEE Conferences"
"A delay-aware topology-based design for Networks-on-Chip applications","H. Elmiligi; A. A. Morgan; M. W. El-Kharashi; F. Gebali","Department of Electrical and Computer Engineering University of Victoria, BC, Canada V8W 3P6; Department of Electrical and Computer Engineering University of Victoria, BC, Canada V8W 3P6; Department of Computer and Systems Engineering Ain Shams University, Cairo, Egypt; Department of Electrical and Computer Engineering University of Victoria, BC, Canada V8W 3P6","2009 4th International Design and Test Workshop (IDT)","","2009","","","1","5","Network delay is a major design parameter for Networks-on-Chip (NoC)-based applications. Improving NoC delay could be achieved at different design phases. At the system level, we study in this paper the impact of the network topology on NoC system delay using graph-theoretic concepts. A topology-based model is developed to calculate the average NoC delay, which is caused by links and routers. The proposed model could be used to select the optimal topology that achieves the minimum network delay for a given NoC application. A case study is presented to show how this model could be used to improve the delay of a given NoC application at early design phases.","2162-0601;2162-061X","978-1-4244-5750-2978-1-4244-5748-9978-1-4244-5750","10.1109/IDT.2009.5404136","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5404136","","Network-on-a-chip;Network topology;Semiconductor device modeling;Application software;Delay systems;Multiaccess communication;Design optimization;Computer networks;Design engineering;Systems engineering and theory","design;graph theory;network-on-chip;telecommunication links;telecommunication network routing;telecommunication network topology","delay-aware topology-based design;networks-on-chip;NoC;design parameter;network delay;graph theory;network links;routers","","","31","","","","","","IEEE","IEEE Conferences"
"Executable system-level specification models containing UML-based behavioral patterns","L. S. Indrusiak; A. Thuy; M. Glesner","Institute of Microelectronic Systems - Technische Universität Darmstadt, E-mail: indrusiak@mes.tu-darmstadt.de; Institute of Microelectronic Systems - Technische Universität Darmstadt, E-mail: a.thuy@gmx.net; Institute of Microelectronic Systems - Technische Universität Darmstadt, E-mail: glesner@mes.tu-darmstadt.de","2007 Design, Automation & Test in Europe Conference & Exhibition","","2007","","","1","6","Behavioral patterns are useful abstractions to simplify the design of the communication-centric systems. Such patterns are traditionally described using UML diagrams, but the lack of execution semantics in UML prevents the co-validation of the patterns together with simulation models and executable specifications which are the mainstream in today's system level design flows. This paper proposes a method to validate UML-based behavioral patterns within executable system models. The method is based on actor orientation and was implemented as an extension of the Ptolemy II framework. A case study is presented and potential applications and extensions of the proposed method are discussed","1530-1591;1558-1101","978-3-9810801-2","10.1109/DATE.2007.364608","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4211813","","Unified modeling language;Object oriented modeling;Mathematical model;Microelectronics;System-level design;Software engineering;Space exploration;Hardware design languages;Design optimization;Pattern recognition","hardware description languages;logic CAD;Unified Modeling Language","executable system-level specification models;UML-based behavioral patterns;communication-centric systems;system level design flow;actor orientation;Ptolemy II","","6","19","","","","","","IEEE","IEEE Conferences"
"Creating Agile Streams for Business & Technical Value","Z. Hunter; D. Spann","NA; NA","Agile 2008 Conference","","2008","","","144","147","Have you ever played the role of business owner and found yourself between ""a rock and a hard place"" of organizational politics when prioritizing backlog features? The Agile Stream approach negates those politics by dedicating development teams to organizational units and allowing those teams to continue working, iteration after iteration, as long as they continue delivering business value.","","978-0-7695-3321","10.1109/Agile.2008.93","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4599466","Agile;Stream;Executive;Process;Flow","Business;Testing;Companies;Planning;Lead;Best practices;Resource management","software engineering","Agile streams;organizational politics;dedicating development teams;organizational units;business value","","","","","","","","","IEEE","IEEE Conferences"
"A Multi-Agent Solution to Distribution Systems Restoration","J. M. Solanki; S. Khushalani; N. N. Schulz","NA; NA; NA","IEEE Transactions on Power Systems","","2007","22","3","1026","1034","The goal to provide faster and faster restoration after a fault is pushing the technical envelope related to new algorithms. While many approaches use centralized strategies, the concept of multi-agent systems (MAS) is creating a new option related to distributed analyses for restoration. This paper provides details on a MAS that restores a power system after a fault. The development of agents and behaviors of the agents are described, including communication of agents. The MAS is tested on two test systems and facilitates both full and partial restoration, including load prioritization and shedding.","0885-8950;1558-0679","","10.1109/TPWRS.2007.901280","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4282055","Decentralized solution;intelligent agents;multi-agent system (MAS)","Power system restoration;Intelligent agent;Multiagent systems;Power system faults;System testing;Switches;Knowledge based systems;Software agents;Power system analysis computing;Automation","load shedding;multi-agent systems;power distribution faults;power engineering computing;power system restoration","distribution systems restoration;multiagent systems;partial restoration;load prioritization;load shedding","","173","21","","","","","","IEEE","IEEE Journals & Magazines"
"A Novel General RFID Framework Based on Middleware and Data Cleaning","L. Ye","NA","2008 Second International Conference on Future Generation Communication and Networking","","2008","1","","194","197","In this paper researches on new RFID general framework based on middle ware and data cleaning are proposed in bus park information integration system. We use middle ware and layer design to lower the coupling; use IOC technology to improve testing, use cleaning method that optimizes the overall accuracy adjusted cleaning costs. A web application system is described and a multi-level distributive application model is provided. It is proved by the experiment that this system has excellent performance and a promising application prospect.","2153-1447;2153-1463","978-0-7695-3431","10.1109/FGCN.2008.74","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4734086","RFID;Middleware;Data cleaning;IOC","Radiofrequency identification;Middleware;Cleaning;Application software;Power system modeling;Software maintenance;Wireless sensor networks;Business communication;Data engineering;Testing","Internet;middleware;radiofrequency identification","general RFID framework;middleware;data cleaning;bus park information integration system;layer design;IOC technology;multilevel distributive application model;web application system","","","8","","","","","","IEEE","IEEE Conferences"
"Automated multidimensional characterization of power amplifier for design and production","C. Nader; H. Altahir; O. Andersen; N. Bjorsell; E. Condo; N. Keskitalo; H. de la Rosa","Center for RF Measurement Technology, University of Gävle, SE-801 76, Sweden; Center for RF Measurement Technology, University of Gävle, SE-801 76, Sweden; Ericsson AB Gävle, SE-800 06, Sweden; Center for RF Measurement Technology, University of Gävle, SE-801 76, Sweden; Center for RF Measurement Technology, University of Gävle, SE-801 76, Sweden; Center for RF Measurement Technology, University of Gävle, SE-801 76, Sweden; Freescale Semiconductor Nordic AB, Kista, SE-164 26, Sweden","2009 IEEE Instrumentation and Measurement Technology Conference","","2009","","","144","148","Designing, optimizing and producing modern power amplifiers (PA) requires new and fast RF (radio frequency) measurement techniques capable of characterizing its real behavior. PAs are a truly multidimensional device where many desired performance parameters are contradictory to each other. This is especially true for the generation of modern communication PAs that require high efficiency, high linearity as well as high bandwidth. This paper presents a software-defined measurement setup for fast and cost efficient multidimensional measurements based on highly accurate standard instruments and a PC. The test bed as well as the graphical user interface is presented along with a demonstration of its functionality. During tuning of tank networks, drain quiescent current, and bias conditions, 3-dimensional graphs can be selected for the most appropriate axes of trade-off parameters to display a true behavior of the PA under test subjected to real-world or close to real-world signals. The measurement system offers the possibility to monitor envelope-tracking dynamic power consumption up to 100 MHz plus the possibility to use high crest factors.","1091-5281","978-1-4244-3352-0978-1-4244-3353","10.1109/IMTC.2009.5168432","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5168432","Automated multidimensional measurements;power amplifier;time domain measurement;state-of-the-art test-bed;gain-distortion-efficiency","Multidimensional systems;Power amplifiers;Production;Radiofrequency amplifiers;Radio frequency;Design optimization;Measurement techniques;Linearity;Bandwidth;Cost function","computerised instrumentation;graphical user interfaces;measurement systems;power amplifiers;radiofrequency measurement","power amplifier multidimensional characterization;radiofrequency measurement techniques;software-defined measurement setup;standard instruments;graphical user interface;tank networks;drain quiescent current;3-dimensional graphs;envelope-tracking dynamic power consumption","","2","9","","","","","","IEEE","IEEE Conferences"
"Fault Injection Campaign for a Fault Tolerant Duplex Framework","G. F. Sacco; R. D. Ferraro; P. von Allmen; D. A. Rennels","Jet Propulsion Laboratory and California Institute of Technology, 4800 Oak Grove Drive, Pasadena, CA 91109, USA; Jet Propulsion Laboratory and California Institute of Technology, 4800 Oak Grove Drive, Pasadena, CA 91109, USA; Jet Propulsion Laboratory and California Institute of Technology, 4800 Oak Grove Drive, Pasadena, CA 91109, USA; Jet Propulsion Laboratory and California Institute of Technology, 4800 Oak Grove Drive, Pasadena, CA 91109, USA; Department of Computer Science, University of California Los Angeles, 4731G Boelter Hall, Los Angeles, CA 90095, USA","2007 IEEE Aerospace Conference","","2007","","","1","9","Software based fault tolerance may allow the use of COTS digital electronics in building a highly reliable computing system for spacecraft. In this work we present the results of a fault injection campaign we conducted on the Duplex Framework (DF). The DF is a software developed by the UCLA group [1], [2] that allows to run two copies (or replicas) of the same program on two different nodes of a commercial off-the-shelf (COTS) computer cluster. By the means of a third process (comparator) running on a different node that constantly monitors the results computed by the two replicas, the DF is able to restart the two replica processes if an inconsistency in their computation is detected. In order to test the reliability of the DF we wrote a simple fault injector that injects faults in the virtual memory of one of the replica process to simulate the effects of radiation in space. These faults occasionally cause the process to crash or produce erroneous outputs. For this study we used two different applications, one that computes an encryption of a input file using the RSA algorithm, and another that optimizes the trade-off between time spent and the fuel consumption for a low-thrust orbit transfer. But the DF is generic enough that any application written in C or Fortran could be used with little or no modification of the original source code. Our results show the potential of such approach in detecting and recovering from radiation induced random errors. This approach is very cost efficient compared to hardware implemented duplex operations and can be adopted to control processes on spacecrafts where the fault rate produced by cosmic rays is not very high.","1095-323X","1-4244-0524-61-4244-0525","10.1109/AERO.2007.352648","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4161526","","Fault tolerance;Space vehicles;Fault tolerant systems;Aerospace electronics;Computer displays;Vehicle crash testing;Computational modeling;Computer crashes;Computer applications;Cryptography","aerospace computing;software fault tolerance;space vehicles","fault injection campaign;fault tolerant duplex framework;software based fault tolerance;spacecraft computing system;COTS digital electronics;duplex framework;commercial off-the-shelf computer cluster;virtual memory;low-thrust orbit transfer;cosmic rays","","1","5","","","","","","IEEE","IEEE Conferences"
"Simulation and Experimental Studies on the Temperature Field of a Wet Shift Clutch during One Engagement","J. Zhang; B. Ma; Y. Zhang; H. Li","NA; NA; NA; NA","2009 International Conference on Computational Intelligence and Software Engineering","","2009","","","1","5","Based on the mathematical model of wet clutch engagement, a dynamic model of a tracked vehicle was developed by means of the software Matlab/Simulink. This model can be used to simulate the shift schedule, piston pressure and relative velocity. A two dimensional heat conduction model was also developed with reasonable selection of proper boundary conditions, contact pressure model and heat flux distribution model. Relative partial differential equations were solved by means of the Matlab/PDE package. Simulation results revealed the temperature distribution along the axial direction and radial direction on the sliding interface of between friction lining and separator disc. A test bench was designed to measure the temperature distribution along the radial direction on the separator disc. The experiment results proved the reliability of the simulation model. The simulation model provided a good engineering tool for optimization design of wet shift clutch.","","978-1-4244-4507","10.1109/CISE.2009.5365857","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5365857","","Mathematical model;Temperature distribution;Particle separators;Vehicle dynamics;Vehicles;Pistons;Boundary conditions;Partial differential equations;Packaging;Friction","clutches;digital simulation;heat conduction;mathematics computing;mechanical engineering computing;partial differential equations;pistons;temperature distribution","mathematical model;wet clutch engagement;tracked vehicle;dynamic model;software Matlab-Simulink;piston pressure;relative velocity;two dimensional heat conduction model;contact pressure model;heat flux distribution model;relative partial differential equations;Matlab-PDE package;radial direction;axial direction;friction lining;separator disc;sliding interface;test bench;temperature distribution;simulation model reliability","","1","11","","","","","","IEEE","IEEE Conferences"
"Efficient hyperkernel learning using second-order cone programming","I. Wai-Hung Tsang; J. Tin-Yau Kwok","Dept. of Comput. Sci., Hong Kong Univ. of Sci. & Technol., China; NA","IEEE Transactions on Neural Networks","","2006","17","1","48","58","The kernel function plays a central role in kernel methods. Most existing methods can only adapt the kernel parameters or the kernel matrix based on empirical data. Recently, Ong et al. introduced the method of hyperkernels which can be used to learn the kernel function directly in an inductive setting. However, the associated optimization problem is a semidefinite program (SDP), which is very computationally expensive, even with the recent advances in interior point methods. In this paper, we show that this learning problem can be equivalently reformulated as a second-order cone program (SOCP), which can then be solved more efficiently than SDPs. Comparison is also made with the kernel matrix learning method proposed by Lanckriet et al. Experimental results on both classification and regression problems, with toy and real-world data sets, show that our proposed SOCP formulation has significant speedup over the original SDP formulation. Moreover, it yields better generalization than Lanckriet et al.'s method, with a speed that is comparable, or sometimes even faster, than their quadratically constrained quadratic program (QCQP) formulation.","1045-9227;1941-0093","","10.1109/TNN.2005.860848","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1593691","Hyperkernel;kernel learning;second-order cone program (SOCP);semidefinite program (SDP)","Kernel;Optimization methods;Learning systems;Functional programming;Polynomials;Laplace equations;Space technology;Bayesian methods;Testing;Cost function","learning (artificial intelligence);computational complexity","hyperkernel learning;second order cone programming;kernel matrix learning;semidefinite program;quadratically constrained quadratic program;computational complexity","Algorithms;Artificial Intelligence;Data Interpretation, Statistical;Disease;Normal Distribution;Regression Analysis;Software","33","38","","","","","","IEEE","IEEE Journals & Magazines"
"Small Class-D spacecraft thermal design, test and analysis - PharmaSat biological experiment","M. F. Diaz-Aguado; S. Ghassemieh; C. VanOutryve; C. Beasley; A. Schooley","Arctic Slope Regional Corporation, Moffett Field, CA 94035, USA; NASA Ames Research Center, CA 94035, USA; San Jose State University, CA 94035, USA; Jacobs Engineering, Moffett Field, CA 94035, USA; Jacobs Engineering, Moffett Field, CA 94035, USA","2009 IEEE Aerospace conference","","2009","","","1","9","Small spacecraft have been increasing in popularity because of their low cost, short turnaround and relative efficiency. In the past, small spacecraft have been primarily used for technology demonstrations, but advances in technology have made the miniaturization of space science possible. PharmaSat is a low cost, three cube size nanosatellite spacecraft, with a biological experiment on board, built at the National Aeronautics and Space Administration (NASA) Ames Research Center (ARC). Small spacecraft present several thermal design challenges. Smaller surface area translates into power and thermal constraints. The spacecraft is thermally designed to operate colder in the Low Earth Orbit (LEO) space environment, and heated to reach the temperatures required by the science payload. The power obtained from the solar panels of limited surface area constrains the power available to heat the payload to required temperatures. The pressurized payload is isolated with low thermal conductance paths from the largely dynamic ambient temperature changes. The thermal design consists of different optical properties of section surfaces, Multi Layer Insulation (MLI), low thermal conductance materials, flexible heaters and thermal spreaders. The payload temperature is controlled with software, temperature sensors and flexible heaters. Finite Difference Analysis and testing were used to aid the thermal design of the spacecraft. Various tests were conducted to verify the thermal design. An infrared imager was used on the electronic boards to find large heat sources and eliminate any possible temperature runaways. The spacecraft was tested in a thermal vacuum chamber to optimize the thermal and power analysis, and validate and qualify the thermal design of the spacecraft for the mission.","1095-323X;1095-323X","978-1-4244-2621-8978-1-4244-2622","10.1109/AERO.2009.4839352","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4839352","","Space vehicles;Testing;Temperature sensors;Thermal conductivity;Payloads;Space technology;Costs;Low earth orbit satellites;Space heating;Infrared heating","design engineering;finite difference methods;heat conduction;space vehicles;thermal analysis","PharmaSat biological experiment;spacecraft thermal design;nanosatellite spacecraft;low earth orbit space environment;multilayer insulation;thermal conductance;finite difference analysis","","9","6","","","","","","IEEE","IEEE Conferences"
"Filtering algorithms for sequencing constraints: Application in physical mapping","H. Zidoum","Dpt. of Computer Science, Sultan Qaboos University - College of Science Al Khod, OMAN","2009 IEEE/ACS International Conference on Computer Systems and Applications","","2009","","","88","95","The physical mapping is a crucial tool in the analysis of the genomic sequences. Algorithms for the mapping process are based on NP-complete combinatorial optimizations. The problem of reconstructing the probe order is equivalent to the consecutive ones problem. PQ-trees have been extensively used as a suitable data structure to test the consecutive ones property (COP) in the hybridization matrix. This paper presents PQR-trees, an extension of PQ-trees. PQR-trees can advantageously handle partial order information on probes. Moreover, we embed PQR-trees in the more general framework of Constraint Programming (CP). CP is an emergent software technology for declarative description and effective solving of large, particularly combinatorial, problems. We introduce Sequences a new data structure in CP and present filtering algorithms for checking the consistency of sequence constraints based on PQR-trees. We present a canonical form that characterizes a family of sequential arrangements of a given set. The relations we are dealing with are classical sets relations isin, sub, ne, = besides sequencing relations such as group, order, and metric constraints. The filtering algorithms are based on incremental consistency techniques used to reduce the PQR-trees and hence, prune the inconsistencies before the labeling phase. We claim that the sequence structure introduces a flexibility criterion on CP which renders it a suitable tool for solving NP-complete combinatorial optimizations such as physical mapping problem.","2161-5322;2161-5330","978-1-4244-3807-5978-1-4244-3806","10.1109/AICCSA.2009.5069309","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5069309","","Filtering algorithms;Probes;Cloning;Sequences;DNA;Genomics;Bioinformatics;Data structures;Labeling;Application software","biology computing;combinatorial mathematics;constraint handling;genetics;matrix algebra;optimisation;tree data structures","physical mapping;genomic sequence analysis;sequencing constraint filtering algorithm;NP-complete combinatorial optimization;PQ-tree data structure;consecutive ones property;hybridization matrix;constraint programming","","","21","","","","","","IEEE","IEEE Conferences"
"An Estimation Model of Vulnerability for Embedded Microprocessors","Y. Chen; S. Hsu; K. Leu","NA; NA; NA","2008 Second International Conference on Secure System Integration and Reliability Improvement","","2008","","","224","225","Embedded systems, and also the embedded microprocessors, have encountered the reliability challenge because the occurring probability of soft errors has a rising trend. When they are applied to safety-critical applications, designs with the fault tolerant consideration are required. For the complicated embedded systems or IP-based system-on-chip (SoC), it is unpractical and not cost-effective to protect the entire system or SoC. Analyzing the vulnerability of systems can help designers not only invest limited resource on the most crucial region but also understand the gain derived from the investment. In this paper we propose a model to fast estimate the microprocessor's vulnerability with only slight simulation effort. From our assessment results, the rank of component vulnerability related to the probability of causing the microprocessor failure can be acquired. By choosing one of the mainstream microprocessors - VLIW (Very Long Instruction Word) processor - as an example, the practical usefulness of our estimation model is demonstrated.","","978-0-7695-3266","10.1109/SSIRI.2008.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4579833","","Microprocessors;Estimation;System-on-a-chip;Computational modeling;Mathematical model;Benchmark testing;Reliability","embedded systems;fault tolerant computing;microprocessor chips;safety-critical software;system-on-chip","embedded microprocessors;embedded systems;safety-critical applications;fault tolerant consideration;IP-based system-on-chip;system vulnerability;microprocessor vulnerability;microprocessor failure;mainstream microprocessors;very long instruction word processor","","2","6","","","","","","IEEE","IEEE Conferences"
"Optimal SVC placement in electric power systems using a genetic algorithms based method","I. Pisica; C. Bulac; L. Toma; M. Eremia","Electric Power Systems Department, Power Engineering Faculty, University ¿Politehnica¿ of Bucharest, Romania; Electric Power Systems Department, Power Engineering Faculty, University ¿Politehnica¿ of Bucharest, Romania; Electric Power Systems Department, Power Engineering Faculty, University ¿Politehnica¿ of Bucharest, Romania; Electric Power Systems Department, Power Engineering Faculty, University ¿Politehnica¿ of Bucharest, Romania","2009 IEEE Bucharest PowerTech","","2009","","","1","6","The problem of improving the voltage profile and reducing power losses in electrical networks is a task that must be solved in an optimal manner. At present time, this optimality can be achieved by efficient usage of existing facilities alongside with installing FACTS devices. The static VAr compensator (SVC) was chosen for study as its maturity and acceptable costs make it more usable in practical applications than other FACTS devices This paper proposes a genetic algorithm that tries to identify the optimal location and size of an SVC. A multi-criteria function is developed, comprising of both operational objectives and investment costs. The computer program is run on a 13 nodes test system, assessing improvements in voltage profile and reducing power losses. The purpose of this study is to validate the solution method in order for it to be adapted for systems of higher dimensionality.","","978-1-4244-2234-0978-1-4244-2235","10.1109/PTC.2009.5281841","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5281841","FACTS;SVC optimal placement;genetic algorithms","Static VAr compensators;Genetic algorithms;Voltage;Cost function;Admittance;Application software;Investments;System testing;Optimization methods;Power transmission lines","flexible AC transmission systems;genetic algorithms;static VAr compensators","optimal SVC placement;static VAr Compensator;electric power system;genetic algorithm;FACTS device;multicriteria function;computer program;13 nodes test system","","20","7","","","","","","IEEE","IEEE Conferences"
"Verification and validation communication layer of embedded Smart Card system","M. T. Lockman; A. Selamat","Faculty of Computer Science and Information Systems, Universiti Teknologi Malaysia,81310 Skudai, Johor, Malaysia; Faculty of Computer Science and Information Systems, Universiti Teknologi Malaysia,81310 Skudai, Johor, Malaysia","2008 International Conference on Electronic Design","","2008","","","1","5","RFID technology smart card has a wider user used in diverse area and application. Usually smart card system works by install a smart card reader at the entrance and connects through network which consist database and system application. Because the system required bigger database that stored staff information, cost of investment will be increase towards the system requirement. This is because usually the staff access can validate through authentication process that compare serial number and data on the card with a serial number stored on database. Embedded system by using rabbit microprocessor can reduce cost of system installation and easy to maintaining. By combining the rabbit 2000 microprocessor as controller the hardware and dynamic C as a software, both hardware and software design are easy to develop with the rabbit 2000 development kit. The development kit contain powerful single board computer called ldquothe jackrabbit boardrdquo that can help developer to write and test complex software and also will be able to prototype circuits that interface to a rabbit 2000 microcontroller. For this development kit we can built a embedded system that optimize their function by compare to conventional microcontroller that only doing specific task ,because several module of smart card application such as smart card module, lcd module, keypad module, buzzer module and electromagnetic lock module are easy to integrate. This paper proposes rapid embedded smart card system development by using rabbit 2000 microprocessor with dynamic C software. Verification and validation process will be study how the communication protocol layer handle the communication between rabbit 2000 and smart card reader.","","978-1-4244-2315","10.1109/ICED.2008.4786745","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4786745","","Smart cards;Rabbits;Databases;Microprocessors;Costs;Embedded system;Hardware;Circuit testing;Microcontrollers;Radiofrequency identification","embedded systems;message authentication;microprocessor chips;radiofrequency identification;smart cards;telecommunication security","embedded smart card system;RFID technology smart card;smart card reader;authentication process;rabbit microprocessor;hardware and software design;microcontroller;smart card module;electromagnetic lock module;communication protocol layer;jackrabbit board","","","7","","","","","","IEEE","IEEE Conferences"
"HW/SW Codesign of the H. 263 Video Coder","A. B. Atitallah; P. Kadionik; F. Ghozzi; P. Nouel; N. Masmoudi; H. Levi","Laboratory of Electronics and Information Technology, National Engineers School of Sfax (E.N.I.S.), BP W, 3038 Sfax - TUNISIA; IXL laboratory ¿ENSEIRB - University Bordeaux1 - CNRS UMR 5818, 351 Cours de la Libération, 33 405 Talence Cedex, FRANCE. benatita@enseirb.fr; IXL laboratory ¿ENSEIRB - University Bordeaux1 - CNRS UMR 5818, 351 Cours de la Libération, 33 405 Talence Cedex, FRANCE. kadionik@enseirb.fr; Laboratory of Electronics and Information Technology, National Engineers School of Sfax (E.N.I.S.), BP W, 3038 Sfax - TUNISIA; IXL laboratory ¿ENSEIRB - University Bordeaux1 - CNRS UMR 5818, 351 Cours de la Libération, 33 405 Talence Cedex, FRANCE. nouel@enseirb.fr; Laboratory of Electronics and Information Technology, National Engineers School of Sfax (E.N.I.S.), BP W, 3038 Sfax - TUNISIA. Nouri.Masmoudi@enis.rnu.tn; IXL laboratory ¿ENSEIRB - University Bordeaux1 - CNRS UMR 5818, 351 Cours de la Libération, 33 405 Talence Cedex, FRANCE","2006 Canadian Conference on Electrical and Computer Engineering","","2006","","","783","787","In this paper, we propose an optimized real-time H.263 video coder. The coder has been implemented in one FPGA device as HW/SW partitioned system. We made time analysis and optimization of the H.263 coder. On the basis of the achieved results, we decided for hardware implementation of the discrete cosine transform (DCT). Remaining parts were realized in software with NIOS II softcore processor. H.263 coder (NIOS II processor, DCT core) has been described by the VHDL language and implemented in Stratix EP1S10 FPGA. Finally, the coder has been tested on the Altera Stratix development board","0840-7789","1-4244-0038-41-4244-0038","10.1109/CCECE.2006.277632","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4054652","H.263;FPGA;HW/SW codesign","Hardware;Discrete cosine transforms;Video compression;Field programmable gate arrays;Videoconference;Transform coding;Laboratories;Testing;High definition video;IEC standards","data compression;discrete cosine transforms;field programmable gate arrays;hardware description languages;hardware-software codesign;video coding","HW-SW codesign;optimized real-time H.263 video coder;HW-SW partitioned system;discrete cosine transform;DCT;NIOS II softcore processor;VHDL language;Stratix EP1S10 FPGA;Altera Stratix development board;video compression scheme","","1","25","","","","","","IEEE","IEEE Conferences"
"Comparison of Intelligent Techniques to Solve Economic Load Dispatch Problem with Line Flow Constraints","S. P. karthikeyan; K. Palanisamy; L. J. Varghese; I. J. Raglend; D. P. Kothari","School of Electrical Sciences, VIT University, Vellore, Tamil Nadu. 632014, INDIA. e-mail: spk25in@yahoo.co.in; School of Electrical Sciences, VIT University, Vellore, Tamil Nadu. 632014, INDIA. e-mail: kpalanisamy79@yahoo.co.in; Christian College of Engineering and Technology, Oddanchatram, Dindigul District, Tamil Nadu, INDIA.; Christian College of Engineering and Technology, Oddanchatram, Dindigul District, Tamil Nadu, INDIA. e-mail: jacobraglend@rediffmail.com; Vice Chancellor, VIT University, Vellore, Tamil Nadu. 632014. INDIA. e-mail: dpk0710@yahoo.com, vc@vit.ac.in","2009 IEEE International Advance Computing Conference","","2009","","","446","452","A comparative analysis using different intelligent techniques has been carried out for the economic load dispatch (ELD) problem considering line flow constraints for the regulated power system to ensure a practical, economical and secure generation schedule. The objective of this paper is to minimize the total production cost of the thermal power generation. Economic load dispatch (ELD) has been applied to obtain optimal fuel cost. Optimal power flow has been carried out to obtain ELD solutions with minimum operating cost satisfying both unit and network constraints. In this paper, various intelligent techniques such as genetic algorithm (GA), evolutionary programming (EP), particle swarm optimization (PSO), and differential evolution (DE) have been applied to obtain ELD solutions. The proposed algorithm has been tested on two sample systems viz IEEE 30 bus system and a 15 unit system. The results obtained by the various intelligent techniques are compared. The solutions obtained are quite encouraging and useful in the economic environment. The algorithm and simulation are carried out using Matlab software.","","978-1-4244-2927-1978-1-4244-2928","10.1109/IADCC.2009.4809052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4809052","Economic Load Dispatch;Genetic Algorithm;Evolutionary Programming;Particle Swarm Optimization;Differential Evolution","Power generation economics;Environmental economics;Fuel economy;Power system economics;Cost function;Load flow analysis;Power system analysis computing;Power generation;Production;Power generation dispatch","genetic algorithms;load dispatching;load flow;particle swarm optimisation;power engineering computing;power transmission economics;thermal power stations","intelligent technique;economic load dispatch problem;line flow constraint;power system;thermal power generation;genetic algorithm;evolutionary programming;particle swarm optimization;differential evolution","","5","27","","","","","","IEEE","IEEE Conferences"
"Benefits of coordination in the operation of hydroelectric power systems: Brazilian case","T. C. Marques; M. A. Cicogna; S. Soares","Fac. of Electr. & Comput. Eng., State Univ. of Campinas, Brazil; Fac. of Electr. & Comput. Eng., State Univ. of Campinas, Brazil; Fac. of Electr. & Comput. Eng., State Univ. of Campinas, Brazil","2006 IEEE Power Engineering Society General Meeting","","2006","","","8 pp.","","This paper is concerned to estimate the benefits of coordination in the operation of hydroelectric power system. Optimization model representing the detail operating characteristics of hydro plants, including the consideration of the influence of forebay and tailrace elevation on hydro conversion efficiency were considered. The benefits of coordination have been estimated through the application of the optimization model in two different approaches. The first approach corresponds to the coordinated operation (CO) where the system is optimized as a whole. The second approach corresponds to the non-coordinated (NCO) operation where each plant optimizes its own operation without caring about the other downstream plants. The benefits were estimated on the calculation of duration curves for hydro generation and the identification of firm energy, as well as average and standard deviation values of energy availability, total cost operation and average productivity. Eighty-one hydro plants from the Brazilian system were used in the numerical tests","1932-5517","1-4244-0493","10.1109/PES.2006.1709574","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1709574","Hydropower systems;Energy availability;Coordination;Firm energy;Duration curves","Power systems;Computer aided software engineering;Power system modeling;Costs;Hydroelectric power generation;Predictive models;Feedback control;Availability;Fuels;Hydroelectric-thermal power generation","hydroelectric power stations;optimisation","hydroelectric power systems;Brazilian case;optimization model;forebay;tailrace elevation;hydro conversion efficiency;coordinated operation;noncoordinated operation;duration curves;firm energy identification;standard deviation values;energy availability;average deviation values;total cost operation;average productivity;Brazilian system","","2","11","","","","","","IEEE","IEEE Conferences"
"Design Space Exploration for a Custom VLIW Architecture: Direct Photo Printer Hardware Setting Using VEX Compiler","D. Saptono; V. Brost; F. Yang; E. Prasetyo","NA; NA; NA; NA","2008 IEEE International Conference on Signal Image Technology and Internet Based Systems","","2008","","","416","421","Increasingly more computing power is demanded for contemporary applications such as multimedia, 3D visualization, and telecommunication. This paper presents a design space exploration (DSE) experience for an embedded VLIW processor that allows finding out the best architecture for given application. The proposed method has been implemented and tested using an image processing chain for direct photo printer. Our results show a considerable improvement in hardware cost and performance. After the best architecture is identified, we applied a technique to optimize the code in VEX system that uses ¿inlining¿ function in order to reduce execution time.","","978-0-7695-3493","10.1109/SITIS.2008.69","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4725835","VLIW;Design Space Exploration;Compiler Optimization;VEX system","Space exploration;VLIW;Printers;Hardware;Computer architecture;Telecommunication computing;Multimedia computing;Visualization;Testing;Image processing","embedded systems;hardware-software codesign;image processing equipment;instruction sets;microprocessor chips;multiprocessing systems;parallel architectures;parallel machines;printers;program compilers","custom VLIW architecture;direct photo printer hardware setting;VEX compiler;design space exploration;embedded VLIW processor;image processing chain;inlining function","","7","6","","","","","","IEEE","IEEE Conferences"
"Exploiting the Correspondence between Micro Patterns and Class Names","J. Singer; C. Kirkham","NA; NA","2008 Eighth IEEE International Working Conference on Source Code Analysis and Manipulation","","2008","","","67","76","This paper argues that semantic information encoded in natural language identifiers is a largely neglected resource for program analysis. First we show that words in Java class names relate to class properties, expressed using the recently developed micro patterns language. We analyse a large corpus of Java programs to create a database that links common class name words with micro patterns. Finally we report on prototype tools integrated with the Eclipse development environment. These tools use the database to inform programmers of particular problems or optimization opportunities in their code.","","978-0-7695-3353","10.1109/SCAM.2008.23","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4637540","Patterns;Programmer Assistance;Java","Java;Benchmark testing;Encoding;Software;Prototypes;Optimization;Arrays","Java;natural languages;object-oriented programming;program diagnostics","micro patterns language;Java class names;semantic information encoding;natural language identifiers;Eclipse development environment;optimization","","8","16","","","","","","IEEE","IEEE Conferences"
"Decision-theoretic exploration of multiProcessor platforms","C. Silvano; D. Sciuto; D. Bruschi; G. Beltrame","Politecnico di Milano, Milano, Italy; Politecnico di Milano, Milano, Italy; Politecnico di Milano, Milano, Italy; Politecnico di Milano, Milano, Italy","Proceedings of the 4th International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS '06)","","2006","","","205","210","In this paper, we present an efficient technique to perform design space exploration of a multi-processor platform that minimizes the number of simulations needed to identify the power-performance approximate Pareto curve. Instead of using semi-random search algorithms (like simulated annealing, tabu search, genetic algorithms, etc.), we use domain knowledge derived from the platform architecture to set-up exploration as a decision problem. Each action in the decision-theoretic framework corresponds to a change in the platform parameters. Simulation is performed only when information about the probability of action outcomes becomes insufficient for a decision. The algorithm has been tested with two multi-media industrial applications, namely an MPEG4 encoder and an Ogg-Vorbis decoder. Results show that the exploration of the number of processors and two-level cache size and policy, can be performed with less than 15 simulations with 95% accuracy, increasing the exploration speed by one order of magnitude when compared to traditional operation research techniques.","","1-59593-370","10.1145/1176254.1176305","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4278516","basic block;micro-architecture simulation;program behavior","Space exploration;Computational modeling;Simulated annealing;Delay;Constraint optimization;Permission;Algorithm design and analysis;Genetic algorithms;Testing;MPEG 4 Standard","decision theory;embedded systems;logic design;Markov processes;system-on-chip","decision-theoretic exploration;multiprocessor platforms;design space exploration;power-performance approximate Pareto curve;domain knowledge;parameterized embedded SoC architectures;system-on-chip;Markov decision process","","9","11","","","","","","IEEE","IEEE Conferences"
"An active controller study of genetic algorithm search","Chen-Fang Tsai; Kuo-Ming Chao","Department of Industrial Management & Enterprise Information, Aletheia University, Taiwan, R.O.C.; School of Mathematical and Information Sciences, Coventry University, CV1 5FB, UK","2009 13th International Conference on Computer Supported Cooperative Work in Design","","2009","","","90","95","Inadequate selection of operators and parameters will produce premature convergence that will be avoided by adopting finer setting of parameters. This research was inspired by the concept of quality control utilizing as a control mechanism for process control in various GA searching processes. The process controllers attempt to apply these mechanisms to optimize the settings of the operators and parameters. In this study, we explored the function of the active setting is to adjust the trade-off between the exploration and exploitation by twelve's searching divisions. It applies evolution evidence to supervise the active setting of the GA parameters. Three different kinds of theoretical test-beds were carried out to validate this approach. The test results have been discovered that the active setting can make better the performance of genetic algorithm search. The test consequences also are showed that active is superior to static setting.","","978-1-4244-3534-0978-1-4244-3535","10.1109/CSCWD.2009.4968040","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4968040","Genetic Algorithms;Active Parameter Setting;Control Chart","Genetic algorithms;Collaborative work;Mirrors;Collaborative software;History;Bridges;Design automation;User interfaces;Engines;Broadcasting","control charts;genetic algorithms;quality control;search problems;statistical process control;statistical testing","active controller;genetic algorithm search;quality control;process control chart;statistical testing;static controller","","","18","","","","","","IEEE","IEEE Conferences"
"Memory Models for an Application-Specific Instruction-set Processor Design Flow","J. Wu; C. Lin; D. Chen; Y. Wang","NA; NA; NA; NA","2008 International Conference on Embedded Software and Systems","","2008","","","471","478","To optimize system performance for a specific target application, embedded system designers may add some new instructions, called application-specific instructions (ASIs), by automatic design flow. In past days, most application-specific instruction-set processor (ASIP) researches focus on reducing instruction latency to improve performance regardless of the impact of memory access. In this paper, a design flow is proposed to automatically generate ASIs and to compare the performance between considering register transferring and regardless of it. The experiment results show the proposed approach can achieve up to 14% performance improvement and 10% memory access reduction comparing to no register transferring consideration.","","978-0-7695-3287","10.1109/ICESS.2008.40","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4595599","ASIP;memory;register;custom instruction;application-specific instruction","Registers;Biological system modeling;Hardware;Assembly;Benchmark testing;Software;Discrete cosine transforms","embedded systems;instruction sets;logic design;memory architecture;microprocessor chips","memory model;application-specific instruction-set processor;system performance;embedded system;automatic design flow;instruction latency;memory access reduction","","2","12","","","","","","IEEE","IEEE Conferences"
"The European network ACE, Antenna Centre of Excellence","J. R. Mosig; P. Ingvarson","Lab. of Electromagn. & Acousti., Ecole Polytech. Fed. de Lausanne, Switzerland; NA","MELECON 2006 - 2006 IEEE Mediterranean Electrotechnical Conference","","2006","","","305","309","In order to improve the European research, the European Commission has created in the 6th Framework Programme (FP6) a new tool, called Network of Excellence (NoE). The Antenna Centre of Excellence, ACE, is such a network. Starting 1 January 2004, it has a duration of 4 years and 49 participants from 17 European countries. More than 250 researchers and around 100 PhD students are involved. ACE deals with the antenna function of radio systems. This includes the electromagnetic interface from conductors to free space radiated waves, the beam-forming functions, whether they are analogue or digital, and adaptive ""smart"" systems to optimize performance. The activities focus on with mm-wave and integrated antennas, wideband and multiband antennas, small and smart antennas, planar and conformal arrays, software, testing, education and dissemination in the antenna area","2158-8473;2158-8481","1-4244-0087","10.1109/MELCON.2006.1653099","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1653099","","Adaptive arrays;Broadband antennas;Electromagnetic radiation;Antenna arrays;Europe;Mobile antennas;Software testing;Frequency;Microwave antennas;Laboratories","adaptive antenna arrays;broadband antennas;conformal antennas;millimetre wave antennas;planar antenna arrays;radio networks","European network ACE;Antenna Centre of Excellence;European Commission;6th Framework Programme;Network of Excellence;European countries;antenna function;radio systems;electromagnetic interface;conductors;free space radiated waves;beam-forming functions;adaptive smart systems;mm-wave antennas;integrated antennas;wideband antennas;multiband antennas;smart antennas;planar arrays;conformal arrays","","","3","","","","","","IEEE","IEEE Conferences"
"PetaCache: A memory-Based Data-Server System","C. Boeheim; S. J. Gowdy; A. Hanushevsky; D. Leith; R. Melen; R. Mount; T. Pulliam; B. Weeks","NA; NA; NA; NA; NA; NA; NA; NA","2006 15th IEEE International Conference on High Performance Distributed Computing","","2006","","","349","350","Scientific advances depend increasingly on agility in the analysis of data, along with access to massive computation. The PetaCache project addresses the data-access issue by recognizing that the future for intense, non-sequential, data access must be based on low latency solid-state storage. The PetaCache architecture aims at a minimum unit cost, highly scalable hardware and software approach that can take advantage of existing and emerging solid-state storage technologies providing data-access latencies in the range 10-100 microseconds. A prototype system has been constructed as a cluster of 64 nodes hosting a total of one terabyte of memory. Client processors retrieve data from the data-server nodes over a switched Ethernet infrastructure using SLAC's xrootd data-server software. The system is in use for performance testing, optimization and trial deployments for scientific data analysis. It also provides an excellent platform for testing new data access paradigms","1082-8907","1-4244-0307","10.1109/HPDC.2006.1652178","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1652178","","Data analysis;Delay;Solid state circuits;Computer architecture;Costs;Hardware;Software prototyping;Prototypes;Information retrieval;Ethernet networks","cache storage;data analysis;file servers;memory architecture","PetaCache architecture;memory-based data-server system;data access;client processor;switched Ethernet infrastructure;SLAC xrootd data-server software","","","5","","","","","","IEEE","IEEE Conferences"
"Simulation of an Automated Warehouse for Steel Tubes","V. Colla; G. Nastasi; N. Matarese; A. Ucci","NA; NA; NA; NA","Tenth International Conference on Computer Modeling and Simulation (uksim 2008)","","2008","","","150","155","The paper presents a software which simulates an existing automated warehouse of steel tubes, including all the movements of the tubes packs conveyors as well as the management of all the operations for their storage and recovery for delivery to customers. The reason to develop such simulator lies in the needing to test new stocking strategies, that could overcome the drawback of the ones which are already implemented in the current system. Such strategies and their possible improvements are presented and discussed. As the simulator elaborates as inputs the real data of the tubes production and customer orders exactly in the format in which they are processed by the real automated warehouse management, realistic simulations are possible. Numerical results are shown, which demonstrates that the proposed improvements in the tube packs allocation and reordering successfully increase the warehouse capacity and efficiency through an improved exploitation of the available space.","","978-0-7695-3114-40-7695-3114","10.1109/UKSIM.2008.20","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4488922","simulation;software;modelling;automated warehouse;logistics;storage strategies;industrial automation;optimization","Steel;Computational modeling;Computer simulation;Storage automation;Humans;Production systems;Conference management;System testing;Manufacturing industries;Investments","","","","4","6","","","","","","IEEE","IEEE Conferences"
"An overview on the developments and improvements of a treatment planning system for BNCT","N. Cerullo; G. G. Daquino; L. Muzi","Pisa Univ., Italy; NA; NA","IEEE Transactions on Nuclear Science","","2006","53","3","1333","1338","Boron Neutron Capture Therapy (BNCT) is a radiation therapy for cancer that employs a neutron beam and a /sup 10/B-loaded drug to selectively kill tumor cells whilst sparing surrounding healthy tissues (HT). In conventional radiation therapy, treatment planning systems (TPSs) implementing simplified models of radiation transport and dose deposition allow to efficiently optimize all the relevant parameters prior to the patient's irradiation. This simplified approach is not feasible in BNCT, because the presence of neutrons requires the use of more complex radiation transport models. For this reason, current BNCT TPSs routinely perform several radiation transport simulations based on the Monte Carlo method. Our team has been involved in BNCT TPS research since 1996, introducing the original trait of employing in the simulation a three-dimensional map of the highly heterogeneous boron distribution in tissues, obtained through PET scanning of the target region. This approach differs markedly from the standard one, in which boron concentration is assumed to be uniform in each ""macro region"" within the patient's head, and its value is estimated on the basis of blood sampling. The first result of this research was the prototype software CARONTE, employed to test the feasibility of the new approach and to carry out a comparative study by applying the two different approaches to the same test case. The results, presented in this paper in terms of the computed physical dose rate due to the /sup 10/B reaction, show how the different assumptions made in the two approaches can significantly influence important TP parameters. This led to the development of Boron Distribution TP Software (BDTPS), an original and complete TPS. The different phases of the experimental validation of BDTPS, which included the design and construction of an ad hoc phantom able to host a number of vials loaded with /sup 10/B solutions, is presented here. The phantom, which subsequently underwent computed tomography (CT) and positron emission tomography (PET) scanning, was irradiated in the High Flux Reactor (HFR) at JRC, Petten, The Netherlands.","0018-9499;1558-1578","","10.1109/TNS.2006.872501","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1645040","Boron neutron capture therapy;dosimetry;image processing;Monte Carlo methods;positron emission tomography;treatment planning system","Boron;Positron emission tomography;Biomedical applications of radiation;Software testing;Imaging phantoms;Computed tomography;Neutron capture therapy;Cancer;Particle beams;Drugs","neutron capture therapy;cancer;tumours;cellular biophysics;Monte Carlo methods;positron emission tomography;blood;dosimetry;phantoms;medical image processing","treatment planning system;BNCT;boron neutron capture therapy;cancer;neutron beam;/sub 10/B-loaded drug;tumor cells;healthy tissues;conventional radiation therapy;radiation transport;dose deposition;patient irradiation;Monte Carlo method;3D map;heterogeneous boron distribution;boron concentration;patient head;blood sampling;prototype software CARONTE;computed physical dose rate;ad hoc phantom;computed tomography;positron emission tomography scanning;PET scanning;High Flux Reactor;HFR;dosimetry;image processing","","3","9","","","","","","IEEE","IEEE Journals & Magazines"
"Embedded Sensor of Forecast Conveyer Belt Breaks","Y. Guan; J. Zhang; Y. Shang; M. Wu; X. Liu","NA; NA; NA; NA; NA","2008 Fifth International Conference on Fuzzy Systems and Knowledge Discovery","","2008","5","","617","621","On the basis of studying conveyer belt break forecast device using X-ray systematically, the mathematical model was found and set up. And it would promote us to research the device further and optimize parameters. Firstly, the mathematical model of conveyer belt break forecast device was built for the first time by ""X-ray testing model"". The method avoids thoroughly the impossibilities of obtaining the mathematical model by use of ""basic particle convey function"". The application of the model shows: it is scientific and precise to build mathematical model by this method and also easily realized. Secondly, the simulation of X-ray break forecast device was realized by using visual programming technology. All the theory and methods above are universal and also applicable to other X-ray no destructive test (NDT) system.","","978-0-7695-3305","10.1109/FSKD.2008.643","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4666598","Conveyer belt;Embedded Sensor;Wire ropes","Belts;X-ray detection;X-ray detectors;Mathematical model;Steel;Technology forecasting;Educational institutions;Software systems;Hardware;Knowledge engineering","belts;computerised instrumentation;conveyors;mathematical analysis;mechanical engineering computing;nondestructive testing;sensors;visual programming;X-ray detection","embedded sensor;forecast conveyer belt breaks;basic particle convey function;visual programming technology;X-ray nondestructive test system","","1","10","","","","","","IEEE","IEEE Conferences"
"A virtual instrumentation-based on-line determination of a single/two phase induction motor drive characteristics at coarse start-up","C. Suciu; R. Campeanu; A. Campeanu; I. Margineanu; A. Danila","Universitatea Transilvania Bra¿ov, Romania; Universitatea Transilvania Bra¿ov, Romania; Universitatea Transilvania Bra¿ov, Romania; Universitatea din Craiova, Romania; Universitatea Transilvania Bra¿ov, Romania","2008 IEEE International Conference on Automation, Quality and Testing, Robotics","","2008","3","","440","443","The single/two phase induction motors (S/TPIM) are used in various low-power, low-cost applications. A representative example is the washing machine drum's main drive. Both manufacturers and designers have augmented their preoccupations and researches to optimize the efficiency of this drive due to the large and increasing number of devices in use and also stimulated by the market requests. At the same time, their efforts have been directed to maintain the drive's main advantage i. e. its low cost. A simple strategy to optimize the S/TPIM drive's efficiency is to replace the fixed capacitance of the starting capacitor of the motor with an electronically switched capacitor. In this approach, an accurate dynamic characteristic of the actuator, i.e. electric motor and washing machine's drum, need to be determined for the implementation of the starting capacitance's command law. In this paper an estimation method of the drive's mechanical strain at coarse start-up is presented. The dynamic model of the drive is based on the direct-and quadrature-axis theory; this approach provides accurate estimates of the drive's response. The on-line measurements of the input voltages and currents are performed with a fast data acquisition board and a virtual instrumentation software environment providing noise-free measurements. The main advantage of this method consists in its simple implementation on the washing machine and to provide torque predictions both in stationary and in dynamic regimes in real operating conditions.","","978-1-4244-2576-1978-1-4244-2577","10.1109/AQTR.2008.4588959","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4588959","","Rotors;Stator windings;Stators;Current measurement;Windings;Induction motors;Magnetic field measurement","data acquisition;electric current measurement;electric machine analysis computing;induction motor drives;virtual instrumentation;voltage measurement;washing machines","virtual instrumentation-based on-line determination;single/two phase induction motor drive;washing machine drum main drive;S/TPIM drive efficiency;electronically switched capacitor;coarse start-up;direct-and quadrature-axis theory;on-line measurement;data acquisition board;noise-free measurement","","2","5","","","","","","IEEE","IEEE Conferences"
"Design and application of a Linux Real Time board for distributed power generation","M. S. Carmeli; F. C. Dezza; G. Marchegiani; M. Mauri; D. Rosati","Department of Electrical Engineering, Politecnico di Milano, 20133 Milano, Italy; Department of Mechanics Politecnico di Milano, 20133 Milano, Italy; MCM Energy Lab. Srl, 20133 Milano, Italy; Department of Mechanics Politecnico di Milano, 20133 Milano, Italy; Department of Electrical Engineering, Politecnico di Milano, 20133 Milano, Italy","2009 35th Annual Conference of IEEE Industrial Electronics","","2009","","","367","372","The use of distributed energy resources is increasing and represents a valid answer to environmental problems and to the significant costs of more traditional energy sources. The need of developing distributed energy systems is strictly related to the need of flexible and scalable high level controller, able to manage control references in order to optimise global system efficiency. This paper proposes a solution based on Real Time Linux environment, which can be applied both to single source system and to multi-source ones. A soft PLC emulator together with other software tools are provided to help application development process. The proposed control board has been developed for wind turbine applications and has been preliminary tested.","1553-572X;1553-572X;1553-572X","978-1-4244-4648-3978-1-4244-4649-0978-1-4244-4650","10.1109/IECON.2009.5414950","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5414950","Wind power generations;microcontrollers;real time systems","Linux;Distributed power generation;Control systems;Application software;Energy resources;Environmental factors;Costs;Energy management;Power system management;Real time systems","distributed power generation;energy resources;Linux;power engineering computing;programmable controllers;software tools;wind turbines","Linux real time board;distributed power generation;distributed energy resources;soft PLC emulator;software tools;application development process;wind turbine applications;high level controller","","1","24","","","","","","IEEE","IEEE Conferences"
"A Methodology of Evolving User Requirements to Launch ERP in Aircraft Industry Environment","I. A. Manarvi; T. Ahmad","Department of Engineering Management, Center of Advanced Studies in Engineering, Islamabad, Pakistan. irfanmanarvi@yahoo.com; National University of Science and Technology, Rawalpindi, Pakistan. tanveerbagga@yahoo.com","2008 IEEE Aerospace Conference","","2008","","","1","21","Aircraft industry environment comprises of a highly advanced set of technologies, complex supply chain management systems, massive budgets and turnovers, wide ranging professionals involved in production or support roles for manufacturing, assembling or testing the aircraft components and assemblies. The production deadlines and sales targets are as critical as any other industry and the competition is global. Even then countries, organizations and entrepreneurs around the world are keen to invest or get involved in various sectors of this industry. An efficient organization would require a highly dynamic system to undertake the planning and execution of all activities in such environment as well as predict future requirements in a global competitive environment. Enterprise Resource Planning (ERP) since its inception has been providing solution to various industries under such complex environment, however the literature available for its applications in aerospace industry is quite limited. Therefore the organizations involved in various activities associated with aircraft manufacturing industry are either unable to launch ERP in their systems or fail during implementation phase because of no clear starting point. This research is focused on providing basic knowledge of developing user requirements for ERP requirements in aircraft industry environment. It would also provide the knowledge of systems and processes peculiar to aircraft industry to the ERP solution providers for optimizing their software for aerospace applications.","1095-323X","978-1-4244-1487-1978-1-4244-1488","10.1109/AERO.2008.4526668","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4526668","","Enterprise resource planning;Aerospace industry;Manufacturing industries;Aircraft manufacture;Assembly systems;Supply chain management;Production systems;System testing;Marketing and sales;Aerodynamics","aerospace industry;aircraft manufacture;aircraft testing;enterprise resource planning;supply chain management","aircraft manufacturing industry;user requirements;enterprise resource planning;ERP;complex supply chain management;aircraft component assembly;aircraft component testing;aerospace industry;aerospace applications","","","13","","","","","","IEEE","IEEE Conferences"
"The use of explicit congestion notification to shape traffic of an intelligent satellite system","W. Almuhtadi; D. R. Murphy; B. Cheng","Intelligent Satellite System Research, Ottawa, Ontario, Algonquin College, EION Inc., Canada; Intelligent Satellite System Research, Ottawa, Ontario, Algonquin College, EION Inc., Canada; Intelligent Satellite System Research, Ottawa, Ontario, Algonquin College, EION Inc., Canada","2008 IEEE Radio and Wireless Symposium","","2008","","","819","822","Since the official standardization of explicit congestion notification (ECN) in 2001, the differentiated service (DS) bits 6 and 7 in packets are now classified for purposes of shaping and prioritization. These classifications are used to mark packet streams for controlling traffic flow in an intelligent satellite system (ISS). Using a forward link connection between a transmitting ground terminal to a geostationary satellite (GEO) acting as a relay, to a receiving hub back on the earth; a traffic shaping software which evaluates the ECN type or classification is used to control traffic flow and the results are observed.","2164-2958;2164-2974","978-1-4244-1462-8978-1-4244-1463","10.1109/RWS.2008.4463618","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4463618","ISS;ECN;Differentiated Service;Traffic Shaping;Congestion Control","Shape;Intelligent systems;Satellites;Traffic control;Testing;Throughput;Yarn;Open source software;Monitoring;Relays","DiffServ networks;satellite communication;telecommunication congestion control;telecommunication traffic","explicit congestion notification;intelligent satellite system;differentiated service;packet streams;traffic flow control;forward link connection;geostationary satellite","","2","7","","","","","","IEEE","IEEE Conferences"
"Visual Modeling and Simulation for Laser Action Area in Rapid Prototyping","W. G. yong; T. G. xing; Z. H. liang; Y. Shan","NA; NA; NA; NA","2009 WRI World Congress on Computer Science and Information Engineering","","2009","5","","435","439","To select optimal processing parameters for rapid prototyping (RP) and improve the product efficiency, a visual modeling and simulating method for laser action area in RP was proposed. The SolidWorks' API (application programming interface) functions were transferred by the VB (visual basic) to program the system. This method could carry out each simulating treatment in processing for various slicing schemes, and it realizes the visualization of rapid prototyping simulation-results in SolidWorks. By analyzing and optimizing the results, a set of optimal parameters was obtained. Thus, it improves the processing efficiency as well as the accuracy and decreases cost.","","978-0-7695-3507","10.1109/CSIE.2009.288","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5170573","","Laser modes;Virtual prototyping;Prototypes;Laser sintering;Computer simulation;Laser theory;Optical materials;Product design;Testing;Computational modeling","application program interfaces;rapid prototyping (industrial);software prototyping;Visual BASIC","visual modeling;laser action;rapid prototyping;application programming interface;visual basic;SolidWorks;slicing schemes;product efficiency","","1","10","","","","","","IEEE","IEEE Conferences"
"Data quality monitor of the muon spectrometer tracking detectors of the ATLAS experiment at the Large Hadron Collider: First experience with cosmic rays","M. Iodice","Istituto Nazionale di Fisica Nucleare - Sezione di Roma Tre, Via della Vasca Navale, 84, 00146 Rome, Italy","2008 IEEE Nuclear Science Symposium Conference Record","","2008","","","2825","2829","The Muon Spectrometer of the ATLAS experiment at the CERN Large Hadron Collider is completely installed and many data have been collected with cosmic rays in different trigger configurations. In the barrel part of the spectrometer, cosmic ray muons are triggered with Resistive Plate Chambers, RPC, and tracks are obtained joining segments reconstructed in three measurement stations equipped with arrays of high-pressure drift tubes, MDT. The data are used to validate the software tools for the data extraction, to assess the quality of the drift tubes response and to test the performance of the tracking programs. We present a first survey of the MDT data quality based on large samples of cosmic ray data selected by the second level processors for the calibration stream. This data stream was set up to provide high statistics needed for the continuous monitor and calibration of the drift tubes response. Track segments in each measurement station are used to define quality criteria and to assess the overall performance of the MDT detectors. Though these data were taken in not optimized conditions, when the gas temperature and pressure was not stabilized, the analysis of track segments shows that the MDT detector system works properly and indicates that the efficiency and space resolution are in line with the results obtained with previous tests with a high energy muon beam.","1082-3654","978-1-4244-2714-7978-1-4244-2715","10.1109/NSSMIC.2008.4774959","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4774959","","Monitoring;Mesons;Spectroscopy;Detectors;Large Hadron Collider;Cosmic rays;Calibration;Software tools;Data mining;Software testing","","","","","10","","","","","","IEEE","IEEE Conferences"
"Development of very high rate and resolution neutron detectors with novel readout and DAQ hard- and software in DETNI","S. S. Alimov; A. Borga; A. Brogna; S. Buzzetti; F. Casinini; W. Dabrowski; T. Fiutowski; B. Gebauer; G. Kemmerling; M. Klein; B. Mindur; G. Modzel; C. Petrillo; F. Sacchetti; C. J. Schmidt; H. K. Soltveit; K. S. Solvag; R. Szczygiel; C. Schulz; C. Thielmann; U. Trunk; P. Wiacek; T. Wilpert","Helmholtz Zentrum Berlin für Materialien und Energie, D-14109, Germany; Physikalisches Institut, Universität Heidelberg, D-69120, Germany; Zentralinstitut für Elektronik, Forschungszentrum Jülich, D-52425, Germany; Physikalisches Institut, Universität Heidelberg, D-69120, Germany; INFM and Dipartimento di Fisica, Universita di Perugia, I-06123, Italy; Faculty of Physics and Applied Computer Science, AGH University of Science and Technology, PL-30-059 Krakow, Poland; Faculty of Physics and Applied Computer Science, AGH University of Science and Technology, PL-30-059 Krakow, Poland; Helmholtz Zentrum Berlin für Materialien und Energie, D-14109, Germany; Zentralinstitut für Elektronik, Forschungszentrum Jülich, D-52425, Germany; Zentralinstitut für Elektronik, Forschungszentrum Jülich, D-52425, Germany; Helmholtz Zentrum Berlin für Materialien und Energie, D-14109, Germany; Physikalisches Institut, Universität Heidelberg, D-69120, Germany; INFM and Dipartimento di Fisica, Universita di Perugia, I-06123, Italy; INFM and Dipartimento di Fisica, Universita di Perugia, I-06123, Italy; Gesellschaft für Schwerionenforschung, D-64291 Darmstadt, Germany; Physikalisches Institut, Universität Heidelberg, D-69120, Germany; Physikalisches Institut, Universität Heidelberg, D-69120, Germany; Faculty of Physics and Applied Computer Science, AGH University of Science and Technology, PL-30-059 Krakow, Poland; Helmholtz Zentrum Berlin für Materialien und Energie, D-14109, Germany; Physikalisches Institut, Universität Heidelberg, D-69120, Germany; Zentralinstitut für Elektronik, Forschungszentrum Jülich, D-52425, Germany; Faculty of Physics and Applied Computer Science, AGH University of Science and Technology, PL-30-059 Krakow, Poland; Helmholtz Zentrum Berlin für Materialien und Energie, D-14109, Germany","2008 IEEE Nuclear Science Symposium Conference Record","","2008","","","1887","1900","In the Joint Research Activity DETNI (DETectors for Neutron Instrumentation) of the FP6 EU Integrated Infrastructure Initiative for Neutron Scattering and Muon Spectroscopy (NMI3) prototypes of three novel modular thermal neutron area detector types, based on thin solid neutron converters, were built and tested, which were developed for time- and wavelength resolved neutron detection with 2-D spatial and time resolutions of 50–1000 µm (FWHM) and of up to 2 ns, respectively, and for counting rates in the 10<sup>7</sup>– 10<sup>8</sup>cps range, i.e. for coping with the highest resolution and rate requirements at pulsed spallation neutron sources with MW average proton beam power like ESS. The detector types are (i) four-fold segmented modules of Silicon microstrip detectors (Si-MSD) with<sup>157</sup>Gd converter layer between two double-sided Si sensors with 80 ¼m strip pitch, (ii) hybrid low-pressure microstrip gas chamber (MSGC) detectors with three-stage gas amplification and 2-D position-sensitive MSGC plates either side of a composite<sup>157</sup>Gd/CsI converter, (iii) CASCADE detectors with cascaded<sup>10</sup>B-coated GEM (Gas Electron Multiplier) foils either side of a 2-D position-sensitive readout electrode. For readout in DETNI prototypes of two novel, channel-wise self-triggered high-rate ASICs, of ADC-FPGA boards with Gigabit glass fiber readout and of the necessary data acquisition firmware and software have been developed. The ASICs, i.e. the 128-channel n-XYTER ASIC, optimized for the Si-MSD and strip hit rates of 200 khits/s, and the 32-channel MSGCROC ASIC for the MSGC with variable amplification and strip rates of 900 khits/s, deliver for each strip spatial and 4 (2) ns time stamp resolution, respectively, the latter e.g. for correlating x and y strips unambiguously, as well as analog amplitude resolution for center-of-gravity interpolation and gating. In this paper the current status of prototyping will be reported.","1082-3654","978-1-4244-2714-7978-1-4244-2715","10.1109/NSSMIC.2008.4774759","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4774759","","Neutrons;Data acquisition;Spatial resolution;Strips;Position sensitive particle detectors;Software prototyping;Prototypes;Microstrip;Application specific integrated circuits;Instruments","","","","4","25","","","","","","IEEE","IEEE Conferences"
"Reducing the Overhead of Real-Time Operating System through Reconfigurable Hardware","M. Song; S. H. Hong; Y. Chung","Kyung Hee University, Yong-in, Korea; Kyung Hee University, Yong-in, Korea; Kyung Hee University, Yong-in, Korea","10th Euromicro Conference on Digital System Design Architectures, Methods and Tools (DSD 2007)","","2007","","","311","316","This paper demonstrates the benefits of hardwiring two of the main RTOS functionalities for multitasking environment. They are context switching and software interrupt processing operations. In particular, the paper proposes reconfiguring the FPGA which includes the proposed one time reconfigurable CPU and RTOS hardware, optimized for a given application by tailoring the entire hardware around the particular tasks involved. As a demonstration, the proposed RTOS hardware was designed and tested on a multichannel speaker system application to yield 60% performance increase over the conventional software- based RTOS.","","0-7695-2978-X978-0-7695-2978","10.1109/DSD.2007.4341486","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4341486","","Real time systems;Operating systems;Hardware;Application software;Kernel;Multitasking;Postal services;Clocks;Control systems;Delay","field programmable gate arrays;logic design;operating systems (computers)","overhead reduction;real-time operating system;reconfigurable hardware;hardwiring;multitasking environment;context switching;software interrupt processing operations;FPGA;CPU;multichannel speaker system","","5","5","","","","","","IEEE","IEEE Conferences"
"Petascale visualization: Approaches and initial results","J. Ahrens; Li-Ta Lo; B. Nouanesengsy; J. Patchett; A. McPherson","Los Alamos National Laboratory, NM 87545, USA; Los Alamos National Laboratory, NM 87545, USA; Los Alamos National Laboratory, NM 87545, USA; Los Alamos National Laboratory, NM 87545, USA; Los Alamos National Laboratory, NM 87545, USA","2008 Workshop on Ultrascale Visualization","","2008","","","24","28","With the advent of the first petascale supercomputer, Los Alamos's Roadrunner, there is a pressing need to address how to visualize petascale data. The crux of the petascale visualization performance problem is interactive rendering, since it is the most computationally intensive portion of the visualization process. For terascale platforms, commodity clusters with graphics processors (GPUs) have been used for interactive rendering. For petascale platforms, visualization and rendering may be able to run efficiently on the supercomputer platform itself. In this work, we evaluated the rendering performance of multi-core CPU and GPU-based processors. To achieve high-performance on multi-core processors, we tested with multi-core optimized raytracing engines for rendering. For real-world performance testing, and to prepare for petascale visualization tasks, we interfaced these rendering engines with VTK and ParaView. Initial results show that rendering software optimized for multi-core CPU processors provides competitive performance to GPUs for the parallel rendering of massive data. The current architectural multi-core trend suggests multi-core based supercomputers are able to provide interactive visualization and rendering support now and in the future.","","978-1-4244-2861","10.1109/ULTRAVIS.2008.5154060","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5154060","","Data visualization;Rendering (computer graphics);Petascale computing;Supercomputers;Computer graphics;Testing;Engines;Pressing;Multicore processing;Software performance","coprocessors;data visualisation;multiprocessing systems;ray tracing;rendering (computer graphics)","petascale supercomputer;Los Alamos's Roadrunner;petascale data visualization;interactive rendering;terascale platforms;commodity clusters;multicore CPU;GPU-based processors;multicore optimized raytracing engines;VTK;ParaView;supercomputers","","3","12","","","","","","IEEE","IEEE Conferences"
"Language Identification from an Indian Multilingual Document Using Profile Features","M. C. Padma; P. A. Vijaya; P. Nagabhushan","NA; NA; NA","2009 International Conference on Computer and Automation Engineering","","2009","","","332","335","In order to reach a larger cross section of people, it is necessary that a document should be composed of text contents in different languages. But on the other hand, this causes practical difficulty in OCRing such a document, because the language type of the text should be pre-determined, before employing a particular OCR. In this research work, this problem of recognizing the language of the text content is addressed, however it is perhaps impossible to design a single recognizer which can identify a large number of scripts/languages. As a via media, in this research we have proposed to work on the prioritized requirements of a particular region, for instance in Karnataka state in India,generally any document including official ones, would contain the text in three languages-English-the language of general importance, Hindi-the language of National importance and Kannada -the language of State/Regional importance. We have proposed to learn identifying the language of the text by thoroughly understanding the nature of top and bottom profiles of the printed text lines in these three languages.Experimentation conducted involved 800 text lines for learning and 600 text lines for testing. The performance has turned out to be 95.4%.","","978-0-7695-3569","10.1109/ICCAE.2009.35","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4804543","Document Image Processing;Multi-lingual document;Language Identification;Top Profile;Bottom Profile;Feature extraction.","Natural languages;Optical character recognition software;Feature extraction;Educational institutions;Text recognition;Automation;Testing;Document image processing;Books;Text analysis","document handling;natural language processing;text analysis","language identification;Indian multilingual document;OCRing;text content;English;Hindi;Kannada","","6","11","","","","","","IEEE","IEEE Conferences"
"Low order radiation forces by analytic interpolation with degree constraint","G. Fanizza; K. Unneland","Division of Optimization and System Theory, Royal Institute of Technology, SE-100 44 Stockholm, Sweden; Department of Engineering Cybernetics, Norwegian University of Science and Technology, N-7491 Trondheim, Norway","2007 46th IEEE Conference on Decision and Control","","2007","","","2405","2410","The positive real modeling of a floating body is considered, whereas the main focus is on the radiation forces and moments. The radiation forces and moments describe the interaction of a floating body with the surrounding fluid. This type of mathematical model is of interest in among control and simulation of dynamical positioned vessels (i.e. ships and offshore platforms) and wave power plants. It has been proven that the radiation forces are passive, but very little attention have been drawn towards low order passive identification of these forces. Traditionally high order models have been obtained, and subsequently model order reduction have been applied to obtain low order models. Here, a direct approach for obtaining low order passive models using analytic interpolation with a degree constraint is applied. A case study involving a 3 degrees of freedom surface vessel is shown to illustrate the features of the proposed approach.","0191-2216","978-1-4244-1497-0978-1-4244-1498","10.1109/CDC.2007.4434656","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4434656","","Interpolation;Mathematical model;Equations;Marine vehicles;Power generation;Constraint theory;Software testing;Marine technology;Force control;USA Councils","hydrodynamics;interpolation;marine systems;motion control;position control","low order radiation forces;analytic interpolation;floating body;radiation moments;dynamical positioned vessels;wave power plants","","","32","","","","","","IEEE","IEEE Conferences"
"The Design of Embedded Web Server with Water Environment Online Monitoring","Q. Zhang; Y. Zhang; M. Liu","NA; NA; NA","2009 International Conference on Computational Intelligence and Software Engineering","","2009","","","1","4","In view of the problems of current water pollution monitoring systems in China, an embedded Web server with water environment online monitoring based on B/S is designed and implemented. Moreover, detail designs of hardware and software platform are given. The concept of symbol table based on the analysis of the characteristics of water pollution monitoring data is introduced as an asynchronous interactive manner of the control software and Web server to implement data collection. AJAX technologies are drawn on optimizing the performance of the embedded Web server. The test results proved that the system has full functions, high stability and good response performance.","","978-1-4244-4507","10.1109/CISE.2009.5366103","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5366103","","Web server;Water pollution;Computerized monitoring;Hardware;Computer science;Centralized control;Microprocessors;Ethernet networks;Pollution measurement;Calendars","environmental science computing;file servers;Internet;Java;water pollution","embedded Web server;water environment online monitoring;water pollution monitoring system;symbol table;AJAX technology","","","8","","","","","","IEEE","IEEE Conferences"
"Optimal Design and Analysis on Bearingless Permanent Magnet-type Synchronous Motors Using Finite Element Method","C. Jiang; H. Zhu; Z. Huang","Jiangsu University / School of Electrical and Information Engineering, Zhenjiang 212013, China. nuaajiangchang@sohu.com; Jiangsu University / School of Electrical and Information Engineering, Zhenjiang 212013, China; Jiangsu University / School of Electrical and Information Engineering, Zhenjiang 212013, China","2006 CES/IEEE 5th International Power Electronics and Motion Control Conference","","2006","2","","1","5","There are complicated relationships among the radial suspension forces, configuration of windings, permanent magnet thickness and currents in bearingless permanent magnet-type synchronous motors (BPMSM), so researching these relationships has important reference value for designing and optimizing BPMSM. Based on the principle of producing radial suspension forces in BPMSM, the mathematics models of radial forces are deduced. The gap magnetic circuits of BPMSM are studied using finite element method when the currents in radial force windings are changed. The demagnetization of permanent magnets is considered. The most critical area in the permanent magnets is made clear for both torque and radial force generations. The relationship between the radial suspension forces and permanent magnet thickness is calculated and analyzed when the permanent magnet thickness is changed under the fixed motor gap. The radial suspension force and the Maxwell force of the additional 2-pole radial windings and additional 6-pole radial windings are compared under the 4-pole motor windings. The relationship between radial suspension force and current is tested on prototype machine with p<sub>M</sub>=2 and p<sub>B</sub>=3 under the state of static suspension; the experiment conclusions have proved that the account results are accurate by using ANSYS software","","1-4244-0448","10.1109/IPEMC.2006.4778183","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4778183","bearingless motor;permanent-type magnet motor;Finite Element Method;radial suspension force","Magnetic analysis;Magnetic levitation;Synchronous motors;Finite element methods;Permanent magnets;Permanent magnet motors;Design optimization;Mathematics;Mathematical model;Magnetic circuits","demagnetisation;finite element analysis;machine windings;magnetic circuits;magnetic forces;permanent magnet motors;synchronous motors","optimal design;bearingless permanent magnet-type synchronous motors;BPMSM;finite element method;radial suspension forces;windings configuration;gap magnetic circuits;radial force windings;demagnetization;fixed motor gap;Maxwell force;prototype machine;ANSYS software","","1","7","","","","","","IEEE","IEEE Conferences"
"Real-Time Based Agent Architecture for Power Plant Control","J. H. Van Sickel; K. Y. Lee","Department of Electrical Engineering, the Pennsylvania State University, University Park, PA 16802; Department of Electrical and Computer Engineering, Baylor University, Waco, TX 76798","2009 15th International Conference on Intelligent System Applications to Power Systems","","2009","","","1","6","Building upon the work of the multi-agent system (MAS) based intelligent heuristic optimal control system (MAS-IHOCS) a new distributed real-time agent framework with time-warp (DRAFT) has been developed to enhance performance and development of multi-agent systems used to control real time processes. Using distributed discrete event simulation techniques for optimistic simulation with time-warp speeds up the simulation of the multi-agent system. Time-warp capabilities are embedded directly into the agents, which are capable of switching between fast-as-possible simulation using time-warp or pseudo-real time simulation. This allows faster development of agent systems with more thorough testing and validation. Additionally, these critical systems must continue to operate safely and securely despite failure of agents. This requires graceful degradation of the multi-agent system. The DRAFT agents will be applied to a simple power plant for demonstration purposes of these qualities.","","978-1-4244-5097-8978-1-4244-5098","10.1109/ISAP.2009.5352876","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5352876","Graceful degradation;multi-agent system;optimistic simulation;power plant;real-time;time-warp","Power generation;Multiagent systems;Time warp simulation;Discrete event simulation;Optimal control;Real time systems;Buildings;Intelligent systems;Intelligent agent;Intelligent control","distributed processing;optimal control;power engineering computing;power generation control;software agents","agent architecture;power plant control;multi-agent system;intelligent heuristic optimal control system;distributed real time agent framework;optimistic simulation","","","8","","","","","","IEEE","IEEE Conferences"
"Commissioning of the ATLAS reconstruction software with first data","A. Gibson","Department of Physics, University of Toronto, 60 Saint George Street, M5S 1A7, Ontario, Canada","2008 IEEE Nuclear Science Symposium Conference Record","","2008","","","2661","2666","Looking towards first LHC collisions, the ATLAS detector is being commissioned using all types of physics data available: cosmic rays, beam-halo and beam-gas events produced during LHC single beam operations. In addition to putting in place the trigger and data acquisition chains, commissioning of the full software chain is a primary goal. This is interesting not only to ensure that the reconstruction, monitoring and simulation chains are ready to deal with LHC physics data, but also to understand the detector performance in view of achieving the physics requirements. Cosmic rays have allowed us to study the ATLAS detector in terms of efficiencies, resolutions, channel integrity, and alignment and calibrations. They have also allowed us to test and optimize the muon combined performance algorithms.","1082-3654","978-1-4244-2714-7978-1-4244-2715","10.1109/NSSMIC.2008.4774925","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4774925","","Large Hadron Collider;Physics;Cosmic rays;Detectors;Event detection;Data acquisition;Monitoring;Calibration;Testing;Mesons","","","","1","16","","","","","","IEEE","IEEE Conferences"
"Automated Design of Spacecraft Telecommunication Subsystems Using Evolutionary Computational Techniques","R. J. Terrile; M. Kordon; M. Postma; J. Salcedo; D. Hanks; E. Wood","Jet Propulsion Laboratory, California Institute of Technology, 4800 Oak Grove Drive, Pasadena, CA 91109. 818-354-6158, rich.terrile@jpl.nasa.gov; Jet Propulsion Laboratory, California Institute of Technology, 4800 Oak Grove Drive, Pasadena, CA 91109. 818-354-6158; Jet Propulsion Laboratory, California Institute of Technology, 4800 Oak Grove Drive, Pasadena, CA 91109. 818-354-6158; Jet Propulsion Laboratory, California Institute of Technology, 4800 Oak Grove Drive, Pasadena, CA 91109. 818-354-6158; Jet Propulsion Laboratory, California Institute of Technology, 4800 Oak Grove Drive, Pasadena, CA 91109. 818-354-6158; Jet Propulsion Laboratory, California Institute of Technology, 4800 Oak Grove Drive, Pasadena, CA 91109. 818-354-6158","2007 IEEE Aerospace Conference","","2007","","","1","9","This paper discusses the application of evolutionary computing to a dynamic space vehicle telecommunication subsystem resource and performance simulation in a parallel processing environment. Our objective is to demonstrate the feasibility, application and advantage of using evolutionary computation techniques for the early design search and optimization of space systems. With this approach, engineers specify several sets of conditional subsystem performance criteria to trade off subsystem goals of mass, cost, performance and risk. Once specified, the integrated evolutionary/simulation software will then automatically generate a design option for each criteria, selecting and sizing telecom elements based on the space system's anticipated performance in the simulated environment. Initial activity plans from an actual JPL mission, Deep Impact (DI) are used to test the software. Our results have shown human-competitive advantages by generating credible design concepts much faster than humans are able to and without the need for expert initial designs.","1095-323X","1-4244-0524-61-4244-0525","10.1109/AERO.2007.352991","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4161709","","Space vehicles;Telecommunication computing;Computational modeling;Computer applications;Concurrent computing;Vehicle dynamics;Parallel processing;Application software;Evolutionary computation;Design optimization","aerospace computing;aerospace instrumentation;aerospace simulation;evolutionary computation;parallel processing;space vehicles;telecommunication computing;telecommunication equipment","spacecraft telecommunication subsystems;evolutionary computing;dynamic space vehicle telecommunication subsystem;parallel processing environment;space systems;conditional subsystem performance criteria;integrated evolutionary/simulation software;telecom elements;simulated environment;actual JPL mission;Deep Impact","","5","10","","","","","","IEEE","IEEE Conferences"
"Data Model and Virtual Database Engine for Grid Environment","H. Wenlong; L. Xiaolin; J. Jixiang; F. Yu; X. Yi","Chinese Academy of Sciences, Beijing; Chinese Academy of Sciences, Beijing; Chinese Academy of Sciences, Beijing; Yuandong Road,GaoCheng Town,Yixing 214257,China; Chinese Academy of Sciences, Beijing","Sixth International Conference on Grid and Cooperative Computing (GCC 2007)","","2007","","","823","829","There are many challenges of data sharing both in grid environment and traditional data integration domains, such as sharing between heterogeneous, distributed data sources. In grid environment, there are many specific features: limited data sharing, frequently changing data sources and large amounts of concurrent users. Under these conditions, it is strongly needed to provide more usable, more flexible system software and unique application programming interfaces to solve these problems. Vega information grid team has provided REVP data model and invented a flexible schema mapping tool employing reference technique to achieve these goals. In addition, performance in grid environment is especially important and needs some different optimization strategy. VIG has taken some optimization measures to improve the performance for distributed query. This paper elaborates the REVP data model and virtual database engine. Performance test shows that the query optimization measures are effective.","2160-4908;2160-4916","0-7695-2871","10.1109/GCC.2007.58","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4293867","","Data models;Engines;Web services;Query processing;Internet;Relational databases;Distributed databases;Computers;System software;Application software","data models;distributed databases;grid computing;query processing","REVP data model;virtual database engine;grid environment;Vega Information Grid team;flexible schema mapping tool;reference technique;query optimization measures;heterogeneous distributed data sources","","1","18","","","","","","IEEE","IEEE Conferences"
"Stochastic search-based neural networks learning algorithms","K. P. Nikolic; I. B. Scepanovic","Department of Informatics, Faculty of Management, Novi Sad, Republic of Serbia; Department of Informatics, Faculty of Management, Novi Sad, Republic of Serbia","2008 9th Symposium on Neural Network Applications in Electrical Engineering","","2008","","","103","107","We report on the application of artificial neural networks (ANN) learning algorithms, based on stochastic search, in learning and optimization processes. In our earlier studies we have shown a possibility for the application of the stochastic search algorithm (SSA) in relation to system optimization and identification, as well as learning processes of certain types of ANN. Here we modify stochastic search method by using certain algorithms as an alternative in ANN learning process. Furthermore, we compare the results from SSA and back propagation error (BPE). In certain cases, SSA are more favourable vs. BPE, particularly during complex learning processes on static ANN. Thus, SSA is effective engineering tool in ANN optimization, as well as in learning processes. We have tested different SSA examples with our specially developed software application.","","978-1-4244-2903-5978-1-4244-2904","10.1109/NEUREL.2008.4685579","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4685579","Learning;Optimization;Multilayer ANN;Stochastic Search Algorithm;Convergence SSAs","Stochastic processes;Neural networks;Artificial neural networks;Network synthesis;Simulated annealing;Convergence;Signal synthesis;Search methods;Artificial intelligence;Informatics","backpropagation;neural nets;search problems;stochastic programming","artificial neural networks learning algorithms;optimization processes;stochastic search algorithm;SSA;back propagation error;software application development","","","22","","","","","","IEEE","IEEE Conferences"
"Bug Mining Model Based on Event-Component Similarity to Discover Similar and Duplicate GUI Bugs","N. K. Nagwani; P. Singh","Department of Computer Sci. & Engg., National Institute of Technology, Raipur, India, nknagwani.cs@nitrr.ac.in; Department of Computer Sci. & Engg., National Institute of Technology, Raipur, India, psingh.cs@nitrr.ac.in","2009 IEEE International Advance Computing Conference","","2009","","","1388","1392","All most all of the bugs related to graphical user interface (GUI) module of applications and are described in terms of events associated with GUI components. In this paper, a bug mining model for discovering duplicate and similar GUI bugs is presented and approach for detecting the similar and duplicate GUI bugs is described. Resolution of similar and duplicate bugs are almost identical, so if similar and duplicates are identified it will optimize the time for fixing reported GUI bugs and it can also help in achieving the faster development. A GUI bug can be transformed into a sequence of events, components and expected implementation requirements for each GUI event. This transformation is used in this paper to discover the similar and duplicate GUI bugs. First all the GUI bugs are transformed into events, components and requirements sequence, then these sequences are pair wise matched and common subsequence is generated which will indicate the similarity for the GUI bugs.","","978-1-4244-2927-1978-1-4244-2928","10.1109/IADCC.2009.4809219","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4809219","similar bugs;duplicate bugs;event component similarity","Graphical user interfaces;Computer bugs;Application software;Databases;Testing;Information retrieval;Image retrieval;Documentation;Software performance;Terminology","graphical user interfaces;program debugging","bug mining model;event-component similarity;graphical user interface module;GUI components;GUI event;duplicate GUI bugs;GUI bugs similarity","","1","11","","","","","","IEEE","IEEE Conferences"
"High-convergence-speed low-computation-complexity SVD algorithm for MIMO-OFDM systems","Cheng-Zhou Zhan; Kai-Yuan Jheng; Yen-Lian Chen; Ting-Jhun Jheng; An-Yeu Wu","Graduate Institute of Electronics Engineering , National Taiwan University, Taipei, Taiwan; Graduate Institute of Electronics Engineering , National Taiwan University, Taipei, Taiwan; Graduate Institute of Electronics Engineering , National Taiwan University, Taipei, Taiwan; Graduate Institute of Electronics Engineering , National Taiwan University, Taipei, Taiwan; Graduate Institute of Electronics Engineering , National Taiwan University, Taipei, Taiwan","2009 International Symposium on VLSI Design, Automation and Test","","2009","","","195","198","Multiple-input multiple-output (MIMO) wireless communication systems with orthogonal frequency-division multiplexing (OFDM) achieve high spectral efficiency high channel capacity, and many MIMO-OFDM systems use the spatial multiplexing technique to improve the system performance. The MIMO-OFDM systems require the singular values and the corresponding singular vectors of the channel matrix by exploiting singular value decomposition (SVD). The information of the right singular-vector matrix can be fed back to the transmitter for linear precoding to improve the error performance when facing the channel matrix with rank deficiency problem. This work proposes a SVD algorithm with fast convergence speed, which is suitable for the MIMO channels with short coherent time. The proposed SVD algorithm has the following features: (1) low total computational complexity, (2) fast convergence speed, (3) the ability of reconfigurable to different numbers of transmitter and receiver antennas, and (4) insensitive to the dynamic range problem, which is suitable for hardware implementation.","","978-1-4244-2781-9978-1-4244-2782","10.1109/VDAT.2009.5158128","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5158128","","MIMO;Matrix decomposition;Frequency division multiplexing;OFDM;Transmitters;Convergence;Wireless communication;Channel capacity;System performance;Vectors","communication complexity;MIMO communication;OFDM modulation;receiving antennas;singular value decomposition;software radio;transmitting antennas;wireless channels","high-convergence-speed low-computation-complexity;SVD algorithm;MIMO-OFDM systems;multiple-input multiple-output systems;wireless communication;orthogonal frequency-division multiplexing;high-spectral efficiency high-channel capacity;spatial multiplexing technique;singular value decomposition;singular-vector matrix;reconfigurable ability;transmitter antenna;receiver antenna;hardware implementation","","4","6","","","","","","IEEE","IEEE Conferences"
"A Framework for Optimal Planning in Large Distribution Networks","S. Najafi; S. H. Hosseinian; M. Abedi; A. Vahidnia; S. Abachezadeh","NA; NA; NA; NA; NA","IEEE Transactions on Power Systems","","2009","24","2","1019","1028","Large scale distribution system planning is a relatively complex and reasonably difficult problem. This paper proposes the application of improved genetic algorithm (GA) for the optimal design of large scale distribution systems in order to provide optimal sizing and locating of the high and medium voltage (HV and MV) substations, as well as medium voltage (MV) feeders routing, using their corresponding fixed and variable costs associated with operational and optimization constraints. The novel approach presented in the paper solves hard satisfactory optimization problems with different constraints in large scale distribution networks. This paper presents a new concept based on loss characteristic matrix introduced for optimal locating of MV substations, followed by new methodology based on graph theory and GA for optimal locating of the HV substations and MV feeders routing in a real size distribution network. Minimum spanning tree algorithm is employed to generate set of feasible initial population. In the present article to reduce computational burden and avoid huge search space leading to infeasible solutions, special coding methods are generated for GA operators to solve optimal feeders routing. The proposed coding methods guarantee the validity of the solution during the progress of the genetic algorithm toward the global optimal solution. The developed GA-based software is tested in a real size large scale distribution system and the well satisfactory results are presented.","0885-8950;1558-0679","","10.1109/TPWRS.2009.2016052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4808226","Distribution system planning (DSP);genetic algorithm (GA);graph theory;long-term load forecasting","Large-scale systems;Substations;Routing;Genetic algorithms;Medium voltage;Constraint optimization;Algorithm design and analysis;Cost function;Design optimization;Graph theory","genetic algorithms;load forecasting;power distribution planning;trees (mathematics)","large distribution network;optimal planning framework;genetic algorithm;medium voltage feeder routing;optimization problems;loss characteristic matrix;MV substation;minimum spanning tree algorithm;coding method","","70","19","","","","","","IEEE","IEEE Journals & Magazines"
"Experimental study of wide-area 10 Gbps IP transport technologies","N. S. V. Rao; S. W. Poole; S. E. Hicks; C. Kemper; S. Hodson; G. Hinkel; J. Lothian","Oak Ridge National Laboratory, USA; Oak Ridge National Laboratory, USA; Oak Ridge National Laboratory, USA; Oak Ridge National Laboratory, USA; Oak Ridge National Laboratory, USA; Oak Ridge National Laboratory, USA; Oak Ridge National Laboratory, USA","MILCOM 2009 - 2009 IEEE Military Communications Conference","","2009","","","1","7","A number of large-scale applications require 10 Gbps data transport rates between high-performance computing, storage and file systems, which are connected over wide-area networks. Achieving such capability requires end-to-end optimization of all required hardware and software components including: (i) long haul SONET or 10 GigE connections; (ii) transport protocols such as TCP and UDT, (iii) encryption devices deployed at connection end points, and (iv) host computing, storage and file systems. We present a systematic experimental study of these technologies over 10 Gbps wide-area connections. We first describe a national-scale network testbed with high-performance edge and host systems, which supports dynamic provisioning of 10 Gbps connections of varying lengths. We generate the performance profiles for TCP and UDP with and without encryption devices, and qualitatively describe the effects of latter on transport methods. For file transfers using TCP and UDT, the encryption devices have the equivalent effect of reduced latency and smoother dynamics, respectively, due to on-board buffers. Our experimental results show that these encryption devices lead to higher or smoother file transfer throughputs in spite of the added latency due to packet encryption and decryption.","2155-7578;2155-7586","978-1-4244-5238-5978-1-4244-5239","10.1109/MILCOM.2009.5379777","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5379777","Network test-bed;10 GigE WAN-PHY;encryptors;HAIPE;TCP;UDP;UDT","Cryptography;File systems;Delay;Large-scale systems;Application software;Computer networks;Hardware;SONET;Transport protocols;System testing","cryptography;IP networks;transport protocols","IP transport technologies;long haul SONET;transport protocols;TCP protocols;UDT protocols;host computing;file systems;encryption devices;bit rate 10 Gbit/s","","2","10","","","","","","IEEE","IEEE Conferences"
"Construction of opening book in connect6 with its application","Jun-jie Tao; Chang-ming Xu; Kang Han; Xin-he Xu","Software College of Northeastern University, Shenyang 110004, China; College of Information Science and Engineering, Shenyang 110004, China; College of Information Science and Engineering, Shenyang 110004, China; College of Information Science and Engineering, Shenyang 110004, China","2009 Chinese Control and Decision Conference","","2009","","","4530","4534","An opening book is an important part in most game-playing computer programs. The so-called opening book is a database which contains an ocean of grandmaster's game records. Usually it is constructed manually by experts, or by selecting the excellent position from the massive raw game records by statistical. At the same time, some opening books can study and be optimized from new positions. But in Connect6, it is impossible to construct an excellent opening book only by experiences from human beings. The reasons are: firstly, the average branch factor in Connect6 is huge; secondly, the threat-based moves broadly existed in many positions, it makes the positions with small differences have opposite results. We use computer to generate the book automatically. This paper presents the way we construct the opening book of Connect6, and the convenience it takes when we test the program.","1948-9439;1948-9447","978-1-4244-2722-2978-1-4244-2723","10.1109/CCDC.2009.5191947","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5191947","opening book;iterative-deepening;connect6;Threat-Based Search","Books;Educational institutions;Databases;Oceans;Humans;Application software;Information science;Testing;State-space methods","computer games;database indexing;tree data structures;tree searching","opening book database construction;Connect6;game-playing computer program;grandmaster game record;average branch factor;threat-based search tree;automatic book generation;iterative deepening;tree data structure;index structure","","5","14","","","","","","IEEE","IEEE Conferences"
"Planning under uncertainty, ensembles of disturbance trees and kernelized discrete action spaces","B. Defourny; D. Ernst; L. Wehenkel","Department of Electrical Engineering and Computer Science of the University of Liège (Belgium); Department of Electrical Engineering and Computer Science of the University of Liège (Belgium); Department of Electrical Engineering and Computer Science of the University of Liège (Belgium)","2009 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning","","2009","","","145","152","Optimizing decisions on an ensemble of incomplete disturbance trees and aggregating their first stage decisions has been shown as a promising approach to (model-based) planning under uncertainty in large continuous action spaces and in small discrete ones. The present paper extends this approach and deals with large but highly structured action spaces, through a kernel-based aggregation scheme. The technique is applied to a test problem with a discrete action space of 6561 elements adapted from the NIPS 2005 SensorNetwork benchmark.","2325-1824;2325-1867","978-1-4244-2761","10.1109/ADPRL.2009.4927538","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4927538","","Uncertainty;Stochastic processes;Application software;Benchmark testing;Decision making;Operations research;Large-scale systems;Power generation;Processor scheduling;Dynamic programming","decision making;optimisation;trees (mathematics)","disturbance trees ensembles;Kernelized discrete action spaces;incomplete disturbance trees;model-based planning;large continuous action spaces;kernel-based aggregation scheme;sensor network","","1","41","","","","","","IEEE","IEEE Conferences"
"Battery management systems in the China-made “Start” series FCHVs","S. Zechang; W. Xuezhe; D. Haifeng","College of Automotive Study, Tongji University, Shanghai, China; College of Automotive Study, Tongji University, Shanghai, China; College of Automotive Study, Tongji University, Shanghai, China","2008 IEEE Vehicle Power and Propulsion Conference","","2008","","","1","6","In the China-made ldquoStartrdquo series fuel cell hybrid vehicles (FCHVs), the power battery pack, which contains a series of Lithium-ion batteries, plays an important role in the dynamic system. For developing the vehicles with high performance and good reliability, the batteries have to be managed to obtain maximum performance under various operating conditions. In this paper, a battery management system (BMS) was introduced which was designed for optimizing the use of the batteries. This system was a level-based system which contains a high-level system called ldquoCentral ECUrdquo (CECU) and several low-level systems called ldquoLocal ECUrdquo (LECU). The software of the battery management system was designed based on the UC/OS-II to guarantee the real-time requirement. The system performs such several tasks including states monitoring (State of Charge, SOC and State of Health, SOH), creepage detection, cell balance, thermal management, power limitation, diagnostics etc. In the two experiments shown in the paper, results indicate that this BMS performs well in managing the power batteries used in ldquoStartrdquo series FCHVs, it could estimate the states of the battery accurately, detect the creepage and do some protect in real time, balance the cell very well and maintain the temperature of the battery pack in a satisfactory manner. Other critical functions have also been tested during the vehiclepsilas on-road testing.","1938-8756","978-1-4244-1848-0978-1-4244-1849","10.1109/VPPC.2008.4677461","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4677461","Battery management system;Lithium-ion;FCHVs","Battery management systems;Power system management;Battery powered vehicles;Thermal management;Energy management;Testing;Fuel cells;Fuel cell vehicles;Vehicle dynamics;Power system reliability","battery management systems;battery powered vehicles;fuel cell vehicles;lithium;secondary cells","battery management systems;China-made start series FCHV;fuel cell hybrid vehicles;power battery pack;lithium-ion batteries;central ECU;local ECU;UC-OS-II;state monitoring;creepage detection;cell balance;thermal management;power limitation;vehicle on-road testing","","5","10","","","","","","IEEE","IEEE Conferences"
"A Hough Transform Rapid Prototyping System using the Matlab Embedded Target for the TI TMS320DM642 EVM","C. G. Ho","MIG Development. Email: cheyneho@netscape.net","2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07","","2007","2","","II-381","II-384","The embedded target for TI TMS320C6000 DSP is a useful development tool, which can be used to generate executable codes for a range of targets, from a set of Simulink model blocks. In this paper we outline the process of code generation for the TMS320DM642 evaluation module (EVM) using a Hough transform algorithm. This was computed using the forward Radon transform, evaluated using the central slice theorem, and involves a 2-D discrete Fourier transform (DFT), a rectangular to polar coordinate transformation, and a 1-D DFT. This was implemented using Simulink model blocks together with custom blocks, which were written using the S-function builder. The embedded target generates all the video capture and display routines required, so that the algorithm can be developed in Matlab and Simulink, before being tested on the EVM. Problems and limitations with the embedded target are highlighted, and solutions and code profiling optimizations are proposed.","1520-6149;2379-190X","1-4244-0727-31-4244-0728","10.1109/ICASSP.2007.366252","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4217425","Digital signal processors;Hough transforms;Image processing;Radon transforms;Software prototyping","Prototypes;Signal processing algorithms;Discrete Fourier transforms;Digital signal processing;Mathematical model;Displays;Image processing;Libraries;Discrete transforms;Biomedical signal processing","digital signal processing chips;discrete Fourier transforms;Hough transforms;program compilers;Radon transforms","Hough transform rapid prototyping system;Matlab embedded target;TI TMS320DM642 EVM;TI TMS320C6000 DSP;Simulink model blocks;code generation;evaluation module;forward Radon transform;central slice theorem;2D discrete Fourier transform;rectangular to polar coordinate transformation;S-function builder;video capture;display routines;code profiling optimizations","","","16","","","","","","IEEE","IEEE Conferences"
"Bayesian Face Recognition Using 2DPCA","N. Liping; Z. Yanbin; L. XinYuan; D. Yuqiang","NA; NA; NA; NA","2009 International Forum on Information Technology and Applications","","2009","2","","567","570","Bayesian subspace analysis has been successfully applied in face recognition. However, the direct application of the algorithm is much more computationally intensive. In this paper, a novel Bayesian approach to face recognition based on 2DPCA is proposed. Firstly the system uses 2DPCA to select the 10 top-ranked candidate images, secondly these candidate images and each testing image are decomposed into low frequency and high frequency sub-band images by applying wavelet transform, Bayesian recognition is parallel processed using these sub-band images. The face recognition result was gained through weigh-adding arraying. Its efficiency and superiority are clarified by comparative experiment on a subset of FERET face data.","","978-0-7695-3600","10.1109/IFITA.2009.223","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5231409","Face recognition;2DPCA;Wavelet Transform;Bayesian theor","Bayesian methods;Face recognition;Covariance matrix;Principal component analysis;Wavelet transforms;Information technology;Application software;Testing;Frequency;Image recognition","belief networks;face recognition;principal component analysis;wavelet transforms","Bayesian subspace analysis;face recognition;wavelet transform;2DPCA;principal component analysis","","1","8","","","","","","IEEE","IEEE Conferences"
"Magnetic characteristics of saturable pulse transformer in magnetic pulse compression system","D. Zhang; P. Yan; J. Wang; Y. Zhou","Institute of Electrical Engineering, Chinese Academy of Sciences, Beijing 100190, CHINA; Institute of Electrical Engineering, Chinese Academy of Sciences, Beijing 100190, CHINA; Institute of Electrical Engineering, Chinese Academy of Sciences, Beijing 100190, CHINA; Institute of Electrical Engineering, Chinese Academy of Sciences, Beijing 100190, CHINA","2009 IEEE Pulsed Power Conference","","2009","","","623","626","Saturable pulse transformer (SPT) is finding increased use in Magnetic Pulse Compressor (MPC). SPTs are fabricated from ferromagnetic amorphous metal alloy ribbons. And the magnetic characteristics of these amorphous metals are discussed in relation to optimizing the performance of SPTs in such applications. To evaluate the magnetic performance of the magnetic core, a `1-cosine' waveforms test stand is designed and built. The test stand is able to operate at modest repetition rates. The magnetization rates achieved in the experiments extend from 0.8 to 2.5 T/¿s, corresponding to saturation times from 1 to 4 ¿s. The current through the core is calculated by voltage across the resistive load, and the loop voltage is picked up with a single wire loop and integrated by software. B-H curves are derived from the measured voltage and current waveforms. Comparisons are presented for two Metal-glass cores which one is Mylar-insulated and the other not. Of the Mylar insulated core the trend is clear that increasing the packing factor, corresponding to increasing the winding tension, and more material in the core, tended to reduce the usable volt-second area and the eddy current loss together. Formulas are given for selecting core dimensions for IGBT switch protection. Finally, more practical energy transfer in SPT and the effects of leakage current are presented by applying custom characteristics to a two-stage MPC in Pspice simulation.","2158-4915;2158-4923","978-1-4244-4064-1978-1-4244-4065","10.1109/PPC.2009.5386352","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5386352","","Pulse transformers;Pulse compression methods;Amorphous magnetic materials;Saturation magnetization;Voltage;Magnetic materials;Photonic crystals;Amorphous materials;Magnetic cores;Testing","eddy current losses;magnetic cores;power semiconductor switches;pulse compression;pulse transformers","magnetic characteristics;saturable pulse transformer;SPT;magnetic pulse compressor;MPC;ferromagnetic amorphous metal alloy ribbons;cosine waveforms test stand;resistive load;loop voltage;single wire loop;B-H curves;current waveforms;Metal-glass cores;eddy current loss;IGBT switch protection;Pspice simulation;packing factor;magnetic pulse compression system","","","8","","","","","","IEEE","IEEE Conferences"
"Improvements of SEU tolerance by spatial redundancy in digital circuits","M. Grecki; G. Jablonski; D. Makowski","Deutches Elektronnen Synchrontron, Hamburg, Germany; Department of Microelectronics and Computer Science, Technical University of Lodz, Poland; Department of Microelectronics and Computer Science, Technical University of Lodz, Poland","2009 MIXDES-16th International Conference Mixed Design of Integrated Circuits & Systems","","2009","","","123","128","Together with development of semiconductor technology the digital circuits are more and more susceptible to Single Event Upsets that changes information stored in memory components in non-predictible way. This is particularly important for circuits operating in radioactive environment but becomes also an important factor limiting reliability for circuits operating at normal conditions. The countermeasure is a redundancy in circuit that allows to detect and correct errors caused by radiation. Unfortunately CAD software provides very limited support to simulate the consequences of SEU and automatically include required redundancy in the FPGA project. Moreover, optimization procedures remove redundant parts and special effort must be made to prevent that. The paper presents a software environment to process VHDL description of the circuit and automatically generate the redundant blocks together with voting circuits. The generated redundancy uses Triple Module Redundancy (TMR) scheme. It also supports the VHDL simulation with SEUs in order to enable identification of the most sensitive components. Since the TMR is costly, the designer can indicate which parts of the circuit should be replicated basing on the results of simulation.","","978-1-4244-4798-5978-83-928756-1","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5289567","SEU;FPGA;TMR;VHDL","Redundancy;Digital circuits;Single event upset;Circuit simulation;Radiation detector circuits;Radiation detectors;Computer errors;Error correction;Field programmable gate arrays;Voting","CAD;digital circuits;field programmable gate arrays;hardware description languages;logic testing;radiation effects;redundancy","single event upsets;digital circuits;semiconductor technology;memory components;radioactive environment;CAD software;FPGA project;software environment;VHDL description;voting circuits;triple module redundancy","","1","20","","","","","","IEEE","IEEE Conferences"
"Practical considerations for Wafer-Level Electromigration Monitoring in high volume production","O. Aubel; T. D. Sullivan; D. Massey; T. c. Lee; T. Merrill; S. Polchlopek; A. Strong","Advanced Micro Devices, 01127 Dresden, Germany, oliver.aubel@amd.com; IBM Microelectronics, Essex Junction, VT 05452, USA; IBM Microelectronics, Essex Junction, VT 05452, USA; IBM Microelectronics, Essex Junction, VT 05452, USA; IBM Microelectronics, Essex Junction, VT 05452, USA; IBM Microelectronics, Essex Junction, VT 05452, USA; IBM Microelectronics, Essex Junction, VT 05452, USA","2006 IEEE International Integrated Reliability Workshop Final Report","","2006","","","105","110","Reliability monitoring is an important part of process control in high volume production. For the back end of line (BEOL), a wafer-level electromigration (WL-EM) test is usually the method of choice to get a good indication of process variation (Schuster, 2001). In this work we present practical normalization procedures to ensure an appropriate wafer to wafer comparison which is independent of variation in cross-sectional area as well as of the initial resistance spread. The measurements have been performed on a commercially available 300mm multi-side probe station, using custom-made software to implement the current ramp and resistance measurement. The test conditions were achieved through Joule heating; the test structures used were 800mum long single lines (no vias) in metal 1 to metal 3, varying in width from 0.14mum to 10mum. After several normalization steps described in this paper we found a strong activation energy dependence on line width. This dependence was linked to issues in temperature investigation using a constant TCR value. Additionally we found a simple way to estimate the current density exponent by optimizing the Arrhenius relation. Overall a comprehensive guideline for constant current WL-EM is presented","1930-8841;2374-8036","1-4244-0296-41-4244-0297","10.1109/IRWS.2006.305221","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4098698","","Electromigration;Monitoring;Production;Testing;Electrical resistance measurement;Process control;Current measurement;Software measurement;Performance evaluation;Probes","current density;electromigration;reliability;semiconductor device manufacture","wafer-level electromigration monitoring;high volume production;initial resistance spread;multiside probe station;custom-made software;current ramp;resistance measurement;Joule heating;activation energy dependence;current density exponent;Arrhenius relation;0.14 to 10 micron;300 mm","","1","11","","","","","","IEEE","IEEE Conferences"
"Clustering and Constraints for Real-time Multicast","W. Cheng; S. Cheng; C. Wu; J. Yue; G. Ye; L. He","NA; NA; NA; NA; NA; NA","2009 IEEE International Conference on Networking, Architecture, and Storage","","2009","","","184","187","Real-time multicast is widely applied on Internet. Unfortunately, current systems share the same problem that they are not able to maintain high and stable performance when the scale grows unexpectedly large or the nodes' performance fluctuation drastically. This paper attempts to solve the problem by means of network structure clustering and constraints. We show that the multicast network can achieve much higher optimizing speed through dynamically clustering nodes with similar performance records. We propose a cluster-based multicast model and constraints on it. We further provide Regular-Balanced constraint as the recommended constraint, which is related to higher ability of scale and failure tolerance. We also test our model by contrast experiments with CoolStreaming and NICE.","","978-0-7695-3741","10.1109/NAS.2009.39","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5197318","clustering;constraint;real-time multicast","Real time systems;Design optimization;Bandwidth;Fluctuations;Extraterrestrial measurements;Computer architecture;Helium;Computer networks;Laboratories;Software engineering","Internet;media streaming;multicast communication","real-time multicast;Internet;nodes performance fluctuation;network structure clustering;regular-balanced constraint;CoolStreaming;NICE","","1","9","","","","","","IEEE","IEEE Conferences"
"The Incremental Evolution of Attack Agents in Xpilot","G. B. Parker; M. Parker","Computer Science, Connecticut College, parker@conncoll.edu; NA","2006 IEEE International Conference on Evolutionary Computation","","2006","","","969","975","In the research presented in this paper, we use incremental evolution to learn multifaceted neural network (NN) controllers for agents operating in the space game Xpilot. Behavioral components specific to the accomplishment of specific tasks, such as bullet-dodging, shooting, and closing on an enemy, are learned in the first increment. These behavioral components are used in the second increment to evolve a NN that prioritizes the output of a two-layer NN depending on that agent's current situation.","1089-778X;1941-0026","0-7803-9487","10.1109/CEC.2006.1688415","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1688415","","Neural networks;System testing;Humans;Artificial intelligence;Intelligent networks;Control systems;Autonomous agents;Genetic algorithms;Artificial neural networks;Open source software","computer games;evolutionary computation;learning (artificial intelligence);neural nets;software agents","incremental evolution;attack agents;Xpilot;multifaceted neural network;space game","","2","7","","","","","","IEEE","IEEE Conferences"
"Control System Design for Solar Energy with LHTS to Fresh Air Conditioning","L. Wang; J. Yang; X. Zhou; P. Li","NA; NA; NA; NA","2009 International Conference on Computational Intelligence, Modelling and Simulation","","2009","","","216","219","With the help of LHTS (Latent Heat Thermal Storage) tank to primary Air Conditioning, a fresh air system of solar energy storage with LHTS is presented. In this paper, heat transfer characteristics of the system and Energy-saving optimizing design method for the compounded system were firstly studied. Then the structure of hardware and software were designed for, and the field and remote monitoring control, basing on CAN bus with the help of ICAN Module and TCP/IP technology, were finally realized. It is proved that there are the following advantages for the control system: easy system operating, convenient monitoring interface for users, flexible network structure based on iCAN protocol, high reliability, convenient to test and develop, and affluent in PC driver supporting which escapes away from the instability and discontinuity of individual solar energy air conditioning system.","2166-8523;2166-8531","978-1-4244-5200-2978-0-7695-3795","10.1109/CSSim.2009.49","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5350105","Solar Energy;Air conditioning;LHTS;CAN bus;Field and Remote Monitoring Control;TCP/IP","Control systems;Solar energy;Air conditioning;Energy storage;Remote monitoring;Solar heating;Heat transfer;Design optimization;Design methodology;Hardware","air conditioning;control system synthesis;controller area networks","control system design;solar energy;LHTS;fresh air conditioning;latent heat thermal storage;heat transfer;energy-saving optimizing design method;CAN bus;TCP/IP technology;flexible network structure","","2","16","","","","","","IEEE","IEEE Conferences"
"An Intelligent Agent Using a Q-Learning Method to Allocate Replicated Data in a Distributed Database","A. R. Morffi; L. M. G. González; D. R. Paz; M. M. Hing","NA; NA; NA; NA","2007 Sixth Mexican International Conference on Artificial Intelligence, Special Session (MICAI)","","2007","","","231","240","The data distribution problem is a critical one that affects the global performance of the distributed database systems because it directly influences the efficiency of the querying process. Due to the complexity of the problem, most of the proposed solutions divide the design process in two parts: the fragmentation and the allocation of the fragments on the different locations in the network. Here we consider the allocation problem with the possibility to replicate fragments, minimizing the total cost, which is in general NP-complete, and propose a method based on Q-learning to solve the allocation of fragments in the design of a distributed database. As a result we obtain for several cases, logical allocation of fragments in a reasonable time.","","978-0-7695-3124","10.1109/MICAI.2007.8","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4659313","allocation problem;distributed databases;reinforcement learning;replication","Intelligent agent;Distributed databases;Cost function;Database systems;Artificial intelligence;Process design;Distributed information systems;Delay;Testing;Learning","computational complexity;distributed databases;learning (artificial intelligence);optimisation;query processing;software agents","intelligent agent;Q-learning method;replicated data allocation;distributed database;data distribution problem;querying process;NP-complete","","","17","","","","","","IEEE","IEEE Conferences"
"Digital RF Processor (DRP™) for wireless transmitters","R. Faust","Texas Instruments Inc., Ra¿anana, Israel","2008 IEEE International Conference on Microwaves, Communications, Antennas and Electronic Systems","","2008","","","1","5","RF transceivers have recently migrated to low-cost digital deep-submicron CMOS processes, where their integration with baseband processors and memory serve to form low cost SoC solutions for various radio standards. Such process environment, optimized for digital logic and SRAM memory, is extremely restrictive for conventional analog and RF designs. This tutorial presents recently developed techniques that transform the RF transceiver circuit design complexities into digital domain, where the strengths of the digital processes are leveraged, allowing the SoC to benefit from simple migration with process node scaling, advanced design automation, re-configurability and testability. Digital signal processing, both hardware and software based, is used extensively to alleviate analog design complexity, allowing the reduction in cost and power consumption in a reconfigurable design environment. This DRPtrade transmitter tutorial focuses on the fundamentals of the digital polar transmitter of the Bluetooth single-chip solution offered by Texas Instruments.","","978-1-4244-2097-1978-1-4244-2098","10.1109/COMCAS.2008.4562784","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4562784","","Radio frequency;Radio transmitters;Transceivers;Costs;Circuit testing;Signal design;CMOS process;Baseband;Design optimization;CMOS logic circuits","CMOS digital integrated circuits;network synthesis;radiofrequency integrated circuits;SRAM chips;system-on-chip;transceivers","digital RF processor;wireless transmitters;RF transceivers;CMOS processes;baseband processors;SoC;SRAM memory;circuit design complexity;digital signal processing;Bluetooth single-chip solution;digital polar transmitter","","","3","","","","","","IEEE","IEEE Conferences"
"Study on Location-Selection of B2C E-Commerce Logistics Distribution Center","X. Bin; W. Chunli; S. Zhanjiang; Y. Lingyun","NA; NA; NA; NA","2009 International Forum on Computer Science-Technology and Applications","","2009","2","","7","10","Aiming at the higher and higher logistics' distribution efficiency and the lower and lower overall transportation cost under e-commerce environment, this paper proposed a location-selection model of two-level logistics distribution mechanism. The first-level distribution center's location has been solved by Steiner tree problem's arithmetic based on minimum spanning tree. For second-level distribution center's location, travelling salesperson problem's improved mathematic model has been established firstly, the model has been solved by the software Lingo secondly and the optimized location has been obtained finally. In the end, some tests have been respectively made to prove the feasibility of the two model, which offers references for establishing multilevel distribution mechanism.","","978-1-4244-5423-5978-1-4244-5422-8978-0-7695-3930","10.1109/IFCSTA.2009.124","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5385018","","Logistics;Costs;Distributed computing;Mathematical model;Computer applications;Application software;Information technology;Transportation;Arithmetic;Mathematics","electronic commerce;logistics;travelling salesman problems;trees (mathematics)","B2C e-commerce logistics distribution center;location selection model;first-level distribution center location;Steiner tree problem arithmetic;minimum spanning tree;second-level distribution center location;travelling salesperson problem;mathematical model;electronic commerce;Lingo software","","","8","","","","","","IEEE","IEEE Conferences"
"CMT: An Equivalent Circuit Modeling Tool for Ultrasonic Transducer","J. Liu; T. Watanabe; N. Kijima; M. Haruta; Y. Murayama; S. Omata","NA; NA; NA; NA; NA; NA","2008 Second International Conference on Sensor Technologies and Applications (sensorcomm 2008)","","2008","","","592","597","The equivalent circuit modeling of sensor can simplify the sensing problem as a pure electronics problem. As one kind of widely used sensor, there are also numerous equivalent circuit models for the ultrasonic transducer. However, the parameters of equivalent circuit of transducer are inconvenient for a common engineer to calculate, since so many variables, constants and formulae are involved in. Motivated by this, a toolkit named CMT is proposed. When given the details about the transducer in use and the requirement about the equivalent circuit, this tool will generate the SPICE file for the circuit. Such a SPICE file can be subsequently used in a circuit simulating software, for test of transducer properties and for the development of sensing system. For a better use, some optimizations are adopted, to minimize the working load of user in data management, and to speedup the calculation of the circuit parameters.","","978-0-7695-3330","10.1109/SENSORCOMM.2008.129","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4622725","equivalent circuit;PZT;transducer;SPICE;EDA","Equivalent circuits;Integrated circuit modeling;SPICE;Transducers;Materials;Software;Object oriented modeling","equivalent circuits;SPICE;ultrasonic transducers","equivalent circuit modeling tool;ultrasonic transducer;SPICE file;circuit simulating software;transducer properties;data management","","","14","","","","","","IEEE","IEEE Conferences"
"Study of an integrated resource management system oriented to ferry companies","X. Liu; Z. Li; X. Huang; Q. Song","Dalian University of Technology, China; Dalian University of Technology, China; Dalian University of Technology, China; Dalianwan New Harbor Service Company, China","2008 3rd International Conference on Intelligent System and Knowledge Engineering","","2008","1","","619","624","To incorporate existing and new information tools, in particular optimization methods and software, an integrated resource management system was proposed for ferry companies. The system adopts a hierarchical system architecture which incorporates all the modules in the three layers: the supporting layer, the application layer, and the management and control layer. The blank ticket rolls were considered as critical resources, and a conceptual storage for blank ticket rolls was developed to complete a ticket lifecycle management approach. To balance the capacities of two or more sailings and enhance profits, a stowage optimization procedure oriented to Ro-Ro shipping was used. The system also provides cubic views for statistics as decision support methods. The system was tested as feasible and effective in two case studies conducted in Dalian, China.","","978-1-4244-2196-1978-1-4244-2197","10.1109/ISKE.2008.4731005","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4731005","","Resource management;Intelligent systems;Knowledge engineering;Optimization methods;Hierarchical systems;Rail transportation;Uncertainty;Navigation;Software tools;Computer architecture","decision support systems;hierarchical systems;statistics;travel industry","integrated resource management system;ferry companies;hierarchical system architecture;blank ticket rolls;ticket lifecycle management;stowage optimization procedure;Ro-Ro shipping;statistics;decision support methods;Dalian;China","","","9","","","","","","IEEE","IEEE Conferences"
"Performance Evaluation of Link Quality Extension in Multihop Wireless Mobile Ad-hoc Networks","M. Ikeda; L. Barolli; M. Hiyama; G. D. Marco; T. Yang; A. Durresi","NA; NA; NA; NA; NA; NA","2009 International Conference on Complex, Intelligent and Software Intensive Systems","","2009","","","311","318","Recently, mobile ad-hoc networks (MANET) are continuing to attract the attention for their potential use in several fields. Most of the work has been done in simulation, because a simulator can give a quick and inexpensive understanding of protocols and algorithms. However, experimentation in the real world are very important to verify the simulation results and to revise the models implemented in the simulator. In this paper, we present the implementation and analysis of our testbed considering the link quality window size (LQWS) parameter for optimized link state routing (OLSR) protocol. We investigate the effect of mobility in the throughput of a MANET. The mobile nodes move toward the destination at a regular speed. When the mobile nodes arrive at the corner, they stop for about three seconds. In our experiments, we consider two cases: only one node is moving (mobile node)and two nodes (intermediate nodes) are moving at the same time. We assess the performance of our testbed in terms of throughput, round trip time, jitter and packet loss. From our experiments, we found that throughput of TCP was improved by reducing LQWS.","","978-1-4244-3569-2978-0-7695-3575","10.1109/CISIS.2009.101","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5066803","MANET;Link Quality Window Size;Testbed;OLSR","Spread spectrum communication;Ad hoc networks;Mobile ad hoc networks;Throughput;Routing protocols;System testing;Mobile computing;Intelligent networks;Computer networks;Performance loss","ad hoc networks;mobile radio;routing protocols;telecommunication computing;transport protocols","link quality extension;multihop wireless mobile ad-hoc networks;link quality window size parameter;optimized link state routing protocol;mobile nodes;round trip time;packet loss;jitter;TCP","","3","16","","","","","","IEEE","IEEE Conferences"
"The application of mixed genetic algorithm in parameter identification of circulating fluidized bed units","P. Han; Z. Wang; Y. Huang","School of Control Science and Engineering, North China Electric Power University, China, 071003 CN; School of Control Science and Engineering, North China Electric Power University, China, 071003 CN; School of Control Science and Engineering, North China Electric Power University, China, 071003 CN","2009 International Conference on Sustainable Power Generation and Supply","","2009","","","1","5","The maximum likelihood algorithm has some requirements regarding the initial values, or else there would be no guarantee for the convergence of the parameters. The Genetic Algorithm (GA) was introduced in this paper to cope with the problem of local convergence in maximum likelihood algorithm. The typical function test showed that the method had manifested both the global convergence as in GA and the high accuracy rate as in maximum likelihood algorithm. Besides, the special software for general model identification was designed for identification in typical thermal systems. The results showed that it was a readable and valuable method in the realm of identification.","2156-9681;2156-969X","978-1-4244-4934","10.1109/SUPERGEN.2009.5348010","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5348010","Thermal process;System identification;Mixed Genetic Algorithm;maximum likelihood algorithm","Genetic algorithms;Parameter estimation;Fluidization;Convergence;Binary codes;Maximum likelihood decoding;Optimization methods;Testing;System identification;Proposals","fluidised beds;genetic algorithms;maximum likelihood estimation;process heating","mixed genetic algorithm;parameter identification;fluidized bed unit;maximum likelihood algorithm;thermal system","","","7","","","","","","IEEE","IEEE Conferences"
"Unequally spaced antenna arrays synthesized via dynamic programming","J. Hooker; R. K. Arora","FAMU-FSU College of Engineering, Department of Electrical and Computer Engineering, 2525 Pottsdamer Street, Tallahassee, Florida 32310 USA; FAMU-FSU College of Engineering, Department of Electrical and Computer Engineering, 2525 Pottsdamer Street, Tallahassee, Florida 32310 USA","2007 IEEE Antennas and Propagation Society International Symposium","","2007","","","5977","5980","In this paper, application of dynamic programming to linear arrays consisting of unequally spaced elements is investigated. Variants of the algorithms proposed previously are created and tested for optimality. The method of total enumeration is utilized to verify how close to optimal the obtained results are. The criterion for comparison used is the peak sidelobe level in a given region of optimization. Where total enumeration is not feasible, a comparison is made with an alternative method of optimization that uses simulated annealing.","1522-3965;1947-1491","978-1-4244-0877-1978-1-4244-0878","10.1109/APS.2007.4396914","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4396914","","Antenna arrays;Dynamic programming;Linear antenna arrays;Apertures;Heuristic algorithms;Testing;Simulated annealing;Educational institutions;Electronic mail;Application software","dynamic programming;linear antenna arrays;simulated annealing","linear antenna arrays;unequally spaced elements;dynamic programming;simulated annealing;peak sidelobe level","","1","4","","","","","","IEEE","IEEE Conferences"
"Development of a Generation Resource Scheduling Case Library","Yuan Liao","NA","2006 Proceeding of the Thirty-Eighth Southeastern Symposium on System Theory","","2006","","","320","324","Large scale generation resource scheduling optimization problem is usually very hard to solve due to its combinatorial nature. Various algorithms such as Lagrangian relaxation based algorithm, Benders algorithm, and genetic algorithms have been proposed in the past to tackle this problem. One challenge facing researchers is how to determine which algorithm to use and how to test and compare the performance of various algorithms. To explore and propose new algorithms, benchmarking of the performance of existing algorithms is essential, since advantages and disadvantages of each algorithm can be better understood through extensive case studies. To carry out such studies, a systematic way based on a comprehensive case library is necessary. This paper describes an approach for building a case library that can be used for testing various resource scheduling algorithms. The implementation details including the development environment and special considerations are presented","0094-2898;2161-8135","0-7803-9457","10.1109/SSST.2006.1619096","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1619096","","Computer aided software engineering;Libraries;Job shop scheduling;Mathematical model;Cost function;Benchmark testing;Large-scale systems;Lagrangian functions;Genetic algorithms;Scheduling algorithm","genetic algorithms;large-scale systems;mathematics computing;power generation scheduling;power system analysis computing;special libraries","generation resource scheduling case library;Lagrangian relaxation algorithm;Benders algorithm;genetic algorithms;unit commitment cases;Matlab","","","5","","","","","","IEEE","IEEE Conferences"
"An H.264 video decoder based on a latest generation DSP","F. Pescador; G. Maturana; M. J. Garrido; E. Juarez; C. Sanz","Electronic and Microelectronic Design Group (GDEM) at the Universidad Politecnica de Madrid, Spain; Electronic and Microelectronic Design Group (GDEM) at the Universidad Politecnica de Madrid, Spain; Electronic and Microelectronic Design Group (GDEM) at the Universidad Politecnica de Madrid, Spain; Electronic and Microelectronic Design Group (GDEM) at the Universidad Politecnica de Madrid, Spain; Electronic and Microelectronic Design Group (GDEM) at the Universidad Politecnica de Madrid, Spain","IEEE Transactions on Consumer Electronics","","2009","55","1","205","212","Latest generation DSPs are becoming more efficient, being able to improve their forerunners while reducing their internal memory size to lower the cost. In this paper, an H.264 video decoder based on a latest generation DSP is described. Both the EDMA and the memory architecture of the processor have been fully exploited to increase the execution speed. Profiling tests have been carried out by using digital TV streams and DVD transcoded sequences. The speed of the new DSP running the decoder is 16% better than that of a forerunner with 20% more internal memory running the same decoder.","0098-3063;1558-4127","","10.1109/TCE.2009.4814436","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4814436","H264;Video Decoder;Software Optimization;Latest Generation DSP","Decoding;Digital signal processing;Costs;ISO standards;Video coding;IEC standards;Memory architecture;Testing;Streaming media;Digital TV","digital signal processing chips;digital television;digital versatile discs;video codecs","H.264 video decoder;digital signal processor;EDMA;memory architecture;execution speed;digital TV streams;DVD transcoded sequences","","20","11","","","","","","IEEE","IEEE Journals & Magazines"
"An Add-On for Managing Behaviours with Priority in JADE","J. A. Suarez-romero; A. Alonso-Betanzos; B. Guijarro-Berdinas","University of A Coruna, Spain; University of A Coruna, Spain; University of A Coruna, Spain","2006 IEEE/WIC/ACM International Conference on Intelligent Agent Technology","","2006","","","713","716","In this article, two new implementations for behaviours in JADE are presented. These new behaviours, while being able to reproduce the functioning of the old JADE's behaviours, allow the user to define priorities. This fact is of vital importance for several multiagent applications. Finally, a test was developed to show that the performance of the new behaviours is similar to the original ones.","","0-7695-2748","10.1109/IAT.2006.30","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4053000","","Java;Processor scheduling;Programming profession;Computer science;Sequential analysis;Yarn;Concurrent computing;Round robin;Intelligent agent;Dynamic scheduling","Java;multi-agent systems;program diagnostics;software agents;software engineering","Java agent development framework;JADE behaviour;multiagent application;add-on application","","","3","","","","","","IEEE","IEEE Conferences"
"Microstrip activities in SAMEER Kolkata Centre","S. Chakrabarti","SAMEER Kolkata Centre, Plot-L2, Block-GP, Sector-V, Salt Lake Electronics Complex, 700091, India","2007 IEEE Applied Electromagnetics Conference (AEMC)","","2007","","","1","4","This paper describes different microstrip antenna and circuit development activities over the last decade in SAMEER Kolkata Centre. Brief description of different microstrip antennas and circuits has been outlined. Possible design details have been narrated and important salient features are mentioned. Design optimizations have been carried out using method of moment (MoM) based commercial simulation software IE3D. Antennas and circuits have been fabricated and tested. Close agreement between theoretical results and test results have been observed. Some of the test results are displayed.","","978-1-4244-1863-3978-1-4244-1864","10.1109/AEMC.2007.4638063","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4638063","Microstrip antennas;microstrip circuits","","method of moments;microstrip antennas;microstrip circuits","SAMEER Kolkata Centre;microstrip antenna development;microstrip circuit development;design optimizations;method of moment;IE3D software","","","","","","","","","IEEE","IEEE Conferences"
"Intelligent compilers","J. Cavazos","Computer and Information Sciences Department, University of Delaware 103 Smith Hall, Newark, DE, USA","2008 IEEE International Conference on Cluster Computing","","2008","","","360","368","The industry is now in agreement that the future of architecture design lies in multiple cores. As a consequence, all computer systems today, from embedded devices to petascale computing systems, are being developed using multicore processors. Although researchers in industry and academia are exploring many different multicore hardware design choices, most agree that developing portable software that achieves high performance on multicore processors is a major unsolved problem. We now see a plethora of architectural features, with little consensus on how the computation, memory, and communication structures in multicore systems will be organized. The wide disparity in hardware systems available has made it nearly impossible to write code that is portable in functionality while still taking advantage of the performance potential of each system. In this paper, we propose exploring the viability of developing intelligent compilers, focusing on key components that will allow application portability while still achieving high performance.","1552-5244;2168-9253","978-1-4244-2639-3978-1-4244-2640","10.1109/CLUSTR.2008.4663796","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4663796","","Optimization;Predictive models;Program processors;Radiation detectors;Training;Computer architecture;Benchmark testing","microprocessor chips;program compilers;software portability","intelligent compilers;computer systems;petascale computing systems;multicore processors;multicore hardware design choices","","4","56","","","","","","IEEE","IEEE Conferences"
"Dynamic Simulation Analysis of the Base of Vertical Motor","W. Mao; J. Li; C. Fu","NA; NA; NA","2009 International Conference on Energy and Environment Technology","","2009","2","","135","138","In this paper, the models of the base of a type vertical motor were adopted to MSC. NASTRAN software to analysis modes and transient response, get transient response, natural frequencies and vibration shape based on element finite simulation, and the mechanics capability was proved, these main destroy forms and locations were gained. The effect of the vibration of the motor was also discussed according to these element finite simulation pictures and data. The results of element finite simulation of the motor related models showed that the structures of the motor base could meet the design requirement. Contrasted analyzes of vibration influence of different foot bolt to restrain the base were carried on. Dynamic Performances of the motor base were gained, which has the guiding sense in the vibrating test and optimizing design for the base of vertical motor.","","978-0-7695-3819","10.1109/ICEET.2009.270","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5365536","base of verticalmotor;finite elemen simulation;transientkintics;natural modet","Analytical models;Vibrations;Transient response;Transient analysis;Frequency;Shape;Foot;Fasteners;Performance evaluation;Testing","electric motors;finite element analysis","dynamic simulation analysis;vertical motor;MSC;NASTRAN software;transient response;element finite simulation;vibration effect","","1","7","","","","","","IEEE","IEEE Conferences"
"Evaluating and optimising accelerometer-based gesture recognition techniques for mobile devices","G. Niezen; G. P. Hancke","Department of Electrical, Electronic and Computer Engineering, University of Pretoria, Lynnwood Road, 0002, South Africa; Department of Electrical, Electronic and Computer Engineering, University of Pretoria, Lynnwood Road, 0002, South Africa","AFRICON 2009","","2009","","","1","6","The objective of this study was to evaluate the various gesture recognition algorithms currently in use, after which the most suitable algorithm was optimized in order to implement it on a mobile device. Gesture recognition techniques studied include hidden Markov models, artificial neural networks and dynamic time warping. A dataset for evaluating the gesture recognition algorithms was gathered using a mobile device's embedded accelerometer. The algorithms were evaluated based on computational efficiency, recognition accuracy and storage efficiency. The optimized algorithm was implemented on the mobile device to test the empirical validity of the study.","2153-0025;2153-0033","978-1-4244-3918-8978-1-4244-3919","10.1109/AFRCON.2009.5308175","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5308175","","Accelerometers;Hidden Markov models;Magnetic sensors;Image sensors;Cameras;Artificial neural networks;Wearable sensors;Mobile computing;Africa;Application software","accelerometers;gesture recognition;hidden Markov models;mobile computing;neural nets","accelerometer-based gesture recognition;mobile devices;hidden Markov model;artificial neural network;dynamic time warping;recognition accuracy;storage efficiency","","7","17","","","","","","IEEE","IEEE Conferences"
"Effective Web Service Composition in Diverse and Large-Scale Service Networks","S. Oh; D. Lee; S. R. T. Kumara","General Motors Research and Development Center, Warren; Pennsylvania State Univerity, University Park; Pennsylvania State Univerity, University Park","IEEE Transactions on Services Computing","","2008","1","1","15","32","The main research focus of Web services is to achieve the interoperability between distributed and heterogeneous applications. Therefore, flexible composition of Web services to fulfill the given challenging requirements is one of the most important objectives in this research field. However, until now, service composition has been largely an error-prone and tedious process. Furthermore, as the number of available web services increases, finding the right Web services to satisfy the given goal becomes intractable. In this paper, toward these issues, we propose an AI planning-based framework that enables the automatic composition of Web services, and explore the following issues. First, we formulate the Web-service composition problem in terms of AI planning and network optimization problems to investigate its complexity in detail. Second, we analyze publicly available Web service sets using network analysis techniques. Third, we develop a novel Web-service benchmark tool called WSBen. Fourth, we develop a novel AI planning-based heuristic Web-service composition algorithm named WSPR. Finally, we conduct extensive experiments to verify WSPR against state-of-the-art AI planners. It is our hope that both WSPR and WSBen will provide useful insights for researchers to develop Web-service discovery and composition algorithms, and software.","1939-1374;2372-0204","","10.1109/TSC.2008.1","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4629388","Information networks;Web-based services;Information networks;Web-based services","Web services;Planning;Artificial intelligence;Strips;Software;Benchmark testing;Equations","semantic Web;Web services","Web service composition;service networks;silver bullet;service oriented architecture;Al planning-based framework;complex network","","98","34","","","","","","IEEE","IEEE Journals & Magazines"
"An Improved Branch and Bound Algorithm for Location-Routing Problems","J. Li; Z. Yunlong; S. Hai","NA; NA; NA","2009 International Forum on Computer Science-Technology and Applications","","2009","1","","58","63","An important limitation of the fixed charge location model, is the assumption that full truckload quantities are shipped from a facility to a customer. In many contexts, shipments are made in less-than-truckload (LTL) quantities from a facility to customers along a multiple-stop route. The different location decision is made from the different cost of delivery. In this paper we are concerned with a particular type of facility location problem in which there exist two echelons of facilities. To make the location decision, the delivery cost of a multiple-stop route are calculated. We note that this is an NP-hard problem and use a Lagrangian relaxation-based branch and bound algorithm to solve this location-routing problem. We present numerical results for various size test problems. Results indicates the method is efficient.","","978-1-4244-5423-5978-1-4244-5422-8978-0-7695-3930","10.1109/IFCSTA.2009.21","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5385132","","Costs;Vehicles;Lagrangian functions;Mathematical model;NP-hard problem;Routing;Constraint optimization;Computer applications;Computer networks;Application software","facility location;goods distribution;optimisation;tree searching","branch and bound algorithm;location-routing problems;fixed charge location model;shipments;less-than-truckload quantities;NP-hard problem;Lagrangian relaxation;facility location problem","","4","15","","","","","","IEEE","IEEE Conferences"
"One-pass multi-layer rate-distortion optimization for quality scalable video coding","X. Li; P. Amon; A. Hutter; A. Kaup","Multimedia Communications and Signal Processing, University of Erlangen-Nuremberg, Germany; Siemens Corporate Technology, Information & Communications, Munich, Germany; Siemens Corporate Technology, Information & Communications, Munich, Germany; Multimedia Communications and Signal Processing, University of Erlangen-Nuremberg, Germany","2009 IEEE International Conference on Acoustics, Speech and Signal Processing","","2009","","","637","640","In this paper, a one-pass multi-layer rate-distortion optimization algorithm is proposed for quality scalable video coding. To improve the overall coding efficiency, the MB mode in the base layer is selected not only based on its rate-distortion performance relative to this layer but also according to its impact on the enhancement layer. Moreover, the optimization module for residues is also improved to benefit inter-layer prediction. Simulations show that the proposed algorithm outperforms the most recent SVC reference software. For eight test sequences, a gain of 0.35 dB on average and 0.75 dB at maximum is achieved at a cost of less than 8% increase of the total coding time.","1520-6149;2379-190X","978-1-4244-2353-8978-1-4244-2354","10.1109/ICASSP.2009.4959664","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4959664","H.264/AVC;SVC;Quality Scalable Video Coding;Multi-Layer RDO","Rate-distortion;Video coding;Static VAr compensators;Scalability;Signal processing algorithms;Gain;Costs;Automatic voltage control;Computational modeling;Computational complexity","rate distortion theory;video coding","one-pass multilayer rate-distortion optimization;quality scalable video coding;coding efficiency","","4","14","","","","","","IEEE","IEEE Conferences"
"Application specific performance indicators for quantitative evaluation of the timing behavior for embedded real-time systems","F. Konig; D. Boers; F. Slomka; U. Margull; M. Niemetz; G. Wirrer","Carl von Ossietzky University Oldenburg, Germany; Carl von Ossietzky University Oldenburg, Germany; Department of Embedded Systems/Real-Time Systems, Faculty of Engineering Science and Computer Sciences, Ulm University, 89069, Germany; 1 mal 1 Software GmbH, Maxstraße 31, D-90762 Fürth, Germany; Continental Automotive GmbH, P.O. Box 100943, D-93009 Regensburg, Germany; Continental Automotive GmbH, P.O. Box 100943, D-93009 Regensburg, Germany","2009 Design, Automation & Test in Europe Conference & Exhibition","","2009","","","519","523","In the design and development of embedded real-time systems the aspect of timing behavior plays a central role. Especially, the evaluation of different scheduling approaches, algorithms and configurations is one of the elementary preconditions for creating not only reliable but also efficient systems - a key for success in industrial mass production. This is becoming even more important as multi-core systems are more and more penetrating the world of embedded systems together with the large (and growing) variety of scheduling policies available for such systems. In this work simple mathematical concepts are used to define performance indicators allowing to quantify the benefit of different solutions of the scheduling challenge for a given application. As a sample application some aspects of analyzing the dynamic behavior of an combustion engine management system for the automotive domain are shown. However, the described approach is flexible in order to support the specific optimization needs arising from the timing requirements defined by the application domain and can be used with simulation data as well as target system measurements.","1530-1591;1558-1101","978-1-4244-3781-8978-3-9810-8015","10.1109/DATE.2009.5090719","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5090719","","Timing;Real time systems;Job shop scheduling;Scheduling algorithm;Mass production;Embedded system;Vehicle dynamics;Combustion;Engines;Automotive engineering","application specific integrated circuits;automotive electronics;embedded systems;timing","application specific performance indicator;embedded real time system timing behavior;scheduling challenge;combustion engine management system;automotive domain","","5","6","","","","","","IEEE","IEEE Conferences"
"State Machine Workflow Modeling and Dataflow Analysis","J. Lu; Y. Yu; G. Li","NA; NA; NA","2008 IEEE Symposium on Advanced Management of Information for Globalized Enterprises (AMIGE)","","2008","","","1","5","Cross-organizational workflow appears to be more and more important in global business. It is a better solution to the problem of cross-organizational collaboration that state machine workflow is able to model business processes based on discrete points. Data will be transferred among different organizations in their collaborations and more human interactivities are needed. Further more, the event driven approach makes it more flexible to state transitions. Therefore, an approach for state machine workflow modeling based on event driven is presented. Using data-dependent methods, it can analyze and optimize the data exceptions to improve process efficiency and data accuracy before compiling workflow processes.","","978-1-4244-2972-1978-1-4244-3694","10.1109/AMIGE.2008.ECP.32","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721474","","Data analysis;Business;Automata;Collaborative work;Humans;Grid computing;Computer science;Sun;Optimization methods;System testing","business data processing;finite state machines;groupware;workflow management software","state machine workflow modeling;dataflow analysis;cross-organizational workflow;global business;cross-organizational collaboration;state machine workflow;business processes;human interactivities;data-dependent methods","","","9","","","","","","IEEE","IEEE Conferences"
"Remote and mobile control in domotics","F. Sandu; M. Romanca; A. Nedelcu; P. Borza; R. Dimova","Transilvania University / Department of Electronics and Computers, Brasov, Romania; Transilvania University / Department of Electronics and Computers, Brasov, Romania; Transilvania University / Department of Electronics and Computers, Brasov, Romania; Transilvania University / Department of Electronics and Computers, Brasov, Romania; Technical University of Varna / Department of Communication Engineering, Bulgaria","2008 11th International Conference on Optimization of Electrical and Electronic Equipment","","2008","","","225","228","The paper presents a system for remote control of distributed and networked drives and data acquisition devices. The system goal is to be integrated in monitoring systems for Intelligent Buildings. The authors developed and tested a wireless acquisition &amp; distribution architecture (IEEE 802.11 ""WiFi""-compliant) based on embedded webservers (with Ethernet-serial-Ethernet tunneling) at the drives' side and PDA/Communicators (Pocket PC Smartphones running MS Windows Mobile CE6) at the remote and mobile control side.","","978-1-4244-1544-1978-1-4244-1545","10.1109/OPTIM.2008.4602527","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4602527","Intelligent Buildings;Remote control systems;Mobile control","Servers;Mobile communication;Temperature measurement;Buildings;Ethernet networks;Personal digital assistants;Software","building management systems;data acquisition;home automation;mobile computing;monitoring;telecontrol;wireless LAN","remote control;mobile control;domotics;distributed drives;networked drives;data acquisition devices;monitoring systems;intelligent buildings;wireless acquisition architecture;wireless distribution architecture;IEEE 802.11;WiFi-compliant architecture;embedded Webservers;Ethernet-serial-Ethernet tunneling;PDA;Pocket PC;Smartphones","","3","14","","","","","","IEEE","IEEE Conferences"
"Digital Analysis of the Breakdown Process in High-Voltage Circuit Breakers","Robin-Jouan; Dufournet; Montillet","NA; NA; NA","2005/2006 IEEE/PES Transmission and Distribution Conference and Exhibition","","2006","","","986","991","More and more computational fluid dynamics tools or CFD tools are used to analyze and study gas flows inside high-voltage circuit breaker chambers and to reduce testing. These tools are built upon the classical physical fluid dynamics equations by integrating very complex models. The numbers of variants of SF<sub>6</sub> interrupting chambers are now quite numerous based on their specific applications. These tools permit to estimate, analyze, and understand better the interrupting process. These tools also increase our knowledge about the gas flow behavior inside the chambers. This leads to the optimization of the shapes and geometric parameters of the circuit breaker interrupters","2160-8555;2160-8563","0-7803-9194","10.1109/TDC.2006.1668636","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1668636","","Electric breakdown;Circuit breakers;Computational fluid dynamics;Fluid flow;Circuit testing;Shape;Software tools;Plasma temperature;Interrupters;Computer interfaces","circuit breakers;computational fluid dynamics;power engineering computing;SF<sub>6</sub> insulation","breakdown process;high-voltage circuit breakers;computational fluid dynamics tools;CFD tools;physical fluid dynamics equations;SF<sub>6</sub> interrupting chambers;gas flow;circuit breaker interrupters","","3","9","","","","","","IEEE","IEEE Conferences"
"Study of LR-Loading Technique for Low-Power Single Flux Quantum Circuits","Y. Yamanashi; T. Nishigai; N. Yoshikawa","NA; NA; NA","IEEE Transactions on Applied Superconductivity","","2007","17","2","150","153","A single-flux-quantum (SFQ) circuit is thought to be very suitable as a peripheral circuit for superconducting quantum bits (qubits), which can manipulate and detect the qubit state at a temperature state similar to qubits. Even though the power consumption of SFQ circuits is extremely small, it is still sufficient to heat the substrate at a temperature below 1 K. We have investigated and demonstrated low-power SFQ circuits for this application, using the LR-loading technique, which can reduce the static power consumption of the SFQ circuits. Simulation results show that the ratio of the switching speed to the time constant of the bias circuit is important for the stable operation of low-power SFQ circuits. The static power consumption of SFQ circuits can be reduced to the same order as the dynamic power consumption through optimization of the circuit parameters. We have designed and tested a low-power SFQ clock generator using the LR-loading technique and confirmed its stable operation at 4.2 K, where the power consumption is reduced by 93% compared with ordinary biased circuits.","1051-8223;1558-2515;2378-7074","","10.1109/TASC.2007.898608","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4277820","Josephson logic;quantum bit;SFQ circuit;superconducting devices;superconducting integrated circuits","Energy consumption;Quantum computing;Temperature;Circuit testing;Superconducting logic circuits;Superconducting devices;Superconducting integrated circuits;Application software;Superconducting device noise;Josephson junctions","superconducting devices","LR-loading technique;single flux quantum circuits;peripheral circuit;superconducting quantum bits;qubit state;bias circuit;SFQ clock generator;temperature 4.2 K","","31","11","","","","","","IEEE","IEEE Journals & Magazines"
"Motivaton of students for futher education in simulation by an applied example in a related other course in engineering education — A case study","M. Klug; P. Hausberger","Department for Advanced Technologies, Hoechstaedtplatz 5, University of Applied Sciences Technikum Vienna, Vienna, 1200, Austria; Department for Advanced Technologies, Hoechstaedtplatz 5, University of Applied Sciences Technikum Vienna, Vienna, 1200, Austria","Proceedings of the 2009 Winter Simulation Conference (WSC)","","2009","","","248","255","The present paper explains, how a simulation-based study course raised significant interest in discrete-event simulation, finally undergoing a simulation focused course. The simulation based course during the course ¿Production structures and design¿ used a model for own studies, acting as a test-bed for optimal setup-time optimization. An easy access to simulation technology was provided by an Excel-based front end, where any figures related to the model were available for student's analysis. Nevertheless the students started to act on their own with the Enterprise Dynamics based simulation model, discovering the benefits of simulation. High-quality elaborations of the students were finally achieved, going far beyond of just answering the related questions. In addition, due to the part-time study program, interest in the company's approach on production planning raised. Subsequently the demand for further dedicated courses in computer simulation raised and were finally addressed.","0891-7736;1558-4305","978-1-4244-5771-7978-1-4244-5770-0978-1-4244-5772","10.1109/WSC.2009.5429330","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5429330","","Engineering education;Packaging;Discrete event simulation;Testing;Analytical models;Computational modeling;Production planning;Electronic learning;Computer aided software engineering;Educational technology","discrete event simulation;educational computing;educational courses;engineering education;human factors","student motivaton;engineering education;discrete-event simulation;production structures and design;Excel-based front end","","","21","","","","","","IEEE","IEEE Conferences"
"Small-file access in parallel file systems","P. Carns; S. Lang; R. Ross; M. Vilayannur; J. Kunkel; T. Ludwig","Mathematics and Computer Science Division, Argonne National Laboratory, IL 60439, USA; Mathematics and Computer Science Division, Argonne National Laboratory, IL 60439, USA; Mathematics and Computer Science Division, Argonne National Laboratory, IL 60439, USA; VMware Inc., 3401 Hillview Ave., Palo Alto, CA 94304, USA; Institute of Computer Science, University of Heidelberg, Germany; Institute of Computer Science, University of Heidelberg, Germany","2009 IEEE International Symposium on Parallel & Distributed Processing","","2009","","","1","11","Today's computational science demands have resulted in ever larger parallel computers, and storage systems have grown to match these demands. Parallel file systems used in this environment are increasingly specialized to extract the highest possible performance for large I/O operations, at the expense of other potential workloads. While some applications have adapted to I/O best practices and can obtain good performance on these systems, the natural I/O patterns of many applications result in generation of many small files. These applications are not well served by current parallel file systems at very large scale. This paper describes five techniques for optimizing small-file access in parallel file systems for very large scale systems. These five techniques are all implemented in a single parallel file system (PVFS) and then systematically assessed on two test platforms. A microbenchmark and the mdtest benchmark are used to evaluate the optimizations at an unprecedented scale. We observe as much as a 905% improvement in small-file create rates, 1,106% improvement in small-file stat rates, and 727% improvement in small-file removal rates, compared to a baseline PVFS configuration on a leadership computing platform using 16,384 cores.","1530-2075","978-1-4244-3751-1978-1-4244-3750","10.1109/IPDPS.2009.5161029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5161029","","File systems;Concurrent computing;Application software;Computer science;Laboratories;Large-scale systems;System testing;Mathematics;Best practices;Benchmark testing","file organisation;parallel processing","small-file access;parallel file system;large I/O operation;very large scale system;small-file create rate;small-file stat rate;small-file removal rate","","48","33","","","","","","IEEE","IEEE Conferences"
"OpenCL embedded profile prototype in mobile device","J. Leskela; J. Nikula; M. Salmela","Devices R&D, Renewal Projects, Nokia Corporation, Finland; Devices R&D, Renewal Projects, Nokia Corporation, Finland; Devices R&D, Renewal Projects, Nokia Corporation, Finland","2009 IEEE Workshop on Signal Processing Systems","","2009","","","279","284","Programmable graphics processing unit (GPU) has over the years become an integral part of today's computing systems. The GPU use-cases have gradually been extended from graphics towards a wide range of applications. Since the programmable GPU is now making its way to mobile devices, it is interesting to study these new use-cases also there. To test this, we created a programming environment based on the embedded profile of the fresh Khronos OpenCL standard and ran it against an image processing workload in a mobile device with CPU and GPU back-ends. The early results on performance and energy consumption with CPU + GPU configuration were promising but also suggest there is room for optimization.","2162-3562;2162-3570","978-1-4244-4335","10.1109/SIPS.2009.5336267","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5336267","OpenCL;Image processing;Parallel processing;Embedded systems;Computer graphics hardware","Prototypes;Kernel;Central Processing Unit;Mobile computing;Image processing;Hardware;Computer graphics;Application software;Computer vision;Programming environments","computer graphics;coprocessors;energy consumption;image processing;optimisation;programming environments","mobile device;Khronos OpenCL standard;embedded profile prototype;programmable graphics processing unit;computing systems;image processing workload;energy consumption;CPU + GPU configuration;optimization","","19","15","","","","","","IEEE","IEEE Conferences"
"Addressing Low Base Rates in Intrusion Detection via Uncertainty-Bounding Multi-Step Analysis","R. J. Cole; P. Liu","NA; NA","2008 Annual Computer Security Applications Conference (ACSAC)","","2008","","","269","278","Existing approaches to characterizing intrusion detection systems focus on performance under test conditions. While it is well-understood that operational conditions may differ from test conditions, little attention has been paid to the question of assessing the effect on IDS results of parameter estimation errors resulting from these differences. In this paper we consider this question in the context of multi-step attacks. We derive simulated distributions of the posterior probability of exploit given the observation of a series of alerts and bounds on the posterior uncertainty given a particular distribution of the model parameters. Knowledge of such bounds introduces the novel prospect of a confidence versus agility tradeoff in IDS administration. Such a tradeoff could give administrators flexibility in IDS configuration, allowing them to choose detection confidence at the price of detection latency, according to organizational priorities.","1063-9527","978-0-7695-3447","10.1109/ACSAC.2008.14","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721564","Intrusion detection;Bayesian network;Probabilistic inference","Intrusion detection;Bayesian methods;Computer security;Expert systems;Uncertainty;Phase detection;Application software;Information analysis;Performance analysis;System testing","inference mechanisms;parameter estimation;probability;security of data","intrusion detection system;low base rate address;uncertainty-bounding multistep analysis;test condition;parameter estimation error;posterior probability distribution;inference uncertainty","","","11","","","","","","IEEE","IEEE Conferences"
"An Improved Ant Colony Algorithm for the Logistics Vehicle Scheduling Problem","Q. Zhang; Q. Zhang","NA; NA","2008 Second International Symposium on Intelligent Information Technology Application","","2008","2","","55","59","The logistics vehicle scheduling problem is a widely existent problem in distribution. In fact, it is the vehicle routing problem with time window. In the vehicle routing problem with time windows (VRPTW), there are two main objectives. The primary objective is to reduce the number of vehicles, the secondary one is to minimize the total distance travelled by all vehicles. This is an NP-complete optimization problem. Ant colony system which is a novel simulated evolutionary algorithm, it can good for NP-hard problem. According to the features of the Vehicle routing problem with time windows and the ant colony algorithm, an improved ant colony system is proposed to solve this problem. It possesses a new state transition rule, a new pheromone updating rule and diverse local search approaches. Finally, Solomon's benchmark instances (VRPTW 100-customer) are tested for the algorithm and shows that the improve ant colony is able to find solutions for VRPTW.","","978-0-7695-3497","10.1109/IITA.2008.520","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4739726","Improved;Ant Colony Algorithm;Logistics;Vehicle Scheduling Problem","Scheduling algorithm;Logistics;Routing;Processor scheduling;Intelligent vehicles;Information technology;Application software;Computer science;Educational institutions;Information science","computational complexity;logistics;optimisation;scheduling;transportation","ant colony algorithm;logistics vehicle scheduling problem;vehicle scheduling problem;vehicle routing problem with time windows;NP-complete optimization problem;NP-hard problem;Solomon benchmark instances","","1","15","","","","","","IEEE","IEEE Conferences"
"The Effect of Organization Process Focus and Organizational Learning on Project Performance: An Examination of Taiwan's Companies","C. Wu; S. Wang; K. Fang","National Yunlin University of Science & Technology, Department of Information Management, Yunlin, Taiwan, R.O.C.; National Formosa University, Department of Information Management, Yunlin, Taiwan, R.O.C.; National Formosa University, Department of Information Management, Yunlin, Taiwan, R.O.C.; National Yunlin University of Science & Technology, Department of Information Management, Yunlin, Taiwan, R.O.C.","PICMET '07 - 2007 Portland International Conference on Management of Engineering & Technology","","2007","","","1083","1089","The impact of organizational learning on project performance has received a great deal of attention in recent years. Process focus is recognized as one of five factors which help to promote organizational learning through out the process. A theoretical model is derived based upon prior researches in literature to examine the effects of organizational learning and process focus on project performance. The structural equation modeling was adopted to test the proposed hypotheses, and Taiwanese corporate IS companies served as examples. The results revealed that organization process focus has a positive impact on organizational learning, which in turn has a positive influence on project performance. Both organization process focus and organizational learning play the influence on project performance. These findings should give valuable information for managers to revisit their priorities in terms of the relative efforts in organization process focus and organization learning.","2159-5100;2159-5119","978-1-8908-4315","10.1109/PICMET.2007.4349429","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4349429","","Project management;Information management;Testing;Software performance;Software quality;Companies;Programming;Scheduling;Equations;Planning","organisational aspects;project management","organization process focus;organizational learning;project performance;Taiwan companies;structural equation modeling;information systems","","2","62","","","","","","IEEE","IEEE Conferences"
"An Adaptive User-Genre-Item Model for Collaborative Filtering","J. Yang; K. F. Li","Software School, Hunan University, Changsha, China. E-mail: rj_jmyang@hnu.cn; Department of Electrical and Computer engineering, University of Victoria, Victoria, Canada. E-mail: kinli@uvic.ca","2007 IEEE Pacific Rim Conference on Communications, Computers and Signal Processing","","2007","","","257","262","Collaborative filtering provides personalized recommendations based on individual user preferences as well as those of other users with similar interests. In collaborative filtering, memory-based approaches make predictions by measuring the whole similarity between two users. When a user has multiple interest genres, those methods seem too optimistic in making correct predictions in some situations. In addition, minor genres are often inhibited due to their minute share of the whole similarity. In this paper, we present a novel approach that combines the advantages of item-item similarity and user-user similarity by introducing a genre component to the relation between user and item. In our approach, the direct user-item relevance is developed into the combination of genre similarity and preference similarity, thus capturing more accurately the relevance between items as well as between user and item. Experimental results from <i>EachMovie</i> and <i>MovieLens</i> datasets show that our approach outperforms four other state-of-the-art collaborative filtering algorithms.","1555-5798;2154-5952","1-4244-1190-4978-1-4244-1189-4978-1-4244-1190","10.1109/PACRIM.2007.4313224","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4313224","Collaborative filtering;genre similarity;preference similarity;relevance measurement","Collaboration;Adaptive filters;Testing;Sparse matrices;Filtering algorithms;Recommender systems;Information filtering;Information filters;Databases;Collaborative software","information filtering;information filters","adaptive user-genre-item model;collaborative filtering;personalized recommendation;user preferences;memory-based approach;multiple interest genres;item-item similarity;user-user similarity;direct user-item relevance;genre similarity;preference similarity","","1","19","","","","","","IEEE","IEEE Conferences"
"Decentralized Coordination Processes","M. Flanagan; P. Noakes; L. Paulsen; B. Verley; J. Weightman; P. Beling","department of Systems and Information Engineering at The University of Virginia.; department of Systems and Information Engineering at The University of Virginia.; department of Systems and Information Engineering at The University of Virginia.; department of Systems and Information Engineering at The University of Virginia.; department of Systems and Information Engineering at The University of Virginia.; Associate Professor in the Department of Systems Engineering at the University of Virginia.","2006 IEEE Systems and Information Engineering Design Symposium","","2006","","","106","111","This project reveals trends in algorithm strategy which helps to optimize the performance of independent agents in coordinated decision making problems. The software test bed constructed for this purpose models a problem consisting of independent agents which must coordinate, in the absence of communication, to achieve a common goal. The only decision making tool provided, in this scenario, is an algorithm designed to help the agents make independent decisions which can ultimately be useful in achieving the goal of the group. This project was motivated by desire to investigate the potential application of coordinated decision making algorithms to robotic surveillance missions on Mars. Thus, the problem formulation is comprised of unmanned aerial vehicles (UAVs) that have a common goal of target acquisition. We designed, implemented and tested several algorithms for success at achieving this goal. Results of this testing reveal that the impact of centralized control on algorithm performance is negligible compared to the benefits of added complexity and intelligence in coordination algorithms. This conclusion is promising for the application of coordinated decision making algorithms to robotic surveillance missions","","1-4244-0474-61-4244-0474","10.1109/SIEDS.2006.278722","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4055121","","Systems engineering and theory;Decision making;Robot kinematics;Testing;Centralized control;Algorithm design and analysis;Surveillance;Unmanned aerial vehicles;Mobile robots;Cost function","aerospace control;centralised control;control engineering computing;decentralised control;decision making;Mars;mobile robots;multi-agent systems;remotely operated vehicles;space vehicles;surveillance","decentralized coordination processes;decision making tool;software test bed;robotic surveillance missions;Mars;unmanned aerial vehicles;centralized control","","1","10","","","","","","IEEE","IEEE Conferences"
"Verifying global minima for L<inf>2</inf>minimization problems","R. Hartley; Yongduek Seo","Australian National University and NICTA, Australia; Sogang University, Korea","2008 IEEE Conference on Computer Vision and Pattern Recognition","","2008","","","1","8","We consider the least-squares (L2) triangulation problem and structure-and-motion with known rotatation, or known plane. Although optimal algorithms have been given for these algorithms under an L-infinity cost function, finding optimal least-squares (L2) solutions to these problems is difficult, since the cost functions are not convex, and in the worst case can have multiple minima. Iterative methods can usually be used to find a good solution, but this may be a local minimum. This paper provides a method for verifying whether a local-minimum solution is globally optimal, by providing a simple and rapid test involving the Hessian of the cost function. In tests of a data set involving 277,000 independent triangulation problems, it is shown that the test verifies the global optimality of an iterative solution in over 99.9% of the cases.","1063-6919","978-1-4244-2242-5978-1-4244-2243","10.1109/CVPR.2008.4587797","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4587797","","Testing;Iterative algorithms;Cost function;Polynomials;Iterative methods;Large-scale systems;Application software;Motion analysis;Constraint optimization;Minimax techniques","computational geometry;iterative methods;least squares approximations","least-squares triangulation problem;optimal algorithms;L-infinity cost function;optimal least-squares solutions;iterative methods","","1","9","","","","","","IEEE","IEEE Conferences"
"The TeraPaths Testbed: Exploring End-to-End Network QoS","D. Katramatos; D. Yu; B. Gibbard; S. McKee","RHIC/ATLAS Computing Facility, Physics Department, Brookhaven National Laboratory, Upton, NY 11793. dkat@bnl.gov; RHIC/ATLAS Computing Facility, Physics Department, Brookhaven National Laboratory, Upton, NY 11793. dtyu@bnl.gov; RHIC/ATLAS Computing Facility, Physics Department, Brookhaven National Laboratory, Upton, NY 11793. gibbard@bnl.gov; Physics Department, University of Michigan, Ann Arbor, MI 48109. smckee@umich.edu","2007 3rd International Conference on Testbeds and Research Infrastructure for the Development of Networks and Communities","","2007","","","1","7","The TeraPaths project at Brookhaven National Laboratory (BNL) investigates the combination of DiffServ-based LAN QoS with WAN MPLS tunnels in creating end-to-end (host-to-host) virtual paths with bandwidth guarantees. These virtual paths prioritize, protect, and throttle network flows in accordance with site agreements and user requests, and prevent the disruptive effects that conventional network flows can cause in one another. This paper focuses on the TeraPaths testbed, a collection of end-site subnets connected through high-performance WANs, serving the research and software development needs of the TeraPaths project. The testbed is rapidly evolving towards a multiple end-site infrastructure, dedicated to QoS networking research, and it offers unique opportunities for experimentation with minimal or no impact on regular, production networking operations.","","978-1-4244-0738-5978-1-4244-0739","10.1109/TRIDENTCOM.2007.4444698","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4444698","network;end-to-end;QoS;DiffServ;MPLS","Wide area networks;Local area networks;Bandwidth;Software testing;Physics;Laboratories;Multiprotocol label switching;Protection;System testing;Control systems","DiffServ networks;local area networks;multiprotocol label switching;quality of service;wide area networks","TeraPaths Testbed;end-to-end network QoS;DiffServ-based LAN;WAN MPLS tunnels;virtual paths;throttle network flows;end-site subnets","","3","15","","","","","","IEEE","IEEE Conferences"
"A Global Repair Operator for Capacitated Arc Routing Problem","Y. Mei; K. Tang; X. Yao","NA; NA; NA","IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)","","2009","39","3","723","734","Capacitated arc routing problem (CARP) has attracted much attention during the last few years due to its wide applications in real life. Since CARP is NP-hard and exact methods are only applicable for small instances, heuristics and metaheuristic methods are widely adopted when solving CARP. This paper demonstrates one major disadvantage encountered by traditional search algorithms and proposes a novel operator named global repair operator (GRO) to address it. We further embed GRO in a recently proposed tabu search algorithm (TSA) and apply the resultant repair-based tabu search (RTS) algorithm to five well-known benchmark test sets. Empirical results suggest that RTS not only outperforms TSA in terms of quality of solutions but also converges to the solutions faster. Moreover, RTS is also competitive with a number of state-of-the-art approaches for CARP. The efficacy of GRO is thereby justified. More importantly, since GRO is not specifically designed for the referred TSA, it might be a potential tool for improving any existing method that adopts the same solution representation.","1083-4419;1941-0492","","10.1109/TSMCB.2008.2008906","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4781617","Capacitated arc routing problem (CARP);global repair operator (GRO);heuristic search;tabu search","Routing;Vehicles;Costs;Application software;Computer science;Computer applications;Laboratories;Benchmark testing;Capacity planning","computational complexity;optimisation;search problems","global repair operator;capacitated arc routing problem;NP-hard problem;metaheuristic methods;heuristics methods;tabu search algorithm;repair-based tabu search algorithm","","34","38","","","","","","IEEE","IEEE Journals & Magazines"
"Kneser–Ney Smoothing With a Correcting Transformation for Small Data Sets","P. Taraba","NA","IEEE Transactions on Audio, Speech, and Language Processing","","2007","15","6","1912","1921","We present a technique which improves the Kneser-Ney smoothing algorithm on small data sets for bigrams, and we develop a numerical algorithm which computes the parameters for the heuristic formula with a correction. We give motivation for the formula with correction on a simple example. Using the same example, we show the possible difficulties one may run into with the numerical algorithm. Applying the algorithm to test data we show how the new formula improves the results on cross-entropy.","1558-7916;1558-7924","","10.1109/TASL.2007.900090","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4276766","Speech processing;speech recognition","Smoothing methods;Entropy;Testing;Speech recognition;Character recognition;Handwriting recognition;Optical character recognition software;Maximum likelihood detection;Maximum likelihood estimation","entropy;maximum likelihood estimation;optimisation;probability;smoothing methods;speech recognition","Kneser-Ney smoothing algorithm;correcting transformation;small data sets;bigrams;numerical algorithm;heuristic formula;cross-entropy;speech processing;speech recognition;probability;maximum-likelihood estimation","","1","7","","","","","","IEEE","IEEE Journals & Magazines"
"An Exact Breadth-First Search Algorithm for the Multiple Constant Multiplications Problem","L. Aksoy; E. O. Gunes; P. Flores","Istanbul Technical University Istanbul, Turkey, aksoyl@itu.edu.tr; Istanbul Technical University Istanbul, Turkey, ece.gunes@itu.edu.tr; IST/INESC-ID, TULisbon Lisbon, Portugal, pff@inesc-id.pt","2008 NORCHIP","","2008","","","41","46","This paper addresses the multiplication of one data sample with multiple constants using addition/subtraction and shift operations, i.e., the multiple constant multiplications (MCM) problem. The MCM problem finds itself and its variants in many applications, such as digital finite impulse response (FIR) filters, linear signal transforms, and computer arithmetic. Although many efficient algorithms have been proposed to implement the MCM using the fewest number of operations, due to the NP-hardness of the problem, they have been heuristics, i.e., they cannot guarantee the minimum solution. In this work, we propose an exact algorithm based on the breadth-first search that finds the minimum number of operations solution of mid-size MCM instances in a reasonable time. The proposed exact algorithm has been tested on a set of instances including FIR filter and randomly generated instances, and compared with the previously proposed efficient heuristics. It is observed from the experimental results that, even though the previously proposed heuristics obtain similar results with the minimum number of operations solutions, there are instances for which the exact algorithm finds better solutions than the prominent heuristics.","","978-1-4244-2492-4978-1-4244-2493","10.1109/NORCHP.2008.4738280","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4738280","","Finite impulse response filter;Signal processing algorithms;Iterative algorithms;Hardware;Application software;Digital filters;Nonlinear filters;Digital arithmetic;Testing;Fast Fourier transforms","computational complexity;multiplying circuits;optimisation;tree searching","breadth-first search algorithm;multiple constant multiplication problem;shift operation;addition-subtraction operation;NP-hard problem;randomly generated instance","","22","14","","","","","","IEEE","IEEE Conferences"
"Effect of thermal interface materials on manufacturing and reliability of Flip Chip PBGA and SiP packages","Li Li; M. Nagar; Jie Xue","Cisco Systems, Inc., San Jose, CA 95134, USA; Cisco Systems, Inc., San Jose, CA 95134, USA; Cisco Systems, Inc., San Jose, CA 95134, USA","2008 58th Electronic Components and Technology Conference","","2008","","","973","978","Power and power density increase in microelectronics is a major challenge for packaging high performance ASIC and microprocessor devices. The thermal interface material (TIM) used between the chip and the heat spreader of the Flip Chip Plastic Ball Grid Array (FC-PBGA) package plays a very important role in the package thermal performance. Not only does it affect package thermal performance, it can also affect assembly yield and package reliability during manufacturing and normal operation. In this study attention has been focused on improving thermal performance, manufacturing yield and reliability of the flip-chip PBGA single chip packages and the System in Package (SiP) modules. Computational Fluid Dynamics (CFD) software was used to investigate the effect of TIM on FC-PBGA thermal performance. The effect of thermal interface material was then studied for controlling the interaction between the heat spreader and the FC-PBGA SiP module to reduce module warpage and to improve module assembly yield. Qualification of TIM for FC-PBGA at both the component level and the system level was discussed. Component level testing data showed that the thermal characteristics and mechanical integrity of the TIM selected can be evaluated by using the same stress conditions used in package reliability qualification. Finally, system level non- operational humidity test results showed that good mechanical reliability at the thermal interface of the FC-PBGA can be achieved by optimizing the heat spreader attaching process.","0569-5503;2377-5726","978-1-4244-2230-2978-1-4244-2231","10.1109/ECTC.2008.4550094","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4550094","","Manufacturing;Materials reliability;Flip chip;Plastic packaging;Assembly;Computational fluid dynamics;Qualifications;Thermal stresses;Microelectronics;Application specific integrated circuits","ball grid arrays;computational fluid dynamics;electronic engineering computing;flip-chip devices;plastic packaging;reliability;system-in-package;thermal analysis","thermal interface materials;flip chip PBGA;SiP packages;ASIC devices;microprocessor devices;flip chip plastic ball grid array package;package thermal performance;system in package modules;computational fluid dynamics software;heat spreader;module warpage;module assembly yield;component level testing data;package reliability qualification;reliability","","7","3","","","","","","IEEE","IEEE Conferences"
"Automated Derivation of Application-aware Error Detectors using Static Analysis","K. Pattabiraman; Z. Kalbarczyk; R. K. Iyer","University of Illinois, USA; University of Illinois, USA; University of Illinois, USA","13th IEEE International On-Line Testing Symposium (IOLTS 2007)","","2007","","","211","216","This paper presents a technique to derive and implement error detectors to protect an application from data errors. The error detectors are derived automatically using compiler-based static analysis from the backward program slice of critical variables in the program. Critical variables are defined as those that are highly sensitive to errors, and deriving error detectors for these variables provides high coverage for errors in any data value used in the program. The error detectors take the form of checking expressions and are optimized for each control flow path followed at runtime. The derived detectors are implemented using a combination of hardware and software. Experiments show that the derived detectors incur low performance overheads while achieving high detection coverage for errors that impact the application.","1942-9398;1942-9401","0-7695-2918","10.1109/IOLTS.2007.21","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4274853","Critical Variables;Compiler techniques;backward slicing;checking expression;path-tracking.","Detectors;Computer crashes;Error correction;Runtime;Application software;Hardware;Protection;Error correction codes;Computer bugs;Timing","program slicing","automated derivation;application-aware error detectors;static analysis;backward program slice","","19","15","","","","","","IEEE","IEEE Conferences"
"Transparent Reconfigurable Acceleration for Heterogeneous Embedded Applications","A. C. S. Beck; M. B. Rutzig; G. Gaydadjiev; L. Carro","Universidade Federal do Rio Grande do Sul, Instituto de Informática - Porto Alegre/Brazil; Delft University of Technology, Computer Engineering - Delft/The Netherlands. caco@inf.ufrgs.br; Universidade Federal do Rio Grande do Sul, Instituto de Informática - Porto Alegre/Brazil. mbrutzig@inf.ufrgs.br; Delft University of Technology, Computer Engineering - Delft/The Netherlands. g.n.gaydadjiev@ewi.tudelft.nl; Universidade Federal do Rio Grande do Sul, Instituto de Informática - Porto Alegre/Brazil. carro@inf.ufrgs.br","2008 Design, Automation and Test in Europe","","2008","","","1208","1213","Embedded systems are becoming increasingly complex. Besides the additional processing capabilities, they are characterized by high diversity of computational models coexisting in a single device. Although reconfigurable architectures have already shown to be a potential solution for such systems, they just present significant speedups of very specific dataflow oriented kernels. Furthermore, reconfigurable fabric is still withheld by the need of special tools and compilers, clearly not sustaining backward software compatibility. In this paper, we propose a new technique to optimize both dataflow and control-flow oriented code in a totally transparent process, without the need of any modification in the source or binary codes. For that, we have developed a Binary Translation algorithm implemented in hardware, which works in parallel to a MIPS processor. The proposed mechanism is responsible for transforming sequences of instructions at runtime to be executed on a dynamic coarse-grain reconfigurable array, supporting speculative execution. Executing the MIBench suite, we show performance improvements of up to 2.5 times, while reducing 1.7 times the required energy, using trivial hardware resources.","1530-1591;1558-1101","978-3-9810801-3-1978-3-9810801-4","10.1109/DATE.2008.4484843","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4484843","","Acceleration;Hardware;Embedded system;Computational modeling;Reconfigurable architectures;Kernel;Fabrics;Software tools;Binary codes;Runtime","embedded systems;logic design;microprocessor chips;reconfigurable architectures","heterogeneous embedded applications;transparent reconfigurable acceleration;embedded systems;reconfigurable architectures;dataflow oriented kernels;control-flow oriented code;binary codes;binary translation algorithm;MIPS processor;dynamic coarse-grain reconfigurable array;speculative execution;MIBench suite","","24","26","","","","","","IEEE","IEEE Conferences"
"SecBus: Operating System controlled hierarchical page-based memory bus protection","Lifeng Su; S. Courcambeck; P. Guillemin; C. Schwarz; R. Pacalet","STMicroelectronics, 13106 Rousset, France; STMicroelectronics, 13106 Rousset, France; STMicroelectronics, 13106 Rousset, France; STMicroelectronics, 13106 Rousset, France; Institut TELECOM ; TELECOM ParisTech ; CNRS LTCI, 06904 Sophia Antipolis, France","2009 Design, Automation & Test in Europe Conference & Exhibition","","2009","","","570","573","This paper presents a new two-levels page-based memory bus protection scheme. A trusted operating system drives a hardware cryptographic unit and manages security contexts for each protected memory page. The hardware unit is located between the internal system bus and the memory controller. It protects the integrity and confidentiality of selected memory pages. For better acceptability the processor (CPU) architecture and the software application level are unmodified. The impact of the security on cost and performance is optimized by several algorithmic and hardware techniques and by a differentiated handling of memory pages, depending on their characteristics.","1530-1591;1558-1101","978-1-4244-3781-8978-3-9810-8015","10.1109/DATE.2009.5090729","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5090729","","Operating systems;Control systems;Protection;Hardware;Security;Cryptography;Memory management;System buses;Computer architecture;Application software","cryptography;microprocessor chips;operating systems (computers);paged storage;system buses","SecBus;page-based memory bus protection;trusted operating system;hardware cryptographic unit;security context;internal system bus;memory controller;memory pages integrity;memory pages confidentiality;CPU processor","","1","12","","","","","","IEEE","IEEE Conferences"
"Fast sizing and simulation of multipole radial flux permanent magnet synchronous machines","I. Elosegui; I. Egana; L. Fontan; A. Garcia Rico; M. Martinez-Iturralde","CEIT, Parque Tecnológico de San Sebastián, P° Mikeletegi 48, 20.009, (Spain); TECNUN, University of Navarra, P° M. Lardizabal 13, 20018. San Sebastián (Spain); CEIT, Parque Tecnológico de San Sebastián, P° Mikeletegi 48, 20.009, (Spain); TECNUN, University of Navarra, P° M. Lardizabal 13, 20018. San Sebastián (Spain); CEIT, Parque Tecnológico de San Sebastián, P° Mikeletegi 48, 20.009, (Spain)","2008 International Symposium on Power Electronics, Electrical Drives, Automation and Motion","","2008","","","1445","1450","This paper provides an automatic tool for the sizing, calculation, optimization and simulation of multipole radial flux permanent magnet synchronous machines (RPMSM). Only four parameters must be defined to obtain the machine: output power, rated voltage, frequency and number of pole pairs. firstly, geometric, electric and magnetic characteristics are obtained. Secondly, an accurate calculation of the sized machine is done. In order to check the influence in machine performance any variable can be modified in optimization loops. All this procedure has been implemented using MATLAB<sup>reg</sup> obtaining different prototypes in a very short time, reducing significantly RPMSM design process. The obtained geometry is exported to a finite element method software where the geometry, mesh, material characteristics and electrical circuit are automatically generated. Finally all the procedure has been contrasted and validated through the construction and testing of a laboratory prototype.","","978-1-4244-1663-9978-1-4244-1664","10.1109/SPEEDHAM.2008.4581163","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4581163","Analytical calculation algorithm;Design methodology;Finite element method;Radial flux permanent magnet machine","Magnetic flux;Optimization;Geometry;Finite element methods;Stators;Prototypes;Saturation magnetization","electric machine analysis computing;finite element analysis;permanent magnet machines;synchronous machines","multipole radial flux permanent magnet synchronous machines;power output;optimization;MATLAB;design process;finite element method software;geometric characteristics;automatic simulation tool;machine sizing;electric characteristics;magnetic characteristics","","","14","","","","","","IEEE","IEEE Conferences"
"Model predictive control of automotive powertrains - first experimental results","B. Saerens; M. Diehl; J. Swevers; E. Van den Bulck","department of mechanical engineering, K.U.Leuven, Celestijnenlaan 300, B-3001 Heverlee, Belgium; electrical engineering department and the optimization in engineering center (OPTEC), K.U.Leuven, Kasteelpark Arenberg 10, B-3001 Heverlee, Belgium; department of mechanical engineering, K.U.Leuven, Celestijnenlaan 300, B-3001 Heverlee, Belgium; department of mechanical engineering, K.U.Leuven, Celestijnenlaan 300, B-3001 Heverlee, Belgium","2008 47th IEEE Conference on Decision and Control","","2008","","","5692","5697","This paper illustrates the capabilities of model predictive control for the control of automotive powertrains. We consider the minimization of the fuel consumption of a gasoline engine through dynamic optimization. The minimization uses a mean value model of the powertrain and vehicle. This model has two state variables: the pressure in the engine manifold and the engine speed. The control input is the throttle valve angle. The model is identified on a universal dynamometer. Optimal state and control trajectories are calculated using Bock¿s direct multiple shooting method implemented in the software MUSCOD-II. The developed approach is illustrated both in simulation and experimentally for a test case where a vehicle accelerates from 1100 rpm to 3700 rpm in 30 s. The optimized trajectories yield minimal fuel consumption. The experiments show that the optimal engine speed trajectory yields a reduction of the fuel consumption of 12% when compared to a linear trajectory. Thus, it is shown that, even with a simple model, a significant amount of fuel can be saved without loss of the fun-to-drive.","0191-2216","978-1-4244-3123-6978-1-4244-3124","10.1109/CDC.2008.4738740","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4738740","","Predictive models;Predictive control;Automotive engineering;Mechanical power transmission;Fuels;Engines;Petroleum;Vehicle dynamics;Manifolds;Valves","automotive components;internal combustion engines;optimal control;position control;power transmission (mechanical);predictive control","model predictive control;automotive powertrains;gasoline engine fuel consumption;dynamic optimization;universal dynamometer;optimal state;trajectory control;Bock direct multiple shooting method;linear trajectory","","14","17","","","","","","IEEE","IEEE Conferences"
"A Post Evaluation Technique for Engineering Project Investment Based on ANP-ENTROPY-TOPSIS","L. Zhou; C. Li; X. Yu","NA; NA; NA","2009 International Conference on Management and Service Science","","2009","","","1","4","Mutual influence always exists among the indices in engineering project investment evaluation system. Since ANP (Analytic Network Process) technique can scientifically reflect the mutual influence among the indices, it is introduced into this paper instead of AHP to overcome the defect that traditional AHP technique can not reflect the relationship among the indices. And in the weight determining process, entropy method is adopted in combination with ANP to avoid the respective unilateralism of subjective weight method and objective weight method. On this basis, TOPSIS method is applied in the comprehensive evaluation for the engineering project investment schemes to quantitatively rank these schemes. Finally, by means of Super Decision software, an example is provided to show the effectiveness and scientificity of this evaluation technique.","","978-1-4244-4638-4978-1-4244-4639","10.1109/ICMSS.2009.5305005","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5305005","","Investments;Entropy;Power engineering and energy;Feedback;Industrial relations;Testing","decision making;investment;project management;statistical analysis","engineering project investment;analytic network process method;AHP technique;weight determining process;entropy method;TOPSIS method;Super Decision software","","","4","","","","","","IEEE","IEEE Conferences"
"Optimized multipinhole design for mouse imaging","K. Vunckx; J. Nuyts; B. Vanbilloen; M. De Saint-Hubert; D. Vanderghinste; D. Rattat; F. M. Mottaghy; M. Defrise","Dept. of Nuclear Medicine, K.U.Leuven, B-3000, Belgium; Dept. of Nuclear Medicine, K.U.Leuven, B-3000, Belgium; Dept. of Radiopharmacy, K.U.Leuven, B-3000, Belgium; Dept. of Nuclear Medicine, K.U.Leuven, B-3000, Belgium; Dept. of Radiopharmacy, K.U.Leuven, B-3000, Belgium; Dept. of Radiopharmacy, K.U.Leuven, B-3000, Belgium; Dept. of Nuclear Medicine, K.U.Leuven, B-3000, Belgium; Dept. of Nuclear Medicine, V.U.Brussel, B-1090, Belgium","2008 IEEE Nuclear Science Symposium Conference Record","","2008","","","4742","4750","To enhance high-sensitivity focused mouse imaging using multipinhole SPECT on a dual head camera, a fast analytical method was used to predict the contrast-to-noise ratio (CNR) in many points of a homogeneous cylinder for a large number of pinhole collimator designs with modest overlap. The design providing the best overall CNR, a configuration with 7 pinholes, was selected. Next, the pinhole pattern was made slightly irregular to reduce multiplexing artifacts. Two identical, but mirrored 7-pinhole plates were manufactured. In addition, the calibration procedure was refined to cope with small deviations of the camera from circular motion. First, the new plates were tested by reconstructing a simulated homogeneous cylinder measurement. Second, a Jaszczak phantom filled with 37 MBq <sup>99m</sup>Tc was imaged on a dual head gamma camera, equipped with the new pinhole collimators. The image quality before and after refined calibration was compared for both heads, reconstructed separately and together. Next, 20 short scans of the same phantom were performed with single and multipinhole collimation to investigate the noise improvement of the new design. Finally, two normal mice were scanned using the new multipinhole designs to illustrate the reachable image quality of abdomen and thyroid imaging. The simulation study indicated that the irregular patterns suppress most multiplexing artifacts. Using body support information strongly reduces the remaining multiplexing artifacts. Refined calibration improved the spatial resolution. Depending on the location in the phantom, the CNR increased with a factor of 1 to 2.5 using the new instead of a single pinhole design. The first proof of principle scans and reconstructions were successful, allowing the release of the new plates and software for preclinical studies in mice.","1082-3654","978-1-4244-2714-7978-1-4244-2715","10.1109/NSSMIC.2008.4774304","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4774304","","Design optimization;Mice;Magnetic heads;Cameras;Collimators;Calibration;Image reconstruction;Imaging phantoms;Image quality;Focusing","","","","3","31","","","","","","IEEE","IEEE Conferences"
"An experimental and numerical investigation of tube bank heat exchanger thermofluids","P. Rodgers; A. Goharzadeh; O. Abd Elhamid Ali; V. Eveloy","Department of Mechanical Engineering, The Petroleum Institute, P.O. Box 2533, Abu Dhabi, United Arab Emirates; Department of Mechanical Engineering, The Petroleum Institute, P.O. Box 2533, Abu Dhabi, United Arab Emirates; National Drilling Company, P.O. Box 4017, Abu Dhabi, United Arab Emirates; Department of Mechanical Engineering, The Petroleum Institute, P.O. Box 2533, Abu Dhabi, United Arab Emirates","EuroSimE 2008 - International Conference on Thermal, Mechanical and Multi-Physics Simulation and Experiments in Microelectronics and Micro-Systems","","2008","","","1","10","Heat exchangers are extensively used in engineering applications, such as for the thermal management of electronic cabinets. Although computational fluid dynamics (CFD) has the potential to provide a more accurate assessment of exchanger thermal performance than empirically-based software, CFD-based parametric analysis of a wide range of exchanger geometries and Reynolds numbers can be computationally prohibitive. This paper proposes and assesses the effectiveness of a dual design strategy, which combines empirical and numerical analyses of heat exchanger thermofluid performance. Empirical analysis serves to provide initial design specifications, while performance is optimized using CFD. The test vehicle consists of a staggered tube bank heat exchanger arrangement (St = Sl = 3.0). Good agreement is obtained between the empirical relationships developed by Martin [ 1 ] for heat transfer and Gaddis and Gnielinski [2] for pressure drop, and corresponding CFD predictions for Reynolds numbers varying from 1,749 to 17,491. Numerical flow field predictions are found to be accurately predicted relative to particle image velocimetry (PIV) measurements for a Reynolds number of 700. This study therefore provides a degree of confidence in using empirical correlations to undertake an initial sizing of tube bank heat exchanger design, to be refined for application specific environments using CFD analysis.","","978-1-4244-2127-5978-1-4244-2128","10.1109/ESIME.2008.4525103","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4525103","","Computational fluid dynamics;Thermal management of electronics;Performance analysis;Heat engines;Application software;Thermal engineering;Thermal management;Engineering management;Software performance;Computational geometry","computational fluid dynamics;drops;flow visualisation;heat exchangers","tube bank heat exchanger thermofluids;thermal management;electronic cabinets;computational fluid dynamics;CFD;empirically-based software;Reynolds numbers;pressure drop;numerical flow field predictions;particle image velocimetry;Reynolds number;empirical correlations","","1","49","","","","","","IEEE","IEEE Conferences"
"Selective clock gating by using wasting toggle rate","L. Li; K. Choi; S. Park; M. Chung","Department of Electrical and Computer Engineering, Illinois Institute of Technology, USA; Department of Electrical and Computer Engineering, Illinois Institute of Technology, USA; IT Convergence & Components Laboratory, SoC R&D Group, Electronics and Telecommunications Research Institute, USA; IT Convergence & Components Laboratory, SoC R&D Group, Electronics and Telecommunications Research Institute, USA","2009 IEEE International Conference on Electro/Information Technology","","2009","","","399","404","In this paper, we propose a RT level power reduction scheme which can be used for any applications that have power problem when designers use traditional design flow. A novel wasting-toggle-rate based clock power reduction technique is introduced and verified along with traditional design flow. The proposed technique can choose optimal clock-gating style selectively to minimize the power based on proposed wastingtoggle-rate analysis at RT level, and the optimization is based on proposed power equations without simulating the design at gate level. We have tested the proposed technique on real industrial multimedia-mobile-processor design. For the accuracy of the power optimization results, all of them are measured at gate level after synthesis by using industrial 65 nanometer technology library. The experimental results show that the technique reduces average 35.84% power comparing with non-clock gating design and 19.28% power comparing with clock-gating design by Power Compiler. The design overhead of the proposed technique is 1.79% increase of area and 2.55% increase of the critical path delay for whole circuit comparing with the original circuit.","2154-0357;2154-0373","978-1-4244-3354-4978-1-4244-3355","10.1109/EIT.2009.5189650","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5189650","","Clocks;Circuits;Character generation;Energy management;Laboratories;Research and development;Design optimization;Electrocardiography;Energy consumption;Application software","clock and data recovery circuits;combinational circuits;logic gates;low-power electronics","selective clock gating;wasting toggle rate;RT level power reduction;multimedia-mobile-processor design;power optimization;critical path delay","","2","10","","","","","","IEEE","IEEE Conferences"
"Development of DC Spark-over Voltage Measuring Instrument for Gas Discharge Tube with Zooming Input Voltage","W. Dai; Y. Xie","NA; NA","2009 Second International Symposium on Computational Intelligence and Design","","2009","2","","465","468","A novel DC spark-over voltage measuring instrument for gas discharge tube is presented in this paper. With a new voltage acquisition theory, which is detailed introduced in the paper, and a zooming input voltage as the testing voltage, the voltage value of gas discharge tube can be detected, when the electricity current passes the gas discharge tube and it is broken. The voltage value is DC spark-over voltage of the gas discharge tube. Hardware and software design are introduced in the paper. Experiment data show the design of the instrument is successful.","","978-0-7695-3865","10.1109/ISCID.2009.262","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5371045","Gas discharge tube;Spark-over voltage;Zooming input voltage","Voltage measurement;Instruments;Gas discharge devices;Particle swarm optimization;System identification;Neurons;Convergence;Genetic algorithms;Least squares methods;Equations","gas-discharge tubes;instruments;sparks;voltage measurement","DC spark-over voltage measuring instrument;gas discharge tube;zooming input voltage;voltage acquisition theory;testing voltage;electricity current","","1","12","","","","","","IEEE","IEEE Conferences"
"Miniaturization and Optimization of RF SAW Filter Using Wafer Level Packaging Technology","T. H. Kim; W. K. Jeung; S. J. Yang; S. M. Choi; S. W. Park; H. H. Kim; J. Ha; M. J. Park; S. Kao; J. P. Hong; S. Yi; J. S. Hwang; J. H. Lim; W. B. Kim","PKG Technology Team, Manufacture Engineering R&D Institute, Samsung Electro-Mechanics Co., LTD, 314, Meatan 3-Dong, Yeoungtong-Gu, Suwon, Gyunggi-Do, Korea; PKG Technology Team, Manufacture Engineering R&D Institute, Samsung Electro-Mechanics Co., LTD, 314, Meatan 3-Dong, Yeoungtong-Gu, Suwon, Gyunggi-Do, Korea; PKG Technology Team, Manufacture Engineering R&D Institute, Samsung Electro-Mechanics Co., LTD, 314, Meatan 3-Dong, Yeoungtong-Gu, Suwon, Gyunggi-Do, Korea; PKG Technology Team, Manufacture Engineering R&D Institute, Samsung Electro-Mechanics Co., LTD, 314, Meatan 3-Dong, Yeoungtong-Gu, Suwon, Gyunggi-Do, Korea; PKG Technology Team, Manufacture Engineering R&D Institute, Samsung Electro-Mechanics Co., LTD, 314, Meatan 3-Dong, Yeoungtong-Gu, Suwon, Gyunggi-Do, Korea; PKG Technology Team, Manufacture Engineering R&D Institute, Samsung Electro-Mechanics Co., LTD, 314, Meatan 3-Dong, Yeoungtong-Gu, Suwon, Gyunggi-Do, Korea; PKG Technology Team, Manufacture Engineering R&D Institute, Samsung Electro-Mechanics Co., LTD, 314, Meatan 3-Dong, Yeoungtong-Gu, Suwon, Gyunggi-Do, Korea; PKG Technology Team, Manufacture Engineering R&D Institute, Samsung Electro-Mechanics Co., LTD, 314, Meatan 3-Dong, Yeoungtong-Gu, Suwon, Gyunggi-Do, Korea; PKG Technology Team, Manufacture Engineering R&D Institute, Samsung Electro-Mechanics Co., LTD, 314, Meatan 3-Dong, Yeoungtong-Gu, Suwon, Gyunggi-Do, Korea; PKG Technology Team, Manufacture Engineering R&D Institute, Samsung Electro-Mechanics Co., LTD, 314, Meatan 3-Dong, Yeoungtong-Gu, Suwon, Gyunggi-Do, Korea; PKG Technology Team, Manufacture Engineering R&D Institute, Samsung Electro-Mechanics Co., LTD, 314, Meatan 3-Dong, Yeoungtong-Gu, Suwon, Gyunggi-Do, Korea; Packaging Project Team, Samsung Advanced Institute of Technology, San 14, Nongseo-Ri Giheung-Eup Yongin Gyunggi-Do, Korea; Packaging Project Team, Samsung Advanced Institute of Technology, San 14, Nongseo-Ri Giheung-Eup Yongin Gyunggi-Do, Korea; Packaging Project Team, Samsung Advanced Institute of Technology, San 14, Nongseo-Ri Giheung-Eup Yongin Gyunggi-Do, Korea","2007 Proceedings 57th Electronic Components and Technology Conference","","2007","","","574","579","In this paper, we describes the wafer level surface acoustic wave (SAW) filter package, 1.0times0.8 mm<sup>2</sup>, which is applicable for radio frequency (RF) stage in mobile phones. The SAW filter is reduced in size and thickness by using a 4"" wafer level package process technique. The technique uses interconnection via and LiTaO<sub>3</sub> (LT)-LiTaO<sub>3</sub> (LT) wafer bonding structure. The interconnection via is formed through LT wafer by using sand blasting or laser drilling method. The AuSn eutectic bonding enables the connection of the signal pad on the SAW chip, with gold metallized LT wafer package. This eutectic bonding ensures that the SAW chip is protected mechanically and connected electrically, with the package. In order to simulate and optimize the structure and characteristics of wafer level SAW filter package, we used HFSS and ADS software. Frequency responses of measurement and simulation are compared with wafer level SAW filter package. The results of reliability tests for wafer level SAW filter package will be discussed.","0569-5503;2377-5726","1-4244-0984-51-4244-0985","10.1109/ECTC.2007.373853","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4249939","","Radio frequency;SAW filters;Wafer scale integration;Packaging;Surface acoustic waves;Wafer bonding;Acoustic waves;Mobile handsets;Drilling;Gold","frequency response;laser beam machining;reliability;surface acoustic wave filters;wafer bonding;wafer level packaging","RF SAW filter;wafer level packaging technology;surface acoustic wave;mobile phones;wafer bonding structure;eutectic bonding;sand blasting;laser drilling method;frequency responses;reliability tests","","3","7","","","","","","IEEE","IEEE Conferences"
"Towards More Accurate Unit Commitment Performance Comparisons","F. Leanez; R. Palma-Behnke","NA; Senior Member, IEEE","2006 IEEE/PES Transmission & Distribution Conference and Exposition: Latin America","","2006","","","1","8","Unit commitment (UC) problem is still an open research field given the economic impact of its multiple applications in nowadays operation planning. It has been solved by a wide variety of optimization and heuristic methods. However, qualitative comparisons have traditionally been limited by two different factors: computer platform dependency and uses of custom testing models instead of standardized benchmark problems. This paper proposes a methodology for comparing UC simulation results based on instance classification and computer performance benchmark indices. It also establishes guidelines towards standardization of UC benchmark models. A complete numerical example using the proposed methodology is presented. Performance comparison results from various solution methodologies are also discussed. It is intended to prepare the basis for future objective benchmark comparisons in the UC research field","","1-4244-0288-31-4244-0287","10.1109/TDCLA.2006.311527","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4104758","Generating scheduling;lagrangian relaxation;power system operation;Unit commitment","Lagrangian functions;Benchmark testing;Genetic algorithms;Logic programming;Application software;Optimization methods;Computational modeling;Power system modeling;Power generation economics;Computer simulation","power generation dispatch;power generation economics;power generation planning;power generation scheduling","unit commitment performance comparisons;economic impacts;computer platform dependency;computer performance benchmark indices","","","44","","","","","","IEEE","IEEE Conferences"
"An Approach of Semantic Similarity Measure between Ontology Concepts Based on Multi Expression Programming","S. Xia; Z. Hu; Q. Niu","NA; NA; NA","2009 Sixth Web Information Systems and Applications Conference","","2009","","","184","188","To improve accuracy of semantic similarity measure between ontology concepts, four main factors that impact on semantic similarity measure is taken into account. They are semantic distance, semantic depth, semantic coincidence and semantic density. Firstly, they were preprocessed to obtain four basic methods for calculating semantic similarity. And then Multi Expression Programming algorithm was adopted to combine and optimize the four basic methods. Thus, an approach of semantic similarity measure between ontology concepts based on Multi Expression Programming is proposed. At last, the approach is tested using dataset extracted from WordNet. The experiment result shows that the approach can be able to exclude the influence of non-key factor and enhance accuracy of semantic similarity measure.","","978-0-7695-3874","10.1109/WISA.2009.34","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5368079","Ontology;hierarchical structure;semantic similarity;Multi Expression Programming (MEP)","Ontologies;Evolutionary computation;Information retrieval;Information systems;Application software;Computer science;Optimization methods;Testing;Genetic programming;Information technology","mathematical programming;ontologies (artificial intelligence)","semantic similarity measure;ontology concepts;multiexpression programming;semantic distance;semantic depth;semantic coincidence;semantic density","","3","10","","","","","","IEEE","IEEE Conferences"
"Preliminary analysis of a legged robot designed to climb vertical surfaces","Y. Li; A. Ahmed; C. Wu; C. Menon","MENRVA Group, School of Engineering Science, Simon Fraser University, Burnaby, Canada; MENRVA Group, School of Engineering Science, Simon Fraser University, Burnaby, Canada; MENRVA Group, School of Engineering Science, Simon Fraser University, Burnaby, Canada; MENRVA Group, School of Engineering Science, Simon Fraser University, Burnaby, Canada","2009 IEEE Toronto International Conference Science and Technology for Humanity (TIC-STH)","","2009","","","887","892","This paper presents the kinematic analysis of a hexapod climbing robot relying on the use of dry adhesion. Kinematics equations are validated in both multi-body software simulation and robotic platform test. A particular trajectory of the legs, conceived to minimize force required to detach the robotic feet from a vertical wall, is proposed and tested. Further study is performed based on data acquired by forces exerted on the tip of each robotic leg during locomotion. Experimental results proved the correctness of kinematic analysis and its potential use for optimizing gait and adhesion features during wall climbing.","","978-1-4244-3877-8978-1-4244-3878","10.1109/TIC-STH.2009.5444374","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5444374","climbing robot;kinematics;hexapod;peeling;dry adhesion;legged robot","Legged locomotion;Adhesives;Leg;Climbing robots;Robot sensing systems;Kinematics;Mobile robots;Testing;Robust control;Navigation","force control;legged locomotion;position control;robot kinematics","legged robot;vertical surface climbing;kinematic analysis;hexapod climbing robot;dry adhesion;multibody software simulation;robotic platform test;force minimization;gait feature;adhesion feature;leg trajectory;robotic feet","","1","24","","","","","","IEEE","IEEE Conferences"
"Energy Management Operating Modes Concerning a Hybrid Heavy Vehicle","K. El Kadri; A. Berthon","L2ES, BELFORT, FRANCE, e-mail: khadija.el-kadri@utbm.fr; University of Franche-Comte - L2ES-, BELFORT, FRANCE, e-mail: alain.berthon@univ-fcomte.fr","EUROCON 2007 - The International Conference on "Computer as a Tool"","","2007","","","1587","1593","The aim of this study is to present mainly the subject of the energy sources hybridization. This work is carried out within the laboratory on a test bench called ECCE """" which is 12 tons truck with several energy sources on board. This project launched by the DGA (governmental organization) gathered several industrial and university partners. Although pollution was not the main characteristic justifying the dimensioning of this vehicle, the development of a software tool by modeling the energy components and power distribution, will make it possible to solve problems of optimization and strategy of management. For understanding and analyzing the operation of the system, this work proposes a simulation tool that takes into account the various components of the car while being focused on the energy groups. After presenting modeling for each part of the vehicle, this paper will show some results for various operating modes of the vehicle. Experimental recordings will be given to validate the correct operation of the vehicle.","","978-1-4244-0812-2978-1-4244-0813","10.1109/EURCON.2007.4400566","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4400566","Hybrid electric vehicle (HEV);lead acid battery;modelling (converters - electrical machines);simulation tool;power electronics;drive and control;design","Energy management;Vehicles;Power system modeling;Laboratories;Testing;Dissolved gas analysis;Environmentally friendly manufacturing techniques;Industrial pollution;Software tools;Power distribution","energy management systems;hybrid electric vehicles","energy management operating modes;hybrid heavy vehicle;energy source hybridization;software tool;simulation tool","","6","25","","","","","","IEEE","IEEE Conferences"
"Texture segmentation benchmark","M. Haindl; S. Mikes","Institute of Information Theory and Automation, of the ASCR, 182 08 Prague, Czech Republic; Institute of Information Theory and Automation, of the ASCR, 182 08 Prague, Czech Republic","2008 19th International Conference on Pattern Recognition","","2008","","","1","4","The Prague texture segmentation data-generator and benchmark is a Web based (http://mosaic.utia.cas.cz) service designed to mutually compare and rank different texture segmenters, and to support new segmentation and classification methods development. The benchmark verifies their performance characteristics on monospectral, multispectral, bidirectional texture function (BTF) data and enables to test their noise robustness, scale, and rotation or illumination invariance. It can easily be used for other applications such as feature selection, image compression, and query by pictorial example, etc. The benchmark functionalities are demonstrated on five previously published image segmentation algorithms evaluation.","1051-4651","978-1-4244-2174-9978-1-4244-2175","10.1109/ICPR.2008.4761118","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4761118","","Image segmentation;Benchmark testing;Layout;Humans;Image databases;Information theory;Design automation;Noise robustness;Lighting;Image coding","image classification;image retrieval;image segmentation;image texture;Internet;unsupervised learning","Prague texture segmentation data-generator algorithm;Web based service;image classification;monospectral data;multispectral data;bidirectional texture function data;feature selection;image compression;query by pictorial example;benchmark software;supervised learning;unsupervised learning;texture segmentation ranking","","18","15","","","","","","IEEE","IEEE Conferences"
"Improving Clinical Relevance in Ensemble Support Vector Machine Models of Radiation Pneumonitis Risk","T. W. Schiller; Y. Chen; I. E. Naqa; J. O. Deasy","NA; NA; NA; NA","2009 International Conference on Machine Learning and Applications","","2009","","","498","503","Patients undergoing thoracic radiation therapy can develop radiation pneumonitis (RP), a potentially fatal inflammation of the lungs. Support vector machines (SVMs), a statistical machine learning method, have recently been used to build binary-outcome RP prediction models with promising results. In this work, we (1) introduce a feature-ranking selection step to limit complexity in ensemble SVM models (2) show that ensembles of SVMs provide a statistically significant performance improvement in the area under the cross-validated receiver operating curve and (3) apply Platt's tuning to generate probability estimates from the component SVMs in order to augment clinical relevance.","","978-0-7695-3926","10.1109/ICMLA.2009.74","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381446","biological effects of radiation;modeling;probability","Support vector machines;Predictive models;Testing;Machine learning;Biomedical applications of radiation;Lungs;Probability;Sensitivity;Application software;Computer science","biological effects of radiation;injuries;lung;medical computing;probability;radiation therapy;risk analysis;support vector machines","ensemble SVM model;support vector machine;radiation pneumonitis risk model;thoracic radiation therapy;lung inflammation;statistical machine learning method;binary outcome prediction models;Platt tuning;probability estimates;feature ranking selection;cross-validated receiver operating curve","","","18","","","","","","IEEE","IEEE Conferences"
"The Design of Battery On-line Monitoring Device Based on Wireless Communication","X. Shi; Z. Cai; Q. Zhai; D. Zhang; H. Kou","NA; NA; NA; NA; NA","2008 ISECS International Colloquium on Computing, Communication, Control, and Management","","2008","2","","393","397","The battery online monitoring device is regarded as the research object, a scheme based on Cirrus logic's EP9315 processor and embedded Linux operating system is proposed. Short-distance wireless communication technology is adopted to transfer data. The hardware structure and software design of the system are also introduced. The battery inner resistance is measured by DC discharge, which guaranteed the reliability of online monitoring. The application of fuzzy neural network (FNN) optimized by genetic algorithm (GA) in battery capacity testing has wide prospect.","2154-9613;2154-963X","978-0-7695-3290","10.1109/CCCM.2008.144","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4609713","Embedded System;Wireless Communication;Inner Resistance;Fuzzy Neural Network;Capacity predict","Batteries;Resistance;Discharges;Voltage measurement;Electrical resistance measurement;Battery charge measurement;Temperature measurement","battery management systems;computerised monitoring;electric resistance measurement;fuzzy neural nets;genetic algorithms;radiotelemetry","battery online monitoring device;Cirrus logic EP9315 processor;embedded Linux operating system;short-distance wireless communication;data transfer;hardware structure;software design;battery inner resistance;DC discharge;fuzzy neural network;genetic algorithm;battery capacity testing","","","6","","","","","","IEEE","IEEE Conferences"
"A service-centric approach for exploiting network intelligence","D. X. Adamopoulos","Department of Technology Education & Digital Systems, University of Piraeus, Greece","2009 Second International Conference on the Applications of Digital Information and Web Technologies","","2009","","","145","150","Building new advanced multimedia telecommunications services in a distributed and heterogeneous environment is very difficult, unless there is a methodology to support the entire service development process in a structured and systematic manner, and assist and constrain service designers and developers by setting out goals and providing specific means to achieve these goals. Therefore, in this paper, after a brief presentation of a proposed service creation methodology, its service design phase is examined in detail focusing on the essential activities and artifacts. In this process, the exploitation of important service engineering techniques and UML modelling principles is especially considered. Finally, alternative and complementary approaches for service design are highlighted and a validation attempt is briefly outlined.","","978-1-4244-4456-4978-1-4244-4457","10.1109/ICADIWT.2009.5273857","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5273857","","Intelligent networks;Telematics;Testing;Telecommunication services;Design optimization;Intelligent structures;Educational technology;Digital systems;Buildings;Multimedia systems","multimedia communication;program verification;software engineering;Unified Modeling Language","network intelligence;service-centric approach;multimedia telecommunications services;heterogeneous environment;distributed environment;service engineering techniques;UML modelling principles;validation attempt;service creation methodology","","1","11","","","","","","IEEE","IEEE Conferences"
"Chainsaw: Using Binary Matching for Relative Instruction Mix Comparison","T. Moseley; D. Grunwald; R. Peri","NA; NA; NA","2009 18th International Conference on Parallel Architectures and Compilation Techniques","","2009","","","125","135","With advances in hardware, instruction set architectures are undergoing continual evolution. As a result, compilers are under constant pressure to adapt and take full advantage of available features. However, current techniques for evaluating relative compiler performance only compare profiles at the application level, ignoring relative performance differences at finer granularities. To ensure that new features are put to good use, a more rigorous approach is necessary. A fundamental step in tuning compiler performance is identifying the specific examples that can be improved. To solve this problem, we present a compiler-independent binary matching technique to compare executions of differently compiled programs and identify intervals where the behavior can be meaningfully compared. Matched intervals can be automatically analyzed to identify anomalous segments of execution where one version performs significantly differently versus another. We present case studies using Chainsaw to identify significant performance anomalies between differently compiled codes.","1089-795X","978-0-7695-3771","10.1109/PACT.2009.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5260556","compilers;profiling;binary matching","Optimizing compilers;Program processors;Testing;Hardware;Instruction sets;Parallel architectures;Computer science;Computer architecture;Performance analysis;Frequency","instruction sets;pattern matching;program compilers;software architecture","Chainsaw;binary matching;relative instruction mix comparison;instruction set architectures;compilers","","","24","","","","","","IEEE","IEEE Conferences"
"A Case Study on Programming Intelligent Swarm Robots Using Pyro Environment and Player/Stage Simulator","B. Lee; J. Ji; G. Woo","NA; NA; NA","2008 International Conference on Convergence and Hybrid Information Technology","","2008","","","3","6","A set of robots which shows an intelligent behavior as a whole is called swarm robots. Programming real swarm robots needs a lot of financial cost because many robot hardware systems should be purchased to test the implemented swarm intelligence. In this paper, we report a case study on programming swarm intelligence to control a set of robots using Pyro. Pyro (Python Robotics) is a Python based programming environment to provide a common platform for controlling robots. In practice, Pyro is normally operates on top of a simulator such as Player/Stage. This paper reports the experience on programming swarm robots searching for target objects in a maze. The swarm robots implemented can successfully move the target objects in a maze to the home location, where the robots started from.","","978-0-7695-3328","10.1109/ICHIT.2008.278","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4622791","","Robots;Robot kinematics;Robot sensing systems;Libraries;Particle swarm optimization;Software;Programming","control engineering computing;digital simulation;intelligent robots;mobile robots;multi-robot systems;programming environments;robot programming","intelligent swarm robot programming;player/stage simulator;robot hardware system;Pyro environment;Python robotics;Python based programming environment;mobile robot controlling platform","","2","12","","","","","","IEEE","IEEE Conferences"
"The Influence of Metal Environment on the Performance of UHF Smart Labels in Theory, Experimental Series and Practice","J. Wagner; R. Fischer; W. A. Gunthner","Department of Material Handling, Material Flow and Logistics, Technische Universität München, Boltzmannstraße 15, Garching, 85748 Germany; Department of Material Handling, Material Flow and Logistics, Technische Universität München, Boltzmannstraße 15, Garching, 85748 Germany; Prof., Department of Material Handling, Material Flow and Logistics, Technische Universität München, Boltzmannstraße 15, Garching, 85748 Germany","2007 1st Annual RFID Eurasia","","2007","","","1","6","RFID is regarded as the future technology for the optimization of logistic processes. But there is a lack of methods and tools to support the logistic planer by identifying and implementing RFID applications. This is picked up by the research project ""RFID in logistics"" at the Technical University of Munich. The target is to point out possible RFID applications and appropriate components by structuring the logistic processes and comparing it to implemented RFID applications and to the results of basic experimental series. Because of the high influence of metal environment on the performance of especially UHF RFID systems analyzes of occurring effects in experimental series are a main focus in the project to provide a statement for the feasibility of possible RFID applications. In the scope of this essay the different physical effects of metal on UHF RFID systems are explained and quantified with the results of executed experimental series. Furthermore the possibilities of mounting metal parts are described and tested including a special absorber material invented by the department. The transferability of the results of the experimental series and their effects of shorting the selection time are shown exemplary by means of a feasibility study for metal pallet management.","","978-975-01566-0","10.1109/RFIDEURASIA.2007.4368122","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4368122","","Radiofrequency identification;Logistics;Companies;Application software;Software tools;Materials handling;Metalworking machines;Performance analysis;Materials testing;Inorganic materials","radiofrequency identification","metal environment;UHF smart labels;RFID systems;special absorber material;metal pallet management","","5","7","","","","","","IEEE","IEEE Conferences"
"A DSP Based H.264 Decoder for a Multi-Format IP Set-Top Box","F. Pescador; C. Sanz; M. J. Garrido; E. Juárez; D. Samper","NA; NA; NA; NA; NA","IEEE Transactions on Consumer Electronics","","2008","54","1","145","153","In this paper, the implementation of a digital signal processor (DSP) based H.264 decoder for a multi-format set-top box is described. Baseline and main profiles are supported. Using several software optimization techniques, the decoder has been fitted into a low-cost DSP. The decoder alone has been tested in simulation, achieving real-time performance with a 600 MHz system clock. Moreover, it has been integrated in a multi-format IP set-top box allowing the implementation of actual environment tests with excellent results. Finally, the decoder has been ported to a latest generation DSP.","0098-3063;1558-4127","","10.1109/TCE.2008.4470037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4470037","","Digital signal processing;Decoding;US Department of Transportation;Digital signal processors;Digital TV;Ethernet networks;Signal processing algorithms;Signal generators;System testing;Real time systems","digital signal processing chips;video coding","DSP;H.264 decoder;multi-format IP set-top box;digital signal processor;software optimization techniques;frequency 600 MHz","","20","28","","","","","","IEEE","IEEE Journals & Magazines"
"Annotated Databases for the Recognition of Screen-Rendered Text","S. Wachenfeld; H. -. Klein; X. Jiang","University of Munster, Germany; University of Munster, Germany; University of Munster, Germany","Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)","","2007","1","","272","276","The recognition of screen-rendered text is a novel task. It is performed e.g. by translation tools which allow users to click on any text on the screen and give a translation. Also some commercial OCR programs start to address the problem of reading screenshots. Optical character recognition on screen-shot images can be very challenging due to very small and smoothed fonts. In order to build and compare recognition approaches for screen-rendered text, the availability of standard databases is a fundamental prerequisite. In this paper two freely available databases are presented, one that consists of annotated screenshot images of 28080 single characters and another holding 400 words extracted from documents plus 2 400 generated isolated words. Both databases include meta-information such as x-height, font type, style and rendering conditions. At the example of a developed recognition system, it is shown how these databases can serve for training, testing and optimization.","1520-5363;2379-2140","0-7695-2822-8978-0-7695-2822","10.1109/ICDAR.2007.4378718","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4378718","","Text recognition;Image databases;Optical character recognition software;System testing;Pattern recognition;Rendering (computer graphics);Operating systems;Character recognition;Character generation;Application software","optical character recognition;text analysis;visual databases","annotated databases;translation tools;screen-rendered text recognition;OCR programs;optical character recognition;screen-shot images","","2","12","","","","","","IEEE","IEEE Conferences"
"Performance Comparison of Various Garbage Collectors on JVM for Web Services","H. Lam; G. S. V. R. K. Rao; C. Eswaran; K. Ng","Faculty of Engineering, Multimedia University, Cyberjaya, Selangor, Malaysia. Tel: +603-83125350 Email: hslam@mmu.edu.my; Faculty of Information Technology, Multimedia University, Cyberjaya, Selangor, Malaysia. Tel: +603-83125458 Email: gsvradha@mmu.edu.my; Faculty of Information Technology, Multimedia University, Cyberjaya, Selangor, Malaysia. Tel: +603-83125458 Email: eswaran@mmu.edu.my; Faculty of Engineering, Multimedia University, Cyberjaya, Selangor, Malaysia. Tel: +603-83125350 Email: ng.kok.seong@mmu.edu.my","2006 International Symposium on Communications and Information Technologies","","2006","","","711","715","The current demand of e-commerce has increased the requirement of efficiency on Java server-side applications. Hence, the continuous availability and the good response-time of the Java virtual machine are needed to satisfy the continuous incoming request from remote clients. This project studied the optimization of garbage collector in Java virtual machine particularly on 3-tier Java server-side application to increase the throughput of real times processing, full utilization of CPU time and more memory efficiency to handle more workloads. The characteristics and the architectures of five JikesRVM garbage collectors were studied. They are CopyMS, GenMS, SemiSpace, GenCopy and MarkSweep. The best performing garbage collectors was determined and the main causes of their overheads were identified. The benchmarking suite, SPECjbb2000 was used to emulate a 3-tier Java server-side application. The performance of the five different garbage collectors on SPECjbb2000 for large and small memory size was compared. In conclusion, CopyMS is found to have the best average throughput for large heap size whereas, GenMS has the best overall performance in memory constraint and can handle the most workloads. On the other hand, GenCopy and SemiSpace demonstrated higher efficiency in handling light workloads. The performance of garbage collectors is proportional to the heap size used. Memory fragmentation and long pause time are two main challenges to be overcome for increasing the application performance. Future works for optimizing garbage collectors are recommended at the end of this report","","0-7803-9740-10-7803-9741","10.1109/ISCIT.2006.339833","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4141478","","Web services;Java;Virtual machining;Application software;Throughput;Memory management;Testing;Hardware;Information technology;Scalability","electronic commerce;Java;virtual machines;Web services","Web services;JVM;e-commerce;Java server-side applications;continuous availability;Java virtual machine;remote clients;3-tier Java server-side application;JikesRVM garbage collectors;CopyMS;GenMS;SemiSpace;GenCopy;MarkSweep;SPECjbb2000;memory fragmentation","","2","2","","","","","","IEEE","IEEE Conferences"
"A Study of Performance improvement on the End-host through Diagnosis of System Metrics","Kook-han Kim; Jong-ho Ryu; Dong-il Seo","161 Gajeong-dong, Yuseong-gu, Daejeon, Korea, ETRI, kimkook@etri.re.kr; NA; NA","2006 8th International Conference Advanced Communication Technology","","2006","1","","769","772","These days in Internet, emerging various applications has become more complicated and needed large bandwidth under the necessity to securely use them. Network bandwidth has increased as much as to support those needs. But it becomes increasingly difficult to utilize them to their full capacity [I]. In other word, up-grading of network bandwidth is not enough solution to enhance performance experienced by end users. There are so many causes including TCP/IP protocol behaviors, application issues and network configuration issues etc. In this paper, I would suggest a new system diagnosis tool to settle specially enhancement of end user & E2E performance. The goals of this tool are to enable an end user & network operator to diagnose an end-host system and E2E performance weak point, offer some advises and optimize the systems by given auto or manual system metrics tuning guideline. And I wish to get an overlay network to understand network characteristics and utilize full network capacity. In this paper, we look over briefly other relative projects and introduce some key metrics to optimize end-host performance","","89-5519-129","10.1109/ICACT.2006.206077","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1625682","E2E(End-to-End) Performance;Diagnostic Metrics;Network characteristic","Bandwidth;Testing;TCPIP;Internet;Protocols;Guidelines;Software measurement;Educational institutions;Laboratories;IP networks","Internet;performance evaluation;transport protocols","system metrics;Internet;network bandwidth;TCP/IP protocol;system diagnosis tool;end to end performance","","","20","","","","","","IEEE","IEEE Conferences"
"Application mapping for chip multiprocessors","Guangyu Chen; Feihui Li; S. W. Son; M. Kandemir","Microsoft, USA; NVIDIA, USA; Penn State University, USA; Penn State University, USA","2008 45th ACM/IEEE Design Automation Conference","","2008","","","620","625","The problem attacked in this paper is one of automatically mapping an application onto a network-on-chip (NoC) based chip multiprocessor (CMP) architecture in a locality-aware fashion. The proposed compiler approach has four major steps: task scheduling, processor mapping, data mapping, and packet routing. In the first step, the application code is parallelized and the resulting parallel threads are assigned to virtual processors. The second step implements a virtual processor-to-physical processor mapping. The goal of this mapping is to ensure that the threads that are expected to communicate frequently with each other are assigned to neighboring processors as much as possible. In the third step, data elements are mapped to memories attached to CMP nodes. The main objective of this mapping is to place a given data item into a node which is close to the nodes that access it. The last step of our approach determines the paths (between memories and processors) for data to travel in an energy efficient manner. In this paper, we describe the compiler algorithms we implemented in detail and present an experimental evaluation of the framework. In our evaluation, we test our entire framework as well as the impact of omitting some of its steps. This experimental analysis clearly shows that the proposed framework reduces energy consumption of our applications significantly (27.41% on average over a pure performance oriented application mapping strategy) as a result of improved locality of data accesses.","0738-100X","978-1-60558-115","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4555892","Compilers;NoC (Network on Chip);Power Optimization;Application Mapping;Chip Multiprocessing","Network-on-a-chip;Yarn;Energy consumption;Application software;Processor scheduling;Routing;Energy efficiency;Testing;Performance analysis;Computer languages","microprocessor chips;network-on-chip;processor scheduling;program compilers","network-on-chip;chip multiprocessors;automatic mapping;locality-aware fashion;task scheduling;processor mapping;data mapping;packet routing;virtual processors;compiler algorithms;application mapping strategy","","","26","","","","","","IEEE","IEEE Conferences"
"Distributed Discrete Event and Pseudo Real-Time Combined Simulation for Multi-Agent Controlled Power Plants","J. H. Van Sickel; K. Y. Lee","Department of Electrical Engineering, Pennsylvania State University, University Park, PA 16802; Department of Electrical and Computer Engineering, Baylor University, Waco, TX 76798","2009 15th International Conference on Intelligent System Applications to Power Systems","","2009","","","1","6","Development, testing, simulation, and validation of complex real-time distributed systems is a challenge on many levels. Designing multi-agent systems for power plant control can be approached numerous ways, but it is important to enhance the flow of development to implementation. Using new Distributed Real-time Agent Framework with Time-warp (DRAFT) agents can enhance the development of multi-agent systems. Discrete event simulation allows a fast as possible simulation and forces synchronization while guaranteeing all tasks are accomplished on time to assist with initial stages of development when it is more important to focus on larger design concepts. The DRAFT agents can switch over to standard operation and operate on psuedo real-time or real-time environments to further develop all aspects of a multi-agent system with real-time requirements. It also gives the agents self simulation capabilities that could be used to enhance overall functionality of the multi-agent system.","","978-1-4244-5097-8978-1-4244-5098","10.1109/ISAP.2009.5352879","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5352879","Discrete event simulation;multi-agent system;optimistic simulation;power plant;real-time;time warp","Discrete event simulation;Power generation;Real time systems;Multiagent systems;System testing;Process design;Switches;Time warp simulation;Power engineering and energy;Application software","discrete event simulation;multi-agent systems;power system control","distributed discrete event simulation;multi-agent systems;power plant control;distributed real-time agent framework;time-warp","","","9","","","","","","IEEE","IEEE Conferences"
"Parallel Implementation of the Novel Approach to Genome Assembly","J. Blazewicz; M. Kasprzak; A. Swiercz; M. Figlerowicz; P. Gawron; D. Platt; L. Szajkowski","NA; NA; NA; NA; NA; NA; NA","2008 Ninth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing","","2008","","","732","737","DNA assembly problem is well known for its high complexity both on biological and computational levels. Traditional laboratory approach to the problem, which involves DNA sequencing by hybridization or by gel electrophoresis, entails a lot of errors coming from experimental and algorithmic stages. DNA sequences constituting the traditional assembly input have lengths about a few hundreds of nucleotides and they cover each other rather sparsely. A new biochemical approach to DNA sequencing gives highly reliable output of relatively low cost and in short time. It is 454 sequencing, based on the pyrosequencing protocol, owned by 454 Life Sciences Corporation. The produced sequences are shorter (about 100-200 nucleotides) but their coverage in the assembled sequence is very dense. In the paper, we propose a parallel implementation of an algorithm dealing well with such data and outperforming other assembly algorithms used in practice. The algorithm is a heuristic based on a graph model, the graph being built on the set of input sequences. Computational tests were performed on real data obtained from the 454 sequencer during sequencing the genome of bacteria Prochlorococcus marinus.","","978-0-7695-3263","10.1109/SNPD.2008.47","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4617459","bioinformatics;DNA assembly;454 sequencing;parallel implementation;graphs;heuristics","Assembly;DNA;Genomics;Bioinformatics;Program processors;Microorganisms;Algorithm design and analysis","biology computing;DNA;graph theory;optimisation;parallel algorithms","parallel implementation;genome assembly;DNA assembly problem;traditional laboratory approach;DNA sequencing;gel electrophoresis;nucleotides;biochemical approach;pyrosequencing protocol;heuristic;graph model;genome sequencing;bacteria Prochlorococcus marinus","","","22","","","","","","IEEE","IEEE Conferences"
"COSI: A Framework for the Design of Interconnection Networks","A. Pinto; L. P. Carloni; A. Sangiovanni-Vincentelli","University of California, Berkeley; Columbia University; University of California, Berkeley","IEEE Design & Test of Computers","","2008","25","5","402","415","This article presents a software framework for communication infrastructure synthesis of distributed systems, which is critical for overall system performance in communication-based design. Particular emphasis is given to on-chip interconnect synthesis of multicore designs.","0740-7475;1558-1918","","10.1109/MDT.2008.138","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4648420","","Multiprocessor interconnection networks;Algorithm design and analysis;Network synthesis;Embedded system;Protocols;Design optimization;Unmanned aerial vehicles;Computer networks;Cost function;Space exploration","logic design;multiprocessor interconnection networks;system-on-chip","interconnection networks;COSI;communication infrastructure synthesis;distributed systems;communication based design;on-chip interconnect synthesis;multicore designs","","13","25","","","","","","IEEE","IEEE Journals & Magazines"
"Decentralized Load Balancing for Improving Reliability in Heterogeneous Distributed Systems","J. E. Pezoa; S. Dhakal; M. M. Hayat","NA; NA; NA","2009 International Conference on Parallel Processing Workshops","","2009","","","214","221","A probabilistic analytical framework for decentralized load balancing (LB) strategies for heterogeneous distributed-computing systems (DCSs) is presented with the overall goal of maximizing the service reliability in the presence of random failures. The service reliability of a DCS is defined as the probability of successfully serving a specified workload before all the computing nodes fail permanently. In the framework considered the service and failure times of nodes are random, the communication times in the network are both tangible and stochastic, and LB is performed synchronously by all the nodes during the runtime of each submitted workload. By taking a novel regenerative stochastic-analysis approach, the service reliability of a two-node DCS is characterized analytically. This formulation, in turn, is used to form and solve an optimization problem, yielding LB policies with maximal reliability. A scalable extension of the two-node formulation to an arbitrary size system is also presented. The validity of the proposed theory is studied using both Monte-Carlo simulations and real experiments on a small-scale testbed.","0190-3918;2332-5690","978-1-4244-4923-1978-0-7695-3803","10.1109/ICPPW.2009.50","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5364246","renewal theory;queuing theory;reliability;distributed computing;load balancing","Load management;Distributed control;Telecommunication network reliability;Stochastic processes;Distributed computing;Parallel processing;Runtime;Testing;Reliability theory;Concurrent computing","distributed processing;Monte Carlo methods;probability;queueing theory;resource allocation;software reliability;stochastic processes","decentralized load balancing;heterogeneous distributed systems;service reliability;regenerative stochastic-analysis approach;optimization problem;Monte-Carlo simulations;queuing theory;distributed computing system;probabilistic analytical framework","","4","10","","","","","","IEEE","IEEE Conferences"
"Incremental Workflow Mining with Optional Patterns","W. Sun; T. Li; W. Peng; T. Sun","School of Computer Science, Florida International University, Miami, FL 33199, USA wsun001@cs.fiu.edu; School of Computer Science, Florida International University, Miami, FL 33199, USA taoli@cs.fiu.edu; School of Computer Science, Florida International University, Miami, FL 33199, USA wpeng002@cs.fiu.edu; Adaptive & Smart Document System Lab, Xerox Innovation Group, Xerox Corporation, Webster, NY 14580, USA Tong.Sun@xeroxlabs.com","2006 IEEE International Conference on Systems, Man and Cybernetics","","2006","4","","2764","2771","For today's business organizations, workflow models play important roles in analyzing the productivity, evaluating the performances and costs, optimizing the business operations, and supporting evolving services and products. Workflow mining, the process of empirically extracting structured process descriptions from a set of real executions, thus has attracted a lot of attention recently. However, there are several challenges that have not been fully addressed in the previous research: (i) How can we mine process models with optional tasks? (ii) How can we efficiently use new available workflow log data to incrementally update pre-existing workflow models or to complete previous partial process models? (iii) How can we compare two different workflow models of similar organizations? In this paper, we present our research efforts to address the above challenges. We present a workflow mining algorithm that is able to mine process models with optional tasks and propose an incremental workflow mining algorithm based on intermediate relationships such as ordering and independence. The intermediate relationships can also be used to facilitate the comparison of two process models. We illustrate our algorithms on example data derived from real world applications.","1062-922X","1-4244-0099-61-4244-0100","10.1109/ICSMC.2006.385292","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4274299","","Organizational aspects;Medical tests;Sun;Performance analysis;Productivity;Performance evaluation;Cost function;Cybernetics;Pattern analysis;Data mining","business data processing;data mining;productivity;workflow management software","incremental workflow mining;optional patterns;business organizations;workflow models;productivity analysis;costing;business operation optimization;structured process description extraction;process models;workflow log data;workflow mining algorithm","","","16","","","","","","IEEE","IEEE Conferences"
"New Estimates for Practical Robust Negativity of Multilinear Functions","S. R. Ross; B. R. Barmish","Department of Electrical and Computer Engineering, University of Wisconsin-Madison. sross@cae.wisc.edu; Department of Electrical and Computer Engineering, University of Wisconsin-Madison. barmish@engr.wisc.edu","2007 American Control Conference","","2007","","","5964","5969","In this paper, we address a fundamental NP-hard problem which arises in many application areas: Determine whether a multilinear function f(x) is negative for all x in a hypercube X. Consistent with the emerging literature, with the goal being to reduce the computational burden, we consider a relaxation of this robust negativity problem. Specifically, we seek to determine whether f(x) remains negative for all x in X except possibly on a set of ""acceptably small"" volume.","0743-1619;2378-5861","1-4244-0988-81-4244-0989","10.1109/ACC.2007.4282347","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4282347","","Robustness;Polynomials;Hypercubes;Computational complexity;NP-hard problem;Cities and towns;Robust control;Application software;Neural networks;Testing","computational complexity;optimisation","NP-hard problem;multilinear function;robust negativity problem relaxation","","2","16","","","","","","IEEE","IEEE Conferences"
"Accelerating Multi-scale Image Fusion Algorithms Using CUDA","S. Yoo; J. Park; C. Jeong","NA; NA; NA","2009 International Conference of Soft Computing and Pattern Recognition","","2009","","","278","282","Recently, fusion speed has emerged as an important factor in the image fusion and a substantial amount of memory and computing power are required for a high-speed fusion. This paper shows approaches to accelerate multi-scale image fusion speed on GPU (graphics processing unit) using CUDA (compute unified device architecture). The GPU has evolved into a very powerful and flexible streaming processor, which provides a good computational power and memory bandwidth. We implement the multi-scale image fusion algorithms using CUDA software platform of the latest version of GPU and theirs fusion speeds are compared and evaluated with implementation in Core2 Quad processor with 2.4 GHz. The GPU version achieved a speedup of 6x-8x over the CPU version.","","978-1-4244-5330-6978-0-7695-3879","10.1109/SoCPaR.2009.63","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5370356","image fusion;GPU;CUDA;Pyramid;Wavelet","Acceleration;Image fusion;Design optimization;Printing;Integer linear programming;Constraint optimization;Testing;Containers;Pattern recognition;Computer applications","computer graphics;coprocessors;image fusion","multi-scale image fusion;CUDA;compute unified device architecture;high-speed fusion;graphics processing unit;flexible streaming processor;frequency 2.4 GHz","","4","20","","","","","","IEEE","IEEE Conferences"
"Building RFID Application Based on AOP and IOC Technology","L. Ye","NA","2009 International Conference on Computational Intelligence and Software Engineering","","2009","","","1","4","RFID technology is an automatic identification technology with low cost tags and high read reliability. This paper presents a new framework for building the general FRID application system base on the RFID and J2EE technology. The J2EE and EJB technology is applied to offer a framework to construct enterprise FRID system structure. The ORM technology is used to separate business and storing; the AOP technology is used to focus the system development on business; the IOC technology is used to improve the testing and cleaning method that can optimize the overall accuracy and adjust cleaning costs. The framework could be applied to develop and deploy the customer application based on the RFID technology rapidly with low costs.","","978-1-4244-4507","10.1109/CISE.2009.5366536","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5366536","","Radiofrequency identification;Costs;Management training;Cleaning;Automatic control;Wireless sensor networks;Marine technology;Vehicle driving;Production;Animals","aspect-oriented programming;Java;radiofrequency identification","RFID application;AOP technology;IOC technology;automatic identification technology;J2EE technology;EJB technology;enterprise FRID system structure;ORM technology;system development","","","6","","","","","","IEEE","IEEE Conferences"
"An FPGA Implementation of Decision Tree Classification","R. Narayanan; D. Honbo; G. Memik; A. Choudhary; J. Zambreno","Electrical Engineering and Computer Science, Northwestern University, Evanston, IL 60208, USA, ran310@eecs.northwestern.edu; Electrical Engineering and Computer Science, Northwestern University, Evanston, IL 60208, USA, dkh301@eecs.northwestern.edu; Electrical Engineering and Computer Science, Northwestern University, Evanston, IL 60208, USA, memik@eecs.northwestern.edu; Electrical Engineering and Computer Science, Northwestern University, Evanston, IL 60208, USA, choudhar@eecs.northwestern.edu; Electrical Engineering and Computer Science, Northwestern University, Evanston, IL 60208, USA; Electrical and Computer Engineering, Iowa State University, Ames, IA 50011, USA, zambreno@iastate.edu","2007 Design, Automation & Test in Europe Conference & Exhibition","","2007","","","1","6","Data mining techniques are a rapidly emerging class of applications that have widespread use in several fields. One important problem in data mining is classification, which is the task of assigning objects to one of several predefined categories. Among the several solutions developed, decision tree classification (DTC) is a popular method that yields high accuracy while handling large datasets. However, DTC is a computationally intensive algorithm, and as data sizes increase, its running time can stretch to several hours. In this paper, we propose a hardware implementation of decision tree classification. We identify the compute-intensive kernel (Gini score computation) in the algorithm, and develop a highly efficient architecture, which is further optimized by reordering the computations and by using a bitmapped data structure. Our implementation on a Xilinx Virtex-II Pro FPGA platform (with 16 Gini units) provides up to 5.58times performance improvement over an equivalent software implementation","1530-1591;1558-1101","978-3-9810801-2","10.1109/DATE.2007.364589","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4211794","","Field programmable gate arrays;Decision trees;Classification tree analysis;Data mining;Hardware;Classification algorithms;Computer architecture;Kernel;Acceleration;Computer science","data mining;data structures;decision trees;field programmable gate arrays","FPGA implementation;decision tree classification method;data mining techniques;hardware implementation;compute-intensive kernel;Gini score computation;bitmapped data structure;FPGA platform;software implementation","","22","10","","","","","","IEEE","IEEE Conferences"
"Direct Quadratic Minimization Using Magnetic Field-Based Computing","S. Sarkar; S. Bhanja","NA; NA","2008 IEEE International Workshop on Design and Test of Nano Devices, Circuits and Systems","","2008","","","31","34","We explore an unconventional front in computing,which we call magnetic field-based computing (MFC), that harnesses the energy minimization aspects of a collection of nanomagnets to solve directly quadratic energy minimization problems, such as those arising in computationaolly intensive computer vision tasks. The Hamiltonian of a collection of bipolar nanomagnets is governed by the pairwise dipolar interactions.The ground state of a nanomagnet collection minimizes this Hamiltonian. We have devised a computational method, based on multi-dimensional scaling, to decide upon the spatial arrangement of nanomagnets that matches a particular quadratic minimization problem. Each variable is represented by a nanomagnet and the distances between them are such that the dipolar interactions match the corresponding pairwise energy term in the original optimization problem. We select the nanomagnets that participate in a specific computation from a field of regularly placed nanomagnets. The nanomagnets that do not participate are deselected using transverse magnetic fields. We demonstrate these ideas by solving Landau-Lifshitz equations as implemented in the NISTpsilas micro-magnetic OOMMF software.","","978-0-7695-3379","10.1109/NDCS.2008.13","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4638329","Nanocomputing;Field Coupled Computing;Computer Vision;Nanomagnets","Magnetic resonance imaging;Perpendicular magnetic anisotropy;Magnetic anisotropy;Magnetic domains;Magnetomechanical effects;Arrays;Magnetic moments","computer vision;electrical engineering computing;micromagnetics;minimisation;multidimensional systems","magnetic field-based computing;directly quadratic energy minimization;bipolar nanomagnets;computer vision;multi-dimensional scaling;ground state;Hamiltonian;spatial arrangement;Landau-Lifshitz equations;micro-magnetic OOMMF software","","4","19","","","","","","IEEE","IEEE Conferences"
"Modern State of Algorithms and Systems Complication Theory","M. Cherkaskyy","EOM chair, Lviv Polytechnic National University, S. Bandery str., 12, Lviv, 79046, Ukraine. E-mail: Cherkas2@lvivoline.com","2006 International Conference - Modern Problems of Radio Engineering, Telecommunications, and Computer Science","","2006","","","5","10","The modern state of the applied theory of abstract and computer algorithms and systems is considered. In the first part the state of the theory and the practical use of formal and informal systems of abstract algorithms are shown. The second part is devoted to the theory of computer algorithms. The SH-model, H-model of algorithm and information characteristics of complexity is defined. In the final part the universal SH-model and the optimization ways of its complexity characteristics are described.","","966-553-507","10.1109/TCSET.2006.4404427","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4404427","algorithm;complexity characteristics;Turing machine;SH-model;hardware-software tools;computer","Turing machines;Arithmetic;Birth disorders;History;Testing;Concrete;Crystallization;Modems;Terminology;Logic","Turing machines","systems complication theory;computer algorithms;abstract algorithms;SH model;information characteristics;Turing machine;hardware software tools","","1","22","","","","","","IEEE","IEEE Conferences"
"A Testbed Development Framework for Cognitive Radio Networks","J. Jia; Q. Zhang","NA; NA","2009 IEEE International Conference on Communications","","2009","","","1","5","Cognitive radio networking is a promising approach to fulfill the future's need for intelligent, high-performance communication and improve the efficiency of overall spectrum utilization. Testbed evaluation of protocols and algorithms is a must for the development of cognitive radio networks. In this paper, we present an integrated testbed framework for cognitive radio networks. The framework includes necessary components for cognitive radio operations: flexible RF front end, software define signal processing, adaptive MAC layer and network layer as well as a cross layer management interface. Such a design eases the cross layer configuration and performance optimization of cognitive protocol stack, while retaining the advantage of modularity. We design a testbed for ad-hoc cognitive radio network.","1550-3607;1938-1883","978-1-4244-3435","10.1109/ICC.2009.5198682","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5198682","","Testing;Cognitive radio;Protocols;Signal processing algorithms;Cross layer design;Intelligent networks;Radio frequency;Adaptive signal processing;Radio spectrum management;Optimization","cognitive radio;protocols","cognitive radio networks;testbed development framework;high-performance communication;overall spectrum utilization efficiency;flexible RF front end;software define signal processing;adaptive MAC layer;network layer;cross layer management interface;cognitive protocol","","6","20","","","","","","IEEE","IEEE Conferences"
"Efficient image alignment using linear appearance models","J. Gonzalez-Mora; N. Guil; E. L. Zapata; F. de la Torre","Department of Computer Architecture, University of Malaga, Spain; Department of Computer Architecture, University of Malaga, Spain; Department of Computer Architecture, University of Malaga, Spain; Robotics Institute, Carnegie Mellon University, USA","2009 IEEE Conference on Computer Vision and Pattern Recognition","","2009","","","2230","2237","Visual tracking is a key component in many computer vision applications. Linear subspace techniques (e.g. eigen-tracking) are one of the most popular approaches to align templates with appearance variations (e.g. illumination, iconic changes). A number of well known tracking algorithms have been proposed in the last years to accurately fit these models to images. Computational efficiency is an important limitation in object tracking algorithms and different efficient techniques, such as the “projected-out” optimization, have been proposed. They reduce the computational cost using an efficient formulation in which many of the involved operations can be precomputed. On the other hand, alternative “simultaneous” algorithms jointly optimize pose and appearance parameters, providing better performance but increasing the computational cost. In this paper, we propose an algorithm for efficient linear appearance model fitting based on the inverse compositional simultaneous optimization of pose and appearance. We introduce a novel formulation which reduces the required computational time while maintaining similar convergence properties of previous “simultaneous” approaches. Experimental results illustrate the capabilities of this algorithm in face tracking.","1063-6919","978-1-4244-3992-8978-1-4244-3991","10.1109/CVPR.2009.5206702","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5206702","","Computational efficiency;Lighting;Convergence;Jacobian matrices;Computer architecture;Robot vision systems;Computer vision;Application software;Time factors;Testing","computer vision;object detection","efficient image alignment;visual tracking;computer vision;linear subspace technique;tracking algorithm;computational efficiency;object tracking;linear appearance model fitting;inverse compositional simultaneous optimization","","4","13","","","","","","IEEE","IEEE Conferences"
"Local authentication mechanism for micro mobility in wireless active network environment","Insu Kim; Keecheon Kim","Sch. of Comput. Sci. & Eng., Kon-Kuk Univ., Seoul, South Korea; Sch. of Comput. Sci. & Eng., Kon-Kuk Univ., Seoul, South Korea","2006 8th International Conference Advanced Communication Technology","","2006","2","","5 pp.","1139","By having a local mobility management scheme such as regional registration suggested in IETF, we can perform regional handoffs instead of global handoffs. Regional handoff by micro-mobility techniques shortens the delay in managing mobility in the visited domain since it eliminates the registration process to the home network of mobile nodes, hence it can also decrease the load of HA(home agent). Using the characteristics of the active network, by employing active nodes in the wireless regional mobility management environment, we can optimize the mechanism of registration and path setup for the mobile data traffic. We propose regional authentication mechanism which combines the micro-mobility techniques and active nodes of active access network for wireless environment","","89-5519-129","10.1109/ICACT.2006.206171","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1625776","AAA;Mobile IP;Handoff","Authentication;Intelligent networks;Mobile radio mobility management;IP networks;Home automation;Software testing;Wireless networks;Computer science;Computer network management;Telecommunication traffic","mobility management (mobile radio);radio networks;telecommunication traffic","local authentication mechanism;wireless active network;local mobility management;regional handoffs;mobile data traffic;micro-mobility techniques","","1","9","","","","","","IEEE","IEEE Conferences"
"High multiplicity scheduling of file transfers with divisible sizes on multiple classes of paths","M. I. Andreica; N. Tapus","Polytechnic University of Bucharest, Computer Science Department, Romania; Polytechnic University of Bucharest, Computer Science Department, Romania","2008 IEEE International Symposium on Consumer Electronics","","2008","","","1","4","Distributed applications and services requiring the transfer of large amounts of data have been developed and deployed world wide. The best effort model of the Internet cannot provide these applications with the so much needed quality of service guarantees, making necessary the development of file transfer scheduling techniques, which optimize the usage of network resources. In this paper we consider the high multiplicity scheduling of file transfers over multiple classes of paths with the objective of minimizing the makespan, when the files have divisible sizes. We also consider another objective, that of maximizing the total profit, in the context of some special types of mutual exclusion constraints (tree and clique constraint graphs).","0747-668X;2159-1423","978-1-4244-2422","10.1109/ISCE.2008.4559556","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4559556","file transfer scheduling;divisible sizes;high multiplicity;makespan minimization;greedy;binary search;mutual exclusion;tree;clique;dynamic programming","Optimal scheduling;Scheduling algorithm;Tree graphs;Application software;Quality of service;Polynomials;Minimization methods;Testing;Processor scheduling;Computer science","file organisation;scheduling","high multiplicity scheduling;file transfers;network resources;mutual exclusion constraints;tree constraint graphs;clique constraint graphs)","","3","7","","","","","","IEEE","IEEE Conferences"
"Moodle - The Fingertip Art for Carrying out Distance Education","B. Wu; G. Cheng","NA; NA","2009 First International Workshop on Education Technology and Computer Science","","2009","3","","927","929","The paper discusses six characteristics of moodle to apply in distance education, and then analyzes a specific case to illustrate how to learn through distance education. The significance of this paper is to keep optimizing about moodle application.","","978-0-7695-3557-9978-1-4244-3581","10.1109/ETCS.2009.744","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4959459","moodle;online Learning Style","Art;Distance learning;Resource management;Electronic mail;Knowledge management;Testing;Appraisal;Educational technology;Mathematics;Open source software","computer aided instruction;distance learning","fingertip art;distance education;moodle application","","","8","","","","","","IEEE","IEEE Conferences"
"Design flow for embedded FPGAs based on a flexible architecture template","B. Neumann; T. von Sydow; H. Blume; T. G. Noll","Chair of Electrical Engineering and Computer Systems, RWTH Aachen University, Schinkelstr. 2, 52062 Aachen, Germany. email: neumann@eecs.rwth-aachen.de; Chair of Electrical Engineering and Computer Systems, RWTH Aachen University, Schinkelstr. 2, 52062 Aachen, Germany. email: sydow@eecs.rwth-aachen.de; Chair of Electrical Engineering and Computer Systems, RWTH Aachen University, Schinkelstr. 2, 52062 Aachen, Germany. email: blume@eecs.rwth-aachen.de; Chair of Electrical Engineering and Computer Systems, RWTH Aachen University, Schinkelstr. 2, 52062 Aachen, Germany. email: tgn@eecs.rwth-aachen.de","2008 Design, Automation and Test in Europe","","2008","","","56","61","Modern digital signal processing applications have an increasing demand for computational power while needing to preserve low power dissipation and high flexibility. For many applications, the growth of algorithmic complexity is already faster than the growth of computational power provided by discrete general purpose processors. A typical approach to address this problem is the combination of a processor core with dedicated accelerators. Since changes in standards or algorithms can change the demands on the accelerators, an attractive alternative to highly customised VLSI- macros is the use of reconfigurable embedded FPGAs (eFPGAs). First commercial products combining a general purpose processor core and an embedded FPGA recently emerged (e.g. Stretch S6000 Menta eFPGA- augmented CPUs). For many digital signal processing applications, a significantly improved efficiency in terms of power dissipation, throughput and chip area can be achieved by tailoring both the processor core and the reconfigurable accelerator to the given application domain. In this work, a methodology to design highly customisable eFPGA-architectures starting from a high level description is presented. The design framework elaborated during this work enables a physically optimised VLSI-design of the specified eFPGA and aims to support simulation of the according eFPGA-macros both on a functional and netlist-level by providing an elementary configuration tool based on the same high level description as the eFPGA-architecture.","1530-1591;1558-1101","978-3-9810801-3-1978-3-9810801-4","10.1109/DATE.2008.4484660","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4484660","","Field programmable gate arrays;Computer architecture;Application software;Power dissipation;Signal processing algorithms;Broadcasting;Embedded computing;Power engineering computing;Digital signal processing;Switches","digital signal processing chips;field programmable gate arrays;integrated circuit design;VLSI","embedded FPGA;flexible architecture template;digital signal processing applications;algorithmic complexity;discrete general purpose processors;processor cores;highly customised VLSI-macros;eFPGA-architectures","","5","13","","","","","","IEEE","IEEE Conferences"
"Research on the key technology of three dimension rendering pipeline based on mobile devices","H. Wu","college of computer and information engineering, zhejiang gongshang university, Hangzhou, China","2009 2nd International Conference on Power Electronics and Intelligent Transportation System (PEITS)","","2009","1","","64","67","This paper discussed three-dimensional rendering based on mobile device platform. Three-dimensional rendering pipeline is need to optimize for the limitations of the mobile device processing power. the paper researched and analysed the related technologies of how to improve the pipeline, including JIT back-end compiler technology to accelerate the back-end rasterization processing, anisotropic texture mapping filter for mobile devices and hybrid adaptive normal map texture compression algorithm. This provides the basis for the 3D rendering pipeline based on mobile devices.","","978-1-4244-4544","10.1109/PEITS.2009.5406965","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5406965","3D rendering pipeline;JIT back-end complier;mobile devices","Pipeline processing;Rendering (computer graphics);Graphics;Paper technology;Application software;Testing;Intelligent transportation systems;Mobile computing;Acceleration;Layout","data compression;just-in-time;mobile computing;program compilers;rendering (computer graphics)","three dimension rendering pipeline;mobile device processing power;JIT backend compiler technology;backend rasterization processing;anisotropic texture mapping filter;hybrid adaptive normal map texture compression algorithm","","","24","","","","","","IEEE","IEEE Conferences"
"A Systematics generator","F. Stubbs; J. St.Quinton; K. Grindley","The Systematics Research Group, 74 Hermitage Court, E1W 1PW, UK; The Systematics Research Group, 74 Hermitage Court, E1W 1PW, UK; The Systematics Research Group, 74 Hermitage Court, E1W 1PW, UK","2008 7th IEEE International Conference on Cybernetic Intelligent Systems","","2008","","","1","6","Systematics is a specification language designed for automatic code generation. Systematics uniquely provides auto-provability by virtue of the inclusion of ldquosystem timerdquo as an inherent characteristic of information. A code generator based on Systematics is described which, unlike other code generators, requires no computing expertise on the part of the user, and is not limited to any specific type of generated application. The generator stores the specification in tabular form, in terms of 8 basic concepts. The generated system includes a pre-written ldquoauto-navigationrdquo function which is called to produce outputs on receipt of stimuli. The Systematics generator has initially been implemented in Microsoft Access to provide ldquoProof of Principlerdquo. A generic version of the Systematics Generator is currently being developed by the Systematics Research Group. This version will be independent of any proprietary database product, but with the ability to ldquofront-endrdquo any such product, and will also allow automatic optimisation of the database.","","978-1-4244-2914-1978-1-4244-2915","10.1109/UKRICIS.2008.4798966","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4798966","","Systematics;Databases;Specification languages;Application software;Information systems;Mirrors;Information processing;System testing;Logic programming;Computational modeling","program compilers;specification languages","Systematics generator;specification language;automatic code generation;autoprovability;system time;autonavigation function","","","6","","","","","","IEEE","IEEE Conferences"
"Efficient Code Density Through Look-up Table Compression","T. Bonny; J. Henkel","University of Karlsruhe, Department of Computer Science, Karlsruhe, Germany, bonny@informatik.uni-karlsruhe.de; University of Karlsruhe, Department of Computer Science, Karlsruhe, Germany, henkel@informatik.uni-karlsruhe.de","2007 Design, Automation & Test in Europe Conference & Exhibition","","2007","","","1","6","Code density is a major requirement in embedded system design since it not only reduces the need for the scarce resource memory but also implicitly improves further important design parameters like power consumption and performance. Within this paper we introduce a novel and efficient hardware-supported approach that belongs to the group of statistical compression schemes as it is based on canonical Huffman coding. In particular, our scheme is the first to also compress the necessary Look-up Tables that can become significant in size if the application is large and/or high compression is desired. Our scheme optimizes the number of generated look-up tables to improve the compression ratio. In average, we achieve compression ratios as low as 49% (already including the overhead of the lookup tables). Thereby, our scheme is entirely orthogonal to approaches that take particularities of a certain instruction set architecture into account. We have conducted evaluations using a representative set of applications and have applied it to three major embedded processor architectures, namely ARM, MIPS and PowerPC","1530-1591;1558-1101","978-3-9810801-2","10.1109/DATE.2007.364390","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4211900","","Table lookup;Dictionaries;Embedded system;Energy consumption;Huffman coding;Reduced instruction set computing;Application software;Costs;Read only memory;Statistical analysis","embedded systems;Huffman codes;instruction sets;program processors;table lookup","code density;look up table compression;embedded system design;scarce resource memory;hardware supported approach;statistical compression schemes;Huffman coding;instruction set architecture;embedded processor architectures;ARM;MIPS;PowerPC","","11","17","","","","","","IEEE","IEEE Conferences"
"Register Pointer Architecture for Efficient Embedded Processors","J. Park; S. Park; J. D. Balfour; D. Black-Schaffer; C. Kozyrakis; W. J. Dally","Computer Systems Laboratory, Stanford University, jongsoo@stanford.edu; Computer Systems Laboratory, Stanford University, sbpark84@stanford.edu; Computer Systems Laboratory, Stanford University, jbalfour@stanford.edu; Computer Systems Laboratory, Stanford University, davidbbs@stanford.edu; Computer Systems Laboratory, Stanford University, kozyraki@stanford.edu; Computer Systems Laboratory, Stanford University, dally@stanford.edu","2007 Design, Automation & Test in Europe Conference & Exhibition","","2007","","","1","6","Conventional register file architectures cannot optimally exploit temporal locality in data references due to their limited capacity and static encoding of register addresses in instructions. In conventional embedded architectures, the register file capacity cannot be increased without resorting to longer instruction words. Similarly, loop unrolling is often required to exploit locality in the register file accesses across iterations because naming registers statically is inflexible. Both optimizations lead to significant code size increases, which is undesirable in embedded systems. In this paper, the authors introduce the register pointer architecture (RPA), which allows registers to be accessed indirectly through register pointers. Indirection allows a larger register file to be used without increasing the length of instruction words. Additional register file capacity allows many loads and stores, such as those introduced by spill code, to be eliminated, which improves performance and reduces energy consumption. Moreover, indirection affords additional flexibility in naming registers, which reduces the need to apply loop unrolling in order to maximize reuse of register allocated variables","1530-1591;1558-1101","978-3-9810801-2","10.1109/DATE.2007.364659","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4211864","","Registers;Computer architecture;Microarchitecture;Embedded system;Energy consumption;Arithmetic;Application software;Computer aided instruction;Embedded computing;Laboratories","embedded systems;memory architecture;microprocessor chips;storage allocation","register file capacity;register pointer architecture;embedded processors;static encoding;loop unrolling;spill code","","5","11","","","","","","IEEE","IEEE Conferences"
"Inside Intel® Core microarchitecture","J. Doweck","Intel® Corporation, USA","2006 IEEE Hot Chips 18 Symposium (HCS)","","2006","","","1","35","This article consists of a collection of slides from the author's conference presentation on Intel's Core product line's microarchitecture, a new foundation for Intel architecture-based mobile, desktop, and server processors that incorporates advanced innovations which optimize performance over a range of market segments. Some of the specific topics discussed include: the special features and system specifications of Intel Core; memory management and prefetching capabilities; system performance and flexibility; multithreading capabilities; and a summary of key features and processing facilities.","","978-1-4673-8867","10.1109/HOTCHIPS.2006.7477876","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7477876","","Program processors;Multicore processing;Microarchitecture;Benchmark testing;Multithreading;Memory management;Software design","microprocessor chips;multiprocessing systems;multi-threading;parallel architectures;storage management","Intel Core product line microarchitecture;Intel architecture-based mobile processor;desktop;server processors;system specifications;memory management;prefetching capabilities;system performance;system flexibility;multithreading capabilities","","","","","","","","","IEEE","IEEE Conferences"
"Application of Mean-field Network In Track Correlation","S. Yang; W. Li; B. Tian; C. Wang; S. Meng","NA; NA; NA; NA; NA","2006 8th international Conference on Signal Processing","","2006","1","","","","In a two-node distributed multisensor system, some algorithms of track correlation can be transformed to a generalized classical assignment problem, and the generalized classical assignment is a combined optimization problem. The model of mean-field network is proposed in this paper to solve the problem. The experimental results illustrate that using the neural network to solve the generalized classical assignment problem not only makes the track association correct percent high, but also cannot increase the computing time exponentially with the number of targets","2164-5221;2164-523X","0-7803-9736-30-7803-9737","10.1109/ICOSP.2006.344462","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4128798","","Target tracking;Statistical analysis;Computer networks;Sequential analysis;Equations;Neural networks;Hopfield neural networks;Testing;Application software;Distributed computing","distributed sensors;neural nets;sensor fusion;target tracking","mean-field network;track correlation;two-node distributed multisensor system;neural network","","","8","","","","","","IEEE","IEEE Conferences"
"Secure Fast Handoff Mechanism in Wireless Active Network Environment","I. Kim; K. Kim","NA; NA","Sixth International Conference on Advanced Language Processing and Web Information Technology (ALPIT 2007)","","2007","","","409","413","To reduce signaling load in the mobile IP, mobile nodes do not register their locations with the HA every time they move in same local domain. Instead, they register only when they cross a domain that consists of a number of routers. To register the location a mobile node sends regional registration request to the GFA through AR and receives back the reply from the GFA. After registration, the GFA and other FAs maintain entry for the mobile node in their visitor lists. Using the characteristics of the active network, by employing active nodes in the wireless regional mobility management environment, we can optimize the mechanism of registration and path setup for the mobile data traffic. But, authentication problems exist in local domain. We propose regional authentication mechanism which combines the micro-mobility techniques and active nodes of active access network for wireless environment","","978-0-7695-2930","10.1109/ALPIT.2007.48","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4460676","MobileIPSecurity","Communication system security;Birth disorders;IP networks;Wireless networks;Mobile radio mobility management;Authentication;Information technology;Computer science;Software testing;Costs","","","","1","5","","","","","","IEEE","IEEE Conferences"
"Systematic Methodology for Designing Reconfigurabl D S Modulator Topologies for Multimode Communication Systems","Ying Wei; Hua Tang; A. Doboli","Department of Electrical and Computer Engineering, Stony Brook University, Stony Brook, NY, 11794 ywei@ece.sunysb.edu; NA; NA","Proceedings of the Design Automation & Test in Europe Conference","","2006","1","","1","6","This paper proposes a methodology for designing reconfigurable continuous-time DeltaSigma modulator topologies. The methodology is based on the concept of generic topology that expresses all possible signal paths in a reconfigurable topology. Topologies are optimized for minimizing the complexity of the topologies, maximizing the sharing of circuitry for different modes, maximizing the topology robustness with respect to circuit nonidealities, and minimizing total power consumption. The paper presents a case study for designing topologies for a three mode reconfigurable DeltaSigma modulator, and compares topologies with state-of-the-art design","1530-1591;1558-1101","3-9810801-1","10.1109/DATE.2006.243764","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1656912","","Design methodology;Circuit topology;Communication standards;Bandwidth;Robustness;Energy consumption;Multiaccess communication;Costs;Communication system software;Nonlinear equations","delta-sigma modulation;logic design;network topology;reconfigurable architectures","DeltaSigma modulator;reconfigurable topologies;multimode communication systems;signal paths","","","11","","","","","","IEEE","IEEE Conferences"
"Exploring Innovative Teaching of Engineering Design Course for Industrial Design Students","Y. C. Liu; S. J. Lu","NA; NA","2009 International Conference on Computational Intelligence and Software Engineering","","2009","","","1","5","Teachers have driven to explore new ways to optimize the results they dedicate to their students. The attempt toward developing an innovative teaching was conducted first year at the industrial design department of Chang Gung University, Taiwan. To explore integrated teaching methods for the course of engineering design, students were asked to design, build and test automata (one kind of mechanical toy) through the engineering design process. This paper investigates the course that integrates the study of design methods, engineering design process, subject matter knowledge, and teamwork through the design and development of automata using mechanisms to create movements. Our preliminary observation on the development of such course shows its potential to teach students that have no prior exposure to the vocabulary of engineering and mechanisms with less formal engineering background.","","978-1-4244-4507","10.1109/CISE.2009.5365368","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5365368","","Education;Design engineering;Process design;Design methodology;Gears;Teamwork;Automatic control;Learning automata;Educational technology;Design optimization","design engineering;engineering education;industrial engineering","innovative teaching;engineering design course;industrial design students","","","18","","","","","","IEEE","IEEE Conferences"
"Congestion management in nodal pricing with genetic algorithm","S. M. H. Nabavi; S. Jadid; M. A. S. Masoum; A. Kazemi","Department of Electrical Engineering, Iran University of Science & Technology, Tehran, Iran, 16846 I.R.I. e-mail: H-nabavi@ee.iust.ac.ir; Department of Electrical Engineering, Iran University of Science & Technology, Tehran, Iran; Department of Electrical Engineering, Iran University of Science & Technology, Tehran, Iran; Department of Electrical Engineering, Iran University of Science & Technology, Tehran, Iran","2006 International Conference on Power Electronic, Drives and Energy Systems","","2006","","","1","5","Congestion cost allocation is an important issue in congestion management. This paper presents a genetic algorithm (GA) to determine the optimal generation levels in a deregulated market. The main issue is congestion in lines, which limits transfer capability of a system with available generation capacity. Nodal pricing method is used to determine locational marginal price (LMP) of each generator at each bus. Simulation results based on the proposed GA and the Power World Simulator software are presented and compared for 3-bus and 5-bus test systems.","","0-7803-9771-10-7803-9772","10.1109/PEDES.2006.344301","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4148008","Congestion management;nodal pricing;deregulated power systems;Genetic Algorithm and optimal bidding strategy","Pricing;Genetic algorithms;Power system management;Costs;Power system modeling;Energy management;Power system simulation;Electricity supply industry deregulation;Optimization methods;System testing","genetic algorithms;power markets;pricing","congestion management;genetic algorithm;optimal generation level;deregulated market;nodal pricing method;locational marginal price;Power World Simulator software","","9","21","","","","","","IEEE","IEEE Conferences"
"Active Graph Cuts","O. Juan; Y. Boykov","CERTIS, Ecole Nationale des Ponts et Chaussees Champs-sur-Marne, France; NA","2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)","","2006","1","","1023","1029","This paper adds a number of novel concepts into global s/t cut methods improving their efficiency and making them relevant for a wider class of applications in vision where algorithms should ideally run in real-time. Our new Active Cuts (AC) method can effectively use a good approximate solution (initial cut) that is often available in dynamic, hierarchical, and multi-label optimization problems in vision. In many problems AC works faster than the state-of-the-art max-flow methods [2] even if initial cut is far from the optimal one. Moreover, empirical speed improves several folds when initial cut is spatially close to the optima. Before converging to a global minima, Active Cuts outputs a multitude of intermediate solutions (intermediate cuts) that, for example, can be used be accelerate iterative learning-based methods or to improve visual perception of graph cuts realtime performance when large volumetric data is segmented. Finally, it can also be combined with many previous methods for accelerating graph cuts.","1063-6919","0-7695-2597","10.1109/CVPR.2006.47","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1640863","","Application software;Acceleration;Visual perception;Costs;Image segmentation;Computer vision;Testing;Computer science;Optimization methods;Iterative methods","","","","45","15","","","","","","IEEE","IEEE Conferences"
"The MADERE radio-activity measurement platform: Developments for a better addressing to the experimental needs.","J. Girard; H. Philibert; S. Testanière; C. Domergue; D. Beretz","Commissariat à l'Energie Atomique, DEN/CADARACHE/DEN/DER/SPEx/LDCI - CEA Cadarache, F13108 Saint Paul Lez Durance - France; Commissariat à l'Energie Atomique, DEN/CADARACHE/DEN/DER/SPEx/LDCI - CEA Cadarache, F13108 Saint Paul Lez Durance - France; Commissariat à l'Energie Atomique, DEN/CADARACHE/DEN/DER/SPEx/LDCI - CEA Cadarache, F13108 Saint Paul Lez Durance - France; Commissariat à l'Energie Atomique, DEN/CADARACHE/DEN/DER/SPEx/LDCI - CEA Cadarache, F13108 Saint Paul Lez Durance - France; Commissariat à l'Energie Atomique, DEN/CADARACHE/DEN/DER/SPEx/LDCI - CEA Cadarache, F13108 Saint Paul Lez Durance - France","2009 1st International Conference on Advancements in Nuclear Instrumentation, Measurement Methods and their Applications","","2009","","","1","8","The main goal of the Reactor Dosimetry is to provide information (reaction rates, fluence, fluence rate...) for the interpretation of experiments irradiated in critical mock-up, test reactors or power nuclear reactors. Various techniques are used, including analysis of irradiated activation or fission dosimeters whose radioactivity is measured afterwards. The MADERE platform (Measurement Applied to DosimEtry for REactors) is a CEA facility which is dedicated to the activation dosimeters manufacturing and their activity measurement after irradiation in a nuclear reactor. The laboratory is accredited by the French Accreditation Committee for specific activity measurements of solid samples using gamma and X-rays spectrometry. The choice of dosimeters takes into account limitations coming from the characteristics of the measurement devices. To meet experimenter's new demands, the MADERE platform set out to improve its offer by lowering the energy of measured radiations down to 10 keV, and the activity level down to the tenth of Becquerel (Bq). Doing so, the range of usable dosimeters and by the way, the energy range of the neutron spectrum is expanded. Dosimeter, wires or foils, few millimeters large, are manufactured using ultra-pure material (Gold, Iron, Nickel,...). Some of them are encapsulated in quartz containers for integration into experimental devices. In order to determine the activity of irradiated metal solid samples, the MADERE platform makes use of several measurement equipments: (1) Three Gamma spectrometry devices equipped with semiconductor diode detectors HPGe set to work on an energy field ranging from 50 keV to 2 MeV. The range of activity covered by these devices spreads from a few tenths of Bq to a few tens of MBq. The ability to measure samples with activities of the order of one tenth of Bq may be used to implement a nondestructive determination of the isotopic composition of lowactive objects. (2) One X-ray spectrometry device equipped with a LEGE (Low Energy GErmanium) GeHP semiconductor detector. The settings of this equipment have been made to work in an energy range from 10 keV to 300 keV. This range allows covering almost all the X energy domain and also easily measuring low energy gamma emitters. For a long time, this device has been exclusively used for relative activity measurements of irradiated niobium using a<sup>93</sup>Nb<sup>m</sup>reference sample (17 keV Xk emitter). This direct comparison measurement method allows getting free from the determination of different measurement parameters, since they are considered proportional between the reference and the measured samples, under the hypothesis that geometry, activity level and measurement conditions are identical. A need has emerged for measuring the 103Rhm activity. It would have required the use of a reference 103Rhm source which is in fact excluded due to the short half-life (56 minutes) of this radionuclide. It was therefore decided to establish the total absorption yield curve in order to perform absolute activity measurement. (1) One precise micro-balance allowing the measurement of sample mass with an accuracy of few micrograms. (2) One fissile measurement device equipped with 4 Nal detectors associated to an automated samples holder. The quality of gamma spectrometry measurements mainly depends on the knowledge of the detection efficiency yield associated to the Source-Detector system geometry. Thus, the MADERE platform uses twenty different calibrated measurement geometries. In order to optimize the time spent on the experimental determination of each associated detection yield, a modeling work is underway, using the ETNA software, developed by the Henri Becquerel French National Primary Laboratory, to derived transfer efficiency and correction coincidence parameters. The precise determination of the sample activity also requires the use of correction factors related to the effects of self absorption, sum peak and geometry. Thus, the MADERE platform has developed tools using its large experimental database which allow the accurate determination of these correction factors to be applied to the sample measurements. Finally, in order to meet the various requirements for the monitoring of the total stored activity in the facility, but also for the traceability of the measurement results and of all the elements contributing to the elaboration of the results provided to its customers, the MADERE platform has developed computerized management tools that maximize its productivity.","","978-1-4244-5207-1978-1-4244-5208","10.1109/ANIMMA.2009.5503714","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5503714","CEA - Radioactivity measurement;photons;X-rays spectrometry;gamma Spectrometry;Reactor dosimetry","Spectroscopy;Geometry;Inductors;Energy measurement;Nuclear measurements;X-rays;Dosimetry;Testing;Gamma ray detection;Gamma ray detectors","dosimetry;gamma-ray spectra;germanium radiation detectors;particle spectrometers;radioactivity measurement;X-ray spectra","MADERE radioactivity measurement;reactor dosimetry;reaction rates;fluence rate;critical mock-up test reactors;power nuclear reactors;irradiated activation;fission dosimeters;Measurement Applied to DosimEtry for REactors;CEA facility;French Accreditation Committee;gamma spectrometry;Three Gamma spectrometry;One X-ray spectrometry;LEGE;Low Energy Germanium GeHP semiconductor detector;Nal detectors;Source-Detector system geometry;ETNA software;Henri Becquerel French National Primary Laboratory","","1","8","","","","","","IEEE","IEEE Conferences"
"A fast quasi-dense matching algorithm with an adaptive window","J. Zhao; J. Chai; G. Men","College of Electronic and Information Engineering, Hebei University, Baoding 071002, China; College of Electronic and Information Engineering, Hebei University, Baoding 071002, China; College of Economics, Hebei University, Baoding 071002, China","2009 Chinese Control and Decision Conference","","2009","","","4375","4379","This paper presents a fast quasi-dense matching algorithm using an adaptive window. The algorithm starts from a set of sparse seed matches, then propagates to the neighboring pixels, finally the most points in the images are matched. During matching, we apply the convolution to the normalized cross correlation (NCC).The confidence coefficient is introduced and the search window is varied in inverse proportion to it. The algorithm has been tested with stereo images and the results demonstrate its accuracy and efficiency. The algorithm also can be applied to a wide range of image pairs including those with large disparity or without rectification even if part of the images are less textured. In particular, with big images and a large disparity range our algorithm turns out to be significantly faster.","1948-9439;1948-9447","978-1-4244-2722-2978-1-4244-2723","10.1109/CCDC.2009.5192404","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5192404","Normalized Cross Correlation;Convolution;Confidence Coefficient;Adaptive Window and Quasi-Dense Matching","Convolution;Stereo vision;Pixel;Educational institutions;Testing;Computer vision;Application software;Optimization methods;Statistical analysis;Gain","convolution;image matching;image texture;search problems;stereo image processing","fast quasidense matching algorithm;adaptive window;sparse seed matches;image matching;convolution;normalized cross correlation;confidence coefficient;search window;inverse proportion;stereo images;image texture","","","15","","","","","","IEEE","IEEE Conferences"
"On Conditional Branches in Optimal Decision Trees","M. B. Baer","Electronics for Imaging, 303 Velocity Way, Foster City, California 94404 USA. Email: Michael.Baer@efi.com","2007 IEEE International Symposium on Information Theory","","2007","","","436","440","The decision tree is one of the most fundamental programming abstractions. A commonly used type of decision tree is the alphabetic binary tree, which uses (without loss of generality) ""less than"" versus ""greater than or equal to"" tests in order to determine one of n outcome events. The process of finding an optimal alphabetic binary tree for a known probability distribution on outcome events usually has the underlying assumption that the cost (time) per decision is uniform and thus independent of the outcome of the decision. This assumption, however, is incorrect in the case of software to be optimized for a given microprocessor, e.g., in compiling switch statements or in fine-tuning program bottlenecks. The operation of the microprocessor generally means that the cost for the more likely decision outcome can or will be less - often far less -than the less likely decision outcome. Here we formulate a variety of O(n<sup>3</sup>)-time O(n<sup>2</sup>)-space dynamic programming algorithms to solve such optimal binary decision tree problems, optimizing for the behavior of processors with predictive branch capabilities, both static and dynamic. In the static case, we use existing results to arrive at entropy-based performance bounds. Solutions to this formulation are often faster in practice than "";optimal""; decision trees as formulated in the literature, and, for small problems, are easily worth the extra complexity in finding the better solution. This can be applied in fast implementation of decoding Huffman codes.","2157-8095;2157-8117","978-1-4244-1397-3978-1-4244-1429","10.1109/ISIT.2007.4557264","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4557264","","Decision trees;Binary trees;Microprocessors;Switches;Dynamic programming;Testing;Probability distribution;Cost function;Heuristic algorithms;Decoding","computational complexity;decision trees;dynamic programming;microprocessor chips;optimising compilers;statistical distributions;trees (mathematics)","optimal alphabetic decision trees;conditional branch;fundamental programming abstraction;probability distribution;microprocessor operation;space dynamic programming algorithm;Huffman codes;decoding","","","30","","","","","","IEEE","IEEE Conferences"
"Implementing a portable Multi-threaded Graph Library: The MTGL on Qthreads","B. W. Barrett; J. W. Berry; R. C. Murphy; K. B. Wheeler","Sandia National Laboratories, Albuquerque, NM, USA; Sandia National Laboratories, Albuquerque, NM, USA; Sandia National Laboratories, Albuquerque, NM, USA; University of Notre Dame and Sandia National Laboraroties, IN, USA","2009 IEEE International Symposium on Parallel & Distributed Processing","","2009","","","1","8","Developing multi-threaded graph algorithms, even when using the MTGL infrastructure, provides a number of challenges, including discovering appropriate levels of parallelism, preventing memory hot spotting, and eliminating accidental synchronization. In this paper, we have demonstrated that using the combination of Qthreads and MTGL with commodity processors enables the development and testing of algorithms without the expense and complexity of a Cray XMT. While achievable performance is lower for both the Opteron and Niagara platform, performance issues are similar. While we believe it is possible to port Qthreads to the Cray XMT, this work is still on-going. Therefore, porting work still must be done to move algorithm implementations between commodity processors and the XMT. Although it is likely that the Qthreads-version of an algorithm will not be as optimized as a natively implemented version of the algorithm, such a performance impact may be an acceptable trade-off for ease of implementation.","1530-2075","978-1-4244-3751-1978-1-4244-3750","10.1109/IPDPS.2009.5161102","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5161102","","Libraries;Sockets;Yarn;Sun;Informatics;Hardware;Scalability;Laboratories;Program processors;Programming profession","multi-threading;software libraries;software portability;synchronisation","portable multithreaded graph library;MTGL;Qthreads;parallelism;memory hot spotting prevention;accidental synchronization;commodity processors;Opteron;Niagara","","11","16","","","","","","IEEE","IEEE Conferences"
"UAV as a Reliable Wingman: A Flight Demonstration","S. Waydo; J. Hauser; R. Bailey; E. Klavins; R. M. Murray","NA; NA; NA; NA; NA","IEEE Transactions on Control Systems Technology","","2007","15","4","680","688","In this brief, we present the results from a flight experiment demonstrating two significant advances in software enabled control: optimization-based control using real-time trajectory generation and logical programming environments for formal analysis of control software. Our demonstration platform consisted of a human-piloted F-15 jet flying together with an autonomous T-33 jet. We describe the behavior of the system in two scenarios. In the first, nominal state communications were present and the autonomous aircraft maintained formation as the human pilot flew maneuvers. In the second, we imposed the loss of high-rate communications and demonstrated an autonomous safe ""lost wingman"" procedure to increase separation and reacquire contact. The flight demonstration included both a nominal formation flight component and an execution of the lost wingman scenario.","1063-6536;1558-0865;2374-0159","","10.1109/TCST.2007.899172","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4252108","Aerospace control;command and control systems;cooperative systems;fault tolerance;formal languages;optimal control","Unmanned aerial vehicles;Aircraft;Control systems;Communication system control;Aerospace electronics;Programming environments;Protocols;Optimal control;Testing;Aerospace simulation","aircraft control;control engineering computing;jets;logic programming;optimal control;position control;remotely operated vehicles","optimal control;fault tolerance;cooperative system;command and control system;aerospace control;autonomous safe lost wingman;high-rate communication;autonomous aircraft;nominal state communication;autonomous T-33 jet;human-piloted F-15 jet;control software;formal analysis;logical programming environment;real-time trajectory;optimization-based control;flight demonstration;reliable wingman;unmanned air vehicle","","13","24","","","","","","IEEE","IEEE Journals & Magazines"
"Detecting Anomalies by Benford's Law","Z. Jasak; L. Banjanovic-Mehmedovic","Internal auditor, NLB Tuzlanska banka d.d., Tuzla, Bosnia and Herzegovina; University of Tuzla, Faculty of Electrical Engineering, Tuzla, Bosnia and Herzegovina","2008 IEEE International Symposium on Signal Processing and Information Technology","","2008","","","453","458","Benford's law gives expected patterns of the digits in numerical data. It can be used as a tool to detect outliers for example in as a test for the authenticity and reliability of transaction level accounting data. Based on Benford's law tests for first two digits, first three digits, last digits, last digit, last two digits have been derived as an additional analytical tool. Benford's law is known as a 'first digit law', 'digit analysis' or 'Benford-Newcomb phenomenon'. The second order test is an analysis of the digit frequencies of the differences between the ordered (ranked) values in a data set. The digit frequencies of these differences approximates the frequencies of Benford's law for most distributions of the original data. From some auditor's point of view it is very important if it is possible to use Benford's law to trace siginificant changes in data in some period of time or detect some trends within them.","2162-7843","978-1-4244-3554-8978-1-4244-3555","10.1109/ISSPIT.2008.4775660","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4775660","Benford's law;digit analisys;detecting frauds;detecting anomalies","Frequency;Testing;Libraries","security of data;software tools","anomaly detection;Benford law;numerical data;outlier detection;transaction level accounting data;first digit law;digit analysis;Benford-Newcomb phenomenon","","","35","","","","","","IEEE","IEEE Conferences"
"Predictive modeling of therapy response in multiple sclerosis using gene expression data","S. Mostafavi; S. Baranzini; J. Oksernberg; P. Mousavi","School of Computing, Queen's University, Kingston, ON, K7L 3N6, Canada. e-mail: sara@cs.queensu.ca; Department of Neurology, University of California, San Francisco; Department of Neurology, University of California, San Francisco.; School of Computing, Queen's University, Kingston, ON, K7L 3N6, Canada. phone: (613)533-6070, e-mail: pmousavi@cs.queensu.ca","2006 International Conference of the IEEE Engineering in Medicine and Biology Society","","2006","","","5519","5522","Transcription profiling studies reveal important insights in regards to molecular events that manifest in phenotypic outcomes such as response to drug therapy. Construction of computational models that accurately predict therapy response is only possible when precise data measurements, robust feature/gene selection, and advanced computational modeling methods are combined with stringent statistical validation and large scale verification of results. Due to the large number of gene expression measurements in transcriptional profiling studies, feature selection represents a bottleneck when constructing computational models. The degree of compromise between selection of the optimal feature set and computational efficiency results in many choices for candidate gene sets which leads to a wide range of classification accuracies. Furthermore, constructing a classification model using a larger-than-necessary gene set along with small number of samples may cause over-fitting the data, resulting in highly optimistic classification accuracies. In this study we present OSeMA, a fast, robust and accurate gene selection-classification framework which results in construction of classification models that are highly predictive of the rIFNB therapy response in multiple sclerosis patients. We assess the performance of OSeMA on held out test data. Additionally, we extensively evaluate OSeMA by comparing it to an exhaustive combinatorial gene selection-classification approach","1557-170X","1-4244-0032-51-4244-0033","10.1109/IEMBS.2006.259681","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4463055","","Predictive models;Medical treatment;Multiple sclerosis;Gene expression;Computational modeling;Robustness;Drugs;Large-scale systems;Computational efficiency;Testing","diseases;feature extraction;genetics;medical computing;molecular biophysics;patient treatment;pattern classification","predictive modeling;rIFNB therapy response;multiple sclerosis patients;gene expression data;transcription profiling;computational modeling methods;feature selection;orthogonal search model analysis;OSeMA;gene selection-classification framework","Algorithms;Artificial Intelligence;Computational Biology;Data Interpretation, Statistical;Gene Expression;Gene Expression Profiling;Gene Expression Regulation;Humans;Multiple Sclerosis;Normal Distribution;Oligonucleotide Array Sequence Analysis;Pattern Recognition, Automated;Reproducibility of Results;Software","2","15","","","","","","IEEE","IEEE Conferences"
"On Relaxing and Steady State Genetic Methods for Satellite Imaging Scheduling","J. Xiao-shan; L. Jun; J. Ning","NA; NA; NA","2008 Ninth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing","","2008","","","141","146","Satellite imaging scheduling problem with energy and memory limit belongs to NP-hard problems. As the mathematical programming model is built, a Lagrangian relaxation method can provide tight upper bound, using a max-weighted path algorithm in the constraint graph and a sub-gradient optimizing method to explore the minimum upper bound. Then, permutation based stochastic local search algorithms are used to optimize imaging scheduling results. Finally, the testing results demonstrate the efficiency of these methods.","","978-0-7695-3263","10.1109/SNPD.2008.108","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4617362","","Imaging;Upper bound;Schedules;Optimal scheduling;Satellites;Batteries;Scheduling","gradient methods;imaging;mathematical programming;satellite communication;scheduling;search problems;stochastic processes;telecommunication network management","steady state genetic methods;satellite imaging scheduling problem;NP-hard problems;mathematical programming model;Lagrangian relaxation method;constraint graph;subgradient optimizing method;permutation;stochastic local search algorithms","","1","10","","","","","","IEEE","IEEE Conferences"
"Determinants of Advance Planning and Scheduling Systems Adoption","P. Hadaya; R. Pellerin","NA; NA","2008 The Third International Conference on Software Engineering Advances","","2008","","","494","499","This paper measures the relative influence of a set of determinants on firms' adoption of advanced planning and scheduling systems (APS). Our theoretical model was tested on data collected from 61 senior managers in the wireless communication sector. Our findings confirm the importance to consider the characteristics of business relationships when deciding to adopt an APS system and demonstrate that adaptive planning and optimization systems are generally implemented in tightly coupled supply networks. Findings also confirm that manufacturing firms and retailers generally implement an ERP system before deciding to exploit the unique capabilities of an APS to optimize their activities with their upstream supply chain partners.","","978-1-4244-3218-9978-0-7695-3372","10.1109/ICSEA.2008.31","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4668151","Advanced Planning and Scheduling;determinants","Enterprise resource planning;Technological innovation;Job shop scheduling;Business;Supply chains;Information systems;Production planning;Information technology;Companies;Control systems","enterprise resource planning;manufacturing resources planning;supply chain management","advance planning and scheduling systems;wireless communication sector;business relationships;optimization systems;manufacturing firms;retailers;ERP system;upstream supply chain partners","","2","44","","","","","","IEEE","IEEE Conferences"
"The Graphics Processor as a Mathematical Coprocessor in MATLAB","A. R. Brodtkorb","NA","2008 International Conference on Complex, Intelligent and Software Intensive Systems","","2008","","","822","827","We present an interface to the graphics processing unit (GPU) from MATLAB, and four algorithms from numerical linear algebra available through this interface; matrix-matrix multiplication, Gauss-Jordan elimination, PLU factorization, and tridiagonal Gaussian elimination. In addition to being a high level abstraction to the GPU, the interface offers background processing, enabling computations to be executed on the CPU simultaneously. The algorithms are shown to be up-to 31 times faster than highly optimized CPU code. The algorithms have only been tested on single precision hardware, but will easily run on new double precision hardware.","","978-0-7695-3109","10.1109/CISIS.2008.68","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4606774","GPU;MATLAB;Numerical Linear Algebra;Background Computation","MATLAB;Least squares approximation;Hardware;Benchmark testing;Pixel;Graphics;Linear algebra","coprocessors;linear algebra;mathematics computing","graphics processor;mathematical coprocessor;MATLAB;graphics processing unit;numerical linear algebra;matrix-matrix multiplication;Gauss-Jordan elimination;PLU factorization;tridiagonal Gaussian elimination;high- level abstraction","","1","31","","","","","","IEEE","IEEE Conferences"
"A Scratch-Pad Memory Aware Dynamic Loop Scheduling Algorithm","O. Ozturk; M. Kandemir; S. H. K. Narayanan","NA; NA; NA","9th International Symposium on Quality Electronic Design (isqed 2008)","","2008","","","738","743","Executing array based applications on a chip multiprocessor requires effective loop parallelization techniques. One of the critical issues that need to be tackled by an optimizing compiler in this context is loop scheduling, which distributes the iterations of a loop to be executed in parallel across the available processors. Most of the existing work in this area targets cache based execution platforms. In comparison, this paper proposes the first dynamic loop scheduler, to our knowledge, that targets scratch-pad memory (SPM) based chip multiprocessors, and presents an experimental evaluation of it. The main idea behind our approach is to identify the set of loop iterations that access the SPM and those that do not. This information is exploited at runtime to balance the loads of the processors involved in executing the loop nest at hand. Therefore, the proposed dynamic scheduler takes advantage of the SPM in performing the loop iteration-to-processor mapping. Our experimental evaluation with eight array/loop intensive applications reveals that the proposed scheduler is very effective in practice and brings between 13.7% and 41.7% performance savings over a static loop scheduling scheme, which is also tested in our experiments.","1948-3287;1948-3295","978-0-7695-3117","10.1109/ISQED.2008.4479830","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4479830","Scheduling and Partitioning;Compilers;Multiprocessor Systems","Scheduling algorithm;Dynamic scheduling;Processor scheduling;Scanning probe microscopy;Runtime;Delay;Application software;Testing;Memory management;Algorithm design and analysis","dynamic scheduling;iterative methods;program compilers","dynamic loop scheduling;executing array;chip multiprocessor;loop parallelization;optimizing compiler;scratch pad memory;loop iteration to processor mapping","","5","21","","","","","","IEEE","IEEE Conferences"
"Prediction-Directed Compression of POMDPs","A. Boularias; M. Izadi; B. Chaib-draa","NA; NA; NA","2008 Seventh International Conference on Machine Learning and Applications","","2008","","","99","105","High dimensionality of belief space in partially observable Markov decision processes (POMDPs) is one of the major causes that severely restricts the applicability of this model. Previous studies have demonstrated that the dimensionality of a POMDP can eventually be reduced by transforming it into an equivalent predictive state representation (PSR). In this paper, we address the problem of finding an approximate and compact PSR model corresponding to a given POMDP model. We formulate this problem in an optimization framework. Our algorithm tries to minimize the potential error that missing some core tests may cause. We also present an empirical evaluation on benchmark problems, illustrating the performance of this approach.","","978-0-7695-3495","10.1109/ICMLA.2008.115","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724961","POMDPs;PSRs;compression;online planning","Computer science;Machine learning;Application software;Predictive models;Benchmark testing;State-space methods;Approximation algorithms;Uncertainty;History;Probability distribution","decision theory;error statistics;Markov processes;minimisation;multi-agent systems","prediction-directed compression;POMDP;high dimensionality;belief space;partially observable Markov decision process;predictive state representation;optimization;error minimization;multiagent system","","","8","","","","","","IEEE","IEEE Conferences"
"Enhancements to MatlabMPI: Easier Compilation, Collective Communication, and Profiling","J. Gardiner; J. Nehrbass; J. C. Chaves; B. Guilfoos; S. Ahalt; A. Krishnamurthy; J. Unpingco; A. Chalker; S. Samsi","Ohio Supercomputer Center, Columbus, OH; Ohio Supercomputer Center, Columbus, OH; Ohio Supercomputer Center, Columbus, OH; Ohio Supercomputer Center, Columbus, OH; Ohio Supercomputer Center, Columbus, OH; Ohio Supercomputer Center, Columbus, OH; Ohio Supercomputer Center, Columbus, OH; Ohio Supercomputer Center, Columbus, OH; Ohio Supercomputer Center, Columbus, OH","2006 HPCMP Users Group Conference (HPCMP-UGC'06)","","2006","","","435","439","This paper provides a brief overview of several enhancements made to the MatlabMPI suite. MatlabMPI is a pure MATLAB code implementation of the core parts of the MPI specifications. The enhancements provide a more attractive option for HPCMP users to design parallel MATLAB code. Intelligent compiler configuration tools have also been delivered to further isolate MatlabMPI users from the complexities of the UNIX environments on the various HPCMP systems. Users are now able to install and use MatlabMPI with less difficulty, greater flexibility, and increased portability. Collective communication functions were added to MatlabMPI to expand functionality beyond the core implementation. Profiling capabilities, producing TAU (tuning and analysis utility) trace files, are now offered to support parallel code optimization. All of these enhancements have been tested and documented on a variety of HPCMP systems. All material, including commented example code to demonstrate the usefulness of MatlabMPI, is available by contacting the authors","","0-7695-2797","10.1109/HPCMP-UGC.2006.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4134093","","Computer languages;MATLAB;Prototypes;High performance computing;Workstations;Computer science;Productivity;Software tools;Supercomputers;System testing","application program interfaces;mathematics computing;message passing;parallel programming;program compilers","MatlabMPI;High Performance Computing Modernization Program;intelligent compiler configuration tools;collective communication;tuning utility;analysis utility;parallel code optimization","","1","6","","","","","","IEEE","IEEE Conferences"
"Approximate Nonmyopic Sensor Selection via Submodularity and Partitioning","W. Liao; Q. Ji; W. A. Wallace","NA; NA; NA","IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans","","2009","39","4","782","794","As sensors become more complex and prevalent, they present their own issues of cost effectiveness and timeliness. It becomes increasingly important to select sensor sets that provide the most information at the least cost and in the most timely and efficient manner. Two typical sensor selection problems appear in a wide range of applications. The first type involves selecting a sensor set that provides the maximum information gain within a budget limit. The other type involves selecting a sensor set that optimizes the tradeoff between information gain and cost. Unfortunately, both require extensive computations due to the exponential search space of sensor subsets. This paper proposes efficient sensor selection algorithms for solving both of these sensor selection problems. The relationships between the sensors and the hypotheses that the sensors aim to assess are modeled with Bayesian networks, and the information gain (benefit) of the sensors with respect to the hypotheses is evaluated by mutual information. We first prove that mutual information is a submodular function in a relaxed condition, which provides theoretical support for the proposed algorithms. For the budget-limit case, we introduce a greedy algorithm that has a constant factor of (1 - 1/<i>e</i>) guarantee to the optimal performance. A partitioning procedure is proposed to improve the computational efficiency of the algorithms by efficiently computing mutual information as well as reducing the search space. For the optimal-tradeoff case, a submodular-supermodular procedure is exploited in the proposed algorithm to choose the sensor set that achieves the optimal tradeoff between the benefit and cost in a polynomial-time complexity.","1083-4427;1558-2426","","10.1109/TSMCA.2009.2014168","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4803774","Active fusion;Bayesian networks (BNs);sensor selection;submodular function","Cost function;Mutual information;Partitioning algorithms;Bayesian methods;Systems engineering and theory;Medical tests;System testing;Military computing;Sensor systems and applications;Application software","belief networks;computational complexity;greedy algorithms;sensor fusion","sensor selection;partitioning procedure;maximum information gain;Bayesian network;mutual information;budget-limit case;greedy algorithm;computational efficiency;optimal-tradeoff case;submodular-supermodular procedure;polynomial-time complexity;active fusion","","4","49","","","","","","IEEE","IEEE Journals & Magazines"
"A Waveguide NbTiN SIS Mixer for THz Array Applications","A. M. Baryshev; F. P. Mena; R. Hesper; T. Zijlstra; C. F. J. Lodewijk; W. Wild; T. M. Klapwijk","SRON and Kapteyn Astronomical Institute, Landleven 12, 9747 AD Groningen, The Netherlands. andrey@sron.rug.nl; SRON and Kapteyn Astronomical Institute, Landleven 12, 9747 AD Groningen, The Netherlands.; SRON and Kapteyn Astronomical Institute, Landleven 12, 9747 AD Groningen, The Netherlands.; Kavli Institute of Nanoscience, Delft University of Technology, Lorentzweg 1, 2628 CJ Delft, The Netherlands.; Kavli Institute of Nanoscience, Delft University of Technology, Lorentzweg 1, 2628 CJ Delft, The Netherlands.; SRON and Kapteyn Astronomical Institute, Landleven 12, 9747 AD Groningen, The Netherlands.; Kavli Institute of Nanoscience, Delft University of Technology, Lorentzweg 1, 2628 CJ Delft, The Netherlands.","2006 Joint 31st International Conference on Infrared Millimeter Waves and 14th International Conference on Teraherz Electronics","","2006","","","392","392","In this paper we will present the design and realization of a double sideband SIS heterodyne mixer for the frequency range of 780-950 GHz. A NbTiN high Tc superconductor in combination with aluminum is used for a SIS junction tuning circuit. We use a double side waveguide probe with a fixed backshort tuner design which was optimized by means of a 3D EM-modeling software. Compact mechanical envelope of the mixer allows for a combination of several units in a focal plane array. We will present results of instantaneous bandwidth measurement for seven mixer units and compare it with theoretical predictions. We also present a heterodyne sensitivity test results for these mixers with the best achieved DSB mixer noise temperature of about 250 K (physical temperature 4.2 K) which is only several times higher than a quantum noise limit for these frequencies.","2162-2027;2162-2035","1-4244-0399-51-4244-0400","10.1109/ICIMW.2006.368600","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4222334","","Frequency;Superconducting device noise;Temperature sensors;High temperature superconductors;Aluminum;Circuit optimization;Probes;Tuners;Design optimization;Bandwidth","focal planes;heterodyne detection;mixers (circuits);superconductor-insulator-superconductor mixers","waveguide mixer;THz array applications;double sideband SIS heterodyne mixer;high Tc superconductor;SIS junction tuning circuit;fixed backshort tuner design;frequency 780 GHz to 950 GHz;NbTiN","","1","3","","","","","","IEEE","IEEE Conferences"
"Collaborative group membership and access control for JXTA","","","2008 3rd International Conference on Communication Systems Software and Middleware and Workshops (COMSWARE '08)","","2008","","","159","166","This paper presents a proposal for group membership and access control services for JXTA, both based on the principle of self-organization and collaboration of peer group members. The need for collaboration strengthens the resistance against free riding and eases management of revocation data. The proposal prioritizes group autonomy and makes use of the concepts of web of trust and small world phenomenon in order to achieve its ends, distancing itself from approaches based on centralized PKI models or trusted third parties external to the group. It also offers an alternative to the basic group membership services distributed with the JXTA platform implementations.","","978-1-4244-1796-4978-1-4244-1797","10.1109/COMSWA.2008.4554399","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4554399","Access control;peer-to-peer;security;group membership;JXTA;distributed systems;web of trust;Java","Collaboration;Access control;Authentication;Proposals;Peer to peer computing;Guidelines;Access protocols;Identity management systems;Graphical user interfaces;Testing","authorisation;formal specification;groupware;peer-to-peer computing;public key cryptography","collaborative group membership;access control;JXTA specification framework;peer group members;group autonomy;centralized PKI models","","1","16","","","","","","IEEE","IEEE Conferences"
"Automatic generation of supervision system based on bond graph tool","B. O. Bouamama","LAGIS, UMR CNRS8146, Cité Scientifique, Polytech¿Lille, F59655 Villeneuve d¿Ascq Cedex, France","2008 5th International Symposium on Mechatronics and Its Applications","","2008","","","1","5","The paper deals with an integrated approach for supervision systems design based on a mechatronic properties of a bond graph tool. The methodology is implemented in a specific software which automatically creates complex process dynamic models from a simple graphical interface, where system components can be dragged from a component data base and interconnected so as to produce the overall system, following the Piping and Instrumentation Diagram. Once the model has been created, the software checks its consistency and performs its structural analysis in order to automatically determine the diagnosis algorithms which should be implemented, and their fault detectability and isolability performances. The friendly graphical user interface allows to test several sensor configurations in order to optimise the diagnostic possibilities. The obtained model can also be used for the simulation of the process and its diagnosis algorithms in normal and faulty situations.","","978-1-4244-2033-9978-1-4244-2034","10.1109/ISMA.2008.4648812","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4648812","FDI;Software;Bond graph;process engineering","","graph theory;graphical user interfaces;mechatronics;production engineering computing;supervisory programs","supervision system;bond graph tool;mechatronic;piping diagram;instrumentation diagram;graphical user interface","","","13","","","","","","IEEE","IEEE Conferences"
"An AVS HDTV video decoder architecture employing efficient HW/SW partitioning","H. Jia; P. Zhang; D. Xie; W. Gao","NA; NA; NA; NA","IEEE Transactions on Consumer Electronics","","2006","52","4","1447","1453","In this paper, we propose an optimized real-time AVS (a Chinese next-generation audio/video coding standard) HDTV video decoder. The decoder has been implemented in a single SoC with HW/SW partitioning. AVS algorithms and complexity are first analyzed. Based on the analysis, a hardware implementation of the MB level 7-stage pipeline is selected. The software tasks are realized with a 32-bit RISC processor. We further propose the optimization of interface and RISC processor based on the proposed architecture. The AVS decoder (RISC processor and hardware accelerators) is described in high-level Verilog/VHDL hardware description language and implemented in a single-chip AVS HDTV real-time decoder. At 148.5 MHz working frequency, the decoder chip can support real-time decoding of NTSC, PAL or HDTV (720p@60 frames/s or 1080i@60 fields/s) bit-streams. Finally, the decoder has been fully tested on a prototyping board","0098-3063;1558-4127","","10.1109/TCE.2006.273169","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4050080","","HDTV;Decoding;Reduced instruction set computing;Hardware design languages;Video coding;Partitioning algorithms;Algorithm design and analysis;Pipelines;Computer architecture;Frequency","audio coding;decoding;hardware description languages;high definition television;pipeline processing;software engineering;system-on-chip;video coding","AVS HDTV video decoder architecture;HW-SW partitioning;audio-video coding standard;single SoC;MB level 7-stage pipeline;software tasks;RISC processor;hardware accelerators;high-level Verilog;VHDL hardware description language","","7","15","","","","","","IEEE","IEEE Journals & Magazines"
"The Impact of Organizational Learning on Lack of Team's Expertise Risk in Information Systems Projects","C. Wu; K. Fang","NA; NA","IEEE International Conference on e-Business Engineering (ICEBE'07)","","2007","","","738","743","During the past decade, information systems investment has grown rapidly worldwide and information systems project development has become one of the most important targets in e-business. Yet, the failure of information systems projects is a common occurrence in many organizations around the world. A theoretical model is derived based upon organizational learning theory and prior research in order to examine the effects of organizational learning on lack of team's expertise risk. A survey method is applied to test the hypotheses proposed by the research model, and Taiwanese corporate companies serve as examples. After survey by questionnaire and analyze the data by structure equation modeling, the result reveals that organizational learning has significantly negative impacts on all of the lack of development expertise risk, lack of domain expertise risk, and lack of general expertise risk. These findings support information systems managers with valuable information to revisit their priorities in terms of the relative efforts in organization learning.","","0-7695-3003-6978-0-7695-3003","10.1109/ICEBE.2007.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4402174","Organizational learning;Lack of team's expertise;Project risk;Project failure","Information systems;Project management;Risk management;Management information systems;Information management;Investments;Design methodology;Software development management;Software tools;Application software","electronic commerce;human resource management;information systems;project management;risk management","team expertise risk;information systems projects;information systems investment;e-business;structure equation modeling;organizational learning","","","35","","","","","","IEEE","IEEE Conferences"
"Increased Mars Rover Autonomy using AI Planning, Scheduling and Execution","T. Estlin; D. Gaines; C. Chouinard; R. Castano; B. Bornstein; M. Judd; I. Nesnas; R. Anderson","Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA 91109 USA. email: Tara.Estlin@jpl.nasa.gov; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA 91109 USA. email: Daniel.Gaines@jpl.nasa.gov; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA 91109 USA. email: Caroline.Chouinard@jpl.nasa.gov; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA 91109 USA. email: Rebecca.Castano@jpl.nasa.gov; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA 91109 USA. email: Benjamin.Bornstein@jpl.nasa.gov; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA 91109 USA. email: Michele.Judd@jpl.nasa.gov; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA 91109 USA. email: Issa.Nesnas@jpl.nasa.gov; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA 91109 USA. email: Robert.Anderson@jpl.nasa.gov","Proceedings 2007 IEEE International Conference on Robotics and Automation","","2007","","","4911","4918","This paper presents technology for performing autonomous commanding of a planetary rover. Through the use of AI planning, scheduling and execution techniques, the OASIS autonomous science system provides capabilities for the automated generation of a rover activity plan based on science priorities, the handling of opportunistic science, including new science targets identified by onboard data analysis software, other dynamic decision-making such as modifying the rover activity plan in response to problems or other state and resource changes. We first describe some of the particular challenges this work has begun to address and then describe our system approach. Finally, we report on our experience testing this software with a Mars rover prototype.","1050-4729","1-4244-0602-11-4244-0601","10.1109/ROBOT.2007.364236","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4209854","","Mars;Artificial intelligence;Data analysis;Navigation;Monitoring;Software safety;Robotics and automation;Image analysis;System testing;Hardware","aerospace robotics;data analysis;mobile robots;planetary rovers;planning (artificial intelligence);scheduling","Mars rover autonomy;artificial intelligent planning;scheduling;planetary rover;OASIS autonomous science system;opportunistic science;data analysis","","7","13","","","","","","IEEE","IEEE Conferences"
"A Federated UDDI System for Concurrent Access to Service Data","Q. Liang; J. Chung","NA; NA","2008 IEEE International Conference on e-Business Engineering","","2008","","","71","78","With service-oriented environments, the real-time acquisition of new software functions relies on dynamic service discovery with the Universal Description, Discovery and Integration (UDDI) protocol. Both the fluidity of services and updates in service offerings affect service users via access operations of service data in registry repositories conformed to the protocol. One valuable usage of UDDI-specific systems is to guarantee real time responses to service enquiries with the service data that reflects the most updated information of services. In this paper, we present a federated UDDI system for service-oriented environment within an enterprise or organization boundary. Its distributed architecture allows the system to easily process data process requirements from service providers and users in a correlated and synchronized way. The system is designed with a special support of concurrent access to service data. We describe the synchronization technology used in the system that is based on optimistic timestamp-ordering. Our simulation tests validate the effectiveness of service data access with our federated UDDI registry system.","","978-0-7695-3395","10.1109/ICEBE.2008.116","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4690602","UDDI System;Concurrent Access;Service Data","Environmental management;Real time systems;Access protocols;Service oriented architecture;System testing;Web services;Quality of service;Data engineering;Management information systems;Conference management","software architecture;Web services","federated UDDI system;service data access;service-oriented environments;organization boundary;service providers;concurrent access;synchronization technology;timestamp-ordering;universal description discovery integration protocol","","1","16","","","","","","IEEE","IEEE Conferences"
"Research of Wireless Data Transmission Based on Embedded Systems","S. Zheng; X. Qi","NA; NA","2009 International Conference on Networking and Digital Society","","2009","2","","162","165","This paper reports on an approach to data transmission in wireless mobile networks, based on embedded system. A methodology and a design flow for wireless data transmission is presented. ADC has been used for hardware design for sometime, but is now being extended to software design application by adding the MCU as one of ADC controller.The approach relies on the exploitation of the features of the applications of wireless mobile communication technology and short messages data transmission technology. Also a further optimization is given to the data packet and sending technology; the author made a test based on these technologies, and achieved the desired results.","","978-0-7695-3635","10.1109/ICNDS.2009.120","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5116709","Embedded system;Analog-digital conversion;Data Packet;Digital Signal","Data communication;Embedded system;Application software;Design methodology;Hardware;Software design;Communication system control;Wireless communication;Mobile communication;Communications technology","analogue-digital conversion;embedded systems;radio networks","wireless data transmission;embedded systems;wireless mobile networks;analog-digital conversion;data packet","","","6","","","","","","IEEE","IEEE Conferences"
"A System for Dynamic Server Allocation in Application Server Clusters","A. Chester; J. W. J. Xue; L. He; S. Jarvis","NA; NA; NA; NA","2008 IEEE International Symposium on Parallel and Distributed Processing with Applications","","2008","","","130","139","Application server clusters are often used to service high-throughput web applications. In order to host more than a single application, an organisation will usually procure a separate cluster for each application. Over time the utilisation of the clusters will vary, leading to variation in the response times experienced by users of the applications. Techniques that statically assign servers to each application prevent the system from adapting to changes in the workload, and are thus susceptible to providing unacceptable levels of service. This paper investigates a system for allocating server resources to applications dynamically, thus allowing applications to automatically adapt to variable workloads. Such a scheme requires meticulous system monitoring, a method for switching application servers between \text it {server pools} and a means of calculating when a server switch should be made (balancing switching cost against perceived benefits). Experimentation is performed using such a switching system on a Web application testbed hosting two applications across eight application servers. The testbed is used to compare several theoretically derived switching policies under a variety of workloads. Recommendations are made as to the suitability of different policies under different workload conditions.","2158-9178;2158-9208","978-0-7695-3471","10.1109/ISPA.2008.88","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4725143","Dynamic resource allocation;Enterprise Resource Allocation","Application software;Resource management;Web server;Switches;Web and internet services;Optimization;Delay;System testing;Capacity planning;Admission control","Internet;resource allocation;system monitoring","dynamic server allocation;application server cluster;service high-throughput Web application;server resources;system monitoring;switching application servers;server switch;switching cost balancing;perceived benefits;switching system;Web application testbed hosting","","5","18","","","","","","IEEE","IEEE Conferences"
"Guaranteed Loop Bound Identification from Program Traces for WCET","M. Bartlett; I. Bate; D. Kazakov","NA; NA; NA","2009 15th IEEE Real-Time and Embedded Technology and Applications Symposium","","2009","","","287","294","Static analysis can be used to determine safe estimates of Worst Case Execution Time. However, overestimation of the number of loop iterations, particularly in nested loops, can result in substantial pessimism in the overall estimate. This paper presents a method of determining exact parametric values of the number of loop iterations for a particular class of arbitrarily deeply nested loops. It is proven that values are guaranteed to be correct using information obtainable from a finite and quantifiable number of program traces. Using the results of this proof, a tool is constructed and its scalability assessed.","1545-3421","978-0-7695-3636","10.1109/RTAS.2009.29","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4840589","","Scalability;Real time systems;Safety;Testing;Application software;Computer science;Embedded system;Hardware;Optimization methods","program control structures;program diagnostics","loop bound identification;program trace;static analysis;worst case execution time;loop iteration","","4","25","","","","","","IEEE","IEEE Conferences"
"Using Abstraction in the Verification of Simulation Coercion","Xinyu Liu; P. F. Reynolds; D. C. Brogan","University of Virginia, USA; NA; NA","20th Workshop on Principles of Advanced and Distributed Simulation (PADS'06)","","2006","","","119","128","Simulation coercion concerns the adaptation of an existing simulation to meet new requirements. Interactions among course-of-action options available during coercion can become sufficiently complex that full verification of the simulation as it is adapted becomes cost-prohibitive. To address this issue we introduce two forms of abstraction, as employed in the model-checking community, to support verification of critical features of the simulation. We extend existing abstraction methods to facilitate our goals, and propose a useful abstraction method based on partial traces. As a case study, we apply our abstraction methods to the verification of a coercion of an existing simulation.","1087-4097","0-7695-2587","10.1109/PADS.2006.38","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1630716","","Computational modeling;Hardware;Automatic control;Real time systems;Constraint optimization;Production;Software engineering;Testing;Conferences;Reasoning about programs","","","","2","12","","","","","","IEEE","IEEE Conferences"
"Two-Stage Model for Information Filtering","X. Zhou; Y. Li; P. Bruza; Y. Xu; R. Y. K. Lau","NA; NA; NA; NA; NA","2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology","","2008","3","","685","689","This thesis presents a novel two-stage model that integrates the theories and techniques from the fields of information retrieval/filtering (IR/IF)and the fields of machine learning and data mining to provide more precise document filtering and retrieval. The first stage is topic filtering. The topic filtering stage is intended to minimize information mismatch by filtering out the most likely irrelevant information based on term-based profiles. Thus, only a relatively small amount of potentially highly relevant documents remain for document ranking. The second stage of the presented method uses pattern mining approach. The objective of the second stage is to solve the problem of information overload. The most likely relevant documents were assigned higher ranks by exploiting patterns in the pattern taxonomy. The second stage is precision oriented. Since relatively small amount of documents are involved at this stage, computational cost is markedly reduced, at the same time, with significant improved results. The new two-stage information filtering model has been evaluated by extensive experiments. The tests were based on well-known IR bench-marking processes, using the latest version of the Reuters dataset, namely Reuters Corpus Volume 1 (RCV1). The performance of the new model was compared with both of the term-based and data mining-based IF models. The results show that more effective and efficient information access has been achieved by combining the strength of information filtering and data mining method.","","978-0-7695-3496","10.1109/WIIAT.2008.390","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4740871","information retrieval;information filtering;user profile;data mining;sequential pattern mining","Information filtering;Information filters;Information retrieval;Internet;Intelligent agent;Data mining;Filtering theory;Software agents;Search engines;Information technology","data mining;document handling;information filtering;learning (artificial intelligence)","information filtering;two-stage model;information retrieval;machine learning;data mining;document filtering;document retrieval;information mismatch minimization;document ranking;pattern mining;information overload problem;pattern taxonomy;topic filtering stage;term-based profile","","1","18","","","","","","IEEE","IEEE Conferences"
"Multicast Voice Transmission over Vehicular Ad Hoc Networks: Issues and Challenges","P. Bucciol; F. Ridolfo; J. C. D. Martin","NA; NA; NA","Seventh International Conference on Networking (icn 2008)","","2008","","","746","751","In this paper we analyze the challenges and issues of transmitting multicast voice streams in a vehicular ad hoc network (VANET). The considered scenario focuses on the multicast transmission of multimedia signals (MP3 streams) between a static and a mobile node. We examine various technical solutions to guarantee vehicle-to-infrastructure connectivity, such as software enhancements and directional antennas. To overcome transmission problems, directions on how to build an optimized client-server streaming software suite are given. The proposed suite is tested with various safety messages in a real-world network testbed. Results of the experiments are presented in terms of both network layer (such as packet loss rate) and application layer (Mean Opinion Score) metrics. Results show how the specific context determines the distribution of error patterns. To improve the performance of the transmission by reducing the average burst error length, block interleaving techniques are taken into consideration. The presented results show that a reasoned choice of the hardware and software parameters enables the transmission of multicast vocal messages by means of the standard 802.11b protocol.","","978-0-7695-3106","10.1109/ICN.2008.107","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4498252","Interleaving;Multicast;Multimedia;VANET","Ad hoc networks;Streaming media;Testing;Multicast protocols;Speech analysis;Digital audio players;Directional antennas;Safety;Application software;Interleaved codes","ad hoc networks;multicast communication;road vehicle radar","multicast voice transmission;vehicular ad hoc networks","","6","30","","","","","","IEEE","IEEE Conferences"
"Exploiting Binary Abstractions in Deciphering Gene Interactions","S. Yoon; A. Garg; E. Chung; H. S. Park; W. Y. Park; G. De Micheli","Computer Systems Laboratory, Stanford University, Stanford, CA 94305, USA. E-mail: sryoon@stanford.edu; Integrated Systems Center, Swiss Federal Institute of Technology (EPFL), Lausanne, CH-1015, Switzerland; Department of Electrical and Electronic Engineering, Yonsei University, Seoul 120-749, Korea; Department of Computer Science and Engineering, Ewha Womans University, Seoul 120-750, Korea; Macrogen Corporation, Seoul 153-023, Korea; Department of Biochemistry, Seoul National University Medical School, Seoul 110-744, Korea; Fellow, IEEE, Integrated Systems Center, Swiss Federal Institute of Technology (EPFL), Lausanne, CH-1015, Switzerland","2006 International Conference of the IEEE Engineering in Medicine and Biology Society","","2006","","","5858","5863","We consider computationally reconstructing gene regulatory networks on top of the binary abstraction of gene expression state information. Unlike previous Boolean network approaches, the proposed method does not handle noisy gene expression values directly. Instead, two-valued ""hidden state"" information is derived from gene expression profiles using a robust statistical technique, and a gene interaction network is inferred from this hidden state information. In particular, we exploit Espresso, a well-known 2-level Boolean logic optimizer in order to determine the core network structure. The resulting gene interaction networks can be viewed as dynamic Bayesian networks, which have key advantages over more conventional Bayesian networks in terms of biological phenomena that can be represented. The authors tested the proposed method with a time-course gene expression data set from microarray experiments on anti-cancer drugs doxorubicin and paclitaxel. A gene interaction network was produced by our method, and the identified genes were validated with a public annotation database. The experimental studies we conducted suggest that the proposed method inspired by engineering systems can be a very effective tool to decipher complex gene interactions in living systems","1557-170X","1-4244-0032-51-4244-0033","10.1109/IEMBS.2006.260194","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4463140","","Gene expression;Computer networks;Boolean functions;DNA;Robustness;Bayesian methods;Bioinformatics;Biomedical engineering;Biology computing;Signal processing","Bayes methods;Boolean algebra;cancer;drugs;genetics;medical computing;molecular biophysics;statistical analysis;tumours","binary abstraction;gene regulatory networks;gene expression state information;Boolean network approach;two-valued hidden state information;robust statistical technique;gene interaction network;2-level Boolean logic optimizer;core network structure;dynamic Bayesian networks;anti-cancer drugs doxorubicin;paclitaxel;public annotation database;decipher complex gene interactions","Algorithms;Bayes Theorem;Computational Biology;Gene Expression Profiling;Gene Expression Regulation;Gene Regulatory Networks;Humans;Models, Genetic;Models, Statistical;Models, Theoretical;Oligonucleotide Array Sequence Analysis;Software;Time Factors","","22","","","","","","IEEE","IEEE Conferences"
"A hybrid Quantum-Inspired Evolutionary Algorithm for open vehicle routing problem","J. Zhang; Yanwei Zhao; Dianjun Peng; Wanliang Wang","MOE key Laboratory of Mechanical Manufacturing and Automation, Zhejiang University of Technology, Hangzhou, 310032, China; MOE key Laboratory of Mechanical Manufacturing and Automation, Zhejiang University of Technology, Hangzhou, 310032, China; College of Information Engineering, Zhejiang University of Technology, Hangzhou, 310032, China; College of Software Engineering, Zhejiang University of Technology, Hangzhou, 310032, China","2009 IEEE/ASME International Conference on Advanced Intelligent Mechatronics","","2009","","","839","844","This paper proposed a hybrid quantum-inspired evolutionary algorithm (HQEA) with 2-OPT sub-routes optimization for open vehicle routing problem (OVRP). In the HQEA, Quantum-Inspired Evolutionary Algorithm is used for global exploration and 2-OPT algorithm is used to optimize sub-routes for convergence acceleration. Moreover, an encoding method of converting Q-bit representation to integer representation is designed. And genetic operators of quantum crossover and quantum variation are applied to enhance exploration. The proposed HQEA is tested based on classical benchmark problems of VRP. Simulation results and comparisons with other algorithms show that the proposed HQEA has much better exploration quality and it is an effective method for OVRP.","2159-6247;2159-6255","978-1-4244-2852-6978-1-4244-2853-3978-1-4244-2854","10.1109/AIM.2009.5229905","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5229905","","Evolutionary computation;Routing;Quantum computing;Benchmark testing;Operations research;Manufacturing automation;Quantum mechanics;Genetic algorithms;Intelligent vehicles;Mechatronics","convergence;evolutionary computation;mathematical operators;transportation;vehicles","hybrid quantum-inspired evolutionary algorithm;open vehicle routing problem;2-OPT algorithm;convergence acceleration;encoding method;Q-bit representation;integer representation;quantum crossover;quantum variation","","1","15","","","","","","IEEE","IEEE Conferences"
"Holistic pathfinding: Virtual wireless chip design for advanced technology and design exploration","M. Nowak; J. Corleto; C. Chun; R. Radojcic","Qualcomm, San Diego, CA 92121, USA; Qualcomm, San Diego, CA 92121, USA; Qualcomm, San Diego, CA 92121, USA; Qualcomm, San Diego, CA 92121, USA","2008 45th ACM/IEEE Design Automation Conference","","2008","","","593","593","As CMOS technology is scaled beyond 45 nm, SOC/SiP design for wireless chips is increasingly constrained by fundamental technology limits, resulting in challenges including parametric variability, leakage, active power, signal integrity, and diminished performance improvement. New materials and innovative device structures are needed to extend CMOS scaling and integrate disruptive ""More than Moore"" functionality, but these can have adverse impact on manufacturing cost and risk. Hence, tradeoff analysis spanning process, device, circuit, memory, package, architecture, software, and business disciplines is required during the advanced technology development cycle to explore and co-optimize technology and design choices. Such methodology, in conjunction with judicious use of test chips, also provides for a bridge from innovative technology solutions to mainstream product adoption. Several approaches are currently in use for ad-hoc exploration of advanced technology and design - typically based on spreadsheet analysis, guru consultation, and/or full trial designs. With the exploding complexity of the optimization space, subject matter knowledge and expert experience needs to be complemented with a structured methodology and tools. A ""Holistic Pathfinding"" methodology is proposed for addressing technology and design tradeoffs early in the development cycle to allow co-optimization all the way up to the system architecture level. A virtual design flow that allows rapid estimation of performance, power and cost attributes of a potential product, as a function of a given set of process or design assumptions is described as shown. Key target features of such a virtual flow and a summary of the attributes of several candidate point tools is presented. Requirements for the tools and methodologies to be used for Pathfinding across the span of disciplines are outlined, hi order to enable system cost and performance/power analyses, the requirements for predictive models that describe variability, leakage, devices, interconnect, and DFM attributes are identified. Examples of Pathfinding application for co-optimization of memory technology and architecture, reduced parametric variability using restricted physical design rules, and exploration of 3D chip stacking are presented to highlight the requirements and gaps in the existing EDA tool solutions. The vision is for a design exploration platform that outputs performance, active and standby power, and cost estimates in reasonable response time, and with physical and variation awareness at the architectural level.","0738-100X","978-1-60558-115","10.1145/1391469.1391620","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4555886","","Chip scale packaging;CMOS technology;Space technology;Cost function;Memory architecture;Signal design;Manufacturing;Circuits;Software packages;Computer architecture","CMOS integrated circuits;electronic design automation;integrated circuit design;system-in-package;system-on-chip;virtual instrumentation","virtual wireless chip design;CMOS technology;SOC;SiP;parametric variability;signal integrity;CMOS scaling;tradeoff analysis spanning process;innovative technology;mainstream product adoption;ad-hoc exploration;spreadsheet analysis;guru consultation;full trial designs;Holistic Pathfinding;virtual design flow;memory technology;EDA tool solutions;3D chip stacking","","2","","","","","","","IEEE","IEEE Conferences"
"Evaluating rate-estimation for a mobility and QoS-aware network architecture","N. V. Lopes; M. J. Nicolau; A. Santos","Department of Informatics, University of Minho, Braga, Portugal; Department of Information Systems, University of Minho, Braga, Portugal; Department of Informatics, University of Minho, Braga, Portugal","SoftCOM 2009 - 17th International Conference on Software, Telecommunications & Computer Networks","","2009","","","348","352","In a nearby future wireless networks will run applications with special QoS requirements. FHMIP is an effective scheme to reduce Mobile IPv6 handover disruption but it does not deal with any other specific QoS requirement. Therefore new traffic management schemes are needed in order to provide QoS guarantees to real-time applications and this implies network mobility optimizations and congestion control support. Traffic management schemes should deal with QoS requirements during handover and should use some resource management strategy in order to achieve this. In this article a new resource management scheme for DiffServ QoS model is proposed, to be used by access routers as an extension to FHMIP micromobility protocol. In order to prevent QoS deterioration, access routers pre-evaluate the impact of accepting all traffic from a mobile node, previous to the handover. This pre-evaluation and post decision on whether or not to accept any, or all, of this new traffic is based on a measurement based admission control procedure. This mobility and QoS-aware network architecture, integrating a simple signaling protocol, a traffic descriptor, and exhibiting adaptive behavior has been implemented and tested using ns-2. All measurements and decisions are based on DiffServ class-of-service aggregations, thus avoiding large flow state information maintenance. Rate estimators are essential mechanisms to the efficiency of this QoS-aware overall architecture. Therefore, in order to be able to choose the rate estimator that better fits this global architecture, two rate estimators - Time Sliding Window (TSW) and Exponential Moving Average (EMA) - have been studied and evaluated by means of ns-2 simulations in QoS-aware wireless mobility scenarios.","","978-1-4244-4973-6978-953-290-015","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306834","","Resource management;Communication system traffic control;Admission control;Traffic control;Diffserv networks;Access protocols;Mobile radio mobility management;Scalability;Proposals;Informatics","mobility management (mobile radio);quality of service;telecommunication congestion control;telecommunication traffic","rate estimation;QoS-aware Network Architecture;mobility;Mobile IPv6;traffic management schemes;mobility optimizations;congestion control support;resource management strategy;time sliding window;exponential moving average;ns-2 simulations;FHMIP","","","12","","","","","","IEEE","IEEE Conferences"
"Quantum algorithms for bio-molecular solutions to the satisfiability problem on a quantum computer","Weng-Long Chang; Ting-Ting Ren; Jun Luo; Mang Feng; Minyi Guo","Department of Computer Science and Information Engineering, National Kaohsiung University of Applied Sciences, Kaohsiung City, Taiwan 807-78, Republic of China; State Key Laboratory of Magnetic Resonance and Atomic and Molecular Physics, Wuhan Institute of Physics and Mathematics, Chinese Academy of Sciences, 430071, China; State Key Laboratory of Magnetic Resonance and Atomic and Molecular Physics, Wuhan Institute of Physics and Mathematics, Chinese Academy of Sciences, 430071, China; State Key Laboratory of Magnetic Resonance and Atomic and Molecular Physics, Wuhan Institute of Physics and Mathematics, Chinese Academy of Sciences, 430071, China; Department of Computer Software, the University of Aizu, Aizu-Wakamatsu City, Fukushima 965-8580, Japan","2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational Intelligence)","","2008","","","1080","1088","We demonstrate that the logic computation performed by the DNA-based algorithm for solving general cases of the satisfiability problem can be implemented by our proposed quantum algorithm on the quantum machine proposed by Deutsch. Moreover, we also prove that the logic computation by the bio-molecular operations proposed by Adleman can be implemented by quantum gates (for example, the Hadamard gate, NOT, CNOT, and CCNOT) on the quantum machine. Furthermore, those NP-complete problems solved on a bio-molecular computer are also solvable on a quantum computer. To test our theory, we carry out a three-qubit NMR experiment for solving the simplest satisfiability problem.","1089-778X;1941-0026","978-1-4244-1822-0978-1-4244-1823","10.1109/CEC.2008.4630931","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4630931","Quantum Algorithms;Quantum Circuits;Nuclear Magnetic Resonance;Molecular Algorithms","Quantum computing;Registers;Logic gates;Computers;Nuclear magnetic resonance;NP-complete problem;Artificial neural networks","biocomputing;computational complexity;optimisation;quantum computing","quantum algorithms;bio-molecular solutions;quantum computer;DNA-based algorithm;logic computation;quantum machine;NP-complete problems","","","17","","","","","","IEEE","IEEE Conferences"
"An Efficient Detection of Conflicting Updates in Valid XML","C. Byun; I. Yun; S. Park","Sogang University, Seoul, 121-742, South Korea; Sogang University, Seoul, 121-742, South Korea; Sogang University, Seoul, 121-742, South Korea","7th IEEE International Conference on Computer and Information Technology (CIT 2007)","","2007","","","17","22","As soon as many users and applications work concurrently on valid XML documents, transaction management for valid XML becomes an important issue. A fundamental problem is the detection of conflicts between operations. The detection of conflicts between operations is very interesting in query optimization for a query compiler and concurrency for a transaction manager. In this paper, we studies the problem of detecting conflicts among XML update operations in the case that input documents are typed by an XML schema (DTD or XML schema). We formalize READ-UPDATE and UPDATE-UPDATE conflicts in valid XML documents, and propose efficient algorithms to detect such conflicts when the update operations are specified using XPath expressions.","","0-7695-2983-6978-0-7695-2983","10.1109/CIT.2007.39","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4385050","","XML;Concurrency control;Detection algorithms;Testing;Concurrent computing;Databases;Information technology;Computer science;Application software;Conference management","concurrency control;query processing;text analysis;transaction processing;XML","conflicting update detection;XML documents;XML transaction management;query optimization;query compiler;concurrency;XML update operation;XML schema;DTD;READ-UPDATE conflict;UPDATE-UPDATE conflict;XPath expressions","","1","13","","","","","","IEEE","IEEE Conferences"
"The Research and Design of Auto-leveling Control System for Vehicle-borne Radar Platform Based on AVR","C. Lu; D. Huang; Z. Jin; Y. Huang","School of Mechatronics Engineering, University of Electronic Science and Technology of China. No.4, 2nd Section, Jianshebei Road, Chengdu, Sichuan, 610054, China. phone: +86-028-83204114; fax: +86-028-83202579; email: chshlusl@163.com; School of Mechatronics Engineering, University of Electronic Science and Technology of China. No.4, 2nd Section, Jianshebei Road, Chengdu, Sichuan, 610054, China. phone: +86-028-83206903; email: dg_huang@vip.163.com; School of Mechatronics Engineering, University of Electronic Science and Technology of China. No.4, 2nd Section, Jianshebei Road, Chengdu, Sichuan, 610054, China. email: jinzhenlin1@126.com; School of Mechatronics Engineering, University of Electronic Science and Technology of China. No.4, 2nd Section, Jianshebei Road, Chengdu, Sichuan, 610054, China. email: h-hyper@163.com","2006 2nd IEEE/ASME International Conference on Mechatronics and Embedded Systems and Applications","","2006","","","1","5","The paper puts forward an auto-leveling control system for vehicle-borne radar platform with four legs based on ATMEL AVR microcontrollers. In particular, the microcontrollers ATmega8515 and ATmega162 are used. By introducing the theory of auto-leveling, the methods to design the hardware and software of the embedded control system are presented. The system adopts highly accurate two-axis tilt sensor and digital AC servo-system, optimizes auto-leveling strategy and the acceleration and deceleration algorithm of servo-motor. The hardware and software design of the system make use of the concept of anti-interference and modularization. The test proves that this auto-leveling control system improves the leveling-precision and the stability of the performance, shortens the leveling-time, and satisfies the maneuverability and operating requirements of the vehicle-borne platform in formidable nature conditions","","0-7803-9721","10.1109/MESA.2006.296949","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4077776","Auto-leveling;AVR;Platform;Vehicle-borne;Embedded system","Control systems;Radar;Microcontrollers;Hardware;Leg;Design methodology;Embedded software;Sensor systems;Acceleration;Software design","control system synthesis;embedded systems;microcontrollers;military radar;military vehicles;servomechanisms","auto-leveling control system;vehicle-borne radar platform;ATMEL AVR microcontrollers;ATmega8515 microcontroller;ATmega162 microcontroller;two-axis tilt sensor;digital AC servo-system;embedded system","","","12","","","","","","IEEE","IEEE Conferences"
"Exploiting intelligent systems techniques within an autonomous regional active network management system","E. M. Davidson; S. D. J. McArthur; M. J. Dolan; J. R. McDonald","Institute for Energy and Environment, University of Strathclyde, Glasgow, Scotland, G1 1XW, UK; Institute for Energy and Environment, University of Strathclyde, Glasgow, Scotland, G1 1XW, UK; NA; Institute for Energy and Environment, University of Strathclyde, Glasgow, Scotland, G1 1XW, UK","2009 IEEE Power & Energy Society General Meeting","","2009","","","1","8","This paper discusses AuRA-NMS, an autonomous regional active network management system currently being developed in the UK through a partnership between several UK universities, two distribution network operators (DNO) and ABB. The scope of control to be undertaken by AuRA-NMS includes: automatic restoration, voltage control, power flow management and implementation of network performance optimisation strategies. Part of the scientific aims of the AuRA-NMS programme is the investigation and comparison of the use of different techniques for making the control decisions above. The techniques under consideration range from the use of optimisation techniques, such as OPF, to the use of intelligent systems techniques, such as constraint programming, case-based reasoning and AI planning. In this paper the authors consider the role that intelligent systems techniques could play within active network management and reports on preliminary results gathered from the testing of prototype software running on commercially available IEC 61850 compliant substation computing hardware, connected to a real time power systems simulator. The importance of an appropriate comparative testing methodology, as well as the need for assessing the robustness of different techniques in the presence of communication failures and measurement errors, is also discussed. A key element in the development of AuRA-NMS is the use of multi-agent systems (MAS) technology to provide a flexible and extensible software architecture in which the techniques above can be deployed. As a result, the use (MAS) in conjunction with IEC 61850 and the common information model within AuRA-NMS is described.","1932-5517","978-1-4244-4241","10.1109/PES.2009.5275418","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5275418","Cooperative systems;distributed control;intelligent systems","Intelligent systems;Intelligent networks;Power system management;Energy management;Power system planning;IEC standards;Power system simulation;Educational institutions;Automatic voltage control;Power system restoration","case-based reasoning;constraint handling;distributed control;distribution networks;IEC standards;load distribution;load flow control;multi-agent systems;planning (artificial intelligence);power engineering computing;power system management;substations;voltage control","intelligent systems techniques;autonomous regional active network management system;AuRA-NMS;UK;distribution network operators;voltage control;automatic network restoration;power flow management;network performance optimisation;constraint programming;case-based reasoning;AI planning;IEC 61850 compliant substation computing hardware;real time power systems simulator;measurement error;multi-agent systems technology;software architecture","","8","32","","","","","","IEEE","IEEE Conferences"
"Tracking stations alignment with Kalman tracks at LHCb","L. Nicolas; A. Hicheur; M. Needham; J. Amoraal; W. Hulsbergen; G. Raven","Laboratoire de Physique des Hautes Énergies (LPHE), EPFL, Lausanne, Switzerland; Laboratoire de Physique des Hautes Énergies (LPHE), EPFL, Lausanne, Switzerland; Laboratoire de Physique des Hautes Énergies (LPHE), EPFL, Lausanne, Switzerland; Nationaal Instituut voor Subatomaire Fysica (NIKHEF), Amsterdam, The Netherlands; Nationaal Instituut voor Subatomaire Fysica (NIKHEF), Amsterdam, The Netherlands; Nationaal Instituut voor Subatomaire Fysica (NIKHEF), Amsterdam, The Netherlands","2008 IEEE Nuclear Science Symposium Conference Record","","2008","","","1714","1719","The LHCb detector, operating at the Large Hadron Collider at CERN, is a single arm spectrometer optimized for the detection of the forward b anti-b production for b physics studies. The reconstruction of vertices and tracks is done by silicon micro-strips and gaseous straw-tube based detectors. In order to achieve good mass resolution for resonances the tracking detectors should be aligned to a precision of the order of ten microns. A software framework has been developed to achieve these goals and has been tested in various configurations. After a description of the software, we present alignment results and show in particular for the first time that a global χ<sup>2</sup>solving for alignment using a locally parameterized track trajectory can be achieved.","1082-3654","978-1-4244-2714-7978-1-4244-2715","10.1109/NSSMIC.2008.4774733","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4774733","","Kalman filters;Detectors;Large Hadron Collider;Spectroscopy;Production;Physics;Silicon;Resonance;Software testing;Trajectory","","","","1","8","","","","","","IEEE","IEEE Conferences"
"Boosting Verification by Automatic Tuning of Decision Procedures","F. Hutter; D. Babic; H. H. Hoos; A. J. Hu","NA; NA; NA; NA","Formal Methods in Computer Aided Design (FMCAD'07)","","2007","","","27","34","Parameterized heuristics abound in computer aided design and verification, and manual tuning of the respective parameters is difficult and time-consuming. Very recent results from the artificial intelligence (AI) community suggest that this tuning process can be automated, and that doing so can lead to significant performance improvements; furthermore, automated parameter optimization can provide valuable guidance during the development of heuristic algorithms. In this paper, we study how such an AI approach can improve a state-of-the-art SAT solver for large, real-world bounded model-checking and software verification instances. The resulting, automatically-derived parameter settings yielded runtimes on average 4.5 times faster on bounded model checking instances and 500 times faster on software verification problems than extensive hand-tuning of the decision procedure. Furthermore, the availability of automatic tuning influenced the design of the solver, and the automatically-derived parameter settings provided a deeper insight into the properties of problem instances.","","0-7695-3023-0978-0-7695-3023","10.1109/FAMCAD.2007.9","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4401979","Decision Procedures;Boolean Satisfiability;Search Parameter Optimization","Boosting;Encoding;Artificial intelligence;Heuristic algorithms;Runtime;Formal verification;Testing;Hardware;Support vector machines;Support vector machine classification","","","","20","25","","","","","","IEEE","IEEE Conferences"
"Securing trade lanes using an autonomous tracking device","C. Binding; F. Dolivo; R. Hermann; D. Husemann; A. Schade","IBM Zurich Research Laboratory, CH-8803 Ruschlikon, Switzerland; IBM Zurich Research Laboratory, CH-8803 Ruschlikon, Switzerland; IBM Zurich Research Laboratory, CH-8803 Ruschlikon, Switzerland; IBM Zurich Research Laboratory, CH-8803 Ruschlikon, Switzerland; IBM Zurich Research Laboratory, CH-8803 Ruschlikon, Switzerland","IET Intelligent Transport Systems","","2009","3","3","268","281","This study describes the experience of building a highly autonomous, secure, container tracking device using commercial, off-the-shelf, hardware to optimise and secure global trade lanes. We describe the motivation for the project and the hardware and software architectures. Our approach to power conservation is discussed, including a key decision algorithm to detect the loading of a container onto an ocean-going vessel. The paper concludes with findings and insights from various field-tests.","1751-956X;1751-9578","","10.1049/iet-its.2008.0068","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5235442","","","embedded systems;freight containers;international trade;ships;software architecture;tracking","autonomous tracking device;trade lanes security;container tracking device;secure global trade lanes;hardware architectures;software architectures;power conservation;decision algorithm","","2","","","","","","","IET","IET Journals & Magazines"
"A simulation environment for dry-expansion evaporators with application to the design of autotuning control algorithms for electronic","A. Beghi; U. Bianchini; C. Bodo; L. Cecchinato","Department of Information Engineering, Univerisity of Padova, Italy; CAREL SpA, Brugine, Italy; Department of Information Engineering, Univerisity of Padova, Italy; Department of Applied Physics, University of Padova, Italy","2008 IEEE International Conference on Automation Science and Engineering","","2008","","","814","820","In this paper we report some results of a research project aimed at deriving high-performance, adaptive control algorithms for electronic expansion valves (EEVs) to be used in finned-coiled, dry-expansion evaporators for refrigeration systems. The approach we use in the design is that of virtual prototyping, with the aim of developing a software environment that can be used for controller design, rapid prototyping, optimization of data collection, and test design. We describe the development of a simulation model of the evaporator, and its use for deriving autotuning PID control algorithms. Experimental results confirm the effectiveness of the approach.","2161-8070;2161-8089","978-1-4244-2022-3978-1-4244-2023","10.1109/COASE.2008.4626519","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4626519","","Bridges;USA Councils;Automation;Conferences","adaptive control;control system CAD;digital simulation;drying;evaporation;refrigerators;self-adjusting systems;software prototyping;virtual prototyping","simulation environment;finned-coiled dry-expansion evaporator;autotuning adaptive control algorithm design;electronic expansion valve;refrigeration system;virtual prototyping;software environment development;controller design;rapid prototyping","","","18","","","","","","IEEE","IEEE Conferences"
"FPGA implementation of a digital FM modem for SDR architecture","Indranil Hatai; Indrajit Chakrabarti","Dept. of Electronics and Electrical Communication Engineering, Indian Institute of Technology, Kharagpur-721302, India; Dept. of Electronics and Electrical Communication Engineering, Indian Institute of Technology, Kharagpur-721302, India","2009 4th International Conference on Computers and Devices for Communication (CODEC)","","2009","","","1","4","FPGA implementation of a high-performance programmable digital FM modem which is appropriate for inclusion in software-defined radio (SDR) architecture. The proposed design consists of reprogrammable, area-optimized and low-power features. The modulator and demodulator contain a compressed direct digital synthesizer (DDS) for generating the carrier frequency with spurious free dynamic range (SFDR) of more than 70 dB. The demodulator has been implemented based on the digital phase locked loop (DPLL) technique. The proposed FM modem has been implemented and tested using Virtex2Pro university board as a target device. The FPGA implementation of the FM modem can operate at a maximum frequency of 104 MHz, involving around 8.4 K equivalent gates in the XC2VP30-7ff896 FPGA device and consuming about 130 mW power.","","978-1-4244-5073","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5407205","Frequency Modulation (FM);SDR Architecture;FPGA;DPLL;DDS","Field programmable gate arrays;Modems;Frequency modulation;Demodulation;Chirp modulation;Digital modulation;Phase locked loops;Voltage-controlled oscillators;Computer architecture;Linearity","digital phase locked loops;field programmable gate arrays;modems;software radio","field programmable gate arrays;digital FM modem;software-defined radio architecture;demodulator;direct digital synthesizer;carrier frequency generation;spurious free dynamic range;digital phase locked loop;DPLL;Virtex2Pro university board;XC2VP30-7ff896 FPGA device;frequency 104 MHz;power 130 mW","","","23","","","","","","IEEE","IEEE Conferences"
"HMM-Fuzzy Model for Recognition of Gait Changes due to Trip-related Falls","M. R. Hassan; R. Begg; S. Taylor; D. K. Kumar","Student Member, IEEE, Dept. of Computer Science and Software Engineering, The University of Melbourne, Carlton, Vic 3010, Australia. mrhassan@csse.unimelb.edu.au; Senior Member, IEEE, Centre for Ageing, Rehabilitation, Exercise & Sport, Victoria University, Melbourne, Vic 8001, Australia. rezaul.begg@vu.edu.au; Centre for Ageing, Rehabilitation, Exercise & Sport, Victoria University, Melbourne, Vic 8001, Australia.; Member, IEEE, School of Electrical Engineering, RMIT University, Melbourne, Victoria, Australia. dinesh@rmit.edu.au","2006 International Conference of the IEEE Engineering in Medicine and Biology Society","","2006","","","1216","1219","This paper reports the use of HMM-based fuzzy rules generation for identifying the differences in gait between people with tendencies to fall and healthy people. This work is built on the work reported earlier by the authors where fuzzy rules were successfully applied in gait pattern recognition. This paper reports the hybridization of HMM with fuzzy logic for improving the recognition accuracy. Gait features were extracted from minimum foot clearance (MFC) data that was collected during continuous walking on a treadmill from 20 elderly subjects, 10 healthy and 10 with reported balance problem and history of falls. The input feature space was divided into a number of groups based on HMM generated log-likelihood values, and consequently each group was applied to construct a new fuzzy rule. Gradient descent method was used to optimize the parameters of the generated rules. These were then applied to recognize differences in the gait in subjects with trip-related falls history. The model's performance was evaluated using a cross- validation protocol applied on the training and testing data. The HMM-Fuzzy model outperformed the Fuzzy-based gait recognition as reflected both in the receiver operating characteristics (ROC) results as well as absolute percentage accuracy","1557-170X","1-4244-0032-51-4244-0033","10.1109/IEMBS.2006.259804","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4461977","","Hidden Markov models;History;Pattern recognition;Fuzzy logic;Feature extraction;Data mining;Foot;Legged locomotion;Senior citizens;Optimization methods","fuzzy logic;gait analysis;hidden Markov models;pattern recognition","HMM-fuzzy model;fuzzy rules;fuzzy-based gait pattern recognition;minimum foot clearance;continuous walking;treadmill;HMM generated log-likelihood values;gradient descent method;cross-validation protocol;receiver operating characteristics","Accidental Falls;Algorithms;Diagnosis, Computer-Assisted;Foot;Gait;Gait Disorders, Neurologic;Gait Disorders, Neurologic;Gait Disorders, Neurologic;Humans;Markov Chains;Pattern Recognition, Automated;Reproducibility of Results;Sensitivity and Specificity","1","9","","","","","","IEEE","IEEE Conferences"
"Consistency Management for Data Grid in OptorSim Simulator","G. Belalem; Y. Slimani","University of Oran - Es Senia, Oran, Algeria; Faculty of Sciences of Tunis","2007 International Conference on Multimedia and Ubiquitous Engineering (MUE'07)","","2007","","","554","560","One of the principal motivations to use the grids computing and data grids comes from the applications using of large sets from data, for example, in high-energy physics or life science to improve the total output of the software environments used to carry these applications on the grids, data replication are deposited on various selected sites. In the field of the grids the majority of the strategies of replication of the data and scheduling of the jobs were tested by simulation. Several simulators of grids were born. One of the most simulators interesting for our study is the OptorSim tool. In this paper, we present an extension of the OptorSim simulator by a consistency management module of the replicas in the Data Grids. This extension corresponds to a hybrid approach of consistency, it inspired by the pessimistic and optimistic approaches of consistency. This suggested approach has two vocations, in the first time, it makes it possible to reduce the response times compared with the completely pessimistic approach, in the second time, it gives a good quality of service compared with the optimistic approach.","","0-7695-2777","10.1109/MUE.2007.108","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4197330","","Resource management;Computational modeling;Computer science;Grid computing;Application software;Testing;Computer network management;Analytical models;Environmental management;Computer simulation","digital simulation;grid computing","consistency management module;data grid;OptorSim simulator;grid computing;data replication","","6","19","","","","","","IEEE","IEEE Conferences"
"A Leakage Management System Based on Clock Gating Infrastructure for a 65-nm Digital Base-Band Modem Chip","U. Ko; C. Raibaut; J. Ciroux; C. Fournet-Fayard; J. L. Lachese; O. Domerego; L. Bouetel; F. Ben-Amar; M. Ball; J. Rosal; N. Culp; F. Piacibello; J. Vaccani; R. Hollingsworth; V. Menezes; M. Clinton; S. Thiruvengadam; S. Gururajarao; M. Chau; R. Lagerquist; A. Er Rachidi; D. Scott; H. Mair; P. Royannez; F. Jumel; U. Ko; C. Raibaut; J. Ciroux; C. Fournet-Fayard; J. L. Lachese; O. Domerego; L. Bouetel; F. Ben-Amar; M. Ball; J. Rosal; N. Culp; F. Piacibello; J. Vaccani; R. Hollingsworth; V. Menezes; M. Clinton; S. Thiruvengadam; S. Gururajarao; M. Chau; R. Lagerquist; A. Er Rachidi; D. Scott; H. Mair; P. Royannez; F. Jumel","Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet; Texas Instruments Inc., Villeneuve Loubet","2006 Symposium on VLSI Circuits, 2006. Digest of Technical Papers.","","2006","","","214","215","In this paper we present a leakage management system which takes advantage of the existing clock gating infrastructure. This methodology avoids both RTL and software changes, at the block and chip level. We illustrate this approach with a 65-nm digital base band modem while achieving standby leakage in the 100-muA range and overall 1200times leakage reduction including process, circuit and system optimization","2158-5601;2158-5636","1-4244-0006","10.1109/VLSIC.2006.1705386","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1705386","","Clocks;Modems;Power system management;Energy management;Switches;Control systems;Random access memory;Voltage;Circuits;Leakage current","electrical faults;integrated circuit testing;modems","leakage management system;clock gating infrastructure;digital base band modem chip;RTL;wireless SoC;65 nm","","4","3","","","","","","IEEE","IEEE Conferences"
"Data analysis for characterizing PNCCDS","R. Andritschke; G. Hartner; R. Hartmann; N. Meidinger; L. Struder","Max-Planck-Institut Halbleiterlabor, Otto-Hahn-Ring 6, 81739 München, Germany; Max-Planck-Institut för extraterrestrische Physik, Germany; PNSensor GmbH, Römerstr. 28, 80803 Mönchen, Germany; Max-Planck-Institut Halbleiterlabor, Otto-Hahn-Ring 6, 81739 München, Germany; Max-Planck-Institut Halbleiterlabor, Otto-Hahn-Ring 6, 81739 München, Germany","2008 IEEE Nuclear Science Symposium Conference Record","","2008","","","2166","2172","The Max-Planck-Institute semiconductor lab develops, fabricates, tests, and qualifies pnCCDs for space and ground based applications. pnCCDs are CCDs showing high quantum efficiency up to 20 keV while delivering good spatial and energy resolution. This article describes the algorithms applied to the raw data as recorded by the data acquisition system. The main purpose of the underlying software is to qualify the individual pnCCD by measurements of monoenergetic X-ray lines, from B-K (183 eV) to Mo-Kα (17.5 keV), typically Mn-Kα (5.9 keV) under various conditions (e.g. temperature, readout speed, electrical supply voltages of the detector and electronics). Therefore characteristic parameters are determined individually for each measurement as there are read noise, gains, charge transfer efficiencies, charge splitting between neighboring pixels, energy resolution, and bad pixels while correcting for offsets, gains, charge transfer inefficiencies, non-linearities of the electronics, and while recombining the charges spread over more than one pixel. These figures are used in three ways: Firstly, operating parameters are optimized by comparing individual measurements. Secondly, the individual device is rated by combining the results of all its measurements. Thus devices can be selected for applications such as measurement setups for DESY, FLASH, or the X-ray test facility PANTER. Especially the flight modules for the X-ray astronomy mission eROSITIA will be chosen based on the key figures. Thirdly, improvements gained from detector and electronics design and production modifications are quantified closing the development loop of pnCCDs and their associated electronics.","1082-3654","978-1-4244-2714-7978-1-4244-2715","10.1109/NSSMIC.2008.4774781","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4774781","pnCCD;data analysis;X-ray detector;correction algorithms","Data analysis;Extraterrestrial measurements;Energy resolution;Charge transfer;Aerospace electronics;Semiconductor device testing;Application software;Data acquisition;Electric variables measurement;Velocity measurement","","","","9","9","","","","","","IEEE","IEEE Conferences"
"Runtime Memory Allocation in a Heterogeneous Reconfigurable Platform","V. Sima; K. Bertels","NA; NA","2009 International Conference on Reconfigurable Computing and FPGAs","","2009","","","71","76","In this paper, we present a runtime memory allocation algorithm, that aims to substantially reduce the overhead caused by shared-memory accesses by allocating memory directly in the local scratch pad memories. We target a heterogeneous platform, with a complex memory hierarchy. Using special instrumentation, we determine what memory areas are used in functions that could run on different processing elements, like, for example a reconfigurable logic array. Based on profile information, the programmer annotates some functions as candidates for accelerated execution. Then, an algorithm decides the best allocation, taking into account the various processing elements and special scratch pad memories of the heterogeneous platform. Tests are performed on our prototype platform, a Virtex ML410 with Linux operating system, containing a PowerPC processor and a Xilinx FPGA, implementing the MOLEN programming paradigm. We test the algorithm using both state of the art H.264 video encoder as well as other synthetic applications. The performance improvement for the H.264 application is 14% compared to the software only version while the overhead is less than 1% of the application execution time. This improvement is the optimal improvement that can be obtained by optimizing the memory allocation. For the synthetic applications the results are within 5% of the optimum.","2325-6532","978-1-4244-5293-4978-0-7695-3917","10.1109/ReConFig.2009.38","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5382030","memory allocation;heterogeneous;runtime;scratch pad memory","Runtime;Application software;Instruments;Reconfigurable logic;Logic arrays;Programmable logic arrays;Programming profession;Acceleration;System testing;Performance evaluation","field programmable gate arrays;reconfigurable architectures;storage management","heterogeneous reconfigurable platform;runtime memory allocation algorithm;shared-memory accesses;heterogeneous platform;complex memory hierarchy;reconfigurable logic array;Virtex ML410;Linux operating system;PowerPC processor;Xilinx FPGA;MOLEN programming paradigm;H.264 video encoder;field programmable gate arrays","","1","14","","","","","","IEEE","IEEE Conferences"
"DECISION SUPPORT SYSTEM FOR HYBRID VEHICLE RELATED DECISIONS - BUILDING THE BUSINESS CASE","P. Faithfull; A. Vinsome; P. Jennings","NA; NA; NA","IET - The Institution of Engineering and Technolgy Hybrid Vehicle Conference 2006","","2006","","","9","26","Decision making in relation to hybrid vehicles is complicated by a lack of experience with the technology, a wide array of alternatives, and the lack of a clear financial business case. This paper presents a software based decision, decision support tool (DST), which assists understanding of the business case. The DST meets key design objectives of providing a structured process which walks the user through the decision, a decision specific interface and a capability to support a breadth of decisions relevant to a variety of stakeholders. The latter two requirements are met through a software framework which is configured from a database to provide contextual prompts in terminology familiar to the user, avoiding the need to understand the phraseology of decision science. Using the PROMETHEE outranking method, evaluated alternatives are ranked in accordance with inputs from one or more decision makers. Analysis techniques such as sensitivity testing allow the user to test the robustness of the output. Optimal numbers of alternatives can be determined within real-world constraints for decisions such as vehicle fleet composition","","0-8634-174","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4077331","","","decision support systems;hybrid electric vehicles;production engineering computing","decision support system;hybrid vehicles;software based decision;decision support tool;PROMETHEE;outranking method;sensitivity testing;vehicle fleet composition","","","","","","","","","IET","IET Conferences"
"mod kaPoW: Protecting the web with transparent proof-of-work","E. Kaiser; Wu-chang Feng","Portland State University, USA; Portland State University, USA","IEEE INFOCOM Workshops 2008","","2008","","","1","6","Attacks from automated Web clients are a significant problem on the Internet. Web sites often employ Turing tests known as CAPTCHAs to combat automated agents. Unfortunately, such defenses require frequent human user input, are becoming less effective as computer vision techniques improve, and can be subverted by adversaries willing to hire humans to solve challenges. Several alternative defenses based upon cryptographic methods rather than human input have been proposed to achieve the same goals. Such ""proof-of-work"" techniques prioritize clients based on their willingness to solve computational challenges of client-specific difficulty set by the server. Unfortunately, few proof-of-work schemes have been deployed since they require wide-scale adoption of special client software to operate properly. To address these problems we present mocLkaPoW, a novel system that has the efficiency and human-transparency of proof-of-work schemes as well as the software backwards-compatibility of CAPTCHA schemes. The system leverages common Web technologies to deliver a challenge, solve it, and submit the client response, while providing accessibility for legacy clients. This paper describes and evaluates a prototype of this system.","","978-1-4244-2219","10.1109/INFOCOM.2008.4544602","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4544602","","Protection;Uniform resource locators;Humans;Access protocols;Filters;Web server;Java;Internet;Automatic testing;Robotics and automation","client-server systems;Internet;telecommunication security","mod_kaPoW;World Wide Web;transparent proof-of-work;Web client attacks;Internet;Web site;software backwards-compatibility;CAPTCHA scheme","","1","16","","","","","","IEEE","IEEE Conferences"
"Evaluating fMRI preprocessing pipelines","S. C. Strother","Rotman Res. Inst., Toronto Univ., Ont., Canada","IEEE Engineering in Medicine and Biology Magazine","","2006","25","2","27","41","This article reviews the evaluation and optimization of the preprocessing steps for blood-oxygenation-level-dependent (BOLD) functional magnetic resonance imaging (fMRI). This technique indirectly measures changes in local neuronal firing rates by measuring associated changes in deoxy-hemoglobin concentrations in nearby blood vessels. Based on the existing literature, it is impossible to make conclusive statements about the optimal algorithm and software implementations for any single preprocessing step, let alone entire pipelines. The author believes that the present focus on the technological testing of preprocessing steps should be balanced by approaches that test the pipeline. This should include all interactions measured using metrics that are closely linked to research and diagnostic questions addressed at the end of the processing pipeline. The goal is to avoid single expedient or default pipelines by developing a framework capable of potentially testing thousands of possible pipeline implementations per dataset. To achieve this goal, researchers depend on recent developments in software tools for managing neuroimaging workflows.","0739-5175;1937-4186","","10.1109/MEMB.2006.1607667","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1607667","","Pipelines;Magnetic resonance imaging;Data analysis;Image processing;Books;Fluid flow measurement;Brain;Design for experiments;Sequences;Image reconstruction","biomedical MRI;medical image processing;blood;oxygen;brain;neurophysiology;blood vessels","fMRI preprocessing pipelines;blood-oxygenation-level-dependent fMRI;functional magnetic resonance imaging;local neuronal firing rates;deoxy-hemoglobin concentrations;blood vessels;optimal algorithm;software implementations;metrics;software tools;neuroimaging workflows","Algorithms;Animals;Brain;Brain;Brain Mapping;Humans;Image Enhancement;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Magnetic Resonance Imaging;Oxygen;Technology Assessment, Biomedical","64","147","","","","","","IEEE","IEEE Journals & Magazines"
"Using Genetic Algorithms to Navigate Partial Enumerable Problem Space for Web Services Composition","Y. Yan; Y. Liang","NRC-IIT, Canada; UNB, Canada","Third International Conference on Natural Computation (ICNC 2007)","","2007","5","","475","479","Web services have received much interest to support business-to-business or enterprise application integration but how to combine these services optimally in a continually growing search space is always a challenge. This paper investigates composing business processes from individual services as a planning problem where a planner determines the execution order and other constraints among services in a process. When there are a large number of Web services available, it is not easy to find an execution path of Web services composition that can satisfy the given request, since the search space for such a composition problem is in general exponentially increasing. The planner has to work with a problem space that is not fully enumerable. This paper presents a method that combines genetic algorithms (GA) with planning to optimize composition results within an incompletely observed problem space. GA helps to navigate the search in the whole space. At each loop of GA, Web service data are queried and a new subspace is built. The planner works with the subspace and calculates a feasible solution. We test our method on a travel domain. The result is an optimized solution, though global optimization is not guaranteed.","2157-9555;2157-9563","0-7695-2875-9978-0-7695-2875","10.1109/ICNC.2007.799","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4344887","","Genetic algorithms;Navigation;Web services;Artificial intelligence;Process planning;Space exploration;Niobium;Computer science;Application software;Optimization methods","genetic algorithms;search problems;Web services","genetic algorithms;partial enumerable problem space;Web services composition;planning problem;search space","","6","15","","","","","","IEEE","IEEE Conferences"
"Field programmable gate array-based design and realisation of automatic censored cell averaging constant false alarm rate detector based on ordered data variability","A. M. Alsuwailem; S. A. Alshebeili; M. H. Alhowaish; S. M. Qasim","King Saud University; King Saud University; Communications and Information Technology Commission; King Saud University","IET Circuits, Devices & Systems","","2009","3","1","12","21","The design and field programmable gate array (FPGA)-based realisation of automatic censored cell averaging (ACCA) constant false alarm rate (CFAR) detector based on ordered data variability (ODV) is discussed here. The ACCA-ODV CFAR algorithm has been recently proposed in the literature for detecting radar target in non-homogeneous background environments. The ACCA-ODV detector estimates the unknown background level by dynamically selecting a suitable set of ranked cells and doing successive hypothesis tests. The proposed detector does not require any prior information about the background environment. It uses the variability index statistic as a shape parameter to accept or reject the ordered cells under investigation. Recent advances in FPGA technology and availability of sophisticated design tools have made it possible to realise the computation intensive ACCA-ODV detector in hardware, in a cost-effective way. The architecture is modular and has been implemented and tested on an Altera Stratix II FPGA using Quartus II software. The post place and route result show that the proposed design can operate at 100-MHz, the maximum clock frequency of the prototyping board and for this frequency the total processing time required to perform a single run is 0.21--s. This amounts to a speedup for the FPGA-based hardware implementation by a factor of -110 as compared to software-based implementation, which takes 23--s to perform the same operation.","1751-858X;1751-8598","","10.1049/iet-cds:20080072","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4773283","","","detector circuits;field programmable gate arrays;logic design","field programmable gate array;automatic censored cell averaging;constant false alarm rate detector;ordered data variability;Altera Stratix II;Quartus II software;hardware implementation","","5","","","","","","","IET","IET Journals & Magazines"
"Techniques for building excellent Operator Machine Interfaces (OMI)","P. Gorman; N. Pappas","The Boeing Company, Seattle, WA, USA; The Boeing Company, Seattle, WA, USA","2007 IEEE/AIAA 26th Digital Avionics Systems Conference","","2007","","","6.A.1-1","6.A.1-8","Establishing a process to continually improve understanding of operator requirements -the why as well as the how-is key to developing an optimal Operator-Machine Interface (OMI) for a large program. The P-8A program is one example of a complex software development, with over 2 million lines of code and a unique and complex OMI. The OMI itself is used as a tool for achieving, and is designed in parallel with understanding end goals, operator decision making chains, and thought processes. The OMI can be designed to support and anticipate information needs, and alert operators to unusual occurrences. Operator actions and decision making techniques can be discovered by using expert operators and capturing their knowledge through the use of multiple tools to identify the problem set: storytelling, decision mapping, is/is not matrices, etc. When a solution set has been identified and agreed to by the customer, resources allocated, and schedule set, a standard process captures and prioritizes further changes, identifies their impact, and decides on implementation. Documents describing design and processes and a Design Description Document describing the current version of the OMI are made accessible to stakeholders at all times. ""What's important is not that we can conceive the idea, but that when we actually test it on people you discover it doesn't work... your intuition is wrong."" -Daniel M. Russell (IBM Almaden / Xerox PARC).","2155-7195;2155-7209","978-1-4244-1107-8978-1-4244-1108","10.1109/DASC.2007.4391947","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4391947","","Companies;Decision making;Process design;Aerospace testing;Logic;Programming;Resource management;Humans;Computer errors;Software performance","man-machine systems;user interfaces","operator machine interfaces;operator decision making;expert operator;documents describing design;design description document","","","7","","","","","","IEEE","IEEE Conferences"
"Web-based evaluation of Parkinson's Disease subjects: Objective performance capacity measurements and subjective characterization profiles","G. V. Kondraske; R. M. Stewart","Human Performance Institute, University of Texas at Arlington, 76019-0180 USA; Human Performance Lab, Presbyterian Hospital of Dallas, TX 75231 USA","2008 30th Annual International Conference of the IEEE Engineering in Medicine and Biology Society","","2008","","","799","802","Parkinson's Disease (PD) is classified as a progressively degenerative movement disorder, affecting approximately 0.2% of the population and resulting in decreased performance in a wide variety of activities of daily living. Motivated by needs associated with the conduct of multi-center clinical trials, early detection, and the optimization of routine management of individuals with PD, we have developed a three-tiered approach to evaluation of PD and other neurologic diseases/disorders. One tier is characterized as “web-based evaluation”, consisting of objective performance capacity tests and subjective questionnaires that target history and symptom evaluation. Here, we present the initial evaluation of three representative, self-administered, objective, web-based performance capacity tests (simple visual-hand response speed, rapid alternating movement quality, and upper extremity neuromotor channel capacity). Twenty-one subjects (13 with PD, 8 without neurologic disease) were evaluated. Generally good agreement was obtained with lab-based tests executed with an experienced test administrator. We conclude that objective performance capacity testing is a feasible component of a web-based evaluation for PD, providing a sufficient level of fidelity to be useful.","1094-687X;1558-4615","978-1-4244-1814-5978-1-4244-1815","10.1109/IEMBS.2008.4649273","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4649273","","","","","Diagnosis, Computer-Assisted;Female;Humans;Internet;Male;Middle Aged;Parkinson Disease;Questionnaires;Reproducibility of Results;Sensitivity and Specificity;Software;Telemedicine;User-Computer Interface","1","9","","","","","","IEEE","IEEE Conferences"
"Automated modeling of the guidance of a K-9","W. Britt; D. M. Bevly; G. Dozier","Department of Computer Science and Software Engineering, Auburn University, AL 36849, USA; Department of Mechanical Engineering, Auburn University, AL 36849, USA; Department of Computer Science, North Carolina A & T State University, Greensboro, 27411, USA","2008 American Control Conference","","2008","","","2467","2474","This paper attempts to automate and replace human guidance in the control of a K-9 unit by modeling that guidance from observation. The ultimate research goal seeks to contribute toward the autonomous command of a trained K-9 unit by analyzing the movement and the behavior of the dog as it responds to command tones. Specifically, GPS and command signal information (from a human trainer) is recorded as a canine follows (or fails to follow) instructions as it moves toward a destination. The data is then processed into training instances and used as training data for a general regression neural network (GRNN). Then, the network is used to classify previously unseen test instances to determine if the behavior at that moment is normal or anomalous (in need of correcting tones). Both representation of training instances and the system parameters of the GRNN are optimized using a simple evolutionary hill-climber (EHC). Given even fairly limited initial data for training, the system performs well, producing relatively few false positives and false negatives in classification.","0743-1619;2378-5861","978-1-4244-2078-0978-1-4244-2079","10.1109/ACC.2008.4586861","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4586861","","Humans;Automatic control;Intelligent sensors;Global Positioning System;Neural networks;Computer science;Intelligent robots;Training data;Testing;Data security","command and control systems;neurocontrollers;regression analysis","automated modeling;human guidance;K-9 unit control;autonomous command;trained K-9 unit;dog;GPS;command signal information;general regression neural network;evolutionary hill-climber","","3","8","","","","","","IEEE","IEEE Conferences"
"Towards Autonomic GIPSY","E. Vassev; J. Paquet","NA; NA","Fifth IEEE Workshop on Engineering of Autonomic and Autonomous Systems (ease 2008)","","2008","","","25","34","The goal of the autonomic GIPSY (AGIPSY) is to make the General Intensional Programming System (GIPSY) capable of self-managing to a far greater extent than it does it now. This paper presents the AGIPSY architecture for autonomic computing based on multiple interacting autonomic GIPSY nodes (GNs). Moreover, we illustrate how this architecture realizes a number of desired aspects of autonomic computing, including goal-driven self-protection, self-healing, self-optimization, and self-configuration. We then present the autonomic architecture for GNs, these being the autonomic elements of the AGIPSY. In this paper, we do not talk about implementation aspects and test results, since these are going to be tackled by our ongoing research and described in another paper.","2168-1864;2168-1872","978-0-7695-3140-30-7695-3140","10.1109/EASe.2008.9","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4488285","autonomic computing;autonomic element;software architecture;GIPSY","Computer architecture;Fault tolerance;Automation;Scalability;Monitoring;Space technology;NASA;Conferences;Computer science;Software engineering","","","","2","18","","","","","","IEEE","IEEE Conferences"
"Virtual entity based rapid prototype developing framework (VE-RPDF) for intelligent robots","G. Tong; C. Tong; M. Liu; Y. Qu","Institute of A.I. and Robotics, Northeastern University, China; Institute of A.I. and Robotics, Northeastern University, China; Robotics Institute, Beijing University of Aeronautics and Astronautics, China; Institute of A.I. and Robotics, Northeastern University, China","2009 IEEE/RSJ International Conference on Intelligent Robots and Systems","","2009","","","1061","1064","To facilitate the development of intelligent robots, a virtual entity based rapid prototype developing framework (VE-RPDF) is proposed. It aims at helping design intelligent robots through the following steps: rapidly setting up a robot prototype, coding for the control strategy and algorithm, and optimizing the robot design by testing it on both virtual entities and real robots. With VE-RPDF, two types of robots, including wheeled robot and humanoid robot, are developed, and the latter one is introduced as a case to verify the validation of VE-RPDF.","2153-0858;2153-0866","978-1-4244-3803-7978-1-4244-3804","10.1109/IROS.2009.5353973","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5353973","","Virtual prototyping;Intelligent robots;Robot control;Robot sensing systems;Service robots;Mobile robots;Intelligent sensors;Prototypes;Robotics and automation;Humanoid robots","control system CAD;humanoid robots;intelligent robots;robot programming;software prototyping","virtual entity based rapid prototype developing framework;intelligent robots;robot design;wheeled robot;humanoid robot;control strategy coding","","1","6","","","","","","IEEE","IEEE Conferences"
"Bandwidth Management For Distributed Systems","A. Tucholski; A. Francesson; A. Majka; J. Sasin; M. Pilarski","NA; NA; NA; NA; NA","2008 Proceedings of 17th International Conference on Computer Communications and Networks","","2008","","","1","3","The method of finding point of bending on efficiency curve in a distributed systems has been presented. The performance of distributed environment was studied as a cost function of mean network throughput and memory buffers availability. The method can be used then to build up an autonomic bandwidth management. The bounding point on efficiency curve was found as well analytically as for testing facility of a distributed laboratory environment. The influence of data replication and scheduling strategy on final results has been included. The computer simulation has been supported by analytical calculation. Two regions of different behavior on universal hyperbolic like dependence of the performance versus mean network throughput was shown up. A method of finding bounding point has been discuss in order to optimize efficiency of the considered system by such parameters like memory buffers and network bandwidth.","1095-2055","978-1-4244-2390-3978-1-4244-2389","10.1109/ICCCN.2008.ECP.158","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4674318","","Bandwidth;Cost function;Throughput;Availability;Computer simulation;Delay;Research and development management;Technology management;Research and development;Telecommunications","grid computing;scheduling;software fault tolerance","distributed systems;mean network throughput;memory buffers;autonomic bandwidth management;data replication;scheduling strategy;grid environment","","","6","","","","","","IEEE","IEEE Conferences"
"Side-Channel Attack Pitfalls","K. Tiri","Platform Validation Architecture, Intel Corporation, USA. kris.tiri@intel.com","2007 44th ACM/IEEE Design Automation Conference","","2007","","","15","20","While cryptographic algorithms are usually strong against mathematical attacks, their practical implementation, both in software and in hardware, opens the door to side-channel attacks. Without expensive equipment or intrusive monitoring, these attacks bypass the mathematical complexity and find the cryptographic key by observing the power consumption or the execution time variations of the device in normal operation mode. The power traces of 8000 encryptions are for instance sufficient to extract the secret key of an unprotected ASIC AES implementation, which is orders of magnitude smaller than the 2128 tests required to brute force the algorithm. A careful implementation can address these vulnerabilities, yet the solutions conflict with the common design goals to optimize for area, performance and power consumption. This paper introduces the side-channel attack pitfalls, which help create or facilitate the observation of the information leakage, discusses mitigation strategies and identifies opportunities for future research.","0738-100X","978-1-59593-627","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4261135","Design;Security;Verification;Side-Channel Attack;Differential Power Analysis;Encryption;Security IC","Energy consumption;Permission;Public key cryptography;Hardware;Design optimization;Public key;Delay effects;Data security;Computer architecture;Software algorithms","cryptography","cryptographic algorithms;side-channel attack pitfalls;information leakage","","8","40","","","","","","IEEE","IEEE Conferences"
"Agent Based Replica Placement in a Data Grid Environement","S. Naseera; K. V. M. Murthy","NA; NA","2009 First International Conference on Computational Intelligence, Communication Systems and Networks","","2009","","","426","430","In a data grid, large quantities of data files are produced and data replication is applied to reduce data access time. Determining when and where to replicate data in order to meet performance goals in grid systems with many users and files, dynamic network and resource characteristics and changing user behavior is difficult. Therefore efficiency and fast access to replicated data are influenced by the location of the resource holding the replica. In this paper, we present an agent based replica placement algorithm to determine the candidate site for the placement of replica. An agent is deployed at each site holding the master copies of the shared data files. To create a replica, each agent prioritizes the resources in the grid based on the resource configuration, bandwidth in the network and the demand for the replica at their sites and then creates a replica at suitable resource locations. We have carried out the simulation using GridSim Toolkit-4.0 for EU Data Grid Testbed1. The simulation results show that the aggregated data transfer time and the execution time for jobs at various resources is less for agent based replica placement.","","978-0-7695-3743","10.1109/CICSYN.2009.77","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5231873","Data Grid;Data Replication;Candidate Site;Resource Location;GridSim Toolkit-4.0","Delay;Testing;Costs;Seismic measurements;Computational intelligence;Computer science;Data engineering;Bandwidth;Availability;Time measurement","digital simulation;grid computing;software agents","data grid environment;data replication;dynamic network;resource characteristics;agent based replica placement algorithm;GridSim Toolkit-4.0;EU Data Grid Testbed1","","4","21","","","","","","IEEE","IEEE Conferences"
"Facial Range Image Matching Using the ComplexWavelet Structural Similarity Metric","S. Gupta; M. P. Sampat; M. K. Markey; A. C. Bovik; Z. Wang","University of Texas at Austin; University of Texas at Austin; University of Texas at Austin; University of Texas at Austin; University of Texas at Arlington","2007 IEEE Workshop on Applications of Computer Vision (WACV '07)","","2007","","","4","4","We propose a novel 3D face recognition algorithm based on facial range image matching using the complex wavelet structural similarity metric (CW-SSIM) metric. Compared with many existing 3D surface matching methods, CW-SSIM is computationally efficient and is robust to small geometrical distortions. Using a data set that contains 360 3D face models of 12 subjects, we tested the performance of the proposed method and compared it with existing 3D surface matching based face recognition algorithms. Verification and identification performance of each algorithm was evaluated by means of the receiver operating characteristic curve and the cumulative match characteristic curve. Among the algorithms tested, the proposed algorithm based on the CW-SSIM resulted in the best overall performance with an equal error rate of 9.13% and a rank 1 recognition rate of 98.6%, significantly better than all the other algorithms. Besides the introduction of a novel approach for 3D face recognition, this is also the first attempt to expand the application scope of complex wavelet domain similarity measure to range image matching in general","1550-5790","0-7695-2794","10.1109/WACV.2007.22","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4118733","","Image matching;Face recognition;Face detection;Humans;Robustness;Application software;Testing;Biometrics;Lighting;Computer vision","face recognition;image matching;wavelet transforms","facial range image matching;complex wavelet structural similarity metric;CW-SSIM metric;face recognition algorithm;3D surface matching method;geometrical distortion;receiver operating characteristic curve;cumulative match characteristic curve","","5","20","","","","","","IEEE","IEEE Conferences"
"A Modular Approach to the Analysis and Evaluation of Particle Filters for Figure Tracking","Ping Wang; J. M. Rehg","Georgia Institute of Technology; NA","2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)","","2006","1","","790","797","This paper presents the first systematic empirical study of the particle filter (PF) algorithms for human figure tracking in video. Our analysis and evaluation follows a modular approach which is based upon the underlying statistical principles and computational concerns that govern the performance of PF algorithms. Based on our analysis, we propose a novel PF algorithm for figure tracking with superior performance called the Optimized Unscented PF. We examine the role of edge and template features, introduce computationally-equivalent sample sets, and describe a method for the automatic acquisition of reference data using standard motion capture hardware. The software and test data are made publicly-available on our project website.","1063-6919","0-7695-2597","10.1109/CVPR.2006.32","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1640834","","Particle filters;Particle tracking;Algorithm design and analysis;Testing;Performance analysis;Humans;State-space methods;Stereo vision;Face detection;Optimization methods","","","","12","28","","","","","","IEEE","IEEE Conferences"
"Ontologies and tools for analyzing and synthesizing LVC confederations","R. Ford; D. Martin; D. Elenius; M. Johnson","SRI International, 333 Ravenswood Ave, Menlo Park, CA 94025, USA; SRI International, 333 Ravenswood Ave, Menlo Park, CA 94025, USA; SRI International, 333 Ravenswood Ave, Menlo Park, CA 94025, USA; SRI International, 4119 Broad Street, San Luis Obispo, CA 93401, USA","Proceedings of the 2009 Winter Simulation Conference (WSC)","","2009","","","1387","1398","Establishing and maintaining interoperability among heterogeneous systems is a major challenge and expense for large business and military projects. Data interoperability and service-oriented architecture (SOA) approaches, while essential, do not provide a complete solution. We describe a complementary approach that uses Web Ontology Language (OWL) and Semantic Web Rule Language (SWRL) to capture information about the roles and capabilities required to complete a task, and the attributes of candidate resources. Our toolset applies automated reasoning to determine whether each candidate resource has the requisite capabilities and is compatible with other resources. If there are multiple candidates for a role, the reasoner ranks the relative goodness of each with respect to constraints and metrics that are appropriate for the specific task needs of the exercise or deployment. We also describe a further application of the ontologies and toolset to assist in the creation of composable data exchange models.","0891-7736;1558-4305","978-1-4244-5771-7978-1-4244-5770-0978-1-4244-5772","10.1109/WSC.2009.5429291","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5429291","","Ontologies;OWL;Semantic Web;Service oriented architecture;Context modeling;Impedance;Testing;Web services;Silver;Discrete event simulation","business data processing;electronic data interchange;inference mechanisms;knowledge representation languages;military computing;ontologies (artificial intelligence);open systems;organisational aspects;semantic Web;software architecture","LVC confederations;heterogeneous systems;business project;military project;data interoperability;service-oriented architecture;Web ontology language;semantic Web rule language;automated reasoning;composable data exchange models;live-virtual-constructive federations","","2","29","","","","","","IEEE","IEEE Conferences"
"Improving VG-RAM WNN Multi-label Text Categorization via Label Correlation","A. F. D. Souza; C. Badue; B. Z. Melotti; F. T. Pedroni; F. L. L. Almeida","NA; NA; NA; NA; NA","2008 Eighth International Conference on Intelligent Systems Design and Applications","","2008","1","","437","442","In multi-label text databases one or more labels, or categories, can be assigned to a single document. In many such databases there can be correlation on the assignment of subsets of the set of categories. This can be exploited to improve machine learning techniques devoted to multi-label text categorization. In this paper, we examine a virtual generalizing random access memory weightless neural network (VG-RAM WNN for short) architecture that takes advantage of the correlation between categories to improve text-categorization performance. We compare the performance of this architecture, that we named data correlated VG-RAM WNN (VG-RAM WNN-COR), with that of standard VG-RAM WNN using four multi-label categorization performance metrics: one-error, ranking loss, average precision and hamming loss. Our experimental results show that VG-RAM WNN-COR has an overall better performance than VG-RAM WNN for the set of metrics considered.","2164-7143;2164-7151","978-0-7695-3382","10.1109/ISDA.2008.298","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4696246","Neural Networks;VG-RAM WNN;Multi-label text categorization","Text categorization;Neural networks;Machine learning;Random access memory;Measurement;Neurons;Intelligent systems;Deductive databases;Performance loss;Testing","learning (artificial intelligence);neural nets;software metrics;text analysis","VG-RAM WNN multilabel text categorization;label correlation;multilabel text databases;machine learning techniques;virtual generalizing random access memory weightless neural network;data correlated VG-RAM WNN","","2","18","","","","","","IEEE","IEEE Conferences"
"An Architectural Characterization Study of Data Mining and Bioinformatics Workloads","B. Ozisikyilmaz; R. Narayanan; J. Zambreno; G. Memik; A. Choudhary","Electrical Engineering and Computer Science, Northwestern University, Evanston, IL 60208, USA. boz283@eecs.northwestern.edu; Electrical Engineering and Computer Science, Northwestern University, Evanston, IL 60208, USA. ran310@eecs.northwestern.edu; Electrical and Computer Engineering, Iowa State University, Ames, IA 50011, USA. zambreno@iastate.edu; Electrical Engineering and Computer Science, Northwestern University, Evanston, IL 60208, USA. memik@eecs.northwestern.edu; Electrical Engineering and Computer Science, Northwestern University, Evanston, IL 60208, USA. choudhar@eecs.northwestern.edu","2006 IEEE International Symposium on Workload Characterization","","2006","","","61","70","Data mining is the process of automatically finding implicit, previously unknown, and potentially useful information from large volumes of data. Advances in data extraction techniques have resulted in tremendous increase in the input data size of data mining applications. Data mining systems, on the other hand, have been unable to maintain the same rate of growth. Therefore, there is an increasing need to understand the bottlenecks associated with the execution of these applications in modern architectures. In this paper, we present MineBench, a publicly available benchmark suite containing fifteen representative data mining applications belonging to various categories: classification, clustering, association rule mining and optimization. First, we highlight the uniqueness of data mining applications. Subsequently, we evaluate the MineBench applications on an 8-way shared memory (SMP) machine and analyze important performance characteristics such as L1 and L2 cache miss rates, branch misprediction rates","","1-4244-0509-21-4244-0508","10.1109/IISWC.2006.302730","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4086134","","Data mining;Bioinformatics;Performance analysis;Application software;Algorithm design and analysis;Streaming media;Multimedia databases;Data engineering;Computer science;Association rules","benchmark testing;biology computing;cache storage;data mining;shared memory systems","architectural characterization;data mining;bioinformatics workloads;data extraction;MineBench;association rule mining;shared memory machine;performance analysis;L1 cache miss rates;L2 cache miss rates;branch misprediction rates","","5","41","","","","","","IEEE","IEEE Conferences"
"Web-Based Control System of an Air Conditioner Experimental Rig with Adaptive Fuzzy PI Controller","J. Bai; H. Xiao; T. Zhu; W. Liu; X. Yang; G. Zhang","NA; NA; NA; NA; NA; NA","2008 International Workshop on Education Technology and Training & 2008 International Workshop on Geoscience and Remote Sensing","","2008","1","","696","700","The paper describes a web-based remote control system for an air conditioner experimental rig, which provides a unified and flexible Web-based interface to access the experimental rig located remotely. The hardware and software infrastructure of the control system is presented. The rig is composed of two test rooms, in which temperature and humidity are required to keep constant. The weakness using conventional PI controllers in the temperature control of the test room is analyzed. To improve the control performance over PI controllers, an adaptive fuzzy PI controller is developed. In the proposed control scheme, the increment of the PI controller parameters can be adjusted online by fuzzy rules according to the error of the controlled variable and the change of the error. Thus, PI controller parameters of the proposed control strategy can be optimized under close-loop operation. Field test results demonstrate that the adaptive Fuzzy PI controller has better performance than conventional PI controllers.","","978-0-7695-3563","10.1109/ETTandGRS.2008.335","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5070250","air conditioner experimental rig;web-based control system;adaptive controller;fuzzy;PI controller","Control systems;Programmable control;Adaptive control;Fuzzy control;Fuzzy systems;Testing;Temperature control;Error correction;Hardware;Humidity","adaptive control;air conditioning;closed loop systems;control engineering computing;fuzzy control;PI control;telecontrol;temperature control","air conditioner experimental rig;Web-based remote control system;Web-based interface;temperature control;adaptive fuzzy PI controller;fuzzy rules;control strategy;close-loop operation","","","7","","","","","","IEEE","IEEE Conferences"
"Perceptual feature based music classification - A DSP perspective for a new type of application","H. Blume; M. Haller; M. Botteck; W. Theimer","Chair for Electrical Engineering and Computer Systems RWTH Aachen University, Schinkelstraße 2, 52062, Germany; Chair for Electrical Engineering and Computer Systems RWTH Aachen University, Schinkelstraße 2, 52062, Germany; Nokia Research Center Meesmannstr. 103, 44807 Bochum, Germany; Nokia Research Center Meesmannstr. 103, 44807 Bochum, Germany","2008 International Conference on Embedded Computer Systems: Architectures, Modeling, and Simulation","","2008","","","92","99","Today, more and more computational power is available not only in desktop computers but also in portable devices such as smart phones or PDAs. At the same time the availability of huge non-volatile storage capacities (flash memory etc.) suggests to maintain huge music databases even in mobile devices. Automated music classification promises to allow keeping a much better overview on huge data bases for the user. Such a classification enables the user to sort the available huge music archives according to different genres which can be either predefined or user defined. It is typically based on a set of perceptual features which are extracted from the music data. Feature extraction and subsequent music classification are very computational intensive tasks. Today, a variety of music features and possible classification algorithms optimized for various application scenarios and achieving different classification qualities are under discussion. In this paper results concerning the computational needs and the achievable classification rates on different processor architectures are presented. The inspected processors include a general purpose P IV dual core processor, heterogeneous digital signal processor architectures like a Nomadik STn8810 (featuring a smart audio accelerator, SAA) as well as an OMAP2420. In order to increase classification performance, different forms of feature selection strategies (heuristic selection, full search and Mann-Whitney-Test) are applied. Furthermore, the potential of a hardware-based acceleration for this class of application is inspected by performing a fine as well as a coarse grain instruction tree analysis. Instruction trees are identified, which could be attractively implemented as custom instructions speeding up this class of applications.","","978-1-4244-1985","10.1109/ICSAMOS.2008.4664851","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4664851","music information retrieval;music classification;feature extraction;processor performance;processor architecture optimization;ASIP","Digital signal processing;Personal digital assistants;Portable computers;Feature extraction;Computer architecture;Application software;Smart phones;Nonvolatile memory;Flash memory;Databases","classification;database management systems;feature extraction;information retrieval;music;signal processing;storage management","music classification;digital signal processing;smart phones;PDA;nonvolatile storage capacities;huge data bases;feature extraction","","3","22","","","","","","IEEE","IEEE Conferences"
"12.75"" Synthetic Aperture Sonar (SAS), High Resolution and Automatic Target Recognition","A. D. Matthews; T. C. Montgomery; D. A. Cook; J. W. Oeschger; J. S. Stroud","Naval Surface Warfare Center Panama City, 110 Vernon Avenue, Panama City, FL 32407. anthony.matthews@navy.mil; Applied Research Laboratory, Penn State University. tcm3@psu.edu; Naval Surface Warfare Center Panama City, 110 Vernon Avenue, Panama City, FL 32407. daniel.a.cook@navy.mil; Naval Surface Warfare Center Panama City, 110 Vernon Avenue, Panama City, FL 32407. john.oeschger@navy.mil; Naval Surface Warfare Center Panama City, 110 Vernon Avenue, Panama City, FL 32407. john.stroud@navy.mil","OCEANS 2006","","2006","","","1","7","The Autonomous Operations Future Naval Capability (AOFNC) program developed a 12.75"" diameter autonomous underwater vehicle (AUV) and a synthetic aperture sonar (SAS12) payload. This system falls under the lightweight designator of the Unmanned Undersea Vehicle (UUV) master plan. Bluefin Robotics Corporation and the Applied Research Laboratory of The Pennsylvania State University (ARL/PSU) developed the vehicle/payload system. In addition to the previous team members, Naval Surface Warfare Center Panama City (NSWC PC) developed the synthetic aperture image processing. The system will include motion compensation and beam formation software, real time data handlers, and automatic target recognition algorithms. NSWC PC provided test range services and test planning to the project, as well. The AUV design is an open frame that allows modular payloads to be attached. The modules are self-contained and the surround is free-flooded. A plastic fairing covers the payload and vehicle subsystems. The payload power and communications are supplied through common interfaces. The vehicle hosts a suite of inertial, environmental, and heading sensors, as well as, a Doppler velocity log (DVL). Data from this sensor suite is combined to provide the information necessary for proper SAS operation. This data is used both in the SAS ping timing and ultimately in the correction of errors due to aperture misalignment. Vehicle and payload data and logs are recorded and used to evaluate system performance. The SAS payload is designed using COTS data acquisition and communication hardware. The SAS operates at 180 kHz in the side looking mode. A suite of arbitrary waveforms can be transmitted to optimize SAS performance in a given environment. The broadband receiver is designed for minimal channel-to-channel gain and phase errors necessary for acquisition of high fidelity signals. Signals are filtered and decimated then passed to the recorder and processing systems. The individual element aperture determines the ultimate resolution limit. In principle, SAS12 can be processed for 25 mm resolution at all ranges out to a maximum of 150 meters. One advantage of SAS is that the data collected can be processed to whatever resolution is defined by the user, within this limit. This is useful in resolution studies because the same data set can be processed for different resolutions. Typically real apertures have a fixed resolution proportional to the physical length. Depending on the real aperture system, this resolution may be constant or vary as a function of range. The system will include real time automatic target recognition (ATR). The ATR consists of a set of algorithms developed by several different contributors. The master algorithm uses a rule based system to combine the information generated by the individual contributors. The result produces a lower false alarm rate that any single algorithm. The authors present performance for comparison to the existing data bases that relate ATR performance to image resolution. ATR performance is affected by clutter, bottom type, target aspect, and many other characteristics, as modified by the SAS resolution. Imagery is presented with ATR performance measures. Sonar performance is discussed in qualitative terms, and is based on image appearance and knowledge of what targets are present in the field. Quantitative performance measures are also presented in terms of requirements of the ATR, Probability of Detection (Pd), and Probability of False Alarm (Pfa)","0197-7385","1-4244-0114-31-4244-0115","10.1109/OCEANS.2006.307046","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4098865","","Synthetic aperture sonar;Target recognition;Payloads;Remotely operated vehicles;Real time systems;Testing;Signal resolution;Underwater vehicles;Robotics and automation;Laboratories","geophysics computing;image processing;oceanographic techniques;synthetic aperture sonar;target tracking;underwater vehicles","synthetic aperture sonar;SAS;automatic target recognition algorithms;Autonomous Operations Future Naval Capability;AOFNC program;autonomous underwater vehicle;AUV;lightweight designator;Unmanned Undersea Vehicle;UUV master plan;Bluefin Robotics Corporation;Applied Research Laboratory;Pennsylvania State University;vehicle-payload system;Naval Surface Warfare Center;Panama City;NSWC;synthetic aperture image processing;beam formation software;real time data handlers;vehicle subsystems;payload power;Doppler velocity log;COTS data acquisition;communication hardware;broadband receiver;signal acquisition;recorder system;data collection;real aperture system;image resolution;image appearance;probability of detection;probability of false alarm;12.75 in;180 kHz;150 m","","2","4","","","","","","IEEE","IEEE Conferences"
"Accuracy and precision of the three-dimensional assessment of the facial surface using a 3-D laser scanner","L. Kovacs; A. Zimmermann; G. Brockmann; H. Baurecht; K. Schwenzer-Zimmerer; N. A. Papadopulos; M. A. Papadopoulos; R. Sader; E. Biemer; H. F. Zeilhofer","Dept. for Plastic & Reconstructive Surg., Tech. Univ. Munich, Germany; Dept. for Plastic & Reconstructive Surg., Tech. Univ. Munich, Germany; Dept. for Plastic & Reconstructive Surg., Tech. Univ. Munich, Germany; NA; NA; NA; NA; NA; NA; NA","IEEE Transactions on Medical Imaging","","2006","25","6","742","754","Three-dimensional (3-D) recording of the surface of the human body or anatomical areas has gained importance in many medical specialties. Thus, it is important to determine scanner precision and accuracy in defined medical applications and to establish standards for the recording procedure. Here we evaluated the precision and accuracy of 3-D assessment of the facial area with the Minolta Vivid 910 3D Laser Scanner. We also investigated the influence of factors related to the recording procedure and the processing of scanner data on final results. These factors include lighting, alignment of scanner and object, the examiner, and the software used to convert measurements into virtual images. To assess scanner accuracy, we compared scanner data to those obtained by manual measurements on a dummy. Less than 7% of all results with the scanner method were outside a range of error of 2 mm when compared to corresponding reference measurements. Accuracy, thus, proved to be good enough to satisfy requirements for numerous clinical applications. Moreover, the experiments completed with the dummy yielded valuable information for optimizing recording parameters for best results. Thus, under defined conditions, precision and accuracy of surface models of the human face recorded with the Minolta Vivid 910 3D Scanner presumably can also be enhanced. Future studies will involve verification of our findings using test persons. The current findings indicate that the Minolta Vivid 910 3D Scanner might be used with benefit in medicine when recording the 3-D surface structures of the face","0278-0062;1558-254X","","10.1109/TMI.2006.873624","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1637532","Accuracy and precision of 3-D scanner systems;landmark identification;three-dimensional imaging;three-dimensional models","Surface emitting lasers;Humans;Biomedical imaging;Medical services;Biomedical equipment;Image converters;Software measurement;Computer errors;Face;Testing","biomedical optical imaging;laser applications in medicine;medical image processing","scanner precision;scanner accuracy;facial Surface;Minolta Vivid 910 3D Laser Scanner;lighting;scanner-object alignment;virtual images","Algorithms;Artificial Intelligence;Cluster Analysis;Face;Humans;Image Enhancement;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Information Storage and Retrieval;Lasers;Pattern Recognition, Automated;Phantoms, Imaging;Reproducibility of Results;Sensitivity and Specificity","41","30","","","","","","IEEE","IEEE Journals & Magazines"
"Design and Implementation of Embedded Digital Album","C. Liang; H. Weidong; L. Fei","NA; NA; NA","2008 Fifth IEEE International Symposium on Embedded Computing","","2008","","","113","118","This paper presents a procedure developing a digital album based on an embedded Linux, using Qt, a powerful development toolkit. Being a new-fashioned digital consumable, the digital album considered as fashionable, user friendly and convenient. The mainly functions of the embedded digital album are that reviewing and managing the photos from DC. At the same time, it will include the following functions: to support the multi-format of picture, i.e. JPEG, BMP, and GIF etc; to display the photos as thumbnails; to configure the options of the system, i.e. choose the language of user interface. It also will support an external storage – CF card. When the CF card is inserted, the pictures will be loaded immediately, and display the thumbnails of the photos storing in this card. The hardware platform used in this project is a processor based on Intel's XScale PXA255, while the software platform is Embedded Linux and the development environment features is Qt and Qt/Embedded. Some key technical problems have been discussed in details, including files scanning, filename list storing, rotation of photos and communication between/inside the modules. Several loading pictures strategies have been compared in terms of time and space, which involves the single-thread & multi-thread, pre-loading and saving thumbnail mechanism, with the detailed test data given, and the optimized strategy has been worked out. The testing data indicated that this digital album system working stably and reliably.","","978-0-7695-3348","10.1109/SEC.2008.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4690734","Embedded Linux;Digital Album;Qt / Embedded Programming","Linux;Hardware;Displays;Operating systems;Cables;Embedded software;Digital cameras;Embedded computing;Programming;Power system management","","","","","6","","","","","","IEEE","IEEE Conferences"
"Scheduling Workflow-based Parameter-Sweep Applications with Best-Intermediate-Result-First Heuristic","K. Srimanotham; V. Muangsin","Scientific Parallel Computer Engineering Lab, Department of Computer Engineering, Chulalongkorn University, Bangkok, Thailand. kunaporn.sr@student.chula.ac.th; Scientific Parallel Computer Engineering Lab, Department of Computer Engineering, Chulalongkorn University, Bangkok, Thailand. veera.m@chula.ac.th","2006 IEEE International Conference on Cluster Computing","","2006","","","1","6","Workflow-based parameter-sweep applications are an important class of parallel jobs on clusters and grid today. Conventional batch schedulers and parameter study tools are not effective for this type of application. Especially, their scheduling policies are usually designed to minimize the makespan of the whole parameter study. However, many parameter-sweep applications also have a primary objective to obtain the best or a few top-ranked results from a large parameter space. This paper describes a new heuristic for scheduling parameter-sweep workflows in order to minimize the turnaround time of the workflows that give the best results. The algorithm is based on dynamically adjusted priority according to intermediate data obtained at some stage in the workflow. The technique is applied on a high-throughput drug screening application. The experimental results show that our technique can significantly improve the correlation between the ranking of the final results and the order of completion of the workflows","1552-5244;2168-9253","1-4244-0328-61-4244-0327","10.1109/CLUSTR.2006.311871","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4100377","","Concurrent computing;Application software;Processor scheduling;Drugs;Grid computing;Protein engineering;Dynamic scheduling;Computer applications;Genetic algorithms;Testing","processor scheduling","workflow-based parameter-sweep scheduling;best-intermediate-result-first heuristic;parallel jobs;high-throughput drug screening application","","1","11","","","","","","IEEE","IEEE Conferences"
"A fuzzy multi-objective tabu-search method for a new bi-objective open shop scheduling problem","O. Seraj; R. Tavakkoli-Moghaddam; F. Jolai","Department of Industrial Engineering, College of Engineering, University of Tehran, P.O. Box 11155-4563, Iran; Department of Industrial Engineering, College of Engineering, University of Tehran, P.O. Box 11155-4563, Iran; Department of Industrial Engineering, College of Engineering, University of Tehran, P.O. Box 11155-4563, Iran","2009 International Conference on Computers & Industrial Engineering","","2009","","","164","169","This paper proposes a novel, bi-objective mixed-integer mathematical programming for an open shop scheduling problem (OSSP) that minimizes the mean tardiness and the mean completion time. To obtain the efficient (Pareto-optimal) solutions, a fuzzy multi-objective decision making (MODM) approach is applied. By the use of this approach, the related auxiliary single objective formulation can be achieved. Since the OSSP are known as a class of NP-hard problems, a tabu search (TS) method is thus used to solve several medium to large-sized instances in reasonable runtime. The efficiency of the results obtained by the proposed TS for small, medium and large-sized instances is evaluated by considering the corresponding overall satisfactory level of all objectives. Futhermore, the adaptability of the yielded solutions of the proposed TS for the small-sized instances is evaluated by comparing the results reported by the LINGO software. Several experiments on different-sized test problems are considered, and the related results show the ability of the proposed TS algorithm to converge to the efficient solutions.","","978-1-4244-4135-8978-1-4244-4136","10.1109/ICCIE.2009.5223549","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5223549","Open shop scheduling problem;Fuzzy multi-objective decision making;Tabu search","Job shop scheduling;Decision making;NP-hard problem;Processor scheduling;Genetic algorithms;Transportation;Industrial engineering;Educational institutions;Mathematical programming;Runtime","computational complexity;decision making;fuzzy set theory;job shop scheduling;Pareto optimisation;scheduling;search problems","fuzzy multiobjective tabu search method;biobjective open shop scheduling problem;mean tardiness minimization;mean completion time minimization;fuzzy multi-objective decision making;Pareto optimal solutions;NP-hard problems;tabu search method;LINGO software","","1","26","","","","","","IEEE","IEEE Conferences"
"Load Shedding for Window Joins on Multiple Data Streams","Y. Law; C. Zaniolo","Bioinformatics Institute, 30 Biopolis Street, Singapore 138671. lawyn@bii.a-star.edu.sg; Computer Science Dept., UCLA, Los Angeles, CA 90095, USA. zaniolo@cs.ucla.edu","2007 IEEE 23rd International Conference on Data Engineering Workshop","","2007","","","674","683","We consider the problem of semantic load shedding for continuous queries containing window joins on multiple data streams and propose a robust approach that is effective with the different semantic accuracy criteria that are required in different applications. In fact, our approach can be used to (i) maximize the number of output tuples produced by joins, and (ii) optimize the accuracy of complex aggregates estimates under uniform random sampling. We first consider the problem of computing maximal subsets of approximate window joins over multiple data streams. Previously proposed approaches are based on multiple pair-wise joins and, in their load-shedding decisions, disregard the content of streams outside the joined pairs. To overcome these limitations, we optimize our load-shedding policy using various predictors of the productivity of each tuple in the window. To minimize processing costs, we use a fast-and-light sketching technique to estimate the productivity of the tuples. We then show that our method can be generalized to produce statistically accurate samples, as needed in, e.g.. the computation of averages, quantiles. and stream mining queries. Tests performed on both synthetic and real-life data demonstrate that our method outperforms previous approaches, while requiring comparable amounts of time and space.","","978-1-4244-0831-3978-1-4244-0832","10.1109/ICDEW.2007.4401054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4401054","","Productivity;Sampling methods;Aggregates;Costs;Frequency;Bioinformatics;Computer science;Robustness;Application software;Testing","query processing","multiple data streams;semantic load shedding;continuous queries;complex aggregates estimates;uniform random sampling;approximate window joins;load-shedding policy","","2","17","","","","","","IEEE","IEEE Conferences"
"A GIS-like training algorithm for log-linear models with hidden variables","G. Heigold; T. Deselaers; R. Schluter; H. Ney","Lehrstuhl für Informatik 6 - Computer Science Department, RWTH Aachen University, Germany; Lehrstuhl für Informatik 6 - Computer Science Department, RWTH Aachen University, Germany; Lehrstuhl für Informatik 6 - Computer Science Department, RWTH Aachen University, Germany; Lehrstuhl für Informatik 6 - Computer Science Department, RWTH Aachen University, Germany","2008 IEEE International Conference on Acoustics, Speech and Signal Processing","","2008","","","4045","4048","Conditional random fields (CRFs) are often estimated using an entropy based criterion in combination with generalized iterative scaling (GIS). GIS offers, upon others, the immediate advantages that it is locally convergent, completely parameter free, and guarantees an improvement of the criterion in each step. GIS, however, is limited in two aspects. GIS cannot be applied when the model incorporates hidden variables, and it can only be applied to optimize the maximum mutual information criterion (MMI). Here, we extend the GIS algorithm to resolve these two limitations. The new approach allows for training log-linear models with hidden variables and optimizes discriminative training criteria different from maximum mutual information (MMI), including minimum phone error (MPE). The proposed GIS-like method shares the above-mentioned theoretical properties of GIS. The framework is tested for optical character recognition on the USPS task, and for speech recognition on the Sietill task for continuous digit string recognition.","1520-6149;2379-190X","978-1-4244-1483-3978-1-4244-1484","10.1109/ICASSP.2008.4518542","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4518542","speech recognition;parameter estimation;maximum entropy;GIS;optical character recognition","Geographic Information Systems;Entropy;Character recognition;Speech recognition;Mutual information;Optical character recognition software;Computer science;Testing;Parameter estimation;Pattern recognition","character recognition;speech recognition","GIS-like training algorithm;log-linear models;conditional random fields;entropy based criterion;generalized iterative scaling;maxmimum mutual information criterion;discriminative training criteria;minimum phone error;speech recognition;optical character recognition;continuous digit string recognition","","2","14","","","","","","IEEE","IEEE Conferences"
"A high throughput binary arithmetic coding engine for h.264/avc","M. Li; W. Wu","NA; NA","2006 8th International Conference on Solid-State and Integrated Circuit Technology Proceedings","","2006","","","1914","1918","Context-based adaptive binary arithmetic coding (CABAC) adopted by the latest video coding standard- H.264/AVC main profile, can achieve good compression performance both in encoding and decoding H.264 syntax element. However, the binary coding engine of CABAC is bitwise in H.264/AVC, which leads to the limited throughput of CABAC encoder. Also the traditional iterative scheme for renormalization in binary arithmetic coding can result in degradation of coding efficiency. To avoid more costly operation in CABAC, an efficient coding design for handling high-resolution video sequence is strongly required. This paper proposes an optimized parallel processing approach and high throughput architecture design for renormalization in binary arithmetic coding engine, which is well suited for efficient software and hardware implementation over H.264/AVC HDTV encoder. Comparing with the traditional design, the results show that this novel approach makes a symbol encoded in one cycle; and for a set of test sequences, the throughput rate has been increased nearly 20% on average","","1-4244-0161-51-4244-0160","10.1109/ICSICT.2006.306505","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4098579","","Throughput;Arithmetic;Engines;Automatic voltage control;Video coding;Encoding;Iterative decoding;Degradation;Video sequences;Design optimization","adaptive codes;arithmetic codes;binary codes;high definition television;parallel processing;video codecs;video coding","H.264 standard;AVC standard;video coding;video sequences;parallel processing;context-based adaptive binary arithmetic coding;CABAC encoder;HDTV encoder;throughput rate","","1","6","","","","","","IEEE","IEEE Conferences"
"On-site analysis of transformer paper insulation using portable spectroscopy for chemometric prediction of aged condition","P. J. Baird; H. Herman; G. C. Stevens","University of Surrey; University of Surrey, GnoSys UK Ltd; University of Surrey, GnoSys UK Ltd","IEEE Transactions on Dielectrics and Electrical Insulation","","2008","15","4","1089","1099","Non-destructive diagnosis of power transformer condition is an increasingly important area of research. Power transformer coils are typically insulated with Kraft paper and immersed in mineral oil. There is a clear need to be able to estimate the aged condition of the coil insulation non-destructively. A portable fibre-optic spectroscopic probe system has been developed which can achieve this quickly by determining the degree of polymerization (DP), an indicator of degradation, to an accuracy of 30 DP units. The system can also determine water content to an accuracy of 0.1% w/w, and oil content and condition, very rapidly from the same measurements. The system uses a broadband spectroscopic diffuse reflectance probe operating in the visible and near infrared (NIR) range and interfaced to dedicated multivariate analysis software. Following calibration and measurement trials in the laboratory, the instrument has recently been adapted for use in the field, in which DP predictions have been made from in-situ measurements on the exposed windings of de-tanked transformers. These values have been validated when checked against DP measurements of the same paper taken in parallel using conventional viscometric methods. The system is now being further optimized and is being developed for other applications in the power industry and other industrial areas such as the paper industry and the plastics recycling industry. The system is modular and different probe types can be attached allowing the measurement of a wide variety of solid material surfaces and liquid media.","1070-9878;1558-4135","","10.1109/TDEI.2008.4591232","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4591232","Power transformers, transformer windings, paper insulation, aging, oil insulation, nondestructive testing, spectroscopy, multivariate statistical analysis","Power transformer insulation;Spectroscopy;Aging;Power transformers;Oil insulation;Probes;Coils;Petroleum;Plastics industry;Minerals","coils;paper;power transformer insulation;statistical analysis;transformer windings","transformer paper insulation;onsite analysis;Chemometric prediction;portable spectroscopy;nondestructive diagnosis;power transformer condition;mineral oil;coil insulation;fibre-optic spectroscopic probe system;broadband spectroscopic diffuse reflectance probe;near infrared range;multivariate analysis software;in-situ measurements;DP measurements;viscometric methods;plastics recycling industry;solid material surfaces;liquid media","","16","13","","","","","","IEEE","IEEE Journals & Magazines"
"Bringing flexibility to virtual screening for enzymatic inhibitors on the grid","M. J. Levesque; Kohei Ichikawa; Susumu Date; J. H. Haga","Department of Bioengineering, University of California, San Diego, La Jolla, USA; The Research Center of Socionetwork Strategies, Kansai University, Osaka, Japan; Cybermedia Center, Osaka University, Japan; Department of Bioengineering, University of California, San Diego, La Jolla, USA","2008 9th IEEE/ACM International Conference on Grid Computing","","2008","","","201","208","Virtual screening has become an important part of the drug discovery process. Grid computing facilitates this process by providing shared computational resources across different international institutions to run computationally intensive, scientific applications without the need for a centralized supercomputer. This study designed and implemented a flexible, scalable platform to perform a large virtual screening experiment on the PRAGMA grid testbed using the molecular docking simulation software DOCK 5.4. Using Opal OP to wrap DOCK as a grid service and PERL for data manipulation purposes, the ldquodruglikerdquo subset of the ZINC database, which contains 2,066,906 compounds, was successfully screened against the catalytic site of a protein tyrosine phosphatase. The screening required 11.56 days laboratory time and utilized 200 processors over 7 clusters. A ranked list of the best binding compounds to the phosphatase was generated and is currently being tested in biological applications for their efficacy and specificity.","2152-1085;2152-1093","978-1-4244-2578-5978-1-4244-2579","10.1109/GRID.2008.4662800","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4662800","","Compounds;Databases;Zinc;Computer architecture;Software;Availability;Web services","database management systems;drug delivery systems;grid computing;medical computing","virtual screening;enzymatic inhibitors;drug discovery process;grid computing;computational resources;PRAGMA grid testbed;molecular docking simulation software;DOCK 5.4;Opal OP;grid service;PERL;data manipulation purposes;ZINC database;protein tyrosine phosphatase","","1","36","","","","","","IEEE","IEEE Conferences"
"Transmission system expansion planning by a branch-and-bound algorithm","M. J. Rider; A. V. Garcia; R. Romero","NA; NA; NA","IET Generation, Transmission & Distribution","","2008","2","1","90","99","A branch and bound (B&amp;B) algorithm using the DC model, to solve the power system transmission expansion planning by incorporating the electrical losses in network modelling problem is presented. This is a mixed integer nonlinear programming (MINLP) problem, and in this approach, the so-called fathoming tests in the B&amp;B algorithm were redefined and a nonlinear programming (NLP) problem is solved in each node of the B&amp;B tree, using an interior-point method. Pseudocosts were used to manage the development of the B&amp;B tree and to decrease its size and the processing time. There is no guarantee of convergence towards global optimisation for the MINLP problem. However, preliminary tests show that the algorithm easily converges towards the best-known solutions or to the optimal solutions for all the tested systems neglecting the electrical losses. When the electrical losses are taken into account, the solution obtained using the Garver system is better than the best one known in the literature.","1751-8687;1751-8695","","10.1049/iet-gtd:20070090","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4436109","","Mathematical model;Planning;Algorithm design and analysis;Convergence;Integrated circuit modeling;Programming;Software","integer programming;losses;nonlinear programming;power transmission planning;tree searching","branch-and-bound algorithm;DC model;power system transmission expansion planning;network modelling problem;mixed integer nonlinear programming;interior-point method;electrical losses;Garver system","","31","","","","","","","IET","IET Journals & Magazines"
"Resource sharing control in Simultaneous MultiThreading microarchitectures","Chen Liu; J. Gaudiot","Department of Electrical and Computer Engineering and Computer Science, University of New Haven, West Haven, CT 06516, USA; Department of Electrical Engineering and Computer Science, University of California, Irvine, 92697, USA","2008 13th Asia-Pacific Computer Systems Architecture Conference","","2008","","","1","8","Simultaneous multithreading (SMT) achieves improved system resource utilization and accordingly higher instruction throughput because it exploits thread-level parallelism (TLP) in addition to conventional instruction-level parallelism (ILP). The key to high-performance SMT is to optimize the distribution of shared system resources among the threads. However, existing dynamic sharing mechanism has no control over the resource distribution, which could cause one thread to grab too many resources and clog the pipeline. Existing fetch policies address the resource distribution problem only indirectly. In this work, we strive to quantitatively determine the balance between controlling resource allocation and dynamic sharing of different system resources with their impact on the performance of SMT processors. We find that controlling the resource sharing of either the instruction fetch queue (IFQ) or the reorder buffer (ROB) is not sufficient if implemented alone. However, controlling the resource sharing of both IFQ and ROB can yield an average performance gain of 38% when compared with dynamic sharing case. The average L1 D-cache miss rate has been reduced by 33%. The average time that the instruction resides in the pipeline has been reduced by 34%. This demonstrates the power of the resource sharing control mechanism we propose.","","978-1-4244-2682-9978-1-4244-2683","10.1109/APCSAC.2008.4625432","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4625432","","Resource management;Pipelines;Benchmark testing;Program processors;Microarchitecture;Radiation detectors;Dynamic scheduling","parallel architectures;resource allocation;software architecture","resource sharing control;simultaneous multithreading microarchitectures;resource utilization;instruction-level parallelism;shared system resources;resource distribution;instruction fetch queue;reorder buffer","","","19","","","","","","IEEE","IEEE Conferences"
"Performance scalability of the JXTA P2P framework","G. Antoniu; L. Cudennec; M. Jan; M. Duigou","IRISA/INRIA, Campus de Beaulieu, 35042 Rennes Cedex, France; IRISA/INRIA, Campus de Beaulieu, 35042 Rennes Cedex, France; IRISA/INRIA, Campus de Beaulieu, 35042 Rennes Cedex, France. Mathieu.Jan@irisa.fr; Project JXTA, Sun Microsystems, Santa Clara, CA, U.S.A. Mike.Duigou@sun.com","2007 IEEE International Parallel and Distributed Processing Symposium","","2007","","","1","10","Features of the P2P model, such as scalability and volatility tolerance, have motivated its use in distributed systems. Several generic P2P libraries have been proposed for building distributed applications. However, very few experimental evaluations of these frameworks have been conducted, especially at large scales. Such experimental analyses are important, since they can help system designers to optimize P2P protocols and better understand the benefits of the P2P model. This is particularly important when the P2P model is applied to special use cases, such as grid computing. This paper focuses on the scalability of two main protocols proposed by the JXTA P2P platform. First, we provide a detailed description of the underlying mechanisms used by JXTA to manage its overlay and propagate messages over it: the rendezvous protocol. Second, we describe the discovery protocol used to find resources inside a JXTA network. We then report a detailed, large-scale, multi-site experimental evaluation of these protocols, using the nine clusters of the French Grid'5000 testbed.","1530-2075","1-4244-0909-81-4244-0910","10.1109/IPDPS.2007.370299","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4228027","","Scalability;Protocols;Libraries;Large-scale systems;Testing;Grid computing;Sun;Peer to peer computing;Building services;Costs","peer-to-peer computing;protocols;software libraries","performance scalability;JXTA P2P framework;distributed systems;P2P libraries;P2P protocols;grid computing;rendezvous protocol","","5","32","","","","","","IEEE","IEEE Conferences"
"Cognitive Radio Testbed: Exploiting Limited Feedback in Tomorrow's Wireless Communication Networks","C. Sokolowski; M. Petrova; A. de Baynast; P. Mahonen","NA; NA; NA; NA","ICC Workshops - 2008 IEEE International Conference on Communications Workshops","","2008","","","493","498","The next generation of wireless communication devices should support advanced features such as high spectral efficiency, broad bandwidth, diverse quality of service (QoS) requirements, and adaptivity. The cognitive radio (CR) is a new paradigm which has a high potential to become a basis for the future wireless systems. This paper is a first step towards the implementation of such a system. Our CR testbed is based on a GNU Radio platform which enables flexibility and reconfigurability of transmission parameters. As machine learning component, we invoke genetic algorithm (GA) to optimize the transmission parameters such as transmission power, modulation order and frequency channel based on the current spectrum conditions. Unlike other CR implementations, our approach requires very limited feedback information at the transmitter (ap 8 bits/packet duration). No transmission model nor additional network state information (NSI)is needed at the transmitter side. Experimentations show that our CR is capable to find free channels within 4-5 iterations even in a highly occupied spectrum scenario. It also offers the optimal trade-off between throughput, reliability, and power consumption depending on the user's QoS requirements.","2164-7038","978-1-4244-2052","10.1109/ICCW.2008.99","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4531947","","Cognitive radio;Testing;Feedback;Wireless communication;Chromium;Quality of service;Radio transmitters;Bandwidth;Machine learning;Genetic algorithms","genetic algorithms;learning (artificial intelligence);public domain software;quality of service;radiocommunication","cognitive radio testbed;wireless communication networks;wireless communication devices;GNU Radio platform;machine learning;genetic algorithm;network state information;QoS requirements","","6","18","","","","","","IEEE","IEEE Conferences"
"Evolution of NN for the Design of Virtual Agents under Limited Resources Constraints","J. J. Davila","School of Cognitive Science at Hampshire College, Amherst, MA 01002 USA. phone: 413-559-5687; fax: 413-559-5438 5438; e-mail: jdavila@hampshire.edu","2007 International Joint Conference on Neural Networks","","2007","","","2135","2140","This paper reports findings on a process for evolving neural networks capable of designing virtual agents. In particular, these virtual agents operate on a system of constrained resources, making the allocation of resources among them an important design consideration. The evolution system optimizes both neural network topologies and connection weights. The experimental results included indicate that the problem cannot be satisfactorily solved by independently evolving topologies or weights. In addition, because there is lack of a priori evidence pointing towards an optimal solution, the evolutionary process used here is able to find better solutions than either global (back propagation) or local (Hebbian) neural network learning algorithms.","2161-4393;2161-4407","978-1-4244-1379-9978-1-4244-1380","10.1109/IJCNN.2007.4371288","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4371288","","Neural networks;Network topology;Random number generation;Resource management;Medical treatment;Personnel;Hemorrhaging;Back;System testing;Layout","neural nets;resource allocation;software agents","virtual agents design;limited resources constraints;evolving neural networks;constrained resources system;resources allocation;neural network topologies;neural network learning algorithms","","","8","","","","","","IEEE","IEEE Conferences"
"A histogram-matching approach to the evolution of bin-packing strategies","R. Poli; J. Woodward; E. K. Burke","Department of Computer Science, University of Essex, UK; Department of Computer Science and IT, University of Nottingham, UK; Department of Computer Science and IT, University of Nottingham, UK","2007 IEEE Congress on Evolutionary Computation","","2007","","","3500","3507","We present a novel algorithm for the one- dimension offline bin packing problem with discrete item sizes based on the notion of matching the item-size histogram with the bin-gap histogram. The approach is controlled by a constructive heuristic function which decides how to prioritise items in order to minimise the difference between histograms. We evolve such a function using a form of linear register-based genetic programming system. We test our evolved heuristics and compare them with hand-designed ones, including the well- known best fit decreasing heuristic. The evolved heuristics are human-competitive, generally being able to outperform high- performance human-designed heuristics.","1089-778X;1941-0026","978-1-4244-1339-3978-1-4244-1340","10.1109/CEC.2007.4424926","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4424926","","Heuristic algorithms;Humans;Genetic programming;Algorithm design and analysis;Histograms;Computer science;Genetic algorithms;Testing;Computer industry;Application software","bin packing;genetic algorithms","histogram-matching approach;bin-packing strategies evolution;one-dimension offline bin packing problem;bin-gap histogram;constructive heuristic function;linear register-based genetic programming system","","13","11","","","","","","IEEE","IEEE Conferences"
"Two phase decision algorithm of replica allocation","Z. Chaoshu; L. Xinsong; W. Zheng; L. Yi","School of Computer Science and Engineering, Univ. of Electronic Science and Technology of China, Chengdu 610054, P. R. China; School of Computer Science and Engineering, Univ. of Electronic Science and Technology of China, Chengdu 610054, P. R. China; School of Computer Science and Engineering, Univ. of Electronic Science and Technology of China, Chengdu 610054, P. R. China; School of Computer Science and Engineering, Univ. of Electronic Science and Technology of China, Chengdu 610054, P. R. China","Journal of Systems Engineering and Electronics","","2006","17","1","206","212","In distributed parallel server system, location and redundancy of replicas have great influence on availability and efficiency of the system. In order to improve availability and efficiency of the system, two phase decision algorithm of replica allocation is proposed. The algorithm which makes use of auto-regression model dynamically predicts the future count of READ and WRITE operation, and then determines location and redundancy of replicas by considering availability, CPU and bands of the network. The algorithm can not only ensure the requirement of availability, but also reduce the system resources consumed by all the operations in a great scale. Analysis and test show that communication complexity and time complexity of the algorithm satisfy O(<inf>n</inf>), resource optimizing scale increases with the increase of READ count.","1004-4132","","10.1016/S1004-4132(06)60036-4","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6071319","distributed parallel server;replica allocation;two phase decision;performance optimizing","Prediction algorithms;Resource management;Availability;Algorithm design and analysis;Redundancy;Tin;Servers","","","","","","","","","","","BIAI","BIAI Journals & Magazines"
"The need for self-managed access nodes in grid environments.","R. Nou; F. Julia; J. Torres","Technical University of Catalonia, Barcelona, Spain; Technical University of Catalonia, Barcelona, Spain; Technical University of Catalonia, Barcelona, Spain","Fourth IEEE International Workshop on Engineering of Autonomic and Autonomous Systems (EASe'07)","","2007","","","5","13","The Grid is constantly growing and it is being used by more and more applications. In this scenario the entry node is an important component in the whole architecture and will become a contention point. In this paper we will demonstrate that the use of a self-managed layer on the entry node of a grid is necessary. A self-managed system can allow more jobs to be accepted and finished correctly. Since it's not acceptable for a grid middleware layer to lose jobs, we would normally need to prioritize the finishing/acceptance of jobs over the response time or the throughput. A prototype of what could be considered an autonomous system, is presented and tested over an installation of Globus Toolkit (GT4) and shows that we can greatly improve the performance of the original middleware by a factor of 30%. In this paper GT is used as an example but it could be added to any grid middleware","","0-7695-2809","10.1109/EASE.2007.25","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4148878","","Middleware;Computer architecture;Prototypes;Mathematical model;Application software;Finishing;Delay;Throughput;System testing;Network topology","grid computing;middleware","self-managed access node;grid environment;self-managed system;grid middleware layer;Globus Toolkit","","1","16","","","","","","IEEE","IEEE Conferences"
"Information Security and Task Interdependence: An Exploratory Investigation","K. J. Knapp; T. E. Marshall","U.S. Air Force Academy, USA; Auburn University, USA","2007 40th Annual Hawaii International Conference on System Sciences (HICSS'07)","","2007","","","246b","246b","This sequential qualitative-quantitative study investigates reported levels of task interdependence by certified information security professionals from organizations worldwide. The empirical tests show that information security ranked high in task interdependence compared to other information system related tasks. Additionally, comparing results from a different survey, information security work demonstrates higher levels of reported task interdependence than telecommunications software development work. We present the results of a demographic analysis of the survey taken by 936 certified information security professionals. Overall, the results suggest that information security work in organizations requires high levels of task interdependence. These findings have implications for researchers by identifying task interdependence-related topics for future study. For practitioners, these findings provide relevant insight into the nature of information security work in organizations","1530-1605","0-7695-2755","10.1109/HICSS.2007.273","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4076880","","Information security;National security;Protection;Data security;Demography;Risk management;Conference management;Educational institutions;System testing;Information systems","business data processing;organisational aspects;security of data","information security professional;task interdependence;information system;telecommunications software development work;demographic analysis","","2","30","","","","","","IEEE","IEEE Conferences"
"State-of-the-Art and Evolution in Public Data Sets and Competitions for System Identification, Time Series Prediction and Pattern Recognition","J. Vandewalle; J. Suykens; B. De Moor; A. Lendasse","Katholieke Universiteit Leuven, ESAT-SCD, Kasteelpark Arenberg 10, B-300 1 Leuven, Belgium. Email: Joos.Vandewalle@esat.kuleuven.be; Katholieke Universiteit Leuven, ESAT-SCD, Kasteelpark Arenberg 10, B-300 1 Leuven, Belgium; Katholieke Universiteit Leuven, ESAT-SCD, Kasteelpark Arenberg 10, B-300 1 Leuven, Belgium; Helsinki Univ. Techrn., Lab. Comp. and Inform. Sc., P.O.Box 5400 FIN-02015 HUT, Finland. Email: lendasse@hut.fi","2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07","","2007","4","","IV-1269","IV-1272","It is the aim of reproducible research to provide mechanisms for objective comparison of methods, algorithms, software and procedures in various research topics. In this paper, we discuss the role of data sets, benchmarks and competitions in the fields of system identification, time series prediction, classification, and pattern recognition in view of creating an environment of reproducible research. Important elements are the data sets, their origin, and the comparison measures that will be used to rank the performance of the methods. The issues are discussed, a comparison is made and recommendations are given.","1520-6149;2379-190X","1-4244-0727-31-4244-0728","10.1109/ICASSP.2007.367308","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4218339","Identification;pattern recognition;prediction methods;time series","System identification;Pattern recognition;Testing;Process design;Design methodology;Software algorithms;Prediction methods;Reproducibility of results;Time measurement;Performance analysis","data analysis;identification;pattern recognition;time series","public data sets;system identification;time series prediction;pattern recognition;reproducible research","","3","26","","","","","","IEEE","IEEE Conferences"
"Holistic Modelling of an Integrated Renewable Energy System Controller, Enabling Rapid Hardware Prototyping","A. P. Ruiz; M. Cirstea","Anglia Ruskin University, East Road, Cambridge, CB1 1PT, UK. AP347@student.apu.ac.uk; Anglia Ruskin University, East Road, Cambridge, CB1 1PT, UK. m.cirstea@apu.ac.uk","2006 International Symposium on Industrial Embedded Systems","","2006","","","1","4","The efficient control of a distributed energy resource (DER) hybrid systems is the key for the optimization of renewable sources used in stand alone generators. The system proposed in this paper is based on a wind/photovoltaic hybrid system with the possibility to integrate a fuel cell as energy backup. The research will result in the production and testing of the hardware prototype of an FPGA based controller for the integrated renewable power generation system. The system proposed uses DK4 (Celoxica) design and modelling software environment based on Handel-C/VHDL programming languages for the rapid prototyping of an FPGA controller.","2150-3109;2150-3117","0-7803-9759-21-4244-0777","10.1109/IES.2006.357479","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4197502","","Renewable energy resources;Control system synthesis;Hardware;Prototypes;Control systems;Wind energy generation;Photovoltaic systems;Software prototyping;Field programmable gate arrays;Distributed control","distributed power generation;field programmable gate arrays;intelligent control;photovoltaic power systems;power electronics;power engineering computing;wind power plants","holistic modelling;integrated renewable energy system controller;rapid hardware prototyping;distributed energy resource hybrid systems;stand alone generators;wind/photovoltaic hybrid system;fuel cell;energy backup;FPGA based controller;integrated renewable power generation system;DK4 design;Celoxica;Handel-C/VHDL programming languages","","","12","","","","","","IEEE","IEEE Conferences"
"Keyphrase extraction-based query expansion in digital libraries","I. Y. Song; R. B. Allen; Z. Obradovic; M. Song","Drexel University, Philadelphia, PA; Drexel University, Philadelphia, PA; Temple University, Philadelphia, PA; Temple University, Philadelphia, PA","Proceedings of the 6th ACM/IEEE-CS Joint Conference on Digital Libraries (JCDL '06)","","2006","","","202","209","In pseudo-relevance feedback, the two key factors affecting the retrieval performance most are the source from which expansion terms are generated and the method of ranking those expansion terms. In this paper, we present a novel unsupervised query expansion technique that utilizes keyphrases and POS phrase categorization. The keyphrases are extracted from the retrieved documents and weighted with an algorithm based on information gain and co-occurrence of phrases. The selected keyphrases are translated into disjunctive normal form (DNF) based on the POS phrase categorization technique for better query refomulation. Furthermore, we study whether ontologies such as WordNet and MeSH improve the retrieval performance in conjunction with the keyphrases. We test our techniques on TREC 5, 6, and 7 as well as a MEDLINE collection. The experimental results show that the use of keyphrases with POS phrase categorization produces the best average precision","","1-59593-354","10.1145/1141753.1141800","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4119126","POS;WordNet;information gain;keyphrase extraction;query expansion","Software libraries;Feedback;Information retrieval;Data mining;Information science;Ontologies;Educational institutions;Proteins;Testing;Speech","document handling;ontologies (artificial intelligence);query formulation;query processing;relevance feedback;unsupervised learning","keyphrase extraction;unsupervised query expansion technique;digital libraries;pseudorelevance feedback;retrieval performance;POS phrase categorization;documents retrieval;disjunctive normal form;query refomulation;ontologies;MEDLINE collection","","","29","","","","","","IEEE","IEEE Conferences"
"Recommending Resources in Mobile Personal Information Management","W. Woerndl; A. Hristov","NA; NA","2009 Third International Conference on Digital Society","","2009","","","149","154","We present an approach for personal information management for mobile devices like PDAs based on the Semantic Desktop. The main objective is to design and realize a recommendation system to identify interesting items (e.g. messages or documents) based on the current context (time and location) and a user's personal ontology. To do so, our algorithm uses an evaluation function to traverse the graph of resources and rank nodes. Relevant resources and other items such as points-of-interests can also be displayed on a map on the mobile device. The ideas have been implemented and (rudimentarily) tested in the ""SeMoDesk"" application.","","978-1-4244-3550-6978-0-7695-3526","10.1109/ICDS.2009.21","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4782867","personal information management;semantic desktop;mobile;recommender system","Information management;Ontologies;Personal digital assistants;Application software;Mobile computing;Information retrieval;Organizing;System testing;User interfaces;Prototypes","information management;information retrieval;mobile computing;ontologies (artificial intelligence)","mobile personal information management;mobile devices;semantic desktop;recommendation system;user personal ontology","","2","11","","","","","","IEEE","IEEE Conferences"
"Research on the Representation of the Programming Task Process of the Problem of ""farmer's dilemma""","L. Yongjian; C. huan; W. Lei; W. Bo; T. Qing","The School of Management, University of Electronic Science and Technology of China, Chengdu, China, 610054. lyj@uestc.edu.cn, swjtlyj@sina.com.cn; The School of Management, University of Electronic Science and Technology of China, Chengdu, China, 610054. cigo_ch@163.com, caohuan@uestc.edu.cn; The School of Management, University of Electronic Science and Technology of China, Chengdu, China, 610054. wanglei706@163.com, wanglei706@sina.com; The School of Management, University of Electronic Science and Technology of China, Chengdu, China, 610054. wangbo41@126.com; The School of Management, University of Electronic Science and Technology of China, Chengdu, China, 610054. haoqing92312@eyou.com","2006 International Conference on Service Systems and Service Management","","2006","1","","138","144","Anatomizing and describing knowledge task process (KTP) is the first step to improve the efficiency of knowledge work. From the angle of method optimizing and time-test, and on the basis of basic logic of motion time analysis, this paper interpreted knowledge task as a series of mind operations, and a procedure of information processing and computing in human's brain. Mind operation is considered as the therblig and algorithm of knowledge task. A programming process of ""farmer's dilemma"" was given as an example to illustrate and analyze KTP by the concept of mind operation with the principle of scientific management and computing theory. This paper characterized the way of analyzing KTP by mind operation and described the paradigm of such analysis in the mean time","2161-1890;2161-1904","1-4244-0451-71-4244-0450","10.1109/ICSSSM.2006.320602","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4114422","knowledge task (KT);mind operation (MO);therblig;knowledge management","Knowledge management;Technology management;Productivity;Optimization methods;Motion analysis;Software engineering;Logic programming;Information analysis;Time series analysis;Information processing","game theory;human factors;knowledge management;programming","knowledge task process;motion time analysis;information processing;programming task process;mind operation;scientific management;knowledge management;farmer dilemma","","1","32","","","","","","IEEE","IEEE Conferences"
"Initial investigation of an automatic registration algorithm for surgical navigation","G. J. Bootsma; J. H. Siewerdsen; M. J. Daly; D. A. Jaffray","Department of Medical Biophysics, University of Toronto, ON M5G 2M9 CANADA; Ontario Cancer Institute, Princess Margaret Hospital, Toronto, M5G 2M9 CANADA; Ontario Cancer Institute, Princess Margaret Hospital, Toronto, M5G 2M9 CANADA; Department of Radiation Physics, Princess Margaret Hospital, Toronto, ON M5G 2M9 CANADA","2008 30th Annual International Conference of the IEEE Engineering in Medicine and Biology Society","","2008","","","3638","3642","The procedure required for registering a surgical navigation system prior to use in a surgical procedure is conventionally a time-consuming manual process that is prone to human errors and must be repeated as necessary through the course of a procedure. The conventional procedure becomes even more time consuming when intra-operative 3D imaging such as the C-arm cone-beam CT (CBCT) is introduced, as each updated volume set requires a new registration. To improve the speed and accuracy of registering image and world reference frames in image-guided surgery, a novel automatic registration algorithm was developed and investigated. The surgical navigation system consists of either Polaris (Northern Digital Inc., Waterloo, ON) or MicronTracker (Claron Technology Inc., Toronto, ON) tracking camera(s), custom software (Cogito running on a PC), and a prototype CBCT imaging system based on a mobile isocentric C-arm (Siemens, Erlangen, Germany). Experiments were conducted to test the accuracy of automatic registration methods for both the MicronTracker and Polaris tracking cameras. Results indicate the automated registration performs as well as the manual registration procedure using either the Claron or Polaris camera. The average root-mean-squared (rms) observed target registration error (TRE) for the manual procedure was 2.58 +/− 0.42 mm and 1.76 +/− 0.49 mm for the Polaris and MicronTracker, respectively. The mean observed TRE for the automatic algorithm was 2.11 +/− 0.13 and 2.03 +/− 0.3 mm for the Polaris and MicronTracker, respectively. Implementation and optimization of the automatic registration technique in Carm CBCT guidance of surgical procedures is underway.","1094-687X;1558-4615","978-1-4244-1814-5978-1-4244-1815","10.1109/IEMBS.2008.4649996","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4649996","","","","","Algorithms;Automatic Data Processing;Equipment Design;Humans;Models, Statistical;Pattern Recognition, Automated;Photography;Reproducibility of Results;Signal Processing, Computer-Assisted;Software;Surgery, Computer-Assisted;Surgery, Computer-Assisted;Tomography Scanners, X-Ray Computed;Tomography, X-Ray Computed","3","19","","","","","","IEEE","IEEE Conferences"
"Design of advanced neuroscience platform","W. Liu; M. S. Chae; Z. Yang; H. Kim","The Department of Electrical Engineering of University of California at Santa Cruz, Santa Cruz, CA 95064 USA; The Department of Electrical Engineering of University of California at Santa Cruz, Santa Cruz, CA 95064 USA; The Department of Electrical Engineering of University of California at Santa Cruz, Santa Cruz, CA 95064 USA; The Department of Electrical Engineering of University of California at Santa Cruz, Santa Cruz, CA 95064 USA","2009 Annual International Conference of the IEEE Engineering in Medicine and Biology Society","","2009","","","5535","5538","This paper presents the design of versatile platform for advanced neuroscience on the high-level brain functions and neural prostheses. The platform enables researchers to record and stimulate brain activities of multiple free behaving animals wirelessly. The platform consists of three major functional blocks of neural interface, wireless communication system, and neural signal processing software, which of each has fundamental challenges to be overcome. Ultra wideband communication system makes it possible to transmit a large amount of data produced by high-density microelectrode wirelessly and miniaturize the entire system so that it can be implanted or carried by animals. Multiple access capability is achieved by a sophisticated communication protocol optimized for biomedical applications. Noise is one of the most important factors affecting the design of the neural interface and neural signal processing technique to reduce the noise is presented. Prototype chips were fabricated to verify the feasibility of the platform and test results are shown as well.","1094-687X;1558-4615","978-1-4244-3296","10.1109/IEMBS.2009.5333190","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5333190","Neuroscience;neural prostheses;platform;ultra wideband;neural signal processing;and noise","Neuroscience;Animals;Biomedical signal processing;Noise reduction;Prosthetics;Brain;Wireless communication;Communication system software;Ultra wideband communication;Microelectrodes","medical signal processing;microelectrodes;neurophysiology;prosthetics;ultra wideband communication","advanced neuroscience platform;high-level brain functions;neural prostheses;brain activities;neural interface;wireless communication system;neural signal processing software;ultrawideband communication system;microelectrode;implant;noise","Algorithms;Animals;Brain;Cats;Electric Stimulation;Electroencephalography;Equipment Design;Equipment Failure Analysis;Neurosciences;Reproducibility of Results;Sensitivity and Specificity;Signal Processing, Computer-Assisted;Systems Integration;Telemetry;User-Computer Interface","1","14","","","","","","IEEE","IEEE Conferences"
"An Open Source Parametric Propeller Design Tool","L. K. P. D'Epagnier; H. Chung; M. J. Stanway; R. W. Kimball","USN, Massachusetts Institute of Technology, 77 Massachusetts Avenue Room 5-309, Cambridge, MA 02139 USA; Woods Hole Oceanographic Institute, 77 Massachusetts Avenue Room 5-309, Cambridge, MA 02139 USA. kdepagnier@alum.mit.edu, squall@mit.edu; Massachusetts Institute of Technology, 77 Massachusetts Avenue Room 5-309, Cambridge, MA 02139 USA. chungh@alum.mit.edu, squall@mit.edu; Massachusetts Institute of Technology, 77 Massachusetts Avenue Room 5-309, Cambridge, MA 02139 USA; Woods Hole Oceanographic Institute, 77 Massachusetts Avenue Room 5-309, Cambridge, MA 02139 USA. squall@mit.edu; Massachusetts Institute of Technology, 77 Massachusetts Avenue Room 5-309, Cambridge, MA 02139 USA; Maine Maritime Academy, 77 Massachusetts Avenue Room 5-309, Cambridge, MA 02139 USA. kimball@mit.edu, squall@mit.edu","OCEANS 2007","","2007","","","1","8","An open source computational propeller/turbine design tool, the user-friendly open source MIT Propeller Vortex Lattice Lifting Line Program (OpenProp), is presented in this paper. This code has applications in propeller design for AUV and ROV thrusters as well as conventional propellers. The code is also being utilized for tidal turbine design. OpenProp is designed to be a fast parametric design tool for use by engineers with little training in propeller design. This tool can also be used by more experienced designers, as a preliminary design tool, to produce a starting design for further refinement using more advanced design and analysis codes. Various design examples are presented, including an ROV propeller design and a contra-rotating AUV Thruster design. These propellers were constructed and performance tested. The performance data for these propeller designs is also presented. The code was validated against the US Navy's PLL code and these results are also presented. The OpenProp propeller design tool is part of a suite of open source tools under development for rapid design, building and testing of propeller design models.","0197-7385","978-0933957-35","10.1109/OCEANS.2007.4449400","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4449400","","Propellers;Blades;Turbines;Graphical user interfaces;Design optimization;Geometry;Lattices;Open source software;Mathematical model;Remotely operated vehicles","geophysics computing;marine engineering;mathematics computing;oceanographic techniques;propellers;remotely operated vehicles;underwater vehicles","open source computational propeller design tool;open source MIT Propeller Vortex Lattice Lifting Line Program;OpenProp;conventional propellers;tidal turbine design;parametric design tool;ROV propeller design;contra-rotating AUV Thruster design;US Navy PLL code","","2","10","","","","","","IEEE","IEEE Conferences"
"Control scheme for a multiphase matrix converter EUROCON2009","J. Szczepanik; T. Sienko","Cracow University of Technology, Faculty of Electrical and Computer Engineering, Department of Electromechanical Energy Conversion, ul. Warszawska 24, 31-155 Kraków, Poland; Cracow University of Technology, Faculty of Electrical and Computer Engineering, Department of Electromechanical Energy Conversion, ul. Warszawska 24, 31-155 Kraków, Poland","IEEE EUROCON 2009","","2009","","","545","551","A matrix NxM multi phase converter is a very simple structure incorporating NxM bi-directional switches, able to convert input voltages into output voltages of any shape and frequency. However, commutation problems and complicated control algorithms keep it from being utilized on a large scale. The paper concentrates on the development of a control algorithm for a multiphase matrix converter, which can fulfill all requirements imposed by the proposed application in power systems where sinusoidal shape of the voltage is important. The proposed algorithm is then tested using Matlab Simulink software and optimized for an application in digital controller.","","978-1-4244-3860-0978-1-4244-3861","10.1109/EURCON.2009.5167685","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5167685","Matrix converter;Multiphase matrix control procedure;Power system control","Matrix converters;Shape control;Application software;Switching converters;Bidirectional control;Switches;Frequency conversion;Large-scale systems;Voltage control;Control systems","digital control;matrix convertors;power system control","multiphase matrix converter;control scheme;NxM bidirectional switches;power system control;sinusoidal shape;Matlab Simulink software;digital controller","","","5","","","","","","IEEE","IEEE Conferences"
"Design of an ESD-Protected Ultra-Wideband LNA in Nanoscale CMOS for Full-Band Mobile TV Tuners","P. Mak; R. Martins","NA; NA","IEEE Transactions on Circuits and Systems I: Regular Papers","","2009","56","5","933","942","This paper presents an electrostatic discharge (ESD)-protected ultra-wideband (UWB) low-noise amplifier (LNA) for full-band (170-to-1700 MHz) mobile TV tuners. It features a PMOS-based open-source input structure to optimize the I/O swings under a mixed-voltage ESD protection while offering an inductorless broadband input impedance match. The amplification core exploiting double current reuse and single-stage thermal-noise cancellation enhances the gain and noise performances with high power efficiency. Optimized in a 90-nm 1.2/2.5-V CMOS process with practical issues taken into account, the LNA using a constant-g<sub>m</sub> bias circuit achieves competitive and robust performances over process, voltage and temperature variation. The simulated voltage gain is 20.6 dB, noise figure is 2.4 to 2.7 dB, and IIP3 is +10.8 dBm . The power consumption is 9.6 mW at 1.2 V. |S<sub>11</sub>| &lt; -10 dB is achieved up to 1.9 GHz without needing any external resonant network. Human Body Model ESD zapping tests of plusmn4 kV at the input pins cause no failure of any device.","1549-8328;1558-0806","","10.1109/TCSI.2009.2015185","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4783014","CMOS;electrostatic discharge (ESD);low-noise amplifier (LNA);mobile TV tuner;radio frequency (RF);thermal-noise cancellation;ultra-wideband (UWB)","Ultra wideband technology;Mobile TV;Tuners;Electrostatic discharge;Voltage;Low-noise amplifiers;Open source software;Protection;Impedance matching;Performance gain","CMOS integrated circuits;electric breakdown;electrostatic discharge;impedance matching;low noise amplifiers;mobile television;nanoelectronics;thermal noise;UHF amplifiers;UHF integrated circuits;ultra wideband technology","mixed-voltage ESD protection;ultra-wideband LNA;nanoscale CMOS;full-band mobile TV tuner;electrostatic discharge;low-noise amplifier;PMOS-based open-source input structure;inductorless broadband input impedance match;single-stage thermal-noise cancellation;double current reuse;human body model;ESD zapping test;frequency 170 MHz to 1700 MHz;size 90 nm;voltage 1.2 V;voltage 2.5 V;gain 20.6 dB;noise figure 2.4 dB to 2.7 dB;power 9.6 mW","","20","32","","","","","","IEEE","IEEE Journals & Magazines"
"High gain planar microstrip antenna at X-band","S. Chakrabarti","SAMEER Kolkata Centre, Plot-L2, Block-GP, Sector-V, Salt Lake Electronics Complex, 700091, India","2007 IEEE Applied Electromagnetics Conference (AEMC)","","2007","","","1","4","In this paper a simple technique to develop a wide bandwidth, high gain circularly polarised planar microstrip patch antenna at X-band frequency has been presented. A two layer electromagnetically coupled stacked microstrip circular patch (EMCP) antenna has been designed, fabricated and tested. The design optimisation was carried out using commercial MoM based simulation software. The inter layer separation and size of the patches of the circularly polarised antenna were optimised by successive simulations using the commercial software. Measured gain flatness was better than 1dB and impedance bandwidth was better than 10% around the centre frequency at X-band. Measured results are also presented.","","978-1-4244-1863-3978-1-4244-1864","10.1109/AEMC.2007.4638003","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4638003","EMCP antenna;Circularly polarised antenna;Wide bandwidth antenna;High gain antenna at X-band","Feeds;Gain;Microstrip antennas;Impedance;Antenna measurements;Bandwidth;Antenna feeds","method of moments;microstrip antennas;planar antennas","high gain planar microstrip antenna;X-band frequency;electromagnetically coupled stacked circular antenna;commercial MoM based simulation software","","1","6","","","","","","IEEE","IEEE Conferences"
"Minimizing network cost in all-optical networks","S. Saha; E. D. Manley; J. S. Deogun","Department of Computer Science and Engineering, University of Nebraska-Lincoln, 68588-0115, U.S.A.; Department of Mathematics and Computer Science, Drake University, Des Moines, IA 50311, U.S.A.; Department of Computer Science and Engineering, University of Nebraska-Lincoln, 68588-0115, U.S.A.","2009 IEEE 3rd International Symposium on Advanced Networks and Telecommunication Systems (ANTS)","","2009","","","1","3","The problem of minimizing the total network cost of an optical network topology by efficient selection of switching sites, size of optical switches, and optical links is investigated in this paper. The problem investigated is NP-hard. Therefore, we develop an efficient heuristic to approximate the solution in polynomial time. A mixed integer quadratic programming (MIQP) formulation of the problem is also presented to find the optimal network cost and compute the efficiency of the heuristic. The total network cost calculated by the heuristic in the experiments is within 19% of its optimal value. Moreover, the total network cost in half of the test problems is within 6% of its optimal value. The heuristic solves the problem with 20 node topologies in less than a second. However, the commercial optimization software can not solve any problem with more than 10 nodes even in two weeks.","2153-1684;2153-1676;2153-1676","978-1-4244-5991-9978-1-4244-5989-6978-1-4244-5990","10.1109/ANTS.2009.5409862","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5409862","","All-optical networks;Cost function;Network topology;Optical fiber networks;Optical switches;Optical fiber communication;Polynomials;Quadratic programming;Computer networks;Testing","integer programming;optical fibre networks;optical links;optical switches;polynomial approximation;telecommunication network topology","network cost;all-optical networks;optical network topology;optical switches;optical links;NP-hard;mixed integer quadratic programming","","4","9","","","","","","IEEE","IEEE Conferences"
"Ms Pac-Man versus ghost-team competition","S. M. Lucas","NA","2009 IEEE Symposium on Computational Intelligence and Games","","2009","","","1","1","Ms. Pac-Man is a challenging and interesting game that is usually viewed form the perspective of the Ms. Pac-Man agent. The game is also provides an excellent environment for testing multi-agent strategies for controlling the team of ghosts, with the aim either of optimising the playability of the game, or simply to minimise the score obtained by the agent. It is this latter more measurable objective that provides the focus of this competition. For this purpose an efficient simulator of the game has been developed with simple software interfaces with which to connect your controllers. The screenshot below shows a game in progress on the simulator. You can enter an agent controller or a ghost-team controller (or even both). The aim of the Ms. Pac-Man agent is to score as many points as possible. The aim of the ghost team controller is to prevent this!","2325-4270;2325-4289","978-1-4244-4814-2978-1-4244-4815","10.1109/CIG.2009.5286502","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5286502","","Testing;Humans;Australia;Permission;Advertising;IEEE catalog;Libraries","","","","1","","","","","","","IEEE","IEEE Conferences"
"Screen-capture Ms Pac-Man","S. M. Lucas","NA","2009 IEEE Symposium on Computational Intelligence and Games","","2009","","","1","1","Ms. Pac-Man is a challenging and interesting game that is usually viewed form the perspective of the Ms. Pac-Man agent. The game also provides an excellent environment for testing multi-agent strategies for controlling the team of ghosts, with the aim either of optimising the playability of the game, or simply to minimise the score obtained by the agent. It is this latter more measurable objective that provides the focus for this competition. For this purpose an efficient simulator of the game has been developed with simple software interfaces with which to connect your controllers. The screenshot below shows a game in progress on the simulator. You can enter an agent controller or a ghost-team controller (or even both). The aim of the Ms. Pac-Man agent is to score as many points as possible. The aim of the ghost team controller is to prevent this!","2325-4270;2325-4289","978-1-4244-4814-2978-1-4244-4815","10.1109/CIG.2009.5286506","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5286506","","Artificial intelligence;Permission;Advertising;IEEE catalog;Libraries;Java;Testing;Focusing","","","","","","","","","","","IEEE","IEEE Conferences"
"A PDA-based Research Platform for Cochlear Implants","A. P. Lobo; P. C. Loizou; N. Kehtarnavaz; M. Torlak; H. Lee; A. Sharma; P. Gilley; V. Peddigari; L. Ramanna","Department of Electrical Engineering, University of Texas at Dallas, Richardson, TX 75080 USA. phone: 972-883-4650; fax: 972-883-2710; e-mail: arthur.lobo@utdallas.edu; Department of Electrical Engineering, University of Texas at Dallas, Richardson, TX 75080 USA. phone: 972-883-4650; fax: 972-883-2710; e-mail: loizou@utdallas.edu; Department of Electrical Engineering, University of Texas at Dallas, Richardson, TX 75080 USA; Department of Electrical Engineering, University of Texas at Dallas, Richardson, TX 75080 USA; Department of Electrical Engineering, University of Texas at Dallas, Richardson, TX 75080 USA; Department of Speech Language and Hearing Science, University of Colorado, Boulder, CO 80309 USA. e-mail: anu.sharma@Colorado.EDU; Department of Speech Language and Hearing Science, University of Colorado, Boulder, CO 80309 USA; Department of Electrical Engineering, University of Texas at Dallas, Richardson, TX 75080 USA; Department of Electrical Engineering, University of Texas at Dallas, Richardson, TX 75080 USA","2007 3rd International IEEE/EMBS Conference on Neural Engineering","","2007","","","28","31","Currently researchers interested in developing new signal processing algorithms for commercially available cochlear implants must rely on coding these algorithms in low-level assembly language. We propose a personal digital assistant (PDA) based research platform for developing and testing in real-time new signal processing strategies for cochlear implants. Software development can be done either in C or in LabVIEW. The C implementation can be further optimized using Intel's primitive routines. In this paper, we report on the real-time implementation of a 16-channel noise-band vocoder algorithm, which is a similar algorithm used in commercially available implant processors. We further report on EEG recordings on the PDA acquired through a compact-flash data acquisition card.","1948-3546;1948-3554","1-4244-0791-51-4244-0792","10.1109/CNE.2007.369603","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4227208","","Cochlear implants;Signal processing algorithms;Personal digital assistants;Assembly;Testing;Digital signal processing;Programming;Vocoders;Electroencephalography;Data acquisition","data acquisition;electroencephalography;handicapped aids;medical signal processing;notebook computers;prosthetics;vocoders","PDA-based research platform;cochlear implants;low-level assembly language;personal digital assistant;real-time new signal processing;software development;16-channel noise-band vocoder algorithm;implant processors;EEG recordings;compact-flash data acquisition card","","11","6","","","","","","IEEE","IEEE Conferences"
"An application based MPI message throughput benchmark","B. W. Barrett; K. S. Hemmert","Sandia National Laboratories, P.O. Box 5800, Albuquerque, NM 87185-1319; Sandia National Laboratories, P.O. Box 5800, Albuquerque, NM 87185-1319","2009 IEEE International Conference on Cluster Computing and Workshops","","2009","","","1","8","Recent trends in high performance computing have renewed interest in the ability of platforms to sustain high message throughput rates. The continued growth in platform scale, combined with emerging application areas, are pushing platforms to support increasing message rates. Best-case message throughput has grown in previous hardware generations due to growing clock rates and software optimization techniques. However, previous work has shown that MPI receive queue length and cache hit rates can drastically impact message throughput, leading to a significantly lower worst-case message throughput. This paper introduces the Sandia message throughput benchmark which measures message throughput using a communication pattern which is neither best-case nor worst-case, but which mimics communication patterns found in real-world applications. Results on InfiniBand, Myrinet, and Cray XT platforms are presented, and suggest that message rates on some networks are greatly impacted by cache invalidation between communication phases, simultaneously sending and receiving, and by communicating with more than one peer simultaneously.","1552-5244;2168-9253","978-1-4244-5011-4978-1-4244-5012","10.1109/CLUSTR.2009.5289198","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5289198","","Throughput;Bandwidth;Application software;Hardware;Delay;Benchmark testing;Scalability;Laboratories;Electronic mail;High performance computing","cache storage;message passing","MPI message throughput;high performance computing;Sandia message throughput benchmark;InfiniBand;Myrinet;Cray XT platform;cache invalidation","","4","17","","","","","","IEEE","IEEE Conferences"
"Experimental framework for evaluating autonomous guidance and control algorithms for agile aerial vehicles","N. Kundak; B. Mettler","Department of Aerospace Engineering and Mechanics, University of Minnesota Twin Cities, 107 Akerman Hall 110 Union St SE, Minneapolis, 55455, USA; Department of Aerospace Engineering and Mechanics, University of Minnesota Twin Cities, 107 Akerman Hall 110 Union St SE, Minneapolis, 55455, USA","2007 European Control Conference (ECC)","","2007","","","293","300","This paper describes the design of an indoor experimental setup for evaluating autonomous guidance and control algorithms for agile aerial vehicles operating in challenging 3D environments and flight conditions. The setup is based on a vision tracking system which dispenses the need for onboard instrumentation making it possible to use highly miniaturized R/C rotorcraft and airplanes. Thanks to the small scale and the indoor setting it is possible to create realistic testing conditions in a controlled fashion. The facility will help develop a methodological framework and benchmark to evaluate the performance of planning algorithms. In addition, the experimental setup provides high quality flight data needed to accurately model small-scale aircraft. It also, provides a unique environment to study human pilot performance and conduct interactive experiments involving piloted and autonomous vehicles.","","978-3-9524173-8","10.23919/ECC.2007.7068647","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7068647","Unmanned aerial vehicles;autonomy;control;trajectory optimization","Vehicles;Helicopters;Vehicle dynamics;Rotors;Planning;Software;Trajectory","aircraft control;computer vision;helicopters;object tracking;path planning","autonomous guidance algorithm;control algorithm;agile aerial vehicle;3D environment;flight condition;vision tracking system;methodological framework;planning algorithm;highly miniaturized R/C rotorcraft;airplanes","","","27","","","","","","IEEE","IEEE Conferences"
"A Task-Pool Parallel I/O Paradigm for an I/O Intensive Application","J. Li; L. Yan; Z. Gao; D. Hei","NA; NA; NA; NA","2009 IEEE International Symposium on Parallel and Distributed Processing with Applications","","2009","","","679","684","In regards to applications like 3D seismic migration, it is quite important to improve the I/O performance within an cluster computing system. Such seismic data processing applications are the I/O intensive applications. For example, large 3D data volume cannot be hold totally in computer memories. Therefore the input data files have to be divided into many fine-grained chunks. Intermediate results are written out at various stages during the execution, and final results are written out by the master process. This paper describes a novel manner for optimizing the parallel I/O data access strategy and load balancing for the above mentioned particular program model. The optimization, based on the application defined API, reduces the number of I/O operations and communication (as compared to the original model). This is done by forming groups of threads with ""group roots"", so to speak, that read input data (determined by an index retrieved from the master process) and then send it to their group members. In the original model, each process/thread reads the whole input data and outputs its own results. Moreover the loads are balanced, for the online dynamic scheduling of access request to process the migration data. Finally, in the actual performance test, the improvement of performance is often more than 60% by comparison with the original model.","2158-9178;2158-9208","978-0-7695-3747","10.1109/ISPA.2009.20","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5207859","task-pool;parallel I/O;load-balancing;I/O intensive","Application software;Data processing;Computer science;Load management;High performance computing;Concurrent computing;Yarn;Petroleum;Distributed processing;Distributed computing","administrative data processing;application program interfaces;dynamic scheduling;input-output programs;resource allocation","task-pool parallel I/O paradigm;I/O intensive application;seismic data processing application;3D data volume;computer memory;parallel I/O data access strategy optimisation;API;application program interface;group roots forming;input/output program","","","10","","","","","","IEEE","IEEE Conferences"
"On-line neuroevolution applied to The Open Racing Car Simulator","L. Cardamone; D. Loiacono; P. L. Lanzi","Dipartimento di Elettronica e Informazione, Politecnico di Milano, Italy; Dipartimento di Elettronica e Informazione, Politecnico di Milano, Italy; Dipartimento di Elettronica e Informazione, Politecnico di Milano, Italy","2009 IEEE Congress on Evolutionary Computation","","2009","","","2622","2629","The application of on-line learning techniques to modern computer games is a promising research direction. In fact, they can be used to improve the game experience and to achieve a true adaptive game AI. So far, several works proved that neuroevolution techniques can be successfully applied to modern computer games but they are usually restricted to offline learning scenarios. In on-line learning problems the main challenge is to find a good trade-off between the exploration, i.e., the search for better solutions, and the exploitation of the best solution discovered so far. In this paper we propose an on-line neuroevolution approach to evolve non-player characters in The Open Car Racing Simulator (TORCS), a state-of-the-art open source car racing simulator. We tested our approach on two on-line learning problems: (i) on-line evolution of a fast controller from scratch and (ii) optimization of an existing controller for a new track. Our results show that on-line neuroevolution can effectively improve the performance achieved during the learning process.","1089-778X;1941-0026","978-1-4244-2958-5978-1-4244-2959","10.1109/CEC.2009.4983271","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4983271","","Computational modeling;Artificial intelligence;Learning;Computer simulation;Application software;Testing;Computational intelligence;Stochastic processes;Analytical models;Physics computing","computer games;Internet;learning (artificial intelligence);neural net architecture","online neuroevolution;open racing car simulator;online learning technique;computer games;true adaptive game AI;offline learning scenario;online learning problem;nonplayer characters;The Open Car Racing Simulator;open source car racing simulator;online evolution","","20","20","","","","","","IEEE","IEEE Conferences"
"Robust and Fast Keypoint Recognition Based on SE-FAST","X. Tan; X. Yang; S. Xiao","NA; NA; NA","2009 Eighth IEEE International Conference on Dependable, Autonomic and Secure Computing","","2009","","","188","193","In this paper, we present a key point recognition scheme, which consists of a novel feature detector and an efficient descriptor. Inspired by FAST (features from accelerated segment test), our feature detector is easy to compute and has high repeatability. Scale-invariance and optimized robustness are gained by extending traditional FAST to scale space.We combine this detector with an adapted version of SURF (speed up robust features) descriptor, providing the system with all means to do feature matching and object detection. Experimental evaluation and comparison with standard SURF using Hessian matrix-based detector are included in this paper, showing improvement in speed with comparable robustness.","","978-1-4244-5421-1978-1-4244-5420-4978-0-7695-3929","10.1109/DASC.2009.32","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5380322","recognition;feature detection;augment reality","Robustness;Computer vision;Detectors;Object detection;Lighting;Testing;Cameras;Laboratories;Intelligent systems;Software systems","computer vision;Hessian matrices;image recognition;object detection","fast keypoint recognition;SE-FAST;feature detector;speed up robust feature descriptor;SURF;feature matching;object detection;Hessian matrix-based detector;computer vision","","","13","","","","","","IEEE","IEEE Conferences"
"Noncontiguous locking techniques for parallel file systems","A. Ching; W. Liao; A. Choudhary; R. Ross; L. Ward","Northwestern University, Evanston, Illinois; Northwestern University, Evanston, Illinois; Northwestern University, Evanston, Illinois; Argonne National Laboratory, Argonne, IL; Sandia National Laboratories, Albuquerque, NM","SC '07: Proceedings of the 2007 ACM/IEEE Conference on Supercomputing","","2007","","","1","12","Many parallel scientific applications use high-level I/O APIs that offer atomic I/O capabilities. Atomic I/O in current parallel file systems is often slow when multiple processes simultaneously access interleaved, shared files. Current atomic I/O solutions are not optimized for handling noncontiguous access patterns because current locking systems have a fixed file system block-based granularity and do not leverage high-level access pattern information. In this paper we present a hybrid lock protocol that takes advantage of new list and datatype byte-range lock description techniques to enable high performance atomic I/O operations for these challenging access patterns. We implement our scalable distributed lock manager (DLM) in the PVFS parallel file system and show that these techniques improve locking throughput over a naive noncontiguous locking approach by several orders of magnitude in an array of lock-only tests. Additionally, in two scientific I/O benchmarks, we show the benefits of avoiding false sharing with our byte-range granular DLM when compared against a block-based lock system implementation.","","978-1-59593-764-3978-1-59593-764","10.1145/1362622.1362658","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5348817","","File systems;Computational modeling;Government;Laboratories;Concurrent computing;Access protocols;Libraries;Application software;Throughput;System testing","","","","4","33","","","","","","IEEE","IEEE Conferences"
"Efficiency and pseudo-randomness of a variant of Zémor-Tillich hash function","C. Petit; N. Veyrat-Charvillon; J. Quisquater","UCL Crypto Group, Place du Levant 3, B-1348 Louvain-La-Neuve (Belgium); UCL Crypto Group, Place du Levant 3, B-1348 Louvain-La-Neuve (Belgium); UCL Crypto Group, Place du Levant 3, B-1348 Louvain-La-Neuve (Belgium)","2008 15th IEEE International Conference on Electronics, Circuits and Systems","","2008","","","906","909","Recent breakthroughs concerning the current standard SHA-1 prompted NIST to launch a competition for a new secure hash algorithm by X.Wang et al (2005). Provably secure hash functions (in the sense that their security relates to the hardness of some mathematical problems by V. Lyubashevsky et al (2006) are particularly interesting from a theoretical point of view but are often much slower than heuristic functions like SHA. In this paper, we consider a variant of ZT hash, a provably secure hash function designed by Zemor and Tillich proposed by J.P Tilich and G.Zemor (1994). Despite some attack proposals, its security has not been fundamentally challenged to this day. Our function is twice as fast as ZT hash and has enhanced security properties. We propose optimized parameters and algorithms to increase the speed of both hash functions. This makes our function one of the most efficient ldquoprovably securerdquo hash functions to this day. Finally, we show that our hash function successfully passes most pseudo-randomness tests in the Dieharder suite.","","978-1-4244-2181-7978-1-4244-2182","10.1109/ICECS.2008.4675001","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4675001","","Cryptography;Security;Telephony;NIST;Distributed computing;Testing;Proposals;Arithmetic;Hardware;Application software","cryptography;random processes","pseudorandomness;Zemor-Tillich hash function","","2","14","","","","","","IEEE","IEEE Conferences"
"Comparisons of Space-Vector Modulation and Carrier-Based Modulation of Multilevel Inverter","W. Yao; H. Hu; Z. Lu","NA; NA; NA","IEEE Transactions on Power Electronics","","2008","23","1","45","51","This paper studies the relations of space-vector modulation (SVM) and carrier-based pulse-width modulation (PWM) for multilevel inverter. The PWMs' generation of SVM can be achieved by carrier-based PWM scheme, but the modulated wave of SVM is acquired by vectors' calculations and switching-states' selection. Based on different selection of redundant switching-states, there are many types of SVM modulated waves, some of which can function equivalently through proper selection of common-mode injections in the case of carrier-based PWM, the others have more freedoms in switching-states' selection than carrier-based PWM. Selection of more switching-states in SVM is propitious to optimize the output voltage, balance the dc power and so on. Then an improved PWM scheme is proposed based on the modulation waves of three-level SVM, which reserves the main advantages of SVM, and can be achieved easily. Finally, a five-level test circuit is built to verify this PWM scheme.","0885-8993;1941-0107","","10.1109/TPEL.2007.911865","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4400131","Carrier-based pulse-width modulation (PWM);multilevel inverter;space vector modulation (SVM)","Pulse width modulation inverters;Support vector machines;Pulse width modulation;Space vector pulse width modulation;Voltage;Pulse inverters;Circuit testing;Power system harmonics;Hardware;Software performance","PWM invertors;switching convertors","space-vector modulation;carrier-based modulation;multilevel inverter;pulse-width modulation;PWM inverter","","186","22","","","","","","IEEE","IEEE Journals & Magazines"
"Simple Time-to-Failure Estimation Techniques for Reliability and Maintenance of Equipment","H. W. Penrose","Dreisilker Electric Motors, Inc.","IEEE Electrical Insulation Magazine","","2009","25","4","14","18","Proper reliability and maintenance best practice processes have a direct impact on equipment availability, throughput capacity, and spare inventories. The purpose of the time-to-failure estimation (TTFE) technique is to provide a tool for engineers and technicians for risk-based reporting of condition- based maintenance tests and inspections. Through the proper application of this technique, corrective action may be prioritized improving the effectiveness of the maintenance program. Instead of stakeholders being required to make decisions based upon experience only, equipment failure, and repair history can be used to enhance the process, improving the availability of critical equipment.","0883-7554;1558-4402","","10.1109/MEI.2009.5191412","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5191412","Reliability and maintenance, time-to-failure estimation, reactive maintenance practices, predictive maintenance practices","Predictive maintenance;Costs;Economic indicators;Electric motors;Reliability engineering;Tribology;Aging;Electric breakdown;Coordinate measuring machines;Application software","condition monitoring;failure analysis;inspection;machine tools;maintenance engineering","time-to-failure estimation;equipment maintenance;equipment availability;equipment reliability;throughput capacity;spare inventories;condition-based maintenance;inspection;equipment failure","","6","10","","","","","","IEEE","IEEE Journals & Magazines"
"EDACluster: an Evolutionary Density and Grid-Based Clustering Algorithm","C. S. d. Oliveira; P. I. Godinho; A. S. G. Meiguins; B. S. Meiguins; A. A. Freitas","NA; NA; NA; NA; NA","Seventh International Conference on Intelligent Systems Design and Applications (ISDA 2007)","","2007","","","143","150","This paper presents EDACluster, an estimation of distribution algorithm (EDA) applied to the clustering task. EDA is an evolutionary algorithm used here to optimize the search for adequate clusters when very little is known about the target dataset. The proposed algorithm uses a mixed approach - density and grid- based - to identify sets of dense cells in the dataset. The output is a list of items and their associated clusters. Items in low-density areas are considered noise and are not assigned to any cluster. This work uses four public domain datasets to perform the tests that compare EDACluster with DBSCAN, a conventional density-based clustering algorithm.","2164-7143;2164-7151","0-7695-2976-3978-0-7695-2976","10.1109/ISDA.2007.118","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4389600","","Clustering algorithms;Electronic design automation and methodology;Algorithm design and analysis;Data mining;Databases;Distributed computing;Grid computing;Evolutionary computation;Testing;Application software","evolutionary computation;pattern clustering","EDACluster;evolutionary density;grid-based clustering algorithm;estimation of distribution algorithm;evolutionary algorithm;public domain datasets;density-based clustering algorithm","","2","23","","","","","","IEEE","IEEE Conferences"
"Techniques for building excellent operator machine interfaces (OMI)","P. Gorman; N. Pappas","The Boeing Company, Seattle, WA, USA.; The Boeing Company, Seattle, WA, USA.","IEEE Aerospace and Electronic Systems Magazine","","2009","24","10","17","22","Establishing a process to continually improve understanding of operator requirements - the why as well as the how - is key to developing an optimal Operator-Machine Interface (OMI) for a large program. The P-8A program is one example of a complex software development, with over 2 million lines of code and a unique and complex OMI. The OMI itself is used as a tool for achieving and is designed in parallel with understanding end goals, operator decision making chains, and thought processes. The OMI can be designed to support and anticipate information needs and alert operators to unusual occurrences. Operator actions and decision-making techniques can be discovered by using expert operators and capturing their knowledge through the use of multiple tools to identify the problem set: storytelling, decision mapping, and is/is not matrices. When a solution set has been identified and agreed to by the customer, resources allocated, and schedule set, a standard process captures and prioritizes further changes, identities their impact, and decides on implementation. Documents describing design and processes and a design description document describing the current version of OMI are made accessible to stakeholders at all times.","0885-8985;1557-959X","","10.1109/MAES.2009.5317781","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5317781","","Companies;Decision making;Process design;Logic;Military aircraft;Programming;Resource management;Aerospace testing;Marketing and sales;Humans","aerospace computing;decision making;decision support systems;formal specification;man-machine systems;resource allocation;user interfaces","operator machine interface;OMI;operator requirement;P-8A program;complex software development;operator decision making chain;resource allocation;design description document;aerospace community;expert operators;storytelling problem;decision mapping problem;is/is not matrices problem","","","7","","","","","","IEEE","IEEE Journals & Magazines"
"Development of a Rule Based Prognostic Tool for HER 2 Positive Breast Cancer Patients","P. J. G. Lisboa; T. A. Etchells; I. H. Jarman; M. S. H. Aung; S. Chabaud; T. Bachelot; D. Perol; T. Gargi; V. Bourdes; S. Bonnevay; S. Negrier","School of Computing and Mathematical Sciences, Liverpool John Moores University, UK. e-mail: p.j.lisboa@ljmu.ac.uk; School of Computing and Mathematical Sciences, Liverpool John Moores University, UK; School of Computing and Mathematical Sciences, Liverpool John Moores University, UK; School of Computing and Mathematical Sciences, Liverpool John Moores University, UK; Centre Léon Bérard-28 rue Laennec-69 373 Lyons Cedex 08-France; Centre Léon Bérard-28 rue Laennec-69 373 Lyons Cedex 08-France; Centre Léon Bérard-28 rue Laennec-69 373 Lyons Cedex 08-France; Centre Léon Bérard-28 rue Laennec-69 373 Lyons Cedex 08-France; THEMIS-ICTA Group-60 avenue Rockefeller-69008, Lyons-France; NA; NA","2007 29th Annual International Conference of the IEEE Engineering in Medicine and Biology Society","","2007","","","5416","5419","A three stage development process for the production of a hierarchical rule based prognosis tool is described. The application for this tool is specific to breast cancer patients that have a positive expression of the HER 2 gene. The first stage is the development of a Bayesian classification neural network to classify for cancer specific mortality. Secondly, low-order Boolean rules are extracted form this model using an orthogonal search based rule extraction (OSRE) algorithm. Further to these rules additional information is gathered from the Kaplan-Meier survival estimates of the population, stratified by the categorizations of the input variables. Finally, expert knowledge is used to further simplify the rules and to rank them hierarchically in the form of a decision tree. The resulting decision tree groups all observations into specific categories by clinical profile and by event rate. The practical clinical value of this decision support tool will in future be tested by external validation with additional data from other clinical centres.","1094-687X;1558-4615","978-1-4244-0787-3978-1-4244-0788","10.1109/IEMBS.2007.4353567","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4353567","","Breast cancer;Decision trees;Input variables;Neural networks;Data mining;Etching;Smoothing methods;Inspection;Production;Bayesian methods","Bayes methods;biological organs;Boolean algebra;cancer;decision support systems;decision trees;genetics;gynaecology;medical computing;molecular biophysics;neural nets","rule based prognostic tool;breast cancer patients;hierarchical rule;HER 2 gene;Bayesian classification;neural network;cancer specific mortality;low-order Boolean rules;orthogonal search based rule extraction;Kaplan-Meier survival;decision tree;decision support tool","Algorithms;Breast Neoplasms;Breast Neoplasms;Female;France;Humans;Incidence;Logistic Models;Prognosis;Proportional Hazards Models;Receptor, erbB-2;Reproducibility of Results;Risk Assessment;Risk Factors;Sensitivity and Specificity;Software;Survival Analysis;Survival Rate","","6","","","","","","IEEE","IEEE Conferences"
"BugFix: A learning-based tool to assist developers in fixing bugs","D. Jeffrey; M. Feng; Neelam Gupta; R. Gupta","University of California, CSE Department, Riverside, USA; University of California, CSE Department, Riverside, USA; University of California, CSE Department, Riverside, USA; University of California, CSE Department, Riverside, USA","2009 IEEE 17th International Conference on Program Comprehension","","2009","","","70","79","We present a tool called BugFix that can assist developers in fixing program bugs. Our tool automatically analyzes the debugging situation at a statement and reports a prioritized list of relevant bug-fix suggestions that are likely to guide the developer to an appropriate fix at that statement. BugFix incorporates ideas from machine learning to automatically learn from new debugging situations and bug fixes over time. This enables more effective prediction of the most relevant bug-fix suggestions for newly-encountered debugging situations. The tool takes into account the static structure of a statement, the dynamic values used at that statement by both passing and failing runs, and the interesting value mapping pairs associated with that statement. We present a case study illustrating the efficacy of BugFix in helping developers to fix bugs.","1092-8138","978-1-4244-3998-0978-1-4244-3997","10.1109/ICPC.2009.5090029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5090029","","Computer bugs;Software debugging;Testing;Machine learning;Programming;Robustness;Fault diagnosis;Failure analysis;Runtime;Error correction","learning (artificial intelligence);program debugging","BugFix;learning-based tool;program bugs;machine learning","","25","36","","","","","","IEEE","IEEE Conferences"
"Combine Pathway Analysis with Random Forests to Hunting for Feature Genes","H. Lin; W. Zheng; D. Li; J. Zhang; L. Hui; Y. Yan; Z. Jian; L. Hong","NA; NA; NA; NA; NA; NA; NA; NA","2009 2nd International Conference on Biomedical Engineering and Informatics","","2009","","","1","5","In this paper, a method combining pathway analysis with random forests was provided. After the important pathways were discovered by computing the classification error rates of out-of-bag (OOB), the feature genes were also discovered according to these important pathways. The important pathways were recombined as the new gene sets and the classification error rates were recomputed by random forests algorithms. According to the rank and the frequency of feature genes, the important feature genes associated with disease were discovered. At each important pathway, the relativity of gene expression was also studied. The results showed that our method was available because the expressions of genes at the same pathway were approximate. Those genes selected by SAM software directly were not feature genes but noises. We also compared random forests with other machine learning methods and found that random forests classification error rates were the lowest. This method can provide biological insight into the study of microarray data.","1948-2914;1948-2922","978-1-4244-4132-7978-1-4244-4134","10.1109/BMEI.2009.5301655","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5301655","","Error analysis;Diseases;Classification tree analysis;Frequency;Gene expression;Learning systems;Testing;Data mining;Impurities;Bioinformatics","genomics;random processes","pathway analysis;random forest;feature genes hunting;gene expression;SAM software;microarray","","","15","","","","","","IEEE","IEEE Conferences"
"Distribution system loss minimization and planning using Cymdist","L. Ramesh; S. Ravindiran; S. P. Chowdhury; S. Chowdhury; Y. H. Song; P. K. Goswami","Dr. M.G.R University, India; Jadavpur University, India; Jadavpur University, India; Women¿s Polytechnic, India; Brunel University, UK; Damodar Valley Corporation, India","2007 42nd International Universities Power Engineering Conference","","2007","","","316","321","The topology of an electrical distribution system (EDS) can be suitably modified to minimize the real and reactive losses. EDS is normally unevenly loaded and hence often need load balancing, which can also be achieved by suitably reconfiguring the network. This paper presents a new approach for reconfiguration of EDS based on efficient analysis in Cymdist software package. The proposed method is tested in 11 KV distribution system for loss minimization.","","978-1-905593-36-1978-1-905593-34","10.1109/UPEC.2007.4468967","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4468967","Distribution systems;loss minimization;distribution system planning;network reconfiguration","Switches;Telecommunication control;Optimization methods;Computer networks;Distributed computing;Automatic control;Costs;Remote monitoring;Substations;Voltage control","power distribution planning","distribution system loss minimization;distribution system planning;Cymdist software package;electrical distribution system;load balancing;network reconfiguration;voltage 11 kV","","3","11","","","","","","IEEE","IEEE Conferences"
"Memory-based context-sensitive spelling correction at web scale","A. Carlson; I. Fette","NA; NA","Sixth International Conference on Machine Learning and Applications (ICMLA 2007)","","2007","","","166","171","We study the problem of correcting spelling mistakes in text using memory-based learning techniques and a very large database of token n-gram occurrences in web text as training data. Our approach uses the context in which an error appears to select the most likely candidate from words which might have been intended in its place. Using a novel correction algorithm and a massive database of training data, we demonstrate higher accuracy on correcting real- word errors than previous work, and very high accuracy at a new task of ranking corrections to non-word errors given by a standard spelling correction package.","","978-0-7695-3069","10.1109/ICMLA.2007.50","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4457226","","Training data;Error correction;Statistics;Machine learning;Databases;Packaging;Dictionaries;Computer science;Testing;Application software","learning (artificial intelligence);natural language processing;spelling aids;text analysis;very large databases","context-sensitive spelling correction;spelling mistakes;memory-based learning techniques;very large database;token n-gram occurrences;Web text","","15","8","","","","","","IEEE","IEEE Conferences"
"A multi-objective genetic algorithm approach to optimal allocation of multi-type FACTS devices for power systems security","D. Radu; Y. Besanger","LEG, ENSIEG, St. Martin d'Heres, France; LEG, ENSIEG, St. Martin d'Heres, France","2006 IEEE Power Engineering Society General Meeting","","2006","","","8 pp.","","A multi-objective programming procedure is used for solving the problem of optimal allocation of flexible AC transmission systems (FACTS) devices in a power system. The evolutionary approach consists of a multi-objective genetic algorithm (MOGA), which is used to characterize the Pareto optimal frontier (non-dominated solutions) and to provide to decision makers and engineers insightful information about the trade-offs to be made. In this paper, two technical and economical objective functions are considered: maximization of system security and minimization of investment cost for FACTS devices. The optimization process is focused on three parameters: the location of FACTS in the network, their types and their sizes. For these proposals, we employed a hybrid software developed in Matlabtrade which uses the EUROSTAGtrade software for load flow calculations. The proposed procedures are successfully tested on an IEEE 14-bus power system for several numbers of FACTS devices","1932-5517","1-4244-0493","10.1109/PES.2006.1709202","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1709202","FACTS devices;multi-objective genetic algorithms;optimal location;Pareto frontier","Genetic algorithms;Power system security;Flexible AC transmission systems;Hybrid power systems;Genetic engineering;Power engineering and energy;Power system economics;Power generation economics;Information security;Investments","flexible AC transmission systems;genetic algorithms;load flow;Pareto optimisation;power system security","multiobjective genetic algorithm;multitype FACTS devices;power systems security;flexible AC transmission system devices;Pareto optimal frontier;load flow calculations","","34","24","","","","","","IEEE","IEEE Conferences"
"Dynamic performance prediction of an adaptive mesh application","M. M. Mathis; D. J. Kerbyson","Comput. & Computational Sci., Los Alamos Nat. Lab., USA; Comput. & Computational Sci., Los Alamos Nat. Lab., USA","Proceedings 20th IEEE International Parallel & Distributed Processing Symposium","","2006","","","8 pp.","","While it is possible to accurately predict the execution time of a given iteration of an adaptive application, it is not generally possible to predict the data-dependent adaptive behavior the application will take and therefore to predict the total execution time for a given execution. To remedy this situation we have developed an executable performance model that can be utilized dynamically at runtime directly from the application of interest. In this manner, the application itself can rapidly predict the expected execution time for its next iteration based on current information on the data layout and level of adaptivity. This enables the application itself to determine: if an optimum level of performance is being achieved (i.e. by comparing measured and predicted times); when to perform a checkpoint (if the next iteration will exceed a predefined time limit between checkpoints); or when to terminate (if the next iteration will exceed the application's system time allocation for instance). The dynamic model is shown to have high accuracy over a number of test cases, even in the presence of interference (system activities that are not a part of application activities)","1530-2075","1-4244-0054","10.1109/IPDPS.2006.1639701","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1639701","","Application software;Laboratories;Runtime;Procurement;Design optimization;Large-scale systems;Hydrodynamics;Adaptive mesh refinement;Electric shock;Computer architecture","checkpointing;iterative methods;mesh generation;operating systems (computers);performance evaluation","dynamic performance prediction;adaptive mesh application;data-dependent adaptive behavior;checkpoint;system time allocation","","1","19","","","","","","IEEE","IEEE Conferences"
"CNNOPT: Learning dynamics and CNN chip-specific robustness","D. Hillier; S. X. de Souza; J. A. K. Suykens; J. Vandewalle","ESAT-SCD-SISTA, K.U. Leuven, Kasteelpark Arenberg 10, B-3001 Leuven (Heverlee) Belgium; Jedlik Laboratory, Faculty of Information Technology, Péter Pázmány Catholic University, 1083 Budapest, Práter u. 50/a, Hungary. e-mail: hillier@digitus.itk.ppke.hu; ESAT-SCD-SISTA, K.U. Leuven, Kasteelpark Arenberg 10, B-3001 Leuven (Heverlee) Belgium. e-mail: samuel.xavierdesouz@esat.kuleuven.be; ESAT-SCD-SISTA, K.U. Leuven, Kasteelpark Arenberg 10, B-3001 Leuven (Heverlee) Belgium. e-mail: johan.suykens@esat.kuleuven.be; ESAT-SCD-SISTA, K.U. Leuven, Kasteelpark Arenberg 10, B-3001 Leuven (Heverlee) Belgium. e-mail: joos.vandewalle@esat.kuleuven.be","2006 10th International Workshop on Cellular Neural Networks and Their Applications","","2006","","","1","1","A software tool with the aim of learning new CNN templates and incorporating the ability to tune templates to individual chip instances is presented. During demonstration, the training process of the grayscale constrained trigger-wave template [1] will be shown. The audience can test the tool by proposing their own operation to be learned/tuned.","2165-0144;2165-0152","1-4244-0639-01-4244-0640","10.1109/CNNA.2006.341591","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4145831","","Cellular neural networks;Robustness;Cost function;Very large scale integration;Gray-scale;Optimization methods;Spatiotemporal phenomena;Neural networks;Microwave integrated circuits;Laboratories","","","","","","","","","","","IEEE","IEEE Conferences"
"A new similarity measure for non-rigid breathing motion compensation of myocardial perfusion MRI","G. Wollny; M. J. Ledesma-Carbayo; P. Kellman; A. Santos","Ciber BNN, Spain and the Group of Biomedical Imaging Techologies, Department of Electronic Engineering, ETSIT, Universidad Politécnica de Madrid, Spain; Group of Biomedical Imaging Techologies, Department of Electronic Engineering, ETSIT, Universidad Politécnica de Madrid, Spain; Laboratory of Cardiac Energetics, National Heart, Lung and Blood Institute, National Institutes of Health, DHHS, Bethesda, Maryland, USA; Group of Biomedical Imaging Techologies, Department of Electronic Engineering, ETSIT, Universidad Politécnica de Madrid, Spain","2008 30th Annual International Conference of the IEEE Engineering in Medicine and Biology Society","","2008","","","3389","3392","Breathing movements during the image acquisition of first-pass gadolinium enhanced, myocardial perfusion Magnetic Resonance Imaging (MRI) hinder a direct automatic analysis of the blood flow of the myocardium. In addition, a qualitative readout by visual tracking is more difficult as well. Non-rigid registration can be used to compensate for these movements in the image series. Because of the local contrast and intensity change over time, the registration criterion needs to be chosen carefully. We propose a measure based on Normalized Gradient Fields (NGF) in order to obtain registration. Since this measure requires strong gradients in the images, we also test combining the measure with the Sum of Squared Differences (SSD) to maintain registration forces over the whole image area. To ensure smoothness, we employ a Laplacian regularizer and use the B-spline based approach to describe the transformation of the image space. Our experiments show that by using NGF good registration results can be obtained for image exhibiting a high intensity contrast. For images with a low intensity contrast, combining NGF and SSD improves the registration results significantly over using NGF only. Both measures are differentiable making possible the application of fast, gradient based optimizers.","1094-687X;1558-4615","978-1-4244-1814-5978-1-4244-1815","10.1109/IEMBS.2008.4649933","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4649933","","","","","Algorithms;Automatic Data Processing;Automation;Contrast Media;Gadolinium;Humans;Magnetic Resonance Imaging;Models, Statistical;Motion;Myocardium;Perfusion;Reproducibility of Results;Respiration;Software","4","9","","","","","","IEEE","IEEE Conferences"
"Emergency departments nurse allocation to face a pandemic influenza outbreak","F. Rico; E. Salari; G. Centeno","4202 E. Fowler Ave. ENB 118, University of South Florida, Tampa, 33813, USA; 4202 E. Fowler Ave. ENB 118, University of South Florida, Tampa, 33813, USA; 4202 E. Fowler Ave. ENB 118, University of South Florida, Tampa, 33813, USA","2007 Winter Simulation Conference","","2007","","","1292","1298","This study proposes a nurse allocation policy to manage patient overflow during a pandemic influenza outbreak. The objective is to minimize the number of patients waiting in queue to be treated for the virus while maximizing patient flow. The model is built using ARENA simulation software and OptQuest heuristic optimization to propose various combinations for the number of nurses needed for healthcare delivery. Results are compared with a basic setting that closely emulates the resources and components in a Veteran's Hospital. The proposed method significantly reduces the number of patients waiting in queue (between 4 to 37 percent on average) for the simulated zones. ARENA process analyzer was used to evaluate various scenarios for nurse availability. Sensitivity on the results for these changes was tested by increasing the flow of patients through the system.","0891-7736;1558-4305","978-1-4244-1305-8978-1-4244-1306","10.1109/WSC.2007.4419734","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4419734","","Influenza;Hospitals;Medical services;Humans;Remuneration;Disaster management;Availability;Capacitive sensors;History;Medical treatment","digital simulation;diseases;emergency services;health care;medical administrative data processing;patient care","emergency department nurse allocation policy;pandemic influenza outbreak;patient overflow management;ARENA simulation software;OptQuest heuristic optimization;healthcare delivery;Veteran Hospital;hospital management","","9","17","","","","","","IEEE","IEEE Conferences"
"Effect of automotive headlamp modeling on automotive aerodynamic drag","Lanfang Jiang; Hong Liu; Guozhong Chai; Guangnan Jiang; Weiming Lin","The MOE Key Laboratory of Mechanical Manufacture and Automation, Zhejiang University of Technology, Hangzhou, China; The MOE Key Laboratory of Mechanical Manufacture and Automation, Zhejiang University of Technology, Hangzhou, China; The MOE Key Laboratory of Mechanical Manufacture and Automation, Zhejiang University of Technology, Hangzhou, China; Shanghai Prosynx Technology Inc, China; Zhejiang King-mazon Machinery Co., Ltd, Lishui, China","2008 9th International Conference on Computer-Aided Industrial Design and Conceptual Design","","2008","","","588","593","Automotive headlamp design, combining science with art, is essential in automotive modeling design. Headlamp modeling design should consider harmonizing with automotive modeling design as well as meeting the national standards of structural design and lighting property. The research aims to present an approach for headlamp modeling design considering automotive aerodynamic drag. The effect of different headlamp modeling design on aerodynamic drag and accordingly three sorts of parameters of headlamp modeling design combining with aesthetic demand are studied. An analytic model of headlamp on automotive aerodynamic drag is established. Numerical simulation of 3-D flow field around the automobile with CFD software is performed. The change of drag coefficient with parameters is analyzed. Finally the design rules of headlamp aerodynamic modeling optimization are concluded. The research effort will enrich the design means of automotive modeling and automotive headlamp, so as to improve the technical property.","","978-1-4244-3290-5978-1-4244-3291","10.1109/CAIDCD.2008.4730637","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4730637","Headlamp;Automotive Modeling Design;Aerodynamic Drag","Automotive engineering;Aerodynamics;Lamps;Automobiles;Fuel economy;Manufacturing automation;Vehicle dynamics;Testing;Shape;Laboratories","aerodynamics;automotive components;computational fluid dynamics;drag;flow simulation;lamps;production engineering computing","automotive headlamp modeling;automotive aerodynamic drag;3-D flow field;CFD software;drag coefficient","","1","12","","","","","","IEEE","IEEE Conferences"
"Modeling of a CMOS Convective Accelerometer for HDL integration","O. Leman; A. Chaehoi; F. Mailly; L. Latorre; P. Nouet","Laboratoire d'Informatique de Robotique et de Microélectronique de Montpellier, UMR 5506 ¿ CNRS / University Montpellier II, 161 rue Ada, 34392, Montpellier - France; Laboratoire d'Informatique de Robotique et de Microélectronique de Montpellier, UMR 5506 ¿ CNRS / University Montpellier II, 161 rue Ada, 34392, Montpellier - France; Laboratoire d'Informatique de Robotique et de Microélectronique de Montpellier, UMR 5506 ¿ CNRS / University Montpellier II, 161 rue Ada, 34392, Montpellier - France; Laboratoire d'Informatique de Robotique et de Microélectronique de Montpellier, UMR 5506 ¿ CNRS / University Montpellier II, 161 rue Ada, 34392, Montpellier - France. latorre@lirmm.fr; Laboratoire d'Informatique de Robotique et de Microélectronique de Montpellier, UMR 5506 ¿ CNRS / University Montpellier II, 161 rue Ada, 34392, Montpellier - France","2006 European Solid-State Device Research Conference","","2006","","","134","137","This paper introduces an analytical modeling of a convection heat CMOS accelerometer. The modeling approach relies on the use of fundamental results and is validated using both experimental data issued from test vehicles and FEM analysis. Since such accelerometers are based on the CMOS technology, the proposed model is intended for further HDL integration. The objective is to enable system level simulation and optimization within standard microelectronic CAD software","1930-8876;2378-6558","1-4244-0301","10.1109/ESSDER.2006.307656","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4099874","","Semiconductor device modeling;Accelerometers;Hardware design languages;Heating;CMOS technology;Temperature sensors;Bridge circuits;Heat transfer;Resistors;Detectors","accelerometers;CMOS integrated circuits;finite element analysis;hardware description languages;micromachining","HDL integration;convection heat CMOS accelerometer;CMOS technology;system level simulation;standard microelectronic CAD software","","1","6","","","","","","IEEE","IEEE Conferences"
"A BnB Mobile Game Online Based on J2ME and J2EE","Q. Zhu; L. Zhao; S. Cao; J. Shen; S. Zhang","NA; NA; NA; NA; NA","2009 Seventh ACIS International Conference on Software Engineering Research, Management and Applications","","2009","","","19","24","Through the example of Bubble and Bubble (BnB) mobile game, the whole development process of mobile game based on J2ME and J2EE is showed. First, Game API in the Mobile Information Device Profile (MIDP) 2.0 is used to build the game engine. Through the optimization and compatibility design of process, middleware framework is selected. Second, the game classes and components design is introduced in detail. Third, the server performance for detonation on the client, the communications time between the clients with the server and the procedure performance of the server have been tested. Various techniques, such as object pool, multi-threaded, socket connection, sprite, Maps etc., are applied in BnB's development. Experiment demonstrates its performance and proves this case is meaningful and useful for other online mobile game development. Some propositions for further research is also suggested.","","978-0-7695-3903","10.1109/SERA.2009.10","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381813","","Mobile computing;Mobile handsets;Manufacturing industries;Toy industry;Toy manufacturing industry;Internet;Middleware;Mobile communication;Sockets;Web server","computer games;Java;middleware;mobile computing;object-oriented programming","BnB mobile game online;J2ME;J2EE;bubble and bubble mobile game;game API;mobile information device profile;MIDP 2.0;game engine;middleware framework;game classes;components design;object pool;multithreading;socket connection","","1","15","","","","","","IEEE","IEEE Conferences"
"A Novel Wireless Sensor Networks Platform for Habitat Surveillance","G. YingMing; J. RenCheng","NA; NA","2008 International Conference on Computer Science and Software Engineering","","2008","4","","1028","1031","Wireless sensor networks are more adaptive to habitat surveillance applications than other technique due to high distribution density and self-organization of tiny sensor nodes. This paper introduced a framework of wireless sensor networks combined with GPRS (General Packet Radio Service) for habitat monitoring. Detailed presentation of a novel sensor node making use of ARM technology and IEEE502.15.4 standard was given. This paper concerned on energy management issue, realizing a light-weight and constant duty cycle energy management policy similar to S-MAC (Sensor Medium Access Control). Minimum, maximum and average power consumption of sensor node, under different CPU and VPB frequency, were measured when node was running test program to finish networking and application function. The results help users choosing the most appropriate frequency to optimize power consumption.","","978-0-7695-3336","10.1109/CSSE.2008.1185","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722794","wireless sensor networks;sensor nodes;energy management","Wireless sensor networks;Surveillance;Energy management;Energy consumption;Frequency;Adaptive systems;Ground penetrating radar;Packet radio networks;Monitoring;Media Access Protocol","IEEE standards;packet radio networks;telecommunication standards;wireless sensor networks","wireless sensor network;habitat surveillance;general packet radio service;habitat monitoring;ARM technology;IEEE502.15.4 standard;constant duty cycle energy management policy;sensor medium access control;power consumption","","1","21","","","","","","IEEE","IEEE Conferences"
"Millimeter-wave artificial dielectric gradient index lenses","V. N. Nguyen; S. H. Yonak; D. R. Smith","Department of Electrical and Computer Engineering, Duke University, Durham, NC, USA 27708; Toyota Motor Engineering and Manufacturing North America, Ann Arbor, MI, USA 48105; Department of Electrical and Computer Engineering, Duke University, Durham, NC, USA 27708","2009 3rd European Conference on Antennas and Propagation","","2009","","","1886","1890","We present an artificial dielectric gradient index lens designed to operate at millimeter-wave frequencies. Finite element simulations are used in conjunction with material parameter retrieval techniques to design an artificial dielectric unit cell with an optimum refractive index range. This refractive index range is then used with ray-tracing software to arrive at an optimized index profile. A lens with this index profile is realized using multilayer printed wiring boards (PWBs) and tested.","2164-3342","978-1-4244-4753-4978-3-00-024573","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5067985","","Lenses;Millimeter wave technology;Refractive index;Optical design;Frequency;Finite element methods;Dielectric materials;Optical materials;Ray tracing;Nonhomogeneous media","dielectric devices;finite element analysis;lens antennas;millimetre wave antennas;printed circuits;ray tracing;refractive index","millimeter-wave artificial dielectric gradient index lenses;finite element simulations;material parameter retrieval techniques;optimum refractive index range;ray-tracing software;multilayer printed wiring boards;PWB","","1","9","","","","","","IEEE","IEEE Conferences"
"Transparent TCP improvement over satellite links","F. Fergnani; G. Mazzini","University of Ferrara, via Saragat 1, 44100, Italy; University of Ferrara, via Saragat 1, 44100, Italy","2007 15th International Conference on Software, Telecommunications and Computer Networks","","2007","","","1","5","Since satellite channel can be used as a part of an heterogeneous network, researchers attention is focused on the optimization of the use of that resource. Unfortunately, protocols studied for wired traditional network, and widely used for Internet services, performs very poorly on this transmission medium, in particular the TCP. In the last years a lot of works have been published about this topic, but up to now there is not a standardized solution able to solve all the problems created to heterogeneous networks by satellite links. In this paper we present a possible solution to the problem of TCP data transmission over satellite links that have as characteristics easy implementation and compatibility to existent standard and protocols. We implemented our solution and tested it on a emulated heterogeneous network scenario and on a real satellite link, and we demonstrate how, adding a very simple mechanism of selective repeat combined to an additional congestion control, the TCP performances over a satellite channel are improved.","","978-953-6114-93-1978-953-6114-95","10.1109/SOFTCOM.2007.4446057","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4446057","","Satellites;Intrusion detection;Wireless LAN;Data encapsulation;Telecommunication traffic;Traffic control;Communications technology;Mathematics;Communication system security;Collaboration","satellite communication;telecommunication congestion control;transport protocols","transparent TCP;satellite links;satellite channel;heterogeneous network;TCP data transmission;selective repeat;congestion control","","","12","","","","","","IEEE","IEEE Conferences"
"Link analysis for BILSAT-1","A. Telli; A. Es","Inf. Technol. & Electron. Res. Inst., Sci. & Technol. Res. Council of Turkey,, Ankara, Turkey; Inf. Technol. & Electron. Res. Inst., Sci. & Technol. Res. Council of Turkey,, Ankara, Turkey","2006 IEEE Aerospace Conference","","2006","","","6 pp.","","BILSAT-1, a 130 kg micro satellite, is Turkey's first low Earth orbit Earth (LEO) observation satellite. The project was started in August 2001 at Surrey Satellite Technologies Limited (SSTL)'s facilities in Guildford, UK and BILSAT-1 was launched successfully from Plesetsk Cosmodrome, Russia in 27 September 2003. The main objective of the mission is remote sensing. The communications subsystem is a core system for Earth observation satellite. It is generally used for telemetry/telecommand signaling, software uploading and image/data transfer. The BILSAT-1 communications subsystem is composed of VHF/UHF (amateur space band) and S-Band (commercial space band). Link analysis is essential for design of any satellite communications subsystem to observe the quality of the link. The link budget should be calculated; the positive link margin has to be guaranteed at all elevation angles that the proposed design requires before the actual manufacturing process starts. For BILSAT-1 communications subsystem, the link budget calculator program has been developed at pre-design phase. The program calculates the received Eb/No and the required Eb/No, and gives the difference as the link margin with respect to the elevation angles. The communications subsystem parameters such as RF output power, antenna size/type, power consumption etc. both at ground station and on board have been optimized according to the simulated links. These parameters ultimately effects overall project cost and time schedule. Also, to aid pre-launch/in-orbit test and to observe in-orbit link quality, some status points have been made available at BILSAT-1 satellite telemetry system. The measurements include a received signal strength indicator (RSSI) that provides an indication of the total signal power received. By using this data, the reliability of the link budget calculator program can be verified. In this paper, the above-mentioned link budget calculator program will be presented and the calculated and the actual link margins will be given and compared","1095-323X","0-7803-9545","10.1109/AERO.2006.1655854","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1655854","","Low earth orbit satellites;Telemetry;Remote sensing;Artificial satellites;Telecommunications;Satellite communication;Manufacturing processes;Radio frequency;Power generation;Energy consumption","artificial satellites;satellite links;satellite telemetry","link analysis;BILSAT-1;microsatellite;low Earth orbit Earth observation satellite;Surrey Satellite Technologies Limited;space mission;remote sensing;telemetry signaling;telecommand signaling;software uploading;image/data transfer;amateur space band;commercial space band;satellite communications subsystem;link budget;satellite telemetry system;received signal strength indicator;130 kg","","","4","","","","","","IEEE","IEEE Conferences"
"Idle Speed Control of Gasoline Engine for Hybrid Electric Vehicle","W. Kai; Z. Yunlong; G. Hui","State Key Laboratory of Automotive Safety and Energy, Tsinghua University, Beijing 100084, P.R.China. phone: 8610-6278-2122; e-mail: kwangthu@gmail.com; State Key Laboratory of Automotive Safety and Energy, Tsinghua University, Beijing 100084, P.R.China. e-mail: daezyl@tsinghua.edu.cn; State Key Laboratory of Automotive Safety and Energy, Tsinghua University, Beijing 100084, P.R.China; United Automotive Electronic Systems Co., Ltd, Shanghai 201206, P.R.China. e-mail: hui.guo99@gmail.com","2006 IEEE International Conference on Vehicular Electronics and Safety","","2006","","","285","288","Hybrid Electric Vehicles use electric throttle instead of mechanical throttle as to distribute power and optimize performance of gasoline engine. At present, in order to keep oil pump and compressor working normally, idle speed control is still needed for some HEV gasoline engines. This article introduces the method we used in the development of idle speed control system based on electric throttle. Besides, the construction and working principle of idle speed control system are given in detail. The software function validation and the preliminary calibration, which are fulfilled in the bench test, indicate that electric throttle has the ability to control idle speed and the method developed by us could control idle speed with high stability and precision.","","1-4244-0758-31-4244-0759","10.1109/ICVES.2006.371600","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4234036","Gasoline Engine;HEV;Idle Speed Control","Velocity control;Petroleum;Engines;Hybrid electric vehicles;Three-term control;Open loop systems;Laboratories;Safety;Arithmetic;Control systems","calibration;engines;hybrid electric vehicles;velocity control","idle speed control system;gasoline engine;hybrid electric vehicle;HEV;electric throttle;software function validation;preliminary calibration","","","8","","","","","","IEEE","IEEE Conferences"
"Hunting sea mines with UUV-based magnetic and electro-optic sensors","G. Sulzberger; J. Bono; R. J. Manley; T. Clem; L. Vaizer; R. Holtzapple","Naval Surface Warfare Center Panama City 110 Vernon Ave Panama City, FL 32407, USA; Naval Surface Warfare Center Panama City 110 Vernon Ave Panama City, FL 32407, USA; Naval Surface Warfare Center Panama City 110 Vernon Ave Panama City, FL 32407, USA; Naval Surface Warfare Center Panama City 110 Vernon Ave Panama City, FL 32407, USA; Naval Surface Warfare Center Panama City 110 Vernon Ave Panama City, FL 32407, USA; Naval Surface Warfare Center Panama City 110 Vernon Ave Panama City, FL 32407, USA","OCEANS 2009","","2009","","","1","5","The US Navy (USN) has recognized the need for effective buried-mine hunting as one of its Organic Mine Countermeasures (MCM) Future Naval Capabilities. Current thinking envisions a two-step process for identifying buried mines. First, an initial survey, or Search-Classify-Map (SCM) mission, will be performed using low-frequency synthetic aperture sonar (SAS). Second, a Reacquire-and-Identify (RI) mission will provide confirmatory final classification by reacquiring the target, at close range, with magnetic, acoustic, and electro-optic sensors, and evaluating properties such as geometric details and magnetic moment that can be fused to identify or definitively classify the object. The goal is to demonstrate a robust capability to identify buried sea mines through sensor fusion. Specifically, the classification results of a passive magnetic sensor and an electro-optic sensor will be generated for fusion with the results from a short-range bottom-looking sonar, with all three sensors co-residing and operating simultaneously on an Unmanned Underwater Vehicle (UUV). The Bluefinl2 Buried Mine Identification (BMI) System will be used as the platform to develop a capability for the identification of buried mines. This system houses the bottom looking sonar, the Real-time Tracking Gradiometer (RTG), and an Electro-Optic Imager (EOT). This paper will address the applications of the RTG, EOI, and data fusion results with bottom looking sonar. The objective for the RTG is the enhancement of the processing that extracts target locations and magnetic moments from the raw RTG data. In particular, we are adding a capability to conduct real-time processing capability to provide autonomous target classification and localization results soon after the UUV passes the target, while the system is still performing the mission. These results will be shared with the vehicle or other sensors for transmission back to a base station when the vehicle surfaces. The objectives for the EOI include additions to the control software and the development of a set of versatile image processing techniques. A significant goal is to develop the ability to make images viewable remotely over the vehicle's RF link. This allows for a quick review of contacts and improved flexibility in mission planning and execution. Image processing goals included the development of image enhancement algorithms that could be applied to all EOI data. The intent of the enhancement algorithms is to enhance image contrast and sharpness to better differentiate targets from background and increase target detail. The software will be used to batch process large amounts of raw EOI images and save them in a format so that the user can scroll through the images using a standard image viewer. In 2008, the Bluefinl2 BMI system participated in multiple sea tests. The data collected from these missions proved that sensor fusion aboard an UUV was possible. Post Mission Analysis (PMA) also concluded that data fusion was successful. Both the RTG and the EOI participated in sea tests of the Bluefinl2 BMI System to evaluate, optimize and demonstrate a BMI capability. Specifically in 2008, this system was demonstrated at Panama City, FL and at AUVfest 2008 in Newport, RI. This paper focuses on the 2008 sea testing using the modified RTG and the EOI sensors and the ability to use near real-time detection.","0197-7385","978-1-4244-4960-6978-0-933957-38","10.23919/OCEANS.2009.5422086","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5422086","","Electrooptic devices;Magnetic sensors;Sensor fusion;Real time systems;System testing;Synthetic aperture sonar;Magnetic moments;Remotely operated vehicles;Image processing;Acoustic sensors","electro-optical devices;magnetic sensors;sensor fusion;synthetic aperture sonar","sea mines;UUV-based magnetic sensors;electro-optic sensors;low-frequency synthetic aperture sonar;sensor fusion;teal-time tracking gradiometer;bottom looking sonar","","2","7","","","","","","IEEE","IEEE Conferences"
"Computer Vision Studies Using Stochastic Resonance/Information-theoretic Methods","D. W. Repperger; R. G. Roberts; A. R. Pinkus","Members, IEEE, Air Force Research Laboratory, Wright-Patterson Air Force Base, Ohio 45433 USA. 937-255-8765; fax: 937-255-8752; e-mail: d.repperger@ieee.org; Members, IEEE, Department of Electrical and Computer Engineering, FAMU-FSU College of Engineering, 2525 Pottsdamer Street, Tallahassee, Florida 32310-6046. e-mail: rroberts@eng.fsu.edu; Members, IEEE, Air Force Research Laboratory, Wright-Patterson Air Force Base, Ohio 45433. email: alan.pinkus@wpafb.af.mil","2007 International Symposium on Computational Intelligence in Robotics and Automation","","2007","","","119","124","An investigation into computer vision techniques is conducted using a procedure from nonlinear dynamics termed ""stochastic resonance."" This work involves concepts from detection theory, information theory and nonlinear dynamics. An information distance metric is synthesized which helps define the dependent measure to be used with the stochastic resonance optimization. Monte Carlo simulations show the efficacy of the proposed method. A class of test objects are presented to fairly evaluate the utility of the proposed methods introduced.","","1-4244-0789-31-4244-0790","10.1109/CIRA.2007.382847","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4269847","","Computer vision;Stochastic resonance;Strontium;Signal processing;Physics;Information theory;Application software;Object recognition;Noise figure;Computational intelligence","computer vision;Monte Carlo methods;stochastic processes","computer vision techniques;nonlinear dynamics;information distance metric;stochastic resonance optimization;Monte Carlo simulations","","","17","","","","","","IEEE","IEEE Conferences"
"Constructing a Context-Aware Service-Oriented Reputation Model Using Attention Allocation Points","R. Alnemr; J. Bross; C. Meinel","NA; NA; NA","2009 IEEE International Conference on Services Computing","","2009","","","451","457","In this paper we examine the problem of rich information environments and the need to narrow the agents attention to what is important for them to interact and later to evaluate and transfer reputation values, using attention allocation technique (AA). We also argue that this cannot be done without the aid of service oriented architecture (SOA). Reputation is used in our work as a service, presenting a new concept- that is reputation-as-a-service (RaaS). We then present a service-oriented model for optimizing the presentation and the use of reputation in order to maximize its value to both users and providers.","","978-1-4244-5183-8978-0-7695-3811","10.1109/SCC.2009.55","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5283922","Reputation;Attention Allocation;SOA;Trust","Context-aware services;Context modeling;Natural languages;Middleware;Humans;Assembly;System testing;Application software;Natural language processing;TV","ubiquitous computing;Web services","context-aware service-oriented reputation model;attention allocation point;service oriented architecture;SOA;reputation-as-a-service;RaaS","","6","14","","","","","","IEEE","IEEE Conferences"
"Intrusion detection using k-Nearest Neighbor","M. Govindarajan; R. Chandrasekaran","Department of Computer Science and Engineering, Annamalai University, Annamalai Nagar, Tamil Nadu, India; Department of Computer Science and Engineering, Annamalai University, Annamalai Nagar, Tamil Nadu, India","2009 First International Conference on Advanced Computing","","2009","","","13","20","Data mining is the use of algorithms to extract the information and patterns derived by the knowledge discovery in databases process. Classification maps data into predefined groups or classes. It is often referred to as supervised learning because the classes are determined before examining the data. In many data mining applications that address classification problems, feature and model selection are considered as key tasks. That is, appropriate input features of the classifier must be selected from a given set of possible features and structure parameters of the classifier must be adapted with respect to these features and a given data set. This paper describes feature selection and model selection simultaneously for k-nearest neighbor (k-NN) classifiers. In order to reduce the optimization effort, various techniques are integrated that accelerate and improve the classifier significantly: hybrid k-NN, comparative cross validation. The feasibility and the benefits of the proposed approach are demonstrated by means of data mining problem: intrusion detection in computer networks. It is shown that, compared to earlier k-NN technique, the run time is reduced by up to 0.01 % and 0.06 % while error rates are lowered by up to 0.002 % and 0.03 % for normal and abnormal behaviour respectively. The algorithm is independent of specific applications so that many ideas and solutions can be transferred to other classifier paradigms.","2377-6927","978-1-4244-4786-2978-1-4244-4787","10.1109/ICADVC.2009.5377998","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5377998","","Intrusion detection;Legged locomotion;Humanoid robots;Induction motors;Hydraulic actuators;Energy consumption;Robot control;Control systems;Testing;Application software","computer network security;data mining;information filtering;learning (artificial intelligence);pattern classification","intrusion detection;data mining;k-nearest neighbor classifier;information extraction;knowledge discovery;databases process;map data classification;supervised learning;feature selection;computer networks","","2","27","","","","","","IEEE","IEEE Conferences"
"Performance Evaluation of a Novel CMP Cache Structure for Hybrid Workloads","X. Zhao; K. Sammut; F. He","NA; NA; NA","Eighth International Conference on Parallel and Distributed Computing, Applications and Technologies (PDCAT 2007)","","2007","","","89","96","The Chip Multiprocessor (CMP) architecture offers parallel multi-thread execution and fast retrieval of shared data that is cached on-chip. In order to obtain the best possible performance with the CMP architecture, the cache architecture must be optimised to reduce time lost during remote cache and off-chip memory accesses. Many researchers proposed CMP cache architectures to improve the system performance, but they have not considered parallel execution of mixed single-thread and multi-thread workloads. In this paper, we propose a hybrid workload-aware cache architecture SPS2, in which each processor has both private and shared L2 caches. We describe the corresponding SPS2 cache coherence protocol with state transition graph. Performance evaluation demonstrates that the proposed SPS2 cache structure has better performance than traditional private L2 and shared L2 when hybrid workloads are applied.","2379-5352","0-7695-3049","10.1109/PDCAT.2007.58","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4420146","","Computer architecture;System performance;Yarn;Benchmark testing;Access protocols;Application software;Sonar navigation;Distributed computing;Helium;Informatics","cache storage;computer architecture;multiprocessing systems;performance evaluation","performance evaluation;CMP cache structure;hybrid workloads;chip multiprocessor architecture;parallel multithread execution;fast shared data retrieval;remote cache access;off-chip memory access;workload-aware cache architecture","","","17","","","","","","IEEE","IEEE Conferences"
"Generating, benchmarking and simulating production schedules: From formalisation to real problems","G. Zülch; P. Steininger; T. Gamber; M. Leupold","Kaiserstr. 12, ifab-Institute of Human and Industrial Engineering University of Karlsruhe, Karlsruhe, 76128, Germany; Kaiserstr. 12, Steinbuch Centre for Computing (SCC), formerly at the ifab-Institute of Human and Industrial Engineering, University of Karlsruhe, Karlsruhe, 76128, Germany; Kaiserstr. 12, ifab-Institute of Human and Industrial Engineering University of Karlsruhe, Karlsruhe, 76128, Germany; Kaiserstr. 12, ifab-Institute of Human and Industrial Engineering University of Karlsruhe, Karlsruhe, 76128, Germany","Proceedings of the 2009 Winter Simulation Conference (WSC)","","2009","","","2238","2249","Production scheduling has attracted the interest of production economics communities for decades, but there is still a gap between academic research, real-world problems, operations research and simulation. Genetic Algorithms (GA) represent a technique that has already been applied to a variety of combinatorial problems. Simulation can be used to find a solution to problems through repetitive simulation runs or to prove a solution computed by an optimization algorithm. We will explain the application of two special GAs for job-shop and resource-constrained project scheduling problems trying to bridge the gap between problem solving by algorithm and by simulation. Possible goals for scheduling problems are to minimize the makespan of a production program or to increase the due-date reliability of jobs or possibly any goal which can be described in a mathematical expression. The approach focuses on integrating a GA into a commercial software product and verifying the results with simulation.","0891-7736;1558-4305","978-1-4244-5771-7978-1-4244-5770-0978-1-4244-5772","10.1109/WSC.2009.5429187","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5429187","","Job shop scheduling;Computational modeling;Optimal scheduling;Humans;Processor scheduling;Industrial engineering;Job production systems;Operations research;Scheduling algorithm;Testing","combinatorial mathematics;genetic algorithms;job shop scheduling","production scheduling;production economics communities;operations research;genetic algorithms;optimization algorithm;job shop scheduling problem;resource constrained project scheduling problem;mathematical expression;combinatorial problems","","","29","","","","","","IEEE","IEEE Conferences"
"Analytical Results on Style-Constrained Bayesian Classification of Pattern Fields","S. Veeramachaneni; G. Nagy","Automated Reasoning Systems Division (SRA), IRST-Istituto per la Ricerca Scientifica e Tecnologica, Trento, Italy; Department of Electrical, Computer, and Systems Engineering, Rensselaer Polytechnic Institute, Troy, NY","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2007","29","7","1280","1285","We formalize the notion of style context, which accounts for the increased accuracy of the field classifiers reported in this journal recently. We argue that style context forms the basis of all order-independent field classification schemes. We distinguish between intraclass style, which underlies most adaptive classifiers, and interclass style, which is a manifestation of interpattern dependence between the features of the patterns of a field. We show how style-constrained classifiers can be optimized either for field error (useful for short fields like zip codes) or for singlet error (for long fields, like business letters). We derive bounds on the reduction of error rate with field length and show that the error rate of the optimal style-constrained field classifier converges asymptotically to the error rate of a style-aware Bayesian singlet classifier.","0162-8828;2160-9292;1939-3539","","10.1109/TPAMI.2007.1030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4204170","Style context;field classification;adaptive classification;Bayesian classification.","Pattern analysis;Bayesian methods;Testing;Optical character recognition software;Error analysis;Pattern recognition;Character recognition;Automatic speech recognition;Speech recognition;Statistical analysis","Bayes methods;pattern classification","style-constrained Bayesian classification;pattern fields;style context;order-independent field classification schemes;intraclass style;interclass style;field error;singlet error","Algorithms;Artificial Intelligence;Bayes Theorem;Image Enhancement;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Pattern Recognition, Automated;Reproducibility of Results;Sensitivity and Specificity","8","19","","","","","","IEEE","IEEE Journals & Magazines"
"On developing smart applications for VANETs: Where are we now? some insights on technical issues and open problems","G. Marfiay; G. Pau; M. Roccetti","Dipartimento di Science dell'Informazione, Università di Bologna - Italy 40136; Computer Science Department - University of California, Los Angeles, 90095, USA; Dipartimento di Science dell'Informazione, Università di Bologna - Italy 40136","2009 International Conference on Ultra Modern Telecommunications & Workshops","","2009","","","1","6","The advances in research on ad hoc networks, the availability of cheap radio interfaces (e.g. WiFi) and the increasing amount of electronic devices installed in vehicles have set the path for vehicular ad hoc networks. In the past few years, vehicular ad hoc network (VANET) research has addressed all layers, trying to optimize from the physical layer to the application layer to support the design of new possible application scenarios. It is often difficult to find a comprehensive approach to VANETs, due to their complexity. It is also often difficult to realize how far, technically, is the implementation of VANET-based application scenarios. We here propose to take one step in such direction, partially reviewing the research in this space and finding out how well can applications such as peer-to-peer file sharing and gaming can be supported. Our final scope is to provide an understanding of how far ahead in time, from the technological point of view, is the implementation of the cited scenarios on a distributed vehicular ad hoc network.","2157-0221;2157-023X","978-1-4244-3942-3978-1-4244-3941","10.1109/ICUMT.2009.5345634","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5345634","","Ad hoc networks;Space technology;Telecommunication traffic;Routing protocols;Electronic mail;Peer to peer computing;Testing;Broadcasting;Storms;Application software","ad hoc networks;communication complexity;mobile radio","VANET;distributed vehicular ad hoc network;cheap radio interfaces;electronic devices;peer-to-peer file sharing;gaming","","3","32","","","","","","IEEE","IEEE Conferences"
"Application profiling on Cell-based clusters","H. Dursun; K. J. Barker; D. J. Kerbyson; S. Pakin","Performance and Architecture Laboratory (PAL), Computer Science for HPC (CCS-1), Los Alamos National Laboratory, NM 87545, USA; Performance and Architecture Laboratory (PAL), Computer Science for HPC (CCS-1), Los Alamos National Laboratory, NM 87545, USA; Performance and Architecture Laboratory (PAL), Computer Science for HPC (CCS-1), Los Alamos National Laboratory, NM 87545, USA; Performance and Architecture Laboratory (PAL), Computer Science for HPC (CCS-1), Los Alamos National Laboratory, NM 87545, USA","2009 IEEE International Symposium on Parallel & Distributed Processing","","2009","","","1","8","In this paper, we present a methodology for profiling parallel applications executing on the IBM PowerXCell 8i (commonly referred to as the ldquoCellrdquo processor). Specifically, we examine Cell-centric MPI programs on hybrid clusters containing multiple Opteron and Cell processors per node such as those used in the petascale Roadrunner system. Our implementation incurs less than 3.2 mus of overhead per profile call while efficiently utilizing the limited local store of the Cell's SPE cores. We demonstrate the use of our profiler on a cluster of hybrid nodes running a suite of scientific applications. Our analyses of inter-SPE communication (across the entire cluster) and function call patterns provide valuable information that can be used to optimize application performance.","1530-2075","978-1-4244-3751-1978-1-4244-3750","10.1109/IPDPS.2009.5161092","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5161092","","Computer architecture;Performance analysis;Application software;Laboratories;Testing;Computer science;Collaboration;Concurrent computing;Computational modeling;Computer simulation","application program interfaces;message passing;multiprocessing systems;parallel processing;workstation clusters","Cell-based clusters;parallel application profiling;IBM PowerXCell 8i;Cell processor;Cell-centric MPI programs;Opteron processors;petascale Roadrunner system;scientific applications;interSPE communication","","","13","","","","","","IEEE","IEEE Conferences"
"Incremental Raycasting of Piecewise Quadratic Surfaces on the GPU","C. Stoll; S. Gumhold; H. Seidel","MPI Informatik. e-mail: stoll@mpi-inf.mpg.de; TU Dresden. e-mail: sg30@mail.inf.tu-dresden.de; MPI Informatik. e-mail: hpseidel@mpi-inf.mpg.de","2006 IEEE Symposium on Interactive Ray Tracing","","2006","","","141","150","To overcome the limitations of triangle and point based surfaces several authors have recently investigated surface representations that are based on higher order primitives. Among these are MPU, SLIM surfaces, dynamic skin surfaces and higher order iso-surfaces. Up to now these representations were not suitable for interactive applications because of the lack of an efficient rendering algorithm. In this paper we close this gap for implicit surface representations of degree two by developing highly optimized GPU implementations of the raycasting algorithm. We investigate techniques for fast incremental raycasting and cover per fragment and per quadric backface culling. We apply the approaches to the rendering of SLIM surfaces, quadratic iso-surfaces over tetrahedral meshes and bilinear quadrilaterals. Compared to triangle based surface approximations of similar geometric error we achieve only slightly lower frame rates but with much higher visual quality due to the quadratic approximation power of the underlying surfaces","","1-4244-0693","10.1109/RT.2006.280225","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4061556","raytracing;graphics hardware","Visualization;Hardware;Rendering (computer graphics);Computer graphics;Surface reconstruction;Geometry;Skin;Testing;Ellipsoids;Software algorithms","approximation theory;computational geometry;computer graphic equipment;data visualisation;ray tracing;rendering (computer graphics)","incremental raycasting;piecewise quadratic surface;GPU;graphics rendering;surface representation;backface culling;SLIM surface;tetrahedral mesh;bilinear quadrilateral;surface approximation;geometric error;quadratic approximation;graphics hardware","","4","21","","","","","","IEEE","IEEE Conferences"
"Performance Evaluation of a Hybrid Testbed for Wireless Mesh Networks","A. Zimmermann; D. Schaffrath; M. Wenig; A. Hannemann; M. Gunes; S. A. Makram","Department of Computer Science, Informatik 4, RWTH Aachen University, Ahornstr. 55, 52074 Aachen, Germany, zimmermann@cs.rwth-aachen.de; Department of Computer Science, Informatik 4, RWTH Aachen University, Ahornstr. 55, 52074 Aachen, Germany, schaffrath@cs.rwth-aachen.de; Department of Computer Science, Informatik 4, RWTH Aachen University, Ahornstr. 55, 52074 Aachen, Germany, wenig@cs.rwth-aachen.de; Department of Computer Science, Informatik 4, RWTH Aachen University, Ahornstr. 55, 52074 Aachen, Germany, hannemann@cs.rwth-aachen.de; Department of Computer Science, Informatik 4, RWTH Aachen University, Ahornstr. 55, 52074 Aachen, Germany, guenes@cs.rwth-aachen.de; Department of Computer Science, Informatik 4, RWTH Aachen University, Ahornstr. 55, 52074 Aachen, Germany, makram@cs.rwth-aachen.de","2007 IEEE International Conference on Mobile Adhoc and Sensor Systems","","2007","","","1","10","Wireless mesh networks (WMN) are supposed to provide flexible and high-performance wireless network access for large indoor and outdoor areas, e.g., community networking and metropolitan area networks. However, these claims are mostly substantiated by simulation studies only as real testbeds are inflexible and associated with high maintenance effort. In this work we present a hybrid, i.e., partly real and partly virtualized, WMN testbed. This provides a high degree of realism while still allowing the flexibility known from simulations. In addition to the architectural discussion we present measurement results from our testbed highlighting the optimization potential of small protocol parameter changes.","2155-6806;2155-6814","978-1-4244-1454-3978-1-4244-1455","10.1109/MOBHOC.2007.4428596","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4428596","","Wireless mesh networks;Routing protocols;Spine;Mobile ad hoc networks;IP networks;Controllability;Software testing;Computer science;Metropolitan area networks;Computational modeling","metropolitan area networks;radio networks","hybrid testbed performance evaluation;wireless mesh networks;community networking;metropolitan area networks;protocol parameters","","7","30","","","","","","IEEE","IEEE Conferences"
"Building Modular Vision Systems with a Graphical Plugin Environment","F. Lomker; S. Wrede; M. Hanheide; J. Fritsch","Bielefeld University, Germany; NA; NA; NA","Fourth IEEE International Conference on Computer Vision Systems (ICVS'06)","","2006","","","2","2","With the increasing interest in computer vision for interactive systems, the challenges of the development process involving many researchers are becoming more prominent. Issues like reuse of algorithms, modularity, and distributed processing are getting more important in the endeavor of building complex vision systems. We present a framework that allows independent development of enclosed components and supports interactive optimization of algorithmic parameters in an online fashion. The communication between components is performed nearly without any slow down compared to a monolithic system. Through the modular concept, all components can be flexibly distributed and reused in other application domains. The suitability of the approach is demonstrated with an example system.","","0-7695-2506","10.1109/ICVS.2006.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1578690","","Machine vision;Computer vision;Interactive systems;Visualization;Software libraries;Computer science;Distributed processing;Man machine systems;System testing;Real time systems","","","","6","25","","","","","","IEEE","IEEE Conferences"
"A novel controller design methodology for uncertain non-linear hardware-in-the-loop simulators","M. MacDiarmid; R. Daniel; M. Bacic","Dept. of Engineering Science, Oxford University, OX1 3PJ, UK; Dept. of Engineering Science, Oxford University, OX1 3PJ, UK; Dept. of Engineering Science, Oxford University, OX1 3PJ, UK","2007 46th IEEE Conference on Decision and Control","","2007","","","3478","3483","This paper describes a new methodology for the design of control systems for hardware-in-the-loop (HWIL) simulators. A HWIL simulator consists of a subset of hardware from a real object interfaced to a control system, with the goal of matching the closed loop behaviour of the simulator to that of the complete object. The simulator design problem is qualitatively different from the standard control design problem; while classical control design aims to reduce uncertainty in plant behaviour, controllers in HWIL simulators must instead match simulator behaviour to the behaviour of the real object, which are both sensitive to the underlying uncertainty in the hardware. This represents a new design challenge that has not yet been addressed in the literature. The design method presented here is general and non-heuristic, and is applicable to a large class of uncertain, non-linear systems. The method is based on the computation of a sub-optimal solution to an optimisation over the space of possible controllers. Experimental results are presented that verify the practicality of the design method.","0191-2216","978-1-4244-1497-0978-1-4244-1498","10.1109/CDC.2007.4434660","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4434660","","Design methodology;Hardware;Nonlinear control systems;Control system synthesis;Computational modeling;Control design;Uncertainty;Software design;System testing;Automotive engineering","closed loop systems;control system synthesis;nonlinear control systems;uncertain systems","uncertain nonlinear hardware-in-the-loop simulator;control system design;closed loop behaviour;HWIL simulator","","1","9","","","","","","IEEE","IEEE Conferences"
"Modelling of organic field-effect transistors for technology and circuit design","S. Mijalkovic; D. Green; A. Nejim; G. Whiting; A. Rankov; E. Smith; J. Halls; C. Murphy","Silvaco Technology Centre, Silvaco Data Systems (Europe), Compass Point, St Ives, Cambridge, UK; Silvaco Technology Centre, Silvaco Data Systems (Europe), Compass Point, St Ives, Cambridge, UK; Silvaco Technology Centre, Silvaco Data Systems (Europe), Compass Point, St Ives, Cambridge, UK; Cambridge Display Technologies, Unit 12 Cardinal Park, Cardinal Way, Godmanchester, PE29 2XG, UK; Cambridge Display Technologies, Unit 12 Cardinal Park, Cardinal Way, Godmanchester, PE29 2XG, UK; Cambridge Display Technologies, Unit 12 Cardinal Park, Cardinal Way, Godmanchester, PE29 2XG, UK; Cambridge Display Technologies, Unit 12 Cardinal Park, Cardinal Way, Godmanchester, PE29 2XG, UK; Cambridge Display Technologies, Unit 12 Cardinal Park, Cardinal Way, Godmanchester, PE29 2XG, UK","2008 26th International Conference on Microelectronics","","2008","","","469","476","As organic field-effect transistors (OFETs) are preparing to take a key role in the flexible and low cost electronics applications, there is a pressing need for predictive device models to support technology optimization and circuit design. This paper focuses on the specific OFET features that challenge current modelling approaches. The presented modelling techniques range from the fundamental semiconductor equations to compact device model representations as required for implementation in advanced TCAD and EDA commercial tools. The models are verified with measured characteristics of advanced OFET device structures.","","978-1-4244-1881-7978-1-4244-1882","10.1109/ICMEL.2008.4559324","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4559324","","OFETs;Circuit synthesis;Electronic design automation and methodology;Organic semiconductors;Organic electronics;Application software;Thin film transistors;Circuit testing;Predictive models;Mechanical factors","field effect transistors;organic semiconductors;semiconductor device models;technology CAD (electronics)","organic field-effect transistors;semiconductor equations;device model;circuit design;OFET device;TCAD;EDA","","6","24","","","","","","IEEE","IEEE Conferences"
"CD-HIT Workflow Execution on Grids Using Replication Heuristics","J. L. Vázquez-Poletti; E. Huedo; R. S. Montero; I. M. Llorente","NA; NA; NA; NA","2008 Eighth IEEE International Symposium on Cluster Computing and the Grid (CCGRID)","","2008","","","735","740","Grid computing has proven to be a solution for big workflow execution, especially in Bioinformatics. However, grid nature itself introduces overheads that make its use in many cases an unfeasible solution if considering wall-time. Different heuristics such as list scheduling, agglomeration and replication are available for optimizing workflow execution. In particular, the replication heuristics have been previously used in heterogeneous environments with good results. In this work, we analyze their use for workflow scheduling on Grid infrastructures. In particular, we study its applications to an intree workflow, generated by the distribution of the CD-HIT application. The experiments were conducted on a testbed made of resources from two different grids and results show a significant reduction of the workflow execution time.","","978-0-7695-3156","10.1109/CCGRID.2008.37","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4534296","Grid Computing;Bioinformatics;CD-HIT;Workflows;Replication Heuristics","Databases;Proteins;Grid computing;Bioinformatics;Workflow management software;Processor scheduling;System testing;Contracts;Performance analysis;Memory management","biology computing;grid computing;resource allocation;scheduling","grid computing;CD-HIT workflow execution;bioinformatics;replication heuristics;heterogeneous environment;workflow scheduling;resource allocation","","","22","","","","","","IEEE","IEEE Conferences"
"A systems approach to teaching trustworthy computing","A. Ganz; A. Kumar","Department of Electrical and Computer Engineering, University of Massachusetts Amherst, 01003, U.S.A.; Department of Electrical and Computer Engineering, University of Massachusetts Amherst, 01003, U.S.A.","2007 37th Annual Frontiers In Education Conference - Global Engineering: Knowledge Without Borders, Opportunities Without Passports","","2007","","","S1C-15","S1C-18","In this paper we present our experience of teaching the ""trustworthy computing"" graduate/undergraduate level pilot course in Fall 2006 as part of the computer engineering curriculum. The main objectives of the course are: 1) To make students realize the practical risks and concerns in system security, 2) To introduce the tools and methodologies used to secure a system; 3) To enable students to design a secure application and test its security features, and 4) To expose students to research issues in trustworthy computing. To achieve these objectives, the course introduces a number of new approaches to teaching trustworthy computing: 1) The course provides the students with a synthesis opportunity through a final project. The students are tasked to design a secure application using a system level approach that incorporates the security concepts presented in class. Using the Threat Modeling tool developed by Microsoft the students can validate and optimize their design; 2) The course exposes the students to state-of-the-art research issues in trustworthy computing and practical tools for system security implemented in real organizations. We achieve this exposure by inviting guest speakers from academia and industry; 3) The course provides the students with the opportunity to pursue research in applications not covered in class but using the concepts introduced in the course. The students present their findings to the class.","0190-5848;2377-634X","978-1-4244-1083-5978-1-4244-1084","10.1109/FIE.2007.4417909","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4417909","Trustworthy Computing;computer engineering;threat modeling tool","Education;Security;Earthquake engineering;Protocols;Application software;Safety;Design engineering;Bridges;Authentication;Engineering profession","computer science education;educational courses","trustworthy computing;graduate-undergraduate level pilot course;computer engineering curriculum;security concepts;threat modeling tool;Microsoft;system security","","1","5","","","","","","IEEE","IEEE Conferences"
"VTQ-Tree: Quadtree Access Method With Variable Threshold; Application To GIS","B. Bessaa; A. Belhadj-Aissa","Image Processing and Radiation Laboratory, Electronic and Computer Science Faculty - USTHB, Algiers, Algeria. bbessaa@yahoo.fr; Image Processing and Radiation Laboratory, Electronic and Computer Science Faculty - USTHB, Algiers, Algeria. h.belhadj@mailcity.com","2007 Urban Remote Sensing Joint Event","","2007","","","1","4","The spatial queries in a geographical information system (GIS) uses the space attributes related to the geometry of the geographical objects. In these queries, research is done according to the location of the object in space. In order to efficiently process these queries, one needs specific access method. Among these methods one finds, the grid file, the R-trees, the quadtrees. In this paper, we propose an alternative of the quadtree with a variable threshold and a modification of the point query algorithm in case of a topological representation model. Instead of having a fixed threshold of S objects by quadrant, we propose to vary this threshold according to the level of the tree depth. Initially one starts with a threshold S<sub>0</sub>, then the threshold is calculated according to the depth level dl with the root depth which is supposed to be null. In GIS the representation and the density of the space objects depend one hand on the scale and/or the resolution of the initial image and on the other hand the space data model representation. For the topological model representation, we propose an optimization of the point query algorithm in order to play down the research time. Finally we tested the proposed algorithms on a data files.","2334-0932","1-4244-0711-71-4244-0712","10.1109/URS.2007.371832","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4234431","Geographical information systems;spatial access method;quadtree;models of representation","Geographic Information Systems;Image processing;Computer science;Information systems;Remote sensing;Application software;Computational geometry;Information geometry;Image resolution;Data models","geographic information systems;quadtrees;query processing","VTQ-tree;quadtree access;variable threshold;geographical information system;point query algorithm;topological representation model;tree depth;data files;spatial access method","","","9","","","","","","IEEE","IEEE Conferences"
"High Resolution FMCW Radar System for Investigating the Polar Ice Caps","C. Cardenas; M. Jenett; J. Winkelmann","Direcci&#x00F3;n de Programas Ant&#x00E1;rticos, Universidad de Magallanes, Casilla 113-D, Punta Arenas, Chile; Hamburg University of Technology, Denickestrasse 22, 21073, Germany; Hamburg University of Technology, Denickestrasse 22, 21073, Germany","German Microwave Conference","","2008","","","1","4","This paper presents a newly developed high resolution FMCW (Frequency Modulated Continuous Wave) radar system for sounding the sub ice topography of the polar ice caps. The system, working in the frequency range from 200 - 400 MHz, was developed to measure thickness and internal layers in cold ice with a resolution better than 1 meter. This also gives the possibility to measure snow accumulation rates, an important input for an improved knowledge of the mass balance of polar ice sheets. First measurements for the test, calibration and optimization of the new ice sounder were taken near Ellsworth Mountains, at Patriot Hills, West Antarctica, at 80deg 18' S, 81deg22' W. They were carried out in the frame of a joint Chilean-German research project of the Universidad de Magallanes (UMAG), Punta Arenas, Chile and Hamburg University of Technology (TUHH), Hamburg, Germany. The complete radar system was installed on a ski mounted support frame, pulled by hand across the ice surface. The measurement results presented in this paper show the capabilities of the developed system to measure ice thickness up to 1000 m and internal layers of the ice body as well.","","978-3-8007-3086","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5756977","","Radar antennas;Radar;Transmitters;Software measurement;Thickness measurement;Receivers","","","","","","","","","","","VDE","VDE Conferences"
"Trajectory planning of a 6-DOF robot based on RBF neural networks","Qingwen Qu; Jixiang Wan; Xiujun Sun","School of Mechanical Engineer, Shandong University of Technology, Zibo, China; School of Mechanical Engineer, Shandong University of Technology, Zibo, China; School of Mechanical Engineer, Shandong University of Technology, Zibo, China","2007 IEEE International Conference on Robotics and Biomimetics (ROBIO)","","2007","","","324","329","A new method for smooth trajectory planning of a 6-DOF robot in joint space is described in this paper. By the researching processes of concrete analysis of trajectory planning on robot's manipulator arm, imitation of trajectory based on kinematics and optimization of trajectory in the joint space, an one-input-six-output RBF neural network model is built and trained taking the discrete time as input and the values of six angles as outputs in joint space. With character of rapid convergence and near approximation, this new algorithm is fault tolerant and irrelative with order of inputs, which can ensure the result trajectory is firing enough. The algorithm has been tested in simulation when the virtual model of the robot was established in software ADAMS, yielding good results by studying the kinematics and the dynamics performance of the robot.","","978-1-4244-1761-2978-1-4244-1758","10.1109/ROBIO.2007.4522182","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4522182","Trajectory planning;RBF neural network;Joint space","Trajectory;Robots;Neural networks;Orbital robotics;Kinematics;Concrete;Process planning;Manipulators;Convergence;Approximation algorithms","control engineering computing;manipulator dynamics;manipulator kinematics;path planning;radial basis function networks","6-DOF robot;RBF neural networks;smooth trajectory planning;joint space;kinematics;manipulator arm;trajectory optimization;robot virtual model","","2","11","","","","","","IEEE","IEEE Conferences"
"Wavelet-based compressive Super-Resolution","Na Fan","Department of Electronic Engineering, East China Normal University, China","2009 Workshop on Applications of Computer Vision (WACV)","","2009","","","1","6","A wavelet based compressive sampling Super Resolution algorithm is developed, in which the energy function optimization is approximated numerically via the Regularized Orthogonal Matching Pursuit. The proposed algorithm works well with a smaller quantity of training image patches and outputs images with satisfactory subjective quality. It is tested on classical benchmark images commonly adopted by Super Resolution researchers with both generic and specialized training sets for comparison with other popular commercial software and state-of-the-art methods. Experiments demonstrate that, the proposed algorithm is competitive among contemporary Super Resolution methods.","1550-5790;1550-5790","978-1-4244-5498-3978-1-4244-5497-6978-1-4244-5496","10.1109/WACV.2009.5403110","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5403110","","Strontium;Image resolution;Pixel;Matching pursuit algorithms;Image coding;Energy resolution;Spatial resolution;Image sampling;Degradation;Wavelet transforms","image matching;image resolution;wavelet transforms","compressive super resolution;wavelet based compressive sampling;energy function optimization;regularized orthogonal matching pursuit","","1","15","","","","","","IEEE","IEEE Conferences"
"Plant Classification Combining Colour and Spectral Cameras for Weed Control Purposes","P. J. Komi; M. R. Jackson; R. M. Parkin","Mechatronics Research Centre, Loughborough University, Loughborough, United Kingdom. Email: p.j.komi@lboro.ac.uk; Mechatronics Research Centre, Loughborough University, Loughborough, United Kingdom. Email: m.r.jackson@lboro.ac.uk; Prof, Mechatronics Research Centre, Loughborough University, Loughborough, United Kingdom. Email: r.m.parkin@lboro.ac.uk","2007 IEEE International Symposium on Industrial Electronics","","2007","","","2039","2042","Weed plant detection and classification is a difficult task for any computer vision system. Previous studies show promising results with either colour camera or spectral imaging solutions. However, typical colour camera solutions have found it hard to deal with overlapping leaves, and spectral solutions often lack in the spatial resolution required for accurate leaf level detection. In this paper a novel system for weed detection and classification is presented using both low-cost RGB (Red, Green, Blue) colour and spectral (400 - 1000 nm) cameras combining the strengths of these individual technologies. The system presented performs accurate leaf level classification and is capable of identification at 97.6% with non-overlapping full leaves in laboratory under controlled lighting conditions. Plant leaf samples from 6 different plant types were used. With dedicated hardware and optimized software the system should be capable of at least 5 km/h real-time operation in field conditions.","2163-5137;2163-5145","978-1-4244-0754-5978-1-4244-0755","10.1109/ISIE.2007.4374921","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4374921","","Cameras;Spatial resolution;Testing;Pixel;Crops;High-resolution imaging;Charge-coupled image sensors;Automatic control;Calibration;Control systems","agriculture;computer vision;image classification;image colour analysis","spectral imaging;computer vision system;weed plant classification;weed plant detection;weed control;spectral cameras;colour cameras","","2","6","","","","","","IEEE","IEEE Conferences"
"Gear-Shift Decoding","M. Ardakani; F. R. Kschischang","NA; NA","IEEE Transactions on Communications","","2006","54","6","1143","1143","This paper considers a class of iterative message-passing decoders for low-density parity-check codes in which the decoder can choose its decoding rule from a set of decoding algorithms at each iteration. Each available decoding algorithm may have different per-iteration computation time and performance. With an appropriate choice of algorithm at each iteration, overall decoding latency can be reduced significantly, compared with standard decoding methods. Such a decoder is called a gear-shift decoder because it changes its decoding rule (shifts gears) in order to guarantee both convergence and maximum decoding speed (minimum decoding latency). Using extrinsic information transfer charts, the problem of finding the optimum (minimum decoding latency) gear-shift decoder is formulated as a computationally tractable dynamic program. The optimum gear-shift decoder is proved to have a decoding threshold equal to or better than the best decoding threshold among those of the available algorithms. In addition to speeding up software decoder implementations, gear-shift decoding can be applied to optimize a pipelined hardware decoder, minimizing hardware cost for a given decoder throughput.","0090-6778;1558-0857","","10.1109/TCOMM.2006.876885","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1643543","","Iterative decoding;Detectors;Signal processing;Phase shift keying;Testing;Iterative algorithms;Delay;Signal detection;Hardware;Fading","","","","5","","","","","","","IEEE","IEEE Journals & Magazines"
"AntiWorm NPU-based Parallel Bloom filters in Giga-Ethernet LAN","Z. Chen; C. Lin; J. Ni; D. Ruan; B. Zheng; Z. Tan; Y. Jiang; X. Peng; A. Luo; B. Zhu; Y. Yue; Y. Wang; P. Ungsunan; F. Ren","Phone: 86-10-62772487, Fax: 86-10-62771138, E-mail: zhenchen@csnet1.cs.tsinghua.edu.cn.; IEEE Member; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA","2006 IEEE International Conference on Communications","","2006","5","","2118","2123","In this paper, an AntiWorm system based on the Intel IXP Network Processor was implemented using the Parallel Bloom filters technique. The AntiWorm system consists of two components: Bloom filters and Exact Matching engines. The Parallel Bloom filters can identify the suspicious traffic quickly and effectively, and then dispatch them to Exact Matching engines for further investigation. Both the principles and the implementation of the AntiWorm system are introduced in detail. With the consideration of the system performance parameters, two feasible implementation solutions are investigated and the advantages and disadvantages are also compared. The selections of configuration parameters of the AntiWorm system are also discussed. A hash scheme based on MD5's function is proposed for implementing fast hash functions. To test the performance of the AntiWorm system, such as throughput and delay, some experiments are carried out with different simulated traffic condition. The internal statistics of IXP network processor are also collected and analyzed for optimizing the system performance. To demonstrate the operation of the AntiWorm system, assaults by Worm Blaster are used in the test bed, and the experimental results prove the effectiveness of the AntiWorm system. The Software Package WormDetector1.0 is also provided as a software release from the research.","1550-3607;1938-1883","1-4244-0355","10.1109/ICC.2006.255083","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4024478","Computer Networks;Network Security;Network Processors;IXP Network Processor;Worms;Parallel Bloom filters;Worm Blaster","Local area networks;Matched filters;Engines;Telecommunication traffic;System performance;System testing;Throughput;Delay;Traffic control;Statistical analysis","","","","1","31","","","","","","IEEE","IEEE Conferences"
"Application-Level QoS: Improving Video Conferencing Quality through Sending the Best Packet Next","I. McDonald; R. Nelson","NA; NA","2008 The Second International Conference on Next Generation Mobile Applications, Services, and Technologies","","2008","","","507","513","In a traditional network stack, data from an application is transmitted in the order that it is received. An algorithm is proposed where information about the priority of packets and expiry times is used by the transport layer to reorder or discard packets at the time of transmission to optimise the use of available bandwidth. This can be used for video conferencing to prioritise important data. This scheme is implemented and compared to unmodified datagram congestion control protocol (DCCP). This algorithm is implemented as an interface to DCCP and tested using traffic modelled on video conferencing software. The results show improvement can be made to video conferencing during periods of congestion - substantially more audio packets arrive on time with the algorithm, which leads to higher quality video conferencing. In many cases video packet arrival rate also increases and adopting the algorithm gives improvements to video conferencing that are better than using unmodified queuing for DCCP. The algorithm proposed is implemented on the server only, so benefits can be obtained on the client without changes being required to the client.","2161-2889;2161-2897","978-0-7695-3333","10.1109/NGMAST.2008.16","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4756479","","Videoconference;Streaming media;Traffic control;Testing;Telecommunication traffic;Next generation networking;Mobile computing;Computer networks;Bandwidth;Protocols","data communication;quality of service;telecommunication congestion control;telecommunication traffic;teleconferencing;transport protocols","application-level QoS;video conferencing;datagram congestion control protocol;traffic modelling;congestion;audio packets;video packet arrival rate;unmodified queuing;data transmission;send best packet next algorithm","","3","29","","","","","","IEEE","IEEE Conferences"
"Model Checking Safety-Critical Systems Using Safecharts","P. Hsiung; Y. Chen; Y. Lin","Department of Computer Science and Information Engineering, National Chung Cheng University, 168, University Road, Min-Hsiung, Chiayi, Taiwan-621, ROC; Department of Computer Science and Information Engineering, National Chung Cheng University, 168, University Road, Min-Hsiung, Chiayi, Taiwan-621, ROC; National Chiao Tung University, Hsinchu, Taiwan- 704, ROC","IEEE Transactions on Computers","","2007","56","5","692","705","With rapid developments in science and technology, we now see the ubiquitous use of different types of safety-critical systems in our daily lives such as in avionics, consumer electronics, and medical systems. In such systems, unintentional design faults might result in injury or even death to human beings. To make sure that safety-critical systems are really safe, there is a need to verify them formally. However, the verification of such systems is getting more and more difficult because designs are becoming very complex. To cope with high design complexity, currently, model-driven architecture design is becoming a well-accepted trend. However, existing methods of testing and standards conformance are restricted to implementation code, so they do not fit very well with model-based approaches. To bridge this gap, we propose a model-based formal verification technique for safety-critical systems. In this work, the model-checking paradigm is applied to the Safecharts model, which was used for modeling but not yet used for verification. Our contributions listed are as follows: first, the safety constraints in Safecharts are mapped to semantic equivalents in timed automata for verification. Second, the theory for safety constraint verification is proven and implemented in a compositional model checker (that is, the state-graph manipulator (SGM)). Third, prioritized and urgent transitions are implemented in SGM to model the risk semantics in Safecharts. Finally, it is shown that the priority-based approach to mutual exclusion of resource usage in the original Safecharts is unsafe and corresponding solutions are proposed. Application examples show the feasibility and benefits of the proposed model-driven verification of safety-critical systems","0018-9340;1557-9956;2326-3814","","10.1109/TC.2007.1021","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4141241","Safety-critical systems;model checking;Safecharts;extended timed automaton.","","automata theory;computational linguistics;formal verification;graph theory;safety-critical software","model checking;safety-critical systems;Safecharts;design complexity;model-driven architecture design;model-based formal verification technique;semantic equivalents;timed automata;safety constraint verification;compositional model checker;state-graph manipulator;risk semantics;mutual exclusion;resource usage","","14","29","","","","","","IEEE","IEEE Journals & Magazines"
"Current-Mode System-on-Chip Interface for SPR-Based Sensing Systems","D. M. Wilson; L. E. Hansen","NA; NA","IEEE Sensors Journal","","2007","7","11","1513","1523","This paper presents a fully integrated hardware solution to processing signals obtained from biochemical sensors that employ surface plasmon resonance (SPR)-based transduction mechanisms. Results are discussed in the context of previous software-based, partial hardware-based, and alternative fully hardware-based solutions for portable SPR systems. As in previous alternative design approaches, this system-on-chip solution is tested in the context of a highly portable sensing configuration consisting of a fiber-based optical path and LED-based light source using a nonfunctionalized SPR probe. The results are applicable to both functionalized (analyte-specific) and nonfunctionalized (bulk refractive index) sensing systems. The output of the chip is an optimized single voltage that represents the refractive index of the sensing environment. The single-chip solution is a novel combination of a photodiode, a photodiode biasing scheme, current-mode dark current/fixed pattern noise compensation, programmable current-mode background (reference) compensation, and an integration stage for weighting signals from multiple wavelengths to compute a single voltage output. Experimental results, including the effects of electronic noise, batch mismatch, and quantization error demonstrate a 6.8 resolution in refractive index units. This result is markedly improved over previous fully integrated solutions (3 resolution), and is comparable to traditional-software solutions (5 resolution) to SPR-based sensing problems.","1530-437X;1558-1748;2379-9153","","10.1109/JSEN.2007.907052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4339514","Analog integrated circuits;analog very large scale integration (VLSI);chemical sensors;optical computation;optical sensors;surface plasmon resonance (SPR)","System-on-a-chip;Refractive index;Signal resolution;Optical fiber testing;Voltage;Photodiodes;Hardware;Signal processing;Biosensors;Optical surface waves","chemical sensors;photodiodes;refractive index;surface plasmon resonance;system-on-chip","system-on-chip interface;SPR-based sensing systems;biochemical sensors;surface plasmon resonance;transduction mechanisms;fiber-based optical path;LED-based light source;photodiode;programmable current-mode background compensation;current-mode dark pattern noise compensation;electronic noise;batch mismatch;quantization error;refractive index","","2","14","","","","","","IEEE","IEEE Journals & Magazines"
"Parallel Environment with High Accuracy for Resolution of Numerical Problems","C. Amaral Holbig; T. Asmuz Diverio; D. Moraes Claudio","NA; NA; NA","IEEE Latin America Transactions","","2009","7","1","114","121","In this paper we describe a high performance environment, like cluster computers, with high accuracy obtained by use of C-XSC library. The C-XSC library is a (free) C++ class library for scientific computing for the development of numerical algorithms delivering highly accurate and automatically verified results by use of the interval arithmetic. These calculus in high accuracy must be available for some basic arithmetic operations, mainly the operations that accomplish the summation and dot product. Because of these aspects, we wish to use the high performance through a cluster environment where we have several nodes executing tasks or calculus. The communication will be done by message passing using the MPI communication library. To obtain the high accuracy in this environment extensions or changes in the parallel programs had done to guarantee that the quality of final result done on cluster, where several nodes collaborate for the final result of the calculus, maintain the same result quality obtained in one sequential high accuracy environment. To validate the environment developed in this work we done basic tests about the dot product, the matrix multiplications, the implementation of interval solvers for banded and dense matrices and the implementation of some numeric methods to solve linear systems with the high accuracy characteristic (some of the methods implemented are used in real life applications like hydrodynamic, agriculture and power electric systems). With these tests we done analysis and comparisons about the performance and accuracy obtained with and without the use of C-XSC library in sequential and parallel programs. With the implementation of these routines and methods will be open a large research field about the study of real life applications that need during their resolution (or in part of their resolution) to calculate arithmetic operations with more accuracy than the accuracy obtained by the traditional computational tools. Our software run on labtec (UFRGS) and Colorado (UPF) clusters. Nowadays we are working in the implementation of parallel versions of programs to solve linear systems (without and with high accuracy) and the optimization of C-XSC library on cluster computers.","1548-0992","","10.1109/TLA.2009.5173473","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5173473","Cluster computing;high accuracy;linear equations;numerical library;verified computing","Libraries;Arithmetic;Calculus;Linear systems;High performance computing;Scientific computing;Clustering algorithms;Message passing;Collaborative work;Life testing","mathematics computing;message passing;numerical analysis;parallel algorithms;workstation clusters","parallel environment;numerical problem;high performance environment;cluster computers;C-XSC library;C++ class library;scientific computing;numerical algorithms;interval arithmetic;arithmetic operation;cluster environment;message passing;MPI communication library;environment extension;parallel programs;matrix multiplication;linear systems;sequential program","","1","22","","","","","","IEEE","IEEE Journals & Magazines"
"The Multi-Agent Data Collection in HLA-Based Simulation System","H. Song; Z. Shen; C. Miao; A. Tan; G. Zhao","Nanyang Technical University, Singapore; Nanyang Technological University, Singapore; Nanyang Technical University, Singapore; Nanyang Technical University, Singapore; Nanyang Technological University, Singapore","21st International Workshop on Principles of Advanced and Distributed Simulation (PADS'07)","","2007","","","61","69","The high level architecture (HLA) for distributed simulation was proposed by the Defense Modeling and Simulation Office of the Department of Defense (DOD) in order to support interoperability among simulations as well as reuse of simulation models. One aspect of reusability is to collect and analyze data generated in simulation exercises, including a record of events that occur during the execution, and the states of simulation objects. In order to improve the performance of existing data collection mechanisms in the HLA simulation system, the paper proposes a multi-agent data collection system. The proposed approach adopts the hierarchical data management/organization mechanism to achieve fast data access which is indispensable to the analysis of simulation exercise. Furthermore, the multi-agent data collection system adopts a formalization expression method to describe the system behavioral characteristics, and implements the hierarchy language supports to the description by combing the XML and Petri net. In addition, we propose an independent reinforcement learning algorithm to generate optimized joint recording program which guarantees that the data collection and query tasks can be rationally distributed among logging agents as well as efficiently utilize computational resource. The testing results indicate that the proposed approach, under the premise of complete collection of simulation data, not only reduces the network load imposed by data collection components, but also provides effective supports to the analysis of simulation exercise.","1087-4097","0-7695-2898","10.1109/PADS.2007.30","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4262792","","Analytical models;Computational modeling;Computer simulation;Distributed computing;Computer architecture;Discrete event simulation;Internet;Telecommunication traffic;Traffic control;Wide area networks","data handling;digital simulation;learning (artificial intelligence);multi-agent systems;open systems;Petri nets;query processing;software architecture;XML","multi-agent data collection;high level architecture;distributed simulation;interoperability;hierarchical data management;XML;Petri net;reinforcement learning algorithm;query tasks;hierarchical data organization mechanism","","3","17","","","","","","IEEE","IEEE Conferences"
"A Microcoded Elliptic Curve Processor for GF(2m) Using FPGA Technology","Q. Pu; J. Huang","Department of Electronics, Science Institute, Information Engineering University, Zhengzhou, He'nan, China. Email: monkey_joan@sina.com.cn; NDSC, Information Engineering Institute, Information Engineering University, Zhengzhou, He'nan, China. Email: ghthjh@sina.com.cn","2006 International Conference on Communications, Circuits and Systems","","2006","4","","2771","2775","The implementation of a microcoded elliptic curve processor for GF(2<sup>m</sup>) using FPGA technology is described. It is a scalable architecture in terms of area and speed that exploits the abilities of reconfigurable hardware to deliver optimized circuitry for different elliptic curves and finite fields. And it simply consists of only one arithmetic unit and one control unit, which is microcoded, facilitating rapid development and functional extension: for example, new applications could be supported simply by inserting more microcodes. The design was successfully tested on a Xilinx Virtex2 XC2V1000-256 device and could perform elliptic curve scalar multiplications with arbitrary points in 0.167 msec in the field GF(2<sup>193</sup>).","","0-7803-9584-00-7803-9585","10.1109/ICCCAS.2006.285243","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4064490","","Elliptic curves;Field programmable gate arrays;Hardware;Elliptic curve cryptography;Public key cryptography;Very large scale integration;Computer architecture;Circuits;Galois fields;Arithmetic","field programmable gate arrays;public key cryptography;software architecture","microcoded elliptic curve processor;GF(2<sup>m</sup>);FPGA technology;reconfigurable hardware;Xilinx Virtex2 XC2V1000-256 device","","3","21","","","","","","IEEE","IEEE Conferences"
"Mechanical characterization of microelectronics embedded in flexible and stretchable substrate","L. Wang; T.zoumpoulidis; M. Bartek; A. Polyakov; K. M. B. Jansen; L. J. Ernst","NA; NA; NA; NA; NA; NA","2006 8th Electronics Packaging Technology Conference","","2006","","","766","772","The acceptable flexibility and stretchability for ultra-thin substrate would be reached by embedding the ultra-thin substrate into the flexible polyimide/silica rubber and patterning the poly-silicon or silicon into square/hexagon segmentations. The segments interconnect by metal wires for the signal communication, different wire shapes are designed to reach more flexible. In this contribution, results of FE optimization on mechanical reliability issues of interconnect in-between segments on ultra-thin polyimide/silica rubber substrates are presented. Generation of cracks within the silicon and dielectric layers is then studied under controlled bending (glass cylinders with diameters of 2 - 10 mm, compressive and tensile stress) using specially for this purpose designed bending tools. Specimen observation is done using an optical microscope with possibility of digital recording and evaluation by pattern recognition software. The results show that the cracks appear first in the dielectric layers in-between the silicon layer segments and only at higher loads propagate or are generated within the silicon itself. The development of first cracks depends significantly on the silicon layer segmentation size which affects both the crack density and the crack width. The crack density increases sharply with the strain for early stage and then increases slightly. The crack width increases steadily. The high flexibility result can be reached that no crack be detected under the bending tests with 2 mm diameters by optimizing segment size and gap. Multilevel FEM simulations are performed in order to increase understanding of the major failure processes and optimizing the interconnect geometry. Results of simulations and experiments compare quite well. Second generation samples are designed with consideration of interconnect in-between segments and avoid of silicon dioxide in-between segments.","","1-4244-0664-11-4244-0665","10.1109/EPTC.2006.342809","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4147338","","Microelectronics;Silicon compounds;Polyimides;Rubber;Dielectric substrates;Optical microscopy;Wires;Shape;Signal design;Iron","cracks;elemental semiconductors;finite element analysis;flexible electronics;integrated circuit interconnections;integrated circuit reliability;silicon;substrates","mechanical characterization;embedded microelectronics;flexible substrate;stretchable substrate;FE optimization;mechanical reliability;ultrathin polyimide substrates;silica rubber substrates;glass cylinders;compressive stress;tensile stress;optical microscope;silicon layer segments;crack density;crack width;multilevel FEM simulations;2 to 10 mm;Si","","5","8","","","","","","IEEE","IEEE Conferences"
"Computation of iron losses in permanent magnet machines by multi-domain simulations","G. Ugalde; G. Almandoz; J. Poza; A. Gonzalez","Faculty of Engineering, University of Mondragon, Loramendi 4, Aptdo 23, 20500 Mondragon (Spain); Faculty of Engineering, University of Mondragon, Loramendi 4, Aptdo 23, 20500 Mondragon (Spain); Faculty of Engineering, University of Mondragon, Loramendi 4, Aptdo 23, 20500 Mondragon (Spain); Orona Eic. Elevator Innovation Centre","2009 13th European Conference on Power Electronics and Applications","","2009","","","1","10","Nowadays the majority of electric drives work at variable speeds. In this kind of drives the electrical machines are supplied by inverters which generate modulated voltages. It is widely known that these non sinusoidal voltages cause additional current harmonics dealing to higher iron losses than with sinusoidal voltages. The iron losses affect to several design constraints as the efficiency and the thermal behavior of electrical machine. Hence, the accurate computation of the iron losses under different supply voltage conditions is very important in order to optimize the machine design as much as possible. This work deals with multi-domain simulations in order to calculate the iron losses in permanent magnet synchronous machines (PMSM) under Pulse Width Modulated (PWM) voltages. The current supplies are implemented in the simulation system MATLAB-SIMULINK<sup>reg</sup>. Whereas the electrical machine is simulated using the Finite Element Method (FEM). The iron losses are computed by a post-processing analysis carried out using the tool so called Loss Surface Model (LSM) which is integrated in the FEM software FLUX<sup>reg</sup> of Cedrat. Finally, experimental tests are performed in order to validate the proposed methodology.","","978-1-4244-4432-8978-90-75815-13","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5279171","Iron Losses;Permanent Magnet Synchronous Machines;PWM voltages;Multi-Domain Simulations","Iron;Magnetic losses;Permanent magnet machines;Computational modeling;Voltage;Pulse width modulation;Pulse width modulation inverters;Design optimization;Space vector pulse width modulation;Magnetic modulators","finite element analysis;permanent magnet machines;power engineering computing;PWM invertors;synchronous machines;variable speed drives","permanent magnet machines;multidomain simulations;synchronous machines;PWM voltages;variable speed drives;electrical machines;inverters;iron losses;finite element method;loss surface model;MATLAB-SIMULINK","","1","11","","","","","","IEEE","IEEE Conferences"
"A Heuristic for Phylogenetic Reconstruction Using Transposition","F. Yue; M. Zhang; J. Tang","Department of Computer Science and Engineering, University of South Carolina, Columbia, SC 29208, USA. Email: yue2@cse.sc.edu; College of Computer Science and Technology, Jilin University, Changchun 130012, China. Email: zm@mail.edu.cn; Department of Computer Science and Engineering, University of South Carolina, Columbia, SC 29208, USA. Email: jtang@cse.sc.edu","2007 IEEE 7th International Symposium on BioInformatics and BioEngineering","","2007","","","802","808","Because of the advent of high-throughput sequencing and the consequent reduction in cost of sequencing, many organisms have been completely sequenced and most of their genes identified; homologies among these genes are also getting established. It thus has become possible to represent whole genomes as ordered lists of gene identifiers and to study the evolution of these entities through computational means, in systematics as well as in comparative genomics. As a result, gene order data (also known as genome rearrangement data) has attracted increasing attention from both biologists and computer scientists as a new type of data for phylogenetic analysis. Methods for reconstructing phylogeny from genome rearrangements include distance-based methods, MCMC methods and direct optimization methods. The latter, pioneered by Sankoff and extended in the software packages of grappa and MGR, is the most accurate approach for inversion phylogeny. However, due to the difficulty of computing the transposition distance, this type of methods has not been applied to datasets where transposition is the only or dominant event. In this paper, we present a heuristic transposition median solver and extend grappa to handle transpositions. Our extensive testing using simulated datasets shows that this method (GRAPPA-TP) is very accurate in terms of ancestor genome inference and phylogenetic reconstruction. It also suggests that model match is critical in phylogenetic analysis, and a fast and accurate method for transposition distance computation is still very important. The new GRAPPA-TP is available from phylo.cse.sc.edu.","","1-4244-1509-8978-1-4244-1509","10.1109/BIBE.2007.4375652","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4375652","","Phylogeny;Genomics;Bioinformatics;Biology computing;Systematics;Costs;Organisms;Evolution (biology);Data analysis;Optimization methods","biology computing;genetics;heuristic programming","phylogenetic reconstruction;high-throughput sequencing;heuristic transposition median solver;extend GRAPPA;ancestor genome inference","","1","29","","","","","","IEEE","IEEE Conferences"
"Control scheme for a multiphase matrix converter EUROCON2009","J. Szczepanik; T. Sienko","Cracow University of Technology, Faculty of Electrical and Computer Engineering, Department of Electromechanical Energy Conversion, ul. Warszawska 24, 31-155 Kraków, Poland; Cracow University of Technology, Faculty of Electrical and Computer Engineering, Department of Electromechanical Energy Conversion, ul. Warszawska 24, 31-155 Kraków, Poland","IEEE EUROCON 2009","","2009","","","1996","2002","A matrix NxM multi phase converter is a very simple structure incorporating NxM bi-directional switches, able to convert input voltages into output voltages of any shape and frequency. However, commutation problems and complicated control algorithms keep it from being utilized on a large scale. The paper concentrates on the development of a control algorithm for a multiphase matrix converter, which can fulfill all requirements imposed by the proposed application in power systems where sinusoidal shape of the voltage is important. The proposed algorithm is then tested using Matlab Simulink software and optimized for an application in digital controller","","978-1-4244-3860-0978-1-4244-3861","10.1109/EURCON.2009.5167921","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5167921","Matrix converter;Multiphase matrix control procedure;Power system control","Matrix converters;Shape control;Application software;Switching converters;Bidirectional control;Switches;Frequency conversion;Large-scale systems;Voltage control;Control systems","","","","","5","","","","","","IEEE","IEEE Conferences"
"IMC: A communication protocol for networked vehicles and sensors","R. Martins; P. S. Dias; E. R. B. Marques; J. Pinto; J. B. Sousa; F. L. Pereira","LSTS - Underwater Systems and Technology Laboratory, Faculdade de Engenharia da Universidade do Porto, Rua Dr. Roberto Frias s/n, 4200-465, Portugal; LSTS - Underwater Systems and Technology Laboratory, Faculdade de Engenharia da Universidade do Porto, Rua Dr. Roberto Frias s/n, 4200-465, Portugal; LSTS - Underwater Systems and Technology Laboratory, Faculdade de Engenharia da Universidade do Porto, Rua Dr. Roberto Frias s/n, 4200-465, Portugal; LSTS - Underwater Systems and Technology Laboratory, Faculdade de Engenharia da Universidade do Porto, Rua Dr. Roberto Frias s/n, 4200-465, Portugal; LSTS - Underwater Systems and Technology Laboratory, Faculdade de Engenharia da Universidade do Porto, Rua Dr. Roberto Frias s/n, 4200-465, Portugal; LSTS - Underwater Systems and Technology Laboratory, Faculdade de Engenharia da Universidade do Porto, Rua Dr. Roberto Frias s/n, 4200-465, Portugal","OCEANS 2009-EUROPE","","2009","","","1","6","This paper presents the Inter-Module Communication (IMC) protocol, a message-oriented protocol designed and implemented in the Underwater Systems and Technology Laboratory (LSTS) to build interconnected systems of vehicles, sensors and human operators that are able to pursue common goals cooperatively by exchanging real-time information about the environment and updated objectives. IMC abstracts hardware and communication heterogeniety by providing a shared set of messages that can be serialized and transferred over different means. The described protocol contrasts with other existing application level protocols by not imposing or assuming a specific software architecture for client applications. Native support can be automatically generated for different programming languages and/or computer architectures resulting in optimized code which can be used both for networked nodes and also for inter-process and inter-thread communication. The protocol has already been tested throughout various experiments led by LSTS where it has taken care of communications between vehicles, sensors and operator consoles. We are now developing the protocol in the direction of having multi-vehicle cooperation using live data from environmental sensors and mixed-initiative user interaction.","","978-1-4244-2522-8978-1-4244-2523","10.1109/OCEANSE.2009.5278245","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5278245","","Protocols;Application software;Underwater communication;Interconnected systems;Underwater vehicles;Sensor systems;Humans;Real time systems;Abstracts;Hardware","computer networks;protocols;road vehicles;wireless sensor networks","networked vehicles;networked sensors;inter-module communication protocol;message-oriented protocol;Underwater Systems and Technology Laboratory;interconnected systems;application level protocols;native support;programming languages;computer architectures;networked nodes;inter-process communication;inter-thread communication;multi-vehicle cooperation;environmental sensors;mixed-initiative user interaction","","32","25","","","","","","IEEE","IEEE Conferences"
"Highly parallel decoding of space-time codes on graphics processing units","K. C. Bollapalli; Y. Wu; K. Gulati; S. Khatri; A. R. Calderbank","Department of Electrical & Computer Engineering, Texas A&M University, College Station, TX 77801; Department of Electrical Engineering, Princeton University, Princeton, NJ 08544; Department of Electrical & Computer Engineering, Texas A&M University, College Station, TX 77801; Department of Electrical & Computer Engineering, Texas A&M University, College Station, TX 77801; Department of Electrical Engineering, Princeton University, Princeton, NJ 08544","2009 47th Annual Allerton Conference on Communication, Control, and Computing (Allerton)","","2009","","","1262","1269","Graphics processing units (GPUs) with a few hundred extremely simple processors represent a paradigm shift for highly parallel computations. We use this emergent GPU architecture to provide a first demonstration of the feasibility of real time ML decoding (in software) of a high rate space-time block code that is representative of codes incorporated in 4th generation wireless standards such as WiMAX and LTE. The decoding algorithm is conditional optimization which reduces to a parallel calculation that is a natural fit to the architecture of low cost GPUs. Experimental results demonstrate that asymptotically the GPU implementation is more than 700 times faster than a standard serial implementation. These results suggest that GPU architectures have the potential to improve the cost / performance tradeoff of 4th generation wireless base stations. Additional benefits might include reducing the time required for system development and the time required for configuration and testing of wireless base stations.","","978-1-4244-5871-4978-1-4244-5870-7978-1-4244-5871","10.1109/ALLERTON.2009.5394528","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5394528","","Decoding;Space time codes;Graphics;Computer architecture;Base stations;Concurrent computing;Block codes;Code standards;Software standards;WiMAX","4G mobile communication;codecs;coprocessors;maximum likelihood decoding;space-time codes","parallel decoding;space-time codes;graphics processing units;ML decoding;4th generation wireless standard;wireless base stations","","5","33","","","","","","IEEE","IEEE Conferences"
"Integration of Ground-Penetrating Radar and Laser Position Sensors for Real-Time 3-D Data Fusion","M. Grasmueck; D. A. Viggiano","NA; NA","IEEE Transactions on Geoscience and Remote Sensing","","2007","45","1","130","137","Full-resolution 3-D ground-penetrating radar (GPR) imaging of the near surface should be simple and efficient. Geoscientists, archeologists, and engineers need a tool capable of generating interpretable subsurface views at centimeter-to-meter resolution of field sites ranging from smooth parking lots to rugged terrain. The authors have integrated novel rotary laser positioning technology with GPR into such a 3-D imaging system. The laser positioning enables acquisition of centimeter accurate x, y, and z coordinates from multiple small detectors attached to moving GPR antennas. Positions streaming with 20 updates/s from each detector are fused in real time with the GPR data. The authors developed software for automated data acquisition and real time 3-D GPR data quality control on slices at selected depths. Industry standard (SEGY) format data cubes and animations are generated within an hour after the last trace has been acquired. Such instant 3-D GPR can be used as an on-site imaging tool supporting field work, hypothesis testing, and optimized excavation and sample collection in the exploration of the static and dynamic nature of the shallow subsurface","0196-2892;1558-0644","","10.1109/TGRS.2006.882253","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4039620","Data acquisition;data processing;ground-penetrating radar (GPR);imaging;lasers;position measurement;terrain mapping","Ground penetrating radar;Laser fusion;Laser radar;Sensor fusion;Fusion power generation;Streaming media;Detectors;Software quality;Data acquisition;Quality control","ground penetrating radar;radar imaging;remote sensing by laser beam;remote sensing by radar;sensor fusion","ground-penetrating radar;laser position sensors;real-time 3D data fusion;interpretable subsurface view;rugged terrain;rotary laser positioning technology;GPR antennas;positions streaming;automated data acquisition;SEGY format data cubes;SEGY animation;imaging tool supporting field work;shallow subsurface","","49","14","","","","","","IEEE","IEEE Journals & Magazines"
"Autonomous Landing and Hazard Avoidance Technology (ALHAT)","C. D. Epp; E. A. Robertson; T. Brady","NASA, Johnson Space Center / EG1, 2101 NASA Parkway, Houston, TX 77058. 281-244-7733, chirold.d.epp@nasa.gov; NASA, Johnson Space Center / EG511, 2101 NASA Parkway, Houston, TX 77058. 281-483-6615, edward.a.robertson@nasa.gov; Charles Stark Draper Laboratory, Inc., 555 Technology Square, MS27, Cambridge, MA 02139. 617-258-2366, tye@draper.com","2008 IEEE Aerospace Conference","","2008","","","1","7","The ALHAT project is funded by NASA to develop an integrated AGNC (autonomous guidance, navigation and control) hardware and software system capable of detecting and avoiding surface hazards and guiding humans and cargo safely, precisely and repeatedly to designated lunar landing sites. There are important interdependencies driving the design of a lunar landing system including such things as lander hazard robustness, landing site conditions (terrain and natural lighting), trajectories, sensors, crew involvement, and others. The ALHAT system must be capable of operating in a wide range of lunar environments and supporting global lunar access for both crewed and robotic missions. This paper discusses the major factors driving the design of a lunar landing system as well as the current state of the technology development. The supporting analysis and testing results will be presented that show the system interdependencies and their relative importance, as well as the trades needed to optimize the landing system. The emphasis is on the final phase of the landing where hazard detection and avoidance (HDA) and hazard relative navigation (HRN) are the primary considerations in achieving a safe landing. The current sensor options being considered and the status of the development of those sensors are discussed.","1095-323X","978-1-4244-1487-1978-1-4244-1488","10.1109/AERO.2008.4526297","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4526297","","Hazards;Moon;Navigation;Space technology;NASA;Control systems;Hardware;Software systems;Humans;Robustness","aircraft landing guidance;lunar surface;planetary rovers;space vehicles","autonomous landing and hazard avoidance technology;surface hazards;lunar landing sites;hazard detection and avoidance;hazard relative navigation;ALHAT","","18","8","","","","","","IEEE","IEEE Conferences"
"TCP performance in hybrid multigranular OBS networks","M. Casoni; C. Raffaelli","Dept. of Information Engineering - University of Modena and Reggio Emilia, Via Vignolese 905, Italy; D.E.I.S. - University of Bologna - Viale Risorgimento 2, Italy","2009 Sixth International Conference on Broadband Communications, Networks, and Systems","","2009","","","1","6","This paper studies TCP performance with different offset times which arise when hybrid multi-granular technology is employed in OBS core nodes. Multi-granular switches are promising devices to match dynamic application needs while optimizing switch costs and feasibility. They foster optical burst switching in multi-service contexts where switching matrices are designed and optimized in application awareness. OBS base offset time is strictly dependent on switching matrix set up time which, in presence of hybrid technology, is different from path to path. These differences impact on end-to-end performance and in particular on TCP throughput. This work studies end-to-end performance for two basic classes of connections, namely slow and fast connections. Simulations results based on a careful set up of ns-2 simulations according to test bed configurations described in literature are provided to show the effects of the employment of different optical technologies on the TCP throughput.","2150-4903","978-963-9799-49","10.4108/ICST.BROADNETS2009.8020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5336338","","Computer architecture;Software engineering;Computer networks;Internet;Protocols;TCPIP;Large-scale systems;Algorithm design and analysis;Object oriented modeling;Educational institutions","optical burst switching;transport protocols","TCP performance;hybrid multigranular OBS networks;optical burst switching;multi-service contexts;switching matrices","","1","19","","","","","","IEEE","IEEE Conferences"
"Quality-Driven TCP Friendly Rate Control for Real-Time Video Streaming","H. Luo; D. Wu; S. Ci; A. Argyriou; H. Wang","NA; NA; NA; NA; NA","IEEE GLOBECOM 2008 - 2008 IEEE Global Telecommunications Conference","","2008","","","1","5","TCP Friendly Rate Control (TFRC) has been designed to provide smoother sending rate than TCP for multimedia applications. However, most existing work on TFRC is restricted within exploring the performance of TFRC itself in wired or wireless networks without considering the interaction between TFRC and other network layers. This paper proposes a quality- driven TFRC framework for real-time video streaming, where real-time video coding at the application layer and the packet sending rate at the transport layer are jointly optimized. The proposed framework is formulated to find the optimal video coding parameters and the sending rate to minimize the end- to-end expected video distortion under a given video playback delay constraint. The proposed framework has been implemented and tested by using both H.264/AVC codec and NS-2 simulator. Experimental results demonstrate that the proposed joint optimization framework can significantly improve the received video quality over the existing schemes, especially when delay bound is tight.","1930-529X","978-1-4244-2324","10.1109/GLOCOM.2008.ECP.969","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4698744","","Streaming media;Delay;Video compression;Video coding;Throughput;Protocols;Communication system traffic control;Encoding;Design engineering;Application software","transport protocols;video coding;video streaming","TCP friendly rate control;real-time video streaming;wired networks;wireless networks;real-time video coding;application layer;end- to-end expected video distortion;H.264/AVC codec;NS-2 simulator","","7","16","","","","","","IEEE","IEEE Conferences"
"Adaptive REACT protocol for Emergency Applications in Vehicular Networks","E. V. d. Velde; C. Blondia","University of Antwerp, Belgium; University of Antwerp, Belgium","32nd IEEE Conference on Local Computer Networks (LCN 2007)","","2007","","","613","619","Vehicular communication is gaining more and more interest from big consortia and companies in the car industry. Wireless communication in a vehicular environment creates unique opportunities but poses also its own challenges. Therefore we proposed in previous work a new routing protocol, REACT, which uses geographical information and is able to react to a fast changing environment. In this paper, we will look at a few optimizations for this protocol in order to make it more adaptive to the current network conditions. As vehicles move very fast, link breaks occur more often. We attempt to predict this behavior, trying to use a link as long as possible but not longer than it really exists. Another important part of the REACT protocol consists of beaconing as means of exchanging position information with neighbors. The interval between two consecutive beacons is an important parameter in determining the end-to-end delay and we propose an algorithm that adapts this parameter in order to speed up neighbor discovery without generating too much beaconing overhead. The two optimizations were implemented and tested in a simulator and the results show a performance amelioration in the end-to-end delay in scenarios with sparse and dense road traffic.","0742-1303","0-7695-3000-1978-0-7695-3000","10.1109/LCN.2007.29","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4367893","","Vehicles;Telecommunication traffic;Ad hoc networks;Routing protocols;Mobile ad hoc networks;Computer networks;Application software;Wireless communication;Delay;Roads","mobile radio;road vehicles;routing protocols","adaptive REACT protocol;vehicular communication;routing protocol;wireless communication;end-to-end delay","","8","13","","","","","","IEEE","IEEE Conferences"
"Novel One-Dimensional Scattering Bar Rule via Computer Aided Design","C. Y. Lin; B. Liou; F. Y. Chiou; C. H. Ku","ProMos Technologies Inc. Hsinchu, Taiwan 30078 chun-yue_lin@promos.com.tw; ProMos Technologies Inc. Hsinchu, Taiwan 30078 ben_liou@promos.com.tw; ProMos Technologies Inc. Hsinchu, Taiwan 30078 karl_chiou@promos.com.tw; ProMos Technologies Inc. Hsinchu, Taiwan 30078 cheng-hung_ku@promos.com.tw","2008 IEEE/SEMI Advanced Semiconductor Manufacturing Conference","","2008","","","173","178","Although one-dimensional scattering bars (SB) are designed to consolidate photolithography patterns to extend process tolerance, but the inability of covering diverse layout environments by employing simple SB rules collected from empirical measurements makes it impossible to utilize SB as a reliable resolution enhancement technology (RET). Therefore, this study presents a novel one-dimensional SB scheme called computational scattering bar rule (CSBR) that assures line-space layouts get suitable SB. An empirical data can be measured and a physical model can then be built to fit the data. Next, basing on such model, wafer images are simulated over numerous critical dimensions (CD) of main feature (MF) with different SB widths and SB spaces to locate optimized SB rules. Two approaches of optimization strategies are proposed to meet different requirements, and are verified by commercial software. This novel scheme significantly contributes to utility of SB.","1078-8743;2376-6697","978-1-4244-1964-7978-1-4244-1965","10.1109/ASMC.2008.4529024","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4529024","Scattering Bar;sub-resolution assist feature;resolution enhancement technology;optical proximity correction;computer aided design","Optical scattering;Space technology;Focusing;Lithography;Semiconductor device modeling;Optical computing;Optical sensors;Lighting;Testing;Bars","electronic design automation;photolithography","1D scattering bar rule;computer aided design;photolithography;resolution enhancement technology;computational scattering bar rule;critical dimensions","","1","7","","","","","","IEEE","IEEE Conferences"
"Design of Application Specific Instruction-Set Processor for image and video filtering","S. Saponara; L. Fanucci; S. Marsi; G. Ramponi; M. Witte; D. Kammler","DIIEIT, University of Pisa, Italy; DIIEIT, University of Pisa, Italy; DEEI, University of Trieste, Italy; DEEI, University of Trieste, Italy; ISS, RWTH, University of Aachen, Germany; ISS, RWTH, University of Aachen, Germany","2006 14th European Signal Processing Conference","","2006","","","1","5","Two architectures for cost-effective and real-time implementation of non-linear image and video filters are presented in the paper. The first architecture is a traditional VHDL-based ASIC (Application Specific Integrated Circuit) design while the second one is an ADL (Architecture Description Language) based ASIP (Application Specific Instruction Set Processor). A system to improve the visual quality of images, based on Retinex-like algorithm, is referred as case study. First, starting from a high-level functional description the design space is explored to achieve a linearized structural C model of the algorithm with finite arithmetic precision. For the algorithm design space exploration visual and complexity criteria are adopted while a statistical analysis of typical input images drives the algorithm optimization process. The algorithm is implemented both as ASIC and ASIP solution in order to explore the trade-off between the flexibility of a software solution and the power and complexity optimization of a dedicated hardware design. The aim is to achieve the desired algorithmic functionality and timing specification at reasonable complexity and power costs. Taking advantage of the processor programmability, the flexibility of the system is increased, involving e.g. dynamic parameter adjustment and color treatment. Gate level implementation results in a 0.18μm standard-cell CMOS technology are presented for both the ASIC and ASIP approach<sup>1</sup>.","2219-5491","","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7071583","","Random access memory;Read only memory;Application specific integrated circuits;Abstracts;Acceleration;CMOS integrated circuits;Benchmark testing","application specific integrated circuits;CMOS integrated circuits;image filtering;instruction sets","application specific instruction-set processor;image filtering;video filtering;VHDL;ASIC;application specific integrated circuit design;ADL;architecture description language;ASIP;high-level functional description;finite arithmetic precision;processor programmability;gate level implementation;CMOS technology;size 0.18 mum","","","10","","","","","","IEEE","IEEE Conferences"
"Configuration and Programming of Heterogeneous Multiprocessors on a Multi-FPGA System Using TMD-MPI","M. Saldana; D. Nunes; E. Ramalho; P. Chow","Department of Electrical and Computer Engineering, University of Toronto, Toronto, ON, Canada M5S 3G4. email: msaldana@eecg.toronto.edu; Department of Electrical and Computer Engineering, University of Toronto, Toronto, ON, Canada M5S 3G4. email: dnunes@eecg.toronto.edu; Department of Electrical and Computer Engineering, University of Toronto, Toronto, ON, Canada M5S 3G4. email: eramalho@eecg.toronto.edu; Department of Electrical and Computer Engineering, University of Toronto, Toronto, ON, Canada M5S 3G4. email: pc@eecg.toronto.edu","2006 IEEE International Conference on Reconfigurable Computing and FPGA's (ReConFig 2006)","","2006","","","1","10","Recent research has shown that FPGAs have true potential to speedup demanding applications even further than what state-of-the art superscalar processors can do. The penalty is the loss of generality in the architecture, but reconfigurability of FPGAs allows them to be reprogrammed for other applications. Therefore, an efficient programming model and a flexible design flow are paramount for this technology to be more widely accepted. Furthermore, in the history of computers, standards have been a positive experience because they provide a common ground for research and development. A programming model for multiprocessor Systems-On-FPGAs should be standard and application independent, but optimized for a particular architecture. In this paper, we use TMD-MPI, a subset implementation of the message passing standard MPI, and a flexible system-level design flow to implement heterogeneous multiprocessor systems-on-chip on FPGAs. Hardware engines are also by using a message passing engine, which encapsulates the TMD-MPI functionality in hardware, to enable the communication between hardware engines and embedded processors. We test the functionality and scalability of the system by implementing a 45-processor system across five FPGAs. As a test example, we solve the heat equation by using the Jacobi iterations method. Some performance metrics are measured to demonstrate the impact of different computing cores on the overall computation","2325-6532","1-4244-0689-71-4244-0690","10.1109/RECONF.2006.307779","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4099999","","Field programmable gate arrays;Hardware;Engines;Computer architecture;Application software;Multiprocessing systems;Message passing;Art;History;Standards development","embedded systems;field programmable gate arrays;iterative methods;message passing;reconfigurable architectures;system-on-chip","heterogeneous multiprocessor systems-on-chip;multiFPGA system;TMD-MPI;superscalar processors;reconfiguration architecture;multiprocessor systems-on-FPGA;system-level design flow;message passing engine;embedded processors;heat equation;Jacobi iterations method","","11","26","","","","","","IEEE","IEEE Conferences"
"Parallel Algorithms for Evaluating Centrality Indices in Real-world Networks","D. A. Bader; K. Madduri","Georgia Institute of Technology, USA; Georgia Institute of Technology, USA","2006 International Conference on Parallel Processing (ICPP'06)","","2006","","","539","550","This paper discusses fast parallel algorithms for evaluating several centrality indices frequently used in complex network analysis. These algorithms have been optimized to exploit properties typically observed in real-world large scale networks, such as the low average distance, high local density, and heavy-tailed power law degree distributions. We test our implementations on real datasets such as the Web graph, protein-interaction networks, movie-actor and citation networks, and report impressive parallel performance for evaluation of the computationally intensive centrality metrics (betweenness and closeness centrality) on high-end shared memory symmetric multiprocessor and multithreaded architectures. To our knowledge, these are the first parallel implementations of these widely-used social network analysis metrics. We demonstrate that it is possible to rigorously analyze networks three orders of magnitude larger than instances that can be handled by existing network analysis (SNA) software packages. For instance, we compute the exact betweenness centrality value for each vertex in a large US patent citation network (3 million patents, 16 million citations) in 42 minutes on 16 processors, utilizing 20GB RAM of the IBM p5 570. SNA packages on the other hand cannot handle graphs with more than hundred thousand edges","0190-3918;2332-5690","0-7695-2636","10.1109/ICPP.2006.57","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1690659","","Parallel algorithms;Computer networks;Complex networks;Algorithm design and analysis;Large-scale systems;Testing;Proteins;Concurrent computing;High performance computing;Computer architecture","multi-threading;parallel algorithms;shared memory systems","parallel algorithms;centrality index evaluation;real-world networks;complex network analysis;high-end shared memory symmetric multiprocessor;multithreaded architectures;social network analysis metrics","","65","52","","","","","","IEEE","IEEE Conferences"
"A Visualization Framework for Web Service Discovery and Selection Based on Quality of Service","F. F. Chua; H. Yuan; S. D. Kim","NA; NA; NA","The 2nd IEEE Asia-Pacific Service Computing Conference (APSCC 2007)","","2007","","","312","319","The visualization of Web service execution process is an emerging research in service-oriented computing (SOC) area. This paper presents a practical visualization framework in putting service discovery and selection process based on quality of service (QoS) attributes into a visual context. The proposed practical architecture serves as a foundation for designing the novel GUIs for different users. By considering the preferences and priorities for service consumers and service providers, we adopt different application UI design criteria and design patterns which tailored to service-based visualization design. Based on that, evaluation is carried out to test the usability, effectiveness and acceptability of the proposed visualization framework.","","0-7695-3051","10.1109/APSCC.2007.52","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4414476","","Web services;Quality of service;Data visualization;Service oriented architecture;Application software;Graphical user interfaces;Context-aware services;Computer architecture;Displays;Navigation","data visualisation;quality of service;Web services","visualization framework;Web service discovery;quality of service;Web service execution process;service-oriented computing;QoS","","3","8","","","","","","IEEE","IEEE Conferences"
"Evaluating ALPHAN: A Communication Protocol for Haptic Interaction","H. A. Osman; M. Eid; A. E. Saddik","Multimedia Communications Research Laboratory - MCRLab, School of Information Technology and Engineering - University of Ottawa, Ottawa, Ontario, K1N 6N5, Canada, HALOsman@mcrlab.uottawa.ca; Multimedia Communications Research Laboratory - MCRLab, School of Information Technology and Engineering - University of Ottawa, Ottawa, Ontario, K1N 6N5, Canada, eid@mcrlab.uottawa.ca; Multimedia Communications Research Laboratory - MCRLab, School of Information Technology and Engineering - University of Ottawa, Ottawa, Ontario, K1N 6N5, Canada, abed@mcrlab.uottawa.ca","2008 Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems","","2008","","","361","366","In our previous work we introduced a novel application layer protocol, named ALPHAN, for haptic data communication. The protocol is characterized by three distinguished features: first, it is designed at the application layer to enable the application to define and control the networking parameters. Second, it is made highly customizable using XML-based descriptions. Finally, the protocol supports multi-buffering mechanisms to prioritize the communicated information. In this paper, we present a thorough evaluation of the protocol using a collaborative haptic game; the balance ball game that we developed is for this purpose. The performance metrics and the test bed of the protocol evaluation are also discussed. Finally, we comment on our findings and provide directions for prospective research.","2324-7347;2324-7355","978-1-4244-2005","10.1109/HAPTICS.2008.4479972","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4479972","Tele-Haptics;networking protocol;XML descriptions;collaborative haptics;H.5.2 [Information Interfaces and Presentation]: User Interfaces ¿ Haptic I/O","Haptic interfaces;Collaboration;Transport protocols;Jitter;Application software;Data communication;Communication system control;Delay;Quality of service;Multimedia communication","computer games;data communication;groupware;haptic interfaces;transport protocols;XML","communication protocol;haptic interaction;application layer protocol;haptic data communication;XML-based descriptions;multibuffering mechanisms;collaborative haptic game","","12","19","","","","","","IEEE","IEEE Conferences"
"Host based intrusion detection using RBF neural networks","U. Ahmed; A. Masood","Computer Science Department, Military College of Signals, National University of Sciences and Technology, Rawalpindi, Pakistan; Computer Science Department, Military College of Signals, National University of Sciences and Technology, Rawalpindi, Pakistan","2009 International Conference on Emerging Technologies","","2009","","","48","51","A novel approach of host based intrusion detection is suggested in this paper that uses Radial basis Functions Neural Networks as profile containers. The system works by using system calls made by privileged UNIX processes and trains the neural network on its basis. An algorithm is proposed that prioritize the speed and efficiency of the training phase and also limits the false alarm rate. In the detection phase the algorithm provides implementation of window size to detect intrusions that are temporally located. Also a threshold is implemented that is altered on basis of the process behavior. The system is tested with attacks that target different intrusion scenarios. The result shows that the radial Basis Functions Neural Networks provide better detection rate and very low training time as compared to other soft computing methods. The robustness of the training phase is evident by low false alarm rate and high detection capability depicted by the application.","","978-1-4244-5630-7978-1-4244-5631","10.1109/ICET.2009.5353204","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5353204","intrusion detection;Host Based Intrusion Detection;neural networks;RBF neural networks","Intrusion detection;Neural networks;Radial basis function networks;Phase detection;Military computing;Application software;Monitoring;Computer science;Educational institutions;Computer networks","algorithm theory;radial basis function networks;security of data;stability;Unix","host based intrusion detection;RBF neural networks;radial basis functions;UNIX processes;speed efficiency algorithm;soft computing methods;robustness","","11","21","","","","","","IEEE","IEEE Conferences"
"Dynamic Voltage Collapse Prediction on a Practical Power System Using Power Transfer Stability Index","M. Nizam; A. Mohamed; A. Hussain","Ph. D Student at The Department of Electrical, Electronic and Systems Engineering, Universiti Kebangsaan Malaysia, Bangi, 43600 Selangor, Malaysia. nizam_kh@ieee.org; Professor at The Department of Electrical, Electronic and Systems Engineering, Universiti Kebangsaan Malaysia, Bangi. azah@eng.ukm.my; Professor at The Department of Electrical, Electronic and Systems Engineering, Universiti Kebangsaan Malaysia, Bangi. aini@eng.ukm.my","2007 5th Student Conference on Research and Development","","2007","","","1","6","This paper presents a combined static and dynamic voltage stability analysis method to predict dynamic voltage collapse in a practical power system using the power transfer stability index (PTSI). In the static voltage stability analysis, the contingencies due to load increase at all the load buses are ranked so as to determine the critical contingencies based on the maximum permissible load. Then, dynamic simulations are carried out only for the critical contingency cases. The performance of the PTSI in predicting voltage collapse is evaluated by comparing it with the voltage collapse proximity indicator. To illustrate the effectiveness of the proposed PTSI, simulations were carried out on a practical power system using the PSS/E software. The effects of induction motors and line outage contingency on dynamic voltage collapse were also investigated. Test results prove the accuracy of the PTSI in predicting voltage collapse in a practical power system.","","978-1-4244-1469-7978-1-4244-1470","10.1109/SCORED.2007.4451398","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4451398","Dynamic;stability;voltage collapse","Power system dynamics;Power system stability;Voltage;Power system simulation;Power system analysis computing;Stability analysis;Impedance;Power generation;Power system modeling;Induction motors","power system dynamic stability;power system simulation","dynamic voltage collapse prediction;power transfer stability index;static voltage stability analysis;load buses;maximum permissible load;voltage collapse proximity indicator;power system;power system simulation;PSS/E software;induction motors effects","","2","9","","","","","","IEEE","IEEE Conferences"
"Covariance based linear precoding in the case of identical longterm channel state information","Haixia Zhang; M. T. Ivrlac; J. A. Nossek; Dongfeng Yuan","Institute for Circuit Theory and Signal Processing, Technische Universität München, Germany; Institute for Circuit Theory and Signal Processing, Technische Universität München, Germany; Institute for Circuit Theory and Signal Processing, Technische Universität München, Germany; School of Information Science and Engineering, Shandong University, China","2008 International ITG Workshop on Smart Antennas","","2008","","","291","295","We extend the covariance based linear precoding theory to the spatial division multiple access (SDMA) downlink processing in the case that all the streams belonging to the same user have identical channel covariance matrices, which happens when the antennas of the user are located in close proximity. In this case, the precoders designed for those streams based on covariance channel state information (CSI) will be identical, therefore, they can not separate the user successfully. To overcome this, we modify the covariance matrices by keeping the eigenvectors unchanged and choosing one eigenvalue per stream if the number of streams equals to the rank of the covariance matrix. Otherwise, the eigenvalues will be divided into different groups with the rule making the sums of each group as similar as possible. Then, the precoders are designed based on the modified covariance matrices with the constraint of total transmit power. The proposed method is tested and simulation results show that it works well and can improve the bit error ratio (BER) of the system significantly.","","978-1-4244-1756-8978-1-4244-1757","10.1109/WSA.2008.4475572","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4475572","","Channel state information;Covariance matrix;MIMO;Transmitters;Eigenvalues and eigenfunctions;Downlink;Receiving antennas;Bit error rate;Computer aided software engineering;Circuit theory","channel coding;covariance matrices;eigenvalues and eigenfunctions;error statistics;MIMO communication;precoding;space division multiple access","linear precoding;covariance;channel state information;spatial division multiple access downlink;eigenvalues;bit error ratio","","1","10","","","","","","IEEE","IEEE Conferences"
"Optimal location of pilot buses by a Genetic Algorithm approach for a coordinated voltage control in distribution systems","O. Richardot; Y. Besanger; D. Radu; N. Hadjsaid","Grenoble Electrical Engineering Laboratory (G2Elab), 38402 Saint Martin d'hères Cedex, France; Grenoble Electrical Engineering Laboratory (G2Elab), 38402 Saint Martin d'hères Cedex, France; Grenoble Electrical Engineering Laboratory (G2Elab), 38402 Saint Martin d'hères Cedex, France; Grenoble Electrical Engineering Laboratory (G2Elab), 38402 Saint Martin d'hères Cedex, France, and with GIE-IDEA, 38402 Saint Martin d'hères Cedex, France","2009 IEEE Bucharest PowerTech","","2009","","","1","7","Large scale integration of distributed generation in distribution networks causes voltage quality problems, but also may be able to provide means to solve theses problems if correctly controlled. A possible way to take advantage of distributed generation is to use it to perform a coordinated voltage control that would maintain voltage to its set-point value at the substation and some other specific nodes called pilot buses. This paper proposes a method for optimal location of these pilot buses in order to ensure the coordinated voltage control performances. For this purpose, we developed a hybrid software based on GAs (genetic algorithms). The proposed procedures are successfully tested on a typical distribution network with distributed generation units for several numbers of pilot buses.","","978-1-4244-2234-0978-1-4244-2235","10.1109/PTC.2009.5281978","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5281978","ancillary services;voltage control;distribution system;genetic algorithm","Genetic algorithms;Voltage control;Power system reliability;Static VAr compensators;Distributed control;Reactive power;Power systems;Optimization methods;Automatic generation control;Automatic voltage control","distributed power generation;genetic algorithms;power distribution control;substations;voltage control","optimal pilot bus location;genetic algorithm approach;coordinated voltage control;distributed generation;large-scale integration;distribution networks;voltage quality problems;substation","","4","9","","","","","","IEEE","IEEE Conferences"
"Optimal estimation of human body segments dynamics using realtime visual feedback","K. Ayusawa; Y. Nakamura; G. Venture","Department of Mechano-Informatics, The University of Tokyo, Japan; Department of Mechano-Informatics, The University of Tokyo, Japan; Department of Mechanical System Engineering, The Tokyo University of Agriculture and Technology, Japan","2009 IEEE/RSJ International Conference on Intelligent Robots and Systems","","2009","","","1627","1632","Mass parameters of the human body segments are mandatory when studying motion dynamics. In orthopedics, biomechanics and rehabilitation they are of crucial importance. Inaccuracies their value generate errors in the motion analysis, misleading the interpretation of results. No systematic method to estimate them has been proposed so far. Rather, parameters are scaled from generic tables or estimated with methods inappropriate for in-patient care. Based on our previous works, we propose a real-time software and its interface that allow to estimate the whole-body segment parameters, and to visualize the progresses of the completion of the identification. The visualization is used as a visual feedback to optimize the excitation and thus the identification results. The method is experimentally tested and obtained results are discussed.","2153-0858;2153-0866","978-1-4244-3803-7978-1-4244-3804","10.1109/IROS.2009.5354711","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5354711","","Humans;Feedback;Interpolation;Biological system modeling;Motion estimation;Orthopedic surgery;Biomechanics;Visualization;Condition monitoring;Robustness","biology computing;biomechanics;data visualisation;graphical user interfaces;real-time systems","human body segments dynamics estimation;realtime visual feedback;motion dynamics;real-time software;visualization;identification;graphic interface","","6","27","","","","","","IEEE","IEEE Conferences"
"SensDep: a design tool for the deployment of heterogeneous sensing devices","R. Ramadan; K. Abdelghany; H. El-Rewini","Dept. of Comput. Sci. & Eng., Southern Methodist Univ., Dallas, TX, USA; NA; NA","Second IEEE Workshop on Dependability and Security in Sensor Networks and Systems","","2006","","","10 pp.","53","In this paper, we introduce SensDep as a software design tool that incorporates several solution strategies to optimize sensor networks cost and coverage. The tool helps the designer answer many ""what-if"" questions that usually arise in the design of surveillance operations networks. Also, it considers several operation capabilities for the sensing devices including reliability, mobility, transfer cost, sensors' cost, lifespan and power self scheduling in addition to the environment parameters during the deployment process. Moreover, it provides the designer the optimal deployment scheme for small size designs using mathematical programming. It also provides near optimal schemes for large scale designs using a set of heuristic solutions. A set of experiments is conducted to test the tool capabilities for different design settings. Several design scenarios are presented to illustrate how the tool can be utilized","","0-7695-2529","10.1109/DSSNS.2006.15","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1630361","","Surveillance;Costs;Monitoring;Sensor phenomena and characterization;Large-scale systems;Wireless sensor networks;Computer science;Design engineering;Art;Civil engineering","mathematical programming;telecommunication network planning;wireless sensor networks","SensDep;heterogeneous sensing devices;software design tool;sensor networks;surveillance operations networks;optimal deployment scheme;mathematical programming;heuristic solutions","","6","22","","","","","","IEEE","IEEE Conferences"
"Mechanical Characterization of Microelectronics Embedded in Flexible Substrate","L. Wang; T. Zoumpoulidis; M. Bartek; A. Polyakov; K. M. B. Jansen; L. J. Ernst","Department of Precision and Micro-system Engineering, Delft University of Technology, Mekelweg 2, 2628 CD Delft, the Netherlands. Email: L.wang@tudelft.nl; Phone: +31-15-2786739; Delft Institute of Microelectronics and Submicron Technology/HiTeC, Delft University of Technology, Mekelweg 4, 2628, CD Delft, the Netherlands.; Delft Institute of Microelectronics and Submicron Technology/HiTeC, Delft University of Technology, Mekelweg 4, 2628, CD Delft, the Netherlands.; Delft Institute of Microelectronics and Submicron Technology/HiTeC, Delft University of Technology, Mekelweg 4, 2628, CD Delft, the Netherlands.; Department of Precision and Micro-system Engineering, Delft University of Technology, Mekelweg 2, 2628 CD Delft, the Netherlands.; Department of Precision and Micro-system Engineering, Delft University of Technology, Mekelweg 2, 2628 CD Delft, the Netherlands.","2006 7th International Conference on Electronic Packaging Technology","","2006","","","1","7","The acceptable flexibility for ultra-thin substrate would be reached by embedding the ultra-thin substrate into the flexible polyimide and patterning the poly-silicon or silicon into square/hexagon segmentations. The segments interconnect by metal wires for the signal communication, different wire shapes are designed to reach more flexible. In this contribution, results of FE optimization on mechanical reliability issues of interconnect in-between segments on ultra-thin polyimide/silica rubber substrates are presented. Generation of cracks within the silicon and dielectric layers is then studied under controlled bending (glass cylinders with diameters of 2-10 mm, compressive and tensile stress) using specially for this purpose designed bending tools. Specimen observation is done using an optical microscope with possibility of digital recording and evaluation by pattern recognition software. The results show that the cracks appear first in the dielectric layers in-between the silicon layer segments and only at higher loads propagate or are generated within the silicon itself. The development of first cracks depends significantly on the silicon layer segmentation size which affects both the crack density and the crack width. The crack density increases sharply with the strain for early stage and then increases slightly. The crack width increases steadily. The high flexibility result can be reached that no crack be detected under the bending tests with 2 mm diameters. Multilevel FEM simulations are performed in order to increase understanding of the major failure processes. Results of simulations and experiments compare quite well. Second generation samples are designed with consideration of interconnect in-between segments and avoid of silicon dioxide in-between segments","","1-4244-0619-61-4244-0620","10.1109/ICEPT.2006.359825","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4198946","","Microelectronics;Polyimides;Silicon compounds;Dielectric substrates;Optical microscopy;Wires;Shape;Signal design;Iron;Rubber","finite element analysis;flexible electronics;integrated circuit interconnections;integrated circuit reliability","mechanical characterization;embedded microelectronics;flexible substrate;ultra thin substrate;mechanical reliability;controlled bending;compressive stress;tensile stress;optical microscope;pattern recognition software;FEM simulations","","","8","","","","","","IEEE","IEEE Conferences"
"Real-time phase noise meter based on a digital signal processor","L. Angrisani; M. D'Arco; C. Greenhall; R. S. L. Moriello","Dip. di Informatica e Sistemistica, Universitá di Napoli Federico II, via Claudio 21, 80125 Napoli. ITALY. Tel: +39 081 7683170, e-mail: angrisan@unina.it; Dip. di Ingegneria Elettrica, Universitá di Napoli Federico II, via Claudio 21, 80125 Napoli. ITALY. Tel: +39 081 7683238, Fax: +39 081 2396897, e-mail: darco@unina.it; Jet Propulsion Laboratory (CIT), 4800 Oak Grove Dr., MS 298-100. Pasadena, CA 91109, USA. Tel: +1 818 393-6944 Fax: +1 818 393-6773, e-mail: cgreenhall@jpl.nasa.gov; Dip. di Ingegneria Elettrica, Universitá di Napoli Federico II, via Claudio 21, 80125 Napoli. ITALY. Tel: +39 081 7683238, Fax: +39 081 2396897, e-mail: rschiano@unina.it","2006 IEEE Instrumentation and Measurement Technology Conference Proceedings","","2006","","","1850","1855","A digital signal-processing meter for phase noise measurement on sinusoidal signals is dealt with. It enlists special hardware architecture, made up of a core digital signal processor connected to a data acquisition board, and takes advantage of a quadrature demodulation-based measurement scheme, already proposed by the authors. Thanks to an efficient measurement process and an optimised implementation of its fundamental stages, the proposed meter succeeds in exploiting all hardware resources in such an effective way as to gain high performance and real-time operation. For input frequencies up to some hundreds of kilohertz the meter is capable both of updating phase noise power spectrum while seamlessly capturing the analyzed signal into its memory, and granting as good frequency resolution as few units of hertz. After a brief outline of the aforementioned measurement scheme, key features of the adopted digital signal processor are highlighted. Hardware architecture, measurement process, and software strategies peculiar to the proposed meter are then described in detail. At the end, some results of experimental tests, carried out on sinusoidal signals provided by function generators and arbitrary waveform generators, give evidence of the meters reliability and efficacy","1091-5281","0-7803-9360-00-7803-9359","10.1109/IMTC.2006.328279","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4124672","Phase noise measurement;Digital signal processor;Data acquisition board;Power spectrum evaluation;Quadrature demodulation","Phase noise;Digital signal processors;Hardware;Frequency;Signal generators;Noise measurement;Phase measurement;Data acquisition;Gain measurement;Performance gain","computerised instrumentation;data acquisition;digital signal processing chips;electric noise measurement;phase noise;real-time systems;signal processing equipment","digital signal processor;phase noise measurement;sinusoidal signals;special hardware architecture;data acquisition;quadrature demodulation;measurement scheme;measurement process;software strategy;function generators;arbitrary waveform generators;power spectrum evaluation","","4","15","","","","","","IEEE","IEEE Conferences"
"Real-Time Computerized Annotation of Pictures","J. Li; J. Z. Wang","NA; NA","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2008","30","6","985","1002","Developing effective methods for automated annotation of digital pictures continues to challenge computer scientists. The capability of annotating pictures by computers can lead to breakthroughs in a wide range of applications, including Web image search, online picture-sharing communities, and scientific experiments. In this work, the authors developed new optimization and estimation techniques to address two fundamental problems in machine learning. These new techniques serve as the basis for the automatic linguistic indexing of pictures - real time (ALIPR) system of fully automatic and high-speed annotation for online pictures. In particular, the D2-clustering method, in the same spirit as K-Means for vectors, is developed to group objects represented by bags of weighted vectors. Moreover, a generalized mixture modeling technique (kernel smoothing as a special case) for nonvector data is developed using the novel concept of hypothetical local mapping (HLM). ALIPR has been tested by thousands of pictures from an Internet photo-sharing site, unrelated to the source of those pictures used in the training process. Its performance has also been studied at an online demonstration site, where arbitrary users provide pictures of their choices and indicate the correctness of each annotation word. The experimental results show that a single computer processor can suggest annotation terms in real time and with good accuracy.","0162-8828;2160-9292;1939-3539","","10.1109/TPAMI.2007.70847","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4420087","Statistical computing;Multimedia databases;Indexing methods;Algorithms;Image/video retrieval;Statistical computing;Multimedia databases;Indexing methods;Algorithms;Image/video retrieval","Internet;Application software;Tagging;Statistical learning;Search engines;Computer networks;Image retrieval;Machine learning;Indexing;Kernel","image processing;indexing;Internet;learning (artificial intelligence);multimedia systems","realtime computerized picture annotation;digital picture annotation;machine learning;automatic linguistic indexing;ALIPR system;online pictures;D2 clustering;generalized mixture modeling;kernel smoothing;hypothetical local mapping;Internet photo sharing","Algorithms;Artificial Intelligence;Computer Systems;Database Management Systems;Databases, Factual;Documentation;Image Enhancement;Image Interpretation, Computer-Assisted;Information Storage and Retrieval;Pattern Recognition, Automated","204","34","","","","","","IEEE","IEEE Journals & Magazines"
"Correlating Scheduling and Load balancing to achieve optimal performance from a cluster","M. V. Gopalachari; P. Sammulal; A. V. Babu","Asst Professor, Dept of CSE, BVRIT, Narsapur Medak, AP, INDIA mvenugopalachari@gmail.com; Asst Professor, Dept of CSE, JNTU College of Engineering, Kakinada, AP, INDIA sammulalporika@gmail.com; Professor, Director of SCDE, JNT University, Hyderabad, A. P, INDIA dravinayababu@yahoo.com","2009 IEEE International Advance Computing Conference","","2009","","","320","325","In recent years, an increasing amount of computer network research has focused on the problem of cluster system in order to achieve higher performance and lower cost. Memory management becomes a prerequisite when handling applications that require immense volume of data for e.g. satellite images used for remote sensing, defense purposes and scientific applications. The load unbalance is the major defect that reduces performance of a cluster system that uses parallel program in a form of SPMD (single program multiple data). Dynamic load unbalancing can solve the load unbalance problem of cluster system and reduce its communication cost. This paper proposes a new algorithm that correlates the scheduling of incoming jobs and balancing of the loads at each node in a multi cluster. This method assigns weights for each node to schedule an incoming job and then load will be balanced dynamically using memory locality as the main factor. The main parameters used in this algorithm are partition size, CPU usage, memory usage, page faults and execution time. The tests evaluated with various applications shown a significant optimization in the cluster performance.","","978-1-4244-2927-1978-1-4244-2928","10.1109/IADCC.2009.4809029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4809029","Cluster Computing;Workload;Load Balancing;Memory Management;Execution Time","Load management;Costs;Memory management;Application software;Clustering algorithms;Processor scheduling;Computer network management;Satellites;Remote sensing;Scheduling algorithm","parallel programming;processor scheduling;resource allocation;storage management;workstation clusters","job scheduling;load balancing;multicluster system performance;computer network;memory management;parallel program;SPMD;dynamic load unbalancing;memory locality","","4","16","","","","","","IEEE","IEEE Conferences"
"Autogeneration of Database Applications from XML Metadata for Web-Based Data Entry","K. Liao; P. Kisuule; J. Ehrlinger; J. Dai","NA; NA; NA; NA","2009 Eighth IEEE/ACIS International Conference on Computer and Information Science","","2009","","","718","723","In a biomedical research environment, there is frequently a need for dynamically generated Web interfaces for patient-related data that can easily accommodate changes in the underlying data model by altering metadata rather than changing the interface code. In this paper, we present AutoGen, a tool developed at the Heart and Vascular Institute (HVI) of Cleveland Clinic for autogenerating the database schema and data-entry-specific Web front end from XML metadata. To specify metadata, we established a tested and proven methodology for systematically capturing data and data entry requirements and encapsulating them into standard XML schema. During the development process, we created a successful approach to incorporating model-view-controller and template technologies for implementing and optimizing AutoGen. Additionally, we conducted a cost-benefit analysis of AutoGen during its first 20 months of use within HVI. Results from both the development perspective and practical usage show that AutoGen and its associated metadata specification method provide a unique approach that can efficiently and effectively address the data entry needs of similar biomedical research environments.","","978-0-7695-3641","10.1109/ICIS.2009.148","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5222909","Develop Web-based Database Application;XML Metadata;Model View Control;Template","XML;Dictionaries;Data models;Heart;Collaboration;Spatial databases;Biomedical computing;Computer interfaces;Information science;Application software","database management systems;Internet;medical administrative data processing;XML","XML metadata;Web-based data entry;patient-related data;AutoGen;model-view-controller;template technology;metadata specification method;biomedical research;Web-based database","","","12","","","","","","IEEE","IEEE Conferences"
"Eight Times Acceleration of Geospatial Data Archiving and Distribution on the Grids","F. Z. Wang; N. Helian; S. Wu; Y. Guo; D. Y. Deng; L. Meng; W. Zhang; J. Crowncroft; J. Bacon; M. A. Parker","NA; NA; NA; NA; NA; NA; NA; NA; NA; NA","IEEE Transactions on Geoscience and Remote Sensing","","2009","47","5","1444","1453","A grid-powered Web Geographical Information Science (GIS)/Web Processing Service (WPS) system has been developed for archiving and distributing large volumes of geospatial data. However, users, WPS servers, and data resources are always distributed across different locations, attempting to access and archive geospatial data from a GIS survey via conventional Hypertext Transport Protocol, Network File System Protocol, and File Transfer Protocol, which often encounters long waits and frustration in wide area network (WAN) environments. To provide a ldquolocal-likerdquo performance, a WAN/grid-optimized protocol known as ldquoGridJetrdquo developed at our lab was used as the underlying engine between WPS servers and clients, which utilizes a wide range of technologies including the one of paralleling the remote file access. No change in the way of using software is required since the multistreamed GridJet protocol remains fully compatible with the existing IP infrastructures. Our recent progress includes a real-world test that PyWPS and Google Earth over the GridJet protocol beat those over the classic ones by a factor of two to eight, where the distribution/archiving distance is over 10 000 km.","0196-2892;1558-0644","","10.1109/TGRS.2008.2010055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4806054","Bulk data transfer;data archiving;geodata set;grid computing;Web Processing Service (WPS)","Acceleration;Access protocols;Transport protocols;Wide area networks;Geographic Information Systems;File servers;Network servers;Information science;File systems;Search engines","geographic information systems;geophysical techniques;geophysics computing;grid computing;Internet;protocols;remote sensing","Geographical Information Science;GIS survey;Web Processing Service;WPS system;geospatial data;data resource;conventional Hypertext Transport Protocol;Network File System Protocol;File Transfer Protocol;wide area network;WAN environment;grid-optimized protocol;Grid jet protocol;IP infrastructure","","2","25","","","","","","IEEE","IEEE Journals & Magazines"
"An efficient stochastic approach to groupwise non-rigid image registration","K. A. Sidorov; S. Richmond; D. Marshall","School of Computer Science, Cardiff University, UK; School of Dentistry, Cardiff University, UK; School of Computer Science, Cardiff University, UK","2009 IEEE Conference on Computer Vision and Pattern Recognition","","2009","","","2208","2213","The groupwise approach to non-rigid image registration, solving the dense correspondence problem, has recently been shown to be a useful tool in many applications, including medical imaging, automatic construction of statistical models of appearance and analysis of facial dynamics. Such an approach overcomes limitations of traditional pairwise methods but at a cost of having to search for the solution (optimal registration) in a space of much higher dimensionality which grows rapidly with the number of examples (images) being registered. Techniques to overcome this dimensionality problem have not been addressed sufficiently in the groupwise registration literature. In this paper, we propose a novel, fast and reliable, fully unsupervised stochastic algorithm to search for optimal groupwise dense correspondence in large sets of unmarked images. The efficiency of our approach stems from novel dimensionality reduction techniques specific to the problem of groupwise image registration and from comparative insensitivity of the adopted optimisation scheme (simultaneous perturbation stochastic approximation (SPSA)) to the high dimensionality of the search space. Additionally, our algorithm is formulated in way readily suited to implementation on graphics processing units (GPU). In evaluation of our method we show a high robustness and success rate, fast convergence on various types of test data, including facial images featuring large degrees of both inter- and intra-person variation, and show considerable improvement in terms of accuracy of solution and speed compared to traditional methods.","1063-6919","978-1-4244-3992-8978-1-4244-3991","10.1109/CVPR.2009.5206516","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5206516","","Stochastic processes;Image registration;Computer science;Application software;Biomedical imaging;Image analysis;Robustness;Dentistry;Cost function;Graphics","approximation theory;computer graphics;coprocessors;face recognition;image registration;statistical analysis","groupwise nonrigid image registration;stochastic approach;dense correspondence problem;medical imaging;statistical models;facial dynamics;optimal registration;dimensionality reduction techniques;simultaneous perturbation stochastic approximation;graphics processing units","","8","17","","","","","","IEEE","IEEE Conferences"
"Super-Resolution Using Regularized Orthogonal Matching Pursuit Based on Compressed Sensing Theory in the Wavelet Domain","N. Fan","NA","2009 Sixth International Conference on Computer Graphics, Imaging and Visualization","","2009","","","349","354","A wavelet based compressed sensing super resolution algorithm is developed, in which the energy function optimization is approximated numerically via the regularized orthogonal matching pursuit. The proposed algorithm works well with a smaller quantity of training image patches and outputs images with satisfactory subjective quality. It is tested on classical images commonly adopted by super resolution researchers with both generic and specialized training sets for comparison with other popular commercial software and state-of-the-art methods. Experiments demonstrate that, the proposed algorithm is competitive among contemporary super resolution methods.","","978-0-7695-3789","10.1109/CGIV.2009.90","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5298815","","Matching pursuit algorithms;Compressed sensing;Wavelet domain;Image resolution;Strontium;Pixel;Wavelet transforms;Energy resolution;Signal resolution;Degradation","approximation theory;data compression;image matching;image resolution;wavelet transforms","wavelet based compressed sensing super resolution algorithm;regularized orthogonal matching pursuit;energy function optimization;training image patch;approximation theory;compressed sensing theory","","5","15","","","","","","IEEE","IEEE Conferences"
"Multi-Agent based Data Integration in Real-world","Y. Dai; S. Zhang","Fudan University, China; Fudan University, China","The Sixth IEEE International Conference on Computer and Information Technology (CIT'06)","","2006","","","19","19","This paper presents a Multi-Agent based Data Integration (MADI) framework for integrating distributed data source across Internet. This framework takes control on high-availability and high performance without compromising the data integrity and security. Special efforts on agent identity management and task scheduling and collaboration strategy were employed to guarantee above assertion. And a built-in Agent Frontier Data Cleaning mechanism can strengthen data integrity and boost up performance. Moreover, this approach is optimized especially for working under coarse network and runtime environment. The results from lab experiment and field test will fortify the claims.","","0-7695-2687","10.1109/CIT.2006.126","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4019845","Multi-Agent System (MAS);Data;Integration;Data Warehouse","Business;Middleware;Distributed computing;Internet;Multiagent systems;Data warehouses;Application software;Information technology;Collaborative work;Cleaning","","","","1","21","","","","","","IEEE","IEEE Conferences"
"Optisim: A System Simulation Methodology for Optically Interconnected HPC Systems","A. K. Kodi; A. Louri","Ohio University; University of Arizona","IEEE Micro","","2008","28","5","22","36","Although CAD tools have significantly assisted electronic system simulation, the system-level optoelectronics modeling field has lagged behind due to a lack of simulation methodologies and tools. Optisim, a system-level modeling and simulation methodology of optical interconnects for HPC systems, can provide computer architects, designers, and researchers with a highly optimized, efficient, and accurate discrete-event environment to test various HPC systems.","0272-1732;1937-4143","","10.1109/MM.2008.76","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4659272","","Optical interconnections;Computational modeling;Optical crosstalk;Optical attenuators;Power system modeling;Computer simulation;Optical computing;Optical design;Bandwidth;Application software","CAD;discrete event simulation;integrated optoelectronics;optical computing;optical interconnections","Optisim;system simulation methodology;optically interconnected HPC systems;CAD tools;system-level optoelectronics modeling;discrete-event environment","","5","18","","","","","","IEEE","IEEE Journals & Magazines"
"Rate distortion optimal bit allocation methods for volumetric data using JPEG 2000","O. M. Kosheleva; B. E. Usevitch; S. D. Cabrera; E. Vidal","Dept. Electr. & Comput. Eng., Texas Univ., El Paso, TX, USA; Dept. Electr. & Comput. Eng., Texas Univ., El Paso, TX, USA; Dept. Electr. & Comput. Eng., Texas Univ., El Paso, TX, USA; NA","IEEE Transactions on Image Processing","","2006","15","8","2106","2112","Computer modeling programs that generate three-dimensional (3-D) data on fine grids are capable of generating very large amounts of information. These data sets, as well as 3-D sensor/measured data sets, are prime candidates for the application of data compression algorithms. A very flexible and powerful compression algorithm for imagery data is the newly released JPEG 2000 standard. JPEG 2000 also has the capability to compress volumetric data, as described in Part 2 of the standard, by treating the 3-D data as separate slices. As a decoder standard, JPEG 2000 does not describe any specific method to allocate bits among the separate slices. This paper proposes two new bit allocation algorithms for accomplishing this task. The first procedure is rate distortion optimal (for mean squared error), and is conceptually similar to postcompression rate distortion optimization used for coding codeblocks within JPEG 2000. The disadvantage of this approach is its high computational complexity. The second bit allocation algorithm, here called the mixed model (MM) approach, mathematically models each slice's rate distortion curve using two distinct regions to get more accurate modeling at low bit rates. These two bit allocation algorithms are applied to a 3-D Meteorological data set. Test results show that the MM approach gives distortion results that are nearly identical to the optimal approach, while significantly reducing computational complexity","1057-7149;1941-0042","","10.1109/TIP.2006.875216","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1658077","Bit-rate allocation;data compression;image coding;JPEG2000;multidimensional coding;rate distortion theory","Rate-distortion;Bit rate;Transform coding;Mathematical model;Mesh generation;Computational complexity;Grid computing;Distortion measurement;Application software;Data compression","data compression;image coding;mean square error methods","rate distortion optimal bit allocation methods;volumetric data;JPEG 2000;computer modeling programs;data compression algorithms;mean squared error;mixed model approach;image coding","Algorithms;Artifacts;Computer Communication Networks;Data Compression;Image Enhancement;Image Enhancement;Image Interpretation, Computer-Assisted;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Quality Control;Reproducibility of Results;Sensitivity and Specificity;Signal Processing, Computer-Assisted","7","15","","","","","","IEEE","IEEE Journals & Magazines"
"Functional prototyping of special purpose machines using hardware-in-the-loop simulation","W. Li; A. Abel","ITI GmbH, Werbegasse. C/2, D-09116 Dresden, Germany; ITI GmbH, Werbegasse. C/2, D-09116 Dresden, Germany","2006 International Technology and Innovation Conference (ITIC 2006)","","2006","","","1771","1778","As a result of the complexity of special purpose machines many problems arise because of the multiplicity of interacting machine components. Before the assembly of the machine, some of the problems can not be early exposed by virtual simulation and can not be corrected, because one part of simulation chains, i.e. components which are supplied by manufacturers as a black box is not available. To develop these special purpose machines, it is necessary to use Hardware in the loop simulation (HiL-simulation). This paper describes principal requirements for software tools used for the HiL-simulation. The experiences of systematic development of the special purpose machine are suggested, which refer to design, functional verification and optimization of an innovative machine concept. Some software tools (Pro/Engineer, SimulationX, Matlab/Simulink/RTW , ETAS-ascet) are discussed and are exemplary used to implement a HiL-simulation strategy for developing free contouring milling machine with high accuracy. A development of a hexapod that is driven with linear telescopic strut consists of subsystems of different engineer disciplines coupling of mechanical and electrical subsystem. The selected software concept permitted further the transfer the coupled simulation on real time hardware. The real time simulation is needed for the hexapod structure of hardware in the loop test rig, which makes possible to test the real CNC controller before the assembly of the machine with sufficient accuracy.","0537-9989","0-86341-696","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4752291","rapid prototyping;hardware-in-the loop;parallel kinematics;e-manufacturing","","","","","","","","","","","","IET","IET Conferences"
"COOL, LCG conditions database for the LHC experiments: Development and deployment status","A. Valassi; R. Basset; M. Clemencic; G. Pucciani; S. A. Schmidt; M. Wache","IT-DM Group at CERN, Geneva, Switzerland; IT-DM Group at CERN, Geneva, Switzerland; PH-LBC Group at CERN, Switzerland; IT-DM Group at CERN, Geneva, Switzerland; Mainz University, Germany; Mainz University, Germany","2008 IEEE Nuclear Science Symposium Conference Record","","2008","","","3021","3028","The Large Hadron Collider (LHC), the world’s largest and highest-energy particle accelerator, designed to collide opposing beams of protons or lead ions, started its operations in September 2008 at the European Organization for Nuclear Research (CERN) in Geneva, Switzerland. To process and analyze the huge amounts of data generated by the four experiments installed at different collision points along the LHC ring, a large distributed computing infrastructure has been set up, the LHC Computing Grid (LCG). The bulk of this data, referred to as ‘event data’, will record the signals left in the sub-detectors of the four LHC experiments by the passage of the particles generated in the collision of the LHC beams. A different set of data, referred to as ‘conditions data’ and needed for the analysis of event data, will record the experimental conditions at the time the event data were collected, such as the measured temperatures or the calculated calibration factors for the several sub-detectors of each LHC experiment. The COOL project provides common software components and tools for the handling of the conditions data of the LHC experiments. It is part of the LCG Persistency Framework (PF), a broader project set up within the context of the LCG Application Area (AA) to devise common persistency solutions for the LHC experiments. COOL software development is the result of the collaboration between the CERN IT Department and ATLAS and LHCb, the two experiments that have chosen it as the basis of their conditions database infrastructure. COOL supports conditions data persistency using several relational technologies (Oracle, MySQL, SQLite and FroNTier), based on the CORAL Common Relational Abstraction Layer. For both experiments, Oracle is the backend used for the deployment of COOL database services at Tier0 and Tier1 sites of the LHC Computing Grid. While the development of new software functionalities is being frozen as LHC operations are ramping up, the main focus for the project in 2008 has shifted to performance optimization for data insertion and retrieval and to the deployment and test of Oracle database services for COOL. In this presentation, we will review the status and plans of both software development and COOL database service deployment at the time of the NSS conference, a few weeks after the start-up of the LHC.","1082-3654","978-1-4244-2714-7978-1-4244-2715","10.1109/NSSMIC.2008.4774995","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4774995","","Large Hadron Collider;Particle beams;Distributed computing;Grid computing;Temperature measurement;Programming;Relational databases;Linear particle accelerator;Colliding beam devices;Colliding beam accelerators","","","","6","11","","","","","","IEEE","IEEE Conferences"
"QoS-based Selection of Services: The Implementation of a Genetic Algorithm","M. C. Jaeger; G. Muehl","TU Berlin, Institute of Telecommunication Systems, Sek. FR 6-10, Franklinstrasse 28/29, D-10587, Germany; TU Berlin, Institute of Telecommunication Systems, Sek. FR 6-10, Franklinstrasse 28/29, D-10587, Germany","Communication in Distributed Systems - 15. ITG/GI Symposium","","2007","","","1","12","In today's businesses we can see the trend that service-oriented architectures (SOA) represent the main paradigm for IT infrastructures. In this setting, software offers its functionality as an electronic service to other software in a network. In order to realise more complex tasks or business processes that are comprised of individual services, compositions of these are formed. Thus, several research efforts cover the creation of service compositions, including their modelling, development and provision. In this setting, individual services must be selected that perform the tasks of the composition. This selection can consider the quality-of-service (QoS). This paper discusses the optimisation problem when selecting services while considering different QoS characteristics. For this problem, we investigate the application of a genetic algorithm. Based on previous research work, the implementation of this algorithm is tested in our simulation environment SENECA in order to compare its performance with other approaches. Furthermore we use the simulation environment in order to determine the impact of different parameters (e.g. mutation rate, fitness function) on the optimisation capability of the genetic algorithm.","","978-3-8007-2980","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5755506","","Quality of service","","","","20","","","","","","","VDE","VDE Conferences"
"Comparative scatterometric CD and edge profile measurements on a MoSi mask using different scatterometers","M. Wurm; A. Diener; B. Bodermann","Physikalisch-Technische Bundesanstalt Bundesallee 100, D-38116 Braunschweig, Germany; Physikalisch-Technische Bundesanstalt Bundesallee 100, D-38116 Braunschweig, Germany; Physikalisch-Technische Bundesanstalt Bundesallee 100, D-38116 Braunschweig, Germany","24th European Mask and Lithography Conference","","2008","","","1","8","At PTB a new type of DUV scatterometer has been developed. The concept of the system is very variable, so that many different types of measurements like e. g. goniometric scatterometry, ellipsometric scatterometry, polarisation dependent reflectometry and ellipsometry can be performed. The main applications are CD, pitch and edge profile characterisation of nano-structured surfaces mainly, but not only, on photomasks. Different operation wavelength down to 193nm can be used. The system is not only a versatile tool for a variety of different at-wavelength metrology connected with state-of-the- art photolithography. For the evaluation of the measurements the inverse diffraction problem has to be solved. For this purpose a special FEM-based software has been developed, which is capable to solve both the direct diffraction problem and the inverse diffraction problem. The latter can be accomplished using different optimisation schemes. To test both our new scatterometer and the newly developed evaluation software we started systematic comparative CD and edge profile measurements on a state of the art MoSi phase shift mask. In this paper first results are presented and compared with results of a commercial scatterometer.","","978-3-8007-3079","10.1117/12.798784","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5755020","Scatterometry;Ellipsometry;Reflectometry;Diffractometry;CD;pitch;edge profile;polarisation;inverse diffraction problem;at-wavelength metrology","Radar measurements;Ellipsometry;Wavelength measurement;Reflectometry","","","","","","","","","","","VDE","VDE Conferences"
"FDTD modeling of UHF partial discharge sensor response","A. J. Reid; M. Stewart; M. D. Judd","University of Strathclyde, Glasgow, G1 1XW, UK; University of Strathclyde, Glasgow, G1 1XW, UK; University of Strathclyde, Glasgow, G1 1XW, UK","2009 International Conference on Sustainable Power Generation and Supply","","2009","","","1","4","Ultra high frequency (UHF) partial discharge sensors are valuable tools for condition monitoring of high voltage equipment. This paper explores the effectiveness of the finite difference time domain (FDTD) numerical modelling technique as a means to support the design and optimisation of these sensors. The responses a monopole, a planar logarithmic spiral and a novel helical spiral sensor to a step electric field have been obtained both empirically, using a gigahertz transverse electromagnetic test cell, and through software modelling by means of the FDTD technique. Results indicate that FDTD modelling can achieve a reasonably accurate prediction of UHF partial discharge sensor response provided specific environmental electromagnetic conditions are met and the model geometry is an acceptable trade-off between accuracy and simulation time. This software-based approach may prove a useful additional tool in UHF sensor design process.","2156-9681;2156-969X","978-1-4244-4934","10.1109/SUPERGEN.2009.5347959","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5347959","FDTD methods;gas insulated switchgear;partial discharges;power system monitoring;power transformers;UHF couplers","Finite difference methods;Time domain analysis;Partial discharges;Electromagnetic modeling;Solid modeling;Predictive models;Spirals;Frequency;Condition monitoring;Voltage","finite difference time-domain analysis;partial discharge measurement;power system measurement;UHF couplers","FDTD modeling;UHF partial discharge sensor;condition monitoring;high voltage equipment","","13","13","","","","","","IEEE","IEEE Conferences"
"Shallow Water High Resolution Multi-Beam Echo Sounder","H. Li; B. Yao; T. Zhou; Y. Wei; B. Chen; X. Liu; H. Yu; N. Weng","College of Underwater Acoustic Engineering, Harbin Engineering University, Harbin, 150001, P.R.China. Email: hsli@hrbeu.edu.cn, hsenli@126.com; College of Underwater Acoustic Engineering, Harbin Engineering University, Harbin, 150001, P.R.China; College of Underwater Acoustic Engineering, Harbin Engineering University, Harbin, 150001, P.R.China; College of Underwater Acoustic Engineering, Harbin Engineering University, Harbin, 150001, P.R.China; College of Underwater Acoustic Engineering, Harbin Engineering University, Harbin, 150001, P.R.China; College of Underwater Acoustic Engineering, Harbin Engineering University, Harbin, 150001, P.R.China; College of Underwater Acoustic Engineering, Harbin Engineering University, Harbin, 150001, P.R.China; College of Underwater Acoustic Engineering, Harbin Engineering University, Harbin, 150001, P.R.China","OCEANS 2008 - MTS/IEEE Kobe Techno-Ocean","","2008","","","1","5","This paper presents a Shallow water High Resolution Multi-Beam Echo Sounder (SHMBES), which will be used widely in modern ocean exploration and underwater engineering. It is highly synthesized and developed on the basis of many new techniques such as theories of underwater acoustic and modern signal processing, Very Large Scale Integration (VLSI), DSP array processing, etc. It is mainly composed of sonar deck unit (including multi-channels receiver and A/D conversion, real-time digital signal processing system, host computer and interface, etc.) and sonar watertight housing (including transducer, signal generation and control phased transmitter, etc.), as well as real-time and postprocessing software. Five parts are discussed in the paper: Section I is system structure and technical specification. Section II is the theory analysis of split sub-array phase detection and multiple sub- array detection methods. In this section the improved multiple sub- array detection method is presented. Section III is precision evaluation experiment. Three sub-array configurations are chose to process the experimental data obtained at sea based on theoretical analysis. It has shown that, with optimized parameters, the multiple sub-array detection method not only realizes the wide coverage, but also improves the detection performance. Section IV lists Songhua Lake and Mogan Lake measurement results of underwater terrain. The instrument validity of hardware and software is tested by these lake experiments. Section V are the conclusions of the paper.","","978-1-4244-2125-1978-1-4244-2126","10.1109/OCEANSKOBE.2008.4531081","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4531081","","Lakes;Signal resolution;Array signal processing;Very large scale integration;Digital signal processing;Sonar;Real time systems;Phase detection;Phased arrays;Oceans","array signal processing;bathymetry;digital signal processing chips;lakes;oceanographic equipment;sonar detection;sonar signal processing;VLSI","shallow water high resolution multibeam echo sounder;SHMBES;ocean exploration;underwater engineering;underwater acoustics;sonar signal processing;very large scale integration;sonar array processing;sonar deck unit;sonar watertight housing;subarray phase detection;multiple subarray detection;Songhua Lake;Mogan Lake;underwater terrain","","6","11","","","","","","IEEE","IEEE Conferences"
"Studies of control strategies for high pressure common rail diesel engine","Wang Hongrong; Zhang Youtong; Wang Jun","School of Mechanical and Vehicular Engineering, Beijing Institute Technology, China; School of Mechanical and Vehicular Engineering, Beijing Institute Technology, China; The Academy of Armored Forces Engineering, Beijing China","2008 IEEE Vehicle Power and Propulsion Conference","","2008","","","1","5","The main objective of this study is to develop the injection control strategy for high pressure common rail diesel engine based on the Electronic Control Unit (ECU) with the micro-controller MPC555. The control software was designed by modularization method according to various working conditions of diesel engine. The software modules include the fuel quantity control module, rail pressure control module, point of the engine operation (POP) management module, four stages of the start procedure control module, and so on. Finally, experiments were carried out on test bench to verify the control feasibility and reliability of the strategies and optimize the emissions, Experimental results showed that the anticipated control effects were achieved and the engine reached the EURO-square exhaust emission standard.","1938-8756","978-1-4244-1848-0978-1-4244-1849","10.1109/VPPC.2008.4677606","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4677606","Common Rail;Diesel Engine;Control Strategies","Pressure control;Rails;Diesel engines;Fuels;Temperature;Open loop systems;Vehicles;Railway engineering;Quality control;Automotive engineering","diesel engines;microcontrollers;pressure control;railway engineering","high pressure common rail diesel engine;injection control strategy;electronic control unit;microcontroller MPC555;modularization method;fuel quantity control module","","4","11","","","","","","IEEE","IEEE Conferences"
"The Effect on Bonding Process and Reliability for ACF Applications","W. R. Jong; S. H. Peng","Dept. of Mechanical Engineering, Chung Yuan Christian University, 200, Chung Pei Rd, Chung Li, Tao Yuan, Taiwan, R.O.C.; Dept. of Mechanical Engineering, Chung Yuan Christian University, 200, Chung Pei Rd, Chung Li, Tao Yuan, Taiwan, R.O.C.","2008 3rd International Microsystems, Packaging, Assembly & Circuits Technology Conference","","2008","","","197","200","Due to the environmental protection issue and regulation, many material suppliers try to find the replacing material. Now, the most popular technologies which can replace the classical Pb/Sn material are lead-free joints and anisotropic conductive films (ACFs). Because conductive particles play an influential role in component, this paper is going to investigate the effects that conductive particles at different spots in package. The CAE software, ANSYS, is used to examine the bonding and reliability assessment of ACF in the fine pitch chip on flex package with a-quarter model under various bonding process parameters. First of all, Taguchi method is used to study the bonding processes and associated parameters. This research also considers the noise factors which generate disturbing function in orthogonal array and discusses the conductive particles which have more distortion in package. After getting the analysis of variance, the thermo-mechanical behaviors of the optimized package undergoes the temperature test. According to the specification of moisture soak, using the moist sensitivity level 1, which is analyzes the phenomenon of moisture absorbed. The package consists of the multi-material components. As the thermo-mechanical status of the package results from the different coefficient of thermal expansion with the loading variations, it is obvious that von Mises in the devices happens between bump and pad, which may cause crack initiation. Taguchi method with noise factors can get more suitable parameters and arrangement of conductive particles, that is, inside one creates more stress than the other. Therefore, the thermo-mechanical behaviors of the optimized package show that the maximum stress and warpage still happen at the interface between the bump-ACF and the ACF-pad in the corner of package at a higher temperature. The simulation technique of heat conduction can be applied to the analysis of moist absorbability as well. Results under moist sensitivity level 1 show that the moisture propagates slowly from outer surface to the inner package.","2150-5934;2150-5942","978-1-4244-3622-4978-1-4244-3624-8978-1-4244-3623","10.1109/IMPACT.2008.4783843","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4783843","","Bonding processes;Packaging;Conducting materials;Thermomechanical processes;Moisture;Temperature sensors;Thermal stresses;Protection;Tin;Joining materials","bonding processes;moisture;packaging;reliability;Taguchi methods;thermal expansion;thin films","bonding process;reliability;anisotropic conductive films;package;flex package;Taguchi method;noise factors;thermo-mechanical behaviors;moisture;thermal expansion","","1","9","","","","","","IEEE","IEEE Conferences"
"A Secure and Privacy-Protecting Aggregation Scheme for Sensor Networks","E. de Cristofaro","SCNLab - Dipartimento di Informatica ed Applicazioni - Università di Salerno, I-84084, Fisciano (SA), Italy, Email: emidec@dia.unisa.it","2007 IEEE International Symposium on a World of Wireless, Mobile and Multimedia Networks","","2007","","","1","5","Wireless sensor networks are emerging as a practical and powerful solution to monitoring problems. However, their use in real world scenarios faces several issues. In fact, sensor nodes are tiny devices with very limited resources, and thus it is important to optimize the battery consumption. For instance, aggregation operations are widely used to minimize transmission operations, that are the most power-consuming operations for the devices. In addition, since sensor nodes operate in open and often hostile environments, security issues must be taken into account. In this paper, we propose a novel aggregation scheme which provides data integrity and privacy-protection and guarantees efficiency with respect to data transmissions, congestion, and memory overhead. To test performance of the scheme, we implemented it within a simulation software environment. We tested its performance to verify its feasibility and we evaluated the computation and power required to ensure security. Simulation results confirmed that our scheme is to be considered efficient enough to being used in real world scenarios.","","978-1-4244-0992-1978-1-4244-0993","10.1109/WOWMOM.2007.4351796","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4351796","","Cryptography;Batteries;Acoustic sensors;Data security;Protection;Monitoring;Computational modeling;Privacy;Wireless sensor networks;Data communication","","","","1","14","","","","","","IEEE","IEEE Conferences"
"Integrated tuning technology for antennas and radio front ends","A. Morris","CTO","2009 IET Seminar on Adaptable and Tunable Antenna Technology for Handsets and Mobile Computing Products","","2009","","","1","34","□ Large and Compelling Opportunity S Additional Modes and Bands (LIE, wiMax, …) ✓ Significant BOM and Footprint Reduction (antennas / switches / filters / amplifiers) ✓ Substantial Platform RF Optimization Cycle Reduction (Weeks reduced to hours) ✓ Performance Improvements (Transmit Efficiency, Receive Sensitivity) □ Differentiated and Enabling Product Technology ✓ CMOS/MEMS integration, Standard Fabless Flows (CMOS cost, scale, yield) ✓ High RF Performance (Ratio, Q, Isolation, Linearity) ✓ CMOS blocks Developed (≤ 100uA w/ 3V supply, SPI interface, ESD protection) ✓ Proven wafer-level sealing (Low Cost, Easy Assembly, Reproducibility, Reliability) ✓ Durable and Reliable (Temperature Stability, Cycling, Environmental robustness) ✓ Applicable to wide range of applications (roadmap leading to full front-end) □ Strong Initial Market Traction ✓ Multiple Tier 1 Handset OEM Development Engagements ✓ Tier 1 Network Operator Investment and Development Engagement ✓ Tier 1 Front-End Module Development Engagement □ Transitioning from Technology Development to Products and Revenue ✓ Initial products developed and tested ✓ 100's delivered for handset integration, testing and software development ✓ Integrated MEMS-CMOS process frozen at Jazz Semiconductor ✓ Application/Product Roadmap using baseline technology — Antennas, PAs, Duplexers … ✓ Tuner product in qualification!","0537-9989","978-1-84919-197","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5352617","","","","","","3","","","","","","","IET","IET Conferences"
"Performing longitudinal measurements in rodents using small animal PET imaging","H. M. Wu; R. W. Silverman; N. G. Harris; R. Sutton","Department of Molecular and Medical Pharmacology, David Geffen School of Medicine, Univ. of California, Los Angeles, USA; Department of Molecular and Medical Pharmacology, David Geffen School of Medicine, Univ. of California, Los Angeles, USA; Department of Surgery, Univ. of California, Los Angeles, USA; Department of Surgery, Univ. of California, Los Angeles, USA","2008 IEEE Nuclear Science Symposium Conference Record","","2008","","","5232","5233","Although tremendous effort and finances were spent in developing small animal PET scanners, this technology remains under utilized as a research tool to study disease states. To advance this goal, we previously developed a microfluidic blood sampling device to overcome the problem of blood sampling and fluid volume loss when performing quantitative PET studies. The device was previously shown to be reliable for deriving input function without changing the physiological stability of a mouse. In this study, we improved methodologies and optimized the animal surgical preparation necessary for prolonging the patency of chronic indwelling catheters. We have also added several features to improve functionality and ease of use for a new prototype of a microfluidic blood sampler and tested this device for accuracy in determining FDG blood curves in rats with a chronic indwelling catheter. We modified the hardware and software of the previous blood sampling device to reduce the cost and provide a more user-friendly interface. We successfully prepared the animal for deriving input functions for multiple microPET studies. Our results suggest that longitudinal microPET imaging in the same animal is possible. These new developments should increase the use of PET imaging in drug development and translational research using small animals.","1082-3654","978-1-4244-2714-7978-1-4244-2715","10.1109/NSSMIC.2008.4774413","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4774413","","Performance evaluation;Rodents;Animals;Positron emission tomography;Blood;Sampling methods;Microfluidics;Catheters;Diseases;Stability","","","","","1","","","","","","IEEE","IEEE Conferences"
"Efficient implementation of fixed-point fft on ADSP-TS201","Xin Li; Feng Liu; Teng Long","Radar Research Laboratory, Beijing Institute of Technology, 100081, China; Radar Research Laboratory, Beijing Institute of Technology, 100081, China; Radar Research Laboratory, Beijing Institute of Technology, 100081, China","2009 IET International Radar Conference","","2009","","","1","4","Due to the DRAM used in the internal memory of ADSP-TS201 produced by ADI, it is not optimized for random access in the conventional FFT algorithm using standard structure. SingLeton structure is used to ensure that the reads in each stage are sequential. Program flow of this algorithm is introduced, and efficient implementation is provided by using appropriate assembly instructions and well designed software pipeline. Test results shows that the performance of FFT on TS201 is greatly improved, and it can be widely used in radar signal processing.","0537-9989","978-1-84919-010","10.1049/cp.2009.0141","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5367625","FFT;TS201;SingLeton;fixed-point;assembly","","DRAM chips;fast Fourier transforms;fixed point arithmetic;radar signal processing","fixed-point FFT;ADSP-TS201;DRAM;radar signal processing;SingLeton","","","","","","","","","IET","IET Conferences"
"Research and improvement of a zero-voltage zero-current switching full-bridge converter","Yan Bao; Sheng Li; Jiuchun Jiang; Weige Zhang","Beijing Jiaotong University, School of Electrical Engineering, 100044, China; Beijing Jiaotong University, School of Electrical Engineering, 100044, China; Beijing Jiaotong University, School of Electrical Engineering, 100044, China; Beijing Jiaotong University, School of Electrical Engineering, 100044, China","2008 IEEE Power Electronics Specialists Conference","","2008","","","825","829","As is known to all, full-bridge topology is one of the most common circuits in the field of power technology. In pursuit of the efficient and diminutive power equipment all the while, soft-switching technology has been widely applied as an important instrument at present. In this paper, a novel zero-voltage zero-current switching (ZVZCS) full-bridge DC/DC converter is presented to improve the topology of previous circuit. The operation principle of previous topology has been analyzed at first. Since the ZVS effect of the previous converterpsilas leading-leg IGBT cannot be realized very well in the light-load situation, the previous ZVZCS full-bridge converter was optimized. For the purpose of testing and validating the operating principle of improved ZVZCS full-bridge converter and all the analysis, the simulation of this novel ZVZCS full-bridge converter has been carried out with the software PSPICE. With the purpose of charging up the electric vehiclepsilas power storage battery which will be used in 2008 Olympic Games, a prototype (10 kW 130 V/80 A) adopting modified ZVZCS converter is designed and validated by experiment. Finally, the results of the simulation and experiment are provided to verify the principle of this novel ZVZCS converter. At the same time, five conclusions are provided too.","0275-9306;2377-6617","978-1-4244-1667-7978-1-4244-1668","10.1109/PESC.2008.4592032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4592032","","Converters;Insulated gate bipolar transistors;Zero voltage switching;Turning;Switches;Inductance;Prototypes","DC-DC power convertors;zero current switching;zero voltage switching","zero-voltage switching;zero-current switching;full-bridge converter;full-bridge topology;soft-switching technology;full-bridge DC-DC converter;ZVS;ZCS;power 10 kW;voltage 130 V;current 80 A","","3","7","","","","","","IEEE","IEEE Conferences"
"The real-time simulation of ball-beam system based on dSPACE","F. Pan; D. Chen; E. Da-zhi; D. Xue","College of Information Science and Engineering, Northeastern University, Shenyang 110004, China; College of Information Science and Engineering, Northeastern University, Shenyang 110004, China; College of Information Science and Engineering, Northeastern University, Shenyang 110004, China; College of Information Science and Engineering, Northeastern University, Shenyang 110004, China","2008 Chinese Control and Decision Conference","","2008","","","221","224","Based on dSPACE real-time simulation technology, the mathematical model is built and servo control simulation system of ball-beam is designed by applying modeling methods of Matlab/Simulink system and environment of dSPACE system software and hardware. Actual controlled object, ball-beam system is placed inside simulation loop for simulation study, the controller is designed and the virtual instrument is built. By watching system running, system performance is tested on line. And the parameters of controller are tuned and optimized by the ITAE criterion, satisfactory control result is achieved.","1948-9439;1948-9447","978-1-4244-1733-9978-1-4244-1734","10.1109/CCDC.2008.4597303","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4597303","dSPACE;Ball-beam System;MATLAB/Simulink;Virtual Instrument;Real-time Simulation","MATLAB;Mathematical model;Control systems;Real time systems;Instruments;Electronic mail;Materials","mathematics computing;servomechanisms;virtual instrumentation","real-time simulation;ball-beam system;dSPACE;servo control simulation system;Matlab;Simulink system","","1","7","","","","","","IEEE","IEEE Conferences"
"Meeting the evolving challenges of the semiconductor industry","M. S. Abadir","Freescale, USA","2007 International Conference on Design & Technology of Integrated Systems in Nanoscale Era","","2007","","","I","I","The semiconductor industry is approaching maturity. Growth expectations scaled back from historical norm Investments to stay competitive are outpacing industry growth. At the same time, customer pressure to keep costs down is relentless. Technology and product development costs are soaring. Very few companies can afford process development and manufacturing expense for leading edge technologies. These rising costs are driving fablite/fabless models, as well as partnerships for technology development. Industry players are also increasing presence in low cost geographies. Market forces is also causing product cycles to continue shrinking while system complexity continues to grow. Semiconductor industry is increasingly driven by the consumers Embedded intelligence is driving consumer growth and creating opportunities for convergence. Convergence sets up a huge growth opportunity - but also a threat to traditional business models. Companies will have to alter traditional mindsets and business models to succeed. Customer expectations are evolving with increased solution focus, not just a silicon focus. This is driving partnerships, global alliances, and differentiated business &amp; investment models Success in the marketplace requires High levels of Integration, so companies must build strong software and systems capability. In addition to the global economic and market challenges, the semiconductor industry is facing several technology challenges as well. As technology scales it is becoming increasingly difficult for traditional design tools and models to accurately predict silicon behavior. Compounding this is the increasing sensitivity to process variations. These trends necessitate a tight coupling between design and manufacturing. This is apparent in the emergence of Design for manufacturability (DFM) as a major thrust in the last few years. DFM disciplines are rapidly gaining acceptance as a mechanism to model lithography effects in design and to incorporate rules and disciplines that would optimize yield. Advanced delay testing techniques have also proven very valuable as a mechanism to measure and analyze silicon behavior in order to understand how it correlates with the design models and to try and close the feedback loop between manufacturing and design. Another major challenge is the emergence of power as a primary focus for most designs. This has been driven primarily by physical and energy constraints as well as by consumer demands for battery-based products. This has driven the need for aggressive support for low power design techniques through the entire design flow encompassing representation, implementation and verification.","","978-1-4244-1277-8978-1-4244-1278","10.1109/DTIS.2007.4449480","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4449480","","Electronics industry;Costs;Silicon;Investments;Manufacturing industries;Semiconductor device manufacture;Companies;Design for manufacture;Product development;Manufacturing processes","consumer behaviour;costing;low-power electronics;product development;semiconductor device manufacture;semiconductor technology","semiconductor industry;product development cost;process development;consumer growth;marketplace;design-for-manufacturability;lithography effects;battery-based products;consumer demands;low power design techniques","","","","","","","","","IEEE","IEEE Conferences"
"The Research of Aerial RS Real-time Image Compression and Transmission Based on DSP","J. Chuan; Q. Qiming; L. Jie; C. Dezhi","NA; NA; NA; NA","2006 IEEE International Symposium on Geoscience and Remote Sensing","","2006","","","813","816","Aerial Remote Sensing (Aerial RS) image compression &amp; transmission on-board system, not only is the core of Aerial RS supervising, but also the key technology about the security of data obtainment. The requirement of Aerial RS is stricter on the data quality and security. If we transmit the images through some special channels while the images are obtained during the task of Aerial RS, the control center on the ground could acquire the status of the whole Aerial RS system. Besides, it could also backup the images immediately. Image compression &amp; transmission system is the important bridge between the Aerial RS system on-board and the control center. The research group of Aerial RS data processing in Peking University, integrates all the necessary technologies including compression algorithm &amp; hardware, integration of function modules, protocol of data packing &amp; unpacking. The research group has developed an efficient compression method that has been optimized both in software structures and hardware architecture. Considering with the narrow space on the airplane, the research group has designed the compression &amp; transmission function module which is installed on the motherboard of the Aerial RS control system. The compression program is compiled in the Code Composer Studio (CCS) development environment, and then the result file compiled is burned into the DSP chip on the module. This paper also introduces an efficient protocol designed by the research group, which can ensure the accuracy of the transmission. It ensures the control center can rebuild &amp; unpack the packages correctly, and avoid fatal errors caused by some false frames and packages during the transmission. This data compression &amp; transmission system, which has been tested in certain different places in China, has achieved the purpose expected.","2153-6996;2153-7003","0-7803-9510","10.1109/IGARSS.2006.208","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4241355","","Image coding;Digital signal processing;Control systems;Data security;Space technology;Hardware;Protocols;Packaging;Remote sensing;Bridges","data compression;digital signal processing chips;image coding;protocols;remote sensing;visual communication","aerial remote sensing;real time image compression;image transmission;protocol;data unpacking;Code Composer Studio;DSP chip;China","","1","7","","","","","","IEEE","IEEE Conferences"
"Design and Fabrication of Non-silicon-based Piezoresistive MEMS Tactile Sensor","A. Wisitsoraat; V. Patanasetagul; A. Tuantranont; N. Poonnikorn","National Electronics and Computer Technology Center, 112 Pahol Yothin Rd., Pathumthani THAILAND. Email: anurat.wisitsoraat@nectec.or.th; National Electronics and Computer Technology Center, 112 Pahol Yothin Rd., Pathumthani THAILAND; National Electronics and Computer Technology Center, 112 Pahol Yothin Rd., Pathumthani THAILAND; Department of Physics, King Mongkut's University of Technology Thonburi 91 Pracha-utit Rd. Bangkok, Thailand","SENSORS, 2006 IEEE","","2006","","","1317","1320","In this work, we develop a new non-silicon-based piezoresistive MEMS tactile sensor by a low-cost fabrication process. Prior to fabrication, the structure was designed and simulated by finite element simulation software to predict the device behaviors. The MEMS tactile sensor with a trampoline membrane structure has been fabricated by successive sputtering of Cr, Al, AlN, indium tin oxide (ITO), and Au layers through electroplated Ni micro-shadow masks over an 8 mm-thick photoresist sacrificial layer on a glass substrate. In addition, the gauge factor of the ITO piezoresistive material was studied as a function of sputtering parameter including oxygen flow rate and film thickness to optimize the sensitivity of the sensor. It was found that the 0.4 mm-thick ITO film deposited with no oxygen flow provide an optimum gauge factor of ~650. The fabricated tactile sensor has been tested for displacement and force sensing and a high sensitivity of 0.2 mV/muN has been achieved.","1930-0395","1-4244-0375-81-4244-0376","10.1109/ICSENS.2007.355872","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4178867","","Fabrication;Piezoresistance;Micromechanical devices;Tactile sensors;Indium tin oxide;Predictive models;Sputtering;Finite element methods;Biomembranes;Chromium","finite element analysis;force sensors;membranes;microsensors;piezoresistance;piezoresistive devices;sputtering;tactile sensors","piezoresistive MEMS tactile sensor;finite element simulation;trampoline membrane structure;sputtering;micro-shadow masks;electroplating;photoresist sacrificial layer;gauge factor;piezoresistive material;force sensing;displacement sensing;size 0.4 mm","","6","13","","","","","","IEEE","IEEE Conferences"
"TCP performance measurement in different GPRS network scenarios","A. K. Othman; M. Zakaria; K. A. Hamid","Universiti Malaysia Sarawak, 94300 Kota Samarahan, Malaysia; Universiti Malaysia Sarawak, 94300 Kota Samarahan, Malaysia; Universiti Malaysia Sarawak, 94300 Kota Samarahan, Malaysia","2007 Asia-Pacific Conference on Applied Electromagnetics","","2007","","","1","5","This paper presents the results of measured TCP performance in one of the commercially deployed live GPRS network in Malaysia under various network scenarios based on userspsila physical locations. End-to-end evaluation of FTP application is used for the assessment. Tracing at the GPRS air interface is done simultaneously, using a proprietary GSM/GPRS air interface testing tool and software. The results show that TCP performance is stable in all scenarios and can adapt fairly well to GPRS mobility and bad channel condition, although at the expense of reduced throughput. TCP imposes its slow start/congestion avoidance mechanism in such situations where packet loss is common. Tuning the TCP parameters to optimise its performance also proves to be beneficial.","","978-1-4244-1434-5978-1-4244-1435","10.1109/APACE.2007.4603985","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4603985","GPRS;TCP;Performance;Network Scenarios","","cellular radio;packet radio networks;transport protocols","different GPRS network;TCP performance measurement;Malaysia;user physical locations;FTP application;GSM/GPRS air interface;start/congestion avoidance","","3","11","","","","","","IEEE","IEEE Conferences"
"Energy Efficient Permanent Magnet IND-SYN Motor","A. Sivaprakasam; K. M. Kumar","Lecturer Department of EEE, Kongu Engineering College, Erode - 638 052, India. sivaa_prakasam@yahoo.com; Professor & Head, Department of EEE, PSG College of Technology, Coimbatore -641 004, India.","2006 3rd IET International Conference on Power Electronics, Machines and Drives - PEMD 2006","","2006","","","326","330","In modem industrialized country about 65% of electrical energy is consumed by AC motors. With the worldwide trend to energy conservation, there is a need to increase the efficiency of AC motors, small and large. To cope up with this, the improvement of materials and optimization of design strategies have been tried out. The Permanent Magnet Ind-Syn motor (PMISM) have been developed in the recent years and gained popularity in various industrial applications. Owing to rare-earth Permanent Magnets, these motors have higher efficiency and higher power-factor. The use of PMISM has become a more attractive option than Induction motors. It is quite possible that this permanent magnet Induction-Synchronous motor will become predominant in the near future. The PMISM has a conventional Induction motor stator and cage rotor wit h permanent magnets buried inside the rotor. The main advantages of PMISM are their simple construction, minimum maintenance, no commutator or slip rings. Hence, this work aims at the design and development of permanent magnet Ind - Syn motor. The motor starts as an Induction motor, gets synchronized and runs at synchronous speed. Attractive features of the proposed model are compactness, energy efficient operation, reduction in active materials used in motor. A prototype has been designed, developed and tested. Results validate the superior performance of this new technology. The above feasible design and simulation of motor has been done through, the CAD/CAE software, namely AutoCAD 2000 and MOTORPRO.","","0-86341-609","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4123539","Permanent magnet Ind-Syn motor (PMISM);Permanent Magnet;Energy efficiency","","","","","","","","","","","","IET","IET Conferences"
"On-line PID controller tuning using genetic algorithm and DSP PC board","P. Fabijanski; R. Lagoda","Institute of Control and Industrial Electronics, Warsaw University of Technology, Poland; Institute of Control and Industrial Electronics, Warsaw University of Technology, Poland","2008 13th International Power Electronics and Motion Control Conference","","2008","","","2087","2090","In this paper we uses a software pack, which allows to adjust PID controller automatically in real time way in the case of changes parameters of used object. PID settings are optimized in real time by using genetic algorithm and are compared with generally accepted quality indicators for control systems like rise time, % overshoot and settling time of step response or such coefficient like oscillations frequency, reduced damping and gain of second order object. A digital signal processor (DSP) is a specialized microprocessor designed specifically for digital signal processing generally in real time applications requiring high computation. The simulation results were verified by some laboratory test.","","978-1-4244-1741-4978-1-4244-1742","10.1109/EPEPEMC.2008.4635574","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4635574","Control of drive;genetic algorithm;DSP;PID controller","Genetic algorithms;Transfer functions;Digital signal processing;Control systems;Real time systems;Tuning;Motion control","control engineering computing;digital signal processing chips;electric drives;genetic algorithms;real-time systems;three-term control","on-line PID controller tuning;genetic algorithm;DSP PC board;digital signal processor;microprocessor;drive control","","4","4","","","","","","IEEE","IEEE Conferences"
"Simple integrated system for wireless backhaul networks","M. R. Ul Islam; L. Faisal; T. Abd Rahman","Wireless Communication Center., Faculty of Electrical Engineering, Universiti Teknologi Malaysia, 81310 Skudai, Johor Bahru, Malaysia; Wireless Communication Center., Faculty of Electrical Engineering, Universiti Teknologi Malaysia, 81310 Skudai, Johor Bahru, Malaysia; Wireless Communication Center., Faculty of Electrical Engineering, Universiti Teknologi Malaysia, 81310 Skudai, Johor Bahru, Malaysia","2008 International Conference on Computer and Communication Engineering","","2008","","","341","345","This paper proposed a simple design of integrated system for wireless back haul network that can work as a wireless bridge and access point. This system has an antenna which is designed to work with frequency range 2.4 - 2.4835 GHz IEEE 802.11g standard and intended for point to multi point link. Another antenna designed on frequency range 5.725 - 5.825 GHz unlicensed band IEEE 802.11a standard for point to point link. Therefore, two antennas on different frequencies and polarizations has been developed and finally incorporated with a router board with two radios. First antenna is the radial waveguide slot array (RWSA) which is an uni-omni directional antenna performs as wireless access point and supports 2.4 GHz clients. The initial element design was optimized in the structure based on Zeland fidelity, FDTD (finite-difference time-domain) with full-3D EM simulation for the radiation pattern as well as return loss. Second antenna used is radial line slot array antenna (RLSA), which is directional antenna and used for main Internet source. Return loss, gain and radiation pattern simulated by software which is developed on Borland C++ 5.0. Tests were setup in different environments, in the lab and outdoors and practical measurements has been taken to validate the simulation results and performance. The proposed design can find its applications in wireless communication systems, especially where cost-effective high-speed wireless Internet access service is required.","","978-1-4244-1691-2978-1-4244-1692","10.1109/ICCCE.2008.4580625","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4580625","RLSA;RWSA;Wireless Backhaul;Return loss;RSSI","Frequency;Directive antennas;Slot antennas;Antenna arrays;Directional antennas;Finite difference methods;Time domain analysis;Antenna radiation patterns;Bridges;Polarization","directive antennas;Internet;microwave antenna arrays;radio access networks;slot antenna arrays;telecommunication network routing;UHF antennas;wireless LAN","integrated system;wireless back haul networks;wireless bridge;wireless access point;IEEE 802.11g standard;point-to-multi point link;IEEE 802.11a standard;router board;radial waveguide slot array;uni-omni directional antenna;Zeland fidelity;finite-difference time-domain;full-3D EM simulation;radiation pattern;radial line slot array antenna;directional antenna;Borland C++ 5.0;cost-effective high-speed wireless Internet access service;frequency 2.4 GHz to 2.4835 GHz;frequency 5.725 GHz to 5.825 GHz","","2","7","","","","","","IEEE","IEEE Conferences"
"Life cycle cost evaluation of off-grid PV-wind hybrid power systems","F. Morea; G. Viciguerra; D. Cucchi; C. Valencia","Calzavara Spa - s.s. 13 Pontebbana, Basiliano (UD), Italy; Calzavara Spa - s.s. 13 Pontebbana, Basiliano (UD), Italy; Labor srl - Area Science Park, Trieste, Italy; Labor srl - Area Science Park, Trieste, Italy","INTELEC 07 - 29th International Telecommunications Energy Conference","","2007","","","439","441","This paper presents a design method based on an expert system for the optimisation of hybrid power systems used in telecom applications. Hybrid power systems for off-grid telecom applications have been widely discussed and tested since they can offer a potentially attractive alternative to diesel-based power systems, but they have not yet proven to be more reliable and cost competitive than conventional solutions. The main issues in hybrid power systems are inadequate component reliability -leading to unpredicted costs and unforeseen harsh environmental conditions exceeding the domain of conventional design techniques. While components are undergoing considerable improvement, system design methods and life cycle cost calculation are not being addressed thoroughly. Several software programs are currently available for simulating the operation of hybrid electric power systems, using either simple algorithms or dynamic methods, but none includes all relevant information for telecom applications: information on local cost, reliability, climatic conditions for sites of interest, remote monitoring, maintenance strategies and actual estimate of maintenance and reduced running costs. An expert system (ES) has been developed on the basis of extensive dynamic simulation, using a conventional simulation model, and all relevant information on cost, maintenance schemes, load profiles specifically tailored for actual ""telecom outdoor applications"" in Italy and other countries in the Mediterranean area. Technology include Photovoltaic and Wind generators, battery storage and backup diesel. The ES has also been used to asses the potential impact of innovative components (such as charge regulators, fuel cells and remote monitoring systems) on reliability and life cycle cost. The ES only requires information generally available at the initial stage of a project (e.g. site installation, network type, reliability requirements) to determine the best hybrid mix of diesel generation, battery storage, photovoltaic and wind generation, for optimal performance in the regions of interest. The method has been validated using data on existing telecom installations and has been applied to the design of a new hybrid power system.","0275-0473;2158-5210","978-1-4244-1627-1978-1-4244-1628","10.1109/INTLEC.2007.4448814","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4448814","","Costs;Hybrid power systems;Power system reliability;Maintenance;Expert systems;Remote monitoring;Photovoltaic systems;Solar power generation;Telecommunication network reliability;Wind energy generation","expert systems;hybrid power systems;life cycle costing;photovoltaic power systems;power engineering computing;telecommunication power supplies;wind power plants","life cycle cost evaluation;off-grid PV-wind hybrid power systems;off-grid telecom applications;diesel-based power systems;expert system;extensive dynamic simulation;telecom outdoor applications;photovoltaic generators;wind generators;battery storage;backup diesel","","7","5","","","","","","IEEE","IEEE Conferences"
"Memory efficient string matching algorithm for network intrusion management system","J. Yu; Y. Xue; J. Li","Department of Automation, Tsinghua University, Beijing 100084, China; Research Institute of Information Technology, Tsinghua University, Beijing 100084, China; Research Institute of Information Technology, Tsinghua University, Beijing 100084, China; Tsinzhua National Laboratory for Information Science and Technolozy, Tsinzhua University, Beijing 100084, China; Research Institute of Information Technology, Tsinghua University, Beijing 100084, China; Tsinzhua National Laboratory for Information Science and Technolozy, Tsinzhua University, Beijing 100084, China","Tsinghua Science and Technology","","2007","12","5","585","593","As the core algorithm and the most time consuming part of almost every modern network intrusion management system (NIMS), string matching is essential for the inspection of network flows at the line speed. This paper presents a memory and time efficient string matching algorithm specifically designed for NIMS on commodity processors. Modifications of the Aho-Corasick (AC) algorithm based on the distribution characteristics of NIMS patterns drastically reduce the memory usage without sacrificing speed in software implementations. In tests on the Snort pattern set and traces that represent typical NIMS workloads, the Snort performance was enhanced 1.480/0–200/0 compared to other well-known alternatives with an automaton size reduction of 4.86–6.11 compared to the standard AC implementation. The results show that special characteristics of the NIMS can be used into a very effective method to optimize the algorithm design.","1007-0214","","10.1016/S1007-0214(07)70137-2","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6071800","string matching;network intrusion management system (NIMS);Aho-Corasick (AC) algorithm","Algorithm design and analysis;Doped fiber amplifiers;Pattern matching;Memory management;Automata;Data structures;Heuristic algorithms","","","","1","","","","","","","TUP","TUP Journals & Magazines"
"Uni-omnidirectional radial waveguide slot array antenna design for wireless local area network","L. F. Abdulrazak; T. A. Rahman","Wireless Communication Center (WCC), Faculty of Electrical Engineering, Universiti Teknologi Malaysia, 81310 UTM Skudai, Malaysia; Wireless Communication Center (WCC), Faculty of Electrical Engineering, Universiti Teknologi Malaysia, 81310 UTM Skudai, Malaysia","2008 Asia-Pacific Symposium on Electromagnetic Compatibility and 19th International Zurich Symposium on Electromagnetic Compatibility","","2008","","","562","565","This paper has considered the development of a low profile unidirectional antenna for WLAN application, performed with frequency range 2.4-2.4835 GHz. The motivation for this work was the need for an effective directional antenna to be used with the access point. The Radial Waveguide Slot Array Antenna (RWSA) is investigated as a low profile, lightly weight, simple but effective, easy to design and fabricate with reasonable cost efficient. The initial element design was first optimized in the structure based on Zeland Fidelity, FDTD (Finite-Difference Time-Domain) with Full-3D EM Simulation for the radiation pattern, and return loss. The second part of this paper deals with producing design relying on optimum simulation results, the antenna tested in terms of return loss, gain and radiation pattern. Finally, experimental trial had been consociate and compared RWSA Antenna with a monopole antenna on an Access point for WLAN by using AirMagnet software. Measurements are conformed to results presented by the prototype simulation.","","978-981-08-0629","10.1109/APEMC.2008.4559937","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4559937","","Antenna arrays;Slot antennas;Wireless LAN;Finite difference methods;Time domain analysis;Antenna radiation patterns;Frequency;Directional antennas;Optical arrays;Costs","directive antennas;finite difference time-domain analysis;slot antenna arrays;UHF antennas;wireless LAN","uni-omnidirectional antenna;wireless local area network;WLAN;directional antenna;Zeland fidelity;finite difference time domain analysis;full-3D EM simulation;radial waveguide slot array;frequency 2.4 GHz to 2.4835 GHz","","2","6","","","","","","IEEE","IEEE Conferences"
"A Complex Applications Framework Supporting Adaptive Routing Strategy for Ad-hoc Networks","I. Benyahia; D. Lapointe","Universite du Quebec, Canada; NA","Advanced Int'l Conference on Telecommunications and Int'l Conference on Internet and Web Applications and Services (AICT-ICIW'06)","","2006","","","74","74","Communication technologies such as mobile and wireless have fostered the development of complex applications. Quality of service (QoS), however, is problematic since such communication technologies are characterized by frequent perturbations and changes in their environment including attenuation distortion, free space loss and refraction resulting in performance degradations such as transmission delay due to the traffic behavior characterized by more frequent packet inter-arrival. To avoid network congestion, it is important to dynamically trigger routing techniques suitable for different environment changes. For this purpose, a software framework characterized by adaptive architecture is used. Our framework design methodology integrates experimental tests including simulation to define the architecture components suitable for different environment behaviors. Simulation experiments are carried out here to evaluate the performance of different routing strategies to be triggered dynamically to optimize the network performance defined by the minimization of transmission delays. A new dynamic routing heuristic is defined in this paper. Our experiments are based on numerous network environment changes such as the traffic behaviour and the topology changes. The generated rules that associate routing strategies with the environment behavior will be triggered to make the network adaptive.","","0-7695-2522","10.1109/AICT-ICIW.2006.5","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1602206","","Routing;Ad hoc networks;Communications technology;Quality of service;Telecommunication traffic;Computer architecture;Attenuation;Space technology;Propagation losses;Performance loss","","","","","10","","","","","","IEEE","IEEE Conferences"
"Real-time reconstruction of the three-dimensional ionosphere using data from a network of GPS receivers","S. V. Fridman; L. J. Nickisch; M. Aiello; M. Hausman","NorthWest Research Associates, Marina, California, USA.; NorthWest Research Associates, Marina, California, USA.; NorthWest Research Associates, Marina, California, USA.; NorthWest Research Associates, Marina, California, USA.","Radio Science","","2006","41","05","1","7","We present a system that processes phase and group delay time series from a network of dual-frequency GPS receivers and produces a dynamic ionospheric model that is consistent with all the input data. The system is intended for monitoring the ionosphere over a fixed geographical area with dimensions of the order of several thousand kilometers. The inversion technique utilized in this system stems from the inversion technique previously developed by our group within the Coordinate Registration Enhancement by Dynamic Optimization (CREDO) project (a software package for inverting the vertical sounding, backscatter sounding, and satellite total electron content (TEC) data for over-the-horizon radar). The core of this technique is Tikhonov's methodology for solving ill-posed problems. We extended the method to multidimensional nonlinear inverse problems and developed techniques for fast numerical solution. The resulting solution for the ionospheric distribution of electron density is guaranteed to be smooth in space and time and to agree with all input data within errors of measurement. The input data consist of time series of absolute TEC and relative TEC (directly calculated from the raw dual-frequency group delays and phase delays, respectively). The system automatically estimates the measurement noise and receiver-transmitter biases. We test the system using archived data from dual-frequency GPS receivers in the southern California Scripps Orbit and Permanent Array Center (SOPAC) network and data from a vertical sounder.","1944-799X","","10.1029/2005RS003341","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7770329","","Receivers;Ionosphere;Global Positioning System;Delays;Mathematical model;Density measurement;Noise measurement","","","","1","","","","","","","AGU","AGU Journals & Magazines"
"Absolute parameters of young stars – I. U Ophiuci","E. Budding; G. İnlek; O. Demircan","NA; NA; NA","Monthly Notices of the Royal Astronomical Society","","2009","393","2","501","510","We have carried out an investigation of the early-type multiple star U Oph. We have used new high-resolution spectroscopy with the High Efficiency and Resolution Canterbury University Large Echelle Spectrograph (HERCULES) and 1-m McLellan Telescope of the University of Canterbury at Mt John University Observatory and literature-sourced optical and ultraviolet photometry. We applied the local reduction package [HERCULES Reduction Software Package (HRSP)] and other software to the spectroscopic data to find radial velocities. Information limit optimization techniques (ILOT) utilizing physically realistic fitting functions were applied to these data to yield new sets of absolute parameters: M<inf>1</inf>= 5.13, M<inf>2</inf>= 4.56 (±2 per cent); R<inf>1</inf>= 3.41, R<inf>2</inf>= 3.08 (±1 per cent); for the early-type eclipsing binary that dominates the system. We have combined times-of-minima photometry with other data for the triple system that makes up ADS 10428A, utilizing the wide orbit of Wolf et al. as well as HIPPARCOS astrometry of U Oph. ILOT techniques applied to the astrometric orbit yield a mass of the third star as 0.83 M<inf>⊙</inf>. We estimate an age of the system of around 30–40 Myr, from the isochrones of Bertelli, results given by Vaz, Andersen & Claret, as well as our own tests with an updated version of Paczyński's stellar modelling code. This age and other details are consistent with a possible origin in Gould's Belt. Such information for this, and comparable young multiple star systems, may help to clarify general properties of star formation and the subtle interactions of stars and their environment.","0035-8711;1365-2966","","10.1111/j.1365-2966.2008.14112.x","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8220951","methods: data analysis;techniques: spectroscopic;binaries: close;stars: early-type;stars: individual: U Oph;Galaxy: stellar content","","","","","","","","","","","","OUP","OUP Journals & Magazines"
"Highland Valley Copper Control Systems Upgrade - Technology, Design, Simulation, and Installation","K. S. Borthwick; J. R. Hargreaves; G. L. Yorke","NA; NA; NA","2008 IEEE Industry Applications Society Annual Meeting","","2008","","","1","8","This paper describes a facility wide distributed control system (DCS) upgrade project at a large copper and molybdenum mineral processing operation. The innovative hardware and software technologies employed in the new control system are discussed. The planning process, migration path and simulation techniques used to ensure the success of such a large and complex DCS upgrade are described. A novel mathematical model and simulator of the primary mill is described. A novel advanced control and optimization algorithm for the primary mill is described. Performance testing of the new advanced control and optimization algorithms using the new primary mill model and simulator is described.","0197-2618","978-1-4244-2278-4978-1-4244-2279","10.1109/08IAS.2008.347","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4659135","","Copper;Control system synthesis;Distributed control;Milling machines;Minerals;Hardware;Control systems;Path planning;Process planning;Mathematical model","computerised control;copper;distributed control;mineral processing industry;molybdenum","distributed control system;mineral processing;migration path;mathematical model;primary mill","","","5","","","","","","IEEE","IEEE Conferences"
"On a geometric approach to the structural identifiability problem and its application in a water quality case study","J. D. Stigter; R. L. M. Peeters","Systems and Control Group, Wageningen University, P.O.Box 17, 6700 AA, The Netherlands; Department of Mathematics, Maastricht University, P.O.Box 616, 6200 MD, The Netherlands","2007 European Control Conference (ECC)","","2007","","","3450","3456","We present and apply an alternative method for the investigation of the well-known parameter identifiability question for non-linear system models. The method is based on a geometric analysis of the parametric output sensitivities and is, in fact, an application of the tools that are available in non-linear control theory to an augmented system, including the parametric output sensitivities. Accessibility Lie algebras are calculated that yield insight (through a simple rank test) in the controllability of this augmented system. The method is demonstrated in an example that is due to Dochain et al [4]. Results are confirmed by the method that has certain advantages in comparison to, for example, the Taylor series approach that seeks for identifiable combinations of parameters through inspection of the individual terms in a Taylor series expansion of the output signal, i.e. application of the well-known method of Pohjanpalo [15]. Parametric output sensitivities (as already noted by Dötsch and Van den Hof [5] and Peeters and Hanzon [13]) play a crucial role in identifiability analysis and we further elaborate on this insight in the current paper. Our goals are (i) to present an interesting method for addressing the (local) identifiability question for non-linear systems and (ii) to gain better understanding in the role of parametric state- and output sensitivities in the identifiability question that stems from an alternative perspective, and that has not been presented in the identification literature. Of course, we are aware of other algorithms and software that establishes an answer to the identifiability question, albeit from a different perspective, e.g. [19], but seek in the current paper mainly for another interpretation and computational framework to address the question of local identifiability, shedding some new light on the problem.","","978-3-9524173-8","10.23919/ECC.2007.7068560","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7068560","","Sensitivity;Mathematical model;Vectors;Equations;Substrates;Taylor series","controllability;geometry;identification;Lie algebras;nonlinear control systems;series (mathematics);water quality","structural identifiability problem;water quality;parameter identifiability;nonlinear system models;geometric analysis approach;parametric output sensitivities;nonlinear control theory;augmented system;accessibility Lie algebras;controllability;Taylor series approach;Taylor series expansion;identifiability analysis;parametric state-sensitivities;computational framework","","","25","","","","","","IEEE","IEEE Conferences"
"Rapid development of an FPGA controller for a Wind / Photovoltaic power system","A. Parera-Ruiz; M. N. Cirstea; D. N. Ilea","Anglia Ruskin University, Cambridge, UK; Anglia Ruskin University, Cambridge, UK; Transilvania University of Brasov, Romania","2008 IEEE International Symposium on Industrial Electronics","","2008","","","2003","2008","The efficient control of distributed energy resources (DERs) is the key for the optimization of renewable energy supplies used in stand-alone micro-generators. A wind / photovoltaic combined power generator, with the possibility to incorporate a hydrogen fuel cell as energy backup or connect the system to the grid, in order to achieve reliable, self-sufficient, continuous power to variable sources and loads, is the structure tackled in this research project and the control of the integrated Wind/PV system is the scope of this paper. The research work takes advantage of the new generation of computer aided design (CAD) tools, able to generate a fast hardware implementation from synthesizable high level modelling code. The proposed system uses DK5, from agility design solutions, design and modelling software environment based on Handel-C programming language. The research led to the rapid development, prototyping and testing of an FPGA based controller for the integrated renewable power generation system.","2163-5137;2163-5145","978-1-4244-1665-3978-1-4244-1666","10.1109/ISIE.2008.4677193","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4677193","","Field programmable gate arrays;Control systems;Photovoltaic systems;Wind energy generation;Wind power generation;Power system reliability;Power system modeling;Distributed control;Energy resources;Renewable energy resources","field programmable gate arrays;fuel cell power plants;photovoltaic power systems;wind power plants","FPGA controller;wind power system;photovoltaic power system;distributed energy resources;renewable energy supplies;stand-alone micro-generators;hydrogen fuel cell;computer aided design tools;agility design solutions;Handel-C programming language","","4","14","","","","","","IEEE","IEEE Conferences"
"A Wireless Medical Monitoring Over a Heterogeneous Sensor Network","M. R. Yuce; P. C. Ng; C. K. Lee; J. Y. Khan; W. Liu","School of Electrical Engineering and Computer Science, University of Newcastle, Callaghan, NSW 2308, Australia. e-mail: mehmet.yuce@newcastle.edu.au; School of Electrical Engineering and Computer Science, University of Newcastle, Callaghan, NSW 2308, Australia; School of Electrical Engineering and Computer Science, University of Newcastle, Callaghan, NSW 2308, Australia; School of Electrical Engineering and Computer Science, University of Newcastle, Callaghan, NSW 2308, Australia; Electrical Engineering Department, University of California, Santa Cruz, CA 95064 USA","2007 29th Annual International Conference of the IEEE Engineering in Medicine and Biology Society","","2007","","","5894","5898","This paper presents a heterogeneous sensor network system that has the capability to monitor physiological parameters from multiple patient bodies by means of different communication standards. The system uses the recently opened medical band called MICS (medical implant communication service) between the sensor nodes and a remote central control unit (CCU) that behaves as a base station. The CCU communicates with another network standard (the Internet or a mobile network) for a long distance data transfer. The proposed system offers mobility to patients and flexibility to medical staff to obtain patient's physiological data on demand basis via Internet. A prototype sensor network including hardware, firmware and software designs has been implemented and tested by incorporating temperature and pulse rate sensors on nodes. The developed system has been optimized for power consumption by having the nodes sleep when there is no communication via a bidirectional communication.","1094-687X;1558-4615","978-1-4244-0787-3978-1-4244-0788","10.1109/IEMBS.2007.4353689","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4353689","","Wireless sensor networks;Biomedical monitoring;Temperature sensors;Patient monitoring;Sensor systems;Communication standards;Condition monitoring;Remote monitoring;Microwave integrated circuits;Implants","biomedical equipment;biomedical measurement;biothermics;data communication;Internet;mobile communication;patient monitoring;prosthetics;telemedicine;temperature sensors;wireless sensor networks","wireless medical monitoring;heterogeneous sensor network;communication standards;medical implant communication service;sensor nodes;remote central control unit;Internet;mobile network;long distance data transfer;temperature sensors;pulse rate sensors;power consumption;bidirectional communication;wireless body sensor network","Computer Communication Networks;Diagnosis, Computer-Assisted;Diagnosis, Computer-Assisted;Equipment Design;Equipment Failure Analysis;Monitoring, Ambulatory;Monitoring, Ambulatory;Signal Processing, Computer-Assisted;Systems Integration;Telemedicine;Telemedicine;Telemetry;Telemetry;Transducers;User-Computer Interface","11","4","","","","","","IEEE","IEEE Conferences"
"Expert System for Power Quality Disturbance Classifier","M. B. I. Reaz; F. Choong; M. S. Sulaiman; F. Mohd-Yasin; M. Kamada","NA; NA; NA; NA; NA","IEEE Transactions on Power Delivery","","2007","22","3","1979","1988","Identification and classification of voltage and current disturbances in power systems are important tasks in the monitoring and protection of power system. Most power quality disturbances are non-stationary and transitory and the detection and classification have proved to be very demanding. The concept of discrete wavelet transform for feature extraction of power disturbance signal combined with artificial neural network and fuzzy logic incorporated as a powerful tool for detecting and classifying power quality problems. This paper employes a different type of univariate randomly optimized neural network combined with discrete wavelet transform and fuzzy logic to have a better power quality disturbance classification accuracy. The disturbances of interest include sag, swell, transient, fluctuation, and interruption. The system is modeled using VHSIC hardware description language (VHDL), a hardware description language, followed by extensive testing and simulation to verify the functionality of the system that allows efficient hardware implementation of the same. This proposed method classifies, and achieves 98.19% classification accuracy for the application of this system on software-generated signals and utility sampled disturbance events.","0885-8977;1937-4208","","10.1109/TPWRD.2007.899774","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4265719","Artificial neural network;classification;feature extraction;fuzzy logic;power quality;VHSIC hardware description language (VHDL);wavelet transform","Expert systems;Power quality;Discrete wavelet transforms;Power system protection;Artificial neural networks;Fuzzy logic;Hardware design languages;Voltage;Monitoring;Feature extraction","expert systems;feature extraction;fuzzy logic;hardware description languages;neural nets;pattern classification;power engineering computing;power supply quality;power system measurement;power system protection;wavelet transforms","expert system;power quality disturbance classifier;power system monitoring;power system protection;discrete wavelet transform;feature extraction;power disturbance signal;artificial neural network;fuzzy logic;VHSIC hardware description language","","86","27","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic and Precise Orthorectification, Coregistration, and Subpixel Correlation of Satellite Images, Application to Ground Deformation Measurements","S. Leprince; S. Barbot; F. Ayoub; J. Avouac","NA; NA; NA; NA","IEEE Transactions on Geoscience and Remote Sensing","","2007","45","6","1529","1558","We describe a procedure to accurately measure ground deformations from optical satellite images. Precise orthorectification is obtained owing to an optimized model of the imaging system, where look directions are linearly corrected to compensate for attitude drifts, and sensor orientation uncertainties are accounted for. We introduce a new computation of the inverse projection matrices for which a rigorous resampling is proposed. The irregular resampling problem is explicitly addressed to avoid introducing aliasing in the ortho-rectified images. Image registration and correlation is achieved with a new iterative unbiased processor that estimates the phase plane in the Fourier domain for subpixel shift detection. Without using supplementary data, raw images are wrapped onto the digital elevation model and coregistered with a 1/50 pixel accuracy. The procedure applies to images from any pushbroom imaging system. We analyze its performance using Satellite pour l'Observation de la Terre (SPOT) images in the case of a null test (no coseismic deformation) and in the case of large coseismic deformations due to the Mw 7.1 Hector Mine, California, earthquake of 1999. The proposed technique would also allow precise coregistration of images for the measurement of surface displacements due to ice-flow or geomorphic processes, or for any other change detection applications. A complete software package, the Coregistration of Optically Sensed Images and Correlation, is available for download from the Caltech Tectonics Observatory website","0196-2892;1558-0644","","10.1109/TGRS.2006.888937","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4215064","Change detection;coseismic displacements;geocoding;image registration;image resampling;optical imagery;orthorectification;Satellite pour l'Observation de la Terre (SPOT);satellites;subpixel correlation","Satellites;Optical sensors;Optical imaging;Image sensors;Sensor systems;Image registration;Phase estimation;Phase detection;Digital elevation models;Pixel","artificial satellites;Fourier analysis;geomorphology;geophysical signal processing;geophysical techniques;image processing;inverse problems;matrix algebra","satellite image orthorectification;satellite image coregistration;satellite image subpixel correlation;ground deformation measurements;optical satellite images;inverse projection matrices;irregular resampling problem;image registration;subpixel shift detection;digital elevation model;pushbroom imaging system;Satellite pour l'Observation de la Terre;SPOT images;coseismic deformations;Hector Mine;California;earthquake;AD 1999;surface displacement measurement;ice flow;geomorphic processes;Coregistration of Optically Sensed Images and Correlation;Caltech Tectonics Observatory website","","319","53","","","","","","IEEE","IEEE Journals & Magazines"
"A mobile-phone based teledermatology system to support self-management of patients suffering from psoriasis","G. Schreier; D. Hayn; P. Kastner; S. Koller; W. Salmhofer; R. Hofmann-Wellenhof","Austrian Research Centers GmbH ¿ ARC / eHealth systems, Austria; Austrian Research Centers GmbH ¿ ARC / eHealth systems, Austria; Austrian Research Centers GmbH ¿ ARC / eHealth systems, Austria; Department of Dermatology and Venereology of the Medical, University Graz, Austria; Department of Dermatology and Venereology of the Medical, University Graz, Austria; Department of Dermatology and Venereology of the Medical, University Graz, Austria","2008 30th Annual International Conference of the IEEE Engineering in Medicine and Biology Society","","2008","","","5338","5341","Psoriasis is one of the most common chronic skin diseases, affecting about 2% of the population world wide. Continuous clinical monitoring with periodic assessment of the state of the disease is essential for long-term therapy optimization. We present a mobile phone based telemedical compliance management system for psoriasis-patients. Using special software, patients can acquire health parameters such as their body weight, take photos of their psoriasis lesions, and report adverse effects. The data are automatically sent to a monitoring centre, where they are provided to the patient's physician via the use of a web-browser. In case of therapy relevant events, email or SMS notifications can be sent to the physician. If necessary, the physician can send feedback messages to the patient, e.g. for admitting the patient to the clinic for further examinations. The system has been implemented and functional tests have proven its functionality. Currently, the system is used in the course of a medical case series.","1094-687X;1558-4615","978-1-4244-1814-5978-1-4244-1815","10.1109/IEMBS.2008.4650420","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4650420","","","","","Cellular Phone;Dermatology;Dermatology;Dermoscopy;Dermoscopy;Diagnosis, Computer-Assisted;Humans;Psoriasis;Psoriasis;Self Care;Self Care;Telemedicine;Telemedicine;Therapy, Computer-Assisted","7","14","","","","","","IEEE","IEEE Conferences"
"Mobile and wide area deployable sensor system for networked services","Z. Pang; J. Chen; D. S. Mendoza; Z. Zhang; J. Gao; Q. Chen; L. Zheng","iPack Vinn Excellence Center, School of Information and Communication Technology, Royal Institute of Technology (KTH), Forum 120, 164 40 Kista-Stockholm, Sweden; iPack Vinn Excellence Center, School of Information and Communication Technology, Royal Institute of Technology (KTH), Forum 120, 164 40 Kista-Stockholm, Sweden; iPack Vinn Excellence Center, School of Information and Communication Technology, Royal Institute of Technology (KTH), Forum 120, 164 40 Kista-Stockholm, Sweden; iPack Vinn Excellence Center, School of Information and Communication Technology, Royal Institute of Technology (KTH), Forum 120, 164 40 Kista-Stockholm, Sweden; iPack Vinn Excellence Center, School of Information and Communication Technology, Royal Institute of Technology (KTH), Forum 120, 164 40 Kista-Stockholm, Sweden; iPack Vinn Excellence Center, School of Information and Communication Technology, Royal Institute of Technology (KTH), Forum 120, 164 40 Kista-Stockholm, Sweden; iPack Vinn Excellence Center, School of Information and Communication Technology, Royal Institute of Technology (KTH), Forum 120, 164 40 Kista-Stockholm, Sweden","SENSORS, 2009 IEEE","","2009","","","1396","1399","A mobile and wide area deployable wireless sensor system, including hardware and software, is developed to enhance the mobility, deployment and capability of wireless sensors for networked services. Due to the dual-layer dual-directional wireless communication capability of a novel WAN-SAN coherent architecture and the removal of fix-installed gateway, all sensor nodes are remotely controllable and seamlessly integrated to internet services. Hardware modules are optimized for ultra low power and compact size. Abstract and extendable application interface is developed, based on SMS and TCP/UDP protocol, to be integrated easily into existing service systems. Hierarchical GPS-LPS Adaptive Localization, is supported based on GPS, wireless cellular ID, RFID, IR-UWB and inertial prediction. Specific data compression technique is adopted for high density data source. Finally, Fresh Food Tracking service is presented as an application example, including some field test data.","1930-0395;1930-0395","978-1-4244-4548-6978-1-4244-5335","10.1109/ICSENS.2009.5398428","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5398428","","Sensor systems;Wireless sensor networks;Hardware;Wireless communication;Computer architecture;Communication system control;Web and internet services;Protocols;Adaptive systems;Global Positioning System","food products;Internet;materials handling;mobile computing;transport protocols;Web services;wireless sensor networks","mobile sensor system;wide area deployable sensor system;networked service;dual layer dual directional wireless communication;WAN-SAN;Internet service;SMS;TCP/UDP protocol;hierarchical GPS-LPS adaptive localization;wireless cellular ID;RFID;IR-UWB;inertial prediction;data compression technique;fresh food tracking service","","9","9","","","","","","IEEE","IEEE Conferences"
"Laboratory prototype of cochlear implant: Design and techniques","H. Ali; T. J. Ahmad; A. Ajaz; S. A. Khan","Center for Advanced Research in Engineering, 19 Ataturk Avenue, Islamabad, Pakistan; Center for Advanced Research in Engineering, Pakistan; College of Electrical and Mechanical Engineering, National University of Sciences and Technology, Pakistan; College of Electrical and Mechanical Engineering, National University of Sciences and Technology, Pakistan","2009 Annual International Conference of the IEEE Engineering in Medicine and Biology Society","","2009","","","803","806","This paper presents design overview of a low cost prototype of Cochlear Implant developed from commercial off-the-shelf components. Design scope includes speech processing module implemented on a commercial digital signal processor, transcutaneous data and power transceiver developed from a single pair of inductive coils and finally a stimulator circuitry for cochlear stimulation. Different speech processing strategies such as CIS, SMSP and F0/F1 have been implemented and tested using a novel, indigenously developed speech processing research module which evaluates the performance of speech processing strategies in software, hardware and practical scenarios. Design overview, simulations and practical results of an optimized inductive link using Class E Power Amplifier are presented. Link was designed at a carrier frequency of 2.5 MHz for 100 mW output power. Receiver logic design and stimulator circuitry was implemented using a PIC microcontroller and off-the-shelf electronic components. Results indicate 40% link efficiency with 128 kbps data transfer rate. This low cost prototype can be used for undertaking cochlear implant research in laboratories.","1094-687X;1558-4615","978-1-4244-3296","10.1109/IEMBS.2009.5333707","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5333707","","Laboratories;Prototypes;Cochlear implants;Speech processing;Costs;Circuits;Power amplifiers;Signal design;Digital signal processors;Transceivers","cochlear implants;coils;digital signal processing chips;inductive power transmission;inductors;microcontrollers;power supplies to apparatus;speech processing;transceivers","cochlear implant laboratory prototype;low cost cochlear implant;cochlear implant design;speech processing module;digital signal processor;transcutaneous data transceiver;power transceiver;inductive coil pair;stimulator circuitry;cochlear stimulation;CIS speech processing strategy;SMSP speech processing strategy;F0-F1 speech processing strategy;Class E power amplifier;receiver logic design;PIC microcontroller;frequency 2.5 MHz;power 100 mW","Cochlear Implants;Equipment Design;Equipment Failure Analysis;Pilot Projects;Signal Processing, Computer-Assisted;Sound Spectrography;Speech Production Measurement","1","20","","","","","","IEEE","IEEE Conferences"
"Welcome to ICCD 2006!","","","2006 International Conference on Computer Design","","2006","","","xii","xiii","The following topics are dealt with: computer design; microarchitecture optimization; timing analysis; IC interconnections; nanotechnology-aware design; multiprocessors and system-on-chip design; robust low-power design; hardware and software scheduling techniques; nanoscale modeling and architectural synthesis; circuit testing; functional verification; application specific processing elements; power-efficient system.","1063-6404","978-0-7803-9706-4978-0-7803-9707","10.1109/ICCD.2006.4380779","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4380779","","","computer architecture;integrated circuit interconnections;logic CAD;logic testing;low-power electronics;microprocessor chips;multiprocessing systems;nanoelectronics;scheduling;synchronisation;system-on-chip","computer design;microarchitecture optimization;timing analysis;IC interconnection;nanotechnology-aware design;multiprocessor design;system-on-chip design;robust low-power design;hardware scheduling;software scheduling;nanoscale modeling;circuit testing;architectural synthesis;functional verification;application specific processing element;power-efficient system","","","","","","","","","IEEE","IEEE Conferences"
"[Cover]","","","Proceedings of the Design Automation & Test in Europe Conference","","2006","1","","1","1","The following topics are dealt with: allocation and scheduling for MPSoCs and NoCs; power grid analysis; large interconnect network analysis; online testing; fault tolerance; model based design and test; transaction level modeling based validation; application specific network-on-chip design; systematic analogue design automation; soft error analysis; concurrent testing; processor and memory design; reconfigurable computing; design for manufacturability; design for yield; analogue and mixed-signal design; processor self-test; fault diagnosis; system level modelling and simulation; power-efficient hardware/software architectures; timing and noise analysis; automotive systems; NoC architectures; low power embedded architectures and platforms; transistor and gate level simulation; system optimisation with embedded software; communication-centric synthesis for MPSoC; clocks and routing; nanotechnology circuits reliability; thermal aspects of low power design; dynamic power aware logic design; defect modeling and detection; data layout optimizations; wireless sensor networks; test data compression; resource constrained scheduling; sequential optimisation, clocking and Boolean matching; semi-formal validation methods; memory, FPGA. and networks-on-chip testing; architectural level synthesis; system level verification; reliable microarchitectures; logic and arithmetic circuit optimisation","1530-1591;1558-1101","3-9810801-1","10.1109/DATE.2006.243941","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1656827","","","automotive electronics;circuit optimisation;design for manufacture;electronic design automation;fault simulation;field programmable gate arrays;formal verification;hardware-software codesign;integrated circuit reliability;integrated circuit testing;integrated memory circuits;logic design;low-power electronics;network-on-chip;reconfigurable architectures;wireless sensor networks","multiprocessor system-on-chip;power grid analysis;interconnect network analysis;online testing;fault tolerance;model based design;transaction level modeling;application specific network-on-chip design;analogue design automation;soft error analysis;concurrent testing;processor design;memory design;reconfigurable computing;design for manufacturability;design for yield;mixed-signal design;processor self-test;fault diagnosis;hardware/software architectures;timing analysis;noise analysis;automotive systems;NoC architectures;low power embedded architectures;low power design;embedded software;nanotechnology circuits reliability;dynamic power aware logic design;defect modeling;data layout optimizations;resource constrained scheduling;sequential optimisation;Boolean matching;semi-formal validation methods;wireless sensor networks;test data compression;FPGA testing;architectural level synthesis;system level verification;arithmetic circuit optimisation;logic circuit optimisation;memory testing","","","","","","","","","IEEE","IEEE Conferences"
"[Title page]","","","2009 Design, Automation & Test in Europe Conference & Exhibition","","2009","","","ii","ii","The following topics were dealt with: interconnection technology; multicore system-on-chip; reconfigurable hardware; MPSoC; task allocation; reliability; scheduling; timing analysis; embedded real-time systems; system-level synthesis; optimization; high-level modeling; verification; system-level test; system-level debug; model-based design; hardware-software system integration; network on chip; analogue layout synthesis; aerospace systems; MEMS; mixed-signal applications;variability test; circuit marginality; flash memory management; space exploration; power optimization techniques; open source hardware IP; nanoelectronics; embedded systems security; on-line testing; fault tolerance;vertical integration; disaggregation; CNTFET; cryptographic functions; runtime checking; contactless testing; multiprocessor real-time systems; analogue synthesis; nonvolatile memory technology; ESL methodology; on-chip communication; on-line error detection; software support; sizing; placement; planning; packaging; automotive systems; programmable SoCs; advanced low-power memory; thermal management; multicore platforms; debugging; diagnosis; health-care electronics; industrial system designs; multimedia; NoC performance optimization; automotive networks,; sensors; architectural synthesis; test pattern generation; interconnect models; industrial system design flow; mass market applications; multicore products; computation models; forward error correction; signal processing; design-for-test; memory-aware compiler techniques; mixed-signal technology design; mixed technology design; high-level power management; media processing; logic synthesis; decomposition techniques; restructuring techniques; test data compression; automation; model generation; model implementation; field programmable architectures; digital design; advanced SAT techniques; baseband processors; MIMO communication systems; UWB communication systems; system level simulation; system level validation; RF testing; DFX engineering; multicycle design.","1530-1591;1558-1101","978-1-4244-3781-8978-3-9810-8015","10.1109/DATE.2009.5090611","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5090611","","","aerospace engineering;analogue integrated circuits;automation;automotive electronics;cryptography;data compression;design for testability;embedded systems;error correction;error detection;fault tolerance;flash memories;formal verification;hardware-software codesign;health care;integrated circuit interconnections;logic devices;low-power electronics;micromechanical devices;MIMO communication;mixed analogue-digital integrated circuits;multimedia systems;multiprocessing systems;nanoelectronics;network-on-chip;optimisation;packaging;program debugging;programmable circuits;random-access storage;reconfigurable architectures;reliability;scheduling;sensors;signal processing;simulation;system-on-chip;task analysis;thermal management (packaging);thin film devices;timing;ultra wideband communication","interconnection technology;multicore system-on-chip;reconfigurable hardware;MPSoC;task allocation;reliability;scheduling;timing analysis;embedded real-time systems;system-level synthesis;high-level modeling;verification;system-level test;system-level debug;model-based design;hardware-software system integration;network on chip;analogue layout synthesis;aerospace systems;MEMS;mixed-signal applications;variability test;circuit marginality;flash memory management;space exploration;power optimization techniques;open source hardware IP;nanoelectronics;embedded systems security;on-line testing;fault tolerance;vertical integration;disaggregation;CNTFET;cryptographic functions;runtime checking;contactless testing;multiprocessor real-time systems;nonvolatile memory technology;ESL methodology;on-chip communication;on-line error detection;software support;sizing;placement;planning;packaging;automotive systems;programmable SoCs;advanced low-power memory;thermal management;multicore platforms;debugging;diagnosis;health-care electronics;industrial system designs;multimedia;NoC performance optimization;automotive networks;sensors;architectural synthesis;test pattern generation;interconnect models;industrial system design flow;mass market applications;multicore products;computation models;forward error correction;signal processing;design-for-test;memory-aware compiler techniques;mixed-signal technology design;mixed technology design;high-level power management;media processing;logic synthesis;decomposition techniques;restructuring techniques;test data compression;automation;model generation;model implementation;field programmable architectures;digital design;advanced SAT techniques;baseband processors;MIMO communication systems;UWB communication systems;system level simulation;system level validation;RF testing;DFX engineering;multicycle design","","","","","","","","","IEEE","IEEE Conferences"
"Perspective on Embedded Systems: Challenges, Solutions and Research Priorities","D. Vernay","CTO, THALES, Paris, France","2008 Design, Automation and Test in Europe","","2008","","","2","2","This paper introduces THALES vision and research priorities for embedded systems and illustrates them through presentations of solutions and on-going research projects and initiatives. Thales effort related to mission-critical systems is focused on advanced high-performance embedded computing platforms, on middleware technologies, on software systems design and verification tools for safety and security and on the emergence of open standards in these domains. THALES is also actively contributing to the development of innovation eco-systems: the Joint Undertaking ARTEMIS in Europe; the Pole de Competitivite SYSTEM@TIC PARIS REGION in France.","1530-1591;1558-1101","978-3-9810801-3-1978-3-9810801-4","10.1109/DATE.2008.4484650","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4484650","","Embedded system;Mission critical systems;Embedded computing;Middleware;Software systems;Software design;Software safety;Security;Software standards;Technological innovation","embedded systems;middleware;program verification;software tools","THALES vision;mission-critical systems;advanced high-performance embedded computing platforms;middleware technologies;software systems design;verification tools","","","","","","","","","IEEE","IEEE Conferences"
"Expanding, theory, and practice: Report on the 4<sup>th</sup>International Workshop on the Automation of Software Test","D. Dranidis; S. P. Masticola; P. Strooper","CITY College, 13 Tsimiski St, 54624 Thessaloniki, Greece; Siemens Corporate Research, 755 College Road East, Princeton, NJ 08540, USA; The University of Queensland, School of ITEE, St Lucia, Qld. 4072 Australia","2009 31st International Conference on Software Engineering - Companion Volume","","2009","","","459","460","The Fourth International Workshop on Automation of Software Test (AST 2009) at the 31st International Conference on Software Engineering (ICSE 2009) expands to two days, supports a special theme of Testing Web Services, adds a Case Studies from Business and Industry session, and includes a charette-style work session. 14 regular papers and 7 short case-study papers will be presented. This report summarizes the organization of the workshop as well as the sessions and papers to be presented.","","978-1-4244-3495","10.1109/ICSE-COMPANION.2009.5071061","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5071061","","Automatic testing;Software testing;Web services;Automation;Educational institutions;Unified modeling language;Search engines;Optimizing compilers;Data security;Software standards","","","","","","","","","","","IEEE","IEEE Conferences"
"[Front cover]","","","2009 Seventh Workshop on Intelligent solutions in Embedded Systems","","2009","","","3","4","The following topics are dealt with: workshop optimization; fault tolerance; non-uniform sensor networks; prioritized medium access control protocol; Poseidon MK6 rebreather; embedded data logging platform; smart transducer interface; IEEE 1451 standard; intelligent deflection router; network-on-chip performance evaluation; reusable coverage-driven verification environment; multiprocessor architecture; electronic musical instrument; scheduling dependent tasks; multicore architecture; distributed hardware algorithm; automotive software architecture; in-vehicle testing; temporal coding schemes; energy efficient data transmission; systems-on-chip; car-gateway middleware; OSGi; Bayesian networks; intelligent adaptable printing systems; distributed management platform; Kalman filtering; real-time operating system; digital controlled energy scavenger power converter; computational complexity estimate; DSR front-end; power estimation; SystemC-based design context and PKtool environment.","","978-1-4244-4838-8978-88-87548-02","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5186373","","","access protocols;belief networks;computational complexity;data loggers;electronic music;fault tolerance;Kalman filters;microprocessor chips;middleware;musical instruments;network-on-chip;operating systems (computers);optimisation;parallel architectures;performance evaluation;software architecture;software reusability;telecommunication network routing;wireless sensor networks","workshop optimization;fault tolerance;non-uniform sensor networks;prioritized medium access control protocol;Poseidon MK6 rebreather;embedded data logging platform;smart transducer interface;IEEE 1451 standard;intelligent deflection router;network-on-chip performance evaluation;reusable coverage-driven verification environment;multiprocessor architecture;electronic musical instrument;scheduling dependent tasks;multicore architecture;distributed hardware algorithm;automotive software architecture;in-vehicle testing;temporal coding schemes;energy efficient data transmission;systems-on-chip;car-gateway middleware;OSGi;Bayesian networks;intelligent adaptable printing systems;distributed management platform;Kalman filtering;real-time operating system;digital controlled energy scavenger power converter;computational complexity estimate;DSR front-end;SystemC-based design context;PKtool environment;power estimation","","","","","","","","","IEEE","IEEE Conferences"
"12th Pacific Rim International Symposium on Dependable Computing - Title","","","2006 12th Pacific Rim International Symposium on Dependable Computing (PRDC'06)","","2006","","","i","iii","The following topics are dealt with: reliability modeling; fault detection; software testing; security; intrusion detection; dependability applications; reliability prediction and optimization","","0-7695-2724","10.1109/PRDC.2006.4","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4041877","","","distributed processing;program testing;security of data;software reliability","reliability modeling;fault detection;software testing;security;intrusion detection;reliability prediction;optimization;dependability applications","","","","","","","","","IEEE","IEEE Conferences"
"Annual Reliability and Maintainability Symposium","","","RAMS '06. Annual Reliability and Maintainability Symposium, 2006.","","2006","","","i","i","The following topics are dealt with: reliability; risk management; statistical analysis; software module testing; failure analysis; maintenance management; optimisation; Bayesian networks; product design","0149-144X","1-4244-0007-41-4244-0008","10.1109/RAMS.2006.1677320","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1677320","","","belief networks;failure analysis;inspection;maintenance engineering;optimisation;reliability theory;risk analysis;software reliability;statistical analysis","reliability;risk management;statistical analysis;software module testing;failure analysis;maintenance management;optimisation;Bayesian networks;product design","","","","","","","","","IEEE","IEEE Conferences"
"IEEE International symposium on performance analysis of systems and software","","","2009 IEEE International Symposium on Performance Analysis of Systems and Software","","2009","","","i","i","The following topics are dealt with: differentiating the roles of IR measurement and simulation for power and temperature-aware design; user- and process-driven dynamic voltage and frequency scaling; accuracy of performance counter measurements; GARNET: a detailed on-chip network model inside a full-system simulator; Cetra: a trace and analysis framework for the evaluation of Cell BE; Zesto: a cycle-level simulator for highly detailed microarchitecture exploration; Lonestar: a suite of parallel irregular program; exploring speculative parallelism in SPEC2006; machine learning based online performance prediction for runtime parallelization and task scheduling; WARP: enabling fast CPU scheduler development and evaluation; CMPSchedSim: evaluating OS/CMP interaction on shared cache management; understanding the cost of thread migration for multi-threaded Java applications running on a multicore platform; the data-centricity of Web 2.0 workloads and its impact on server performance; characterizing and optimizing the memory footprint of De Novo Short Read DNA sequence assembly; analytic model of optimistic software transactional memory; analyzing CUDA workloads using a detailed GPU simulator; evaluating GPUs for network packet signature matching; online compression of cache-filtered address traces; analysis of the TRIPS prototype block predictor; experiment flows and microbenchmarks for reverse engineering of branch predictor structures; analyzing the impact of on-chip network traffic on program phases for CMPs; SuiteSpecks and SuiteSpots: a methodology for the automatic conversion of benchmarking programs into intrinsically checkpointed assembly code; accurately approximating superscalar processor performance from traces; and QUICK: a flexible full-system functional model.","","978-1-4244-4184","10.1109/ISPASS.2009.4919624","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4919624","","","benchmark testing;coprocessors;electric potential;Internet;Java;learning (artificial intelligence);multiprocessing systems;multi-threading;performance evaluation;reverse engineering;scheduling;simulation;storage management","power-aware design;temperature-aware design;process-driven dynamic voltage;frequency scaling;performance counter measurements;on-chip network model;full-system simulator;GARNET;Cetra;trace and analysis framework;Cell BE;Zesto;cycle-level simulator;microarchitecture exploration;Lonestar;machine learning;online performance prediction;runtime parallelization;task scheduling;WARP;fast CPU scheduler development;CMPSchedSim;shared cache management;multi-threaded Java application;multicore platform;data centricity;Web 2.0;server performance;memory footprint;De Novo Short Read DNA sequence assembly;optimistic software transactional memory;GPU simulator;network packet signature matching;online compression;cache-filtered address traces;TRIPS prototype block predictor;reverse engineering;branch predictor structures;on-chip network traffic;SuiteSpecks;SuiteSpots;benchmarking programs;assembly code;superscalar processor performance;flexible full-system functional model","","","","","","","","","IEEE","IEEE Conferences"
"[Front cover]","","","2009 IEEE AUTOTESTCON","","2009","","","c1","c1","The following topics are dealt with: net-centric solutions; test program set design &amp; rehost; ATE instrumentation; ATML applications; software applications; prognostics &amp; health monitoring; cost-effective solutions; digital applications; hardware/software solutions; design for testability; IEEE 1641; DoD ATS framework; diagnostics method; synthetic instruments and test asset optimization.","1088-7725;1558-4550","978-1-4244-4980-4978-1-4244-4981","10.1109/AUTEST.2009.5314068","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5314068","","","automatic test pattern generation;computerised instrumentation;condition monitoring;design for testability;fault diagnosis;IEEE standards;military systems","net-centric solution;test program set design;ATML applications;health monitoring;hardware-software solution;design-for-testability;IEEE 1641;DoD ATS framework;synthetic instruments;test asset optimization","","","","","","","","","IEEE","IEEE Conferences"
"Contents","","","Proceedings of IEEE East-West Design & Test Symposium (EWDTS'08)","","2008","","","1","2","The following topics are dealt with: optimized constraint logic programming-based propagation sequence generation; CMOS sequential standard cells; microprocessor coverage-directed verification; statecharts FPGA implementation; low-power multidimensional loop fusion; timing-driven placement algorithm; digital PLL lock detector; SoC memory faulty cells; vector-logical diagnosis method; system level hardware design; NoC architecture reliability; in-circuit testing; floor planning; routing techniques; and Petri Nets.","","978-1-4244-3402-2978-1-4244-3403","10.1109/EWDTS.2008.5580163","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5580163","","Testing;Software;System-on-a-chip;Analytical models;Hardware;Predictive models;Process control","CMOS logic circuits;constraint handling;digital phase locked loops;digital storage;fault diagnosis;field programmable gate arrays;integrated circuit layout;integrated circuit reliability;integrated circuit testing;low-power electronics;microprocessor chips;network routing;network-on-chip;Petri nets;sequential circuits;timing","constraint logic programming;sequence generation;CMOS sequential standard cells;microprocessor coverage-directed verification;FPGA implementation;low-power multidimensional loop fusion;timing-driven placement algorithm;digital PLL lock detector;SoC memory faulty cells;vector-logical diagnosis;system level hardware design;NoC architecture reliability;in-circuit testing;floor planning;routing techniques;Petri Nets","","","","","","","","","IEEE","IEEE Conferences"
"MEMSTECH 2009","","","2009 5th International Conference on Perspective Technologies and Methods in MEMS Design","","2009","","","1","4","This paper dealt with the following topics: analysis, modelling, and design methods of microsensors, microactuators, and nanotechnology, and/or sensor and actuator system; software system, models, algorithms, methods, testing, verification, reliability, optimisation, and strategies of embedded system; application of informatics to engineering education; and, embedded system applications for electron device design.","","978-1-4244-4516-5978-966-2191-06","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5069687","","Algorithm design and analysis;Embedded system;Application software;Design methodology;Microsensors;Microactuators;Nanotechnology;Sensor systems and applications;Actuators;Software systems","embedded systems;engineering education;microactuators;microsensors;nanotechnology;optimisation;reliability;software engineering","microsensors;microactuators;nanotechnology;software system;reliability;optimisation;embedded system;electron device design;engineering education","","","","","","","","","IEEE","IEEE Conferences"
"Update: Taiwanese Software Spots Stock-Market Stinkers","","","IEEE Spectrum","","2008","45","5","12","12","Taiwanese computer scientists have developed a genetic algorithm one that evolves to improve its performance that can predict the impending demise or distress of publicly traded companies. The algorithm starts with 39 variables, such as sales growth rate and market value, and assigns weights to their influence. Over several iterations it optimizes the weights and then uses pattern-recognition routines to pick the losers. So far the algorithm has been tested only on Taiwanese companies, but the scientists say that it can evolve to work anywhere.","0018-9235;1939-9340","","10.1109/MSPEC.2008.4505296","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4505296","","Wind energy;Software performance;Genetics;Software algorithms;Marketing and sales;Testing;Neutrons;Mass spectroscopy;Space vehicles;Moon","commerce;genetic algorithms;iterative methods;pattern recognition;stock markets","Taiwanese software spots;stock-market stinkers;genetic algorithm;publicly traded company;iterative method;pattern-recognition","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","2009 International Conference on Industrial Mechatronics and Automation","","2009","","","c1","c1","The following topics are dealt with: bionic neuron unified model; library-similar knowledge-increasable neural network group; wireless sensor networks; multi-intelligent agents; ant colony optimization; personalized distance learning system; network vulnerability detection; cantilever beam MEMS switch; logo-removal forgery detection; dilute acid delinting; OBB trees; virtual assembly; plasma etching processs monitoring; TSHD swell compensator; video supervision technology; metal oxide semiconductor sensor; RBS neural network; dual core FFT processor; FPGA; storage bin capacity; constrained manipulators intelligent active vibration control; computer supported large scale collaborative manufacturing; digital meter identifier; numbers recognition; compressed natural gas station vehicles; aggressive driving analysis; city road traffic accident; particle swarm optimization; automation testing process modeling method; unmanned helicopter; micro integrated navigation system; software protection; neuromorphic vision system; biological nervous systems; handback skin texture; co-occurence matrix; CAN bus; environment supervision; nonsingular terminal sliding mode; intelligentized teaching resources center; cellular automota; multiple-depot vehicle routing problem; Gauss-Newton image reconstruction algorithm; electrical capacitance tomography; discrete 2D wavelet transform; moving windows autoregressive quadratic model; automobile braking performance test system; intrusion detection systems; symmetric encryption; self-learning epistemologies; machine learning; passenger behavior; obfuscated virus detection; vehicle travelling data recorder; brushless DC motor speed control; SRG modeling; stochastic dynamic Leontief input-output model; photo-multiplier tube; mobile IPv6; mobility management schemes; shunt reactor neutral inductance; deep Web database; PCM electromagnetism characteristics; handwriting identification; description logics reasoning mechanism; financial time series; product quality control; Chinese Gooseberry processing; virtual data auto integration; motion control; multi-electrode quartz crystal resonant; mobile robot; power lines corner detection; genetic algorithm; auto adaptive access network; VLAN-IP technology; microelectromechanical systems; switch-based visual-servo controller; cold-chain logistics delivery schedule; hot strip laminar coding; retrieval oriented robust image hashing; fire alarm system; dual-signal detection; ERP system; IIFWA operator; DIFWG operator; OWC-LOWA operator; risk investment; universal rolling mills; diesel engine active control; data analysis; capital budgeting; electromagnetic tomography; finite element method; groundwater level prediction; non-darcy seepage modeling; body surface representation; mine microseism monitoring 3D visualization system; OpenGL; telescopic shock absorber; intelligent learning objects; personal query; role preference ontology; PCB defect inspection system design; cost-sensitive ant colony data mining algorithm; HVAC fan machinery fault diagnosis method; computer aided decision support system; substation ground wire monitoring system; WLAN; wind generator power forecast; document classification algorithm; intelligent network based mobility control and CDMA.","","978-1-4244-3817","10.1109/ICIMA.2009.5156539","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5156539","","","alarm systems;angular velocity control;automobiles;beams (structures);behavioural sciences computing;biocybernetics;biology computing;braking;brushless DC motors;budgeting;cantilevers;cellular automata;code division multiple access;computer graphics;computerised tomography;controller area networks;cryptography;data analysis;data mining;decision support systems;diesel engines;discrete wavelet transforms;distance learning;document handling;driver information systems;fault diagnosis;field programmable gate arrays;finite element analysis;Gaussian processes;genetic algorithms;handwriting recognition;helicopters;HVAC;image reconstruction;image texture;industrial robots;IP networks;learning (artificial intelligence);libraries;logistics;machine control;manipulators;matrix algebra;microprocessor chips;microswitches;mobile robots;mobility management (mobile radio);multi-agent systems;neural nets;object detection;particle swarm optimisation;power cables;query processing;radial basis function networks;remotely operated vehicles;road accidents;servomechanisms;signal detection;sputter etching;transportation;trees (mathematics);variable structure systems;vibration control;video surveillance;wireless LAN;wireless sensor networks","bionic neuron unified model;library-similar knowledge-increasable neural network group;wireless sensor networks;multi-intelligent agents;ant colony optimization;personalized distance learning system;network vulnerability detection;cantilever beam MEMS switch;logo-removal forgery detection;dilute acid delinting;OBB trees;virtual assembly;plasma etching processs monitoring;TSHD swell compensator;video supervision technology;metal oxide semiconductor sensor;RBS neural network;dual core FFT processor;FPGA;storage bin capacity;constrained manipulators intelligent active vibration control;computer supported large scale collaborative manufacturing;digital meter identifier;numbers recognition;compressed natural gas station vehicles;aggressive driving analysis;road traffic accident;particle swarm optimization;automation testing process modeling method;unmanned helicopter;micro integrated navigation system;software protection;neuromorphic vision system;biological nervous systems;handback skin texture;co-occurence matrix;CAN bus;environment supervision;nonsingular terminal sliding mode;intelligentized teaching resources center;cellular automota;multiple-depot vehicle routing problem;Gauss-Newton image reconstruction algorithm;electrical capacitance tomography;discrete 2D wavelet transform;moving windows autoregressive quadratic model;automobile braking performance test system;intrusion detection systems;symmetric encryption;self-learning epistemologies;machine learning;passenger behavior;obfuscated virus detection;vehicle travelling data recorder;brushless DC motor speed control;SRG modeling;stochastic dynamic Leontief input-output model;photo-multiplier tube;mobile IPv6;mobility management schemes;shunt reactor neutral inductance;deep Web database;PCM electromagnetism characteristics;handwriting identification;description logics reasoning mechanism;financial time series;product quality control;Chinese Gooseberry processing;virtual data auto integration;motion control;multi-electrode quartz crystal resonant;mobile robot;power lines corner detection;genetic algorithm;auto adaptive access network;VLAN-IP technology;microelectromechanical systems;switch-based visual-servo controller;cold-chain logistics delivery schedule;hot strip laminar coding;retrieval oriented robust image hashing;fire alarm system;dual-signal detection;ERP system;IIFWA operator;DIFWG operator;OWC-LOWA operator;risk investment;universal rolling mills;diesel engine active control;data analysis;capital budgeting;electromagnetic tomography;finite element method;groundwater level prediction;non-darcy seepage modeling;body surface representation;mine microseism monitoring 3D visualization system;OpenGL;telescopic shock absorber;intelligent learning objects;personal query;role preference ontology;PCB defect inspection system design;cost-sensitive ant colony data mining algorithm;HVAC fan machinery fault diagnosis method;computer aided decision support system;substation ground wire monitoring system;WLAN;wind generator power forecast;document classification algorithm;intelligent network based mobility control;CDMA","","","","","","","","","IEEE","IEEE Conferences"
"[Title page i]","","","2009 12th Euromicro Conference on Digital System Design, Architectures, Methods and Tools","","2009","","","i","i","This paper dealt with the following topics: systems-on-a-chip; multiprocessor; system synthesis; circuit design; fault tolerance; digital system design; arithmetic circuit synthesis; system-level energy optimization; HW-SW embedded systems; flexible digital radio; programmable-re-configurable architectures; embedded-digital system applications; digital system testing; logic synthesis; wireless sensor networks; and, other poster papers.","","978-0-7695-3782","10.1109/DSD.2009.242","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5350148","","","digital radio;fault tolerance;flexible electronics;hardware-software codesign;logic circuits;multiprocessing systems;network synthesis;optimisation;system-on-chip;wireless sensor networks","systems-on-a-chip;multiprocessor;system synthesis;circuit design;fault tolerance;digital system design;arithmetic circuit synthesis;system-level energy optimization;HW-SW embedded systems;flexible digital radio;programmable-re-configurable architectures;embedded-digital system applications;digital system testing;logic synthesis;wireless sensor networks","","","","","","","","","IEEE","IEEE Conferences"
"[Front cover]","","","2007 International Conference on Perspective Technologies and Methods in MEMS Design","","2007","","","1","1","The following topics are dealt with: microelectromechanical systems (MEMS); microsensor design; microactuator modeling; microdesign analysis; optimization; models development; embedded system design; software systems; system design algorithms; system design strategies; system testing; system verification; system reliability; sensor systems; actuator systems; MEMS applications; microsystems simulation; power devices; technology transfer; mixed design education; and integrated circuits applications.","","978-966-553-614","10.1109/MEMSTECH.2007.4283409","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4283409","","","electrical engineering computing;electrical engineering education;embedded systems;integrated circuit design;microactuators;microsensors;power electronics;reliability;technology transfer;testing","integrated circuits;mixed design education;technology transfer;power devices;microsystems simulation;actuator systems;sensor systems;system reliability;system verification;system testing;system design strategies;system design algorithms;software systems;embedded system design;models development;optimization;microdesign analysis;microactuator modeling;microsensor design;microelectromechanical systems;MEMS design","","","","","","","","","IEEE","IEEE Conferences"
"[Front and back covers]","","","2009 4th International Conference on Design & Technology of Integrated Systems in Nanoscal Era","","2009","","","xxix","xxx","The following topics are dealt with: system-level modeling; noise-tolerant design; graphical optimisation; RF and wireless designs: multistandard RF nonuniformly sampling receiver, RF microsystems for wireless sensors networks, and wireless transceivers; hardware-software codesign architecture; networks-on-chip; device modeling and simulation- FinFET, spin-transfer torque based magnetic tunnel junction, c-silicon solar cells, MISS structure based silicon, and ballistic double gate MOSFETs; fault simulation, analysis, and testing; power optimisation; and signal processing.","","978-1-4244-4320-8978-1-4244-4321","10.1109/DTIS.2009.4938001","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4938001","","","electrical faults;magnetic tunnelling;MIS structures;MOSFET;optimisation;signal processing;solar cells;transceivers;wireless sensor networks","system-level modeling;noise-tolerant design;graphical optimisation;multistandard RF nonuniformly sampling receiver;RF microsystems;wireless sensors networks;wireless transceivers;hardware-software codesign architecture;networks-on-chip;FinFET;spin-transfer torque based magnetic tunnel junction;c-silicon solar cells;MISS structure based silicon;ballistic double gate MOSFET;fault;power optimisation;signal processing","","","","","","","","","IEEE","IEEE Conferences"
"Introduction to Feedback Control (Morris, K.; 2001) [Book Review]","P. Dorato","NA","IEEE Control Systems Magazine","","2007","27","1","75","77","The book is examined in the context of its suitability for a first engineering course in feedback control. The text covers several topics normally included in the most current introductory feedback control texts, including: transfer functions and state-space descriptions; Routh-Hurwitz and Nyquist stability criteria; and state-feedback and linear-quadratic regulator theory. It also includes more contemporary topics, such as: robust stability and performance for unstructured perturbations; strong stabilization and Youla parameterization; generalized plants and H/sub/spl infin// design; and H/sub/spl infin// optimization by model matching and interpolation theory. Short appendices are included on normed linear spaces, matrix and abstract algebra, and state-space system manipulations. The material is well presented, with careful proofs of key results. Each chapter includes numerical examples, together with a good list of homework exercises. Some Matlab functions for computing numerical solution are mentioned, but the discussion of software is limited. Unfortunately, the mathematical level of the test is too high, at least for most programs in the United States. The book also omits several standard topics associated with engineering course on feedback control, such as modeling of physical systems, root-locus analysis, digital control, and an introduction to nonlinear systems. As it is, the book would make a very good test for a graduate engineering course in linear feedback control. It could also be a valuable reference for graduate engineering students or applied mathematics students interested in research in the control area.","1066-033X;1941-000X","","10.1109/MCS.2007.284512","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4064851","","Book reviews;Feedback control;Mathematical model;Testing;Transfer functions;Stability criteria;Regulators;Robust stability;Design optimization;Interpolation","","","","","11","","","","","","IEEE","IEEE Journals & Magazines"
"Perspective technologies and methods in MEMS design","","","2008 International Conference on Perspective Technologies and Methods in MEMS Design","","2008","","","c1","c1","The following topics are dealt with: analysis, modelling, research and design methods of microsensors and microactuators; software systems, models, algorithms, methods and strategies of embedded system design; field problems in embedded system, modelling and design; problems of testing, verification, reliability and optimization in embedded systems; modelling and design; sensors and actuators.","","978-966-2191-00","10.1109/MEMSTECH.2008.4558718","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4558718","","","embedded systems;microactuators;micromechanical devices;microsensors","MEMS design;microelectromechanical devices;microsensors;microactuators;embedded system design;embedded system modelling;embedded system testing;embedded system verification;embedded system reliability","","","","","","","","","IEEE","IEEE Conferences"
